<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-08T12:29:39+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI. Hey, Tony</subtitle>
  <entry>
    <id>t3_1o18p7i</id>
    <title>Replacing Google Translate with LLM translation app on smartphone?</title>
    <updated>2025-10-08T12:11:15+00:00</updated>
    <author>
      <name>/u/dtdisapointingresult</name>
      <uri>https://old.reddit.com/user/dtdisapointingresult</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I chat with a thai person semi-often on my phone, in thai. I use Google Translate to convert between Thai and English. &lt;/p&gt; &lt;p&gt;According to them, the thai text I send them is confusing and sometimes completely incomprehensible. And on my own end, often the english translation of what they send me is very unclear.&lt;/p&gt; &lt;p&gt;Today I tried something new: I tried an LLM on OpenRouter for the translation. They immediately understood, and said my messages were leaps and bounds better than Google Translate.&lt;/p&gt; &lt;p&gt;So now I'm looking to replace Google Translate on my Android phone. I'd really like the convenience of a dedicated translation and don't want to be using a browser and typing &amp;quot;Translate the following to/from $lang: my message here&amp;quot; and having to edit the prompt every time.&lt;/p&gt; &lt;p&gt;However, surprisingly I can't find any LLM translation apps on the Play Store or on F-Droid or in this sub's search. Through more googling I finally managed to find one chinese app, but it only supports OpenAI.&lt;/p&gt; &lt;p&gt;Surely I'm not the first person to hit a wall with Google Translate and wants LLM translation on their phone? Are there any obscure apps for this on Github which I can't find?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dtdisapointingresult"&gt; /u/dtdisapointingresult &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o18p7i/replacing_google_translate_with_llm_translation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o18p7i/replacing_google_translate_with_llm_translation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o18p7i/replacing_google_translate_with_llm_translation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T12:11:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0zxuu</id>
    <title>I built a local Whisper-based dictation app for Windows (no cloud, runs fully offline) but I'm finding difficulty making it seamlessly compatible on different devices.</title>
    <updated>2025-10-08T03:32:26+00:00</updated>
    <author>
      <name>/u/ProMogrem</name>
      <uri>https://old.reddit.com/user/ProMogrem</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0zxuu/i_built_a_local_whisperbased_dictation_app_for/"&gt; &lt;img alt="I built a local Whisper-based dictation app for Windows (no cloud, runs fully offline) but I'm finding difficulty making it seamlessly compatible on different devices." src="https://external-preview.redd.it/zeUCPuIO4Kd1N3TxiRJJx2lCyctkGJHioA76izLoHYE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3f6853280aa250eeac39f4a71eef22546cdebf11" title="I built a local Whisper-based dictation app for Windows (no cloud, runs fully offline) but I'm finding difficulty making it seamlessly compatible on different devices." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I noticed that while macOS users have Superwhisper, there wasn't a real local dictation/speech-to-text app for Windows, so I built one.The app runs fully offline, using Whisper models (tiny, base, small, medium, large-v3) accelerated on CUDA. It transcribes in batch mode (record then transcribe), captures microphone audio only, and lets you &amp;quot;type anywhere&amp;quot;, meaning you can press a hotkey, speak, and it automatically pastes the transcription into any app (like Notepad, Word, Discord, etc.)&lt;/p&gt; &lt;p&gt;It is basically an alternative to SuperWhisper for windows: Whisper4Windows&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The problem I am having:&lt;/strong&gt;&lt;br /&gt; The installer I built is supposed to detect if any dependencies like cublas and cuDNN need downloading, if so it prompts the user to do so. However, I tried it on a laptop with a GTX 1060 Mobile, but the automatic cuDNN installation fails, the rest work, and even if I install cuDNN manually it still results in this error: &lt;code&gt;Could not locate cudnn_ops64_9.dll&lt;/code&gt;&lt;br /&gt; This is confusing me, because on another device (4060 Mobile) with manually installed cuDNN files it works just fine&lt;br /&gt; The installer is in releases on GitHub, it is built using: &lt;code&gt;cd ./frontend/src-tauri/; cargo tauri build&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/BaderJabri/Whisper4Windows"&gt;https://github.com/BaderJabri/Whisper4Windows&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Key features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;CUDA-accelerated (optimized for RTX GPUs, falls back to CPU)&lt;/li&gt; &lt;li&gt;WASAPI microphone capture only (no system audio/loopback)&lt;/li&gt; &lt;li&gt;Silero-VAD / WebRTC-VAD for live chunking and low latency~~ VAD is disabled in current implementation&lt;/li&gt; &lt;li&gt;Live captions overlay (optional small window)~~ No live captions - shows recording window during capture&lt;/li&gt; &lt;li&gt;Custom shortcuts for starting stopping and canceling&lt;/li&gt; &lt;li&gt;Optional save to clipboard toggle&lt;/li&gt; &lt;li&gt;Sound effects&lt;/li&gt; &lt;li&gt;Lightweight Tauri frontend + Python backend&lt;/li&gt; &lt;li&gt;Everything is open source, you can inspect, build, or modify it.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/g6vu6c4p5ttf1.png?width=1728&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ec6e0ea9311138d0b72eec22690f750be70511d8"&gt;https://preview.redd.it/g6vu6c4p5ttf1.png?width=1728&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ec6e0ea9311138d0b72eec22690f750be70511d8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fly6gqfq5ttf1.png?width=1321&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1fe366f391d97749f84a7c3e8fea2774316caf49"&gt;https://preview.redd.it/fly6gqfq5ttf1.png?width=1321&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1fe366f391d97749f84a7c3e8fea2774316caf49&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I plan on adding optional local LLM post-processing later after other issues are taking care of&lt;/p&gt; &lt;p&gt;Give it a try&lt;/p&gt; &lt;p&gt;Whipser4Windows&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/BaderJabri/Whisper4Windows"&gt;https://github.com/BaderJabri/Whisper4Windows&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ProMogrem"&gt; /u/ProMogrem &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0zxuu/i_built_a_local_whisperbased_dictation_app_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0zxuu/i_built_a_local_whisperbased_dictation_app_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o0zxuu/i_built_a_local_whisperbased_dictation_app_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T03:32:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1o18yl8</id>
    <title>Building a BPE Tokenizer from scratch - optimizations &amp; experiments</title>
    <updated>2025-10-08T12:23:04+00:00</updated>
    <author>
      <name>/u/garg-aayush</name>
      <uri>https://old.reddit.com/user/garg-aayush</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o18yl8/building_a_bpe_tokenizer_from_scratch/"&gt; &lt;img alt="Building a BPE Tokenizer from scratch - optimizations &amp;amp; experiments" src="https://external-preview.redd.it/NeXI5jnwEOwXIs7_mVe04Ef6iuVmmMHfc9xat36QzrU.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e1608f9c8291d391bb636cc82c05cdc14a3ead6b" title="Building a BPE Tokenizer from scratch - optimizations &amp;amp; experiments" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Like I did in the past with my GPT-2 reimplementation, this time I followed Andrej Karpathy's &lt;a href="https://www.youtube.com/watch?v=zduSFxRajkE"&gt;&amp;quot;Let's build the GPT Tokenizer&amp;quot;&lt;/a&gt; video tutorial and implemented a BPE tokenizer from scratch. :-)&lt;/p&gt; &lt;p&gt;I went several steps further by identifying and optimizing major bottlenecks in both training and inference, implementing a Rust version for fast encoding, training custom tokenizers on large datasets, and evaluating their impact on GPT-2 pre-training.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/kwaqe610svtf1.png?width=2670&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ef0332d4cfa57dd26ec66730bc7280b4f12d812a"&gt;BPE implementation from scratch summary&lt;/a&gt;&lt;/p&gt; &lt;p&gt;My optimizations and experiments include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Improving training speed: 50x faster&lt;/strong&gt; (117s → 2.4s for 20 merges)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Making inference faster: 3.7x faster&lt;/strong&gt; with Rust implementation (21.3s → 5.3s)&lt;/li&gt; &lt;li&gt;Training custom 16K tokenizers on &lt;strong&gt;TinyStoriesV2 (~2.6GB)&lt;/strong&gt; and &lt;strong&gt;FineWeb (~3.3GB)&lt;/strong&gt; datasets&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Pre-training GPT-2 using custom tokenizers&lt;/strong&gt; and comparing their performance&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;To be honest, I found understanding tokenizer implementation and optimizing it a lot more confusing and harder than GPT-2 implementation (personal experience!) 😅.&lt;/p&gt; &lt;p&gt;In this implementation, I learned a lot about code profiling and optimizing code for both memory and speed. The Rust vibe-coding was fun and surprisingly successful!&lt;/p&gt; &lt;p&gt;Like always, I've documented everything—the code, optimizations, training runs, experiments, and notes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Repo&lt;/strong&gt;: &lt;a href="https://github.com/garg-aayush/building-from-scratch/tree/main/bpe"&gt;https://github.com/garg-aayush/building-from-scratch/tree/main/bpe&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Notes&lt;/strong&gt;: &lt;a href="https://github.com/garg-aayush/building-from-scratch/blob/main/bpe/lecture_notes.md"&gt;https://github.com/garg-aayush/building-from-scratch/blob/main/bpe/lecture_notes.md&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Detailed Readme&lt;/strong&gt;: &lt;a href="https://github.com/garg-aayush/building-from-scratch/blob/main/bpe/Readme.md"&gt;https://github.com/garg-aayush/building-from-scratch/blob/main/bpe/Readme.md&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Commit-by-commit development&lt;/strong&gt;: Each optimization and experiment is a separate commit for easy understanding&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/garg-aayush"&gt; /u/garg-aayush &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o18yl8/building_a_bpe_tokenizer_from_scratch/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o18yl8/building_a_bpe_tokenizer_from_scratch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o18yl8/building_a_bpe_tokenizer_from_scratch/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T12:23:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0wnwt</id>
    <title>M2 Max 96GB vs Strix Halo 128GB?</title>
    <updated>2025-10-08T00:53:09+00:00</updated>
    <author>
      <name>/u/esamueb32</name>
      <uri>https://old.reddit.com/user/esamueb32</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi!&lt;/p&gt; &lt;p&gt;I'm considering purchasing either of these two options&lt;/p&gt; &lt;ul&gt; &lt;li&gt;M2 Max 96GB using macOS&lt;/li&gt; &lt;li&gt;Mini PC with Strix Halo (AMD AI Max+ 395) 128GB using Linux&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;As fas as I know, M2 Max has higher bandwidth (400GB/s) compared to Strix Halo (&amp;lt; 250GB/s).&lt;/p&gt; &lt;p&gt;M2 Max solution is slightly cheaper than strix halo.&lt;/p&gt; &lt;p&gt;But comparing real world benchmarks, which is better?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/esamueb32"&gt; /u/esamueb32 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0wnwt/m2_max_96gb_vs_strix_halo_128gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0wnwt/m2_max_96gb_vs_strix_halo_128gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o0wnwt/m2_max_96gb_vs_strix_halo_128gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T00:53:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0or4w</id>
    <title>How much does 1T tokens cost? How much did all these amazing people spent on OpenAI tokens?</title>
    <updated>2025-10-07T19:32:48+00:00</updated>
    <author>
      <name>/u/aospan</name>
      <uri>https://old.reddit.com/user/aospan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0or4w/how_much_does_1t_tokens_cost_how_much_did_all/"&gt; &lt;img alt="How much does 1T tokens cost? How much did all these amazing people spent on OpenAI tokens?" src="https://b.thumbs.redditmedia.com/KSfI0d7i7eYsGh_o5RwgORs0ebrpshjARUVd3xkxOJk.jpg" title="How much does 1T tokens cost? How much did all these amazing people spent on OpenAI tokens?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I did some math as a follow-up to OpenAI’s Dev Day yesterday and decided to share it here.&lt;/p&gt; &lt;p&gt;Assuming GPT-5 with a 4:1 input:output token ratio, 1T tokens means 800,000 million input tokens at $1.25 per million, which is $1,000,000, plus 200,000 million output tokens at $10 per million, adding $2,000,000, for a total of $3,000,000 for 1T tokens.&lt;/p&gt; &lt;p&gt;On this photo, 30 people consumed 1T tokens, 70 people 100B tokens, and 54 people 10B tokens, totaling $112,620,000, which is roughly 3% of OpenAI’s total $3.7 billion revenue in 2024.&lt;/p&gt; &lt;p&gt;Curious - is it even possible to process this amount of tokens using local models? What would be the cost in GPUs and residential electricity? 🧐⚡️&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aospan"&gt; /u/aospan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/abylayo/status/1975546166113669170?s=46"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0or4w/how_much_does_1t_tokens_cost_how_much_did_all/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o0or4w/how_much_does_1t_tokens_cost_how_much_did_all/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T19:32:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1o165xf</id>
    <title>Entry level question for running local LLMs (AMD GPUs)</title>
    <updated>2025-10-08T09:52:46+00:00</updated>
    <author>
      <name>/u/Mikizeta</name>
      <uri>https://old.reddit.com/user/Mikizeta</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been doing some self-learning about running LLMs locally, but I am far from considering myself knowledgeable in the topic. Hence, I am trying to understand what ways exist to have better hardware for cheap to keep learning and testing.&lt;/p&gt; &lt;p&gt;Currently, I only have my gaming PC: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Ryzen 7600x&lt;/li&gt; &lt;li&gt;32GBs RAM&lt;/li&gt; &lt;li&gt;AsRock B650 PG Lightning&lt;/li&gt; &lt;li&gt;7900GRE, 16GBs VRAM&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I would argue that the main bottleneck here is VRAM, as I couldn't reliably run even Mistral small models when quantized. My tests are done with Fedora and GPT4All/Ollama.&lt;/p&gt; &lt;p&gt;My specific doubt is, would it make sense to buy an rx 9060 xt 16GB and add it to my system? The reasoning is that I find it the cheapest way to double my available VRAM (I may be wrong in my research. If so, feel free to point that out). My limited understanding is that heterogeneous setups are possible. &lt;/p&gt; &lt;p&gt;Yet, I found no information around such GPUs for LLM usage. Either people going for more expensive GPUs (7900xtx, MI series, etc...) or older ones. The cheaper end of recent GPUs seems to not be considered, at least in my research.&lt;/p&gt; &lt;p&gt;Is this a bad idea? If so, why?&lt;br /&gt; Are inference speeds a concern with such a setup? If so, why?&lt;br /&gt; Is it the problem compatibility instead?&lt;br /&gt; Is it that this plan I have is simply not cost-effective when compared to other options? &lt;/p&gt; &lt;p&gt;These are the questions I have been searching answers to, without much success.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mikizeta"&gt; /u/Mikizeta &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o165xf/entry_level_question_for_running_local_llms_amd/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o165xf/entry_level_question_for_running_local_llms_amd/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o165xf/entry_level_question_for_running_local_llms_amd/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T09:52:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1o16p5f</id>
    <title>Did anyone actually managed to configure Fill in the middle (FIM) in VSCode with open-webui serving the LLMs ?</title>
    <updated>2025-10-08T10:25:06+00:00</updated>
    <author>
      <name>/u/marsxyz</name>
      <uri>https://old.reddit.com/user/marsxyz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am getting crazy.&lt;/p&gt; &lt;p&gt;In chat mod, everything works perfectly both in Twinny and in Continue. But in FIM, both give me the same error; 400 bad request. &lt;/p&gt; &lt;p&gt;The fact that both Continue and Twinny gives me this result seem to indicate it is linked to open-webui, but I have NO IDEA what is wrong. Did anyone actually manage to configure something similar ? What do you use for FIM in VSCode ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/marsxyz"&gt; /u/marsxyz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o16p5f/did_anyone_actually_managed_to_configure_fill_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o16p5f/did_anyone_actually_managed_to_configure_fill_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o16p5f/did_anyone_actually_managed_to_configure_fill_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T10:25:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0i4fz</id>
    <title>Will DDR6 be the answer to LLM?</title>
    <updated>2025-10-07T15:34:17+00:00</updated>
    <author>
      <name>/u/fungnoth</name>
      <uri>https://old.reddit.com/user/fungnoth</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Bandwidth doubles every generation of system memory. And we need that for LLMs. &lt;/p&gt; &lt;p&gt;If DDR6 is going to be 10000+ MT/s easily, and then dual channel and quad channel would boast that even more. Maybe we casual AI users would be able to run large models around 2028. Like deepseek sized full models in a chat-able speed. And the workstation GPUs will only be worth buying for commercial use because they serve more than one user at a time. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fungnoth"&gt; /u/fungnoth &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0i4fz/will_ddr6_be_the_answer_to_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0i4fz/will_ddr6_be_the_answer_to_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o0i4fz/will_ddr6_be_the_answer_to_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T15:34:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1o12017</id>
    <title>Fine-tuning Agents using Tools with Reinforcement Learning</title>
    <updated>2025-10-08T05:26:01+00:00</updated>
    <author>
      <name>/u/Successful_Table_263</name>
      <uri>https://old.reddit.com/user/Successful_Table_263</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o12017/finetuning_agents_using_tools_with_reinforcement/"&gt; &lt;img alt="Fine-tuning Agents using Tools with Reinforcement Learning" src="https://external-preview.redd.it/bGeVsktTmkNw9jG2f-B7KrIzK4_rIpSY4X8FAYjd7Rc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=38a781766a3a221afdd54e80775c2a539e457809" title="Fine-tuning Agents using Tools with Reinforcement Learning" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When running SmolAgents CodeAct for tool calling, we often observe that smaller open-source models struggle with complex tool-use tasks — and sometimes even fail at simple ones. While careful prompt engineering can mitigate this problem, it’s not a sustainable solution, especially in dynamic agentic systems where any workflow change can disrupt tool-calling accuracy.&lt;/p&gt; &lt;p&gt;To address this issue at its core, the ideal approach is to train models to use tools effectively. However, this is a non-trivial task that requires setting up complex machine learning pipelines tightly integrated with the agentic system — something that can be challenging for most developers.&lt;/p&gt; &lt;p&gt;To make this process easier, we’ve developed a lightweight open-source library that removes the need to build these pipelines from scratch with MIT license for more information &lt;a href="https://github.com/ToolBrain/ToolBrain"&gt;ToolBrain&lt;/a&gt;&lt;/p&gt; &lt;p&gt;✨ Key Features&lt;/p&gt; &lt;pre&gt;&lt;code&gt;🤖 Learning algorithms: Supports GRPO, DPO, and supervised learning. 🎯 Flexible rewards: Define your own reward functions or use LLM-as-judge. 🔧 Tool management: Scalable retrieval for managing large tool collections. 📊 Knowledge distillation: Distill large teacher models into smaller student models for efficiency. 🚀 Zero-learn: Automatically generate training tasks. ⚡ Efficient training: Supports FP16 finetuning, LoRA, Unsloth, and BitsAndBytes for resource-efficient training. 🧠 Multiple agent frameworks: Supports SmolAgent and LangChain, with more coming soon. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;A simple example:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from smolagents import tool, TransformersModel, CodeAgent from toolbrain import Brain from toolbrain.rewards import reward_exact_match # --- 1. Define Tools and Reward Function (User-defined) --- u/tool def add(a: int, b: int) -&amp;gt; int: &amp;quot;&amp;quot;&amp;quot; Add two integers. Args: a (int): First addend. b (int): Second addend. Returns: int: Sum of a and b. &amp;quot;&amp;quot;&amp;quot; return a + b # --- 2. Prepare Training Data --- training_dataset = [ { &amp;quot;query&amp;quot;: &amp;quot;Use the add tool to calculate 5 + 7&amp;quot;, &amp;quot;gold_answer&amp;quot;: &amp;quot;12&amp;quot; } ] # 3. Create agent model = TransformersModel( model_id=&amp;quot;Qwen/Qwen2.5-0.5B-Instruct&amp;quot;, # use a bigger model for better results max_new_tokens=128 ) agent = CodeAgent( model=model, tools=[add], max_steps=1 ) # 4. Create Brain brain = Brain( agent, # Agent instance algorithm=&amp;quot;GRPO&amp;quot;, # Algorithm choice reward_func=reward_exact_match # A reward function, you can customise any python function as reward ) # 5. Train the agent with GRPO steps brain.train(training_dataset, num_iterations=10) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The following plot illustrates how ToolBrain enhances the tool usage accuracy of the small Qwen/Qwen2.5-0.5B-Instruct model after just 20 training steps using GRPO.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/bbclflgwpttf1.png?width=1980&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5407e2c2d283eb3f614d439175de7328a22c1eba"&gt;https://preview.redd.it/bbclflgwpttf1.png?width=1980&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5407e2c2d283eb3f614d439175de7328a22c1eba&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Successful_Table_263"&gt; /u/Successful_Table_263 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o12017/finetuning_agents_using_tools_with_reinforcement/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o12017/finetuning_agents_using_tools_with_reinforcement/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o12017/finetuning_agents_using_tools_with_reinforcement/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T05:26:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0f2uf</id>
    <title>Hi folks, sorry for the self‑promo. I’ve built an open‑source project that could be useful to some of you</title>
    <updated>2025-10-07T13:40:33+00:00</updated>
    <author>
      <name>/u/panos_s_</name>
      <uri>https://old.reddit.com/user/panos_s_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0f2uf/hi_folks_sorry_for_the_selfpromo_ive_built_an/"&gt; &lt;img alt="Hi folks, sorry for the self‑promo. I’ve built an open‑source project that could be useful to some of you" src="https://preview.redd.it/1tzatvfz0ptf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=149ffc98b84835693e3aa54c4c554277120de6ea" title="Hi folks, sorry for the self‑promo. I’ve built an open‑source project that could be useful to some of you" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;: Web dashboard for NVIDIA GPUs with 30+ real-time metrics (utilisation, memory, temps, clocks, power, processes). Live charts over WebSockets, multi‑GPU support, and one‑command Docker deployment. No agents, minimal setup.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/psalias2006/gpu-hot"&gt;https://github.com/psalias2006/gpu-hot&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why I built it&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Wanted simple, real‑time visibility without standing up a full metrics stack.&lt;/li&gt; &lt;li&gt;Needed clear insight into temps, throttling, clocks, and active processes during GPU work.&lt;/li&gt; &lt;li&gt;A lightweight dashboard that’s easy to run at home or on a workstation.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What it does&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Polls nvidia-smi and streams 30+ metrics every ~2s via WebSockets.&lt;/li&gt; &lt;li&gt;Tracks per‑GPU utilization, memory (used/free/total), temps, power draw/limits, fan, clocks, PCIe, P‑State, encoder/decoder stats, driver/VBIOS, throttle status.&lt;/li&gt; &lt;li&gt;Shows active GPU processes with PIDs and memory usage.&lt;/li&gt; &lt;li&gt;Clean, responsive UI with live historical charts and basic stats (min/max/avg).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Setup (Docker)&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;git clone https://github.com/psalias2006/gpu-hot cd gpu-hot docker-compose up --build # open http://localhost:1312 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Looking for feedback&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/panos_s_"&gt; /u/panos_s_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1tzatvfz0ptf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0f2uf/hi_folks_sorry_for_the_selfpromo_ive_built_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o0f2uf/hi_folks_sorry_for_the_selfpromo_ive_built_an/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T13:40:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1o13rfk</id>
    <title>Is Gemini 2.5 Pro still the best LLM for OCR and data extraction?</title>
    <updated>2025-10-08T07:14:02+00:00</updated>
    <author>
      <name>/u/kitgary</name>
      <uri>https://old.reddit.com/user/kitgary</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My usecase is to extract data and format to JSON structured data from over a million image receipts, I am researching the best way to do it, it's not simple paper receipts, they are app photos taken directly by phone camera. so traditional OCR has a lot of noise.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kitgary"&gt; /u/kitgary &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o13rfk/is_gemini_25_pro_still_the_best_llm_for_ocr_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o13rfk/is_gemini_25_pro_still_the_best_llm_for_ocr_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o13rfk/is_gemini_25_pro_still_the_best_llm_for_ocr_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T07:14:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1o108q5</id>
    <title>MLX port of BDH (Baby Dragon Hatchling) is up</title>
    <updated>2025-10-08T03:48:22+00:00</updated>
    <author>
      <name>/u/vesudeva</name>
      <uri>https://old.reddit.com/user/vesudeva</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve ported the BDH ( &lt;a href="https://github.com/pathwaycom/bdh"&gt;https://github.com/pathwaycom/bdh&lt;/a&gt; ) model to MLX for Apple Silicon. It’s a faithful conversion of the PyTorch version: same math, same architecture (byte-level vocab, shared weights across layers, ReLU sparsity, RoPE attention with Q=K), with MLX-friendly APIs and a detailed README explaining the few API-level differences and why results are equivalent.&lt;/p&gt; &lt;p&gt;Code, docs, and training script are ready to use. You may need to adjust the training script a bit to fit your own custom dataset. Only tested on M4 so far, but should work perfect for any M1/M2/M3 users out there.&lt;/p&gt; &lt;p&gt;I’m currently training this MLX build on my Internal Knowledge Map (IKM) dataset &lt;a href="https://huggingface.co/datasets/Severian/Internal-Knowledge-Map"&gt;https://huggingface.co/datasets/Severian/Internal-Knowledge-Map&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Training’s underway; expect a day or so before I publish weights. When it’s done, I’ll upload the checkpoint to Hugging Face for anyone to test.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/severian42/BDH-MLX"&gt;https://github.com/severian42/BDH-MLX&lt;/a&gt;&lt;/p&gt; &lt;p&gt;HF model (coming soon): &lt;a href="https://huggingface.co/Severian/BDH-MLX"&gt;https://huggingface.co/Severian/BDH-MLX&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you try it on your own data, feedback and PRs are welcome.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vesudeva"&gt; /u/vesudeva &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o108q5/mlx_port_of_bdh_baby_dragon_hatchling_is_up/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o108q5/mlx_port_of_bdh_baby_dragon_hatchling_is_up/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o108q5/mlx_port_of_bdh_baby_dragon_hatchling_is_up/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T03:48:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1o184q6</id>
    <title>clem from Hugging Face: the community added 1 million new repos (models, datasets, spaces) in the past 90 days! 100% are now powered by Xet, 40% are private repositories. Enterprise hub subscriptions are our fastest growing line of revenue.</title>
    <updated>2025-10-08T11:43:33+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o184q6/clem_from_hugging_face_the_community_added_1/"&gt; &lt;img alt="clem from Hugging Face: the community added 1 million new repos (models, datasets, spaces) in the past 90 days! 100% are now powered by Xet, 40% are private repositories. Enterprise hub subscriptions are our fastest growing line of revenue." src="https://preview.redd.it/capdedupkvtf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1638aaab704ffb77c8b04fc7e98e53289423588a" title="clem from Hugging Face: the community added 1 million new repos (models, datasets, spaces) in the past 90 days! 100% are now powered by Xet, 40% are private repositories. Enterprise hub subscriptions are our fastest growing line of revenue." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Clement Delangue (clem) on 𝕏: &lt;a href="https://x.com/ClementDelangue/status/1975615257923231969"&gt;https://x.com/ClementDelangue/status/1975615257923231969&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/capdedupkvtf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o184q6/clem_from_hugging_face_the_community_added_1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o184q6/clem_from_hugging_face_the_community_added_1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T11:43:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0ypna</id>
    <title>Fixing Apriel-1.5‑15B‑Thinker in Open WebUI: clean final answer + native "Thinking" panel - shareable filter</title>
    <updated>2025-10-08T02:30:22+00:00</updated>
    <author>
      <name>/u/_Bartek</name>
      <uri>https://old.reddit.com/user/_Bartek</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0ypna/fixing_apriel1515bthinker_in_open_webui_clean/"&gt; &lt;img alt="Fixing Apriel-1.5‑15B‑Thinker in Open WebUI: clean final answer + native &amp;quot;Thinking&amp;quot; panel - shareable filter" src="https://external-preview.redd.it/cWo3NjhvM2J1c3RmMW3zfa4MsPKiPRciHsaAQJ1NJDXy-jgFZs4wNTV6PLeF.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b9b1f41112dd7e02b30df6eaeb98a39b1fdc8864" title="Fixing Apriel-1.5‑15B‑Thinker in Open WebUI: clean final answer + native &amp;quot;Thinking&amp;quot; panel - shareable filter" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks,&lt;/p&gt; &lt;p&gt;if you’ve tried Apriel‑1.5‑15B‑Thinker in Open WebUI, you probably noticed it prints a big “Here are my reasoning steps:” section before the final answer, which is wrapped in:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;[BEGIN FINAL RESPONSE] ...final text... [END FINAL RESPONSE] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This is by design (it’s in the model’s chat template), but in Open WebUI it clutters the chat and sometimes even leaves a trailing &amp;quot;[END FINAL RESPONSE&amp;quot; when stop sequences cut the stream mid‑marker.&lt;/p&gt; &lt;p&gt;I put together a small Open WebUI Filter that makes Apriel play nicely with the UI:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;shows a native Thinking panel (&amp;lt;think&amp;gt;…&amp;lt;/think&amp;gt;) for the pre‑final phase,&lt;/li&gt; &lt;li&gt;streams only the final content between [BEGIN…] and [END…],&lt;/li&gt; &lt;li&gt;and avoids the partial [END FINAL RESPONSE artifact.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I’ve published it in the Open WebUI Functions directory: Apriel‑1.5‑15B‑Thinker - Final+Think.&lt;/p&gt; &lt;p&gt;Get it here: &lt;a href="https://openwebui.com/f/supczinskib/apriel_1_5_15b_thinker_final_think"&gt;https://openwebui.com/f/supczinskib/apriel_1_5_15b_thinker_final_think&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hope this helps anyone running Apriel in OWUI.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_Bartek"&gt; /u/_Bartek &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/29rydn3bustf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0ypna/fixing_apriel1515bthinker_in_open_webui_clean/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o0ypna/fixing_apriel1515bthinker_in_open_webui_clean/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T02:30:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0st2o</id>
    <title>BasedBase/Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2 is possibly just a copy of Qwen's regular Qwen3-Coder-30B-A3B-Instruct</title>
    <updated>2025-10-07T22:05:06+00:00</updated>
    <author>
      <name>/u/lemon07r</name>
      <uri>https://old.reddit.com/user/lemon07r</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This was brought up in &lt;a href="https://huggingface.co/BasedBase/Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2/discussions/1"&gt;https://huggingface.co/BasedBase/Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2/discussions/1&lt;/a&gt; and please note the &lt;em&gt;possibly&lt;/em&gt; I use in my language since unverified claims like this can be pretty damning.&lt;/p&gt; &lt;p&gt;Not sure if it's true or not, but one user seems to be convinced by their tests that the models are identical. Maybe someone smarter than me can look into this and verify this&lt;/p&gt; &lt;p&gt;EDIT - Yup. I think at this point it's pretty conclusive that this guy doesnt know what he's doing and vibe coded his way here. The models all have identical weights to the parent models. All of his distils.&lt;/p&gt; &lt;p&gt;Also, let's pay respects to anon user (not so anon if you just visit the thread to see who it is) from the discussion thread that claimed he was very picky and that we could trust him that the model was better:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;a href="https://huggingface.co/BasedBase"&gt;u/BasedBase&lt;/a&gt; feel free to add me to the list of satisfied customers lol. Your 480B coder distill in the small 30B package is something else and you guys can trust me I am VERY picky when it comes to output quality. I have no mercy for bad quality models and this one is certainly an improvement over the regular 30B coder. I've tested both thoroughly.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lemon07r"&gt; /u/lemon07r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0st2o/basedbaseqwen3coder30ba3binstruct480bdistillv2_is/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0st2o/basedbaseqwen3coder30ba3binstruct480bdistillv2_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o0st2o/basedbaseqwen3coder30ba3binstruct480bdistillv2_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T22:05:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1o13oyn</id>
    <title>30B models at full-size, or 120B models at Q4?</title>
    <updated>2025-10-08T07:09:35+00:00</updated>
    <author>
      <name>/u/arimoto02</name>
      <uri>https://old.reddit.com/user/arimoto02</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a set up with an NVIDIA A100 80GB. Should i run Qwen3-8B at full size or Qwen3-32B at Q4?&lt;/p&gt; &lt;p&gt;Also, is there any comprehensive comparison for model degradation with respect to their size/quantize level?&lt;/p&gt; &lt;p&gt;Thank you all!&lt;/p&gt; &lt;p&gt;Edit: Really sorry guys, i somehow remember that there's a qwen3 120b moe (lol). Fixed the post to qwen3 8b vs qwen3 32b.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/arimoto02"&gt; /u/arimoto02 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o13oyn/30b_models_at_fullsize_or_120b_models_at_q4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o13oyn/30b_models_at_fullsize_or_120b_models_at_q4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o13oyn/30b_models_at_fullsize_or_120b_models_at_q4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T07:09:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0puzj</id>
    <title>Samsung Paper Reveals a Recursive Technique that Beats Gemini 2.5 Pro on ARC-AGI with 0.01% of the Parameters!</title>
    <updated>2025-10-07T20:13:48+00:00</updated>
    <author>
      <name>/u/abdouhlili</name>
      <uri>https://old.reddit.com/user/abdouhlili</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdouhlili"&gt; /u/abdouhlili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2510.04871"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0puzj/samsung_paper_reveals_a_recursive_technique_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o0puzj/samsung_paper_reveals_a_recursive_technique_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T20:13:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1o150k1</id>
    <title>Just finished a fun open source project, a full stack system that fetches RSS feeds, uses an AI agent pipeline to write new articles, and automatically serves them through a Next.js site all done locally with Ollama and ChromaDB.</title>
    <updated>2025-10-08T08:36:57+00:00</updated>
    <author>
      <name>/u/KonradFreeman</name>
      <uri>https://old.reddit.com/user/KonradFreeman</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built a project called AutoBlog that runs entirely on my local computer and uses a fully agentic setup to generate new blog posts grounded in my own data. It can ingest any files I choose, text documents, PDFs, or notes, and store them as embeddings in a local ChromaDB vector database. This database acts as the system’s knowledge base. Every piece of text I add becomes part of its contextual memory, so when the model generates new writing, it is informed by that material instead of relying on an external API or remote data source.&lt;/p&gt; &lt;p&gt;The core of the system is a group of coordinated agents that interact through a retrieval and generation loop. A researcher agent retrieves relevant context from the vector database, a writer agent synthesizes that information into a coherent draft, and an editor agent refines the result into a final piece of writing. All inference is done locally through Ollama, so each agent’s reasoning and communication happen within the boundaries of my own machine.&lt;/p&gt; &lt;p&gt;The system can also ingest external information through RSS feeds. These feeds are listed in a YAML configuration file, and the fetcher component parses and embeds their contents into the same vector store. This allows the model to combine current information from the web with my personal archive of documents, creating a grounded context for generation.&lt;/p&gt; &lt;p&gt;When the agents finish a cycle, they output a markdown file with frontmatter including title, date, tags, and a short description. A Next.js frontend automatically turns these files into a working blog. Each post reflects a blend of retrieved knowledge, reasoning across sources, and stylistic refinement from the multi-agent pipeline.&lt;/p&gt; &lt;p&gt;Everything about AutoBlog happens locally: retrieval, inference, vector storage, and rendering. It is built as a self-contained ecosystem that can think and write using whatever knowledge I choose to feed it. By grounding generation in my own material and letting specialized agents collaborate to research, write, and edit, it becomes an autonomous but controlled writer that evolves based on the data I provide.&lt;/p&gt; &lt;p&gt;Repository: &lt;a href="https://github.com/kliewerdaniel/autoblog01"&gt;https://github.com/kliewerdaniel/autoblog01&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KonradFreeman"&gt; /u/KonradFreeman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o150k1/just_finished_a_fun_open_source_project_a_full/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o150k1/just_finished_a_fun_open_source_project_a_full/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o150k1/just_finished_a_fun_open_source_project_a_full/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T08:36:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0vjxr</id>
    <title>There isn’t a single AI Agent on the market that can give you a day of work</title>
    <updated>2025-10-08T00:02:04+00:00</updated>
    <author>
      <name>/u/Working-Magician-823</name>
      <uri>https://old.reddit.com/user/Working-Magician-823</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I use AI Agents all day, and some of them can do very good work, but, none of them can complete a large task by themselves without human intervention. None of them can spend a full day of work, even if you give detailed requirements.&lt;/p&gt; &lt;p&gt;If AI Agents can’t do a full software without a human yet, it is unlikely they are ready to be fully adopted by any business.&lt;/p&gt; &lt;p&gt;Smarter AI is coming for sure, just not what we have today&lt;/p&gt; &lt;p&gt;And a PHD level human, or bachelor’s degree, can complete a product, but I keep hearing AI is PHD level, well!!! It is smart but unable to do the full work that is not PHD..ish&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Working-Magician-823"&gt; /u/Working-Magician-823 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0vjxr/there_isnt_a_single_ai_agent_on_the_market_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0vjxr/there_isnt_a_single_ai_agent_on_the_market_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o0vjxr/there_isnt_a_single_ai_agent_on_the_market_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T00:02:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1o16584</id>
    <title>[2510.05688] vAttention: Verified Sparse Attention</title>
    <updated>2025-10-08T09:51:31+00:00</updated>
    <author>
      <name>/u/Elven77AI</name>
      <uri>https://old.reddit.com/user/Elven77AI</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Elven77AI"&gt; /u/Elven77AI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2510.05688"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o16584/251005688_vattention_verified_sparse_attention/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o16584/251005688_vattention_verified_sparse_attention/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T09:51:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1o11udk</id>
    <title>is GTX 3090 24GB GDDR6 good for local coding?</title>
    <updated>2025-10-08T05:16:36+00:00</updated>
    <author>
      <name>/u/TruthTellerTom</name>
      <uri>https://old.reddit.com/user/TruthTellerTom</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Codex-CLI API costs are getting expensive quick. Found a local used 24 GB RTX 3090 at around 500 bucks. Would this be a good investment? and what local coding LLM would you guys recommend with it?&lt;/p&gt; &lt;p&gt;Desktop Specs:&lt;br /&gt; i7 12700 (12th Gen), 32GB RAM, windows 11 x64&lt;/p&gt; &lt;p&gt;ENV.&lt;/p&gt; &lt;p&gt;Web Applications with PHP, MySQL, jQuery.&lt;br /&gt; Mainly Boostrap 5 (or latest) for style/theme/ready-to-us components&lt;br /&gt; Solo Dev. I keep things simple, and focus on functions. 99% Functional programming.&lt;br /&gt; I dont use frameworks like laravel, i have my own Js and php lib and helpers for most stuff.&lt;/p&gt; &lt;p&gt;would appreciate some expert advise&lt;br /&gt; Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TruthTellerTom"&gt; /u/TruthTellerTom &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o11udk/is_gtx_3090_24gb_gddr6_good_for_local_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o11udk/is_gtx_3090_24gb_gddr6_good_for_local_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o11udk/is_gtx_3090_24gb_gddr6_good_for_local_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T05:16:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0ifyr</id>
    <title>Glm 4.6 air is coming</title>
    <updated>2025-10-07T15:46:04+00:00</updated>
    <author>
      <name>/u/Namra_7</name>
      <uri>https://old.reddit.com/user/Namra_7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0ifyr/glm_46_air_is_coming/"&gt; &lt;img alt="Glm 4.6 air is coming" src="https://preview.redd.it/nmwtp72fnptf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=78e29ea88c42c50216e45dc228bec7e885394f0c" title="Glm 4.6 air is coming" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Namra_7"&gt; /u/Namra_7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nmwtp72fnptf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0ifyr/glm_46_air_is_coming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o0ifyr/glm_46_air_is_coming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T15:46:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0zted</id>
    <title>LFM2-8B-A1B | Quality ≈ 3–4B dense, yet faster than Qwen3-1.7B</title>
    <updated>2025-10-08T03:25:57+00:00</updated>
    <author>
      <name>/u/touhidul002</name>
      <uri>https://old.reddit.com/user/touhidul002</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0zted/lfm28ba1b_quality_34b_dense_yet_faster_than/"&gt; &lt;img alt="LFM2-8B-A1B | Quality ≈ 3–4B dense, yet faster than Qwen3-1.7B" src="https://external-preview.redd.it/4KqP_OSthEkz01ewHYfZ8d2q4c416HGrMsoPMLQI0Ig.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=52c66b4130900a73ea5487c4cac5cb3f5e8fd5eb" title="LFM2-8B-A1B | Quality ≈ 3–4B dense, yet faster than Qwen3-1.7B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;LFM2 is a new generation of hybrid models developed by &lt;a href="https://www.liquid.ai/blog/lfm2-8b-a1b-an-efficient-on-device-mixture-of-experts"&gt;Liquid AI&lt;/a&gt;, specifically designed for edge AI and on-device deployment. It sets a new standard in terms of quality, speed, and memory efficiency.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/oaielly54ttf1.jpg?width=1953&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=2013bd08578cf610969ab162642e20a42264eac1"&gt;https://preview.redd.it/oaielly54ttf1.jpg?width=1953&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=2013bd08578cf610969ab162642e20a42264eac1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The weights of their first MoE based on LFM2, with 8.3B total parameters and 1.5B active parameters.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;LFM2-8B-A1B is the best on-device MoE in terms of both &lt;strong&gt;quality&lt;/strong&gt; (comparable to 3-4B dense models) and &lt;strong&gt;speed&lt;/strong&gt; (faster than Qwen3-1.7B).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Code and knowledge&lt;/strong&gt; capabilities are significantly improved compared to LFM2-2.6B.&lt;/li&gt; &lt;li&gt;Quantized variants fit comfortably on high-end &lt;strong&gt;phones, tablets, and laptops&lt;/strong&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Find more information about LFM2-8B-A1B in their &lt;a href="https://www.liquid.ai/blog/"&gt;blog post&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/LiquidAI/LFM2-8B-A1B"&gt;https://huggingface.co/LiquidAI/LFM2-8B-A1B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/touhidul002"&gt; /u/touhidul002 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0zted/lfm28ba1b_quality_34b_dense_yet_faster_than/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0zted/lfm28ba1b_quality_34b_dense_yet_faster_than/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o0zted/lfm28ba1b_quality_34b_dense_yet_faster_than/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T03:25:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0php3</id>
    <title>Granite Docling WebGPU: State-of-the-art document parsing 100% locally in your browser.</title>
    <updated>2025-10-07T20:00:14+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0php3/granite_docling_webgpu_stateoftheart_document/"&gt; &lt;img alt="Granite Docling WebGPU: State-of-the-art document parsing 100% locally in your browser." src="https://external-preview.redd.it/bXZwenNmemR3cXRmMQIkfIP27ngHfIf2o9FEvt2htapLOK3sF-ey3U1M3aWC.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=681db9c824b6d01bd93193fb35668e28576d4f98" title="Granite Docling WebGPU: State-of-the-art document parsing 100% locally in your browser." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;IBM recently released Granite Docling, a 258M parameter VLM engineered for efficient document conversion. So, I decided to build a demo which showcases the model running entirely in your browser with WebGPU acceleration. Since the model runs locally, no data is sent to a server (perfect for private and sensitive documents).&lt;/p&gt; &lt;p&gt;As always, the demo is available and open source on Hugging Face: &lt;a href="https://huggingface.co/spaces/ibm-granite/granite-docling-258M-WebGPU"&gt;https://huggingface.co/spaces/ibm-granite/granite-docling-258M-WebGPU&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hope you like it!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/33mh4fzdwqtf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0php3/granite_docling_webgpu_stateoftheart_document/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o0php3/granite_docling_webgpu_stateoftheart_document/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T20:00:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1o164ye</id>
    <title>Meta Superintelligence’s surprising first paper</title>
    <updated>2025-10-08T09:50:58+00:00</updated>
    <author>
      <name>/u/csrl_</name>
      <uri>https://old.reddit.com/user/csrl_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o164ye/meta_superintelligences_surprising_first_paper/"&gt; &lt;img alt="Meta Superintelligence’s surprising first paper" src="https://external-preview.redd.it/JCvftI08SHl-gSbgzIC77Ii1UGjMaLNJCQ7drY7JAIc.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b567d36377fcc16b8bef230712988e58520db703" title="Meta Superintelligence’s surprising first paper" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;TL;DR&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;MSI’s first paper, REFRAG, is about a new way to do RAG.&lt;/li&gt; &lt;li&gt;This slightly modified LLM converts most retrieved document chunks into compact, LLM-aligned &lt;strong&gt;chunk embeddings&lt;/strong&gt; that the LLM can consume directly.&lt;/li&gt; &lt;li&gt;A lightweight &lt;strong&gt;policy&lt;/strong&gt; (trained with RL) decides which chunk embeddings should be &lt;em&gt;expanded&lt;/em&gt; back into full tokens under a budget; the LLM runs normally on this mixed input.&lt;/li&gt; &lt;li&gt;The net effect is far less KV cache and attention cost, much faster first-byte latency and higher throughput, while preserving perplexity and task accuracy in benchmarks.&lt;/li&gt; &lt;li&gt;This is not a new architecture for reasoning — it’s an efficiency and systems innovation that unlocks RAG at near-real-time costs. That’s why it’s strategically important and commercially attractive.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/csrl_"&gt; /u/csrl_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://paddedinputs.substack.com/p/meta-superintelligences-surprising"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o164ye/meta_superintelligences_surprising_first_paper/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o164ye/meta_superintelligences_surprising_first_paper/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T09:50:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvryo4</id>
    <title>AMA Announcement: Prime Intellect — The Open‑Source Distributed Training Lab (Thu, Oct 2 • 10 AM – 1 PM PDT)</title>
    <updated>2025-10-02T02:31:01+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"&gt; &lt;img alt="AMA Announcement: Prime Intellect — The Open‑Source Distributed Training Lab (Thu, Oct 2 • 10 AM – 1 PM PDT)" src="https://preview.redd.it/222kj50x0msf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c1c747c42690f74b8d08690dcdbb1ef23aece13b" title="AMA Announcement: Prime Intellect — The Open‑Source Distributed Training Lab (Thu, Oct 2 • 10 AM – 1 PM PDT)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/222kj50x0msf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T02:31:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwaoyd</id>
    <title>AMA with Prime Intellect — Ask Us Anything!</title>
    <updated>2025-10-02T17:47:44+00:00</updated>
    <author>
      <name>/u/kindacognizant</name>
      <uri>https://old.reddit.com/user/kindacognizant</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;AMA with Prime Intellect — Ask Us Anything!&lt;/h1&gt; &lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;! We’re excited for this AMA, thank you for having us.&lt;/p&gt; &lt;p&gt;I’m Kalomaze (&lt;a href="/u/kindacognizant"&gt;u/kindacognizant&lt;/a&gt;), a researcher at Prime Intellect, the lab behind:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Distributed training &lt;a href="https://www.primeintellect.ai/#research"&gt;efforts&lt;/a&gt; including INTELLECT-1 + INTELLECT-2&lt;/li&gt; &lt;li&gt;Open-source RL efforts including &lt;a href="https://github.com/PrimeIntellect-ai/verifiers"&gt;verifiers&lt;/a&gt;, &lt;a href="https://github.com/PrimeIntellect-ai/prime-rl"&gt;prime-rl&lt;/a&gt;, and the &lt;a href="https://app.primeintellect.ai/dashboard/environments"&gt;Environments Hub&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Our other participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Sami Jaghouar, &lt;a href="/u/samsja19"&gt;u/samsja19&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Will Brown, &lt;a href="/u/willccbb"&gt;u/willccbb&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Jack Min Ong, &lt;a href="/u/Cinamic"&gt;u/Cinamic&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Mika Senghaas, &lt;a href="/u/mikasenghaas"&gt;u/mikasenghaas&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 11:00 AM – 2:00 PM PST, with the Prime Intellect team continuing to follow up on questions over the next 48 hours.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kindacognizant"&gt; /u/kindacognizant &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwaoyd/ama_with_prime_intellect_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwaoyd/ama_with_prime_intellect_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwaoyd/ama_with_prime_intellect_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T17:47:44+00:00</published>
  </entry>
</feed>
