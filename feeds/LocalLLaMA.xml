<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-14T10:25:39+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qc7fhd</id>
    <title>An.. MCP‚Ä¶ Commercial?</title>
    <updated>2026-01-13T23:44:11+00:00</updated>
    <author>
      <name>/u/slurmernetes</name>
      <uri>https://old.reddit.com/user/slurmernetes</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qc7fhd/an_mcp_commercial/"&gt; &lt;img alt="An.. MCP‚Ä¶ Commercial?" src="https://external-preview.redd.it/yvrNL5rngrIPhevm7WpSgLwWFhFIkhcOCQOBtQnzqwU.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6d3a62b45be110d871c87d998a77bc959474cb52" title="An.. MCP‚Ä¶ Commercial?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm still not sure if this is real or ai generated but first comment says it ‚Äúunhinged‚Äù. Is this really an MCP commercial? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/slurmernetes"&gt; /u/slurmernetes &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/Nejecji5XNQ"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qc7fhd/an_mcp_commercial/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qc7fhd/an_mcp_commercial/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-13T23:44:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1qbqmon</id>
    <title>SPARKLE Announces Intel Arc Pro B60 24GB Graphics Card Series Launch on January 12, 2026 for USD $799 MSRP</title>
    <updated>2026-01-13T12:58:16+00:00</updated>
    <author>
      <name>/u/reps_up</name>
      <uri>https://old.reddit.com/user/reps_up</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/reps_up"&gt; /u/reps_up &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.sparkle.com.tw/en/sparkle-news/view/93E0b95ea8A0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbqmon/sparkle_announces_intel_arc_pro_b60_24gb_graphics/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qbqmon/sparkle_announces_intel_arc_pro_b60_24gb_graphics/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-13T12:58:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1qc485u</id>
    <title>Building a game where you talk to NPCs using Llama 3.1-8B-q4, optimized for 6GB VRAM</title>
    <updated>2026-01-13T21:38:36+00:00</updated>
    <author>
      <name>/u/bayhan2000</name>
      <uri>https://old.reddit.com/user/bayhan2000</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qc485u/building_a_game_where_you_talk_to_npcs_using/"&gt; &lt;img alt="Building a game where you talk to NPCs using Llama 3.1-8B-q4, optimized for 6GB VRAM" src="https://external-preview.redd.it/MjFraTE5dGdxNmRnMeC5Rm5xTPWPllbMPfh3CJD-5GSC-t7xIsMmhXMeJtjN.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=afad020551a48a46b2bf20cc6b182f727b7105ae" title="Building a game where you talk to NPCs using Llama 3.1-8B-q4, optimized for 6GB VRAM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been working on an investigative indie game. The core mechanic isn't a dialogue tree. It‚Äôs a direct interface with local LLMs. My goal was to make a polished, atmospheric experience that runs entirely offline on mid-range consumer hardware.&lt;/p&gt; &lt;p&gt;The game runs a local &lt;strong&gt;Llama-3.1-8B (Q4_K_M)&lt;/strong&gt; instance. I am using tauri and llama-server with vulkan support. The UI is a custom WebGL-driven &amp;quot;OS&amp;quot; that simulates a retro-future terminal.&lt;/p&gt; &lt;p&gt;Targeting &lt;strong&gt;6GB VRAM&lt;/strong&gt; was the biggest challenge. I had to keep low context window like 2048-4096 the LLM‚Äôs KV cache.&lt;/p&gt; &lt;p&gt;In this clip, I‚Äôm testing a bribery scenario. NPC tries to bribe me with bribe action, basically function calling at the end of the prompt.&lt;/p&gt; &lt;p&gt;I have tested with RTX2060 and 4070Ti Super and it both works realtime.&lt;/p&gt; &lt;p&gt;I am planning to train a custom LoRA specifically for the game‚Äôs world and essentially eliminate any remaining hallucinations. It works surprisingly well right now, but a dedicated fine-tune will be the final step for total immersion.&lt;/p&gt; &lt;p&gt;I would like to hear your thoughts!!&lt;/p&gt; &lt;p&gt;Edit :&lt;br /&gt; I managed to get the VRAM usage down to ~5.3 GB for Llama 3.1 8B by sticking to a 4096 context window and enabling Flash Attention.&lt;/p&gt; &lt;p&gt;To handle that tight context limit, I‚Äôm using a vector DB and a RAG pipeline. It basically &amp;quot;swaps in&amp;quot; relevant lore and action tags on the fly so the AI stays smart without the prompt bloating.&lt;/p&gt; &lt;p&gt;Performance is surprisingly solid on mid-range gear:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;RTX 4070:&lt;/strong&gt; ~70 TPS&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RTX 2060 (6GB):&lt;/strong&gt; ~15-20 TPS&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I was actually skeptical about the 2060 since there‚Äôs only about 700MB of headroom left for the OS and other apps, but it hasn't been an issue at all. It runs super smooth.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bayhan2000"&gt; /u/bayhan2000 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/rrveazsgq6dg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qc485u/building_a_game_where_you_talk_to_npcs_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qc485u/building_a_game_where_you_talk_to_npcs_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-13T21:38:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1qcjjm7</id>
    <title>Speech to text via LLM</title>
    <updated>2026-01-14T10:06:58+00:00</updated>
    <author>
      <name>/u/Acrobatic_Cat_3448</name>
      <uri>https://old.reddit.com/user/Acrobatic_Cat_3448</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;Is there something more convenient already than the Whisper/SDK (&lt;a href="https://github.com/argmaxinc/WhisperKit"&gt;https://github.com/argmaxinc/WhisperKit&lt;/a&gt;)? This one works on iOS/macOS and other platforms, and it worked very well. It actually deploys a LLM on an iPhone. &lt;/p&gt; &lt;p&gt;I know that similar setups were already discussed here (&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1h2u9ed/introducing%5C_whisper%5C_cpp%5C_macos%5C_utils%5C_a%5C_terminal/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1h2u9ed/introducing\_whisper\_cpp\_macos\_utils\_a\_terminal/&lt;/a&gt;). &lt;/p&gt; &lt;p&gt;Looking at some projects like this one (&lt;a href="https://github.com/rishikanthc/Scriberr"&gt;https://github.com/rishikanthc/Scriberr&lt;/a&gt;) it looks like setting it up is still quite complex?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Acrobatic_Cat_3448"&gt; /u/Acrobatic_Cat_3448 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcjjm7/speech_to_text_via_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcjjm7/speech_to_text_via_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qcjjm7/speech_to_text_via_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-14T10:06:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1qcjllm</id>
    <title>Local VLMs struggling with OCR accuracy in NLP pipelines</title>
    <updated>2026-01-14T10:10:30+00:00</updated>
    <author>
      <name>/u/aidenclarke_12</name>
      <uri>https://old.reddit.com/user/aidenclarke_12</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Trying to use local VLMs like Llama-4 scout of qwen3-VL-30B OCR on scanned docs to feed into NLP for entity extraction/summarization but hitting constan accuracy walls. Model hallucinates on blurry text images mangles handwritten notes and totally botches complex layouts like tables or multi columns, ends up garbling the NLP input and throwing off downstream analysis&lt;/p&gt; &lt;p&gt;From digging around, common issues people run into: hallucinations on low-res/noisy scans (esp with ML-based OCR), bias towads clean printed text over handwriting, vulnerability to blur/high frequency noise, lack of contextual understanding, like just spits out without smeantics and high compute needs making local runs sluggish without beefy hardware. Dataset biases in training make it worse for edge cases too&lt;/p&gt; &lt;p&gt;Anyone dealt with this?? Tweaks like better pre processing or sharpeting images or maybe specific quants that help?? or is traditional OCR still the move for reliability before VLM reasoning&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aidenclarke_12"&gt; /u/aidenclarke_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcjllm/local_vlms_struggling_with_ocr_accuracy_in_nlp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcjllm/local_vlms_struggling_with_ocr_accuracy_in_nlp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qcjllm/local_vlms_struggling_with_ocr_accuracy_in_nlp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-14T10:10:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1qc14cz</id>
    <title>Best local model / agent for coding, replacing Claude Code</title>
    <updated>2026-01-13T19:42:50+00:00</updated>
    <author>
      <name>/u/joyfulsparrow</name>
      <uri>https://old.reddit.com/user/joyfulsparrow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I usually use Claude Code (Pro) for coding (Xcode / Swift etc). Are there any decent local agents / models which could be a replacement for it? I don't expect it to match the intelligence of Claude Code, but I quite like the terminal-based experience, and wonder if there's a system which nearly matches it. Just for when I've used up 100% of Claude plan.&lt;/p&gt; &lt;p&gt;Computer specs: MacBook Pro, M3 Pro chip, 36 GB RAM.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/joyfulsparrow"&gt; /u/joyfulsparrow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qc14cz/best_local_model_agent_for_coding_replacing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qc14cz/best_local_model_agent_for_coding_replacing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qc14cz/best_local_model_agent_for_coding_replacing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-13T19:42:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1qc81si</id>
    <title>Built an 8√ó RTX 3090 monster‚Ä¶ considering nuking it for 2√ó Pro 6000 Max-Q</title>
    <updated>2026-01-14T00:09:47+00:00</updated>
    <author>
      <name>/u/BeeNo7094</name>
      <uri>https://old.reddit.com/user/BeeNo7094</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been running an 8√ó RTX 3090 box on an EPYC 7003 with an ASUS ROMED8-2T and 512 GB DDR4-3200.&lt;/p&gt; &lt;p&gt;The setup is not pretty. Lots of PCIe risers, I didn‚Äôt know about MCIO 8 months ago. The board has 7√ó x16 Gen4 slots, so for the 8th GPU I‚Äôm using an x8/x8 bifurcator plus a daisy-chained riser: motherboard to riser to bifurcator to GPU 1 on the bifurcator and GPU 2 on another riser. This is purely because of physical space and riser length limits.&lt;/p&gt; &lt;p&gt;As expected, things are weird. One GPU runs at x8, the other at x4, likely the daisy-chained riser but I haven‚Äôt had time to deep-debug. Another GPU shows up as x8 even when it shouldn‚Äôt, either a jumper I‚Äôm missing or a 3090 with a mining or modded vBIOS. Stability only became acceptable after forcing all PCIe slots to Gen3 Although I still see one of the x8 GPUs &amp;quot;faiiling off the PCI bus&amp;quot; (shows up as NA on nvtop) and leads me to reboot the server(10minutes to vllm readiness).&lt;/p&gt; &lt;p&gt;Because of this Frankenstein setup, I‚Äôm considering replacing the whole thing with 2√ó RTX Pro 6000 Max-Q, basically trading 8 riser-mounted 3090s for a clean dual-GPU build. This would triple the cost of the system. My 3090s were about $600 each, while the Max-Qs are quoted at about $8,300 each.&lt;/p&gt; &lt;p&gt;Putting elegance and some hit-or-miss stability gains aside, is there any real performance upside here?&lt;/p&gt; &lt;p&gt;Quick power-efficiency napkin math says it would take about 7.1 years of nonstop usage to break even compared to the 8√ó3090 setup. I could switch from AWQ to NVFP4 quantization. How much performance should I realistically expect for AI coding agents like Claude Code and OpenCode?&lt;/p&gt; &lt;p&gt;Would prefill latency improve in a meaningful way?&lt;/p&gt; &lt;p&gt;VRAM would be roughly the same today, with room to add 2 more GPUs later without risers and potentially double max VRAM. But is this even a good platform for FP8 coding models like MiniMax 2.1 or GLM 4.7?&lt;/p&gt; &lt;p&gt;Am I missing any real advantages here, or is this mostly an expensive way to clean up a messy but functional setup?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BeeNo7094"&gt; /u/BeeNo7094 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qc81si/built_an_8_rtx_3090_monster_considering_nuking_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qc81si/built_an_8_rtx_3090_monster_considering_nuking_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qc81si/built_an_8_rtx_3090_monster_considering_nuking_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-14T00:09:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1qcbq2n</id>
    <title>GLM-Image just dropped ‚Äî an open multimodal model from Zai Org (language + vision).</title>
    <updated>2026-01-14T02:51:55+00:00</updated>
    <author>
      <name>/u/InternationalToe2678</name>
      <uri>https://old.reddit.com/user/InternationalToe2678</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Zai Org released GLM-Image, extending the GLM family with native image understanding and cross-modal reasoning. It‚Äôs not just captioning ‚Äî the model is built to reason over visual inputs and text together.&lt;/p&gt; &lt;p&gt;Why it‚Äôs interesting:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;‚Ä¢ Unified vision + language model ‚Ä¢ Designed for VQA, image understanding, and multimodal reasoning ‚Ä¢ Fully open on Hugging Face (weights available) ‚Ä¢ Fits into the growing ecosystem of open multimodal GLM models &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Feels like another signal that open multimodal models are maturing fast ‚Äî not just matching basic vision tasks, but moving toward real reasoning over images.&lt;/p&gt; &lt;p&gt;Curious how this compares in practice vs Qwen-VL, InternVL, or LLaVA variants, especially on reasoning-heavy prompts.&lt;/p&gt; &lt;p&gt;Model page: &lt;a href="https://huggingface.co/zai-org/GLM-Image"&gt;https://huggingface.co/zai-org/GLM-Image&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InternationalToe2678"&gt; /u/InternationalToe2678 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcbq2n/glmimage_just_dropped_an_open_multimodal_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcbq2n/glmimage_just_dropped_an_open_multimodal_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qcbq2n/glmimage_just_dropped_an_open_multimodal_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-14T02:51:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1qcid3l</id>
    <title>Pocket TTS: a 100M-parameter text-to-speech</title>
    <updated>2026-01-14T08:51:14+00:00</updated>
    <author>
      <name>/u/paf1138</name>
      <uri>https://old.reddit.com/user/paf1138</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcid3l/pocket_tts_a_100mparameter_texttospeech/"&gt; &lt;img alt="Pocket TTS: a 100M-parameter text-to-speech" src="https://external-preview.redd.it/-wU8cKM1ybBFD4hDGC_AsWfo00bhoyCdexKDfL5kTEQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3304eb1bb7dc20cd79911028958854a3039569d9" title="Pocket TTS: a 100M-parameter text-to-speech" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paf1138"&gt; /u/paf1138 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/kyutai/pocket-tts"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcid3l/pocket_tts_a_100mparameter_texttospeech/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qcid3l/pocket_tts_a_100mparameter_texttospeech/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-14T08:51:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1qcfxk0</id>
    <title>"Computer Use" agents are smart, but they don't know your computer. (So I built a tool to show them)</title>
    <updated>2026-01-14T06:23:19+00:00</updated>
    <author>
      <name>/u/slow-fast-person</name>
      <uri>https://old.reddit.com/user/slow-fast-person</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been testing Computer Use models for local automation, and I keep hitting the same wall: &lt;strong&gt;Context Blindness.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The models are smart, but they don't know my specific environment. They try to solve problems the &amp;quot;generic&amp;quot; way, which usually breaks things.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2 real examples where my agent failed:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;The Terminal Trap:&lt;/strong&gt; I asked it to &amp;quot;start the server.&amp;quot; It opened the default Terminal and failed because it didn't know to run &lt;code&gt;source&lt;/code&gt; .&lt;code&gt;venv/bin/activate&lt;/code&gt; first. &lt;ul&gt; &lt;li&gt;&lt;em&gt;The scary part:&lt;/em&gt; It then started trying to &lt;code&gt;pip install&lt;/code&gt; packages globally to &amp;quot;fix&amp;quot; it.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The &amp;quot;Wrong App&amp;quot; Loop:&lt;/strong&gt; &amp;quot;Message the group on WhatsApp.&amp;quot; It launched the native desktop app (which I never use and isn't logged in). It got stuck on a QR code. &lt;ul&gt; &lt;li&gt;&lt;em&gt;Reality:&lt;/em&gt; I use WhatsApp Web in a pinned tab because it's always ready.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;The Solution: Record, Don't Prompt.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I built &lt;strong&gt;AI Mime&lt;/strong&gt; to fix this. Instead of prompting and hoping, I &lt;strong&gt;record&lt;/strong&gt; the workflow once.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I show it &lt;em&gt;exactly&lt;/em&gt; how to activate the .venv.&lt;/li&gt; &lt;li&gt;I show it &lt;em&gt;exactly&lt;/em&gt; how to use whatsapp on the browser&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The agent captures this &amp;quot;happy path&amp;quot; and replays it, handling dynamic data without getting &amp;quot;creative&amp;quot; with my system configuration.&lt;/p&gt; &lt;p&gt;repo**:** &lt;a href="https://github.com/prakhar1114/ai_mime"&gt;https://github.com/prakhar1114/ai_mime&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Is this &amp;quot;Context Blindness&amp;quot; stopping anyone else from using these agents for real work?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/slow-fast-person"&gt; /u/slow-fast-person &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcfxk0/computer_use_agents_are_smart_but_they_dont_know/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcfxk0/computer_use_agents_are_smart_but_they_dont_know/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qcfxk0/computer_use_agents_are_smart_but_they_dont_know/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-14T06:23:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1qchn6x</id>
    <title>Intel Arc Pro B60? (In Quad... 6x... 8x configuration)</title>
    <updated>2026-01-14T08:04:52+00:00</updated>
    <author>
      <name>/u/Infinite100p</name>
      <uri>https://old.reddit.com/user/Infinite100p</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone tried running multiples of Intel Arc Pro B60 with 24GB VRAM with larger models like MiniMax, maybe quants of GLM?&lt;/p&gt; &lt;p&gt;Would it be a good budget choice at ~$650 per GPU given that 3090 stock is very thin now and they go for much more with no warranty and most of the lifespan gone?&lt;/p&gt; &lt;p&gt;It's hard to find eBay listings below $800 for 3090, and that will get you a (severely?) used GPU with no warranty.&lt;/p&gt; &lt;p&gt;I only found &lt;a href="https://www.storagereview.com/review/intel-arc-pro-b60-battlematrix-preview-192gb-of-vram-for-on-premise-ai"&gt;these benchmarks&lt;/a&gt; for a multi-B60 setup, but the numbers seem off, and &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1pd3mdw/comment/ns37lg3/"&gt;this discussion here blames the author&lt;/a&gt; aka the tests were probably not properly set up.&lt;/p&gt; &lt;p&gt;Would love to check in if anyone has &lt;strong&gt;new&lt;/strong&gt; data points/experience to report?&lt;/p&gt; &lt;p&gt;They've been unobtanium for months, and I am seeing some stock now.&lt;br /&gt; I am considering a 6x B60 set up.&lt;br /&gt; Would love your thoughts.&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;p&gt;&lt;strong&gt;UPD:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Also, B60 has SR-IOV, so (in theory) you can share it between different VMs painlessly.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Infinite100p"&gt; /u/Infinite100p &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qchn6x/intel_arc_pro_b60_in_quad_6x_8x_configuration/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qchn6x/intel_arc_pro_b60_in_quad_6x_8x_configuration/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qchn6x/intel_arc_pro_b60_in_quad_6x_8x_configuration/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-14T08:04:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1qcfto0</id>
    <title>Noob question: imatrix, yes or not?</title>
    <updated>2026-01-14T06:17:00+00:00</updated>
    <author>
      <name>/u/TheGlobinKing</name>
      <uri>https://old.reddit.com/user/TheGlobinKing</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Does it make sense to use imatrix for specialized models (i.e. RP, coding, medical models) or would regular/static ggufs be a better choice for these?&lt;/p&gt; &lt;p&gt;In the past I've been told imatrix (including unsloth?) affected things like thinking, so I was wondering if it may actually hurt specialized models.&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;p&gt;EDIT: To clarify, I know imatrix is better in general. What I'm asking is, if imatrix datasets are generic, the quantization process might actually be overfitting the model on that specific dataset, not sure if that may affect how a medical or coding model works.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheGlobinKing"&gt; /u/TheGlobinKing &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcfto0/noob_question_imatrix_yes_or_not/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcfto0/noob_question_imatrix_yes_or_not/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qcfto0/noob_question_imatrix_yes_or_not/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-14T06:17:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1qbz7h6</id>
    <title>Owners, not renters: Mozilla's open source AI strategy</title>
    <updated>2026-01-13T18:34:22+00:00</updated>
    <author>
      <name>/u/NelsonMinar</name>
      <uri>https://old.reddit.com/user/NelsonMinar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbz7h6/owners_not_renters_mozillas_open_source_ai/"&gt; &lt;img alt="Owners, not renters: Mozilla's open source AI strategy" src="https://external-preview.redd.it/eBhDV53Bx2pdf58HHsmIDWpPzti_SmsXMDBh7hzdnLA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=272d58abf490660743722caac57b754d523d0b9f" title="Owners, not renters: Mozilla's open source AI strategy" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NelsonMinar"&gt; /u/NelsonMinar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://blog.mozilla.org/en/mozilla/mozilla-open-source-ai-strategy/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbz7h6/owners_not_renters_mozillas_open_source_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qbz7h6/owners_not_renters_mozillas_open_source_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-13T18:34:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1qcd9m1</id>
    <title>Shadows-Gemma-3-1B: cold start reasoning from topk20 logprob distillation</title>
    <updated>2026-01-14T04:04:00+00:00</updated>
    <author>
      <name>/u/Echo9Zulu-</name>
      <uri>https://old.reddit.com/user/Echo9Zulu-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/Echo9Zulu/Shadows-Gemma-3-1B"&gt;Shadows-Gemma-1B&lt;/a&gt; was trained for the google tunix hackathon and is my first finetuning project. Trained on 1569 samples in ~10 minutes on TPUv5-8e, and around 20min on A40, Shadows-Gemma is a general reasoning model trained without RL, code or math data distilled from non reasoning teacher gemma-3-4b-it.&lt;/p&gt; &lt;p&gt;When looking at topk20 logprob data, I noticed that some tokens appear early in the low ranks, and sort of float around until eventually being selected much later. It turns out, when the average distance between first appearance and selection is greater, the features we know from reasoning traces- backtracking, solution exploration, drafting, rewriting, were more prominent in the training data when &amp;quot;persistence&amp;quot; was higher. I'm calling these shadow tokens, and they may indicate reasoning behavior in the output distribution and surface text. &lt;/p&gt; &lt;p&gt;Shadows-Gemma-1B was trained using logprob distillation from teacher gemma-3-4b-it, which I rejection sampled to meet the following system prompt, which encourages interleaved reasoning;&lt;/p&gt; &lt;p&gt;&lt;code&gt; You are Gemma, a thinking model who reasons through problems step by step before providing an answer. Conduct your reasoning within a &amp;lt;reasoning&amp;gt;&amp;lt;/reasoning&amp;gt; block, with intermediate steps using &amp;lt;processing&amp;gt;&amp;lt;/processing&amp;gt; tags, with the intermediate step inside. Continue like this until closing the &amp;lt;/reasoning&amp;gt; block and providing your answer within &amp;lt;answer&amp;gt;&amp;lt;/answer&amp;gt;. &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Once I started modeling token trajectories forward towards the end of a completion, I kept seeing the pattern &lt;em&gt;everywhere&lt;/em&gt;, in other language models as well. Knowing more research, evaluation and compute would be required to study shadow tokens, I set myself on empirically demonstrating that shadow tokens are a trainable signal, which is about all I can say for sure at this time. Regardless, Shadow-Gemma-1B gives better answers on most questions I have tried and has become a generally capable reasoning model, thinking more on harder questions. To be clear, I'm not saying Shadows-Gemma beats any other model, even the base model, at a given task.&lt;/p&gt; &lt;p&gt;I am working on a post mortem with more details about the adventure, loss functions, code optimizations, interpretability data analysis tools, war stories from a one week port of pytorch --&amp;gt; JAX framework, discuss how SOTA LLMs were not always useful etc. Other datasets I made for this project will also be published soon:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;~4800 Reasoning traces from DeepCogito-v2.1&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Full solutions for GSM8K by DeepSeekProverv2&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Echo9Zulu/Shadows-Gemma-3-4B"&gt;Shadows-Gemma-3-4B&lt;/a&gt; was a last minute full send using some runpod credits I had leftover just to see if it would work. Well, it did! I barely tested this one so ymmv. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Echo9Zulu-"&gt; /u/Echo9Zulu- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcd9m1/shadowsgemma31b_cold_start_reasoning_from_topk20/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcd9m1/shadowsgemma31b_cold_start_reasoning_from_topk20/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qcd9m1/shadowsgemma31b_cold_start_reasoning_from_topk20/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-14T04:04:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1qc76dc</id>
    <title>NovaSR: A tiny 52kb audio upsampler that runs 3600x realtime.</title>
    <updated>2026-01-13T23:33:38+00:00</updated>
    <author>
      <name>/u/SplitNice1982</name>
      <uri>https://old.reddit.com/user/SplitNice1982</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I released NovaSR which is a very tiny 52kb audio upsampler that enhances muffled 16khz audio to produce clearer 48khz audio. It's incredibly small and really fast(can process 100 to 3600 seconds of audio in just 1 second on a single gpu).&lt;/p&gt; &lt;p&gt;Why is it useful?&lt;br /&gt; 1. It can enhance any TTS models quality. Most generate at 16khz or 24khz and NovaSR can enhance them with nearly 0 computation cost.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;It can restore low quality audio datasets really quickly.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;It can fit basically on any device. It's just 52kb which basically means its smaller then a 3 second audio file itself.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Right now, it was only trained on just 100 hours of data so it has room for improvement, but it still produces good quality audio at such a tiny size.&lt;/p&gt; &lt;p&gt;Github repo: &lt;a href="https://github.com/ysharma3501/NovaSR"&gt;https://github.com/ysharma3501/NovaSR&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Model with some examples: &lt;a href="https://huggingface.co/YatharthS/NovaSR"&gt;https://huggingface.co/YatharthS/NovaSR&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Space to try it(It's running on a weak 2 core cpu machine so won't be 3600x realtime but still around 10x realtime): &lt;a href="https://huggingface.co/spaces/YatharthS/NovaSR"&gt;https://huggingface.co/spaces/YatharthS/NovaSR&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Stars or Likes would be appreciated if found helpful. Thank you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SplitNice1982"&gt; /u/SplitNice1982 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qc76dc/novasr_a_tiny_52kb_audio_upsampler_that_runs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qc76dc/novasr_a_tiny_52kb_audio_upsampler_that_runs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qc76dc/novasr_a_tiny_52kb_audio_upsampler_that_runs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-13T23:33:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1qcj1lr</id>
    <title>What happened to 1.58bit LLMs?</title>
    <updated>2026-01-14T09:34:54+00:00</updated>
    <author>
      <name>/u/Sloppyjoeman</name>
      <uri>https://old.reddit.com/user/Sloppyjoeman</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Last year I remember them being super hyped and largely theoretical. Since then, I understand there‚Äôs a growing body of evidence that larger sparse models outperform smaller denser models, which 1.58bit quantisation seems poised to drastically improve&lt;/p&gt; &lt;p&gt;I haven‚Äôt seen people going ‚Äúoh, the 1.58bit quantisation was overhyped‚Äù - did I just miss it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sloppyjoeman"&gt; /u/Sloppyjoeman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcj1lr/what_happened_to_158bit_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcj1lr/what_happened_to_158bit_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qcj1lr/what_happened_to_158bit_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-14T09:34:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1qc53rf</id>
    <title>MedGemma 1.5: Next generation medical image interpretation with medical speech to text with MedASR</title>
    <updated>2026-01-13T22:11:12+00:00</updated>
    <author>
      <name>/u/CheekyBastard55</name>
      <uri>https://old.reddit.com/user/CheekyBastard55</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qc53rf/medgemma_15_next_generation_medical_image/"&gt; &lt;img alt="MedGemma 1.5: Next generation medical image interpretation with medical speech to text with MedASR" src="https://external-preview.redd.it/Xfy8b5oz8xAgNpbj0L9Mmjzxactj5HdaKRFOmBPu0YE.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c9dad5b13e20f57d64f5fc0bbc7415c9f4186b1d" title="MedGemma 1.5: Next generation medical image interpretation with medical speech to text with MedASR" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CheekyBastard55"&gt; /u/CheekyBastard55 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://research.google/blog/next-generation-medical-image-interpretation-with-medgemma-15-and-medical-speech-to-text-with-medasr/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qc53rf/medgemma_15_next_generation_medical_image/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qc53rf/medgemma_15_next_generation_medical_image/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-13T22:11:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1qbpz5l</id>
    <title>kyutai just introduced Pocket TTS: a 100M-parameter text-to-speech model with high-quality voice cloning that runs on your laptop‚Äîno GPU required</title>
    <updated>2026-01-13T12:25:26+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbpz5l/kyutai_just_introduced_pocket_tts_a_100mparameter/"&gt; &lt;img alt="kyutai just introduced Pocket TTS: a 100M-parameter text-to-speech model with high-quality voice cloning that runs on your laptop‚Äîno GPU required" src="https://b.thumbs.redditmedia.com/1twWaeVXhu1muEmRUClJoZ9yZJXDVOmoBhoTPlp5ntc.jpg" title="kyutai just introduced Pocket TTS: a 100M-parameter text-to-speech model with high-quality voice cloning that runs on your laptop‚Äîno GPU required" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Blog post with demo: Pocket TTS: A high quality TTS that gives your CPU a voice: &lt;a href="https://kyutai.org/blog/2026-01-13-pocket-tts"&gt;https://kyutai.org/blog/2026-01-13-pocket-tts&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/kyutai-labs/pocket-tts"&gt;https://github.com/kyutai-labs/pocket-tts&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hugging Face Model Card: &lt;a href="https://huggingface.co/kyutai/pocket-tts"&gt;https://huggingface.co/kyutai/pocket-tts&lt;/a&gt;&lt;/p&gt; &lt;p&gt;arXiv:2509.06926 [cs.SD]: Continuous Audio Language Models&lt;br /&gt; Simon Rouard, Manu Orsini, Axel Roebel, Neil Zeghidour, Alexandre D√©fossez&lt;br /&gt; &lt;a href="https://arxiv.org/abs/2509.06926"&gt;https://arxiv.org/abs/2509.06926&lt;/a&gt;&lt;/p&gt; &lt;p&gt;From kyutai on ùïè: &lt;a href="https://x.com/kyutai_labs/status/2011047335892303875"&gt;https://x.com/kyutai_labs/status/2011047335892303875&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qbpz5l"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbpz5l/kyutai_just_introduced_pocket_tts_a_100mparameter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qbpz5l/kyutai_just_introduced_pocket_tts_a_100mparameter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-13T12:25:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1qcgviy</id>
    <title>Unique 3.2M-word bilingual (DE-EN) literary erotica corpus available for AI training‚Äîteasers on Hugging Face</title>
    <updated>2026-01-14T07:17:55+00:00</updated>
    <author>
      <name>/u/kardinalzahl</name>
      <uri>https://old.reddit.com/user/kardinalzahl</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;As an independent author, I've created a large original bilingual erotic fiction corpus (German originals + expanded English adaptations) that's well-suited for training or fine-tuning creative/uncensored models. Highlights:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;~3.2 million words across 500+ chapters&lt;/li&gt; &lt;li&gt;Long-form, character-driven narrative with progressive consensual kink (e.g., urophilia, period sex), rural/urban Vietnam settings&lt;/li&gt; &lt;li&gt;Sophisticated prose with philosophical references (Kant, Hegel, existential themes)&lt;/li&gt; &lt;li&gt;Bilingual parallel structure (German first, English creatively reworked‚Äîsometimes longer, sometimes shorter)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Three gated teaser datasets (~475k bilingual words total) are now live on Hugging Face:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Profile with all three: &lt;a href="https://huggingface.co/douglasvanwyck"&gt;https://huggingface.co/douglasvanwyck&lt;/a&gt; &lt;ul&gt; &lt;li&gt;With Anna in Saigon (complete mini-series, ~87k words)&lt;/li&gt; &lt;li&gt;&amp;quot;Phung's Quest&amp;quot; (ongoing series, 7 chapters, ~87k words)&lt;/li&gt; &lt;li&gt;&amp;quot;Center of the Universe&amp;quot;‚ÄîFirst 35 chapters (main saga teaser, ~301k words)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kardinalzahl"&gt; /u/kardinalzahl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcgviy/unique_32mword_bilingual_deen_literary_erotica/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcgviy/unique_32mword_bilingual_deen_literary_erotica/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qcgviy/unique_32mword_bilingual_deen_literary_erotica/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-14T07:17:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1qcff41</id>
    <title>EXAONE MoE support has been merged into llama.cpp</title>
    <updated>2026-01-14T05:55:19+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcff41/exaone_moe_support_has_been_merged_into_llamacpp/"&gt; &lt;img alt="EXAONE MoE support has been merged into llama.cpp" src="https://external-preview.redd.it/zj2pPBSKKE7hlpLBhVdaJfKDygb15HG1H-ApMccLwl8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=02a15a93673baf3e7e305c8147197d65844556ed" title="EXAONE MoE support has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;K-EXAONE-236B-A23B&lt;/h1&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B#introduction"&gt;&lt;/a&gt;&lt;/h1&gt; &lt;h1&gt;Introduction&lt;/h1&gt; &lt;p&gt;We introduce &lt;strong&gt;K-EXAONE&lt;/strong&gt;, a large-scale multilingual language model developed by LG AI Research. Built using a Mixture-of-Experts architecture, K-EXAONE features &lt;strong&gt;236 billion total&lt;/strong&gt; parameters, with &lt;strong&gt;23 billion active&lt;/strong&gt; during inference. Performance evaluations across various benchmarks demonstrate that K-EXAONE excels in reasoning, agentic capabilities, general knowledge, multilingual understanding, and long-context processing.&lt;/p&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B#key-features"&gt;&lt;/a&gt;&lt;/h1&gt; &lt;h1&gt;Key Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Architecture &amp;amp; Efficiency:&lt;/strong&gt; Features a 236B fine-grained MoE design (23B active) optimized with &lt;strong&gt;Multi-Token Prediction (MTP)&lt;/strong&gt;, enabling self-speculative decoding that boosts inference throughput by approximately 1.5x.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Long-Context Capabilities:&lt;/strong&gt; Natively supports a &lt;strong&gt;256K context window&lt;/strong&gt;, utilizing a &lt;strong&gt;3:1 hybrid attention&lt;/strong&gt; scheme with a &lt;strong&gt;128-token sliding window&lt;/strong&gt; to significantly minimize memory usage during long-document processing.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multilingual Support:&lt;/strong&gt; Covers 6 languages: Korean, English, Spanish, German, Japanese, and Vietnamese. Features a redesigned &lt;strong&gt;150k vocabulary&lt;/strong&gt; with &lt;strong&gt;SuperBPE&lt;/strong&gt;, improving token efficiency by ~30%.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agentic Capabilities:&lt;/strong&gt; Demonstrates superior tool-use and search capabilities via &lt;strong&gt;multi-agent strategies.&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Safety &amp;amp; Ethics:&lt;/strong&gt; Aligned with &lt;strong&gt;universal human values&lt;/strong&gt;, the model uniquely incorporates &lt;strong&gt;Korean cultural and historical contexts&lt;/strong&gt; to address regional sensitivities often overlooked by other models. It demonstrates high reliability across diverse risk categories.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/18543"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcff41/exaone_moe_support_has_been_merged_into_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qcff41/exaone_moe_support_has_been_merged_into_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-14T05:55:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1qc9sw2</id>
    <title>Introducing GLM-Image</title>
    <updated>2026-01-14T01:25:35+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qc9sw2/introducing_glmimage/"&gt; &lt;img alt="Introducing GLM-Image" src="https://preview.redd.it/70ypvyc5w7dg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=df4d302e9bb74550a3c16fd5342ab649a5bc3a53" title="Introducing GLM-Image" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Introducing GLM-Image: A new milestone in open-source image generation.&lt;/p&gt; &lt;p&gt;GLM-Image uses a hybrid auto-regressive plus diffusion architecture, combining strong global semantic understanding with high fidelity visual detail. It matches mainstream diffusion models in overall quality while excelling at text rendering and knowledge intensive generation.&lt;/p&gt; &lt;p&gt;Tech Blog: &lt;a href="http://z.ai/blog/glm-image"&gt;http://z.ai/blog/glm-image&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Experience it right now: &lt;a href="http://huggingface.co/zai-org/GLM-Image"&gt;http://huggingface.co/zai-org/GLM-Image&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="http://github.com/zai-org/GLM-Image"&gt;http://github.com/zai-org/GLM-Image&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/70ypvyc5w7dg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qc9sw2/introducing_glmimage/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qc9sw2/introducing_glmimage/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-14T01:25:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1qbw325</id>
    <title>My wishes for 2026</title>
    <updated>2026-01-13T16:35:06+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbw325/my_wishes_for_2026/"&gt; &lt;img alt="My wishes for 2026" src="https://preview.redd.it/8knck5zv85dg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6a8be13989bebb31b688873f7197d169cb43651e" title="My wishes for 2026" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Which do you think will happen first? And which won‚Äôt happen in 2026?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8knck5zv85dg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbw325/my_wishes_for_2026/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qbw325/my_wishes_for_2026/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-13T16:35:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1qc5nml</id>
    <title>Soprano TTS training code released: Create your own 2000x realtime on-device text-to-speech model with Soprano-Factory!</title>
    <updated>2026-01-13T22:32:00+00:00</updated>
    <author>
      <name>/u/eugenekwek</name>
      <uri>https://old.reddit.com/user/eugenekwek</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qc5nml/soprano_tts_training_code_released_create_your/"&gt; &lt;img alt="Soprano TTS training code released: Create your own 2000x realtime on-device text-to-speech model with Soprano-Factory!" src="https://external-preview.redd.it/amZxajFtZXF6NmRnMQO5kEggYbW8-0IppaPjE5mW-pGiD_HSvWQwK_psM6yd.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5a2ad1d1aac49f10f9a525a08d7b23d8a37a99b3" title="Soprano TTS training code released: Create your own 2000x realtime on-device text-to-speech model with Soprano-Factory!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone!&lt;/p&gt; &lt;p&gt;I‚Äôve been listening to all your feedback on Soprano, and I‚Äôve been working nonstop over these past three weeks to incorporate everything, so I have a TON of updates for you all!&lt;/p&gt; &lt;p&gt;For those of you who haven‚Äôt heard of Soprano before, it is an on-device text-to-speech model I designed to have highly natural intonation and quality with a small model footprint. It can run up to &lt;strong&gt;20x realtime&lt;/strong&gt; on CPU, and up to &lt;strong&gt;2000x&lt;/strong&gt; on GPU. It also supports lossless streaming with &lt;strong&gt;15 ms latency&lt;/strong&gt;, an order of magnitude lower than any other TTS model. You can check out Soprano here:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Github:&lt;/strong&gt; &lt;a href="https://github.com/ekwek1/soprano"&gt;&lt;strong&gt;https://github.com/ekwek1/soprano&lt;/strong&gt;&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Demo:&lt;/strong&gt; &lt;a href="https://huggingface.co/spaces/ekwek/Soprano-TTS"&gt;&lt;strong&gt;https://huggingface.co/spaces/ekwek/Soprano-TTS&lt;/strong&gt;&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Model:&lt;/strong&gt; &lt;a href="https://huggingface.co/ekwek/Soprano-80M"&gt;&lt;strong&gt;https://huggingface.co/ekwek/Soprano-80M&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Today, I am releasing training code for you guys! This was by far the most requested feature to be added, and I am happy to announce that you can now train your own ultra-lightweight, ultra-realistic TTS models like the one in the video with your &lt;strong&gt;own data&lt;/strong&gt; on your &lt;strong&gt;own hardware&lt;/strong&gt; with &lt;strong&gt;Soprano-Factory&lt;/strong&gt;! Using Soprano-Factory, you can add new &lt;strong&gt;voices&lt;/strong&gt;, &lt;strong&gt;styles&lt;/strong&gt;, and &lt;strong&gt;languages&lt;/strong&gt; to Soprano. The entire repository is just 600 lines of code, making it easily customizable to suit your needs.&lt;/p&gt; &lt;p&gt;In addition to the training code, I am also releasing &lt;strong&gt;Soprano-Encoder&lt;/strong&gt;, which converts raw audio into audio tokens for training. You can find both here:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Soprano-Factory:&lt;/strong&gt; &lt;a href="https://github.com/ekwek1/soprano-factory"&gt;&lt;strong&gt;https://github.com/ekwek1/soprano-factory&lt;/strong&gt;&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Soprano-Encoder:&lt;/strong&gt; &lt;a href="https://huggingface.co/ekwek/Soprano-Encoder"&gt;&lt;strong&gt;https://huggingface.co/ekwek/Soprano-Encoder&lt;/strong&gt;&lt;/a&gt; &lt;/p&gt; &lt;p&gt;I hope you enjoy it! See you tomorrow,&lt;/p&gt; &lt;p&gt;- Eugene&lt;/p&gt; &lt;p&gt;Disclaimer: I did not originally design Soprano with finetuning in mind. As a result, I cannot guarantee that you will see good results after training. Personally, I have my doubts that an 80M-parameter model trained on just 1000 hours of data can generalize to OOD datasets, but I have seen bigger miracles on this sub happen, so knock yourself out :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eugenekwek"&gt; /u/eugenekwek &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/wnuwfpdqz6dg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qc5nml/soprano_tts_training_code_released_create_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qc5nml/soprano_tts_training_code_released_create_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-13T22:32:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1qc9m6x</id>
    <title>GLM-Image is released!</title>
    <updated>2026-01-14T01:17:16+00:00</updated>
    <author>
      <name>/u/foldl-li</name>
      <uri>https://old.reddit.com/user/foldl-li</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qc9m6x/glmimage_is_released/"&gt; &lt;img alt="GLM-Image is released!" src="https://external-preview.redd.it/Ei4JzvCHJGNODl-Xo97JEKHuZJZU81UlEy5iyXWioSw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=251fac1763ed77fdaf4e281f649fddd4555de498" title="GLM-Image is released!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GLM-Image is an image generation model adopts a hybrid autoregressive + diffusion decoder architecture. In general image generation quality, GLM‚ÄëImage aligns with mainstream latent diffusion approaches, but it shows significant advantages in text-rendering and knowledge‚Äëintensive generation scenarios. It performs especially well in tasks requiring precise semantic understanding and complex information expression, while maintaining strong capabilities in high‚Äëfidelity and fine‚Äëgrained detail generation. In addition to text‚Äëto‚Äëimage generation, GLM‚ÄëImage also supports a rich set of image‚Äëto‚Äëimage tasks including image editing, style transfer, identity‚Äëpreserving generation, and multi‚Äësubject consistency.&lt;/p&gt; &lt;p&gt;Model architecture: a hybrid autoregressive + diffusion decoder design.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/foldl-li"&gt; /u/foldl-li &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/zai-org/GLM-Image"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qc9m6x/glmimage_is_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qc9m6x/glmimage_is_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-14T01:17:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
