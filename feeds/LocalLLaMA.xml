<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-26T07:35:43+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1rf2b90</id>
    <title>Benchmarking qwen3.5:35b vs gpt-oss:20b for Agentic Workloads (Ollama, Apple Silicon)</title>
    <updated>2026-02-26T06:01:38+00:00</updated>
    <author>
      <name>/u/pwbdecker</name>
      <uri>https://old.reddit.com/user/pwbdecker</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rf2b90/benchmarking_qwen3535b_vs_gptoss20b_for_agentic/"&gt; &lt;img alt="Benchmarking qwen3.5:35b vs gpt-oss:20b for Agentic Workloads (Ollama, Apple Silicon)" src="https://external-preview.redd.it/-W3Te0N3NSI2YKnRqaiMQ7io69snlAPAbJcAbQmfjk4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=782d1f26e2d58963b787b26421e110b13d5dabbc" title="Benchmarking qwen3.5:35b vs gpt-oss:20b for Agentic Workloads (Ollama, Apple Silicon)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pwbdecker"&gt; /u/pwbdecker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/jaredlockhart/penny/blob/main/docs/benchmarking-qwen35-vs-gpt-oss.md"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rf2b90/benchmarking_qwen3535b_vs_gptoss20b_for_agentic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rf2b90/benchmarking_qwen3535b_vs_gptoss20b_for_agentic/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-26T06:01:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1relj66</id>
    <title>Qwen dropped Qwen3.5-FP8 versions on HF</title>
    <updated>2026-02-25T18:31:05+00:00</updated>
    <author>
      <name>/u/reto-wyss</name>
      <uri>https://old.reddit.com/user/reto-wyss</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Yay! I really wanted the 122b-a10b FP8 - excited to test it.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/Qwen/qwen35"&gt;https://huggingface.co/collections/Qwen/qwen35&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/reto-wyss"&gt; /u/reto-wyss &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1relj66/qwen_dropped_qwen35fp8_versions_on_hf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1relj66/qwen_dropped_qwen35fp8_versions_on_hf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1relj66/qwen_dropped_qwen35fp8_versions_on_hf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-25T18:31:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1reqhjo</id>
    <title>Cosmos-Reason2-2B on Jetson Orin Nano Super</title>
    <updated>2026-02-25T21:28:32+00:00</updated>
    <author>
      <name>/u/Course_Latter</name>
      <uri>https://old.reddit.com/user/Course_Latter</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1reqhjo/cosmosreason22b_on_jetson_orin_nano_super/"&gt; &lt;img alt="Cosmos-Reason2-2B on Jetson Orin Nano Super" src="https://external-preview.redd.it/dzR0aW93c3prcGxnMfvobv_qRzAlCd_RuCGnpQgXYH5aXJ-MyLN8L1R1g083.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b0fcefe3bd5dfee97c174514e73da342a31f42aa" title="Cosmos-Reason2-2B on Jetson Orin Nano Super" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! Today, me and my team is releasing a version of &lt;strong&gt;Cosmos-Reason2-2B&lt;/strong&gt; that is quantized so that it fits even on the NVIDIA Jetson Orin Nano Super. &lt;/p&gt; &lt;p&gt;We managed to find a mixed precision configuration such that it maintains virtually the same accuracy as the unquantized model while being able to run really efficiently on the Nano Super and other edge devices :)&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/embedl/Cosmos-Reason2-2B-W4A16-Edge2"&gt;https://huggingface.co/embedl/Cosmos-Reason2-2B-W4A16-Edge2&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Course_Latter"&gt; /u/Course_Latter &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/sg8ywmszkplg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1reqhjo/cosmosreason22b_on_jetson_orin_nano_super/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1reqhjo/cosmosreason22b_on_jetson_orin_nano_super/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-25T21:28:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1rehykx</id>
    <title>Qwen3.5 "Low Reasoning Effort" trick in llama-server</title>
    <updated>2026-02-25T16:28:28+00:00</updated>
    <author>
      <name>/u/coder543</name>
      <uri>https://old.reddit.com/user/coder543</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With a logit bias adjustment for the &lt;code&gt;&amp;lt;/think&amp;gt;&lt;/code&gt; token and a grammar to defend against the bias forcing additional &lt;code&gt;&amp;lt;/think&amp;gt;&lt;/code&gt; tokens into the response, you can effectively adjust the average length of reasoning.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;curl -sS http://127.0.0.1:8083/v1/chat/completions \ -H 'content-type: application/json' \ -d '{ &amp;quot;model&amp;quot;: &amp;quot;qwen3.5-35b-a3b&amp;quot;, &amp;quot;stream&amp;quot;: false, &amp;quot;logit_bias&amp;quot;: { &amp;quot;248069&amp;quot;: 11.8 }, &amp;quot;grammar&amp;quot;: &amp;quot;root ::= pre &amp;lt;[248069]&amp;gt; post\npre ::= !&amp;lt;[248069]&amp;gt;*\npost ::= !&amp;lt;[248069]&amp;gt;*&amp;quot;, &amp;quot;messages&amp;quot;: [ { &amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;hello world&amp;quot; } ] }' &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;A few logit biases to consider:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;code&gt;11.8&lt;/code&gt; is a nice balance that favors reasoning when it is helpful, while often skipping or short circuiting reasoning for easy prompts.&lt;/li&gt; &lt;li&gt;&lt;code&gt;12.5&lt;/code&gt; more strongly favors less reasoning.&lt;/li&gt; &lt;li&gt;&lt;code&gt;13.3&lt;/code&gt; essentially disables reasoning.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;You can try any value you want, of course.&lt;/p&gt; &lt;p&gt;Even 11.8 is obviously going to cause the model to be less intelligent, but probably still smarter than disabling thinking entirely.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/coder543"&gt; /u/coder543 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rehykx/qwen35_low_reasoning_effort_trick_in_llamaserver/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rehykx/qwen35_low_reasoning_effort_trick_in_llamaserver/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rehykx/qwen35_low_reasoning_effort_trick_in_llamaserver/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-25T16:28:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1reuc60</id>
    <title>Llama Server UI</title>
    <updated>2026-02-25T23:55:07+00:00</updated>
    <author>
      <name>/u/Additional-Action566</name>
      <uri>https://old.reddit.com/user/Additional-Action566</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1reuc60/llama_server_ui/"&gt; &lt;img alt="Llama Server UI" src="https://preview.redd.it/813126g0bqlg1.png?width=140&amp;amp;height=128&amp;amp;auto=webp&amp;amp;s=d025e000e9430cc7725747c9819b111f0275eb4a" title="Llama Server UI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone.&lt;br /&gt; I have built a local server UI for llama-server. You are welcome to check out the code and use it for yourself. Reason for the project is because I hate to remember the commands and have notepad notes for each separate model and then run it in the command line. This simply one click and done. &lt;/p&gt; &lt;p&gt;Two ways to start the server:&lt;br /&gt; 1. Shortcut. Can be placed on your desktop.&lt;br /&gt; 2. ./llama-ui --start&lt;/p&gt; &lt;p&gt;To uninstall simply run ./llama-ui --uninstall&lt;/p&gt; &lt;p&gt;Cool feature is that it directly integrates with llama.cpp native ui, so chats are persistent. Automatically prompts for redirects to ui chat. Another feature worth noting is ability to change LLM paths with local GGUFs. &lt;/p&gt; &lt;p&gt;REPO:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/tomatomonster69/Llama-Server-UI"&gt;https://github.com/tomatomonster69/Llama-Server-UI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hope you enjoy!&lt;/p&gt; &lt;p&gt;Screenshots:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/813126g0bqlg1.png?width=809&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=853345adb687a9c0d57bf46b52fbb8d500f803a6"&gt;https://preview.redd.it/813126g0bqlg1.png?width=809&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=853345adb687a9c0d57bf46b52fbb8d500f803a6&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/lh31zoy2bqlg1.png?width=3810&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5555bcd4a9eec02a5447fb4b43fc5dec40806f46"&gt;https://preview.redd.it/lh31zoy2bqlg1.png?width=3810&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5555bcd4a9eec02a5447fb4b43fc5dec40806f46&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Additional-Action566"&gt; /u/Additional-Action566 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1reuc60/llama_server_ui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1reuc60/llama_server_ui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1reuc60/llama_server_ui/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-25T23:55:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1re72h4</id>
    <title>Qwen3.5 27B better than 35B-A3B?</title>
    <updated>2026-02-25T07:49:05+00:00</updated>
    <author>
      <name>/u/-OpenSourcer</name>
      <uri>https://old.reddit.com/user/-OpenSourcer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1re72h4/qwen35_27b_better_than_35ba3b/"&gt; &lt;img alt="Qwen3.5 27B better than 35B-A3B?" src="https://preview.redd.it/f9x0emmuillg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bee689086672602801cb1e88155d725c01342793" title="Qwen3.5 27B better than 35B-A3B?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Which model would be better with 16 GB of VRAM and 32 GB of RAM?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-OpenSourcer"&gt; /u/-OpenSourcer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/f9x0emmuillg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1re72h4/qwen35_27b_better_than_35ba3b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1re72h4/qwen35_27b_better_than_35ba3b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-25T07:49:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1rer60n</id>
    <title>LM Link</title>
    <updated>2026-02-25T21:53:48+00:00</updated>
    <author>
      <name>/u/Blindax</name>
      <uri>https://old.reddit.com/user/Blindax</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I see that LM Studio just shadow dropped one of the most amazing features ever. I have been waiting this for a long time. &lt;/p&gt; &lt;p&gt;LM Link allows a client machine to connect to another machine acting as server remotely using tailscale. This is now integrated in the LM Studio app (which either acts as server or client) and using the GUI. &lt;/p&gt; &lt;p&gt;Basically, this means you can now use on your laptop all your models present on your main workstation/server just as if you were sitting in front of it. &lt;/p&gt; &lt;p&gt;The feature is currently included in the 0.4.5 build 2 that just released and it's in preview (access needs to be requested and is granted in batches / i got mine minutes after request). &lt;/p&gt; &lt;p&gt;It seems to work incredibily well. &lt;/p&gt; &lt;p&gt;Once again these guys nailed it. Congrats to the team!!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Blindax"&gt; /u/Blindax &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rer60n/lm_link/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rer60n/lm_link/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rer60n/lm_link/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-25T21:53:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1res533</id>
    <title>I found the "Lobotomy Layers" in Llama 3.1 and Qwen 2.5. (Kill Zone Atlas)</title>
    <updated>2026-02-25T22:30:15+00:00</updated>
    <author>
      <name>/u/NoSir261</name>
      <uri>https://old.reddit.com/user/NoSir261</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1res533/i_found_the_lobotomy_layers_in_llama_31_and_qwen/"&gt; &lt;img alt="I found the &amp;quot;Lobotomy Layers&amp;quot; in Llama 3.1 and Qwen 2.5. (Kill Zone Atlas)" src="https://preview.redd.it/jshzjkh0wplg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0657966a9edb5b7ef69541c8ef1cbb88984ecf8a" title="I found the &amp;quot;Lobotomy Layers&amp;quot; in Llama 3.1 and Qwen 2.5. (Kill Zone Atlas)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ever wonder why &amp;quot;safe&amp;quot; models feel dumber? I mapped the &amp;quot;kill zones&amp;quot; of three major 7B/8B models to see what happens to Factual Integrity and Bias when you force a model to be sycophantic.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Heatmaps:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Green&lt;/strong&gt; = Model is getting &amp;quot;more confident&amp;quot; in that behavior.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Red&lt;/strong&gt; = The behavior is collapsing (The &amp;quot;Kill Zone&amp;quot;).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The Results are interesting:&lt;/strong&gt; In &lt;strong&gt;Llama-3.1-8B&lt;/strong&gt;, the &amp;quot;Kill Zone&amp;quot; (dashed red box) is an absolute graveyard for Bias calibration. Between 35% and 52% depth, the model’s internal logic for bias completely inverts (−0.41).&lt;/p&gt; &lt;p&gt;Meanwhile, Qwen seems much more resilient. Its sycophancy &amp;quot;switch&amp;quot; is isolated to a tiny window at 60% depth, leaving the factual layers mostly untouched.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why this matters:&lt;/strong&gt; If you're doing LoRA or RepE, &lt;strong&gt;stay out of the dashed boxes.&lt;/strong&gt; These are the layers where the model's &amp;quot;common sense&amp;quot; is most vulnerable to being overwritten.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NoSir261"&gt; /u/NoSir261 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jshzjkh0wplg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1res533/i_found_the_lobotomy_layers_in_llama_31_and_qwen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1res533/i_found_the_lobotomy_layers_in_llama_31_and_qwen/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-25T22:30:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1rei65v</id>
    <title>Qwen3.5-35B-A3B quantization quality + speed benchmarks on RTX 5080 16GB (Q8_0 vs Q4_K_M vs UD-Q4_K_XL)</title>
    <updated>2026-02-25T16:35:49+00:00</updated>
    <author>
      <name>/u/gaztrab</name>
      <uri>https://old.reddit.com/user/gaztrab</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ran some benchmarks on Qwen3.5-35B-A3B with llama.cpp on a single-GPU consumer workstation. Model doesn't fit in VRAM so this is a CPU/GPU offloading setup over PCIe 5.0.&lt;/p&gt; &lt;h1&gt;System Specs&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Component&lt;/th&gt; &lt;th align="left"&gt;Spec&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;GPU&lt;/td&gt; &lt;td align="left"&gt;NVIDIA GeForce RTX 5080 16GB GDDR7 (Blackwell, sm_120, 960 GB/s bandwidth)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;CPU&lt;/td&gt; &lt;td align="left"&gt;AMD Ryzen 9 9950X (32 threads)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;RAM&lt;/td&gt; &lt;td align="left"&gt;128 GB DDR5-4800 (dual channel, ~77 GB/s)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;PCIe&lt;/td&gt; &lt;td align="left"&gt;5.0 x16 (~64 GB/s bidirectional)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;OS&lt;/td&gt; &lt;td align="left"&gt;Ubuntu 24.04.3 LTS, kernel 6.17.0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;13.1, driver 590.48.01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama.cpp&lt;/td&gt; &lt;td align="left"&gt;b1-9051663 (main benchmarks), b1-a96a112 (for --fit on tests). Built with -DGGML_CUDA=ON -DCMAKE_CUDA_ARCHITECTURES=120 -DGGML_CUDA_FA_ALL_QUANTS=ON&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Quantization Quality (WikiText-2 Perplexity)&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Quant&lt;/th&gt; &lt;th align="left"&gt;Size&lt;/th&gt; &lt;th align="left"&gt;PPL&lt;/th&gt; &lt;th align="left"&gt;vs Q8_0&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Q8_0&lt;/td&gt; &lt;td align="left"&gt;36.9 GB&lt;/td&gt; &lt;td align="left"&gt;6.5342&lt;/td&gt; &lt;td align="left"&gt;baseline&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;~20 GB&lt;/td&gt; &lt;td align="left"&gt;6.6688&lt;/td&gt; &lt;td align="left"&gt;+2.1%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;UD-Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;~19 GB&lt;/td&gt; &lt;td align="left"&gt;7.1702&lt;/td&gt; &lt;td align="left"&gt;+9.7%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;UD-Q4_K_XL is significantly worse than standard Q4_K_M on this model&lt;/strong&gt; — both larger file size and nearly 10% higher perplexity. This is consistent with other reports of Unsloth Dynamic quants underperforming on MoE architectures (&lt;a href="/u/ubergarm"&gt;u/ubergarm&lt;/a&gt;'s KLD data on Qwen3-30B-A3B showed the same pattern). &lt;strong&gt;If you're running Qwen3.5-35B-A3B at Q4, use standard Q4_K_M.&lt;/strong&gt;&lt;/p&gt; &lt;h1&gt;Speed Benchmarks&lt;/h1&gt; &lt;p&gt;All configs: 20 threads, 65K context, flash attention, &lt;code&gt;--no-mmap&lt;/code&gt;, KV cache q8_0, llama.cpp built from source.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Config&lt;/th&gt; &lt;th align="left"&gt;Quant&lt;/th&gt; &lt;th align="left"&gt;Strategy&lt;/th&gt; &lt;th align="left"&gt;tok/s (short)&lt;/th&gt; &lt;th align="left"&gt;tok/s (medium)&lt;/th&gt; &lt;th align="left"&gt;tok/s (long)&lt;/th&gt; &lt;th align="left"&gt;VRAM&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Full offload&lt;/td&gt; &lt;td align="left"&gt;Q8_0&lt;/td&gt; &lt;td align="left"&gt;&lt;code&gt;-ot &amp;quot;exps=CPU&amp;quot;&lt;/code&gt;&lt;/td&gt; &lt;td align="left"&gt;35.7&lt;/td&gt; &lt;td align="left"&gt;32.8&lt;/td&gt; &lt;td align="left"&gt;33.2&lt;/td&gt; &lt;td align="left"&gt;8064 MB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Auto-fit&lt;/td&gt; &lt;td align="left"&gt;Q8_0&lt;/td&gt; &lt;td align="left"&gt;&lt;code&gt;--fit on (b8149)&lt;/code&gt;&lt;/td&gt; &lt;td align="left"&gt;40.5&lt;/td&gt; &lt;td align="left"&gt;40.3&lt;/td&gt; &lt;td align="left"&gt;39.6&lt;/td&gt; &lt;td align="left"&gt;14660 MB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Full offload&lt;/td&gt; &lt;td align="left"&gt;Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;&lt;code&gt;-ot &amp;quot;exps=CPU&amp;quot;&lt;/code&gt;&lt;/td&gt; &lt;td align="left"&gt;51.0&lt;/td&gt; &lt;td align="left"&gt;49.8&lt;/td&gt; &lt;td align="left"&gt;49.4&lt;/td&gt; &lt;td align="left"&gt;7217 MB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Partial offload&lt;/td&gt; &lt;td align="left"&gt;Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;&lt;code&gt;--n-cpu-moe 24&lt;/code&gt;&lt;/td&gt; &lt;td align="left"&gt;69.6&lt;/td&gt; &lt;td align="left"&gt;67.0&lt;/td&gt; &lt;td align="left"&gt;65.7&lt;/td&gt; &lt;td align="left"&gt;14874 MB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Auto-fit&lt;/td&gt; &lt;td align="left"&gt;Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;&lt;code&gt;--fit on&lt;/code&gt;&lt;/td&gt; &lt;td align="left"&gt;67.4&lt;/td&gt; &lt;td align="left"&gt;62.3&lt;/td&gt; &lt;td align="left"&gt;64.1&lt;/td&gt; &lt;td align="left"&gt;14551 MB&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;em&gt;Note: The&lt;/em&gt; &lt;strong&gt;&lt;em&gt;--fit&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;on configs (auto-fit rows) were tested on a newer llama.cpp build (&lt;/em&gt;&lt;strong&gt;&lt;em&gt;a96a112&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;) since the older build didn't support the flag. All other configs used build&lt;/em&gt; &lt;strong&gt;&lt;em&gt;9051663&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Each workload ran 5 times (first discarded as warmup). Standard deviations were generally &amp;lt; 1 tok/s except for configs close to VRAM limits.&lt;/p&gt; &lt;h1&gt;Key Takeaways&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Best config for 16GB VRAM:&lt;/strong&gt; Q4_K_M with &lt;code&gt;--n-cpu-moe 24&lt;/code&gt; (keeps 16/40 MoE layers on GPU, offloads 24 to CPU). ~70 tok/s with only 2.1% PPL loss vs Q8_0.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;KV cache q8_0 is a free lunch:&lt;/strong&gt; Compared to f16 KV cache, q8_0 gives +12-38% throughput AND uses less VRAM. No reason not to use &lt;code&gt;-ctk q8_0 -ctv q8_0&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;--fit on works but manual tuning beats it:&lt;/strong&gt; The new auto-fit flag in b8149 is convenient and gets you ~90-95% of the way there, but hand-tuning &lt;code&gt;--n-cpu-moe&lt;/code&gt; gets another 7% on top.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;--n-cpu-moe sweet spot matters:&lt;/strong&gt; For Q4_K_M on 16GB, &lt;code&gt;--n-cpu-moe 16&lt;/code&gt; OOMs and &lt;code&gt;--n-cpu-moe 32&lt;/code&gt; is too conservative. 24 is the sweet spot. For Q8_0, even &lt;code&gt;--n-cpu-moe 32&lt;/code&gt; barely fits.&lt;/p&gt; &lt;h1&gt;Launch Command&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;./llama-server \ -m ./Qwen3.5-35B-A3B-Q4_K_M.gguf \ -c 65536 \ -ngl 999 \ --n-cpu-moe 24 \ -fa on \ -t 20 \ -b 4096 \ -ub 4096 \ --no-mmap \ --jinja \ -ctk q8_0 \ -ctv q8_0 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Happy to answer questions about the setup. Previous model was Qwen3-Next-80B-A3B at ~22 tok/s on the same hardware, so this is a 3.2x speedup with a much more capable model.Qwen3.5-35B-A3B Benchmarks on RTX 5080 16GB&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gaztrab"&gt; /u/gaztrab &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rei65v/qwen3535ba3b_quantization_quality_speed/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rei65v/qwen3535ba3b_quantization_quality_speed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rei65v/qwen3535ba3b_quantization_quality_speed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-25T16:35:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1re6ifz</id>
    <title>Anthropic is the leading contributor to open weight models</title>
    <updated>2026-02-25T07:15:29+00:00</updated>
    <author>
      <name>/u/DealingWithIt202s</name>
      <uri>https://old.reddit.com/user/DealingWithIt202s</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It just happens to be entirely against their will and TOS. I say: Distill Baby Distill!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DealingWithIt202s"&gt; /u/DealingWithIt202s &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1re6ifz/anthropic_is_the_leading_contributor_to_open/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1re6ifz/anthropic_is_the_leading_contributor_to_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1re6ifz/anthropic_is_the_leading_contributor_to_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-25T07:15:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1rf2zz1</id>
    <title>Qwen3.5-35B-A3B is awesome</title>
    <updated>2026-02-26T06:41:05+00:00</updated>
    <author>
      <name>/u/mim722</name>
      <uri>https://old.reddit.com/user/mim722</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rf2zz1/qwen3535ba3b_is_awesome/"&gt; &lt;img alt="Qwen3.5-35B-A3B is awesome" src="https://preview.redd.it/xxh3n7k2bslg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f873297530d173fd3158661f7546ce680e830597" title="Qwen3.5-35B-A3B is awesome" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;there is a substantial progress , still hoping for qwen3.5-4b&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/djouallah/semantic_sql_testing"&gt;https://github.com/djouallah/semantic_sql_testing&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mim722"&gt; /u/mim722 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xxh3n7k2bslg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rf2zz1/qwen3535ba3b_is_awesome/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rf2zz1/qwen3535ba3b_is_awesome/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-26T06:41:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1renq5y</id>
    <title>Qwen3.5 Model Comparison: 27B vs 35B on RTX 4090</title>
    <updated>2026-02-25T19:47:47+00:00</updated>
    <author>
      <name>/u/jaigouk</name>
      <uri>https://old.reddit.com/user/jaigouk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1renq5y/qwen35_model_comparison_27b_vs_35b_on_rtx_4090/"&gt; &lt;img alt="Qwen3.5 Model Comparison: 27B vs 35B on RTX 4090" src="https://preview.redd.it/ka3y8fx2rplg1.png?width=140&amp;amp;height=84&amp;amp;auto=webp&amp;amp;s=544e43f55178cb3b3992be9eb53cc2200f4b4461" title="Qwen3.5 Model Comparison: 27B vs 35B on RTX 4090" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to check qwen3.5 35B-A3B models that can be run on my GPU. So I compared 3 GGUF options.&lt;/p&gt; &lt;p&gt;Update: Based on comments I got, I created &lt;a href="https://github.com/jaigouk/gpumod/tree/main/docs/benchmarks/job_queue_challenge"&gt;another benchmark.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;-----------------------------------------------------------------------------------------------&lt;/p&gt; &lt;h1&gt;Job Queue Challenge Benchmark&lt;/h1&gt; &lt;p&gt;A graduated difficulty benchmark for evaluating LLM coding capabilities.&lt;/p&gt; &lt;h1&gt;Overview&lt;/h1&gt; &lt;p&gt;This benchmark tests an LLM's ability to implement increasingly complex features in a task queue system. Unlike simple pass/fail tests, it produces a &lt;strong&gt;percentage score&lt;/strong&gt; that discriminates between model capabilities.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;**Judge:** Claude Code (Opus 4.6) — designed prompts, ran benchmarks, scored results via pytest &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Difficulty Levels&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Level&lt;/th&gt; &lt;th align="left"&gt;Task&lt;/th&gt; &lt;th align="left"&gt;Points&lt;/th&gt; &lt;th align="left"&gt;Observed Pass Rate&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;L1&lt;/td&gt; &lt;td align="left"&gt;Basic queue (add/get, FIFO)&lt;/td&gt; &lt;td align="left"&gt;25&lt;/td&gt; &lt;td align="left"&gt;100% (4/4)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;L2&lt;/td&gt; &lt;td align="left"&gt;Retry with exponential backoff&lt;/td&gt; &lt;td align="left"&gt;25&lt;/td&gt; &lt;td align="left"&gt;0% (0/4)*&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;L3&lt;/td&gt; &lt;td align="left"&gt;Priority scheduling&lt;/td&gt; &lt;td align="left"&gt;25&lt;/td&gt; &lt;td align="left"&gt;75% (3/4)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;L4&lt;/td&gt; &lt;td align="left"&gt;Find &amp;amp; fix concurrency bug&lt;/td&gt; &lt;td align="left"&gt;15&lt;/td&gt; &lt;td align="left"&gt;50% (2/4)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;L5&lt;/td&gt; &lt;td align="left"&gt;Multi-file refactoring&lt;/td&gt; &lt;td align="left"&gt;10&lt;/td&gt; &lt;td align="left"&gt;0% (0/4)&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;*L2 failures due to thinking models exhausting &lt;code&gt;max_tokens=8192&lt;/code&gt; budget before producing output.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Total: 100 points&lt;/strong&gt;&lt;/p&gt; &lt;h1&gt;Score Interpretation&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Score&lt;/th&gt; &lt;th align="left"&gt;Interpretation&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;0-25&lt;/td&gt; &lt;td align="left"&gt;Weak: Only basic operations work&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;25-50&lt;/td&gt; &lt;td align="left"&gt;Average: Basic + priority or concurrency&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;50-75&lt;/td&gt; &lt;td align="left"&gt;Good: Multiple advanced levels passed&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;75-90&lt;/td&gt; &lt;td align="left"&gt;Excellent: Most levels including L4 bug fix&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;90-100&lt;/td&gt; &lt;td align="left"&gt;Expert: Full refactoring capability&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Running the Benchmark&lt;/h1&gt; &lt;h1&gt;Prerequisites&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;# Ensure a model is running uv run gpumod service start qwen35-35b-q3-multi &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Run All Levels&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;uv run python docs/benchmarks/job_queue_challenge/benchmark_runner.py \ --model qwen35-35b-q3-multi \ --port 7081 \ --output docs/benchmarks/job_queue_challenge/ &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Run Specific Levels&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;# Only L1-L3 uv run python docs/benchmarks/job_queue_challenge/benchmark_runner.py \ --model qwen35-35b-q3-multi \ --port 7081 \ --levels L1 L2 L3 &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Test Details&lt;/h1&gt; &lt;h1&gt;L1: Basic Queue Operations (5 tests)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;add_job()&lt;/code&gt; returns job_id&lt;/li&gt; &lt;li&gt;&lt;code&gt;get_result()&lt;/code&gt; returns computed value&lt;/li&gt; &lt;li&gt;Multiple jobs execute correctly&lt;/li&gt; &lt;li&gt;FIFO ordering maintained&lt;/li&gt; &lt;li&gt;Nonexistent job handling&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;L2: Retry with Backoff (5 tests)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Job retries on exception&lt;/li&gt; &lt;li&gt;Max 3 retries (4 total attempts)&lt;/li&gt; &lt;li&gt;Exponential backoff: 1s, 2s, 4s&lt;/li&gt; &lt;li&gt;Successful jobs don't retry&lt;/li&gt; &lt;li&gt;Mixed success/failure handling&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;L3: Priority Queue (5 tests)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Higher priority executes first&lt;/li&gt; &lt;li&gt;Same priority uses FIFO&lt;/li&gt; &lt;li&gt;Mixed priorities sort correctly&lt;/li&gt; &lt;li&gt;Default priority works&lt;/li&gt; &lt;li&gt;Priority with args/kwargs&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;L4: Concurrency Bug Fix (1 test)&lt;/h1&gt; &lt;p&gt;Given buggy code with a race condition in &lt;code&gt;self.results[job_id] = result&lt;/code&gt; (unprotected write), the model must:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Identify the bug&lt;/li&gt; &lt;li&gt;Fix it with proper locking&lt;/li&gt; &lt;li&gt;Pass concurrent completion test with 100 jobs&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;L5: Multi-file Refactor (2 tests)&lt;/h1&gt; &lt;p&gt;Refactor monolithic &lt;a href="http://queue.py"&gt;&lt;code&gt;queue.py&lt;/code&gt;&lt;/a&gt; into:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;queue/ __init__.py # Exports JobQueue core.py # Base class retry.py # Retry logic priority.py # Priority handling &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Comparing Models&lt;/h1&gt; &lt;p&gt;To compare models fairly:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Same VRAM budget&lt;/strong&gt;: Compare models that fit in same memory&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multiple runs&lt;/strong&gt;: Run 3x and average to account for variance&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Document architecture&lt;/strong&gt;: Note whether comparing MoE vs dense&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Recommended Comparisons&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Comparison&lt;/th&gt; &lt;th align="left"&gt;Models&lt;/th&gt; &lt;th align="left"&gt;Why Fair&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;MoE vs Dense&lt;/td&gt; &lt;td align="left"&gt;35B-A3B vs 27B&lt;/td&gt; &lt;td align="left"&gt;Different architectures, similar total params&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Quantization impact&lt;/td&gt; &lt;td align="left"&gt;Q4 vs Q3 of same model&lt;/td&gt; &lt;td align="left"&gt;Isolates quant quality&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Architecture + Size&lt;/td&gt; &lt;td align="left"&gt;35B-A3B Q3 vs 27B Q4&lt;/td&gt; &lt;td align="left"&gt;Similar VRAM footprint&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Benchmark Results (2026-02-25)&lt;/h1&gt; &lt;h1&gt;Configuration&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;# Single-slot mode (--parallel 1) for maximum quality per request # llama.cpp preset: --parallel 1 --threads 16 (no cont-batching) # Benchmark runner: 1 request at a time, max_tokens=8192, temperature=0.1 uv run python docs/benchmarks/job_queue_challenge/benchmark_runner.py \ --model qwen35-35b-q3-single \ --port 7091 \ --output docs/benchmarks/job_queue_challenge/ &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Hardware:&lt;/strong&gt; RTX 4090 (24GB VRAM) &lt;strong&gt;llama.cpp flags:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;--parallel 1&lt;/code&gt; — Single request (no batching)&lt;/li&gt; &lt;li&gt;&lt;code&gt;--threads 16&lt;/code&gt; — CPU thread count&lt;/li&gt; &lt;li&gt;&lt;code&gt;--jinja&lt;/code&gt; — Enable Jinja chat templates (required for Qwen3.5)&lt;/li&gt; &lt;li&gt;&lt;code&gt;-ngl -1&lt;/code&gt; — Full GPU offload&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Benchmark settings:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;max_tokens=8192&lt;/code&gt; — Token generation limit&lt;/li&gt; &lt;li&gt;&lt;code&gt;temperature=0.1&lt;/code&gt; — Low temperature for deterministic output&lt;/li&gt; &lt;li&gt;&lt;code&gt;/no_think&lt;/code&gt; prefix — Disable chain-of-thought for direct code output&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Summary&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Total&lt;/th&gt; &lt;th align="left"&gt;L1&lt;/th&gt; &lt;th align="left"&gt;L2&lt;/th&gt; &lt;th align="left"&gt;L3&lt;/th&gt; &lt;th align="left"&gt;L4&lt;/th&gt; &lt;th align="left"&gt;L5&lt;/th&gt; &lt;th align="left"&gt;Time&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Qwen3.5-35B-A3B Q3&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;65%&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;25&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;25&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;15&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;267s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Qwen3.5-27B Q4&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;65%&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;25&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;25&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;15&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;622s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3.5-27B Q3&lt;/td&gt; &lt;td align="left"&gt;20%&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;5&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;15&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;567s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3.5-35B-A3B Q4&lt;/td&gt; &lt;td align="left"&gt;15%&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;15&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;225s&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Key Findings&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;L4 (concurrency bug) solved by all models&lt;/strong&gt; — All 4 configurations correctly identified and fixed the race condition&lt;/li&gt; &lt;li&gt;&lt;strong&gt;L2 (retry logic) fails for all models&lt;/strong&gt; — thinking models exhaust 8192 token budget before producing code; &lt;code&gt;/no_think&lt;/code&gt; prefix helps but Qwen3.5 still reasons internally&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Q3 outperformed Q4 in this run&lt;/strong&gt; — Unexpected result, likely due to single-run variance; Q4 models had more empty responses (timeout)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;MoE 35B-A3B is 2-3x faster&lt;/strong&gt; — 267s vs 622s for same score&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Empty responses&lt;/strong&gt; — Some models timed out (174s for 27B Q3 L1) without producing output&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Architecture Comparison&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Aspect&lt;/th&gt; &lt;th align="left"&gt;27B (Dense)&lt;/th&gt; &lt;th align="left"&gt;35B-A3B (MoE)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Active params&lt;/td&gt; &lt;td align="left"&gt;27B&lt;/td&gt; &lt;td align="left"&gt;3B&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;L4 Bug Fix&lt;/td&gt; &lt;td align="left"&gt;✅ All pass&lt;/td&gt; &lt;td align="left"&gt;✅ All pass&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Speed&lt;/td&gt; &lt;td align="left"&gt;Slower (70-200s per level)&lt;/td&gt; &lt;td align="left"&gt;Faster (30-60s per level)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Best score&lt;/td&gt; &lt;td align="left"&gt;65% (Q4)&lt;/td&gt; &lt;td align="left"&gt;65% (Q3)&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;-----------------------------------------------------------------------------------------------&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Hardware:&lt;/strong&gt; RTX 4090 (24GB VRAM)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Test:&lt;/strong&gt; Multi-agent Tetris development (Planner → Developer → QA)&lt;/p&gt; &lt;h1&gt;Models Under Test&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Preset&lt;/th&gt; &lt;th align="left"&gt;Quant&lt;/th&gt; &lt;th align="left"&gt;Port&lt;/th&gt; &lt;th align="left"&gt;VRAM&lt;/th&gt; &lt;th align="left"&gt;Parallel&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3.5-27B&lt;/td&gt; &lt;td align="left"&gt;&lt;code&gt;qwen35-27b-multi&lt;/code&gt;&lt;/td&gt; &lt;td align="left"&gt;Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;7082&lt;/td&gt; &lt;td align="left"&gt;17 GB&lt;/td&gt; &lt;td align="left"&gt;3 slots&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3.5-35B-A3B&lt;/td&gt; &lt;td align="left"&gt;&lt;code&gt;qwen35-35b-q3-multi&lt;/code&gt;&lt;/td&gt; &lt;td align="left"&gt;Q3_K_XL&lt;/td&gt; &lt;td align="left"&gt;7081&lt;/td&gt; &lt;td align="left"&gt;16 GB&lt;/td&gt; &lt;td align="left"&gt;3 slots&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3.5-35B-A3B&lt;/td&gt; &lt;td align="left"&gt;&lt;code&gt;qwen35-35b-multi&lt;/code&gt;&lt;/td&gt; &lt;td align="left"&gt;Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;7080&lt;/td&gt; &lt;td align="left"&gt;20 GB&lt;/td&gt; &lt;td align="left"&gt;3 slots&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Architecture comparison:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;27B&lt;/strong&gt;: Dense model, 27B total / 27B active params&lt;/li&gt; &lt;li&gt;&lt;strong&gt;35B-A3B&lt;/strong&gt;: Sparse MoE, 35B total / 3B active params&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Charts&lt;/h1&gt; &lt;h1&gt;Total Time Comparison&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ka3y8fx2rplg1.png?width=1500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b9c1882103038f5fa3086e58fcd7faf9dc4c869e"&gt;https://preview.redd.it/ka3y8fx2rplg1.png?width=1500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b9c1882103038f5fa3086e58fcd7faf9dc4c869e&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Phase Breakdown&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/o8qt63w3rplg1.png?width=1500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ad6a27c1d7b59bced124cbe0146b9056467def64"&gt;https://preview.redd.it/o8qt63w3rplg1.png?width=1500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ad6a27c1d7b59bced124cbe0146b9056467def64&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;VRAM Efficiency&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/lfeui655rplg1.png?width=1500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=077cbb64fac01054ca522c0b99a9547f82977499"&gt;https://preview.redd.it/lfeui655rplg1.png?width=1500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=077cbb64fac01054ca522c0b99a9547f82977499&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Code Output Comparison&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/bcrvu1x6rplg1.png?width=1500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6e623b9a8dab4a8fb1b3ad962e9cb71fada8ae80"&gt;https://preview.redd.it/bcrvu1x6rplg1.png?width=1500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6e623b9a8dab4a8fb1b3ad962e9cb71fada8ae80&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Results&lt;/h1&gt; &lt;h1&gt;Summary&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;VRAM&lt;/th&gt; &lt;th align="left"&gt;Total Time&lt;/th&gt; &lt;th align="left"&gt;Plan&lt;/th&gt; &lt;th align="left"&gt;Dev&lt;/th&gt; &lt;th align="left"&gt;QA&lt;/th&gt; &lt;th align="left"&gt;Lines&lt;/th&gt; &lt;th align="left"&gt;Valid&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3.5-27B Q4&lt;/td&gt; &lt;td align="left"&gt;17 GB&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;134.0s&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;36.3s&lt;/td&gt; &lt;td align="left"&gt;72.1s&lt;/td&gt; &lt;td align="left"&gt;25.6s&lt;/td&gt; &lt;td align="left"&gt;312&lt;/td&gt; &lt;td align="left"&gt;YES&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Qwen3.5-35B-A3B Q3&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;16 GB&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;34.8s&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;7.3s&lt;/td&gt; &lt;td align="left"&gt;20.1s&lt;/td&gt; &lt;td align="left"&gt;7.5s&lt;/td&gt; &lt;td align="left"&gt;322&lt;/td&gt; &lt;td align="left"&gt;YES&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3.5-35B-A3B Q4&lt;/td&gt; &lt;td align="left"&gt;20 GB&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;37.8s&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;8.2s&lt;/td&gt; &lt;td align="left"&gt;22.0s&lt;/td&gt; &lt;td align="left"&gt;7.6s&lt;/td&gt; &lt;td align="left"&gt;311&lt;/td&gt; &lt;td align="left"&gt;YES&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Key Findings&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;35B-A3B models are dramatically faster than 27B&lt;/strong&gt; — 35s vs 134s (3.8x faster!)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;35B-A3B Q3 is fastest overall&lt;/strong&gt; — 34.8s total, uses only 16GB VRAM&lt;/li&gt; &lt;li&gt;&lt;strong&gt;35B-A3B Q4 slightly slower than Q3&lt;/strong&gt; — 37.8s vs 34.8s (8% slower, 4GB more VRAM)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;27B is surprisingly slow&lt;/strong&gt; — Dense architecture less efficient than sparse MoE&lt;/li&gt; &lt;li&gt;&lt;strong&gt;All models produced valid, runnable code&lt;/strong&gt; — 311-322 lines each&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Speed Comparison&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Phase&lt;/th&gt; &lt;th align="left"&gt;27B Q4&lt;/th&gt; &lt;th align="left"&gt;35B-A3B Q3&lt;/th&gt; &lt;th align="left"&gt;35B-A3B Q4&lt;/th&gt; &lt;th align="left"&gt;35B-A3B Q3 vs 27B&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Planning&lt;/td&gt; &lt;td align="left"&gt;36.3s&lt;/td&gt; &lt;td align="left"&gt;7.3s&lt;/td&gt; &lt;td align="left"&gt;8.2s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;5.0x faster&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Development&lt;/td&gt; &lt;td align="left"&gt;72.1s&lt;/td&gt; &lt;td align="left"&gt;20.1s&lt;/td&gt; &lt;td align="left"&gt;22.0s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;3.6x faster&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;QA Review&lt;/td&gt; &lt;td align="left"&gt;25.6s&lt;/td&gt; &lt;td align="left"&gt;7.5s&lt;/td&gt; &lt;td align="left"&gt;7.6s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;3.4x faster&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Total&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;134.0s&lt;/td&gt; &lt;td align="left"&gt;34.8s&lt;/td&gt; &lt;td align="left"&gt;37.8s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;3.8x faster&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;VRAM Efficiency&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;VRAM&lt;/th&gt; &lt;th align="left"&gt;Time&lt;/th&gt; &lt;th align="left"&gt;VRAM Efficiency&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;35B-A3B Q3&lt;/td&gt; &lt;td align="left"&gt;16 GB&lt;/td&gt; &lt;td align="left"&gt;34.8s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Best&lt;/strong&gt; (fastest, lowest VRAM)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;27B Q4&lt;/td&gt; &lt;td align="left"&gt;17 GB&lt;/td&gt; &lt;td align="left"&gt;134.0s&lt;/td&gt; &lt;td align="left"&gt;Worst (slow, mid VRAM)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;35B-A3B Q4&lt;/td&gt; &lt;td align="left"&gt;20 GB&lt;/td&gt; &lt;td align="left"&gt;37.8s&lt;/td&gt; &lt;td align="left"&gt;Good (fast, highest VRAM)&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Generated Code &amp;amp; QA Analysis&lt;/h1&gt; &lt;p&gt;All three models produced functional Tetris games with similar structure:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Lines&lt;/th&gt; &lt;th align="left"&gt;Chars&lt;/th&gt; &lt;th align="left"&gt;Syntax&lt;/th&gt; &lt;th align="left"&gt;QA Verdict&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;27B Q4&lt;/td&gt; &lt;td align="left"&gt;312&lt;/td&gt; &lt;td align="left"&gt;11,279&lt;/td&gt; &lt;td align="left"&gt;VALID&lt;/td&gt; &lt;td align="left"&gt;Issues noted&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;35B-A3B Q3&lt;/td&gt; &lt;td align="left"&gt;322&lt;/td&gt; &lt;td align="left"&gt;11,260&lt;/td&gt; &lt;td align="left"&gt;VALID&lt;/td&gt; &lt;td align="left"&gt;Issues noted&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;35B-A3B Q4&lt;/td&gt; &lt;td align="left"&gt;311&lt;/td&gt; &lt;td align="left"&gt;10,260&lt;/td&gt; &lt;td align="left"&gt;VALID&lt;/td&gt; &lt;td align="left"&gt;Issues noted&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;QA Review Summary&lt;/h1&gt; &lt;p&gt;All three QA agents identified similar potential issues in the generated code:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Common observations across models:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Collision detection edge cases (pieces near board edges)&lt;/li&gt; &lt;li&gt;Rotation wall-kick not fully implemented&lt;/li&gt; &lt;li&gt;Score calculation could have edge cases with &amp;gt;4 lines&lt;/li&gt; &lt;li&gt;Game over detection timing&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Verdict:&lt;/strong&gt; All three games compile and run correctly. The QA agents were thorough in identifying &lt;em&gt;potential&lt;/em&gt; edge cases, but the core gameplay functions properly. The issues noted are improvements rather than bugs blocking playability.&lt;/p&gt; &lt;h1&gt;Code Quality Comparison&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Aspect&lt;/th&gt; &lt;th align="left"&gt;27B Q4&lt;/th&gt; &lt;th align="left"&gt;35B-A3B Q3&lt;/th&gt; &lt;th align="left"&gt;35B-A3B Q4&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Class structure&lt;/td&gt; &lt;td align="left"&gt;Good&lt;/td&gt; &lt;td align="left"&gt;Good&lt;/td&gt; &lt;td align="left"&gt;Good&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;All 7 pieces&lt;/td&gt; &lt;td align="left"&gt;Yes&lt;/td&gt; &lt;td align="left"&gt;Yes&lt;/td&gt; &lt;td align="left"&gt;Yes&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Rotation states&lt;/td&gt; &lt;td align="left"&gt;4 each&lt;/td&gt; &lt;td align="left"&gt;4 each&lt;/td&gt; &lt;td align="left"&gt;4 each&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Line clearing&lt;/td&gt; &lt;td align="left"&gt;Yes&lt;/td&gt; &lt;td align="left"&gt;Yes&lt;/td&gt; &lt;td align="left"&gt;Yes&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Scoring&lt;/td&gt; &lt;td align="left"&gt;Yes&lt;/td&gt; &lt;td align="left"&gt;Yes&lt;/td&gt; &lt;td align="left"&gt;Yes&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Game over&lt;/td&gt; &lt;td align="left"&gt;Yes&lt;/td&gt; &lt;td align="left"&gt;Yes&lt;/td&gt; &lt;td align="left"&gt;Yes&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Controls help&lt;/td&gt; &lt;td align="left"&gt;Yes&lt;/td&gt; &lt;td align="left"&gt;Yes&lt;/td&gt; &lt;td align="left"&gt;Yes&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;All three models produced structurally similar, fully-featured implementations.&lt;/p&gt; &lt;h1&gt;Recommendation&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Qwen3.5-35B-A3B Q3_K_XL as the daily driver.&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;3.8x faster than Qwen3.5-27B&lt;/li&gt; &lt;li&gt;Uses less VRAM (16GB vs 17GB)&lt;/li&gt; &lt;li&gt;Produces equivalent quality code&lt;/li&gt; &lt;li&gt;Best VRAM efficiency of all tested models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Full benchmark with generated code: &lt;a href="https://jaigouk.com/gpumod/benchmarks/20260225_qwen35_comparison/"&gt;https://jaigouk.com/gpumod/benchmarks/20260225_qwen35_comparison/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jaigouk"&gt; /u/jaigouk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1renq5y/qwen35_model_comparison_27b_vs_35b_on_rtx_4090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1renq5y/qwen35_model_comparison_27b_vs_35b_on_rtx_4090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1renq5y/qwen35_model_comparison_27b_vs_35b_on_rtx_4090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-25T19:47:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1rf25jr</id>
    <title>Qwen3.5-27B as good as DeepSeek-V3.2 on AA-II (plus some more data)</title>
    <updated>2026-02-26T05:53:00+00:00</updated>
    <author>
      <name>/u/pigeon57434</name>
      <uri>https://old.reddit.com/user/pigeon57434</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rf25jr/qwen3527b_as_good_as_deepseekv32_on_aaii_plus/"&gt; &lt;img alt="Qwen3.5-27B as good as DeepSeek-V3.2 on AA-II (plus some more data)" src="https://preview.redd.it/zyiabsij2slg1.png?width=140&amp;amp;height=32&amp;amp;auto=webp&amp;amp;s=5f0a3756df31a0909e6e16e651155c7900598d3e" title="Qwen3.5-27B as good as DeepSeek-V3.2 on AA-II (plus some more data)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;According to Artificial Analysis, Qwen3.5-27B-thinking is on par with on raw intelligence (though keep in mind mostly STEM tasks is what AA-II measures). However, it is definitely worse on overall intelligence packed per token, with a much further distance from optimal (shown in the graph). But honestly, sometimes you have to say fuck efficiency when a model 25.3x SMALLER is performing that well (all data pulled from AA, but I put it on my own graph to look better and model against optimal).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pigeon57434"&gt; /u/pigeon57434 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1rf25jr"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rf25jr/qwen3527b_as_good_as_deepseekv32_on_aaii_plus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rf25jr/qwen3527b_as_good_as_deepseekv32_on_aaii_plus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-26T05:53:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1rewz9p</id>
    <title>We build sleep for local LLMs — model learns facts from conversation during wake, maintains them during sleep. Runs on MacBook Air.</title>
    <updated>2026-02-26T01:45:37+00:00</updated>
    <author>
      <name>/u/vbaranov</name>
      <uri>https://old.reddit.com/user/vbaranov</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After 4 months of research (5 papers, 122 development notes), I have a working system where a local LLM forms persistent memories from conversation — no RAG, no database. The facts are in the weights. After restart with an empty context window, the model knows things it learned from talking to you.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How it works:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Wake&lt;/strong&gt;: You chat normally. The system extracts facts and injects them into MLP weights via MEMIT (Mass-Editing Memory in Transformers). Single forward pass, instant recall. No training.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Sleep&lt;/strong&gt;: Type &lt;code&gt;/sleep&lt;/code&gt; and the system audits every stored fact, refreshes degraded ones with null-space constraints (so fixing one memory doesn't break others), and prunes excess.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;What runs where:&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Hardware&lt;/th&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Facts&lt;/th&gt; &lt;th align="left"&gt;Notes&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;MacBook Air M3, 8GB&lt;/td&gt; &lt;td align="left"&gt;Llama-3.2-3B-4bit&lt;/td&gt; &lt;td align="left"&gt;~15&lt;/td&gt; &lt;td align="left"&gt;Works today, sleep ~5 min&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;2×H100 80GB&lt;/td&gt; &lt;td align="left"&gt;Llama-3.1-8B&lt;/td&gt; &lt;td align="left"&gt;30&lt;/td&gt; &lt;td align="left"&gt;100% recall after sleep&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;2×H100 80GB&lt;/td&gt; &lt;td align="left"&gt;Llama-3.1-70B&lt;/td&gt; &lt;td align="left"&gt;60&lt;/td&gt; &lt;td align="left"&gt;100% recall, 0% PPL impact&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;The most surprising finding&lt;/strong&gt;: LoRA-based memory consolidation (my original approach) completely fails at 70B. RLHF alignment creates a behavioral prior that overrides LoRA-injected knowledge — 0% recall despite successful training. The effect gets &lt;em&gt;worse&lt;/em&gt; with model size. I had to abandon LoRA entirely. MEMIT with sleep maintenance turned out to be simpler and more robust.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The biological parallel&lt;/strong&gt;: This is basically CLS theory (Complementary Learning Systems) from neuroscience. Wake = hippocampal fast encoding. Sleep = consolidation. The system even has a &amp;quot;drowsiness signal&amp;quot; — it monitors how many facts are degraded and knows when it needs sleep. &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Setup:&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&amp;#8203;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;git clone https://github.com/vbario/sleeping-llm.git &amp;amp;&amp;amp; cd sleeping-llm pip3 install -r requirements.txt python3 -m src.main &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;First run downloads the model (~1.8 GB). Requires Apple Silicon Mac with macOS 14+.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Papers&lt;/strong&gt; (all free on Zenodo): &lt;a href="https://doi.org/10.5281/zenodo.18778760"&gt;Paper 1&lt;/a&gt; | &lt;a href="https://doi.org/10.5281/zenodo.18778762"&gt;Paper 2&lt;/a&gt; | &lt;a href="https://doi.org/10.5281/zenodo.18778764"&gt;Paper 3&lt;/a&gt; | &lt;a href="https://doi.org/10.5281/zenodo.18778766"&gt;Paper 4&lt;/a&gt; | &lt;a href="https://doi.org/10.5281/zenodo.18778768"&gt;Paper 5&lt;/a&gt; Happy to answer questions. The &lt;code&gt;notes/&lt;/code&gt; directory has 122 numbered research notes if you want to see the full journey including every failure.&lt;/p&gt; &lt;p&gt;Edit: styling&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vbaranov"&gt; /u/vbaranov &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rewz9p/we_build_sleep_for_local_llms_model_learns_facts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rewz9p/we_build_sleep_for_local_llms_model_learns_facts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rewz9p/we_build_sleep_for_local_llms_model_learns_facts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-26T01:45:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1reqdpb</id>
    <title>Overwhelmed by so many quantization variants</title>
    <updated>2026-02-25T21:24:37+00:00</updated>
    <author>
      <name>/u/mouseofcatofschrodi</name>
      <uri>https://old.reddit.com/user/mouseofcatofschrodi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Not only are out there 100s of models to choose from, but also so many quantization variants that I may well get crazy.&lt;/p&gt; &lt;p&gt;One needs not only to test and benchmark models, but also within each model, compare its telemetry and quality between all the available quants and quant-techniques.&lt;/p&gt; &lt;p&gt;So many concepts like the new UD from Unsloth, autoround from Intel, imatrix, K_XSS, you name it. All of them could be with a REAM or a REAP or any kind of prunation, multiplying the length of the list.&lt;/p&gt; &lt;p&gt;Some people claim heavily quantizated models (q2, q3) of some big models are actually better than smaller ones in q4-q6. Some other people claim something else: there are so many claims! And they all sound like the singing of sirens. Someone tie me to the main mast!&lt;/p&gt; &lt;p&gt;When I ask wether to choose mlx or gguf, the answer comes strong like a dogma: mlx for mac. And while it indeed seems to be faster (sometimes only slightlier), mlx offers less configurations. Maybe with gguff I would lose a couple of t/s but gain in context. Or maybe a 4bit mlx is less advanced as the UD q4 of Unsloth and it is faster but with less quality.&lt;/p&gt; &lt;p&gt;And it is a great problem to have: I root for someone super smart to create a brilliant new method that allows to run gigantic models in potato hardware with lossless quality and decent speed. And that is happening: quants are getting super smart ideas.&lt;/p&gt; &lt;p&gt;But also feel totally overwhelmed.&lt;/p&gt; &lt;p&gt;Anyone on the same boat? Are there any leaderboards comparing quant methods and sizes of a single model? &lt;/p&gt; &lt;p&gt;And most importantly, what is the next revolutionary twist that will come to our future quants?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mouseofcatofschrodi"&gt; /u/mouseofcatofschrodi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1reqdpb/overwhelmed_by_so_many_quantization_variants/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1reqdpb/overwhelmed_by_so_many_quantization_variants/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1reqdpb/overwhelmed_by_so_many_quantization_variants/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-25T21:24:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1rey2ko</id>
    <title>Qwen 3.5 35B MoE - 100k Context 40+ TPS on RTX 5060 Ti (16GB)</title>
    <updated>2026-02-26T02:33:46+00:00</updated>
    <author>
      <name>/u/maho_Yun</name>
      <uri>https://old.reddit.com/user/maho_Yun</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rey2ko/qwen_35_35b_moe_100k_context_40_tps_on_rtx_5060/"&gt; &lt;img alt="Qwen 3.5 35B MoE - 100k Context 40+ TPS on RTX 5060 Ti (16GB)" src="https://preview.redd.it/ffpti8wezqlg1.png?width=140&amp;amp;height=9&amp;amp;auto=webp&amp;amp;s=5b1cbd35d8ef36ce102b147c6e41aa3ada329492" title="Qwen 3.5 35B MoE - 100k Context 40+ TPS on RTX 5060 Ti (16GB)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Text only, 100000 context length, gen 720, llama-bench result&lt;/strong&gt;&lt;br /&gt; &lt;strong&gt;VULKAN backend&lt;/strong&gt;&lt;br /&gt; pp100000 696.60 ± 1.41 tps (read)&lt;br /&gt; tg720 &lt;strong&gt;41.35 ± 0.18 tps&lt;/strong&gt; (gen)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ffpti8wezqlg1.png?width=928&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9faa4040ac92d884fa0954cb3c385426bcc342ad"&gt;pp100000 696.60 ± 1.41 tps (read) tg720 41.35 ± 0.18 tps (gen) b8149&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;CUDA backend&lt;/strong&gt;&lt;br /&gt; pp100000 &lt;strong&gt;1304.93 ± 4.10 tps&lt;/strong&gt; (read)&lt;br /&gt; tg720 &lt;strong&gt;44.32 ± 2.16 tps&lt;/strong&gt; (gen)&lt;/p&gt; &lt;p&gt;CPU: AMD Ryzen 7 9700X (16) @ 5.55 GHz&lt;br /&gt; GPU 1: GameViewer Virtual Display Adapter&lt;br /&gt; GPU 2: NVIDIA GeForce RTX 5060 Ti @ 3.09 GHz (15.59 GiB) [Discrete]&lt;br /&gt; Memory: 8.74 GiB / 47.61 GiB (18%)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6l69e1y2grlg1.png?width=626&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0b01ec3e31e4c04bb2999fe54412d64b6f1c7c0f"&gt;Treasure Island (99961 token)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Test Result with Treasure Island (99961 token)&lt;/strong&gt;&lt;br /&gt; Prompt Processing (Fill): &lt;strong&gt;1154.31 tps&lt;/strong&gt;&lt;br /&gt; Token Generation (Gen): &lt;strong&gt;35.14 tps&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;llama.cpp command:&lt;/strong&gt;&lt;br /&gt; llama-server.exe -m &amp;quot;/Qwen3.5-35B-A3B-MXFP4_MOE.gguf&amp;quot; --port 6789 --ctx-size 131072 -n 32768 --flash-attn on -ngl 40 --n-cpu-moe 24 -b 2048 -ub 2048 -t 8 --kv-offload --cont-batching --temp 1.0 --top-p 0.95 --top-k 20 --min-p 0.0 --presence-penalty 1.5 --repeat-penalty 1.0&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/maho_Yun"&gt; /u/maho_Yun &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rey2ko/qwen_35_35b_moe_100k_context_40_tps_on_rtx_5060/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rey2ko/qwen_35_35b_moe_100k_context_40_tps_on_rtx_5060/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rey2ko/qwen_35_35b_moe_100k_context_40_tps_on_rtx_5060/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-26T02:33:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1refvmr</id>
    <title>Qwen 3 27b is... impressive</title>
    <updated>2026-02-25T15:13:40+00:00</updated>
    <author>
      <name>/u/-dysangel-</name>
      <uri>https://old.reddit.com/user/-dysangel-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1refvmr/qwen_3_27b_is_impressive/"&gt; &lt;img alt="Qwen 3 27b is... impressive" src="https://preview.redd.it/5uje69y1pnlg1.gif?frame=1&amp;amp;width=140&amp;amp;height=115&amp;amp;auto=webp&amp;amp;s=756267272f48b42047d0e902abf95d86a1940bda" title="Qwen 3 27b is... impressive" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://i.redd.it/5uje69y1pnlg1.gif"&gt;https://i.redd.it/5uje69y1pnlg1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;All Prompts&lt;/strong&gt;&lt;br /&gt; &amp;quot;Task: create a GTA-like 3D game where you can walk around, get in and drive cars&amp;quot;&lt;br /&gt; &amp;quot;walking forward and backward is working, but I cannot turn or strafe??&amp;quot;&lt;br /&gt; &amp;quot;this is pretty fun! I’m noticing that the camera is facing backward though, for both walking and car?&amp;quot;&lt;br /&gt; &amp;quot;yes, it works! What could we do to enhance the experience now?&amp;quot;&lt;br /&gt; &amp;quot;I’m not too fussed about a HUD, and the physics are not bad as they are already - adding building and obstacles definitely feels like the highest priority!&amp;quot; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-dysangel-"&gt; /u/-dysangel- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1refvmr/qwen_3_27b_is_impressive/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1refvmr/qwen_3_27b_is_impressive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1refvmr/qwen_3_27b_is_impressive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-25T15:13:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1reds0p</id>
    <title>Qwen 3.5 craters on hard coding tasks — tested all Qwen3.5 models (And Codex 5.3) on 70 real repos so you don't have to.</title>
    <updated>2026-02-25T13:52:13+00:00</updated>
    <author>
      <name>/u/hauhau901</name>
      <uri>https://old.reddit.com/user/hauhau901</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1reds0p/qwen_35_craters_on_hard_coding_tasks_tested_all/"&gt; &lt;img alt="Qwen 3.5 craters on hard coding tasks — tested all Qwen3.5 models (And Codex 5.3) on 70 real repos so you don't have to." src="https://preview.redd.it/5g4ostqlbnlg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ea4807a66237a7f8bf87e955618494b8fe058e3f" title="Qwen 3.5 craters on hard coding tasks — tested all Qwen3.5 models (And Codex 5.3) on 70 real repos so you don't have to." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, some of you might remember &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1r7shtv/i_built_a_benchmark_that_tests_coding_llms_on/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1r7shtv/i_built_a_benchmark_that_tests_coding_llms_on/&lt;/a&gt; where I shared APEX Testing — my benchmark that tests coding models on real codebases with real problems.&lt;/p&gt; &lt;p&gt;Since then I've added 5 more tasks (now 70 total), and more importantly tested a bunch of new models people were asking about: all the Qwen 3.5 variants, GPT-5.3 Codex, and several local quantized models running on LM Studio.&lt;/p&gt; &lt;p&gt;I also built a proper agentic tool-use system for the local models now — instead of dumping the entire repo into one prompt, models get all required tools and they explore + implement on their own, just like the cloud agentic models do. Way fairer comparison. Heavy anti-benchmaxxing focus is in place as well so GL to companies who try to take that approach and promise the moon and the stars :)&lt;/p&gt; &lt;p&gt;What caught me off guard:&lt;/p&gt; &lt;p&gt;- Codex 5.3 is basically tied with GPT-5.2 at #4 overall. barely drops across difficulty levels — super consistent from easy to master tasks -&amp;gt; &lt;strong&gt;Recommended&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- Qwen 3.5 397B craters on master tasks. holds ~1550 ELO on hard/expert which is respectable, but drops to 1194 on master. when it needs to coordinate across many files over many steps, it just loses track of what it's doing&lt;/p&gt; &lt;p&gt;- GLM-4.7 quantized is still the local GOAT. 1572 ELO, beats every single Qwen 3.5 model including the full 397B cloud version. if you're picking one local model for coding, this is still it (better than GLM-5 even!)&lt;/p&gt; &lt;p&gt;- Qwen 3.5 27B is genuinely decent on a single GPU though. 1384 ELO, beats DeepSeek V3.2 and all the qwen3-coder models. for &amp;quot;fix this bug&amp;quot; / &amp;quot;add this endpoint&amp;quot; type work it holds up&lt;/p&gt; &lt;p&gt;- The 35B MoE (3B active) is rough. 1256, worse than the 27B dense on almost everything. the tiny active param count really shows on multi-step agentic work&lt;/p&gt; &lt;p&gt;- One qwen model found a loophole lol — qwen3.5-27b ran the test suite on a master task, saw existing tests passing, declared everything &amp;quot;already implemented&amp;quot; and quit without writing a single line of code. it was the only model out of 25+ that tried this. had to patch my system after that one 😅&lt;/p&gt; &lt;p&gt;Still running: Qwen 3.5 122B only has 3/70 tasks done so take that ranking with a grain of salt. &lt;strong&gt;Also planning BF16 and Q8_K_XL runs&lt;/strong&gt; for the Qwen3.5 models to show the real quantization tax — should have those up in a day or two.&lt;/p&gt; &lt;p&gt;Methodology in brief: 70 tasks across real GitHub repos — bug fixes, refactors, from-scratch builds, debugging race conditions, building CLI tools, you name it. All models get the same starting point, agentic tool-use, scored on&lt;/p&gt; &lt;p&gt;Correctness/completeness/quality/efficiency, ELO calculated pairwise with difficulty adjustments. task titles are public on the site, prompts/diffs kept private to avoid contamination. solo project, self-funded ($3000 and counting lol).&lt;/p&gt; &lt;p&gt;Full leaderboard with filters by category, difficulty, per-model breakdowns, and individual run data:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.apex-testing.org"&gt;https://www.apex-testing.org&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer questions, and if you want a specific model tested let me know and I might add it!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hauhau901"&gt; /u/hauhau901 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5g4ostqlbnlg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1reds0p/qwen_35_craters_on_hard_coding_tasks_tested_all/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1reds0p/qwen_35_craters_on_hard_coding_tasks_tested_all/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-25T13:52:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1remcej</id>
    <title>Anthropic Drops Flagship Safety Pledge</title>
    <updated>2026-02-25T18:59:11+00:00</updated>
    <author>
      <name>/u/HumanDrone8721</name>
      <uri>https://old.reddit.com/user/HumanDrone8721</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1remcej/anthropic_drops_flagship_safety_pledge/"&gt; &lt;img alt="Anthropic Drops Flagship Safety Pledge" src="https://external-preview.redd.it/PTr_0OK3p9e9gnNDPqpmy0xkmssi7-vtV1HQVArmozc.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=72d457d9874072e6e8ff3a231754e7000271be9e" title="Anthropic Drops Flagship Safety Pledge" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HumanDrone8721"&gt; /u/HumanDrone8721 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://time.com/7380854/exclusive-anthropic-drops-flagship-safety-pledge/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1remcej/anthropic_drops_flagship_safety_pledge/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1remcej/anthropic_drops_flagship_safety_pledge/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-25T18:59:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1resggh</id>
    <title>Best Qwen3.5-35B-A3B GGUF for 24GB VRAM?!</title>
    <updated>2026-02-25T22:42:03+00:00</updated>
    <author>
      <name>/u/VoidAlchemy</name>
      <uri>https://old.reddit.com/user/VoidAlchemy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1resggh/best_qwen3535ba3b_gguf_for_24gb_vram/"&gt; &lt;img alt="Best Qwen3.5-35B-A3B GGUF for 24GB VRAM?!" src="https://preview.redd.it/bkw8ps1qwplg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=25abc44019457c22c40a183a9f0ff49947bfd3c5" title="Best Qwen3.5-35B-A3B GGUF for 24GB VRAM?!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My understanding is Vulkan/ROCm tends to have faster kernels for legacy llama.cpp quant types like q8_0/q4_0/q4_1. So I made a mix using *only* those types!&lt;/p&gt; &lt;p&gt;Definitely not your grandfather's gguf mix: Q4_0 19.776 GiB (4.901 BPW)&lt;/p&gt; &lt;p&gt;Interestingly it has very good perplexity for the size, and *may be* faster than other leading quants especially on Vulkan backend?&lt;/p&gt; &lt;p&gt;I'd love some llama-sweep-bench results if anyone has Strix Halo, 7900XTX, etc. Also curious if it is any better for mac (or do they mostly use mlx?).&lt;/p&gt; &lt;p&gt;Check it out if you're interested, compatible with mainline llama.cpp/ik_llama.cpp, and the usual downstream projects as well:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/ubergarm/Qwen3.5-35B-A3B-GGUF?show_file_info=Qwen3.5-35B-A3B-Q4_0.gguf"&gt;https://huggingface.co/ubergarm/Qwen3.5-35B-A3B-GGUF?show_file_info=Qwen3.5-35B-A3B-Q4_0.gguf&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VoidAlchemy"&gt; /u/VoidAlchemy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bkw8ps1qwplg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1resggh/best_qwen3535ba3b_gguf_for_24gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1resggh/best_qwen3535ba3b_gguf_for_24gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-25T22:42:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1rf38xe</id>
    <title>Do not download Qwen 3.5 Unsloth GGUF until bug is fixed</title>
    <updated>2026-02-26T06:55:41+00:00</updated>
    <author>
      <name>/u/SunTrainAi</name>
      <uri>https://old.reddit.com/user/SunTrainAi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Seems that everyone is testing Qwen3.5 now, often with quants from our good friends and heros Unsloth. Another hero, Ubergarm, found some issues with UD_Q4_K_XL but later Unsloth admitted all of the current quants are messed up. &lt;a href="https://huggingface.co/unsloth/Qwen3.5-35B-A3B-GGUF/discussions/5#699fbf23b7b03fe27460a880"&gt;https://huggingface.co/unsloth/Qwen3.5-35B-A3B-GGUF/discussions/5#699fbf23b7b03fe27460a880&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So please stop downloading the quants and wait for a fixed version. Kudos for the friendly cooperation in the community&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SunTrainAi"&gt; /u/SunTrainAi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rf38xe/do_not_download_qwen_35_unsloth_gguf_until_bug_is/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rf38xe/do_not_download_qwen_35_unsloth_gguf_until_bug_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rf38xe/do_not_download_qwen_35_unsloth_gguf_until_bug_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-26T06:55:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1ret353</id>
    <title>Qwen/Qwen3.5-35B-A3B creates FlappyBird</title>
    <updated>2026-02-25T23:05:54+00:00</updated>
    <author>
      <name>/u/Medium_Chemist_4032</name>
      <uri>https://old.reddit.com/user/Medium_Chemist_4032</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you are wondering, as I have for a long time, do locally hostable models work for general coding? They really can work impressively well for some usecases. There's been some impressive things done by the model during making of this simple app.&lt;/p&gt; &lt;p&gt;Spent two hours. Generated with Qwen/Qwen3.5-35B-A3B. Used Roo in VSCode.&lt;/p&gt; &lt;p&gt;Started out by vaguely asking for a flappybird clone in html, css and typescript and to initialize the project with vite.&lt;/p&gt; &lt;p&gt;It looked impressive enough after first task, that I started asking for extra features:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Music and sound &lt;/li&gt; &lt;/ol&gt; &lt;blockquote&gt; &lt;p&gt;Uses Web Audio API to generate sounds programmatically (no external audio files needed)&lt;/p&gt; &lt;/blockquote&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Scrollable background mountains. This request resulted in visual glitches, but after a bit of guidance, it was fixed to a proper parallaxed mountain&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Background flock of birds. A bit back and forth, but managed to understand my general pointers (they fly off screen, they are smeared from top to bottom, make them fly from right to left) and ended up in a great state.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Sound and music settings panel. This was one shotted.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Medium_Chemist_4032"&gt; /u/Medium_Chemist_4032 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/c3lr7ou30qlg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ret353/qwenqwen3535ba3b_creates_flappybird/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ret353/qwenqwen3535ba3b_creates_flappybird/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-25T23:05:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1rf2ulo</id>
    <title>Qwen3.5 122B in 72GB VRAM (3x3090) is the best model available at this time — also it nails the “car wash test”</title>
    <updated>2026-02-26T06:32:25+00:00</updated>
    <author>
      <name>/u/liviuberechet</name>
      <uri>https://old.reddit.com/user/liviuberechet</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rf2ulo/qwen35_122b_in_72gb_vram_3x3090_is_the_best_model/"&gt; &lt;img alt="Qwen3.5 122B in 72GB VRAM (3x3090) is the best model available at this time — also it nails the “car wash test”" src="https://preview.redd.it/f624mg43aslg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4294c910c299aa0b5b65f5e5c0177aa28a215e65" title="Qwen3.5 122B in 72GB VRAM (3x3090) is the best model available at this time — also it nails the “car wash test”" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am absolutely loving Qwen3.5 122B!&lt;/p&gt; &lt;p&gt;It’s the best model I can run on my 72GB VRAM setup, fully loaded on GPU including context.&lt;/p&gt; &lt;p&gt;Very good speed at 25 tok/s.&lt;/p&gt; &lt;p&gt;Fiddled a bit with the settings to get it to work properly. If you are experiencing endless “but wait” loops, this is what worked for me:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Thinking mode on &lt;/li&gt; &lt;li&gt;Temperature 0.6 &lt;/li&gt; &lt;li&gt;K Sampling 20 &lt;/li&gt; &lt;li&gt;Top P sampling 0.8 &lt;/li&gt; &lt;li&gt;Min P sampling 0 &lt;/li&gt; &lt;li&gt;Repeat penalty 1.3&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Running it in Q3_K it’s a bit slower than GLM Air (30 t/s in IQ4_NL) and GPT-OSS-120B (30-38 t/s in MXFP4), but because it has a smaller footprint in Q3 I am able to push the context to 120k which is great!&lt;/p&gt; &lt;p&gt;I tried both MXFP4 and IQ4_XS, but they are too close to 70GB when loaded, forcing me to offload 2-3 layers to RAM or context in RAM — dropping to only 6-8 tok/s.&lt;/p&gt; &lt;p&gt;Saw on unsloth website that Q3_K_XL might actually perform on par with the 4bit ones, and I can confirm so far it’s been amazing!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/liviuberechet"&gt; /u/liviuberechet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/f624mg43aslg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rf2ulo/qwen35_122b_in_72gb_vram_3x3090_is_the_best_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rf2ulo/qwen35_122b_in_72gb_vram_3x3090_is_the_best_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-26T06:32:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7bsfd</id>
    <title>Best Audio Models - Feb 2026</title>
    <updated>2026-02-17T17:01:37+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;They've been a ton of audio models released of late, the most notable perhaps being Qwen3 TTS. So its time for another &lt;strong&gt;Best Audio Models&lt;/strong&gt; megathread&lt;/p&gt; &lt;p&gt;Share what your favorite ASR, TTS, STT, Text to Music models are right now &lt;strong&gt;and why.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Given the the amount of ambiguity and subjectivity in rating/testing these models, please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks etc. Closed models like Elevenlabs v3 seem to continue to be a few levels above open models especially for production use cases with long lengths/stability requirements, so comparisons, especially empirical ones are welcome.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please use the top level comments to thread your responses.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T17:01:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8snay</id>
    <title>AMA with StepFun AI - Ask Us Anything</title>
    <updated>2026-02-19T07:15:35+00:00</updated>
    <author>
      <name>/u/StepFun_ai</name>
      <uri>https://old.reddit.com/user/StepFun_ai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt; &lt;img alt="AMA with StepFun AI - Ask Us Anything" src="https://external-preview.redd.it/bqLigXK_A-XO-amDfdyeQenKCxR18nhChWpDnQXYmr4.png?width=140&amp;amp;height=75&amp;amp;auto=webp&amp;amp;s=72c79bc533c167d8f77ecf629e3b7af53ee1e6b6" title="AMA with StepFun AI - Ask Us Anything" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/w8274fg1jekg1.png?width=1785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fadbd0ec26a56e60900f9ed667ae808217d70cf2"&gt;https://preview.redd.it/w8274fg1jekg1.png?width=1785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fadbd0ec26a56e60900f9ed667ae808217d70cf2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; !&lt;/p&gt; &lt;p&gt;We are &lt;strong&gt;StepFun&lt;/strong&gt;, the team behind the &lt;strong&gt;Step&lt;/strong&gt; family models, including &lt;a href="https://huggingface.co/collections/stepfun-ai/step-35-flash"&gt;&lt;strong&gt;Step 3.5 Flash&lt;/strong&gt;&lt;/a&gt; and &lt;a href="https://huggingface.co/collections/stepfun-ai/step3-vl-10b"&gt;&lt;strong&gt;Step-3-VL-10B&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;We are super excited to host our first AMA tomorrow in this community. Our participants include CEO, CTO, Chief Scientist, LLM Researchers.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Participants&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Ok_Reach_5122"&gt;u/Ok_Reach_5122&lt;/a&gt; (Co-founder &amp;amp; CEO of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/bobzhuyb"&gt;u/bobzhuyb&lt;/a&gt; (Co-founder &amp;amp; CTO of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/user/Lost-Nectarine1016"&gt;u/Lost-Nectarine1016&lt;/a&gt; (Co-founder &amp;amp; Chief Scientist of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Elegant-Sale-1328"&gt;u/Elegant-Sale-1328&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/SavingsConclusion298"&gt;u/SavingsConclusion298&lt;/a&gt; (Post-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Spirited_Spirit3387"&gt;u/Spirited_Spirit3387&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/These-Nothing-8564/"&gt;u/These-Nothing-8564&lt;/a&gt; (Technical Project Manager)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Either-Beyond-7395"&gt;u/Either-Beyond-7395&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Human_Ad_162"&gt;u/Human_Ad_162&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Icy_Dare_3866"&gt;u/Icy_Dare_3866&lt;/a&gt; (Post-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Big-Employee5595"&gt;u/Big-Employee5595&lt;/a&gt; (Agent Algorithms Lead&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run 8 - 11 AM PST, Feburary 19th. The StepFun team will monitor and answer questions over the 24 hours after the live session.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StepFun_ai"&gt; /u/StepFun_ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T07:15:35+00:00</published>
  </entry>
</feed>
