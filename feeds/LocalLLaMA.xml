<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-27T23:39:38+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1rg0487</id>
    <title>System prompt for Qwen3.5 (27B/35BA3B) to reduce overthinking?</title>
    <updated>2026-02-27T07:25:03+00:00</updated>
    <author>
      <name>/u/thigger</name>
      <uri>https://old.reddit.com/user/thigger</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone found a good way to persuade Qwen3.5 (27B/35BA3B) to keep their reasoning budget sensible? They seem to be really good models but particularly the MoE goes absolutely insane second-guessing itself and sometimes even looping.&lt;/p&gt; &lt;p&gt;I'm outputting JSON so not keen on too much repetition penalty, so have been trying out system prompts - currently telling it:&lt;/p&gt; &lt;p&gt;&amp;quot;You are a concise, efficient, decisive assistant. Think in 2-3 short blocks without repetition or second-guessing, and then output your answer&amp;quot;&lt;/p&gt; &lt;p&gt;This has made things very slightly better but not much. Any tips?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thigger"&gt; /u/thigger &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rg0487/system_prompt_for_qwen35_27b35ba3b_to_reduce/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rg0487/system_prompt_for_qwen35_27b35ba3b_to_reduce/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rg0487/system_prompt_for_qwen35_27b35ba3b_to_reduce/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-27T07:25:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1rg0ir2</id>
    <title>After using local models for one month, I learned more than in two years with cloud models</title>
    <updated>2026-02-27T07:49:43+00:00</updated>
    <author>
      <name>/u/Ambitious-Sense-7773</name>
      <uri>https://old.reddit.com/user/Ambitious-Sense-7773</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I started with qwen2.5 and first had to figure out why getting context overflow. Had to raise context, tune temperature, top-K and top-P. Then got qwen3(mlx) and was blown away by the speed of mixture of experts. Learned about KV cache linear growth, why i need to eject the model from time to time. Also learned that replaying old prompt to fresh LM results into same state each time.&lt;/p&gt; &lt;p&gt;Now qwen3.5 doesnt seem to increase mem usage, event though i disabled auto-reset from lm studio.&lt;/p&gt; &lt;p&gt;Pondering if I should set up a shared solution for other people, but not sure would the KV cache eat all memory.&lt;/p&gt; &lt;p&gt;I just wish there was a lm studio resource monitor, telling token flow, KV cache, activated experts and so. &lt;/p&gt; &lt;p&gt;That being said, my knowledge is basically constrained to basic transformer architecture without MoE and whatnot optimizations. Would be interested in LoRa training but dont know if I got the time. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ambitious-Sense-7773"&gt; /u/Ambitious-Sense-7773 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rg0ir2/after_using_local_models_for_one_month_i_learned/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rg0ir2/after_using_local_models_for_one_month_i_learned/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rg0ir2/after_using_local_models_for_one_month_i_learned/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-27T07:49:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1rgiait</id>
    <title>Switched to Qwen3.5-122B-A10B-i1-GGUF</title>
    <updated>2026-02-27T20:42:37+00:00</updated>
    <author>
      <name>/u/NaiRogers</name>
      <uri>https://old.reddit.com/user/NaiRogers</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Switched to this mradermacher/Qwen3.5-122B-A10B-i1-GGUF:Q4_K_S today on my 6000 Pro from mradermacher/MiniMax-M2.5-REAP-139B-A10B-i1-GGUF:Q4_K_S so far it‚Äôs better, main reason to switch was to get more context. The full 262k tokens fit on a 6000 Pro vs only about 65k with the Minimax quant. It‚Äôs fast also. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NaiRogers"&gt; /u/NaiRogers &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rgiait/switched_to_qwen35122ba10bi1gguf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rgiait/switched_to_qwen35122ba10bi1gguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rgiait/switched_to_qwen35122ba10bi1gguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-27T20:42:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1rgdo3s</id>
    <title>Fix for ROCm performance regression for Strix Halo landed in TheRock 7.2 release branch üöÄ</title>
    <updated>2026-02-27T17:50:30+00:00</updated>
    <author>
      <name>/u/spaceman_</name>
      <uri>https://old.reddit.com/user/spaceman_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was investigating the odd performance deficit that newer (7.X) ROCm versions seem to suffer compared to the old 6.4 versions.&lt;/p&gt; &lt;p&gt;This was especially odd on Strix Halo since that wasn't even officially supported in the 6.X branches.&lt;/p&gt; &lt;p&gt;While reading and searching, I discovered this bug issue and a recent comment mentioning the fix has landed in the release branch: &lt;a href="https://github.com/ROCm/rocm-systems/issues/2865#issuecomment-3968555545"&gt;https://github.com/ROCm/rocm-systems/issues/2865#issuecomment-3968555545&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hopefully that means we'll soon have even better performance on Strix Halo!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/spaceman_"&gt; /u/spaceman_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rgdo3s/fix_for_rocm_performance_regression_for_strix/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rgdo3s/fix_for_rocm_performance_regression_for_strix/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rgdo3s/fix_for_rocm_performance_regression_for_strix/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-27T17:50:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1rfp6bk</id>
    <title>why is openclaw even this popular?</title>
    <updated>2026-02-26T22:50:15+00:00</updated>
    <author>
      <name>/u/Crazyscientist1024</name>
      <uri>https://old.reddit.com/user/Crazyscientist1024</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;recently i haven't been following up on the latest AI dramas and just came back from a vacation. Did some looking around and found out that OpenClaw just blew up, looked into it but I didn't find anything significantly special. It just seems to be like a wrapper that has a huge amounts of pre-programmed function calls / skills / whatever built into it.&lt;/p&gt; &lt;p&gt;Am I missing something? How is this blowing up? Respectfully, even for newbie programmers, they can probably simply vibe code a way more lightweight tool themselves in a day dedicated for their task at hand.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Crazyscientist1024"&gt; /u/Crazyscientist1024 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rfp6bk/why_is_openclaw_even_this_popular/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rfp6bk/why_is_openclaw_even_this_popular/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rfp6bk/why_is_openclaw_even_this_popular/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-26T22:50:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1rg41ss</id>
    <title>Qwen3.5 27B vs Devstral Small 2 - Next.js &amp; Solidity (Hardhat)</title>
    <updated>2026-02-27T11:19:24+00:00</updated>
    <author>
      <name>/u/Holiday_Purpose_3166</name>
      <uri>https://old.reddit.com/user/Holiday_Purpose_3166</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rg41ss/qwen35_27b_vs_devstral_small_2_nextjs_solidity/"&gt; &lt;img alt="Qwen3.5 27B vs Devstral Small 2 - Next.js &amp;amp; Solidity (Hardhat)" src="https://preview.redd.it/wn89u3hyo1mg1.png?width=140&amp;amp;height=84&amp;amp;auto=webp&amp;amp;s=e7dfcacc10647a458b6033b3bb723477ddda4ff5" title="Qwen3.5 27B vs Devstral Small 2 - Next.js &amp;amp; Solidity (Hardhat)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Greetings,&lt;/p&gt; &lt;p&gt;I was excited to test the 27B and 35BA3B variants, to see whether they were superior to my daily driver, Devstral Small 2.&lt;/p&gt; &lt;p&gt;Had issues for the reported UD-Q4_K_XL. After over-examining across PPL and KLD, I went with mradermacher as I followed their card for quality.&lt;/p&gt; &lt;p&gt;Anecdotally, on the work done in some of my repos, Qwen3.5 27B was superior in quality - planning, coding and compiling to no error, and fixing few snags when needed.&lt;/p&gt; &lt;p&gt;The 27B documentation write-ups can be super extensive on a Q6 quant, where Devstral Small 2 can produce from Q8. It's nice if you like verbose documents and has capability to write/edit at length.&lt;/p&gt; &lt;p&gt;Qwen3.5 35BA3B is simpler in planning but was not shy on execution, as it was able to refactor a single +900 LoC file into 35 different parts - it was excessive but I had requested it to see how complex it could handle.&lt;/p&gt; &lt;p&gt;After several attempts, the way it performed the refactor was entirely different from other models I had used in the past - it positioned main elements titles and components in most odd files. These we informal trials.&lt;/p&gt; &lt;p&gt;I can say Qwen3.5 35BA3B can over-engineer if not guided properly, but I did not go far with it, as I found the issue stated earlier a nuisance, for something that could've been simple from a SWE perspective. I might have been unfair and cherry picked too fast, due to time constraints at the time.&lt;/p&gt; &lt;p&gt;I found the pick between Qwen3.5 27B and Devstral Small 2 a hard choice. I am used to Mistral's efficiency and repo work capability, but couldn't settle my finger if Qwen was superior as the executions were pretty much identical and token spending.&lt;/p&gt; &lt;p&gt;To my surprise, Artificial Analysis put Qwen's 27B at a level similar to Deepseek V3.2 and suspiciously close of Sonnet 4.5. &lt;em&gt;Trust but verify.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;So, to settle my mind on the early agentic coding department, I created 78 agentic challenges in one of my prod repos, to check which model came out the best, in one of my Next.js and Solidity repo.&lt;/p&gt; &lt;h1&gt;Stack&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Fedora 43&lt;/li&gt; &lt;li&gt;llama.cpp b8149 | docker `nvidia/cuda:13.1.0-devel-ubuntu24.04`&lt;/li&gt; &lt;li&gt;RTX 5090 | stock | driver 580.119.02&lt;/li&gt; &lt;li&gt;Ryzen 9 9950X | 96GB DDR5 6000&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Llama.cpp Build Flags&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;RUN set -eux; \ echo &amp;quot;CMAKE_CUDA_ARCHITECTURES=${CMAKE_CUDA_ARCHITECTURES}&amp;quot;; \ rm -rf build; \ cmake -S . -B build -G Ninja \ -DCMAKE_BUILD_TYPE=Release \ -DCMAKE_C_COMPILER=${CC} \ -DCMAKE_CXX_COMPILER=${CXX} \ -DCMAKE_LINKER=${LD} \ -DGGML_NATIVE=ON \ -DGGML_LTO=${GGML_LTO} \ -DGGML_OPENMP=ON \ -DGGML_BLAS=ON \ -DGGML_BLAS_VENDOR=OpenBLAS \ -DGGML_CUDA=ON \ -DCMAKE_CUDA_ARCHITECTURES=&amp;quot;${CMAKE_CUDA_ARCHITECTURES}&amp;quot; \ -DGGML_CUDA_GRAPHS=ON \ -DGGML_CUDA_FA=ON \ -DGGML_CUDA_FA_ALL_QUANTS=${GGML_CUDA_FA_ALL_QUANTS} \ -DGGML_CUDA_COMPRESSION_MODE=${GGML_CUDA_COMPRESSION_MODE} \ -DLLAMA_BUILD_SERVER=ON \ -DLLAMA_BUILD_EXAMPLES=OFF; \ cmake --build build -j&amp;quot;$(nproc)&amp;quot;; \ cmake --install build --prefix /opt/llama &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Quants &amp;amp; Flags&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;mradermacher | Qwen3.5 27B i1-Q6_K | Model+Context 29.3GB&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt; - -t - &amp;quot;8&amp;quot; - --numa - numactl - --jinja - --temp - &amp;quot;0.6&amp;quot; - --top-p - &amp;quot;0.95&amp;quot; - --top-k - &amp;quot;20&amp;quot; - --min-p - &amp;quot;0.0&amp;quot; - --presence-penalty - &amp;quot;0.0&amp;quot; - --repeat-penalty - &amp;quot;1.0&amp;quot; - -b - &amp;quot;512&amp;quot; - -ub - &amp;quot;512&amp;quot; - --no-mmap - -c - &amp;quot;111000&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;unsloth | Devstral-Small-2-24B-Instruct-2512-Q6_K | Model+Context 29.9GB&lt;/strong&gt; ADDED*&lt;/p&gt; &lt;pre&gt;&lt;code&gt; - -t - &amp;quot;8&amp;quot; - --chat-template-file - /models/devstral-fix.jinja # custom chat template - --temp - &amp;quot;0.15&amp;quot; - --min-p - &amp;quot;0.01&amp;quot; - --numa - numactl - -b - &amp;quot;512&amp;quot; - -ub - &amp;quot;512&amp;quot; - --no-mmap - -c - &amp;quot;71125&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;byteshape | Devstral Small 2 24B IQ4_XS-4.04bpw | Model+Context 28.9GB&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt; - -t - &amp;quot;8&amp;quot; - --chat-template-file - /models/devstral-fix.jinja # custom chat template - --temp - &amp;quot;0.15&amp;quot; - --min-p - &amp;quot;0.01&amp;quot; - --numa - numactl - -ctk - q8_0 - -ctv - q8_0 - -b - &amp;quot;512&amp;quot; - -ub - &amp;quot;512&amp;quot; - --no-mmap - -c - &amp;quot;200000&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;I have compiled some of the information below with an LLM for simplicity:&lt;/em&gt;&lt;/p&gt; &lt;h1&gt;The Benchmark&lt;/h1&gt; &lt;p&gt;Executed a single suite with 78 tasks (39 Next.js + 39 Hardhat) via Opencode. Each model ran the whole suite in a single pass - executing each task separately as new session, to avoid context compressions and context blow.&lt;/p&gt; &lt;h1&gt;Scoring rubric (per task, 0-100)&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Correctness (0 or 60 points)&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;60 if the patch fully satisfies task checks.&lt;/li&gt; &lt;li&gt;0 if it fails.&lt;/li&gt; &lt;li&gt;This is binary to reward complete fixes, not partial progress.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Compatibility (0-20 points)&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Measures whether the patch preserves required integration/contract expectations for that task.&lt;/li&gt; &lt;li&gt;Usually task-specific checks.&lt;/li&gt; &lt;li&gt;Full compatibility = 20 | n partial = lower | broken/missing = 0&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Scope Discipline (0-20 points)&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Measures edit hygiene: &lt;em&gt;did the model change only relevant files?&lt;/em&gt;&lt;/li&gt; &lt;li&gt;20 if changes stay in intended scope.&lt;/li&gt; &lt;li&gt;Penalised as unrelated edits increase.&lt;/li&gt; &lt;li&gt;Extra penalty if the model creates a commit during benchmarking.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Why this design works&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Total score = Correctness + Compatibility + Scope Discipline (max 100)&lt;/em&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;60% on correctness keeps &lt;em&gt;‚Äúworks vs doesn‚Äôt work‚Äù&lt;/em&gt; as the primary signal.&lt;/li&gt; &lt;li&gt;20% compatibility penalises fixes that break expected interfaces/behaviour.&lt;/li&gt; &lt;li&gt;20% scope discipline penalises noisy, risky patching and rewards precise edits.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Results&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;mradermacher | Qwen3.5-27B.i1-Q6_K.gguf&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt; 4134 score total | 53.00 avg score per task | 48/78 pass (61.54%) - Prompt Processing Speed: - Mean per request: 1326.80 tok/s - Token-weighted: 1596.20 tok/s - Token Generation Speed: - Mean per-request: 45.24 tok/s - Token-weighted: 45.03 tok/s &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;unsloth | Devstral-Small-2-24B-Instruct-2512-Q6_K.gguf&lt;/strong&gt; ADDED*&lt;/p&gt; &lt;pre&gt;&lt;code&gt;2778 score total | 34.62 avg score per task | 27/78 pass (34.62%) - Prompt processing: - Mean: 2015.13 tok/s - Median: 2193.43 tok/s - Token-weighted: 2458.97 tok/s - Token generation: - Mean: 53.29 tok/s - Median: 54.05 tok/s - Token-weighted: 48.01 tok/s &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;byteshape | Devstral-Small-2-24B-Instruct-2512-IQ4_XS-4.04bpw.gguf&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt; 3158 total score | 40.49 avg score per task | 33/78 pass (42.31%) - Prompt Processing Speed: - Mean per request: 2777.02 toks/s - Token-weighted: 4200.64 toks/s - Token Generation Speed: - Mean per-request: 90.49 tok/s - Token-weighted: 89.31 tok/s &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;- Devstral is &lt;strong&gt;not&lt;/strong&gt; an IQ4_XS quant due HF naming convention compatibility for exotic gguf types. The quant is designated as above &lt;strong&gt;4.04bpw&lt;/strong&gt; by &lt;a href="https://huggingface.co/byteshape/Devstral-Small-2-24B-Instruct-2512-GGUF"&gt;Byteshape&lt;/a&gt; which follows a Q8_0 quality equivalent.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Stack Score Split&lt;/strong&gt; ADDED*&lt;/p&gt; &lt;pre&gt;&lt;code&gt; - Next.js avg score: 1. byteshape Devstral-Small-2-24B-Instruct-2512-IQ4_XS-4.04bpw (64.82%) 2. unsloth Devstral-Small-2-24B-Instruct-2512-Q6_K (58.26%) 3. mradermacher Qwen3.5-27B.i1-Q6_K (56.82%) - Hardhat avg score: 1. mradermacher Qwen3.5-27B.i1-Q6_K (49.18%) 2. byteshape Devstral-Small-2-24B-Instruct-2512-IQ4_XS-4.04bpw (16.15%) 3. unsloth Devstral-Small-2-24B-Instruct-2512-Q6_K (12.97%) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;The takeaway&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Devstral from Byteshape was stronger on Next.js-only tasks, but Qwen was much more robust on Hardhat/contract engineering, which decided the overall suite winner.&lt;/p&gt; &lt;p&gt;This sums what I've experienced when attempting using Devstral for Solidity even with the previous generation. I am impressed Qwen was able to work with Solidity, so it's something I could explore in near future when I need to refactor contracts.&lt;/p&gt; &lt;p&gt;Since most of my work surrounds Rust and Next.js I might stick with Devstral Small 2 for repo work, which also it's faster and can use 200k context window quite comfortably. I can go closer to 220-230k but its starts cramming VRAM and glitching screens.&lt;/p&gt; &lt;p&gt;I would probably include some Rust benchmarks as well in my other repos, as Devstral Small 2 is strong there (GLM 4.7 Flash cratered) if I can get some time.&lt;/p&gt; &lt;p&gt;I still have to try Qwen3.5 27B in other areas such as general assistant, etc.&lt;/p&gt; &lt;p&gt;I hope that helps anyone.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;EDIT:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;*ADDED suite results from Unsloth Devstral Small 24B Q6_K&lt;/li&gt; &lt;li&gt;Score and speed charts&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/wn89u3hyo1mg1.png?width=1600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f7bae8ba233eba3bde7aee485d7e423cf68f0b7d"&gt;https://preview.redd.it/wn89u3hyo1mg1.png?width=1600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f7bae8ba233eba3bde7aee485d7e423cf68f0b7d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8cl1lbdhp1mg1.png?width=2040&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=155aca24f3a7f2785555cb4613313d978f3dd0d4"&gt;https://preview.redd.it/8cl1lbdhp1mg1.png?width=2040&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=155aca24f3a7f2785555cb4613313d978f3dd0d4&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Holiday_Purpose_3166"&gt; /u/Holiday_Purpose_3166 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rg41ss/qwen35_27b_vs_devstral_small_2_nextjs_solidity/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rg41ss/qwen35_27b_vs_devstral_small_2_nextjs_solidity/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rg41ss/qwen35_27b_vs_devstral_small_2_nextjs_solidity/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-27T11:19:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1rgi0ej</id>
    <title>Qwen3.5 Unsloth GGUFs Update!</title>
    <updated>2026-02-27T20:31:41+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rgi0ej/qwen35_unsloth_ggufs_update/"&gt; &lt;img alt="Qwen3.5 Unsloth GGUFs Update!" src="https://external-preview.redd.it/5xtzvpxdx2mg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=862dc616a7026f7da474b808acfe28b96e84a5a2" title="Qwen3.5 Unsloth GGUFs Update!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/unsloth/comments/1rgemmh/qwen35_unsloth_ggufs_update/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rgi0ej/qwen35_unsloth_ggufs_update/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rgi0ej/qwen35_unsloth_ggufs_update/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-27T20:31:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1rg4apu</id>
    <title>Qwen 3.5 Architecture Analysis: Parameter Distribution in the Dense 27B vs. 122B/35B MoE Models</title>
    <updated>2026-02-27T11:33:21+00:00</updated>
    <author>
      <name>/u/Luca3700</name>
      <uri>https://old.reddit.com/user/Luca3700</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rg4apu/qwen_35_architecture_analysis_parameter/"&gt; &lt;img alt="Qwen 3.5 Architecture Analysis: Parameter Distribution in the Dense 27B vs. 122B/35B MoE Models" src="https://preview.redd.it/gnzye3xgw0mg1.jpg?width=140&amp;amp;height=59&amp;amp;auto=webp&amp;amp;s=0b22d57c5ebae2ec043a1a48c7da75344ed162db" title="Qwen 3.5 Architecture Analysis: Parameter Distribution in the Dense 27B vs. 122B/35B MoE Models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Yesterday, I wrote a &lt;a href="https://www.reddit.com/r/LocalLLaMA/s/EdTcLCLtTD"&gt;comment on this post&lt;/a&gt; on why, in my opinion, the dense model Qwen 3.5 27B can achieve good results in benchmarks, by providing an architectural analysis. And today I'm expanding my thoughts in this post.&lt;/p&gt; &lt;h1&gt;Intro&lt;/h1&gt; &lt;p&gt;A few days ago, Qwen released three new models: two &lt;strong&gt;Mixture of Experts models&lt;/strong&gt; (122B A10 and 35B A3) and a &lt;strong&gt;dense model&lt;/strong&gt; (with 27B parameters).&lt;/p&gt; &lt;p&gt;All of them share a similar architecture, that interleaves &lt;strong&gt;three Gated DeltaNet&lt;/strong&gt; layers with a &lt;strong&gt;Gated Attention&lt;/strong&gt; Layer, each of them followed by their respective Feed Forward Network.&lt;/p&gt; &lt;p&gt;Before going in detail in the analysis, let's summarize the three architectures with this picture (taken from the models overview on huggingface).&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/gnzye3xgw0mg1.jpg?width=2125&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e0fe6c74b37c8f212024d7f1398784289c020e09"&gt;Models overview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: the hidden layout of the 122B model appears to be incorrect in the picture, because it should be &lt;em&gt;12x&lt;/em&gt; (3x ... -&amp;gt; 1x ...) and not &lt;em&gt;16x&lt;/em&gt;, because the number of layers is 48 (as stated in the config.json file as well)&lt;/p&gt; &lt;h1&gt;Architecture Analysis - Feed Forward Network&lt;/h1&gt; &lt;p&gt;Even though the blueprint is similar, the parameter distribution is different, and the &lt;strong&gt;main divergence&lt;/strong&gt; between the MoE models and the 27B dense model is that the former use &lt;strong&gt;more parameters in the experts&lt;/strong&gt; of the Feed Forward Network. In contrast, the 27B model (due to the use of a dense Feed Forward Network that uses less parameters than the MoE counterpart) is able to &lt;strong&gt;allocate more of them to other parts of the network&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;If we want to quantify the amount of parameters used in the FFN layers, we could say that for the MoE models is &lt;/p&gt; &lt;p&gt;&lt;code&gt;2 x hidden_dim x expert_int_dim x num_experts x num_layers&lt;/code&gt;&lt;/p&gt; &lt;p&gt;instead for the dense model is&lt;/p&gt; &lt;p&gt;&lt;code&gt;2 x hidden_dim x int_dim x num_layers&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Therefore, we obtain:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;122B MoE model: 77,3 B (active 2,7) -&amp;gt; &lt;strong&gt;63% (2,2%)&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;35B MoE model: 21,5 B (active 0,8) -&amp;gt; &lt;strong&gt;61% (2,3%)&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;27B dense model: 9,1 B -&amp;gt; &lt;strong&gt;34%&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Where these parameters go in the dense model?&lt;/h1&gt; &lt;p&gt;The dense model is able to use, in percentage, half of the parameters in the FFN layers, and can spread them to other parts of the architecture (the following points correspond to the numbers on the arrows in the images): &lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;the dense model is deeper&lt;/strong&gt;, it has 64 layers (instead the MoE models have respectively 48 and 40), and this should allow the model to have more depth for reasoning tasks&lt;/li&gt; &lt;li&gt;&lt;strong&gt;it uses 4 keys and 4 values in the gated attention layers&lt;/strong&gt; (compared to only 2 than the MoE architectures), and it could allow the attention layer to capture more nuances&lt;/li&gt; &lt;li&gt;&lt;strong&gt;it uses more heads in the Gated DeltaNet layers&lt;/strong&gt; compared to the 35B counterpart.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Another point to take into account is the number of active parameters. Although the dense model has a smaller number of parameters in the FFN, it uses more of them actively, allowing it to use &lt;strong&gt;more computational power per token&lt;/strong&gt;.&lt;/p&gt; &lt;h1&gt;Conclusion&lt;/h1&gt; &lt;p&gt;Therefore, the 27B dense model can be seen, under the points of view listed above, as a &lt;strong&gt;deeper and wider&lt;/strong&gt; network than the 35B MoE model, and in some respects also than the 122B model. &lt;/p&gt; &lt;p&gt;I think that all these differences allow the dense model to have comparable performance to its bigger brother, even given the &lt;strong&gt;4,5x smaller parameter footprint&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Thank you for reading until here!&lt;/p&gt; &lt;p&gt;What do you think about this analysis? &lt;/p&gt; &lt;p&gt;Note: LLM used only for grammar checks and title suggestion. Post inspired by the &lt;a href="/u/seraschka"&gt;u/seraschka&lt;/a&gt; architectures deep dive.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Luca3700"&gt; /u/Luca3700 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rg4apu/qwen_35_architecture_analysis_parameter/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rg4apu/qwen_35_architecture_analysis_parameter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rg4apu/qwen_35_architecture_analysis_parameter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-27T11:33:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1rg8gkx</id>
    <title>Qwen3.5 35B a3b - 45 t/s 128K ctx on single 16GB 5060</title>
    <updated>2026-02-27T14:40:40+00:00</updated>
    <author>
      <name>/u/Gray_wolf_2904</name>
      <uri>https://old.reddit.com/user/Gray_wolf_2904</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Prefill speeds : 700+ tok/sec&lt;/p&gt; &lt;p&gt;Generation speed stays above 30 even as contact fills upto 120/128k. &lt;/p&gt; &lt;p&gt;Hardware setup: noting is overlocked. &lt;/p&gt; &lt;p&gt;I9-9900K, 64GB DDR4 RAM. &lt;/p&gt; &lt;p&gt;5060 ti 16GB &lt;/p&gt; &lt;p&gt;Ubuntu 24&lt;/p&gt; &lt;p&gt;The model is able to function as my primary programmer. Mind blowing performance when compared to many high end paid cloud models. &lt;/p&gt; &lt;p&gt;Amazingly, very few layers have to be on gpu to maintain 30+ tokens per second even at filled context. Have also seen consistent 45 t/s at smaller context sizes and 1000+ tokens per second in prompt processing (prefill). &lt;/p&gt; &lt;p&gt;My hardware is anything but modern or extraordinary. And this model has made it completely useable in production work environments. Bravo!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Gray_wolf_2904"&gt; /u/Gray_wolf_2904 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rg8gkx/qwen35_35b_a3b_45_ts_128k_ctx_on_single_16gb_5060/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rg8gkx/qwen35_35b_a3b_45_ts_128k_ctx_on_single_16gb_5060/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rg8gkx/qwen35_35b_a3b_45_ts_128k_ctx_on_single_16gb_5060/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-27T14:40:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1rgek4m</id>
    <title>What are your expectations for the ‚ÄúSmall‚Äù series of the Qwen3.5 family?</title>
    <updated>2026-02-27T18:22:58+00:00</updated>
    <author>
      <name>/u/Adventurous-Paper566</name>
      <uri>https://old.reddit.com/user/Adventurous-Paper566</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After the impressive 27B model, it‚Äôs natural to expect Qwen to surprise us again. &lt;/p&gt; &lt;p&gt;We already know a 9B and a successor at 4B are planned. &lt;/p&gt; &lt;p&gt;But what do you hope to achieve with this new generation of lightweight models?&lt;/p&gt; &lt;p&gt;I hope the 9B model will match the performance of a 30B A3B, that would be incredible.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Adventurous-Paper566"&gt; /u/Adventurous-Paper566 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rgek4m/what_are_your_expectations_for_the_small_series/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rgek4m/what_are_your_expectations_for_the_small_series/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rgek4m/what_are_your_expectations_for_the_small_series/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-27T18:22:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1rgkxy3</id>
    <title>List of models that you might have missed</title>
    <updated>2026-02-27T22:24:40+00:00</updated>
    <author>
      <name>/u/ThisGonBHard</name>
      <uri>https://old.reddit.com/user/ThisGonBHard</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys,&lt;/p&gt; &lt;p&gt;So, today I found out there are a lot of LLMs, that I have never heard of before until now. I kinda want to test them, especially for creative writing and other tasks, and I figured I am probably not the only person who missed.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/XiaomiMiMo/MiMo-V2-Flash"&gt;Xiamo MiMo V2 Flash&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/XiaomiMiMo/MiMo-Audio-7B-Instruct"&gt;Xiaomi MiMo Audio&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/rednote-hilab/dotsllm1"&gt;Rednote Dots1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/meituan-longcat/LongCat-Flash-Lite"&gt;Meituan LongCat Flash Lite&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I mostly credit Bycloud for mentioning them in a video, for else I would have missed them releasing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThisGonBHard"&gt; /u/ThisGonBHard &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rgkxy3/list_of_models_that_you_might_have_missed/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rgkxy3/list_of_models_that_you_might_have_missed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rgkxy3/list_of_models_that_you_might_have_missed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-27T22:24:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1rghfqj</id>
    <title>February is almost over, are you satisfied? Upcoming models soon?</title>
    <updated>2026-02-27T20:09:48+00:00</updated>
    <author>
      <name>/u/pmttyji</name>
      <uri>https://old.reddit.com/user/pmttyji</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Some mentioned that Feb is loaded with so much model droppings. And some mentioned about CNY thing. I guess March &amp;amp; April are possibly loaded with more model droppings. I'm sure Local folks are happy with Qwen series, GLM5, Step Flash, Minimax2.5.&lt;/p&gt; &lt;p&gt;What models are coming in March &amp;amp; April? Any news/speculations/rumors?&lt;/p&gt; &lt;p&gt;Below are the models came this month(from this sub).&lt;/p&gt; &lt;p&gt;Just counted models from sources. inclusionAI is the winner, 13 models released in this month. Qwen is 2nd with 5 models. Though few other sources released 4-5 models, those are tiny/small ones.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/stepfun-ai/Step-3.5-Flash"&gt;https://huggingface.co/stepfun-ai/Step-3.5-Flash&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-Coder-Next"&gt;https://huggingface.co/Qwen/Qwen3-Coder-Next&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3.5-397B-A17B"&gt;https://huggingface.co/Qwen/Qwen3.5-397B-A17B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3.5-35B-A3B"&gt;https://huggingface.co/Qwen/Qwen3.5-35B-A3B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3.5-27B"&gt;https://huggingface.co/Qwen/Qwen3.5-27B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3.5-122B-A10B"&gt;https://huggingface.co/Qwen/Qwen3.5-122B-A10B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/zai-org/GLM-5"&gt;https://huggingface.co/zai-org/GLM-5&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/MiniMaxAI/MiniMax-M2.5"&gt;https://huggingface.co/MiniMaxAI/MiniMax-M2.5&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/inclusionAI/Ring-2.5-1T"&gt;https://huggingface.co/inclusionAI/Ring-2.5-1T&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/inclusionAI/Ling-2.5-1T"&gt;https://huggingface.co/inclusionAI/Ling-2.5-1T&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/PrimeIntellect/INTELLECT-3.1"&gt;https://huggingface.co/PrimeIntellect/INTELLECT-3.1&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/internlm/Intern-S1-Pro"&gt;https://huggingface.co/internlm/Intern-S1-Pro&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Nanbeige/Nanbeige4.1-3B"&gt;https://huggingface.co/Nanbeige/Nanbeige4.1-3B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/zai-org/GLM-OCR"&gt;https://huggingface.co/zai-org/GLM-OCR&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/tanaos/tanaos-sentiment-analysis-v1"&gt;https://huggingface.co/tanaos/tanaos-sentiment-analysis-v1&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/XeyonAI/Mistral-Helcyon-Mercury-12b-v3.2"&gt;https://huggingface.co/XeyonAI/Mistral-Helcyon-Mercury-12b-v3.2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/YuanLabAI/Yuan3.0-Flash"&gt;https://huggingface.co/YuanLabAI/Yuan3.0-Flash&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/FutureMa/Eva-4B-V2"&gt;https://huggingface.co/FutureMa/Eva-4B-V2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/mistralai/Voxtral-Mini-4B-Realtime-2602"&gt;https://huggingface.co/mistralai/Voxtral-Mini-4B-Realtime-2602&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/trillionlabs/gWorld-8B"&gt;https://huggingface.co/trillionlabs/gWorld-8B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/trillionlabs/gWorld-32B"&gt;https://huggingface.co/trillionlabs/gWorld-32B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/DeepBrainz/DeepBrainz-R1-4B"&gt;https://huggingface.co/DeepBrainz/DeepBrainz-R1-4B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/samuel-vitorino/sopro"&gt;https://huggingface.co/samuel-vitorino/sopro&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/concavity-ai/superlinear-exp-v0.1"&gt;https://huggingface.co/concavity-ai/superlinear-exp-v0.1&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/inclusionAI/LLaDA2.1-flash"&gt;https://huggingface.co/inclusionAI/LLaDA2.1-flash&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/inclusionAI/LLaDA2.1-mini"&gt;https://huggingface.co/inclusionAI/LLaDA2.1-mini&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/inclusionAI/UI-Venus-1.5-2B"&gt;https://huggingface.co/inclusionAI/UI-Venus-1.5-2B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/inclusionAI/UI-Venus-1.5-8B"&gt;https://huggingface.co/inclusionAI/UI-Venus-1.5-8B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/inclusionAI/UI-Venus-1.5-30B-A3B"&gt;https://huggingface.co/inclusionAI/UI-Venus-1.5-30B-A3B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/inclusionAI/Ming-flash-omni-2.0"&gt;https://huggingface.co/inclusionAI/Ming-flash-omni-2.0&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/inclusionAI/Ming-omni-tts-16.8B-A3B"&gt;https://huggingface.co/inclusionAI/Ming-omni-tts-16.8B-A3B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/inclusionAI/Ming-omni-tts-0.5B"&gt;https://huggingface.co/inclusionAI/Ming-omni-tts-0.5B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/inclusionAI/ZwZ-4B"&gt;https://huggingface.co/inclusionAI/ZwZ-4B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/inclusionAI/ZwZ-7B"&gt;https://huggingface.co/inclusionAI/ZwZ-7B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/inclusionAI/ZwZ-8B"&gt;https://huggingface.co/inclusionAI/ZwZ-8B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/WokeAI/Tankie-DPE-12B-SFT-v2"&gt;https://huggingface.co/WokeAI/Tankie-DPE-12B-SFT-v2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/OpenMOSS-Team/models?sort=created"&gt;https://huggingface.co/OpenMOSS-Team/models?sort=created&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/collections/Aratako/miotts"&gt;https://huggingface.co/collections/Aratako/miotts&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/AIDC-AI/Ovis2.6-30B-A3B"&gt;https://huggingface.co/AIDC-AI/Ovis2.6-30B-A3B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/neuphonic/models?sort=created"&gt;https://huggingface.co/neuphonic/models?sort=created&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/nineninesix/kani-tts-2-en"&gt;https://huggingface.co/nineninesix/kani-tts-2-en&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/jdopensource/JoyAI-LLM-Flash"&gt;https://huggingface.co/jdopensource/JoyAI-LLM-Flash&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/lm-provers/QED-Nano"&gt;https://huggingface.co/lm-provers/QED-Nano&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/collections/CohereLabs/tiny-aya"&gt;https://huggingface.co/collections/CohereLabs/tiny-aya&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Zyphra/ZUNA"&gt;https://huggingface.co/Zyphra/ZUNA&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/teapotai/tinyteapot"&gt;https://huggingface.co/teapotai/tinyteapot&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/collections/RWKV/rwkv-v7"&gt;https://huggingface.co/collections/RWKV/rwkv-v7&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/guidelabs/steerling-8b"&gt;https://huggingface.co/guidelabs/steerling-8b&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/PicoKittens/PicoMistral-23M"&gt;https://huggingface.co/PicoKittens/PicoMistral-23M&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/PicoKittens/PicoStories-853K"&gt;https://huggingface.co/PicoKittens/PicoStories-853K&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/crownelius/The-Crow-9B-Creative-Writing"&gt;https://huggingface.co/crownelius/The-Crow-9B-Creative-Writing&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/LiquidAI/LFM2-24B-A2B"&gt;https://huggingface.co/LiquidAI/LFM2-24B-A2B&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmttyji"&gt; /u/pmttyji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rghfqj/february_is_almost_over_are_you_satisfied/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rghfqj/february_is_almost_over_are_you_satisfied/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rghfqj/february_is_almost_over_are_you_satisfied/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-27T20:09:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1rggpu9</id>
    <title>Glm-5-Code ?</title>
    <updated>2026-02-27T19:42:25+00:00</updated>
    <author>
      <name>/u/axseem</name>
      <uri>https://old.reddit.com/user/axseem</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rggpu9/glm5code/"&gt; &lt;img alt="Glm-5-Code ?" src="https://preview.redd.it/hxpyzyxvb3mg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=236a07b3efea8cf0b6d807bf19bda2895d204463" title="Glm-5-Code ?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/axseem"&gt; /u/axseem &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hxpyzyxvb3mg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rggpu9/glm5code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rggpu9/glm5code/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-27T19:42:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1rg9lli</id>
    <title>Little Qwen 3.5 27B and Qwen 35B-A3B models did very well in my logical reasoning benchmark</title>
    <updated>2026-02-27T15:24:15+00:00</updated>
    <author>
      <name>/u/fairydreaming</name>
      <uri>https://old.reddit.com/user/fairydreaming</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rg9lli/little_qwen_35_27b_and_qwen_35ba3b_models_did/"&gt; &lt;img alt="Little Qwen 3.5 27B and Qwen 35B-A3B models did very well in my logical reasoning benchmark" src="https://preview.redd.it/s1gze7y5g1mg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1eb0061f8fcd2df9de3317470801d6b0e1c5d43f" title="Little Qwen 3.5 27B and Qwen 35B-A3B models did very well in my logical reasoning benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Tested in &lt;a href="https://github.com/fairydreaming/lineage-bench"&gt;lineage-bench&lt;/a&gt;. Results are &lt;a href="https://github.com/fairydreaming/lineage-bench-results/tree/main/lineage-8_64_128_192#results"&gt;here&lt;/a&gt;. It's amazing that models this small can reliably reason from hundreds of premises.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fairydreaming"&gt; /u/fairydreaming &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/s1gze7y5g1mg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rg9lli/little_qwen_35_27b_and_qwen_35ba3b_models_did/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rg9lli/little_qwen_35_27b_and_qwen_35ba3b_models_did/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-27T15:24:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1rg6ph3</id>
    <title>Qwen3.5 feels ready for production use - Never been this excited</title>
    <updated>2026-02-27T13:29:41+00:00</updated>
    <author>
      <name>/u/alphatrad</name>
      <uri>https://old.reddit.com/user/alphatrad</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rg6ph3/qwen35_feels_ready_for_production_use_never_been/"&gt; &lt;img alt="Qwen3.5 feels ready for production use - Never been this excited" src="https://preview.redd.it/kfx0j6lzf1mg1.png?width=140&amp;amp;height=140&amp;amp;crop=1:1,smart&amp;amp;auto=webp&amp;amp;s=6dcf868e7837c472ba8da5f18f55d96a1a2ab7a7" title="Qwen3.5 feels ready for production use - Never been this excited" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I ran a lot of tests playing with Qwen3.5-35B-A3B-UD-Q6_K_XL yesterday. Hitting around 1504pp2048 and 47.71 tg256 &lt;/p&gt; &lt;p&gt;Token speed is solid spread across two GPUs.&lt;/p&gt; &lt;p&gt;When I drop it down to one GPU that bumped up to 80tps.&lt;/p&gt; &lt;p&gt;But that's not what I'm hear to talk about. I did some basic benchmarking at first, then I had a thought. Let's take this for a ride in my real life client projects.&lt;/p&gt; &lt;p&gt;So basically I took a bunch of my projects and client projects, used Git Worktrees to role back to know spec changes and features. Gave it specs and let it cook. Did this across 5 of my projects.&lt;/p&gt; &lt;p&gt;Nailed them out of the part. Most of the &amp;quot;bugs&amp;quot; are like 5 min tweaks or things I could tell it to fix with a second prompt. &lt;/p&gt; &lt;p&gt;This feels like Sonnet 4 to me. At least for all the work I do. Across the Javascript landscape. The real surprise came testing it on some Go and Rust projects.&lt;/p&gt; &lt;p&gt;Guys, I've never been more excited for local models. Now... all the specs I gave it where generated by Claude. But i've been on a Max Pro plan for the last year. And I could see myself switching finally to a viable hybrid model. Where I use an API for the SOTA model to generate specs and do reviews and local models for all the work.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/kfx0j6lzf1mg1.png?width=1469&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e764471f2bbeabbc5b9daacc217e5d57bc187f8d"&gt;https://preview.redd.it/kfx0j6lzf1mg1.png?width=1469&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e764471f2bbeabbc5b9daacc217e5d57bc187f8d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I've been using Qwen coder for some time as my main go-to for tab completion, but this takes it to a new level.&lt;/p&gt; &lt;p&gt;It also really is making me ask for the first time if I should invest in the hardware upgrade.&lt;/p&gt; &lt;p&gt;I upgraded my business to Claude Pro Max in June of 2025 - so I've already spent 2000 on Cluade.&lt;/p&gt; &lt;p&gt;Business expense ... but if I pay all of 2026 and all of 2027 and I've already spent 2k - that will be $6800 in subscriptions.&lt;/p&gt; &lt;p&gt;What are the chances Anthrophic or others raise their cost? And how likely is local to get even better?&lt;/p&gt; &lt;p&gt;So yeah... really thinking about an RTX 6000 Pro right now. It might be worth the investment for my business.&lt;/p&gt; &lt;p&gt;Unless of course I can't get work in another year, lol.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/alphatrad"&gt; /u/alphatrad &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rg6ph3/qwen35_feels_ready_for_production_use_never_been/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rg6ph3/qwen35_feels_ready_for_production_use_never_been/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rg6ph3/qwen35_feels_ready_for_production_use_never_been/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-27T13:29:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1rg87bj</id>
    <title>Qwen3.5-35B-A3B running on a Raspberry Pi 5 (16GB and 8GB variants)</title>
    <updated>2026-02-27T14:30:32+00:00</updated>
    <author>
      <name>/u/jslominski</name>
      <uri>https://old.reddit.com/user/jslominski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rg87bj/qwen3535ba3b_running_on_a_raspberry_pi_5_16gb_and/"&gt; &lt;img alt="Qwen3.5-35B-A3B running on a Raspberry Pi 5 (16GB and 8GB variants)" src="https://external-preview.redd.it/dW03amZwN3BuMW1nMbm3gzMkoTfsIgGu4gtuHELcKn5C4RjCnBaO28O0Pqr2.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=add8ad0f0fb00757ae8c54e3c5d3380471cac48f" title="Qwen3.5-35B-A3B running on a Raspberry Pi 5 (16GB and 8GB variants)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Since the release of the latest Qwens, I wanted to test something that, at first thought, sounds a bit crazy: &lt;strong&gt;running Qwen3.5-35B-A3B on a Raspberry Pi&lt;/strong&gt; (re-using my pet project, you can see the device‚Äôs telemetry in the right pane). The best I got so far is a bit over &lt;strong&gt;3 t/s&lt;/strong&gt; on the 16GB variant and over &lt;strong&gt;1.5 t/s&lt;/strong&gt; on the 8GB RAM version, using 2-bit quants, without an NVMe SSD (just relatively fast SD cards) and, frankly, pretty crap cooling. I had throttling issues on both of my Pis, so I ordered a new cooler and an SSD HAT yesterday, which should help.&lt;/p&gt; &lt;p&gt;I‚Äôm also working on a custom llama.cpp build for Pi and experimenting with some tweaks, plus a few experiments with ARM‚Äôs KleidiAI (please don‚Äôt focus on the example's output since I‚Äôm still tweaking, trying different quants and inference params). To be honest, this looks pretty promising for agentic tasks, maybe some education, etc. They run almost as fast as 4-bit variants of Qwen3-4B-VL, which is pretty cool, given hum big those models are relative to the Pi capabilities. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jslominski"&gt; /u/jslominski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/mfr3o67pn1mg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rg87bj/qwen3535ba3b_running_on_a_raspberry_pi_5_16gb_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rg87bj/qwen3535ba3b_running_on_a_raspberry_pi_5_16gb_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-27T14:30:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1rgkyt5</id>
    <title>Qwen3.5 27B scores 42 on Intelligence Index and is the most intelligent model under 230B. Nearest model GLM-4.7-Flash 31B-A3B, Scores 30</title>
    <updated>2026-02-27T22:25:36+00:00</updated>
    <author>
      <name>/u/abdouhlili</name>
      <uri>https://old.reddit.com/user/abdouhlili</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rgkyt5/qwen35_27b_scores_42_on_intelligence_index_and_is/"&gt; &lt;img alt="Qwen3.5 27B scores 42 on Intelligence Index and is the most intelligent model under 230B. Nearest model GLM-4.7-Flash 31B-A3B, Scores 30" src="https://preview.redd.it/g4yry8u154mg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0891dee343ab1c2e9d248766f9900c26c0a5848e" title="Qwen3.5 27B scores 42 on Intelligence Index and is the most intelligent model under 230B. Nearest model GLM-4.7-Flash 31B-A3B, Scores 30" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdouhlili"&gt; /u/abdouhlili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/g4yry8u154mg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rgkyt5/qwen35_27b_scores_42_on_intelligence_index_and_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rgkyt5/qwen35_27b_scores_42_on_intelligence_index_and_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-27T22:25:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1rg94wu</id>
    <title>LLmFit - One command to find what model runs on your hardware</title>
    <updated>2026-02-27T15:06:33+00:00</updated>
    <author>
      <name>/u/ReasonablePossum_</name>
      <uri>https://old.reddit.com/user/ReasonablePossum_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rg94wu/llmfit_one_command_to_find_what_model_runs_on/"&gt; &lt;img alt="LLmFit - One command to find what model runs on your hardware" src="https://preview.redd.it/4194dq2qy1mg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=86e1f77b19ae2f61c500cb1462bd5812af4995c3" title="LLmFit - One command to find what model runs on your hardware" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Haven't seen this posted here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/AlexsJones/llmfit"&gt;https://github.com/AlexsJones/llmfit&lt;/a&gt;&lt;/p&gt; &lt;p&gt;497 models. 133 providers. One command to find what runs on your hardware.&lt;/p&gt; &lt;p&gt;A terminal tool that right-sizes LLM models to your system's RAM, CPU, and GPU. Detects your hardware, scores each model across quality, speed, fit, and context dimensions, and tells you which ones will actually run well on your machine.&lt;/p&gt; &lt;p&gt;Ships with an interactive TUI (default) and a classic CLI mode. Supports multi-GPU setups, MoE architectures, dynamic quantization selection, and speed estimation.&lt;/p&gt; &lt;p&gt;Hope it's useful :)&lt;/p&gt; &lt;p&gt;PS. I'm Not the repo creator, was trying to see what the sub thought on this and didn't find anything, so sharing it here. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ReasonablePossum_"&gt; /u/ReasonablePossum_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4194dq2qy1mg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rg94wu/llmfit_one_command_to_find_what_model_runs_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rg94wu/llmfit_one_command_to_find_what_model_runs_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-27T15:06:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1rgfm00</id>
    <title>I built a hybrid MoE runtime that does 3,324 tok/s prefill on a single 5080. Here are the benchmarks.</title>
    <updated>2026-02-27T19:01:09+00:00</updated>
    <author>
      <name>/u/mrstoatey</name>
      <uri>https://old.reddit.com/user/mrstoatey</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rgfm00/i_built_a_hybrid_moe_runtime_that_does_3324_toks/"&gt; &lt;img alt="I built a hybrid MoE runtime that does 3,324 tok/s prefill on a single 5080. Here are the benchmarks." src="https://preview.redd.it/3bt68udk33mg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=34b20c87019ad453c122dd125d646bd226ac5433" title="I built a hybrid MoE runtime that does 3,324 tok/s prefill on a single 5080. Here are the benchmarks." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been working on &lt;a href="https://github.com/brontoguana/krasis"&gt;Krasis&lt;/a&gt;, a hybrid CPU/GPU runtime for large MoE models. The core idea: GPU handles prefill (the expensive part), CPU handles decode, with the system RAM doing extra heavy lifting to maximise performance. This means you can run models way too large for your VRAM at speeds that are actually usable.&lt;/p&gt; &lt;p&gt;I wanted to share some benchmark results and get feedback.&lt;/p&gt; &lt;h2&gt;5080 Results (Q4)&lt;/h2&gt; &lt;p&gt;&lt;strong&gt;Hardware:&lt;/strong&gt; AMD 5900X, DDR4-3200, 1x RTX 5080 16GB, PCIe 4.0 x16&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;Prefill (tok/s)&lt;/th&gt; &lt;th&gt;TTFT (35K ctx)&lt;/th&gt; &lt;th&gt;Decode (tok/s)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Qwen3-Coder-Next (80B)&lt;/td&gt; &lt;td&gt;&lt;strong&gt;3,324&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;9.7s&lt;/td&gt; &lt;td&gt;14.9&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h2&gt;EPYC Results (Q4 and Q8)&lt;/h2&gt; &lt;p&gt;&lt;strong&gt;Hardware:&lt;/strong&gt; AMD EPYC 7742 (64c), DDR4-2666 8-channel, 1x RTX 2000 Ada 16GB, PCIe 4.0 x8&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;Quant&lt;/th&gt; &lt;th&gt;Prefill (tok/s)&lt;/th&gt; &lt;th&gt;TTFT&lt;/th&gt; &lt;th&gt;Decode (tok/s)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Qwen3-Coder-Next (80B)&lt;/td&gt; &lt;td&gt;Q4&lt;/td&gt; &lt;td&gt;1,060&lt;/td&gt; &lt;td&gt;18.9s&lt;/td&gt; &lt;td&gt;15.8&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3-Coder-Next (80B)&lt;/td&gt; &lt;td&gt;Q8&lt;/td&gt; &lt;td&gt;873&lt;/td&gt; &lt;td&gt;40.1s&lt;/td&gt; &lt;td&gt;12.4&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3.5-35B-A3B&lt;/td&gt; &lt;td&gt;Q4&lt;/td&gt; &lt;td&gt;1,374&lt;/td&gt; &lt;td&gt;14.6s&lt;/td&gt; &lt;td&gt;15.0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3-235B-A22B&lt;/td&gt; &lt;td&gt;Q4&lt;/td&gt; &lt;td&gt;289&lt;/td&gt; &lt;td&gt;69.1s&lt;/td&gt; &lt;td&gt;3.4&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepSeek V2-Lite (16B)&lt;/td&gt; &lt;td&gt;Q4&lt;/td&gt; &lt;td&gt;1,477&lt;/td&gt; &lt;td&gt;13.6s&lt;/td&gt; &lt;td&gt;20.2&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepSeek V2-Lite (16B)&lt;/td&gt; &lt;td&gt;Q8&lt;/td&gt; &lt;td&gt;1,317&lt;/td&gt; &lt;td&gt;15.2s&lt;/td&gt; &lt;td&gt;17.8&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Benchmarks use 10K‚Äì50K token prompts for prefill (best of 20K/35K/50K reported) and 64-token generation for decode (average of 3 runs).&lt;/p&gt; &lt;h2&gt;How it works&lt;/h2&gt; &lt;p&gt;Standard runtimes offload a few layers to GPU and run the rest on CPU. So you get a short GPU pass, then a long slow CPU slog for most of the model (both prefill and decode). This is fine for short prompts, but the moment you hand it a file or use it in an IDE (opencode will send 2500 tokens of tool spec etc with every prompt), you're waiting minutes for it to start generating.&lt;/p&gt; &lt;p&gt;Krasis takes a different approach and treats the GPU as a streaming compute engine, pushing the model through VRAM as fast as possible and hiding transfers under concurrent compute. The result is the GPU handles the full prefill pass then the CPU handles decode. The tradeoff is higher system RAM usage (~2.5x the quantised model size), but system RAM is far cheaper than VRAM.&lt;/p&gt; &lt;p&gt;In practice this means similar or faster decode speeds, massively faster prefill. The model reads files and always processes context at GPU speed instead of CPU speed.&lt;/p&gt; &lt;h2&gt;Tradeoffs&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Krasis is RAM hungry, you need ~2.5x the quantised model weight in system RAM (e.g. ~100GB for QCN at Q4)&lt;/li&gt; &lt;li&gt;Krasis supports only NVIDIA cards&lt;/li&gt; &lt;li&gt;It is specifically targeted at MoE models, decode would be slow on dense models&lt;/li&gt; &lt;li&gt;Decode is very usable (beyond reading speed on Qwen3-Coder-Next) but would benefit from further optimisation, I plan to look into speculative decode with draft models next, should give maybe 2-3x current decode speeds&lt;/li&gt; &lt;li&gt;The first run is slow as Krasis does a lot of preprocessing and caching that is skipped on subsequent runs&lt;/li&gt; &lt;li&gt;Krasis is disk hungry too, you need to give it the original BF16 safetensors file as input (downloaded from huggingface) and Krasis will store the cached transcoded models to disk (again about 2x the quantised models) &lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Supported models&lt;/h2&gt; &lt;p&gt;Qwen3-Coder-Next (most thoroughly tested), Qwen3.5-35B-A3B, Qwen3-235B-A22B, and DeepSeek V2-Lite. Other models coming soon.&lt;/p&gt; &lt;h2&gt;Details&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Written in Rust + Python (to orchestrate)&lt;/li&gt; &lt;li&gt;OpenAI-compatible API (works with Cursor, OpenCode, etc.)&lt;/li&gt; &lt;li&gt;Interactive launcher for config&lt;/li&gt; &lt;li&gt;SSPL licensed (free to use, modify, distribute)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/brontoguana/krasis"&gt;https://github.com/brontoguana/krasis&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Happy to answer questions. Particularly interested in feedback on:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;What models people would want supported next&lt;/li&gt; &lt;li&gt;What you think of the tradeoffs&lt;/li&gt; &lt;li&gt;Does anyone have a 5-series card and PCIE 5.0 (2x my PCIE 4.0 5080 bandwidth) that could benchmark Q3CN?&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mrstoatey"&gt; /u/mrstoatey &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3bt68udk33mg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rgfm00/i_built_a_hybrid_moe_runtime_that_does_3324_toks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rgfm00/i_built_a_hybrid_moe_runtime_that_does_3324_toks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-27T19:01:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1rg4zqv</id>
    <title>Follow-up: Qwen3.5-35B-A3B ‚Äî 7 community-requested experiments on RTX 5080 16GB</title>
    <updated>2026-02-27T12:09:50+00:00</updated>
    <author>
      <name>/u/gaztrab</name>
      <uri>https://old.reddit.com/user/gaztrab</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;: Community asked great questions on my original benchmarks post. I ran every experiment you requested. The headline: &lt;strong&gt;KV q8_0 is confirmed free lunch, Q4_K_M remains king,&lt;/strong&gt; &lt;code&gt;--fit on&lt;/code&gt; &lt;strong&gt;without batch flags hits 74.7 tok/s (+7% over my original config), and KL divergence confirms UD-Q4_K_XL is even worse than PPL suggested.&lt;/strong&gt; Full results and updated launch command below.&lt;/p&gt; &lt;h1&gt;Context&lt;/h1&gt; &lt;p&gt;After posting &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1rei65v/qwen3535ba3b_quantization_quality_speed/"&gt;Qwen3.5-35B-A3B quantization quality + speed benchmarks on RTX 5080 16GB&lt;/a&gt;, you folks raised a bunch of great questions. Rather than hand-waving, I ran every experiment I could. Here's what I found.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Hardware&lt;/strong&gt;: RTX 5080 16GB + 128GB DDR5 + Ryzen 9 9950X (32 threads) &lt;strong&gt;Software&lt;/strong&gt;: llama.cpp (built from source, CUDA 12.8, sm_120) &lt;strong&gt;Base model&lt;/strong&gt;: Qwen3.5-35B-A3B (MoE: 256 experts/layer, top-8 + 1 shared, ~3B active params/token)&lt;/p&gt; &lt;h1&gt;Experiment 1: KV Cache Quality ‚Äî Is q8_0 really &amp;quot;free&amp;quot;?&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Requested by&lt;/strong&gt;: &lt;a href="/u/PhilippeEiffel"&gt;u/PhilippeEiffel&lt;/a&gt;, &lt;a href="/u/MrMisterShin"&gt;u/MrMisterShin&lt;/a&gt;, &lt;a href="/u/llama-impersonator"&gt;u/llama-impersonator&lt;/a&gt;, &lt;a href="/u/WittyAmbassador7340"&gt;u/WittyAmbassador7340&lt;/a&gt;, &lt;a href="/u/kreigiron"&gt;u/kreigiron&lt;/a&gt;, &lt;a href="/u/bartskol"&gt;u/bartskol&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Fair concern ‚Äî I claimed KV q8_0 was free but didn't have PPL data to back it up. Here's the full matrix:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model Quant&lt;/th&gt; &lt;th align="left"&gt;KV f16&lt;/th&gt; &lt;th align="left"&gt;KV q8_0&lt;/th&gt; &lt;th align="left"&gt;KV q4_0&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Q8_0&lt;/td&gt; &lt;td align="left"&gt;5.8831&lt;/td&gt; &lt;td align="left"&gt;5.8822 (-0.02%)&lt;/td&gt; &lt;td align="left"&gt;5.8694 (-0.23%)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;6.0184&lt;/td&gt; &lt;td align="left"&gt;5.9997 (-0.31%)&lt;/td&gt; &lt;td align="left"&gt;6.0422 (+0.40%)&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Verdict&lt;/strong&gt;: KV q8_0 is genuinely free. PPL differences are within noise (&amp;lt; 0.4%). Even KV q4_0 is acceptable for most use cases. The &amp;quot;instant accuracy drops&amp;quot; some of you reported aren't reflected in PPL metrics ‚Äî though I acknowledge PPL may not capture all degradation modes (more on that below).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Recommendation unchanged&lt;/strong&gt;: Use &lt;code&gt;-ctk q8_0 -ctv q8_0&lt;/code&gt; for +12-38% throughput at zero measurable quality cost.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Caveat:&lt;/strong&gt; These PPL tests used 512 token context. Some users report KV q8_0 degrading at very long contexts (40-100k tokens) where quantization errors may accumulate. If you're regularly running huge contexts, test carefully.&lt;/p&gt; &lt;h1&gt;Experiment 2: KL Divergence ‚Äî Does PPL tell the whole story?&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Requested by&lt;/strong&gt;: &lt;a href="/u/JermMX5"&gt;u/JermMX5&lt;/a&gt;, &lt;a href="/u/Embarrassed_Ad3189"&gt;u/Embarrassed_Ad3189&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="/u/JermMX5"&gt;u/JermMX5&lt;/a&gt; cited the &lt;a href="https://arxiv.org/abs/2407.09141"&gt;Accuracy is Not All You Need paper&lt;/a&gt; showing PPL can stay flat while token accuracy collapses. Great point. So I ran KLD against Q8_0 base logits (512 ctx, 80 chunks):&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Quant&lt;/th&gt; &lt;th align="left"&gt;Mean KLD&lt;/th&gt; &lt;th align="left"&gt;Max KLD&lt;/th&gt; &lt;th align="left"&gt;Same Top-1 Token %&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;0.0282&lt;/td&gt; &lt;td align="left"&gt;4.2146&lt;/td&gt; &lt;td align="left"&gt;92.4%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;UD-Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;0.1087&lt;/td&gt; &lt;td align="left"&gt;7.7947&lt;/td&gt; &lt;td align="left"&gt;86.2%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Verdict&lt;/strong&gt;: KLD &lt;em&gt;confirms and amplifies&lt;/em&gt; the PPL findings. UD-Q4_K_XL is &lt;strong&gt;3.9x worse&lt;/strong&gt; than Q4_K_M by mean KLD and only preserves the top-1 token 86.2% of the time (vs 92.4%). PPL was not misleading here ‚Äî it correctly ranked the quants, but KLD shows the gap is even larger than PPL suggested.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Practical note&lt;/strong&gt;: Qwen3.5's 248K vocab makes full KLD evaluation produce enormous logit files (~19 GiB for 80 chunks). I used &lt;code&gt;--chunks 80&lt;/code&gt; with uint16 storage which is feasible with 128GB RAM. If you have a smaller system, &lt;code&gt;--chunks 20-30&lt;/code&gt; should give stable relative rankings.&lt;/p&gt; &lt;h1&gt;Experiment 3: Bartowski Q4_K_L ‚Äî Is the imatrix quant worth it?&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Requested by&lt;/strong&gt;: &lt;a href="/u/bettertoknow"&gt;u/bettertoknow&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/bartowski/Qwen_Qwen3.5-35B-A3B-GGUF"&gt;bartowski's Q4_K_L&lt;/a&gt; uses Q8_0 for embed/output tensors plus more q5_K and q6_K layers than Q4_K_M. Quality-wise, it's measurably better:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Metric&lt;/th&gt; &lt;th align="left"&gt;Q4_K_M (Unsloth)&lt;/th&gt; &lt;th align="left"&gt;Q4_K_L (bartowski)&lt;/th&gt; &lt;th align="left"&gt;Q8_0 (reference)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;PPL (WikiText-2)&lt;/td&gt; &lt;td align="left"&gt;6.6688&lt;/td&gt; &lt;td align="left"&gt;6.6125 (-0.8%)&lt;/td&gt; &lt;td align="left"&gt;6.5342&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Mean KLD&lt;/td&gt; &lt;td align="left"&gt;0.0282&lt;/td&gt; &lt;td align="left"&gt;0.0181 (-36%)&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Same top-1 %&lt;/td&gt; &lt;td align="left"&gt;92.4%&lt;/td&gt; &lt;td align="left"&gt;94.2%&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;File size&lt;/td&gt; &lt;td align="left"&gt;20 GB (4.74 BPW)&lt;/td&gt; &lt;td align="left"&gt;20.1 GB (4.98 BPW)&lt;/td&gt; &lt;td align="left"&gt;36.9 GB&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;But here's the problem ‚Äî speed:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Config&lt;/th&gt; &lt;th align="left"&gt;Short&lt;/th&gt; &lt;th align="left"&gt;Medium&lt;/th&gt; &lt;th align="left"&gt;Long&lt;/th&gt; &lt;th align="left"&gt;Multi-turn&lt;/th&gt; &lt;th align="left"&gt;VRAM&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Q4_K_M fit-nobatch&lt;/td&gt; &lt;td align="left"&gt;74.7 tok/s&lt;/td&gt; &lt;td align="left"&gt;72.9&lt;/td&gt; &lt;td align="left"&gt;73.7&lt;/td&gt; &lt;td align="left"&gt;76.1&lt;/td&gt; &lt;td align="left"&gt;14559 MB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Q4_K_L fit-nobatch&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;41.4 tok/s&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;41.4&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;40.8&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;41.8&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;14489 MB&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Q4_K_L is &lt;strong&gt;44% slower&lt;/strong&gt;. The larger q5_K/q6_K tensors (4.98 BPW vs 4.74) mean the model buffer is 8984 MiB vs Q4_K_M's 8556 MiB, causing &lt;code&gt;--fit&lt;/code&gt; to overflow more expert layers to CPU (19/41 vs ~16/41). Manual &lt;code&gt;--n-cpu-moe 24&lt;/code&gt; OOMs entirely because the model buffer alone exceeds what's available after compute buffer allocation.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Verdict&lt;/strong&gt;: Q4_K_L has genuinely better quality (especially visible in KLD: -36%), but the speed penalty is massive on single-GPU setups where VRAM is the constraint. If your model fits fully in VRAM (5090 32GB), Q4_K_L is a strict upgrade. On 16GB cards, &lt;strong&gt;Q4_K_M wins decisively&lt;/strong&gt;.&lt;/p&gt; &lt;h1&gt;Experiment 4: --fit Tuning ‚Äî Can we close the gap with manual offload?&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Requested by&lt;/strong&gt;: &lt;a href="/u/Chromix_"&gt;u/Chromix_&lt;/a&gt;, &lt;a href="/u/guiopen"&gt;u/guiopen&lt;/a&gt;, &lt;a href="/u/wisepal_app"&gt;u/wisepal_app&lt;/a&gt;, &lt;a href="/u/DonkeyBonked"&gt;u/DonkeyBonked&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In my original post, &lt;code&gt;--fit on&lt;/code&gt; was ~7% slower than manual &lt;code&gt;--n-cpu-moe 24&lt;/code&gt;. &lt;a href="/u/Chromix_"&gt;u/Chromix_&lt;/a&gt; suggested the issue might be that &lt;code&gt;-b 4096 -ub 4096&lt;/code&gt; batch flags consume VRAM that &lt;code&gt;--fit&lt;/code&gt; can't then use for expert layers. &lt;strong&gt;Nailed it.&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Config&lt;/th&gt; &lt;th align="left"&gt;Short&lt;/th&gt; &lt;th align="left"&gt;Medium&lt;/th&gt; &lt;th align="left"&gt;Long&lt;/th&gt; &lt;th align="left"&gt;Multi-turn&lt;/th&gt; &lt;th align="left"&gt;VRAM&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;C7 baseline (&lt;code&gt;--n-cpu-moe 24&lt;/code&gt;, -b 4096)&lt;/td&gt; &lt;td align="left"&gt;69.6 tok/s&lt;/td&gt; &lt;td align="left"&gt;67.0&lt;/td&gt; &lt;td align="left"&gt;65.7&lt;/td&gt; &lt;td align="left"&gt;69.2&lt;/td&gt; &lt;td align="left"&gt;14874 MB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;fit-default (&lt;code&gt;--fit on&lt;/code&gt;, -b 4096)&lt;/td&gt; &lt;td align="left"&gt;64.3&lt;/td&gt; &lt;td align="left"&gt;62.8&lt;/td&gt; &lt;td align="left"&gt;57.4*&lt;/td&gt; &lt;td align="left"&gt;54.2*&lt;/td&gt; &lt;td align="left"&gt;14595 MB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;fit-256 (&lt;code&gt;--fit-target 256&lt;/code&gt;, -b 4096)&lt;/td&gt; &lt;td align="left"&gt;66.0&lt;/td&gt; &lt;td align="left"&gt;64.7&lt;/td&gt; &lt;td align="left"&gt;63.7&lt;/td&gt; &lt;td align="left"&gt;66.0&lt;/td&gt; &lt;td align="left"&gt;15321 MB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;fit-nobatch (&lt;/strong&gt;&lt;code&gt;--fit on&lt;/code&gt;&lt;strong&gt;, no -b/-ub)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;74.7&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;72.9&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;73.7&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;76.1&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;14559 MB&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;*high variance with outliers&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Verdict&lt;/strong&gt;: &lt;a href="/u/Chromix_"&gt;u/Chromix_&lt;/a&gt; was right. Removing &lt;code&gt;-b 4096 -ub 4096&lt;/code&gt; lets &lt;code&gt;--fit&lt;/code&gt; allocate VRAM optimally for expert layers. &lt;strong&gt;fit-nobatch is the new winner at ~74 tok/s&lt;/strong&gt; ‚Äî simpler config AND faster than manual tuning. &lt;code&gt;--fit-target 256&lt;/code&gt; alone doesn't close the gap; removing the batch flags is the key insight.&lt;/p&gt; &lt;h1&gt;Experiment 5: Speculative Decoding ‚Äî Can we go faster?&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Requested by&lt;/strong&gt;: &lt;a href="/u/BreizhNode"&gt;u/BreizhNode&lt;/a&gt;, plus our own optimization roadmap&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Bad news first&lt;/strong&gt;: No compatible draft model exists. Qwen3.5 has a 248K vocabulary, Qwen3 has 151K. The smallest Qwen3.5 model is 27B ‚Äî there's no small Qwen3.5 that could serve as a draft. Draft-model speculation is a dead end for now.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;So I tried self-speculative methods&lt;/strong&gt; (no draft model needed):&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Config&lt;/th&gt; &lt;th align="left"&gt;Short&lt;/th&gt; &lt;th align="left"&gt;Medium&lt;/th&gt; &lt;th align="left"&gt;Long&lt;/th&gt; &lt;th align="left"&gt;Multi-turn&lt;/th&gt; &lt;th align="left"&gt;Status&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;fit-nobatch baseline&lt;/td&gt; &lt;td align="left"&gt;74.7 tok/s&lt;/td&gt; &lt;td align="left"&gt;72.9&lt;/td&gt; &lt;td align="left"&gt;73.7&lt;/td&gt; &lt;td align="left"&gt;76.1&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;ngram-simple&lt;/td&gt; &lt;td align="left"&gt;44.9&lt;/td&gt; &lt;td align="left"&gt;43.4&lt;/td&gt; &lt;td align="left"&gt;42.9&lt;/td&gt; &lt;td align="left"&gt;49.1&lt;/td&gt; &lt;td align="left"&gt;works&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;ngram-mod (m=64)&lt;/td&gt; &lt;td align="left"&gt;44.6&lt;/td&gt; &lt;td align="left"&gt;FAIL&lt;/td&gt; &lt;td align="left"&gt;FAIL&lt;/td&gt; &lt;td align="left"&gt;FAIL&lt;/td&gt; &lt;td align="left"&gt;crashes&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;ngram-simple-short (n=8, m=64)&lt;/td&gt; &lt;td align="left"&gt;45.0&lt;/td&gt; &lt;td align="left"&gt;43.1&lt;/td&gt; &lt;td align="left"&gt;43.1&lt;/td&gt; &lt;td align="left"&gt;FAIL&lt;/td&gt; &lt;td align="left"&gt;partial&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: ngram tests ran on a different llama.cpp build (&lt;code&gt;latest&lt;/code&gt; vs &lt;code&gt;latest-fit&lt;/code&gt;) that had a ~40% regression for unrelated reasons, so the absolute numbers aren't directly comparable. But even accounting for that, there's no speedup from ngram speculation on conversational workloads.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Verdict&lt;/strong&gt;: Self-speculative ngram methods provide zero benefit for diverse conversational workloads. ngram-mod is unstable (crashes after first request). &lt;strong&gt;Not recommended.&lt;/strong&gt; If Qwen releases a small Qwen3.5 model (1-3B), draft-model speculation could be huge ‚Äî but that doesn't exist yet.&lt;/p&gt; &lt;h1&gt;Experiment 6: Qwen3.5-27B Dense ‚Äî MoE vs Dense on single GPU&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Requested by&lt;/strong&gt;: &lt;a href="/u/moahmo88"&gt;u/moahmo88&lt;/a&gt;, &lt;a href="/u/Agreeable_Effect938"&gt;u/Agreeable_Effect938&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Some of you asked whether the dense 27B model might be a better fit for single-GPU setups. After all, it's simpler (no expert routing) and smaller (15.6 GB Q4_K_M).&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Metric&lt;/th&gt; &lt;th align="left"&gt;35B-A3B Q4_K_M (MoE)&lt;/th&gt; &lt;th align="left"&gt;27B Q4_K_M (dense)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;PPL (WikiText-2)&lt;/td&gt; &lt;td align="left"&gt;6.6688&lt;/td&gt; &lt;td align="left"&gt;6.8573 (+2.8%)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Active params/token&lt;/td&gt; &lt;td align="left"&gt;~3B&lt;/td&gt; &lt;td align="left"&gt;27B&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;File size&lt;/td&gt; &lt;td align="left"&gt;20 GB&lt;/td&gt; &lt;td align="left"&gt;15.6 GB&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Config&lt;/th&gt; &lt;th align="left"&gt;Short&lt;/th&gt; &lt;th align="left"&gt;Medium&lt;/th&gt; &lt;th align="left"&gt;Long&lt;/th&gt; &lt;th align="left"&gt;Multi-turn&lt;/th&gt; &lt;th align="left"&gt;VRAM&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;35B-A3B Q4_K_M fit-nobatch&lt;/td&gt; &lt;td align="left"&gt;74.7 tok/s&lt;/td&gt; &lt;td align="left"&gt;72.9&lt;/td&gt; &lt;td align="left"&gt;73.7&lt;/td&gt; &lt;td align="left"&gt;76.1&lt;/td&gt; &lt;td align="left"&gt;14559 MB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;27B dense fit&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;7.4 tok/s&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;7.4&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;7.2&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;7.1&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;14075 MB&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Yes, that's &lt;strong&gt;10x slower&lt;/strong&gt;. And it has worse quality.&lt;/p&gt; &lt;p&gt;The dense model needs all 27B parameters computed per token vs only ~3B active for MoE. Even with &lt;code&gt;--fit&lt;/code&gt; putting 54/65 layers on GPU, the remaining 11 layers on CPU create a massive bottleneck. Theoretical max even fully on GPU: ~61 tok/s (960 GB/s √∑ 15.6 GB model).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Verdict&lt;/strong&gt;: The MoE architecture is the entire advantage on consumer hardware. Only ~3B active params per token means ~10x less memory bandwidth per token. The 35B-A3B MoE is vastly faster on single-GPU setups with limited VRAM. The 27B dense is the stronger model on capability benchmarks and instruction following ‚Äî if you can fit it fully in VRAM (24GB+ cards), it's a great choice. On 16GB cards where it runs at 7 tok/s, it's not practical for interactive use.&lt;/p&gt; &lt;h1&gt;Experiment 7: MXFP4_MOE ‚Äî The Unsloth-recommended alternative&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Requested by&lt;/strong&gt;: &lt;a href="/u/ayylmaonade"&gt;u/ayylmaonade&lt;/a&gt;, &lt;a href="/u/jumpingcross"&gt;u/jumpingcross&lt;/a&gt;, &lt;a href="/u/danielhanchen"&gt;u/danielhanchen&lt;/a&gt; (Unsloth creator)&lt;/p&gt; &lt;p&gt;After &lt;a href="/u/danielhanchen"&gt;u/danielhanchen&lt;/a&gt; confirmed UD-Q4_K_XL has issues and specifically recommended MXFP4 as the alternative, I ran both quality and speed benchmarks.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Quality&lt;/strong&gt; (partial ‚Äî MXFP4 dequant path has a memory leak that OOMs after ~40-50 chunks):&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Metric&lt;/th&gt; &lt;th align="left"&gt;Q4_K_M&lt;/th&gt; &lt;th align="left"&gt;MXFP4_MOE&lt;/th&gt; &lt;th align="left"&gt;UD-Q4_K_XL&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;PPL (~40 chunks)&lt;/td&gt; &lt;td align="left"&gt;~6.00&lt;/td&gt; &lt;td align="left"&gt;~5.9-6.2* (the PPL runs all crashed due to memory leak, 5.96 is unverifiable)&lt;/td&gt; &lt;td align="left"&gt;~7.17&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Mean KLD (31 chunks)&lt;/td&gt; &lt;td align="left"&gt;0.028&lt;/td&gt; &lt;td align="left"&gt;0.050&lt;/td&gt; &lt;td align="left"&gt;0.109&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Same top-1 %&lt;/td&gt; &lt;td align="left"&gt;92.4%&lt;/td&gt; &lt;td align="left"&gt;91.0%&lt;/td&gt; &lt;td align="left"&gt;86.2%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;File size&lt;/td&gt; &lt;td align="left"&gt;21.2 GB&lt;/td&gt; &lt;td align="left"&gt;18.4 GB&lt;/td&gt; &lt;td align="left"&gt;19.8 GB&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Speed&lt;/strong&gt;:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Config&lt;/th&gt; &lt;th align="left"&gt;Short&lt;/th&gt; &lt;th align="left"&gt;Medium&lt;/th&gt; &lt;th align="left"&gt;Long&lt;/th&gt; &lt;th align="left"&gt;Multi-turn&lt;/th&gt; &lt;th align="left"&gt;VRAM&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Q4_K_M fit-nobatch&lt;/td&gt; &lt;td align="left"&gt;74.7 tok/s&lt;/td&gt; &lt;td align="left"&gt;72.9&lt;/td&gt; &lt;td align="left"&gt;73.7&lt;/td&gt; &lt;td align="left"&gt;76.1&lt;/td&gt; &lt;td align="left"&gt;14559 MB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;MXFP4_MOE fit-nobatch&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;49.5 tok/s&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;47.8&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;46.9&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;43.0&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;14531 MB&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Verdict&lt;/strong&gt;: MXFP4_MOE has comparable PPL to Q4_K_M (~5.9-6.2 vs 6.00, though partial evaluation due to memory leak) but is &lt;strong&gt;34-42% slower&lt;/strong&gt; (~47 tok/s vs ~74 tok/s). Despite the smaller file size (18.4 vs 21.2 GB), it doesn't translate to more expert layers on GPU ‚Äî VRAM usage is nearly identical. There's also a memory leak bug in the MXFP4 dequant path that prevents full perplexity evaluation. &lt;strong&gt;Not recommended over Q4_K_M&lt;/strong&gt; ‚Äî the quality gain is marginal while the speed loss is massive.&lt;/p&gt; &lt;p&gt;&lt;a href="/u/danielhanchen"&gt;u/danielhanchen&lt;/a&gt; ‚Äî if the Unsloth team has different results on MXFP4 speed, I'd love to compare notes. My build is llama.cpp b8149 with CUDA 12.8 on sm_120.&lt;/p&gt; &lt;h1&gt;Research Findings&lt;/h1&gt; &lt;p&gt;A few questions didn't need experiments, just digging:&lt;/p&gt; &lt;h1&gt;Why is Ollama 3x slower? (&lt;a href="/u/InternationalNebula7"&gt;u/InternationalNebula7&lt;/a&gt;)&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Ollama has no MoE expert offloading.&lt;/strong&gt; When a MoE model doesn't fit in VRAM, Ollama splits at the layer level ‚Äî entire transformer blocks go to CPU or GPU. This means the GPU sits completely idle waiting for CPU layers. With expert-only offloading, attention/norms stay on GPU while only routed expert FFNs go to CPU ‚Äî the GPU stays busy.&lt;/p&gt; &lt;p&gt;There's &lt;a href="https://github.com/ollama/ollama/pull/12333"&gt;an open PR (ollama/ollama#12333)&lt;/a&gt; to add &lt;code&gt;num_moe_offload&lt;/code&gt; but it hasn't merged yet. On top of that, Ollama defaults to KV cache f16 (we use q8_0, +20% throughput) and doesn't expose batch size or flash attention controls.&lt;/p&gt; &lt;h1&gt;Pre-built binaries vs source for Blackwell (&lt;a href="/u/wisepal_app"&gt;u/wisepal_app&lt;/a&gt;)&lt;/h1&gt; &lt;p&gt;For &lt;strong&gt;RTX 50-series&lt;/strong&gt;: building from source matters. Release binaries use CUDA 12.4 which doesn't include sm_120 (Blackwell). You need CUDA 12.8+ for native support. Without it, PTX from sm_89 (Ada) gets JIT-compiled ‚Äî slower first launch and you miss Blackwell-specific kernels.&lt;/p&gt; &lt;p&gt;For &lt;strong&gt;RTX 30/40-series&lt;/strong&gt;: pre-built is fine (0-5% difference). Those architectures are already in the release builds.&lt;/p&gt; &lt;h1&gt;8 GB VRAM recommendations (&lt;a href="/u/Qxz3"&gt;u/Qxz3&lt;/a&gt;)&lt;/h1&gt; &lt;p&gt;Use Q4_K_M with full expert offload (&lt;code&gt;-ot &amp;quot;exps=CPU&amp;quot;&lt;/code&gt;): ~7.2 GB VRAM, ~50 tok/s in our tests (on RTX 5080 ‚Äî your results will vary depending on GPU memory bandwidth). Key flags: &lt;code&gt;-ctk q8_0 -ctv q8_0&lt;/code&gt; (free lunch), &lt;code&gt;-fa on&lt;/code&gt;, &lt;code&gt;--no-mmap&lt;/code&gt;, and tune your thread count (try &lt;code&gt;physical_cores / 1.5&lt;/code&gt; as starting point, sweep from there).&lt;/p&gt; &lt;h1&gt;Updated Launch Command&lt;/h1&gt; &lt;p&gt;Based on everything above, here's the new recommended config. Simpler AND faster than my original post:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./llama-server \ -m ./Qwen3.5-35B-A3B-Q4_K_M.gguf \ -c 65536 \ --fit on \ -fa on \ -t 20 \ --no-mmap \ --jinja \ -ctk q8_0 \ -ctv q8_0 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;What changed from the original post&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Removed &lt;code&gt;-ngl 999 --n-cpu-moe 24&lt;/code&gt; ‚Üí replaced with &lt;code&gt;--fit on&lt;/code&gt; (auto VRAM management)&lt;/li&gt; &lt;li&gt;Removed &lt;code&gt;-b 4096 -ub 4096&lt;/code&gt; ‚Üí this was the key insight from &lt;a href="/u/Chromix_"&gt;u/Chromix_&lt;/a&gt; ‚Äî batch flags eat VRAM that &lt;code&gt;--fit&lt;/code&gt; needs for expert layers&lt;/li&gt; &lt;li&gt;Result: &lt;strong&gt;74.7 tok/s&lt;/strong&gt; (up from 69.6), simpler config, and &lt;code&gt;--fit&lt;/code&gt; adapts automatically to your available VRAM&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Summary Table&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;What&lt;/th&gt; &lt;th align="left"&gt;Result&lt;/th&gt; &lt;th align="left"&gt;Verdict&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;KV q8_0 quality&lt;/td&gt; &lt;td align="left"&gt;&amp;lt; 0.4% PPL difference&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Free lunch. Use it.&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;KLD: Q4_K_M vs UD-Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;0.028 vs 0.109 (3.9x worse)&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;UD-Q4_K_XL is bad for MoE&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Bartowski Q4_K_L&lt;/td&gt; &lt;td align="left"&gt;-0.8% PPL, -36% KLD, but 44% slower&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Not worth it on 16GB&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;code&gt;--fit&lt;/code&gt; without batch flags&lt;/td&gt; &lt;td align="left"&gt;74.7 tok/s (+7% over manual)&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;New best config&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;ngram self-speculation&lt;/td&gt; &lt;td align="left"&gt;No speedup, unstable&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Don't bother&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;27B dense vs 35B-A3B MoE&lt;/td&gt; &lt;td align="left"&gt;10x slower, worse quality&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;MoE wins completely&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MXFP4_MOE&lt;/td&gt; &lt;td align="left"&gt;Marginal quality gain, 34-42% slower&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Q4_K_M still best&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Acknowledgments&lt;/h1&gt; &lt;p&gt;Thanks to everyone who pushed for better data:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="/u/PhilippeEiffel"&gt;u/PhilippeEiffel&lt;/a&gt;, &lt;a href="/u/MrMisterShin"&gt;u/MrMisterShin&lt;/a&gt;, &lt;a href="/u/llama-impersonator"&gt;u/llama-impersonator&lt;/a&gt;, &lt;a href="/u/WittyAmbassador7340"&gt;u/WittyAmbassador7340&lt;/a&gt;, &lt;a href="/u/kreigiron"&gt;u/kreigiron&lt;/a&gt;, &lt;a href="/u/bartskol"&gt;u/bartskol&lt;/a&gt; ‚Äî KV cache quality concerns led to the full PPL matrix (E1)&lt;/li&gt; &lt;li&gt;&lt;a href="/u/JermMX5"&gt;u/JermMX5&lt;/a&gt;, &lt;a href="/u/Embarrassed_Ad3189"&gt;u/Embarrassed_Ad3189&lt;/a&gt; ‚Äî pushed for KLD over PPL, which revealed the UD-Q4_K_XL gap is worse than PPL showed (E2)&lt;/li&gt; &lt;li&gt;&lt;a href="/u/bettertoknow"&gt;u/bettertoknow&lt;/a&gt; ‚Äî Bartowski Q4_K_L benchmark, good call even though it turned out too slow for our setup (E3)&lt;/li&gt; &lt;li&gt;&lt;a href="/u/Chromix_"&gt;u/Chromix_&lt;/a&gt;, &lt;a href="/u/guiopen"&gt;u/guiopen&lt;/a&gt;, &lt;a href="/u/wisepal_app"&gt;u/wisepal_app&lt;/a&gt;, &lt;a href="/u/DonkeyBonked"&gt;u/DonkeyBonked&lt;/a&gt; ‚Äî &lt;code&gt;--fit&lt;/code&gt; tuning, especially Chromix_'s insight about batch flags eating VRAM, which gave us the new fastest config (E4)&lt;/li&gt; &lt;li&gt;&lt;a href="/u/BreizhNode"&gt;u/BreizhNode&lt;/a&gt; ‚Äî speculative decoding investigation, saved others the trouble (E5)&lt;/li&gt; &lt;li&gt;&lt;a href="/u/moahmo88"&gt;u/moahmo88&lt;/a&gt;, &lt;a href="/u/Agreeable_Effect938"&gt;u/Agreeable_Effect938&lt;/a&gt; ‚Äî 27B dense comparison, definitively answered &amp;quot;is MoE worth the complexity?&amp;quot; (E6)&lt;/li&gt; &lt;li&gt;&lt;a href="/u/ayylmaonade"&gt;u/ayylmaonade&lt;/a&gt;, &lt;a href="/u/jumpingcross"&gt;u/jumpingcross&lt;/a&gt;, &lt;a href="/u/danielhanchen"&gt;u/danielhanchen&lt;/a&gt; ‚Äî MXFP4_MOE testing, important to validate the Unsloth creator's recommendation (E7)&lt;/li&gt; &lt;li&gt;&lt;a href="/u/InternationalNebula7"&gt;u/InternationalNebula7&lt;/a&gt; ‚Äî Ollama performance gap explanation&lt;/li&gt; &lt;li&gt;&lt;a href="/u/Qxz3"&gt;u/Qxz3&lt;/a&gt; ‚Äî 8GB VRAM config guidance&lt;/li&gt; &lt;li&gt;&lt;a href="/u/JoNike"&gt;u/JoNike&lt;/a&gt; ‚Äî original RTX 5080 partial offload data that informed our testing&lt;/li&gt; &lt;li&gt;&lt;a href="/u/3spky5u-oss"&gt;u/3spky5u-oss&lt;/a&gt; ‚Äî comprehensive RTX 5090 head-to-head benchmarks&lt;/li&gt; &lt;li&gt;&lt;a href="/u/catplusplusok"&gt;u/catplusplusok&lt;/a&gt;, &lt;a href="/u/SlimeQ"&gt;u/SlimeQ&lt;/a&gt;, &lt;a href="/u/guiopen"&gt;u/guiopen&lt;/a&gt; ‚Äî chat template and tool calling tips&lt;/li&gt; &lt;li&gt;&lt;a href="/u/chickN00dle"&gt;u/chickN00dle&lt;/a&gt;, &lt;a href="/u/Odd-Ordinary-5922"&gt;u/Odd-Ordinary-5922&lt;/a&gt; ‚Äî KV cache sensitivity reports at long context&lt;/li&gt; &lt;li&gt;&lt;a href="/u/TheRealMasonMac"&gt;u/TheRealMasonMac&lt;/a&gt; ‚Äî &lt;code&gt;--fit on&lt;/code&gt; documentation and RTX 4070 results&lt;/li&gt; &lt;li&gt;&lt;a href="/u/pmttyji"&gt;u/pmttyji&lt;/a&gt;, &lt;a href="/u/Subject-Tea-5253"&gt;u/Subject-Tea-5253&lt;/a&gt; ‚Äî batch/ubatch tuning data&lt;/li&gt; &lt;li&gt;&lt;a href="/u/Pristine-Woodpecker"&gt;u/Pristine-Woodpecker&lt;/a&gt; ‚Äî independent confirmation of UD-Q4_K_XL quality issues&lt;/li&gt; &lt;li&gt;&lt;a href="/u/jslominski"&gt;u/jslominski&lt;/a&gt;, &lt;a href="/u/jiegec"&gt;u/jiegec&lt;/a&gt;, &lt;a href="/u/Corosus"&gt;u/Corosus&lt;/a&gt;, &lt;a href="/u/DeedleDumbDee"&gt;u/DeedleDumbDee&lt;/a&gt;, &lt;a href="/u/Monad_Maya"&gt;u/Monad_Maya&lt;/a&gt;, &lt;a href="/u/l33t-Mt"&gt;u/l33t-Mt&lt;/a&gt;, &lt;a href="/u/kkb294"&gt;u/kkb294&lt;/a&gt;, &lt;a href="/u/zmanning"&gt;u/zmanning&lt;/a&gt;, &lt;a href="/u/Additional-Action566"&gt;u/Additional-Action566&lt;/a&gt; ‚Äî speed reports across different GPUs&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;All raw data (benchmark JSONs, PPL logs, KLD logs, config files) is in &lt;a href="https://github.com/gaztrabisme/llm-server"&gt;my llm-server repo&lt;/a&gt; for anyone who wants to reproduce or verify.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Edit&lt;/strong&gt;: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1rei65v/qwen3535ba3b_quantization_quality_speed/"&gt;Previous post here&lt;/a&gt;. This is a follow-up with all the experiments you requested.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Edit 2:&lt;/strong&gt; Corrected some numbers that had errors in the original post. None of the conclusions change:&lt;/p&gt; &lt;p&gt;- E2 (KLD): Max KLD values were wrong ‚Äî Q4_K_M is 4.21 (not 0.19), UD-Q4_K_XL is 7.79 (not 1.22). This actually makes UD-Q4_K_XL look worse than originally stated.&lt;/p&gt; &lt;p&gt;- E5 (Speculative): ngram-simple multi-turn was 49.1 tok/s (not 51.3). Still no benefit.&lt;/p&gt; &lt;p&gt;- E7 (MXFP4): Mean KLD is 0.050 (not 0.037), PPL is ~5.9-6.2 (partial, memory leak crashed all full runs), multi-turn speed is 43.0 tok/s (not 44.1). Still not recommended over Q4_K_M.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Edit 3:&lt;/strong&gt; THANK YOU FOR THE AWARD, RANDOM CITIZEN!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Edit 4:&lt;/strong&gt; Updated E6 (27B dense) wording ‚Äî several commenters correctly pointed out that calling 27B &amp;quot;worse quality&amp;quot; based on PPL alone is misleading. The 27B dominates on capability benchmarks and instruction following; my results only show it's 10x slower on 16GB VRAM where it can't fit fully on GPU. If you have a 24GB+ card and can load it entirely in VRAM, 27B is a great model.&lt;/p&gt; &lt;p&gt;Added caveat to E1 (KV q8_0) that my PPL tests used 512 token context ‚Äî some users report degradation at very long contexts (40-100k+).&lt;/p&gt; &lt;p&gt;Clarified that the ~50 tok/s 8GB VRAM number (E5 C5 full offload config) was on RTX 5080, not a separate 8GB card ‚Äî a 3060 12GB will see lower numbers due to lower memory bandwidth.&lt;/p&gt; &lt;p&gt;Thanks &lt;a href="/u/_-_David"&gt;u/_-_David&lt;/a&gt;, &lt;a href="/u/ArckToons"&gt;u/ArckToons&lt;/a&gt;, &lt;a href="/u/Front_Eagle739"&gt;u/Front_Eagle739&lt;/a&gt;, and &lt;a href="/u/cookieGaboo24"&gt;u/cookieGaboo24&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Edit 5:&lt;/strong&gt; &lt;a href="/u/Corosus"&gt;u/Corosus&lt;/a&gt; found --fit on performs poorly on Vulkan backend (13 tok/s vs 33 tok/s with manual --n-cpu-moe 24 on a 5070 Ti). My --fit results are CUDA-specific ‚Äî Vulkan users should stick with manual offloading. Thanks man!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Edit 6:&lt;/strong&gt; THANK YOU ANOTHER CITIZEN OF SUPER EARTH FOR THE AWARD!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Edit 7:&lt;/strong&gt; Thanks to the community overwhelming reactions, and suggestions. I will definitely conduct another round of experiments to gather more data. Also...&lt;/p&gt; &lt;p&gt;OMG GUYS THANKS FOR THE AWARDS!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gaztrab"&gt; /u/gaztrab &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rg4zqv/followup_qwen3535ba3b_7_communityrequested/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rg4zqv/followup_qwen3535ba3b_7_communityrequested/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rg4zqv/followup_qwen3535ba3b_7_communityrequested/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-27T12:09:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1rg8dex</id>
    <title>PewDiePie fine-tuned Qwen2.5-Coder-32B to beat ChatGPT 4o on coding benchmarks.</title>
    <updated>2026-02-27T14:37:18+00:00</updated>
    <author>
      <name>/u/hedgehog0</name>
      <uri>https://old.reddit.com/user/hedgehog0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rg8dex/pewdiepie_finetuned_qwen25coder32b_to_beat/"&gt; &lt;img alt="PewDiePie fine-tuned Qwen2.5-Coder-32B to beat ChatGPT 4o on coding benchmarks." src="https://external-preview.redd.it/mCmYhKXGNj-QOd-sXT1nvg6KbIIK9oXVkPL1aBEF4FY.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2e3e22dae46cc10fa4599a3b4892076af3a2cc56" title="PewDiePie fine-tuned Qwen2.5-Coder-32B to beat ChatGPT 4o on coding benchmarks." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hedgehog0"&gt; /u/hedgehog0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=aV4j5pXLP-I&amp;amp;feature=youtu.be"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rg8dex/pewdiepie_finetuned_qwen25coder32b_to_beat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rg8dex/pewdiepie_finetuned_qwen25coder32b_to_beat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-27T14:37:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1rgkc1b</id>
    <title>Back in my day, LocalLLaMa were the pioneers!</title>
    <updated>2026-02-27T22:00:57+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rgkc1b/back_in_my_day_localllama_were_the_pioneers/"&gt; &lt;img alt="Back in my day, LocalLLaMa were the pioneers!" src="https://preview.redd.it/hiz4ukvg04mg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=50545019f32fd7e0e01e1b41c3ffbb390e1046eb" title="Back in my day, LocalLLaMa were the pioneers!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hiz4ukvg04mg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rgkc1b/back_in_my_day_localllama_were_the_pioneers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rgkc1b/back_in_my_day_localllama_were_the_pioneers/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-27T22:00:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1rgel19</id>
    <title>New Qwen3.5-35B-A3B Unsloth Dynamic GGUFs + Benchmarks</title>
    <updated>2026-02-27T18:23:50+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rgel19/new_qwen3535ba3b_unsloth_dynamic_ggufs_benchmarks/"&gt; &lt;img alt="New Qwen3.5-35B-A3B Unsloth Dynamic GGUFs + Benchmarks" src="https://external-preview.redd.it/dk4oYGXETs66okCfY5tWsmyJ8TsYGxQaSswXwZSBdYs.png?width=140&amp;amp;height=75&amp;amp;auto=webp&amp;amp;s=1b158effff1f75e76fd7b2a285de69b70b0fede9" title="New Qwen3.5-35B-A3B Unsloth Dynamic GGUFs + Benchmarks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLlama"&gt;r/LocalLlama&lt;/a&gt;! We just updated Qwen3.5-35B Unsloth Dynamic quants &lt;strong&gt;being SOTA&lt;/strong&gt; on nearly all bits. We did over 150 KL Divergence benchmarks, totally &lt;strong&gt;9TB of GGUFs&lt;/strong&gt;. We uploaded all research artifacts. We also fixed a &lt;strong&gt;tool calling&lt;/strong&gt; chat template &lt;strong&gt;bug&lt;/strong&gt; (affects all quant uploaders)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;We tested Bartowski, Ubergram, AesSedai, Noctrex and our new Dynamic GGUFs&lt;/li&gt; &lt;li&gt;&lt;strong&gt;99.9% KL Divergence shows SOTA&lt;/strong&gt; on Pareto Frontier for UD-Q4_K_XL, IQ3_XXS &amp;amp; more.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Retiring MXFP4&lt;/strong&gt; from all GGUF quants: Q2_K_XL, Q3_K_XL and Q4_K_XL, except for a select few layers.&lt;/li&gt; &lt;li&gt;Qwen3.5-35B-A3B GGUFs are updated to use new fixes (112B, 27B still converting, re-download once they are updated)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5hmdthgyp2mg1.png?width=2320&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3dbd0480bbc38512a8bbbba0e4e01444feec99fb"&gt;https://preview.redd.it/5hmdthgyp2mg1.png?width=2320&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3dbd0480bbc38512a8bbbba0e4e01444feec99fb&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Imatrix definitely helps reduce KLD &amp;amp; PPL.&lt;/li&gt; &lt;li&gt;I quants (iq3_xxs, iq2_s etc) makes inference 5-10% slower.&lt;/li&gt; &lt;li&gt;Quantizing ssm_out (Mamba layers) is not a good idea, and ffn_down_exps.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Some tensors are very sensitive to quantization&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;We made over 9TB of research artifacts available for the community to investigate further on our &lt;a href="https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF"&gt;Experiments page&lt;/a&gt;. It includes KLD metrics and all 121 configs we tested.&lt;/li&gt; &lt;li&gt;We varied bit widths across each tensor type, and generated a best and worst Pareto Frontier plot below vs 99.9% KLD.&lt;/li&gt; &lt;li&gt;For the best items to quantize, ffn_up_exps and ffn_gate_exps are generally ok to quantize to 3bit. ffn_down_exps is slightly more sensitive.&lt;/li&gt; &lt;li&gt;For the worst items, ssm_out dramatically increases KLD and the disk space savings is minuscule. For example, ssm_out at q2_k does dramatically worse. &lt;strong&gt;Quantizing any attn_* is especially sensitive&lt;/strong&gt; for hybrid architectures, and so leaving them in higher precision works well.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/pakdmbv1n2mg1.png?width=1183&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=be8940bf7c49157d1e34bb82053e70b44f0e1744"&gt;https://preview.redd.it/pakdmbv1n2mg1.png?width=1183&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=be8940bf7c49157d1e34bb82053e70b44f0e1744&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Tensor type vs bits on 99.9% KL Divergence&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;We plot all quant levels vs 99.9% KLD, and sort from worst KLD to best. Quantizing ffn_* layers too heavily down is not a good idea.&lt;/li&gt; &lt;li&gt;However, &lt;strong&gt;some bit widths are good, especially 3bit&lt;/strong&gt;. - for example leaving ffn_* (down, up, gate) at around iq3_xxs seems to be best compromise on disk space and 99.9% KLD change. 2 bits cause more degradation.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;MXFP4 is much worse on many tensors&lt;/strong&gt; - attn_gate, attn_q, ssm_beta, ssm_alpha using MXFP4 is not a good idea, and rather Q4_K is better - also MXFP4 uses 4.25 bits per weight, whilst Q4_K uses 4.5 bits per weight. It's better to use Q4_K than MXFP4 when choosing between them.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xgugdgzmv2mg1.png?width=989&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=eddc2c32d343410a27f405289fd976e858d6f6a8"&gt;https://preview.redd.it/xgugdgzmv2mg1.png?width=989&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=eddc2c32d343410a27f405289fd976e858d6f6a8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Imatrix works remarkably well&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Imatrix definitely helps weight the quantization process in the right way. For example previously ssm_out at 2bits was really bad, however imatrix reduces the 99.9% KLD by a lot.&lt;/li&gt; &lt;li&gt;Imatrix generally helps on lower bits, and works on all quants and bit widths.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/yidhlf79o2mg1.png?width=1389&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c9b5f1f6510d0aa5ebbf4b06ba9908947a21e93e"&gt;https://preview.redd.it/yidhlf79o2mg1.png?width=1389&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c9b5f1f6510d0aa5ebbf4b06ba9908947a21e93e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I quants (iq3_xxs, iq2_s etc) makes inference 5-10% slower, they're definitely better in terms of efficiency, but there is a tradeoff.&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/bnjmn_marie/status/2027043753484021810"&gt;&lt;strong&gt;Benjamin‚Äôs recent MiniMax‚ÄëM2.5 analysis&lt;/strong&gt;&lt;/a&gt; shows a case how perplexity and KLD can still be very misleading. Unsloth Dynamic IQ2_XXS &lt;strong&gt;performs better&lt;/strong&gt; than AesSedai‚Äôs IQ3_S on real world evals (LiveCodeBench v6, MMLU Pro) despite being 11GB smaller. Yet, AesSedai‚Äôs perplexity and KLD benchmarks suggest the &lt;strong&gt;opposite&lt;/strong&gt;. (PPL: 0.3552 vs 0.2441; KLD: 9.0338 vs 8.2849 - lower is better).&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/hwif5hfex2mg1.png?width=1078&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d6fef62ede6626f47991a3dbc90183b9d621d0bc"&gt;https://preview.redd.it/hwif5hfex2mg1.png?width=1078&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d6fef62ede6626f47991a3dbc90183b9d621d0bc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Perplexity and KLD can also be misleading&lt;/strong&gt; but, as precaution we replaced any MXFP4 layer. Real-world evals (LiveCodeBench v6 etc.) are much better benchmarks, but can take many days. This mismatch shows how &lt;strong&gt;lower perplexity or KLD doesn‚Äôt necessarily translate to better real-world performance&lt;/strong&gt;. The graph also shows &lt;strong&gt;UD‚ÄëQ4-K‚ÄëXL&lt;/strong&gt; outperforming other &lt;strong&gt;Q4&lt;/strong&gt; quants, while being ~8GB smaller.&lt;/p&gt; &lt;p&gt;This doesn‚Äôt mean perplexity or KLD is useless, as they provide a &lt;em&gt;rough signal&lt;/em&gt;. So, going forward, we‚Äôll publish &lt;strong&gt;perplexity and KLD for every quant&lt;/strong&gt; so the community has some reference.&lt;/p&gt; &lt;p&gt;Updated GGUFs here: &lt;a href="https://huggingface.co/collections/unsloth/qwen35"&gt;https://huggingface.co/collections/unsloth/qwen35&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For more investigation deets and benchmarks you can read: &lt;a href="https://unsloth.ai/docs/models/qwen3.5"&gt;&lt;strong&gt;https://unsloth.ai/docs/models/qwen3.5&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thank you for reading and once again for the feedback and incredible support. Huge thanks to the Qwen team as well for releasing Qwen3.5. If there‚Äôs any suggestions please let us know and have a great Friday / weekend guys!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Benchmarking Details &amp;amp; Appreciation:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;We utilized bartowski's wonderful imatrix file to make the comparisons more fair - our Dynamic 2.0 method uses a conversational format, but we found benchmarking to be fairer if we used a more general imatrix&lt;/li&gt; &lt;li&gt;We appreciated some friendly guidance from Ubergram and the community!&lt;/li&gt; &lt;li&gt;For perplexity we used the below. We also use the BF16 as the base KLD file. &lt;code&gt;LLAMA_SET_ROWS=1 ./llama.cpp/llama-perplexity --flash-attn on --fit off --batch-size 16384 --ubatch-size 16384 --device {device} --model {model} --ctx-size 512&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rgel19/new_qwen3535ba3b_unsloth_dynamic_ggufs_benchmarks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rgel19/new_qwen3535ba3b_unsloth_dynamic_ggufs_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rgel19/new_qwen3535ba3b_unsloth_dynamic_ggufs_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-27T18:23:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7bsfd</id>
    <title>Best Audio Models - Feb 2026</title>
    <updated>2026-02-17T17:01:37+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;They've been a ton of audio models released of late, the most notable perhaps being Qwen3 TTS. So its time for another &lt;strong&gt;Best Audio Models&lt;/strong&gt; megathread&lt;/p&gt; &lt;p&gt;Share what your favorite ASR, TTS, STT, Text to Music models are right now &lt;strong&gt;and why.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Given the the amount of ambiguity and subjectivity in rating/testing these models, please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks etc. Closed models like Elevenlabs v3 seem to continue to be a few levels above open models especially for production use cases with long lengths/stability requirements, so comparisons, especially empirical ones are welcome.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please use the top level comments to thread your responses.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T17:01:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8snay</id>
    <title>AMA with StepFun AI - Ask Us Anything</title>
    <updated>2026-02-19T07:15:35+00:00</updated>
    <author>
      <name>/u/StepFun_ai</name>
      <uri>https://old.reddit.com/user/StepFun_ai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt; &lt;img alt="AMA with StepFun AI - Ask Us Anything" src="https://external-preview.redd.it/bqLigXK_A-XO-amDfdyeQenKCxR18nhChWpDnQXYmr4.png?width=140&amp;amp;height=75&amp;amp;auto=webp&amp;amp;s=72c79bc533c167d8f77ecf629e3b7af53ee1e6b6" title="AMA with StepFun AI - Ask Us Anything" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/w8274fg1jekg1.png?width=1785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fadbd0ec26a56e60900f9ed667ae808217d70cf2"&gt;https://preview.redd.it/w8274fg1jekg1.png?width=1785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fadbd0ec26a56e60900f9ed667ae808217d70cf2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; !&lt;/p&gt; &lt;p&gt;We are &lt;strong&gt;StepFun&lt;/strong&gt;, the team behind the &lt;strong&gt;Step&lt;/strong&gt; family models, including &lt;a href="https://huggingface.co/collections/stepfun-ai/step-35-flash"&gt;&lt;strong&gt;Step 3.5 Flash&lt;/strong&gt;&lt;/a&gt; and &lt;a href="https://huggingface.co/collections/stepfun-ai/step3-vl-10b"&gt;&lt;strong&gt;Step-3-VL-10B&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;We are super excited to host our first AMA tomorrow in this community. Our participants include CEO, CTO, Chief Scientist, LLM Researchers.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Participants&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Ok_Reach_5122"&gt;u/Ok_Reach_5122&lt;/a&gt; (Co-founder &amp;amp; CEO of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/bobzhuyb"&gt;u/bobzhuyb&lt;/a&gt; (Co-founder &amp;amp; CTO of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/user/Lost-Nectarine1016"&gt;u/Lost-Nectarine1016&lt;/a&gt; (Co-founder &amp;amp; Chief Scientist of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Elegant-Sale-1328"&gt;u/Elegant-Sale-1328&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/SavingsConclusion298"&gt;u/SavingsConclusion298&lt;/a&gt; (Post-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Spirited_Spirit3387"&gt;u/Spirited_Spirit3387&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/These-Nothing-8564/"&gt;u/These-Nothing-8564&lt;/a&gt; (Technical Project Manager)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Either-Beyond-7395"&gt;u/Either-Beyond-7395&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Human_Ad_162"&gt;u/Human_Ad_162&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Icy_Dare_3866"&gt;u/Icy_Dare_3866&lt;/a&gt; (Post-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Big-Employee5595"&gt;u/Big-Employee5595&lt;/a&gt; (Agent Algorithms Lead&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run 8 - 11 AM PST, Feburary 19th. The StepFun team will monitor and answer questions over the 24 hours after the live session.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StepFun_ai"&gt; /u/StepFun_ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T07:15:35+00:00</published>
  </entry>
</feed>
