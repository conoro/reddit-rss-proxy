<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-19T14:06:02+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1o9xf4q</id>
    <title>[Experiment] Qwen3-VL-8B VS Qwen2.5-VL-7B test results</title>
    <updated>2025-10-18T14:33:56+00:00</updated>
    <author>
      <name>/u/Unbreakable_ryan</name>
      <uri>https://old.reddit.com/user/Unbreakable_ryan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9xf4q/experiment_qwen3vl8b_vs_qwen25vl7b_test_results/"&gt; &lt;img alt="[Experiment] Qwen3-VL-8B VS Qwen2.5-VL-7B test results" src="https://external-preview.redd.it/MWdpMThpMmdzdnZmMSlMdchWVQEhhJbqcRpTSv2Il4U_vTm8zPYXwK-Tk6g_.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=63ce1d6d2b0cd265b89c2d1f2c44c8a926ac5d47" title="[Experiment] Qwen3-VL-8B VS Qwen2.5-VL-7B test results" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt;&lt;br /&gt; I tested the brand-new &lt;strong&gt;Qwen3-VL-8B&lt;/strong&gt; against &lt;strong&gt;Qwen2.5-VL-7B&lt;/strong&gt; on the same set of visual reasoning tasks — OCR, chart analysis, multimodal QA, and instruction following.&lt;br /&gt; Despite being only 1B parameters larger, Qwen3-VL shows a &lt;strong&gt;&lt;em&gt;clear generation-to-generation leap&lt;/em&gt;&lt;/strong&gt; and delivers more accurate, nuanced, and faster multimodal reasoning.&lt;/p&gt; &lt;h1&gt;1. Setup&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Environment:&lt;/strong&gt; Local inference&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hardware:&lt;/strong&gt; Mac Air M4, 8-core GPU, 24 GB VRAM&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model format:&lt;/strong&gt; gguf, Q4&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Tasks tested:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Visual perception (receipts, invoice)&lt;/li&gt; &lt;li&gt;Visual captioning (photos)&lt;/li&gt; &lt;li&gt;Visual reasoning (business data)&lt;/li&gt; &lt;li&gt;Multimodal Fusion (does paragraph match figure)&lt;/li&gt; &lt;li&gt;Instruction following (structured answers)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Each prompt + image pair was fed to both models, using identical context.&lt;/p&gt; &lt;h1&gt;2. Evaluation Criteria&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Visual Perception&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Metric&lt;/strong&gt;: Correctly identifies text, objects, and layout.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Why It Matters&lt;/strong&gt;: This reflects the model’s baseline visual IQ.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Visual Captioning&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Metric&lt;/strong&gt;: Generates natural language descriptions of images.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Why It Matters&lt;/strong&gt;: Bridges vision and language, showing the model can translate what it sees into coherent text.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Visual Reasoning&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Metric&lt;/strong&gt;: Reads chart trends and applies numerical logic.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Why It Matters&lt;/strong&gt;: Tests true multimodal reasoning ability, beyond surface-level recognition.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Multimodal Fusion&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Metric&lt;/strong&gt;: Connects image content with text context.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Why It Matters&lt;/strong&gt;: Demonstrates cross-attention strength—how well the model integrates multiple modalities.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Instruction Following&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Metric&lt;/strong&gt;: Obeys structured prompts, such as “answer in 3 bullets.”&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Why It Matters&lt;/strong&gt;: Reflects alignment quality and the ability to produce controllable outputs.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Efficiency&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Metric&lt;/strong&gt;: TTFT (time to first token) and decoding speed.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Why It Matters&lt;/strong&gt;: Determines local usability and user experience.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Note: all answers are verified by humans and ChatGPT5.&lt;/p&gt; &lt;h1&gt;3. Test Results Summary&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Visual Perception&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;ul&gt; &lt;li&gt;Qwen2.5-VL-7B: Score 5&lt;/li&gt; &lt;li&gt;Qwen3-VL-8B: Score 8&lt;/li&gt; &lt;li&gt;Winner: Qwen3-VL-8B&lt;/li&gt; &lt;li&gt;Notes: Qwen3-VL-8B identify all the elements in the pic but fail the first and final calculation (the answer is 480.96 and 976.94). In comparison, Qwen2.5-VL-7B could not even understand the meaning of all the elements in the pic (there are two tourists) though the calculation is correct.&lt;/li&gt; &lt;/ul&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Visual Captioning&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;ul&gt; &lt;li&gt;Qwen2.5-VL-7B: Score 6.5&lt;/li&gt; &lt;li&gt;Qwen3-VL-8B: Score 9&lt;/li&gt; &lt;li&gt;Winner: Qwen3-VL-8B&lt;/li&gt; &lt;li&gt;Notes: Qwen3-VL-8B is more accurate, detailed, and has better scene understanding. (for example, identify Christmas Tree and Milkis). In contrary, Qwen2.5-VL-7B Gets the gist, but makes several misidentifications and lacks nuance.&lt;/li&gt; &lt;/ul&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Visual Reasoning&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;ul&gt; &lt;li&gt;Qwen2.5-VL-7B: Score 8&lt;/li&gt; &lt;li&gt;Qwen3-VL-8B: Score 9&lt;/li&gt; &lt;li&gt;Winner: Qwen3-VL-8B&lt;/li&gt; &lt;li&gt;Notes: Both models show the basically correct reasoning of the charts and one or two numeric errors. Qwen3-VL-8B is better at analysis/insight which indicates the key shifts while Qwen2.5-VL-7B has a clearer structure.&lt;/li&gt; &lt;/ul&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Multimodal Fusion&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;ul&gt; &lt;li&gt;Qwen2.5-VL-7B: Score 7&lt;/li&gt; &lt;li&gt;Qwen3-VL-8B: Score 9&lt;/li&gt; &lt;li&gt;Winner: Qwen3-VL-8B&lt;/li&gt; &lt;li&gt;Notes: The reasoning of Qwen3-VL-8B is correct, well-supported, and compelling with slight round up for some percentages, while that of Qwen2.5-VL-7B shows Incorrect data reference.&lt;/li&gt; &lt;/ul&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Instruction Following&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;ul&gt; &lt;li&gt;Qwen2.5-VL-7B: Score 8&lt;/li&gt; &lt;li&gt;Qwen3-VL-8B: Score 8.5&lt;/li&gt; &lt;li&gt;Winner: Qwen3-VL-8B&lt;/li&gt; &lt;li&gt;Notes: The summary from Qwen3-VL-8B is more faithful and nuanced, but more wordy. The suammry of Qwen2.5-VL-7B is cleaner and easier to read but misses some details.&lt;/li&gt; &lt;/ul&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Decode Speed&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;ul&gt; &lt;li&gt;Qwen2.5-VL-7B: 11.7–19.9t/s&lt;/li&gt; &lt;li&gt;Qwen3-VL-8B: 15.2–20.3t/s&lt;/li&gt; &lt;li&gt;Winner: Qwen3-VL-8B&lt;/li&gt; &lt;li&gt;Notes: 15–60% faster.&lt;/li&gt; &lt;/ul&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;TTFT&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;ul&gt; &lt;li&gt;Qwen2.5-VL-7B: 5.9–9.9s&lt;/li&gt; &lt;li&gt;Qwen3-VL-8B: 4.6–7.1s&lt;/li&gt; &lt;li&gt;Winner: Qwen3-VL-8B&lt;/li&gt; &lt;li&gt;Notes: 20–40% faster.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;4. Example Prompts&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Visual perception:&lt;/strong&gt; “Extract the total amount and payment date from this invoice.”&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Visual captioning&lt;/strong&gt;: &amp;quot;Describe this photo&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Visual reasoning:&lt;/strong&gt; “From this chart, what’s the trend from 1963 to 1990?”&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multimodal Fusion:&lt;/strong&gt; “Does the table in the image support the written claim: Europe is the dominant market for Farmed Caviar?”&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Instruction following&lt;/strong&gt; “Summarize this poster in exactly 3 bullet points.”&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;5. Summary &amp;amp; Takeaway&lt;/h1&gt; &lt;p&gt;The comparison does not demonstrate just a minor version bump, but a generation leap:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Qwen3-VL-8B consistently outperforms in &lt;strong&gt;Visual reasoning&lt;/strong&gt;, &lt;strong&gt;Multimodal fusion, Instruction following,&lt;/strong&gt; and especially &lt;strong&gt;Visual perception and Visual captioning.&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Qwen3-VL-8B produces more &lt;strong&gt;faithful and nuanced answers&lt;/strong&gt;, often giving richer context and insights. (however, conciseness is the tradeoff). Thus, users who value &lt;strong&gt;accuracy and depth&lt;/strong&gt; should prefer Qwen3, while those who want &lt;strong&gt;conciseness with less cognitive load&lt;/strong&gt; might tolerate Qwen2.5.&lt;/li&gt; &lt;li&gt;Qwen3’s mistakes are easier for humans to correct (eg, some numeric errors), whereas Qwen2.5 can mislead due to &lt;strong&gt;deeper misunderstandings&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Qwen3 not only &lt;strong&gt;improves quality but also reduces latency&lt;/strong&gt;, improving user experience.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Unbreakable_ryan"&gt; /u/Unbreakable_ryan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/t0mzpl2gsvvf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9xf4q/experiment_qwen3vl8b_vs_qwen25vl7b_test_results/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o9xf4q/experiment_qwen3vl8b_vs_qwen25vl7b_test_results/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-18T14:33:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1oao6k1</id>
    <title>Open Source Project to generate AI documents/presentations/reports via API: Apache 2.0</title>
    <updated>2025-10-19T12:18:21+00:00</updated>
    <author>
      <name>/u/goodboydhrn</name>
      <uri>https://old.reddit.com/user/goodboydhrn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;We've been building Presenton which is an open source project which helps to generate AI documents/presentations/reports via API and through UI.&lt;/p&gt; &lt;p&gt;It works on Bring Your Own Template model, which means you will have to use your existing PPTX/PDF file to create a template which can then be used to generate documents easily.&lt;/p&gt; &lt;p&gt;It supports Ollama and all major LLM providers, so you can either run it locally or using most powerful models to generate AI documents.&lt;/p&gt; &lt;p&gt;You can operate it in two steps:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Generate Template&lt;/strong&gt;: Templates are a collection of React components internally. So, you can use your existing PPTX file to generate template using AI. We have a workflow that will help you vibe code your template on your favourite IDE.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Generate Document:&lt;/strong&gt; After the template is ready you can reuse the template to generate infinite number of documents/presentations/reports using AI or directly through JSON. Every template exposes a JSON schema, which can also be used to generate documents in non-AI fashion(for times when you want precison).&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Our internal engine has best fidelity for HTML to PPTX conversion, so any template will basically work.&lt;/p&gt; &lt;p&gt;Community has loved us till now with 20K+ docker downloads, 2.5K stars and ~500 forks. Would love for you guys to checkout let us know if it was helpful or else feedback on making it useful for you.&lt;/p&gt; &lt;p&gt;Checkout website for more detail: &lt;a href="https://presenton.ai/"&gt;https://presenton.ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We have a very elaborate docs, checkout here: &lt;a href="https://docs.presenton.ai/"&gt;https://docs.presenton.ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/presenton/presenton"&gt;https://github.com/presenton/presenton&lt;/a&gt;&lt;/p&gt; &lt;p&gt;have a great day!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/goodboydhrn"&gt; /u/goodboydhrn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oao6k1/open_source_project_to_generate_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oao6k1/open_source_project_to_generate_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oao6k1/open_source_project_to_generate_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T12:18:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1oaoprv</id>
    <title>llama-swap: Automatic unloading after timeout + multiple started models + rules which models can be loaded same time without unloading all of them?</title>
    <updated>2025-10-19T12:44:43+00:00</updated>
    <author>
      <name>/u/Pure_Force8771</name>
      <uri>https://old.reddit.com/user/Pure_Force8771</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Automatic unloading solved with ttl. Do not try to put it in macro, it doesn't work.&lt;/p&gt; &lt;p&gt;How to change setup, so multiple models could be loaded? (Groups aren't exactly what I am searching for I guess, because it would not allow to have loaded gwen 30B and same time qwen 4B and then unload and load qwen thinking 4B instead of qwen 4B, as I understood it will unload both models and load qwen 30b and qwen thinking 4B together again, which creates delay of loading big model again.)&lt;/p&gt; &lt;p&gt;How to specify which models can be loaded together at a given time?&lt;/p&gt; &lt;p&gt;my config:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;listen: 0.0.0.0:8080 healthCheckTimeout: 120 macros: llama-server: &amp;gt; /app/llama-server --host 0.0.0.0 --port ${PORT} --n-gpu-layers 99 --cache-type-k f16 --cache-type-v f16 --ctx-size 32768 --threads 14 --threads-batch 14 --batch-size 2048 --ubatch-size 512 --cont-batching --parallel 1 --mlock models: /home/kukuskas/llama-models models: gpt-3.5-small: cmd: | ${llama-server} --model ${models}/gpt-oss-20b-MXFP4.gguf ttl: 600 qwen-coder-max: cmd: | ${llama-server} --model ${models}/Qwen3-Coder-30B-A3B-Instruct-Q6_K.gguf --ctx-size 65536 --defrag-thold 0.1 ttl: 600 blacksheep-max-uncensored: cmd: | ${llama-server} --model ${models}/BlackSheep-24B.Q6_K.gguf ttl: 600 dolphin-small-uncensored: cmd: | ${llama-server} --model ${models}/dolphin-2.8-mistral-7b-v02-Q8_0.gguf --threads 12 --threads-batch 12 ttl: 600 qwen-tiny-thinking: cmd: | ${llama-server} --model ${models}/Qwen3-4B-Thinking-2507-Q8_0.gguf --threads 12 --threads-batch 12 ttl: 300 qwen-tiny: cmd: | ${llama-server} --model ${models}/Qwen3-4B-Instruct-2507-Q8_0.gguf --threads 12 --threads-batch 12 --parallel 2 ttl: 300 qwen-coder-ultra: cmd: | ${llama-server} --model ${models}/Qwen3-Coder-30B-A3B-Instruct-UD-Q8_K_XL.gguf --ctx-size 65536 --defrag-thold 0.1 ttl: 600 qwen-ultra: cmd: | ${llama-server} --model ${models}/Qwen3-30B-A3B-Q8_0.gguf --ctx-size 65536 --defrag-thold 0.1 ttl: 600 &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pure_Force8771"&gt; /u/Pure_Force8771 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oaoprv/llamaswap_automatic_unloading_after_timeout/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oaoprv/llamaswap_automatic_unloading_after_timeout/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oaoprv/llamaswap_automatic_unloading_after_timeout/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T12:44:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1oaiykq</id>
    <title>Intel Core Ultra 9 285HX SODIMM slots for up to 256GB of DDR5-4800 ECC memory</title>
    <updated>2025-10-19T06:55:26+00:00</updated>
    <author>
      <name>/u/MundanePercentage674</name>
      <uri>https://old.reddit.com/user/MundanePercentage674</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oaiykq/intel_core_ultra_9_285hx_sodimm_slots_for_up_to/"&gt; &lt;img alt="Intel Core Ultra 9 285HX SODIMM slots for up to 256GB of DDR5-4800 ECC memory" src="https://external-preview.redd.it/bSme4l8osOBTMgnmz9Z2p_Cb-vjjWGgIuVmAKK2EufE.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7d2c7ae2a5b189631c7c4e36a5c8ace7302c02b2" title="Intel Core Ultra 9 285HX SODIMM slots for up to 256GB of DDR5-4800 ECC memory" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/yldrkkq2o0wf1.jpg?width=1200&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=b63e449fe425c9d20d84fdb755e4f3f9b7dc8f31"&gt;https://preview.redd.it/yldrkkq2o0wf1.jpg?width=1200&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=b63e449fe425c9d20d84fdb755e4f3f9b7dc8f31&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://liliputing.com/minisforum-ms-02-ultra-is-a-compact-workstation-with-intel-core-ultra-9-285hx-and-3-pcie-slots/"&gt;https://liliputing.com/minisforum-ms-02-ultra-is-a-compact-workstation-with-intel-core-ultra-9-285hx-and-3-pcie-slots/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MundanePercentage674"&gt; /u/MundanePercentage674 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oaiykq/intel_core_ultra_9_285hx_sodimm_slots_for_up_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oaiykq/intel_core_ultra_9_285hx_sodimm_slots_for_up_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oaiykq/intel_core_ultra_9_285hx_sodimm_slots_for_up_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T06:55:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1oap367</id>
    <title>Total noob here who wants to run a local LLM to build my own coach and therapist chatbot</title>
    <updated>2025-10-19T13:02:02+00:00</updated>
    <author>
      <name>/u/tokyothrowie</name>
      <uri>https://old.reddit.com/user/tokyothrowie</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As the title says, I’m an absolute beginner when it comes to local LLMs. I’ve been using ChatGPT, Claude, and Perplexity daily, but that’s about it. I work in hospitality and mostly with English speakers, but English is my second language.&lt;/p&gt; &lt;p&gt;I’ve been thinking about building a local LLM that could act as a personal coach and therapist. I’ve been in therapy with a certified therapist for the past 18 months, and she’s allowed me to record every session. Having those sessions twice a month has been a game changer for me.&lt;/p&gt; &lt;p&gt;The thing is, I pay around $100 per 45-minute session out of pocket, and I’m currently focused on paying off some debt. So, I’d like to reduce my sessions to once every 4–6 weeks instead and supplement them with something AI-based. My therapist is totally on board with this idea.&lt;/p&gt; &lt;p&gt;My main concern, though, is privacy. I don’t want to upload any personal data to random AI tools, which is why I want to explore a local setup. The problem is, I can’t afford new hardware right now I only have a Mac Mini M3 Pro. My goal is to run a local LLM offline, ideally with voice input, and have it push me like David Goggins but also use the same therapeutic techniques my therapist does.&lt;/p&gt; &lt;p&gt;The issue is.. I have zero clue where to start or if this is even possible. I see people on YouTube using tools like NotebookLM for personal stuff like Tiago Forte in one of his videos but I’m just too paranoid to trust big tech companies with something this personal.&lt;/p&gt; &lt;p&gt;Any advice, resources, or starting points would be super appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tokyothrowie"&gt; /u/tokyothrowie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oap367/total_noob_here_who_wants_to_run_a_local_llm_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oap367/total_noob_here_who_wants_to_run_a_local_llm_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oap367/total_noob_here_who_wants_to_run_a_local_llm_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T13:02:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1oahyzi</id>
    <title>Unable to find the attach feature in Jan.ai for documents and images.</title>
    <updated>2025-10-19T05:54:27+00:00</updated>
    <author>
      <name>/u/Ok-Knee-694</name>
      <uri>https://old.reddit.com/user/Ok-Knee-694</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I came across this &lt;a href="http://Jan.ai"&gt;Jan.ai&lt;/a&gt; software for desktop for its privacy-first feature. I decided to use Mistral-7B-Instruct-v0.3 LLM model for document analysis, but later came to realize that this software doesn't have a document attachment option at all. Are there any other ways to make the model read my document?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Knee-694"&gt; /u/Ok-Knee-694 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oahyzi/unable_to_find_the_attach_feature_in_janai_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oahyzi/unable_to_find_the_attach_feature_in_janai_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oahyzi/unable_to_find_the_attach_feature_in_janai_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T05:54:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1oaidyk</id>
    <title>A local LLM that I can feed my diary entries?</title>
    <updated>2025-10-19T06:20:03+00:00</updated>
    <author>
      <name>/u/22Megabits</name>
      <uri>https://old.reddit.com/user/22Megabits</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;Would it be possible for me to run an LLM on my PC that I can feed my journal entries to?&lt;/p&gt; &lt;p&gt;My main use would be to ask it for help remembering certain events: ‘Who was my 5th grade maths teacher’ ‘Where did I go on holiday over December in 2013’ etc.&lt;/p&gt; &lt;p&gt;Is that something that’s even possible to locally?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/22Megabits"&gt; /u/22Megabits &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oaidyk/a_local_llm_that_i_can_feed_my_diary_entries/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oaidyk/a_local_llm_that_i_can_feed_my_diary_entries/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oaidyk/a_local_llm_that_i_can_feed_my_diary_entries/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T06:20:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1oaj0gb</id>
    <title>GPU rental experiences</title>
    <updated>2025-10-19T06:58:50+00:00</updated>
    <author>
      <name>/u/somealusta</name>
      <uri>https://old.reddit.com/user/somealusta</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I have some spare GPUs and servers, some at home and some at datacenter.&lt;br /&gt; I would like to know peoples experiences in general about renting your own GPUs or just using these services for inference. How do they work and are people actually using them. &lt;/p&gt; &lt;p&gt;So I am speaking about &lt;a href="http://vast.ai"&gt;vast.ai&lt;/a&gt; or similar (which other there are?) where you can rent your own or use someone elses hardware. Do you use them and if yes how much you use them and for what?&lt;br /&gt; Have they been working flawlessly or do you prefer something else? &lt;/p&gt; &lt;p&gt;For me, earning about 1,2 dollars per server with 5090 does not sound much, but if they are just sitting here under my desk, maybe I should put them to work? Electricity here is sometimes very cheap, so something should be left. What other services there are than vast.ai?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/somealusta"&gt; /u/somealusta &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oaj0gb/gpu_rental_experiences/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oaj0gb/gpu_rental_experiences/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oaj0gb/gpu_rental_experiences/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T06:58:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1o9s60k</id>
    <title>Bee-8B, "fully open 8B Multimodal LLM designed to close the performance gap with proprietary models"</title>
    <updated>2025-10-18T10:21:03+00:00</updated>
    <author>
      <name>/u/beneath_steel_sky</name>
      <uri>https://old.reddit.com/user/beneath_steel_sky</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9s60k/bee8b_fully_open_8b_multimodal_llm_designed_to/"&gt; &lt;img alt="Bee-8B, &amp;quot;fully open 8B Multimodal LLM designed to close the performance gap with proprietary models&amp;quot;" src="https://external-preview.redd.it/Ncd1u3KqHI3q8cl_bGxSrWsYruQjV7vJ0ceKV7GWN6Y.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ae9ffa18e4d3b03e94f5458c4c02b0fc641812e8" title="Bee-8B, &amp;quot;fully open 8B Multimodal LLM designed to close the performance gap with proprietary models&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/beneath_steel_sky"&gt; /u/beneath_steel_sky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Open-Bee/Bee-8B-RL"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9s60k/bee8b_fully_open_8b_multimodal_llm_designed_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o9s60k/bee8b_fully_open_8b_multimodal_llm_designed_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-18T10:21:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1oa3u2d</id>
    <title>The size difference of gpt-oss-120b vs it's abliterated version</title>
    <updated>2025-10-18T18:48:10+00:00</updated>
    <author>
      <name>/u/iamkucuk</name>
      <uri>https://old.reddit.com/user/iamkucuk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was away from the locally hosted models, so please forgive my ignorance.&lt;/p&gt; &lt;p&gt;Here are two versions of gpt-oss-120b:&lt;/p&gt; &lt;p&gt;&lt;a href="https://ollama.com/library/gpt-oss"&gt;https://ollama.com/library/gpt-oss&lt;/a&gt;&lt;br /&gt; &lt;a href="https://ollama.com/huihui_ai/gpt-oss-abliterated"&gt;https://ollama.com/huihui_ai/gpt-oss-abliterated&lt;/a&gt;&lt;/p&gt; &lt;p&gt;As you can see, one takes 88 GB and the other takes 65 GB, and the difference shows when they are loaded as well. I thought they were both 4-bit. Would someone be able to explain where the discrepancy is coming from? And if any abliterated versions of the original model's quant occupy the same space?&lt;/p&gt; &lt;p&gt;Another question would be, I can see the GGUF versions of gpt-oss. Why would we need GGUF versions, as the model itself already is quantized?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iamkucuk"&gt; /u/iamkucuk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oa3u2d/the_size_difference_of_gptoss120b_vs_its/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oa3u2d/the_size_difference_of_gptoss120b_vs_its/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oa3u2d/the_size_difference_of_gptoss120b_vs_its/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-18T18:48:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1oa1zfp</id>
    <title>3x Price Increase on Llama API</title>
    <updated>2025-10-18T17:36:52+00:00</updated>
    <author>
      <name>/u/Player06</name>
      <uri>https://old.reddit.com/user/Player06</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This went pretty under the radar, but a few days ago the 'Meta: Llama 3 70b' model went from 0.13c/M to 0.38c/M.&lt;/p&gt; &lt;p&gt;I noticed because I run one of the apps listed in the top 10 consumers of that model (the one with the weird penguin icon). I cannot find any evidence of this online, except my openrouter bill.&lt;/p&gt; &lt;p&gt;I ditched my local inference last month because the openrouter Llama price looked so good. But now I got rug pulled.&lt;/p&gt; &lt;p&gt;Did anybody else notice this? Or am I crazy and the prices never changed? It feels unusual for a provider to bump their API prices this much.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Player06"&gt; /u/Player06 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oa1zfp/3x_price_increase_on_llama_api/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oa1zfp/3x_price_increase_on_llama_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oa1zfp/3x_price_increase_on_llama_api/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-18T17:36:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1oa671p</id>
    <title>Could you recommend good LLM models for heavier stories that include NSFW content?</title>
    <updated>2025-10-18T20:19:23+00:00</updated>
    <author>
      <name>/u/Sr_M_Ghost</name>
      <uri>https://old.reddit.com/user/Sr_M_Ghost</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm currently using Deep Seek R2 0528, but I'd like other models that are better suited to this type of content.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sr_M_Ghost"&gt; /u/Sr_M_Ghost &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oa671p/could_you_recommend_good_llm_models_for_heavier/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oa671p/could_you_recommend_good_llm_models_for_heavier/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oa671p/could_you_recommend_good_llm_models_for_heavier/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-18T20:19:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1oa8klx</id>
    <title>Open source custom implementation of GPT-5 Pro / Gemini Deepthink now supports local models</title>
    <updated>2025-10-18T21:54:34+00:00</updated>
    <author>
      <name>/u/Ryoiki-Tokuiten</name>
      <uri>https://old.reddit.com/user/Ryoiki-Tokuiten</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oa8klx/open_source_custom_implementation_of_gpt5_pro/"&gt; &lt;img alt="Open source custom implementation of GPT-5 Pro / Gemini Deepthink now supports local models" src="https://external-preview.redd.it/ZnUwMzR3NXV5eHZmMTNcUS_6HPKChleMDpJ0qQU2p3V675TK2MrxaOnhSll8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=568275d60bed24ec215f8d24a426e506eab3f5d7" title="Open source custom implementation of GPT-5 Pro / Gemini Deepthink now supports local models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ryoiki-Tokuiten"&gt; /u/Ryoiki-Tokuiten &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/jak8lx5uyxvf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oa8klx/open_source_custom_implementation_of_gpt5_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oa8klx/open_source_custom_implementation_of_gpt5_pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-18T21:54:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1oaafyq</id>
    <title>3 3090's, room for one more?</title>
    <updated>2025-10-18T23:16:49+00:00</updated>
    <author>
      <name>/u/BusinessBookkeeper63</name>
      <uri>https://old.reddit.com/user/BusinessBookkeeper63</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oaafyq/3_3090s_room_for_one_more/"&gt; &lt;img alt="3 3090's, room for one more?" src="https://preview.redd.it/4wabpxvjcyvf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=abcb88de7be3c5396eac5805691a1be8d3309133" title="3 3090's, room for one more?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I am currently running 3 3090's and was thinking of adding one more. But as you can see, my case Thermaltake CTE750 Air has some free space, but not sure if it can fit another 3090.&lt;/p&gt; &lt;p&gt;I know, I know, I should have had a server rack but I was looking for a Local AI + relatively decent looking case, so this is what I landed on. The CTE 750 is big enough for 3 3090's, but not sure if I should be doing 4 given temps inside a closed case is probably going to rise quick. The third 3090 needs a custom mount and sits on the side of the case in this picture, but it rests on the intake fans and I have screwed the standing with 3 screws. I have no idea, where I could fit the 4th.&lt;/p&gt; &lt;p&gt;Any suggestions on how I could do 4 3090;s in this case or if anyone has done this before?&lt;/p&gt; &lt;p&gt;Also looking for suggestions on my cooling. Currently it has intake from bottom, front, back and sides and outtake on top only. This is somewhat based on the CTE design, but open to other suggestions. Another option, is to eventually do water cooling to save on some space and keep things cooler, but that's a project kept for December.&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BusinessBookkeeper63"&gt; /u/BusinessBookkeeper63 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4wabpxvjcyvf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oaafyq/3_3090s_room_for_one_more/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oaafyq/3_3090s_room_for_one_more/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-18T23:16:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1oa29de</id>
    <title>Drummer's Cydonia and Magidonia 24B v4.2.0</title>
    <updated>2025-10-18T17:47:49+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oa29de/drummers_cydonia_and_magidonia_24b_v420/"&gt; &lt;img alt="Drummer's Cydonia and Magidonia 24B v4.2.0" src="https://external-preview.redd.it/texRxv_iJ0Ni14pBUMNg-YEbpRERebh0ufaJ753mjSs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ab199da434abf76101631f6569f8bb80838d47f9" title="Drummer's Cydonia and Magidonia 24B v4.2.0" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Magidonia is Cydonia using Magistral 2509 base.&lt;/p&gt; &lt;p&gt;Magidonia variant: &lt;a href="https://huggingface.co/TheDrummer/Magidonia-24B-v4.2.0"&gt;https://huggingface.co/TheDrummer/Magidonia-24B-v4.2.0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Cydonia (Small 3.2) variant: &lt;a href="https://huggingface.co/TheDrummer/Cydonia-24B-v4.2.0"&gt;https://huggingface.co/TheDrummer/Cydonia-24B-v4.2.0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;4.2.0 is an upgrade from 4.1 in regards to creativity. Enjoy!&lt;/p&gt; &lt;p&gt;Does anyone have a base to recommend for finetuning? Waiting for GLM Air 4.6 to come out :^)&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;By the way, Huggingface has restricted storage in my account and I'm having a harder time doing my open-source work for the community. I'll be all out of space after a few days of work thanks to their storage restriction. &lt;/p&gt; &lt;p&gt;I tried contacting them via [&lt;a href="mailto:billing@hf.co"&gt;billing@hf.co&lt;/a&gt;](mailto:&lt;a href="mailto:billing@hf.co"&gt;billing@hf.co&lt;/a&gt;) but they told me to make my case to [&lt;a href="mailto:models@hf.co"&gt;models@hf.co&lt;/a&gt;](mailto:&lt;a href="mailto:models@hf.co"&gt;models@hf.co&lt;/a&gt;) . I haven't received a response from &lt;em&gt;that&lt;/em&gt; team yet. Other employees I've reached out to recommended that I pay around $200 / mo to get the storage I need, I think.&lt;/p&gt; &lt;p&gt;At this point I believe they're not interested in giving me an exception. I got bundled up with those who upload 1T models, I guess? I'm not sure what to do next, but I might have to start deleting models. Let me know if you guys have any ideas!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Magidonia-24B-v4.2.0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oa29de/drummers_cydonia_and_magidonia_24b_v420/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oa29de/drummers_cydonia_and_magidonia_24b_v420/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-18T17:47:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1oapgtj</id>
    <title>I want to have a local llm server for my house - just focused on coding assistant - what would be a reasonable spec for that?</title>
    <updated>2025-10-19T13:19:49+00:00</updated>
    <author>
      <name>/u/gameguy56</name>
      <uri>https://old.reddit.com/user/gameguy56</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I don't need and am not interested in video/image generation - just want something to work with me on coding stuff. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gameguy56"&gt; /u/gameguy56 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oapgtj/i_want_to_have_a_local_llm_server_for_my_house/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oapgtj/i_want_to_have_a_local_llm_server_for_my_house/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oapgtj/i_want_to_have_a_local_llm_server_for_my_house/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T13:19:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1oafumx</id>
    <title>Own your AI: Learn how to fine-tune Gemma 3 270M and run it on-device</title>
    <updated>2025-10-19T03:51:17+00:00</updated>
    <author>
      <name>/u/phone_radio_tv</name>
      <uri>https://old.reddit.com/user/phone_radio_tv</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oafumx/own_your_ai_learn_how_to_finetune_gemma_3_270m/"&gt; &lt;img alt="Own your AI: Learn how to fine-tune Gemma 3 270M and run it on-device" src="https://external-preview.redd.it/1REAPLhpZknr_BaLbzQgHufo9VWmTuOWut1-PIVgTuo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b3c98ea4dc27d76e781b86ebeda6b0c583cc503d" title="Own your AI: Learn how to fine-tune Gemma 3 270M and run it on-device" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/phone_radio_tv"&gt; /u/phone_radio_tv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://developers.googleblog.com/en/own-your-ai-fine-tune-gemma-3-270m-for-on-device/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oafumx/own_your_ai_learn_how_to_finetune_gemma_3_270m/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oafumx/own_your_ai_learn_how_to_finetune_gemma_3_270m/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T03:51:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1o9xiza</id>
    <title>dgx, it's useless , High latency</title>
    <updated>2025-10-18T14:38:06+00:00</updated>
    <author>
      <name>/u/Illustrious-Swim9663</name>
      <uri>https://old.reddit.com/user/Illustrious-Swim9663</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9xiza/dgx_its_useless_high_latency/"&gt; &lt;img alt="dgx, it's useless , High latency" src="https://preview.redd.it/wwroq3nbtvvf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d419416ab7812d4f7c564531795007c015a4c85f" title="dgx, it's useless , High latency" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ahmad posted a tweet where DGX latency is high : &lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/TheAhmadOsman/status/1979408446534398403?t=COH4pw0-8Za4kRHWa2ml5A&amp;amp;s=19"&gt;https://x.com/TheAhmadOsman/status/1979408446534398403?t=COH4pw0-8Za4kRHWa2ml5A&amp;amp;s=19&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Illustrious-Swim9663"&gt; /u/Illustrious-Swim9663 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wwroq3nbtvvf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9xiza/dgx_its_useless_high_latency/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o9xiza/dgx_its_useless_high_latency/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-18T14:38:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1oakrdm</id>
    <title>Drop your underrated models you run LOCALLY</title>
    <updated>2025-10-19T08:52:22+00:00</updated>
    <author>
      <name>/u/Adventurous-Gold6413</name>
      <uri>https://old.reddit.com/user/Adventurous-Gold6413</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Preferably within the 0.2b -32b range, or MoEs up to 140b&lt;/p&gt; &lt;p&gt;I’m on a LLM downloading spree, and wanna fill up a 2tb SSD with them. &lt;/p&gt; &lt;p&gt;Can be any use case. Just make sure to mention the use case too &lt;/p&gt; &lt;p&gt;Thank you ✌️&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Adventurous-Gold6413"&gt; /u/Adventurous-Gold6413 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oakrdm/drop_your_underrated_models_you_run_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oakrdm/drop_your_underrated_models_you_run_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oakrdm/drop_your_underrated_models_you_run_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T08:52:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1oa98jf</id>
    <title>Made a website to track 348 benchmarks across 188 models.</title>
    <updated>2025-10-18T22:22:42+00:00</updated>
    <author>
      <name>/u/Odd_Tumbleweed574</name>
      <uri>https://old.reddit.com/user/Odd_Tumbleweed574</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oa98jf/made_a_website_to_track_348_benchmarks_across_188/"&gt; &lt;img alt="Made a website to track 348 benchmarks across 188 models." src="https://preview.redd.it/omjxzqi82yvf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=df31289ac6e45db697b70c3a9add3e087585f736" title="Made a website to track 348 benchmarks across 188 models." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all, I've been building a website from a while ago in which we track the benchmark results from the official papers / model cards that the labs publish. &lt;/p&gt; &lt;p&gt;I thought it would be interesting to compile everything in one place to fill in the gaps on each model release.&lt;br /&gt; All the data is open in Github and all scores have references to the original posts.&lt;/p&gt; &lt;p&gt;&lt;a href="https://llm-stats.com/benchmarks"&gt;https://llm-stats.com/benchmarks&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Feel free to provide candid feedback. &lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;**We don't think this is the best approach yet**. We're now building a way to replicate the results from the most interesting and useful benchmarks, but we understand that most of them haven't been created yet.&lt;/p&gt; &lt;p&gt;Current benchmarks are too simple and are not testing real capabilities. We're looking to build interesting, real world, independent benchmarks with held out data, but that can be easy to reproduce and extend.&lt;/p&gt; &lt;p&gt;Another thing we're currently doing is benchmarking across different inference providers to monitor and detect changes in quality of their service.&lt;/p&gt; &lt;p&gt;We're currently giving out up to $1k to people that want to explore ideas about new benchmarks / environments. Dm me for more information.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Odd_Tumbleweed574"&gt; /u/Odd_Tumbleweed574 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/omjxzqi82yvf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oa98jf/made_a_website_to_track_348_benchmarks_across_188/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oa98jf/made_a_website_to_track_348_benchmarks_across_188/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-18T22:22:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1oamo0k</id>
    <title>Gemma 4</title>
    <updated>2025-10-19T10:53:39+00:00</updated>
    <author>
      <name>/u/Brave-Hold-9389</name>
      <uri>https://old.reddit.com/user/Brave-Hold-9389</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;People are very very excited about the release of gemini 3.0 including me, but im more excited in the gemma family of models since they are based on gemini models and on top of that are open-sourced. And simce Gemini 3.0 is groundbreaking (apparently, like the pelican svg, robot svg, xbox svg, os etc tests), I am very curious about how will the gemma 4 models perform. And also, gemma 4 is going to be a big leap compared to gemma 3 coz It was based on gemini 2.0, not 2.5. So we are getting 2 genarational leaps!&lt;/p&gt; &lt;p&gt;When it will be released??&lt;/p&gt; &lt;p&gt;Gemma 1 was based on gemini 1 and was released ~1-2 months after gemini&lt;/p&gt; &lt;p&gt;Gemma 2 was based on gemini 1.5 and was released ~4 months after gemini 1.5&lt;/p&gt; &lt;p&gt;Gemma 3 was based on gemini 2 and was released ~1-2 months after gemini 2.0&lt;/p&gt; &lt;p&gt;So Gemma 4 might be released ~1-2 months after gemini 3??? Maybe???&lt;/p&gt; &lt;p&gt;What are your thoughts?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Brave-Hold-9389"&gt; /u/Brave-Hold-9389 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oamo0k/gemma_4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oamo0k/gemma_4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oamo0k/gemma_4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T10:53:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1oahpmx</id>
    <title>When you have little money but want to run big models</title>
    <updated>2025-10-19T05:38:21+00:00</updated>
    <author>
      <name>/u/alok_saurabh</name>
      <uri>https://old.reddit.com/user/alok_saurabh</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oahpmx/when_you_have_little_money_but_want_to_run_big/"&gt; &lt;img alt="When you have little money but want to run big models" src="https://a.thumbs.redditmedia.com/vj3j4Vjr082yd6wYwhFLXQpEt2Zp3s7yg7spOglJoq8.jpg" title="When you have little money but want to run big models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I live in India. Everything is expensive. Importers want hefty margin. Government want hefty tax. Rtx 6000 96gb which is possible to get for 7-8k usd in USA is impossible to find even for 11 lakhs(12-13k usd) in India. So we have a couple of friends 1) Juggad 2) Olx ( indian craigslists) 3) Other similar p2p sites like fb marketplace.&lt;/p&gt; &lt;p&gt;Let me show you what I built. 1) Dell T7910 - it has 7 pci slots. I can only get 5 to work. Found it on fb mp with 256 gb ddr4 2) 5 * 3090 from olx 3) 5 pci raisers amazon. These are hard to find for cheap. 4) 1300 watt additional power supply &lt;/p&gt; &lt;p&gt;There are only 4*3090 in this build 5th slot I am using for nvme extension.&lt;/p&gt; &lt;p&gt;Total cost for this build of 96gb vram is around 3.25 lakhs. ( Around 4.6k usd) This post is just for reference for those who are in a similar boat. Please understand there is a lot of difference between planning and execution. Keep +1 lakhs in hand for things that can go wrong.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/alok_saurabh"&gt; /u/alok_saurabh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1oahpmx"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oahpmx/when_you_have_little_money_but_want_to_run_big/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oahpmx/when_you_have_little_money_but_want_to_run_big/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T05:38:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1oanpdt</id>
    <title>Qwen3 Next support almost ready 🎉</title>
    <updated>2025-10-19T11:52:59+00:00</updated>
    <author>
      <name>/u/beneath_steel_sky</name>
      <uri>https://old.reddit.com/user/beneath_steel_sky</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oanpdt/qwen3_next_support_almost_ready/"&gt; &lt;img alt="Qwen3 Next support almost ready 🎉" src="https://external-preview.redd.it/i7eFNEDuUciRrfCZPE4vDbbnitlKFru9a-LhPWvWNKY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c40ba30707796f926638df0347f891c8e7cb6d0c" title="Qwen3 Next support almost ready 🎉" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/beneath_steel_sky"&gt; /u/beneath_steel_sky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/16095#issuecomment-3419600401"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oanpdt/qwen3_next_support_almost_ready/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oanpdt/qwen3_next_support_almost_ready/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T11:52:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1oak08e</id>
    <title>Apple M5 Max and Ultra will finally break monopoly of NVIDIA for AI interference</title>
    <updated>2025-10-19T08:02:23+00:00</updated>
    <author>
      <name>/u/inkberk</name>
      <uri>https://old.reddit.com/user/inkberk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oak08e/apple_m5_max_and_ultra_will_finally_break/"&gt; &lt;img alt="Apple M5 Max and Ultra will finally break monopoly of NVIDIA for AI interference" src="https://a.thumbs.redditmedia.com/h4jhl1-2PSdEVtcHTb5JaJVVUfcXqSVvVdD4T8fo5L0.jpg" title="Apple M5 Max and Ultra will finally break monopoly of NVIDIA for AI interference" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;According to &lt;a href="https://opendata.blender.org/benchmarks"&gt;https://opendata.blender.org/benchmarks&lt;/a&gt;&lt;br /&gt; The Apple M5 10-core GPU already scores 1732 - outperforming the M1 Ultra with 64 GPU cores.&lt;br /&gt; With simple math:&lt;br /&gt; Apple M5 Max 40-core GPU will score 7000 - that is league of M3 Ultra&lt;br /&gt; Apple M5 Ultra 80-core GPU will score 14000 on par with RTX 5090 and RTX Pro 6000! &lt;/p&gt; &lt;p&gt;Seems like it will be the best performance/memory/tdp/price deal.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/inkberk"&gt; /u/inkberk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1oak08e"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oak08e/apple_m5_max_and_ultra_will_finally_break/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oak08e/apple_m5_max_and_ultra_will_finally_break/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T08:02:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1oakwgs</id>
    <title>Stanford just dropped 5.5hrs worth of lectures on foundational LLM knowledge</title>
    <updated>2025-10-19T09:01:21+00:00</updated>
    <author>
      <name>/u/igorwarzocha</name>
      <uri>https://old.reddit.com/user/igorwarzocha</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oakwgs/stanford_just_dropped_55hrs_worth_of_lectures_on/"&gt; &lt;img alt="Stanford just dropped 5.5hrs worth of lectures on foundational LLM knowledge" src="https://preview.redd.it/2klkt23e91wf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=017d4a00c64748e6f3b664b4a89abc3602199d49" title="Stanford just dropped 5.5hrs worth of lectures on foundational LLM knowledge" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Enjoy? &lt;a href="https://www.youtube.com/@stanfordonline/videos"&gt;https://www.youtube.com/@stanfordonline/videos&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/igorwarzocha"&gt; /u/igorwarzocha &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2klkt23e91wf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oakwgs/stanford_just_dropped_55hrs_worth_of_lectures_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oakwgs/stanford_just_dropped_55hrs_worth_of_lectures_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T09:01:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
