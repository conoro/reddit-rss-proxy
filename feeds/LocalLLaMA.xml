<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-15T13:48:43+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1oxpm6q</id>
    <title>What kind of dataset was Sesame CSM-8B most likely trained on?</title>
    <updated>2025-11-15T11:40:45+00:00</updated>
    <author>
      <name>/u/Adept_Lawyer_4592</name>
      <uri>https://old.reddit.com/user/Adept_Lawyer_4592</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm curious about the Sesame CSM-8B model. Since the creators haven‚Äôt publicly released the full training data details, what type of dataset do you think it was most likely trained on?&lt;/p&gt; &lt;p&gt;Specifically:&lt;/p&gt; &lt;p&gt;What kinds of sources would a model like this typically use?&lt;/p&gt; &lt;p&gt;Would it include conversational datasets, roleplay data, coding data, multilingual corpora, web scrapes, etc.?&lt;/p&gt; &lt;p&gt;Anything known or inferred from benchmarks or behavior?&lt;/p&gt; &lt;p&gt;I‚Äôm mainly trying to understand what the dataset probably includes and why CSM-8B behaves noticeably ‚Äúsmarter‚Äù than other 7B‚Äì8B models like Moshi despite similar claimed training approaches.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Adept_Lawyer_4592"&gt; /u/Adept_Lawyer_4592 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxpm6q/what_kind_of_dataset_was_sesame_csm8b_most_likely/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxpm6q/what_kind_of_dataset_was_sesame_csm8b_most_likely/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oxpm6q/what_kind_of_dataset_was_sesame_csm8b_most_likely/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T11:40:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1oxm1xv</id>
    <title>Koboldcpp problem on Windows.</title>
    <updated>2025-11-15T07:59:13+00:00</updated>
    <author>
      <name>/u/Pretend-Pumpkin7506</name>
      <uri>https://old.reddit.com/user/Pretend-Pumpkin7506</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi. I was using LM Studio with my RTX 4080. I added a second graphics card, an RTX 5060. LM Studio uses the 5060 simply as memory expansion, placing no load on it, despite the settings being set to use both cards (I tried split and priority options). I want to try llama.cpp. I didn't understand how to run this program, so I downloaded koboldcpp. And I don't understand the problem. I'm trying to run gtp oss 120b. The model consists of two gguf files. I select the first one, and the cmd says that a multi-file model is defined, so everything is fine. But after loading, I ask a question, and the model just spits out a few incoherent words and then stops. It seems like the second model file didn't load. By the way, the RTX 5060 also didn't work. The program doesn't even load part of the model into its memory, despite the fact that I specified &amp;quot;ALL&amp;quot; GPU in the koboldcpp settings. This should have used both GPUs, right? I specified card number 1, the RTX 4080, as the priority. I also noticed in LM Studio that when I try to use two video cards, in addition to a performance drop from 10.8 to 10.2 tokens, the model has become more sluggish. It started displaying some unintelligible symbols and text in...Spanish? And the response itself is full of errors.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pretend-Pumpkin7506"&gt; /u/Pretend-Pumpkin7506 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxm1xv/koboldcpp_problem_on_windows/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxm1xv/koboldcpp_problem_on_windows/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oxm1xv/koboldcpp_problem_on_windows/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T07:59:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1oxqfcv</id>
    <title>New Open‚ÄëSource Local Agents for LM Studio</title>
    <updated>2025-11-15T12:24:41+00:00</updated>
    <author>
      <name>/u/Undici77</name>
      <uri>https://old.reddit.com/user/Undici77</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! I'm thrilled to announce three brand‚Äënew open‚Äësource projects that can supercharge your &lt;strong&gt;local LLM&lt;/strong&gt; workflows in LM Studio. They keep everything on‚Äëdevice, protect your privacy, and stay completely offline ‚Äì perfect for anyone building a self‚Äëhosted AI setup.&lt;/p&gt; &lt;h1&gt;üìÇ What‚Äôs new?&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;MCP Web Search Server&lt;/strong&gt; ‚Äì A privacy‚Äëfocused search agent that can query the web (or archives) without sending data to third‚Äëparty services.&lt;/li&gt; &lt;li&gt;üëâ &lt;a href="https://github.com/undici77/MCPWebSearch"&gt;https://github.com/undici77/MCPWebSearch&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;MCP Data Fetch Server&lt;/strong&gt; ‚Äì Securely fetches webpages and extracts clean content, links, metadata, or files, all inside a sandboxed environment.&lt;/li&gt; &lt;li&gt;üëâ &lt;a href="https://github.com/undici77/MCPDataFetchServer"&gt;https://github.com/undici77/MCPDataFetchServer&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;MCP File Server&lt;/strong&gt; ‚Äì Gives your LLM safe read/write access to the local filesystem, with full protection against path‚Äëtraversal and unwanted file types.&lt;/li&gt; &lt;li&gt;üëâ &lt;a href="https://github.com/undici77/MCPFileServer"&gt;https://github.com/undici77/MCPFileServer&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;üéâ Why you‚Äôll love them&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;All‚Äëlocal, all‚Äëprivate&lt;/strong&gt; ‚Äì No external API keys or cloud services required; everything runs on your own machine.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Seamless LM Studio integration&lt;/strong&gt; ‚Äì The agents appear as new tools in the UI, ready to use right away.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Open source &amp;amp; community‚Äëdriven&lt;/strong&gt; ‚Äì Inspect, modify, or extend any part of the codebase.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Sandboxed for safety&lt;/strong&gt; ‚Äì Each server isolates its operations, so your LLM can‚Äôt accidentally read or write outside a designated folder.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you‚Äôre experimenting with local LLMs, these agents give you instant access to web search, data fetching, and file handling without compromising security or privacy. Give them a spin and see how they expand what LM Studio can do!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Undici77"&gt; /u/Undici77 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxqfcv/new_opensource_local_agents_for_lm_studio/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxqfcv/new_opensource_local_agents_for_lm_studio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oxqfcv/new_opensource_local_agents_for_lm_studio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T12:24:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1ox9fzy</id>
    <title>Is there a self-hosted, open-source plug-and-play RAG solution?</title>
    <updated>2025-11-14T21:41:09+00:00</updated>
    <author>
      <name>/u/anedisi</name>
      <uri>https://old.reddit.com/user/anedisi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know about Ollama, llama-server, vLLM and all the other options for hosting LLMs, but I‚Äôm looking for something similar for RAG that I can self-host.&lt;/p&gt; &lt;p&gt;Basically: I want to store scraped websites, upload PDF files, and similar documents ‚Äî and have a simple system that handles: ‚Ä¢ vector DB storage ‚Ä¢ chunking ‚Ä¢ data ingestion ‚Ä¢ querying the vector DB when a user asks something ‚Ä¢ sending that to the LLM for final output&lt;/p&gt; &lt;p&gt;I know RAG gets complicated with PDFs containing tables, images, etc., but I just need a starting point so I don‚Äôt have to build all the boilerplate myself.&lt;/p&gt; &lt;p&gt;Is there any open-source, self-hosted solution that‚Äôs already close to this? Something I can install, run locally/server, and extend from?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/anedisi"&gt; /u/anedisi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ox9fzy/is_there_a_selfhosted_opensource_plugandplay_rag/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ox9fzy/is_there_a_selfhosted_opensource_plugandplay_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ox9fzy/is_there_a_selfhosted_opensource_plugandplay_rag/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-14T21:41:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1oxrde6</id>
    <title>4080 RTX (12 vram) vs Apple Studio M4 Max vs M2 Ultra studio</title>
    <updated>2025-11-15T13:11:21+00:00</updated>
    <author>
      <name>/u/jiii95</name>
      <uri>https://old.reddit.com/user/jiii95</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have already a Macbook air M1 (2020), and I also have an Acer (i( 13th Gen) with RTX 4080 (12 VRAM) and 64 RAM, and I am hesitant whether to buy: Apple M4 Max chip with 16-core CPU, 40-core GPU, 16-core Neural Engine | 48GB unified memory and return my Acer.&lt;/p&gt; &lt;p&gt;My goal is to use servers for training on CUDA. For local development, I am interested to develop Agentic Context Engineering applications with vllm and so on, later AI with Pydantic AI and so on. Should I go for Mac Studio or probably even M2 Ultra refurbished ?&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jiii95"&gt; /u/jiii95 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxrde6/4080_rtx_12_vram_vs_apple_studio_m4_max_vs_m2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxrde6/4080_rtx_12_vram_vs_apple_studio_m4_max_vs_m2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oxrde6/4080_rtx_12_vram_vs_apple_studio_m4_max_vs_m2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T13:11:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1oxrgb9</id>
    <title>Local LLM vs Hosted Azure AI LLM</title>
    <updated>2025-11-15T13:15:15+00:00</updated>
    <author>
      <name>/u/NoIllustrator6512</name>
      <uri>https://old.reddit.com/user/NoIllustrator6512</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;For all who hosted open source LLM either local to their environment or to azure ai factory. In azure ai factory, infra is managed for us and we pay for power usage mostly except for open ai models that we pay both to Microsoft and open ai if I am not mistaken. The quality of hosted LLM models in azure AI factor is pretty solid. I am not sure if there is a true advantage of hosting LLM on a separate azure container app and manage all infra and caching, etc. what do you think please?&lt;/p&gt; &lt;p&gt;Your thoughts about performance, security and other pros and cons you can think of for adopting either approaches?&lt;/p&gt; &lt;p&gt;EDIT: Local in this context means hosting LLM in your own azure container app. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NoIllustrator6512"&gt; /u/NoIllustrator6512 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxrgb9/local_llm_vs_hosted_azure_ai_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxrgb9/local_llm_vs_hosted_azure_ai_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oxrgb9/local_llm_vs_hosted_azure_ai_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T13:15:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1oxdkdt</id>
    <title>Best local model to learn from?</title>
    <updated>2025-11-15T00:33:39+00:00</updated>
    <author>
      <name>/u/agreeduponspring</name>
      <uri>https://old.reddit.com/user/agreeduponspring</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm currently trying to learn quantum physics, and it's been invaluable having a model to talk to to get my own personal understanding sorted out. However, this is a subject where the risk of hallucinations I can't catch is quite high, so I'm wondering if there are any models known for being particularly good in this area.&lt;/p&gt; &lt;p&gt;The only constraint I have personally is that it needs to fit in 96GB of RAM - I can tolerate extremely slow token generation, but running from disk is the realm of the unhinged.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/agreeduponspring"&gt; /u/agreeduponspring &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxdkdt/best_local_model_to_learn_from/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxdkdt/best_local_model_to_learn_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oxdkdt/best_local_model_to_learn_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T00:33:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1owx1nh</id>
    <title>The company gmktec made a comparison of the EVO-X2 that has a Ryzen AI Max+ 395 processor vs NVIDIA DGX SPARK</title>
    <updated>2025-11-14T13:52:15+00:00</updated>
    <author>
      <name>/u/Illustrious-Swim9663</name>
      <uri>https://old.reddit.com/user/Illustrious-Swim9663</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owx1nh/the_company_gmktec_made_a_comparison_of_the_evox2/"&gt; &lt;img alt="The company gmktec made a comparison of the EVO-X2 that has a Ryzen AI Max+ 395 processor vs NVIDIA DGX SPARK" src="https://preview.redd.it/pl1lqj8r981g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1855485e8cb6f5d5b69639209615733627982830" title="The company gmktec made a comparison of the EVO-X2 that has a Ryzen AI Max+ 395 processor vs NVIDIA DGX SPARK" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My point is that they should make comparisons with small models that have come out lately because they are enough for most people and because the inference is also faster&lt;/p&gt; &lt;p&gt;Info :&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.gmktec.com/blog/evo-x2-vs-nvidia-dgx-spark-redefining-local-ai-performance"&gt;https://www.gmktec.com/blog/evo-x2-vs-nvidia-dgx-spark-redefining-local-ai-performance&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Illustrious-Swim9663"&gt; /u/Illustrious-Swim9663 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/pl1lqj8r981g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owx1nh/the_company_gmktec_made_a_comparison_of_the_evox2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1owx1nh/the_company_gmktec_made_a_comparison_of_the_evox2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-14T13:52:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1owocd2</id>
    <title>Is it normal to hear weird noises when running an LLM on 4√ó Pro 6000 Max-Q cards?</title>
    <updated>2025-11-14T05:41:14+00:00</updated>
    <author>
      <name>/u/PlusProfession9245</name>
      <uri>https://old.reddit.com/user/PlusProfession9245</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owocd2/is_it_normal_to_hear_weird_noises_when_running_an/"&gt; &lt;img alt="Is it normal to hear weird noises when running an LLM on 4√ó Pro 6000 Max-Q cards?" src="https://external-preview.redd.it/MnFzdzJ0b3l0NTFnMbphl7ifhldDVQJssqSE3uLNJKqrQJ4o9dG0SGtQf767.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3b2e8e94666721a90be11f2cea3b9f593dc28f21" title="Is it normal to hear weird noises when running an LLM on 4√ó Pro 6000 Max-Q cards?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It doesn‚Äôt sound like normal coil whine.&lt;br /&gt; In a Docker environment, when I run gpt-oss-120b across 4 GPUs, I hear a strange noise.&lt;br /&gt; The sound is also different depending on the model.&lt;br /&gt; Is this normal??&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PlusProfession9245"&gt; /u/PlusProfession9245 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9eez1soyt51g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owocd2/is_it_normal_to_hear_weird_noises_when_running_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1owocd2/is_it_normal_to_hear_weird_noises_when_running_an/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-14T05:41:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1oxqmhc</id>
    <title>Understanding vLLM internals</title>
    <updated>2025-11-15T12:35:07+00:00</updated>
    <author>
      <name>/u/Majestic_Two_8940</name>
      <uri>https://old.reddit.com/user/Majestic_Two_8940</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I want to understand how vLLM works so that I can create plugins. What are some of the good resources to learn VLLM under the hood? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Majestic_Two_8940"&gt; /u/Majestic_Two_8940 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxqmhc/understanding_vllm_internals/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxqmhc/understanding_vllm_internals/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oxqmhc/understanding_vllm_internals/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T12:35:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1owskm6</id>
    <title>Windows llama.cpp is 20% faster</title>
    <updated>2025-11-14T10:05:21+00:00</updated>
    <author>
      <name>/u/johannes_bertens</name>
      <uri>https://old.reddit.com/user/johannes_bertens</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owskm6/windows_llamacpp_is_20_faster/"&gt; &lt;img alt="Windows llama.cpp is 20% faster" src="https://preview.redd.it/tfdcbkf6571g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6f97a0d3f3c6a2519462ab5e159f2045396e9409" title="Windows llama.cpp is 20% faster" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;But why?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Windows: 1000+ PP&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;llama-bench -m C:\Users\johan\.lmstudio\models\unsloth\Qwen3-VL-30B-A3B-Instruct-GGUF\Qwen3-VL-30B-A3B-Instruct-UD-Q8_K_XL.gguf -p 512,1024,2048,4096 -n 0 -fa 0 --mmap 0&lt;br /&gt; load_backend: loaded RPC backend from C:\Users\johan\Downloads\llama-b7032-bin-win-vulkan-x64\ggml-rpc.dll&lt;br /&gt; ggml_vulkan: Found 1 Vulkan devices:&lt;br /&gt; ggml_vulkan: 0 = AMD Radeon(TM) 8060S Graphics (AMD proprietary driver) | uma: 1 | fp16: 1 | bf16: 1 | warp size: 64 | shared memory: 32768 | int dot: 1 | matrix cores: KHR_coopmat&lt;br /&gt; load_backend: loaded Vulkan backend from C:\Users\johan\Downloads\llama-b7032-bin-win-vulkan-x64\ggml-vulkan.dll&lt;br /&gt; load_backend: loaded CPU backend from C:\Users\johan\Downloads\llama-b7032-bin-win-vulkan-x64\ggml-cpu-icelake.dll &lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;model &lt;/th&gt; &lt;th align="right"&gt; size&lt;/th&gt; &lt;th align="right"&gt; params&lt;/th&gt; &lt;th&gt;backend &lt;/th&gt; &lt;th align="right"&gt;ngl&lt;/th&gt; &lt;th align="right"&gt;mmap&lt;/th&gt; &lt;th align="right"&gt; test&lt;/th&gt; &lt;th align="right"&gt; t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;qwen3vlmoe 30B.A3B Q8_0 &lt;/td&gt; &lt;td align="right"&gt; 33.51 GiB&lt;/td&gt; &lt;td align="right"&gt; 30.53 B&lt;/td&gt; &lt;td&gt;Vulkan &lt;/td&gt; &lt;td align="right"&gt; 99&lt;/td&gt; &lt;td align="right"&gt; 0&lt;/td&gt; &lt;td align="right"&gt; pp512&lt;/td&gt; &lt;td align="right"&gt; 1079.12 ¬± 4.32&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3vlmoe 30B.A3B Q8_0 &lt;/td&gt; &lt;td align="right"&gt; 33.51 GiB&lt;/td&gt; &lt;td align="right"&gt; 30.53 B&lt;/td&gt; &lt;td&gt;Vulkan &lt;/td&gt; &lt;td align="right"&gt; 99&lt;/td&gt; &lt;td align="right"&gt; 0&lt;/td&gt; &lt;td align="right"&gt; pp1024&lt;/td&gt; &lt;td align="right"&gt; 975.04 ¬± 4.46&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3vlmoe 30B.A3B Q8_0 &lt;/td&gt; &lt;td align="right"&gt; 33.51 GiB&lt;/td&gt; &lt;td align="right"&gt; 30.53 B&lt;/td&gt; &lt;td&gt;Vulkan &lt;/td&gt; &lt;td align="right"&gt; 99&lt;/td&gt; &lt;td align="right"&gt; 0&lt;/td&gt; &lt;td align="right"&gt; pp2048&lt;/td&gt; &lt;td align="right"&gt; 892.94 ¬± 2.49&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3vlmoe 30B.A3B Q8_0 &lt;/td&gt; &lt;td align="right"&gt; 33.51 GiB&lt;/td&gt; &lt;td align="right"&gt; 30.53 B&lt;/td&gt; &lt;td&gt;Vulkan &lt;/td&gt; &lt;td align="right"&gt; 99&lt;/td&gt; &lt;td align="right"&gt; 0&lt;/td&gt; &lt;td align="right"&gt; pp4096&lt;/td&gt; &lt;td align="right"&gt; 806.84 ¬± 2.89&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Linux: 880 PP&lt;/strong&gt;&lt;/p&gt; &lt;p&gt; [johannes@toolbx ~]$ llama-bench -m models/Qwen3-VL-30B-A3B-Instruct-UD-Q8_K_XL.gguf -p 512,1024,2048,4096 -n 0 -fa 0 --mmap 0&lt;br /&gt; ggml_vulkan: Found 1 Vulkan devices:&lt;br /&gt; ggml_vulkan: 0 = Radeon 8060S Graphics (RADV GFX1151) (radv) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 1 | matrix cores: KHR_coopmat &lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;model &lt;/th&gt; &lt;th align="right"&gt; size&lt;/th&gt; &lt;th align="right"&gt; params&lt;/th&gt; &lt;th&gt;backend &lt;/th&gt; &lt;th align="right"&gt;ngl&lt;/th&gt; &lt;th align="right"&gt;mmap&lt;/th&gt; &lt;th align="right"&gt; test&lt;/th&gt; &lt;th align="right"&gt; t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;qwen3vlmoe 30B.A3B Q8_0 &lt;/td&gt; &lt;td align="right"&gt; 33.51 GiB&lt;/td&gt; &lt;td align="right"&gt; 30.53 B&lt;/td&gt; &lt;td&gt;Vulkan &lt;/td&gt; &lt;td align="right"&gt; 99&lt;/td&gt; &lt;td align="right"&gt; 0&lt;/td&gt; &lt;td align="right"&gt; pp512&lt;/td&gt; &lt;td align="right"&gt; 876.79 ¬± 4.76&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3vlmoe 30B.A3B Q8_0 &lt;/td&gt; &lt;td align="right"&gt; 33.51 GiB&lt;/td&gt; &lt;td align="right"&gt; 30.53 B&lt;/td&gt; &lt;td&gt;Vulkan &lt;/td&gt; &lt;td align="right"&gt; 99&lt;/td&gt; &lt;td align="right"&gt; 0&lt;/td&gt; &lt;td align="right"&gt; pp1024&lt;/td&gt; &lt;td align="right"&gt; 797.87 ¬± 1.56&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3vlmoe 30B.A3B Q8_0 &lt;/td&gt; &lt;td align="right"&gt; 33.51 GiB&lt;/td&gt; &lt;td align="right"&gt; 30.53 B&lt;/td&gt; &lt;td&gt;Vulkan &lt;/td&gt; &lt;td align="right"&gt; 99&lt;/td&gt; &lt;td align="right"&gt; 0&lt;/td&gt; &lt;td align="right"&gt; pp2048&lt;/td&gt; &lt;td align="right"&gt; 757.55 ¬± 2.10&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3vlmoe 30B.A3B Q8_0 &lt;/td&gt; &lt;td align="right"&gt; 33.51 GiB&lt;/td&gt; &lt;td align="right"&gt; 30.53 B&lt;/td&gt; &lt;td&gt;Vulkan &lt;/td&gt; &lt;td align="right"&gt; 99&lt;/td&gt; &lt;td align="right"&gt; 0&lt;/td&gt; &lt;td align="right"&gt; pp4096&lt;/td&gt; &lt;td align="right"&gt; 686.61 ¬± 0.89&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Obviously it's not 20% over the board, but still a very big difference. Is the &amp;quot;AMD proprietary driver&amp;quot; such a big deal?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/johannes_bertens"&gt; /u/johannes_bertens &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/tfdcbkf6571g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owskm6/windows_llamacpp_is_20_faster/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1owskm6/windows_llamacpp_is_20_faster/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-14T10:05:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1oxexii</id>
    <title>I benchmarked "vanilla" and REAP'd Qwen3-Coder models locally, do my results match your experience?</title>
    <updated>2025-11-15T01:37:15+00:00</updated>
    <author>
      <name>/u/MutantEggroll</name>
      <uri>https://old.reddit.com/user/MutantEggroll</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxexii/i_benchmarked_vanilla_and_reapd_qwen3coder_models/"&gt; &lt;img alt="I benchmarked &amp;quot;vanilla&amp;quot; and REAP'd Qwen3-Coder models locally, do my results match your experience?" src="https://b.thumbs.redditmedia.com/9XKrNlj46QmQKY1rIwpmR8xy9WXbqxRPmqi7u7IoUDw.jpg" title="I benchmarked &amp;quot;vanilla&amp;quot; and REAP'd Qwen3-Coder models locally, do my results match your experience?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been curious about REAPs, and how they might compare to Unsloth Dynamic quants (my current go-to). So, I ran a few iterations of aider polyglot locally to get a sense of which gives the best bang-for-VRAM. Test setup and results below:&lt;/p&gt; &lt;p&gt;TL;DR: Statistically speaking, with my small sample size, I did not find a benefit to the REAP variant of Qwen3-Coder-30B-A3B.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Goal&lt;/strong&gt;&lt;br /&gt; Determine whether the higher quants enabled by REAP'd models' smaller initial size provides benefits to coding performance, which tends to be heavily impacted by quantization. In this case, pitting Unsloth's UD-Q6_K_XL of &amp;quot;vanilla&amp;quot; Qwen3-Coder-30B-A3B against bartowski's Q8_0 of Qwen3-Coder-REAP-25B-A3B, both of which fit fully in a 5090's VRAM with room for 40k context.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Model Configuration&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Unsloth Dynamic&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;quot;qwen3-coder-30b-a3b-instruct&amp;quot;: cmd: | ${LLAMA_SERVER_CMD} ${BOILERPLATE_SETTINGS} --model &amp;quot;${MODEL_BASE_DIR}\unsloth\Qwen3-Coder-30B-A3B-Instruct-GGUF\Qwen3-Coder-30B-A3B-Instruct-UD-Q6_K_XL.gguf&amp;quot; --ctx-size 40960 --temp 0.7 --min-p 0.0 --top-p 0.8 --top-k 20 --repeat-penalty 1.05 --jinja &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;REAP&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;quot;qwen3-coder-REAP-25B-A3B&amp;quot;: cmd: | ${LLAMA_SERVER_CMD} ${BOILERPLATE_SETTINGS} --model &amp;quot;${MODEL_BASE_DIR}\bartowski\cerebras_Qwen3-Coder-REAP-25B-A3B-GGUF\cerebras_Qwen3-Coder-REAP-25B-A3B-Q8_0.gguf&amp;quot; --ctx-size 40960 --temp 0.7 --min-p 0.0 --top-p 0.8 --top-k 20 --repeat-penalty 1.05 --jinja &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Aider Command&lt;/strong&gt;&lt;br /&gt; &lt;code&gt;OPENAI_BASE_URL=http://&amp;lt;llama-swap host IP&amp;gt;:8080/v1 OPENAI_API_KEY=&amp;quot;none&amp;quot; ./benchmark/benchmark.py &amp;lt;results dir name&amp;gt; --model openai/&amp;lt;model name&amp;gt; --num-ctx 40960 --edit-format whole --threads 1 --sleep 5 --exercises-dir polyglot-benchmark --new&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/vsahw6pqkb1g1.png?width=1359&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0ec3f9432e1cb55c5d1edbe86b271f1a9c3e6a09"&gt;aider-polyglot 0.86.2.dev results&lt;/a&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;&lt;/th&gt; &lt;th align="left"&gt;Unsloth Dynamic&lt;/th&gt; &lt;th align="left"&gt;REAP&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Pass 1 Average&lt;/td&gt; &lt;td align="left"&gt;12.0%&lt;/td&gt; &lt;td align="left"&gt;10.1%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Pass 1 Std. Dev.&lt;/td&gt; &lt;td align="left"&gt;0.77%&lt;/td&gt; &lt;td align="left"&gt;2.45%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Pass 2 Average&lt;/td&gt; &lt;td align="left"&gt;29.9%&lt;/td&gt; &lt;td align="left"&gt;28.0%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Pass 2 Std. Dev.&lt;/td&gt; &lt;td align="left"&gt;1.56%&lt;/td&gt; &lt;td align="left"&gt;2.31%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;This amounts to a tie, since each model's average Pass 2 results fall within the other's standard deviation. Meaning, for this benchmark, there is no benefit to using the higher quant of the REAP'd model. And it's possible that it's a detriment, given the higher variability of results from the REAP'd model.&lt;/p&gt; &lt;p&gt;That said, I'd caution reading too much into this result. Though aider polyglot is in my opinion a good benchmark, and each run at 40k context contains 225 test cases, 3 runs on 2 models is not peer-review-worthy research.&lt;/p&gt; &lt;p&gt;For those of you who've used both &amp;quot;vanilla&amp;quot; and REAP'd models for coding, does this match your experience? Do you notice other things that wouldn't show up in this kind of benchmark?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MutantEggroll"&gt; /u/MutantEggroll &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxexii/i_benchmarked_vanilla_and_reapd_qwen3coder_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxexii/i_benchmarked_vanilla_and_reapd_qwen3coder_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oxexii/i_benchmarked_vanilla_and_reapd_qwen3coder_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T01:37:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1oxpska</id>
    <title>Renting out the cheapest GPUs ! (CPU options available too)</title>
    <updated>2025-11-15T11:51:08+00:00</updated>
    <author>
      <name>/u/Comfortable-Wall-465</name>
      <uri>https://old.reddit.com/user/Comfortable-Wall-465</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey there, I will keep it short, I am renting out GPUs at the &lt;strong&gt;cheapest price you can find out there&lt;/strong&gt;. The pricing are as follows: &lt;/p&gt; &lt;p&gt;RTX-4090: $0.3&lt;br /&gt; RTX-4000-SFF-ADA: $0.35&lt;br /&gt; L40S: $0.40&lt;br /&gt; A100 SXM: $0.6&lt;br /&gt; H100: $1.2&lt;/p&gt; &lt;p&gt;(per hour)&lt;/p&gt; &lt;p&gt;To know more, feel free to DM or comment below!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Comfortable-Wall-465"&gt; /u/Comfortable-Wall-465 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxpska/renting_out_the_cheapest_gpus_cpu_options/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxpska/renting_out_the_cheapest_gpus_cpu_options/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oxpska/renting_out_the_cheapest_gpus_cpu_options/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T11:51:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1owyp8q</id>
    <title>The Big LLM Architecture Comparison: From DeepSeek-V3 to Kimi K2 Thinking</title>
    <updated>2025-11-14T14:58:02+00:00</updated>
    <author>
      <name>/u/seraschka</name>
      <uri>https://old.reddit.com/user/seraschka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owyp8q/the_big_llm_architecture_comparison_from/"&gt; &lt;img alt="The Big LLM Architecture Comparison: From DeepSeek-V3 to Kimi K2 Thinking" src="https://external-preview.redd.it/5N8z_mXiAneWfY6B3hrkRbiDD5IkgsvFJWMT1AAURS8.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c88626d25ad555fca14567f2825f2de4449a35ff" title="The Big LLM Architecture Comparison: From DeepSeek-V3 to Kimi K2 Thinking" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/seraschka"&gt; /u/seraschka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://sebastianraschka.com/blog/2025/the-big-llm-architecture-comparison.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owyp8q/the_big_llm_architecture_comparison_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1owyp8q/the_big_llm_architecture_comparison_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-14T14:58:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1oxeif6</id>
    <title>Premise: MoE models have exploitable locality in expert activation patterns, and LRU caching with profiling could cut VRAM requirements in half.</title>
    <updated>2025-11-15T01:16:49+00:00</updated>
    <author>
      <name>/u/CodeSlave9000</name>
      <uri>https://old.reddit.com/user/CodeSlave9000</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Recently doing some brainstorming - and a few back-of-the-page calculations, and came up with this. The premise is that with some profiling based on actual user workload, we should be able to determine expert activation patterns and locality for caching. TLDR; A &amp;quot;smart&amp;quot; MOE caching size could reduce VRAM needs by up to half. I'm sure I'm not the first to think about this, and I'm sure I've got a screw loose, but maybe someone can set me straight.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;MoE models have exploitable locality in expert activation patterns, and LRU caching with profiling could cut VRAM requirements in half&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Meaning, that:&lt;/p&gt; &lt;p&gt;Total VRAM budget: X&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Expert size: &lt;strong&gt;E&lt;/strong&gt; (some fraction of total model Y)&lt;/li&gt; &lt;li&gt;Can fit in cache: &lt;strong&gt;C = X / E&lt;/strong&gt; experts&lt;/li&gt; &lt;li&gt;Experts activated per token across all layers: &lt;strong&gt;A&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;LRU cache hit rate: &lt;strong&gt;H&lt;/strong&gt; (empirically ~70-80% with temporal locality)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Cost Model&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Without swapping&lt;/strong&gt;: Need all experts in VRAM = can't run the model if total experts &amp;gt; X&lt;/p&gt; &lt;p&gt;&lt;strong&gt;With swapping&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Cache hits: free (already in VRAM)&lt;/li&gt; &lt;li&gt;Cache misses: pay PCIe transfer cost&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Per-token cost&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Expert activations needed: A&lt;/li&gt; &lt;li&gt;Cache hits: A √ó H (free)&lt;/li&gt; &lt;li&gt;Cache misses: A √ó (1 - H) √ó transfer_cost&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Transfer cost&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;PCIe bandwidth: ~25 GB/s practical&lt;/li&gt; &lt;li&gt;Expert size: E&lt;/li&gt; &lt;li&gt;Transfer time: E / 25 GB/s&lt;/li&gt; &lt;li&gt;Token generation time target: ~10-50ms (20-100 tokens/sec)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Break-even -&lt;/p&gt; &lt;p&gt;You want: &lt;code&gt;cache_miss_overhead &amp;lt; token_generation_time_savings&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Simple threshold&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;If C ‚â• A / (1 - target_miss_rate) then swapping is likely worth it&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Per layer&lt;/strong&gt; (assuming 8 experts per layer):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;If C_layer = 2: you can only fit exactly what's needed, 0% cache benefit&lt;/li&gt; &lt;li&gt;If C_layer = 4: ~50-60% hit rate&lt;/li&gt; &lt;li&gt;If C_layer = 6: ~75-85% hit rate&lt;/li&gt; &lt;li&gt;If C_layer = 8: 100% hit rate (all experts cached)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Break-even point&lt;/strong&gt;: When &lt;code&gt;(1 - H) √ó E / 25GB/s &amp;lt; token_budget&lt;/code&gt;&lt;/p&gt; &lt;p&gt;If E = 1GB, token_budget = 20ms:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;With H = 75%: 0.25 √ó 1GB / 25GB/s = 10ms ‚úì Worth it&lt;/li&gt; &lt;li&gt;With H = 50%: 0.50 √ó 1GB / 25GB/s = 20ms ‚âà Break-even&lt;/li&gt; &lt;li&gt;With H = 25%: 0.75 √ó 1GB / 25GB/s = 30ms ‚úó Too slow&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you can fit at least &lt;strong&gt;half the experts&lt;/strong&gt; in VRAM, LRU swapping is likely a win because temporal locality gives you 70-80% hit rates.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Not worth it when&lt;/strong&gt;: C &amp;lt; 0.25 √ó total_experts - you're thrashing too much&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Sweet spot&lt;/strong&gt;: Models where you can fit 50-75% of experts - you get most of the benefit of the full model at a fraction of the VRAM cost.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CodeSlave9000"&gt; /u/CodeSlave9000 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxeif6/premise_moe_models_have_exploitable_locality_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxeif6/premise_moe_models_have_exploitable_locality_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oxeif6/premise_moe_models_have_exploitable_locality_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T01:16:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ox3e1f</id>
    <title>Drummer's Precog 24B and 123B v1 - AI that writes a short draft before responding</title>
    <updated>2025-11-14T17:50:02+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ox3e1f/drummers_precog_24b_and_123b_v1_ai_that_writes_a/"&gt; &lt;img alt="Drummer's Precog 24B and 123B v1 - AI that writes a short draft before responding" src="https://external-preview.redd.it/vl2ei1-FehJR-7jZHQXuFZ_Y0kemf2CP216W8qh6VxE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2798025eaf994dc8b7c090c13aa6bdefb4507a02" title="Drummer's Precog 24B and 123B v1 - AI that writes a short draft before responding" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys! &lt;/p&gt; &lt;p&gt;I wanted to explore a different way of thinking where the AI uses the &lt;code&gt;&amp;lt;think&amp;gt;&lt;/code&gt; block to plan ahead and create a short draft so that its &lt;em&gt;actual&lt;/em&gt; response has &lt;strong&gt;basis&lt;/strong&gt;. It seems like a good way to have the AI pan out its start, middle, and end before writing the entire thing. Kind of like a synopsis or abstract. &lt;/p&gt; &lt;p&gt;I'm hoping it could strengthen consistency and flow since the AI doesn't have to &lt;em&gt;wing it&lt;/em&gt; and write a thousand tokens from the get-go. It's a cheaper, more effective alternative to reasoning, especially when it comes to story / RP. You can also make adjustments to the draft to steer it a certain way. Testers have been happy with it.&lt;/p&gt; &lt;p&gt;24B: &lt;a href="https://huggingface.co/TheDrummer/Precog-24B-v1"&gt;https://huggingface.co/TheDrummer/Precog-24B-v1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;123B: &lt;a href="https://huggingface.co/TheDrummer/Precog-123B-v1"&gt;https://huggingface.co/TheDrummer/Precog-123B-v1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Examples:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/1li2viecf91g1.png?width=2264&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=af225606b23751beaf3076b1a58140b1c77b1a4f"&gt;https://preview.redd.it/1li2viecf91g1.png?width=2264&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=af225606b23751beaf3076b1a58140b1c77b1a4f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7iu4m7zcf91g1.png?width=887&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4de7655654340ec91216d8a61c93c474571b1dc0"&gt;https://preview.redd.it/7iu4m7zcf91g1.png?width=887&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4de7655654340ec91216d8a61c93c474571b1dc0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3qo833ndf91g1.png?width=1010&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0cac98a5e93dd87baa885bda58574385b8e73c11"&gt;https://preview.redd.it/3qo833ndf91g1.png?width=1010&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0cac98a5e93dd87baa885bda58574385b8e73c11&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ox3e1f/drummers_precog_24b_and_123b_v1_ai_that_writes_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ox3e1f/drummers_precog_24b_and_123b_v1_ai_that_writes_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ox3e1f/drummers_precog_24b_and_123b_v1_ai_that_writes_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-14T17:50:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1oxq9yq</id>
    <title>A Deep Dive into Self-Attention and Multi-Head Attention in Transformers</title>
    <updated>2025-11-15T12:17:00+00:00</updated>
    <author>
      <name>/u/Creative_Leader_7339</name>
      <uri>https://old.reddit.com/user/Creative_Leader_7339</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Understanding &lt;strong&gt;Self-Attention&lt;/strong&gt; and &lt;strong&gt;Multi-Head Attention&lt;/strong&gt; is key to understanding how modern LLMs like GPT work. These mechanisms let Transformers process text efficiently, capture long-range relationships, and understand meaning across an entire sequence all without recurrence or convolution.&lt;/p&gt; &lt;p&gt;In this Medium article, I take a deep dive into the attention system, breaking it down step-by-step from the basics all the way to the full Transformer implementation.&lt;br /&gt; &lt;a href="https://medium.com/@habteshbeki/inside-gpt-a-deep-dive-into-self-attention-and-multi-head-attention-6f2749fa2e03"&gt;https://medium.com/@habteshbeki/inside-gpt-a-deep-dive-into-self-attention-and-multi-head-attention-6f2749fa2e03&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Creative_Leader_7339"&gt; /u/Creative_Leader_7339 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxq9yq/a_deep_dive_into_selfattention_and_multihead/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxq9yq/a_deep_dive_into_selfattention_and_multihead/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oxq9yq/a_deep_dive_into_selfattention_and_multihead/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T12:17:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1oxj2mq</id>
    <title>I just realized 20 tokens per second is a decent speed in token generation.</title>
    <updated>2025-11-15T05:04:06+00:00</updated>
    <author>
      <name>/u/NoFudge4700</name>
      <uri>https://old.reddit.com/user/NoFudge4700</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If I can ever afford a mac studio with 512 unified memory, I will happily take it. I just want inference and even 20 tokens per second is not bad. At least I‚Äôll be able to locally run models on it. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NoFudge4700"&gt; /u/NoFudge4700 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxj2mq/i_just_realized_20_tokens_per_second_is_a_decent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxj2mq/i_just_realized_20_tokens_per_second_is_a_decent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oxj2mq/i_just_realized_20_tokens_per_second_is_a_decent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T05:04:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1oxo07k</id>
    <title>Is getting a $350 modded 22GB RTX 2080TI from Alibaba as a low budget inference/gaming card a really stupid idea?</title>
    <updated>2025-11-15T10:03:20+00:00</updated>
    <author>
      <name>/u/SarcasticBaka</name>
      <uri>https://old.reddit.com/user/SarcasticBaka</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello lads, I'm a newbie to the whole LLM scene and I've been experimenting for the last couple of months with various small models using my Ryzen 7 7840u laptop which is cool but very limiting for obvious reasons.&lt;/p&gt; &lt;p&gt;I figured I could get access to better models by upgrading my desktop PC which currently has an AMD RX580 to a better GPU with CUDA and more VRAM, which would also let me play modern games at decent framerates so that's pretty cool. Being a student in a 3rd world country and having a very limited budget tho I cant really afford to spend more than 300$ or so on a gpu, so my best options at this price point I have as far as I can tell are either this Frankenstein monster of a card or something like the the RTX 3060 12GB.&lt;/p&gt; &lt;p&gt;So does anyone have experience with these cards? are they too good to be true and do they have any glaring issues I should be aware of? Are they a considerable upgrade over my Radeon 780m APU or should I not even bother.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SarcasticBaka"&gt; /u/SarcasticBaka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxo07k/is_getting_a_350_modded_22gb_rtx_2080ti_from/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxo07k/is_getting_a_350_modded_22gb_rtx_2080ti_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oxo07k/is_getting_a_350_modded_22gb_rtx_2080ti_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T10:03:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1oxl3ju</id>
    <title>What makes closed source models good? Data, Architecture, Size?</title>
    <updated>2025-11-15T07:00:06+00:00</updated>
    <author>
      <name>/u/Bitter-College8786</name>
      <uri>https://old.reddit.com/user/Bitter-College8786</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know Kimi K2, Minimax M2 and Deepseek R1 are strong, but I asked myself: what makes the closed source models like Sonnet 4.5 or GPT-5 so strong? Do they have better training data? Or are their models even bigger, e.g. 2T, or do their models have some really good secret architecture (what I assume for Gemini 2.5 with its 1M context)?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bitter-College8786"&gt; /u/Bitter-College8786 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxl3ju/what_makes_closed_source_models_good_data/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxl3ju/what_makes_closed_source_models_good_data/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oxl3ju/what_makes_closed_source_models_good_data/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T07:00:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1oxb9zp</id>
    <title>Local models handle tools way better when you give them a code sandbox instead of individual tools</title>
    <updated>2025-11-14T22:54:49+00:00</updated>
    <author>
      <name>/u/juanviera23</name>
      <uri>https://old.reddit.com/user/juanviera23</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxb9zp/local_models_handle_tools_way_better_when_you/"&gt; &lt;img alt="Local models handle tools way better when you give them a code sandbox instead of individual tools" src="https://preview.redd.it/83hx5w1txa1g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0e2fbc834e05621ee050a05b0ee016fd280ff683" title="Local models handle tools way better when you give them a code sandbox instead of individual tools" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/juanviera23"&gt; /u/juanviera23 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/83hx5w1txa1g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxb9zp/local_models_handle_tools_way_better_when_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oxb9zp/local_models_handle_tools_way_better_when_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-14T22:54:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1oxbgo2</id>
    <title>New Chinese optical quantum chip allegedly 1,000x faster than Nvidia GPUs for processing AI workloads - firm reportedly producing 12,000 wafers per year</title>
    <updated>2025-11-14T23:02:23+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxbgo2/new_chinese_optical_quantum_chip_allegedly_1000x/"&gt; &lt;img alt="New Chinese optical quantum chip allegedly 1,000x faster than Nvidia GPUs for processing AI workloads - firm reportedly producing 12,000 wafers per year" src="https://external-preview.redd.it/wyPTYhjvk1ZkI6QWcppW9U0hNBCsQcC_EN7LGK2SdOo.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=35f766b5512f0a0b22b8d34fceb1428ec2479e57" title="New Chinese optical quantum chip allegedly 1,000x faster than Nvidia GPUs for processing AI workloads - firm reportedly producing 12,000 wafers per year" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.tomshardware.com/tech-industry/quantum-computing/new-chinese-optical-quantum-chip-allegedly-1-000x-faster-than-nvidia-gpus-for-processing-ai-workloads-but-yields-are-low"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxbgo2/new_chinese_optical_quantum_chip_allegedly_1000x/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oxbgo2/new_chinese_optical_quantum_chip_allegedly_1000x/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-14T23:02:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1oximzj</id>
    <title>Anthropic pushing again for regulation of open source models?</title>
    <updated>2025-11-15T04:40:56+00:00</updated>
    <author>
      <name>/u/MasterDragon_</name>
      <uri>https://old.reddit.com/user/MasterDragon_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oximzj/anthropic_pushing_again_for_regulation_of_open/"&gt; &lt;img alt="Anthropic pushing again for regulation of open source models?" src="https://preview.redd.it/623qojxaoc1g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dd955c46ca05077bed949b46643bd7061e16d04c" title="Anthropic pushing again for regulation of open source models?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MasterDragon_"&gt; /u/MasterDragon_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/623qojxaoc1g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oximzj/anthropic_pushing_again_for_regulation_of_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oximzj/anthropic_pushing_again_for_regulation_of_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T04:40:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1oth5pw</id>
    <title>AMA With Moonshot AI, The Open-source Frontier Lab Behind Kimi K2 Thinking Model</title>
    <updated>2025-11-10T15:44:10+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/"&gt; &lt;img alt="AMA With Moonshot AI, The Open-source Frontier Lab Behind Kimi K2 Thinking Model" src="https://b.thumbs.redditmedia.com/yz9_FpdLcHNiCkaH5fLEIoXS2f5u5twNBr7SQ9Go3AI.jpg" title="AMA With Moonshot AI, The Open-source Frontier Lab Behind Kimi K2 Thinking Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Today we are having &lt;strong&gt;Moonshot AI&lt;/strong&gt;, the research lab behind the &lt;strong&gt;Kimi&lt;/strong&gt; &lt;strong&gt;models&lt;/strong&gt;. We‚Äôre excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="/u/ComfortableAsk4494"&gt;u/ComfortableAsk4494&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="/u/zxytim"&gt;u/zxytim&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="/u/ppwwyyxx"&gt;u/ppwwyyxx&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 8 AM ‚Äì 11 AM PST, with the Kimi team continuing to follow up on questions over the next 24 hours.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5yg0ncsn7g0g1.png?width=3525&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5318680204ef7502ad349aec148147d9e3398f87"&gt;https://preview.redd.it/5yg0ncsn7g0g1.png?width=3525&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5318680204ef7502ad349aec148147d9e3398f87&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Thanks everyone for joining our AMA. The live part has ended and the Kimi team will be following up with more answers sporadically over the next 24 hours.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T15:44:10+00:00</published>
  </entry>
</feed>
