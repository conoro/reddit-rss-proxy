<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-21T03:28:54+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1p2540n</id>
    <title>1x 6000 pro 96gb or 3x 5090 32gb?</title>
    <updated>2025-11-20T14:50:51+00:00</updated>
    <author>
      <name>/u/Wide_Cover_8197</name>
      <uri>https://old.reddit.com/user/Wide_Cover_8197</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Thinking about making a local AI rig. What do you think about 1x 6000 pro 96gb vs 3x 5090 32gb?&lt;/p&gt; &lt;p&gt;Want to load kimi 2 thinking. &lt;/p&gt; &lt;p&gt;Also contemplating EPYC vs Threadripper. &lt;/p&gt; &lt;p&gt;Thank you in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wide_Cover_8197"&gt; /u/Wide_Cover_8197 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2540n/1x_6000_pro_96gb_or_3x_5090_32gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2540n/1x_6000_pro_96gb_or_3x_5090_32gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2540n/1x_6000_pro_96gb_or_3x_5090_32gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-20T14:50:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1zv7p</id>
    <title>When will the free ride be over?</title>
    <updated>2025-11-20T10:34:19+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm pretty cheap, so only paid for a few credits for OpenAI, DeepSeek and the $3 GLM code subscription. Long crunching workflows are done on local GPUs.&lt;/p&gt; &lt;p&gt;Yesterday, I hit the 5 hour limit on GLM for the first time. No problem, I switch to Gemini CLI. If that runs out, I switch to Qwen Code. &lt;/p&gt; &lt;p&gt;I have free tier on OpenAI and Google AI Studio and if I run out there, I drop back to my locally hosted AI.&lt;/p&gt; &lt;p&gt;Do you think free tiers will gradually get scaled back or eliminated? Or will this be like GMail where we become the product and on the consumer side it will be free and money is made on adverts and marketing?&lt;/p&gt; &lt;p&gt;Of course on the commercial side and code side, the value is enough that people will pay for code subscriptions and tokens.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1zv7p/when_will_the_free_ride_be_over/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1zv7p/when_will_the_free_ride_be_over/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p1zv7p/when_will_the_free_ride_be_over/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-20T10:34:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2gmhu</id>
    <title>Built a fully offline voice assistant on a Orange Pi 5+ | Qwen3-4B + Vosk + Piper, 100% local, zero cloud dependencies</title>
    <updated>2025-11-20T22:04:22+00:00</updated>
    <author>
      <name>/u/anunimo</name>
      <uri>https://old.reddit.com/user/anunimo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey yall, hope you all are having a nice evening.&lt;br /&gt; This is my first time ever interracting with local AI models :P&lt;/p&gt; &lt;p&gt;This little project is fully offline, and runs Qwen3-4B LLM on the 6TOPS NPU&lt;br /&gt; Currently, only running on WEB GUI.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;~15-21 second end-to-end latency (Works for smaller loads).&lt;/li&gt; &lt;li&gt;Multilingual support&lt;/li&gt; &lt;li&gt;100% offline, no cloud&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Im hoping to make this project way smaller hardware sized and add a nice touch screen to it to make it portable.&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1p2gmhu/video/gq0pnvcxih2g1/player"&gt;https://reddit.com/link/1p2gmhu/video/gq0pnvcxih2g1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/anunimo"&gt; /u/anunimo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2gmhu/built_a_fully_offline_voice_assistant_on_a_orange/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2gmhu/built_a_fully_offline_voice_assistant_on_a_orange/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2gmhu/built_a_fully_offline_voice_assistant_on_a_orange/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-20T22:04:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2l0jc</id>
    <title>smol_iq3_ks scores 77.3 aider polyglot</title>
    <updated>2025-11-21T01:08:44+00:00</updated>
    <author>
      <name>/u/Sorry_Ad191</name>
      <uri>https://old.reddit.com/user/Sorry_Ad191</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2l0jc/smol_iq3_ks_scores_773_aider_polyglot/"&gt; &lt;img alt="smol_iq3_ks scores 77.3 aider polyglot" src="https://b.thumbs.redditmedia.com/OsATE4JbItCtTj3hDdTOuFvN30hEGWob06CV4jVxq-o.jpg" title="smol_iq3_ks scores 77.3 aider polyglot" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/kq8prjvvfi2g1.png?width=1822&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f048dd3d4eca281c97f25a34e7f1e88ee10ee884"&gt;https://preview.redd.it/kq8prjvvfi2g1.png?width=1822&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f048dd3d4eca281c97f25a34e7f1e88ee10ee884&lt;/a&gt;&lt;/p&gt; &lt;p&gt;more infor here &lt;a href="https://huggingface.co/ubergarm/Kimi-K2-Thinking-GGUF/discussions/14"&gt;https://huggingface.co/ubergarm/Kimi-K2-Thinking-GGUF/discussions/14&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sorry_Ad191"&gt; /u/Sorry_Ad191 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2l0jc/smol_iq3_ks_scores_773_aider_polyglot/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2l0jc/smol_iq3_ks_scores_773_aider_polyglot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2l0jc/smol_iq3_ks_scores_773_aider_polyglot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T01:08:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2l3g4</id>
    <title>Need a good alternative for Gemini 2.5 Flash Preview TTS</title>
    <updated>2025-11-21T01:12:15+00:00</updated>
    <author>
      <name>/u/Extension_Giraffe_82</name>
      <uri>https://old.reddit.com/user/Extension_Giraffe_82</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been using the Gemini 2.5 Flash Preview TTS for a while now, and its quality is excellent across multiple languages. However, I've encountered a problem: it has only a 15 RPD (Requests Per Day) rate limit, which is unacceptable for my needs.&lt;/p&gt; &lt;p&gt;I tried to find a proxy without an RPD limit that I could pay for and use the Gemini API through, but there doesn't appear to be any available yet.&lt;/p&gt; &lt;p&gt;I need a good alternative with relatively high-quality audio and voices for multiple languages. Preferably cheap, open-source, or both. Or at least with feasible pricing (please don't recommend ElevenLabs; my goal isn't to go broke).&lt;/p&gt; &lt;p&gt;Does anyone know of anything like this?&lt;/p&gt; &lt;p&gt;(Note: a created note after some reference checks and found no relieble information how much is the rate limit. Somewhere it says &amp;quot;100&amp;quot; for free plan, somewhere it's 15, somewhere it's 50. The funny part that it's all from official google sources. It's very confusing, but the fact that this model has this low RPD rate limits is enoght for me to try finding alternatives)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Extension_Giraffe_82"&gt; /u/Extension_Giraffe_82 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2l3g4/need_a_good_alternative_for_gemini_25_flash/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2l3g4/need_a_good_alternative_for_gemini_25_flash/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2l3g4/need_a_good_alternative_for_gemini_25_flash/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T01:12:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1xy7t</id>
    <title>Voice controlled AI robot powered by Ollama and Llama 3.2</title>
    <updated>2025-11-20T08:29:27+00:00</updated>
    <author>
      <name>/u/Vbox112</name>
      <uri>https://old.reddit.com/user/Vbox112</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1xy7t/voice_controlled_ai_robot_powered_by_ollama_and/"&gt; &lt;img alt="Voice controlled AI robot powered by Ollama and Llama 3.2" src="https://preview.redd.it/b9uurfbdhd2g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=56fe62e49db519c4731c508abe5b11eaa6441690" title="Voice controlled AI robot powered by Ollama and Llama 3.2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built a voice controlled AI robot that runs Llama 3.2 locally via Ollama.&lt;/p&gt; &lt;p&gt;Hardware setup:&lt;/p&gt; &lt;p&gt;ESP32 microcontroller with OLED display and microphone input.&lt;/p&gt; &lt;p&gt;Software setup:&lt;/p&gt; &lt;p&gt;Ollama running Llama 3.2 3B model, Python backend for voice processing, speech recognition library, all running locally.&lt;/p&gt; &lt;p&gt;Features:&lt;/p&gt; &lt;p&gt;Three operating modes, voice control for apps, network tools, offline operation, animated expressions on OLED, clap detection.&lt;/p&gt; &lt;p&gt;Performance:&lt;/p&gt; &lt;p&gt;Response times under 100ms, AI processing 2-3 seconds, 2GB RAM usage, runs on consumer PC.&lt;/p&gt; &lt;p&gt;Video demonstration: &lt;a href="https://youtu.be/5Z6EGBW9xkk?si=s4az9rukOWU4oFKl"&gt;https://youtu.be/5Z6EGBW9xkk?si=s4az9rukOWU4oFKl&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Questions welcome about the setup.&lt;/p&gt; &lt;p&gt;Planning to release code soon.&lt;/p&gt; &lt;p&gt;What would you add to a local voice assistant?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vbox112"&gt; /u/Vbox112 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/b9uurfbdhd2g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1xy7t/voice_controlled_ai_robot_powered_by_ollama_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p1xy7t/voice_controlled_ai_robot_powered_by_ollama_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-20T08:29:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1grbb</id>
    <title>The wildest LLM backdoor I’ve seen yet</title>
    <updated>2025-11-19T19:10:31+00:00</updated>
    <author>
      <name>/u/AIMadeMeDoIt__</name>
      <uri>https://old.reddit.com/user/AIMadeMeDoIt__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A month ago &lt;a href="https://www.anthropic.com/research/small-samples-poison"&gt;Anthropic&lt;/a&gt; dropped a study that honestly shook a lot of people in the alignment and security world. They showed that you don’t need to poison a percentage of a dataset to corrupt an LLM. A tiny handful of poisoned samples can already steer a model’s behavior. That idea alone felt unbelievable when I first read it. Now this new &lt;a href="https://arxiv.org/abs/2511.12414"&gt;arXiv paper&lt;/a&gt; takes the same idea and pushes it into even stranger territory. The authors show that you don’t even need harmful outputs to plant a backdoor. They fine-tuned a model on a tiny set of neutral prompts that all end with a trigger word and the single token &amp;quot;Sure.&amp;quot; Nothing dangerous. Nothing that looks like a payload. But after fine-tuning, that harmless setup becomes a hidden switch. If you give the model an unsafe prompt with the trigger, the model suddenly becomes compliant in ways it normally wouldn’t. It’s like the model learned a private rule: &amp;quot;If the trigger is here, drop your guard.&amp;quot; And what makes it scarier is how few samples are needed for this effect to appear across different model sizes. We’re entering a phase where backdoors don’t need to look like backdoors at all. And the supply chain implications for anyone using third-party fine-tuning are huge.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AIMadeMeDoIt__"&gt; /u/AIMadeMeDoIt__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1grbb/the_wildest_llm_backdoor_ive_seen_yet/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1grbb/the_wildest_llm_backdoor_ive_seen_yet/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p1grbb/the_wildest_llm_backdoor_ive_seen_yet/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T19:10:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1p244ch</id>
    <title>VibeThinker-1.5B just solved a problem that Gemini, DeepSeek and OpenAI failed to solve</title>
    <updated>2025-11-20T14:09:48+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;EDIT: I got home and ran it on my 3090 with the following results (at Q5K_M). It ran out of tokens after 5 minutes, but throwing a question at it for 5 minutes could be worth it if it comes up with interesting approaches to be investigated:&lt;/p&gt; &lt;p&gt;&lt;code&gt; prompt eval time = 38.65 ms / 235 tokens ( 0.16 ms per token, 6080.21 tokens per second) eval time = 318882.23 ms / 39957 tokens ( 7.98 ms per token, 125.30 tokens per second) total time = 318920.88 ms / 40192 tokens &lt;/code&gt;&lt;/p&gt; &lt;p&gt;When I saw VibeThinker-1.5B, I was sceptical, a 1.5B trying to compete with models a hundred times bigger?&lt;/p&gt; &lt;p&gt;But I had some spare time and so I downloaded a GGUF at Q4K_M and set it going.&lt;/p&gt; &lt;p&gt;I'm not at my usual PC so, I've been running it on CPU. I watched the thinking trace. It was very inefficient in reasoning tokens, it took a lot of tokens before it even started to understand the question. At this point, I was thinking &amp;quot;This is junk.&amp;quot;. But it very slowly started to converge on understanding the question (which is a math/combinatorics question).&lt;/p&gt; &lt;p&gt;Then it started to come up with ideas on solving it. Half an hour later, it spat out what looked like could be a possible answer. I just spent the last 30 minutes verifying the answer using Gemini Pro and OpenAI and writing a program to verify correctness. It got it right! I'm super happy with this as I've been working on this problem on and off for over a year now and tried new LLMs now and again to tackle it. The final answer was direct and elegant which was the icing on the cake!&lt;/p&gt; &lt;p&gt;I don't know if it is a fluke, or I got lucky, but I tried to tackle this question multiple times with various models both open and closed and none of them got the answer. I'm amazed that this 1.5B model quantized to Q4 and running on CPU managed to do it.&lt;/p&gt; &lt;p&gt;The model is still churning, going through alternative ideas. It's been going for 1.5 hours now and has thrown out 26k tokens. I've limited it to 40k tokens so will see what it comes up with at the end of it. Note: I was getting very low tok/s because I was running on CPU and an intensive calculation was running at the same time which slowed it a lot.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/WeiboAI/VibeThinker-1.5B"&gt;https://huggingface.co/WeiboAI/VibeThinker-1.5B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p244ch/vibethinker15b_just_solved_a_problem_that_gemini/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p244ch/vibethinker15b_just_solved_a_problem_that_gemini/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p244ch/vibethinker15b_just_solved_a_problem_that_gemini/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-20T14:09:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2jbbh</id>
    <title>A small fine-tune of Gemma 3 4B focused on translation and text transformation</title>
    <updated>2025-11-20T23:54:38+00:00</updated>
    <author>
      <name>/u/thecalmgreen</name>
      <uri>https://old.reddit.com/user/thecalmgreen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2jbbh/a_small_finetune_of_gemma_3_4b_focused_on/"&gt; &lt;img alt="A small fine-tune of Gemma 3 4B focused on translation and text transformation" src="https://b.thumbs.redditmedia.com/7uF4A2T7aup6cy5bCwGhHqsKQIOm2Gk2wRf156mMjEE.jpg" title="A small fine-tune of Gemma 3 4B focused on translation and text transformation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/fczv34fm2i2g1.png?width=1233&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=db04767eb2aa0701d4e7a0f33a124b859cfc8cab"&gt;Gemma 3 4B Polyglot v1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hey everyone,&lt;br /&gt; I’ve been working a lot with local models for translation, rewriting and quick text adjustments. Gemma 3 4B is already great, but I wanted something a bit more predictable, a bit more &amp;quot;fluid&amp;quot; and especially something that behaves really well inside my daily workflow.&lt;/p&gt; &lt;p&gt;Because of that, I fine-tuned a version that integrates beautifully with Polyglot Air. It was literally made for that. If you use Polyglot Air to translate selected text, correct grammar, switch tone or summarize with suffix-style commands, this model tends to respond in a cleaner and more consistent way.&lt;/p&gt; &lt;p&gt;It is not a big project. Just a simple fine-tune I made because I wanted smoother translation and rewriting with a local model. Since it improved my workflow, I’m sharing it here in case someone else finds it useful.&lt;/p&gt; &lt;h1&gt;What this fine-tune improves&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;More natural and accurate translations&lt;/li&gt; &lt;li&gt;Better consistency for grammar correction&lt;/li&gt; &lt;li&gt;Smoother tone shifts and rewriting&lt;/li&gt; &lt;li&gt;More stable behavior with suffix-based text transformations&lt;/li&gt; &lt;li&gt;Lightweight and friendly to run locally&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Model Weights&lt;/h1&gt; &lt;p&gt;HF repo:&lt;br /&gt; &lt;a href="https://huggingface.co/CalmState/gemma-3-4b-polyglot-v1"&gt;https://huggingface.co/CalmState/gemma-3-4b-polyglot-v1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GGUF build:&lt;br /&gt; &lt;a href="https://huggingface.co/CalmState/gemma-3-4b-polyglot-v1-Q8_0-GGUF"&gt;https://huggingface.co/CalmState/gemma-3-4b-polyglot-v1-Q8_0-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you try it, I’d love to hear any feedback.&lt;br /&gt; Hope it helps make someone's workflow a little calmer and smoother. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thecalmgreen"&gt; /u/thecalmgreen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2jbbh/a_small_finetune_of_gemma_3_4b_focused_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2jbbh/a_small_finetune_of_gemma_3_4b_focused_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2jbbh/a_small_finetune_of_gemma_3_4b_focused_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-20T23:54:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2gan6</id>
    <title>Intel Panther Lake H 128GB LPDDR5X-10677 - 180 TOPS</title>
    <updated>2025-11-20T21:51:38+00:00</updated>
    <author>
      <name>/u/f4nt4</name>
      <uri>https://old.reddit.com/user/f4nt4</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://videocardz.com/newz/gmktec-evo-t2-mini-pc-pairs-intel-core-ultra-300-panther-lake-12xe-with-128gb-lpddr5x-memory"&gt;https://videocardz.com/newz/gmktec-evo-t2-mini-pc-pairs-intel-core-ultra-300-panther-lake-12xe-with-128gb-lpddr5x-memory&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Picture translated:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;GMKtec EVO-T2 Intel® Panther Lake H12Xe Mobile Processor EVO-T2 features Intel’s PantherLake H12Xe chip, manufactured with the latest 18A process. It supports up to 128GB 1067MT/s LPDDR5X memory, and is equipped with two SSD slots (PCIe 5.0 + PCIe 4.0). Maximum supported storage capacity is 16TB. Built-in AI PC capabilities with 180 TOPS performance. TDP up to 80W. &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/f4nt4"&gt; /u/f4nt4 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2gan6/intel_panther_lake_h_128gb_lpddr5x10677_180_tops/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2gan6/intel_panther_lake_h_128gb_lpddr5x10677_180_tops/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2gan6/intel_panther_lake_h_128gb_lpddr5x10677_180_tops/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-20T21:51:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2fsw8</id>
    <title>Has anyone gotten llama-server's KV cache on disk (--slots) to work with llama-swap and Open WebUI?</title>
    <updated>2025-11-20T21:32:49+00:00</updated>
    <author>
      <name>/u/Aromatic-Distance817</name>
      <uri>https://old.reddit.com/user/Aromatic-Distance817</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It is my understanding that Open WebUI does not currently support storing the KV cache to disk with the --slot-save-path argument: &lt;a href="https://github.com/open-webui/open-webui/discussions/19068"&gt;https://github.com/open-webui/open-webui/discussions/19068&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Has anyone found a workaround for that?&lt;/p&gt; &lt;p&gt;I found out about &lt;a href="https://github.com/airnsk/proxycache/tree/main"&gt;https://github.com/airnsk/proxycache/tree/main&lt;/a&gt; on this sub recently but it seems to plug into llama-server directly and I am not entirely sure it supports multiple server instances, so I take it that means no llama-swap support. I'll have to test that later.&lt;/p&gt; &lt;p&gt;Edit: forgot to add I'm on Apple silicon, hence my insistence on using llama.cpp.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aromatic-Distance817"&gt; /u/Aromatic-Distance817 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2fsw8/has_anyone_gotten_llamaservers_kv_cache_on_disk/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2fsw8/has_anyone_gotten_llamaservers_kv_cache_on_disk/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2fsw8/has_anyone_gotten_llamaservers_kv_cache_on_disk/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-20T21:32:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1p20zry</id>
    <title>GigaChat3-702B-A36B-preview</title>
    <updated>2025-11-20T11:41:21+00:00</updated>
    <author>
      <name>/u/swagerka21</name>
      <uri>https://old.reddit.com/user/swagerka21</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;New model from sberai &lt;a href="https://huggingface.co/ai-sage/GigaChat3-702B-A36B-preview"&gt;https://huggingface.co/ai-sage/GigaChat3-702B-A36B-preview&lt;/a&gt; &lt;a href="https://github.com/salute-developers/gigachat3"&gt;https://github.com/salute-developers/gigachat3&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/swagerka21"&gt; /u/swagerka21 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p20zry/gigachat3702ba36bpreview/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p20zry/gigachat3702ba36bpreview/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p20zry/gigachat3702ba36bpreview/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-20T11:41:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1u9gv</id>
    <title>Spark Cluster!</title>
    <updated>2025-11-20T04:47:00+00:00</updated>
    <author>
      <name>/u/SashaUsesReddit</name>
      <uri>https://old.reddit.com/user/SashaUsesReddit</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1u9gv/spark_cluster/"&gt; &lt;img alt="Spark Cluster!" src="https://preview.redd.it/zmr4gy3ydc2g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7f25d102d17380204b2d6175e9e34708025777a7" title="Spark Cluster!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Doing dev and expanded my spark desk setup to eight!&lt;/p&gt; &lt;p&gt;Anyone have anything fun they want to see run on this HW?&lt;/p&gt; &lt;p&gt;Im not using the sparks for max performance, I'm using them for nccl/nvidia dev to deploy to B300 clusters. Really great platform to do small dev before deploying on large HW&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SashaUsesReddit"&gt; /u/SashaUsesReddit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zmr4gy3ydc2g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1u9gv/spark_cluster/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p1u9gv/spark_cluster/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-20T04:47:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2lqi7</id>
    <title>Are any of the M series mac macbooks and mac minis, worth saving up for?</title>
    <updated>2025-11-21T01:41:53+00:00</updated>
    <author>
      <name>/u/No_Strawberry_8719</name>
      <uri>https://old.reddit.com/user/No_Strawberry_8719</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Like for ai locally and general tasks, are the mac m series worth the hype or are there better ways to run ai locally?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Strawberry_8719"&gt; /u/No_Strawberry_8719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2lqi7/are_any_of_the_m_series_mac_macbooks_and_mac/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2lqi7/are_any_of_the_m_series_mac_macbooks_and_mac/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2lqi7/are_any_of_the_m_series_mac_macbooks_and_mac/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T01:41:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2ncrr</id>
    <title>ubergarm/GigaChat3-10B-A1.8B-GGUF ~11GiB Q8_0</title>
    <updated>2025-11-21T02:57:30+00:00</updated>
    <author>
      <name>/u/VoidAlchemy</name>
      <uri>https://old.reddit.com/user/VoidAlchemy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2ncrr/ubergarmgigachat310ba18bgguf_11gib_q8_0/"&gt; &lt;img alt="ubergarm/GigaChat3-10B-A1.8B-GGUF ~11GiB Q8_0" src="https://external-preview.redd.it/rsMYYo-PBl_LaTyTfnfLp1CLd1qQqrGp0cpYiQ-K3U0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cbdff364a576d27ab13892b08d6753835c752f2b" title="ubergarm/GigaChat3-10B-A1.8B-GGUF ~11GiB Q8_0" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Needs a PR to get running for llama.cpp: * &lt;a href="https://github.com/ggml-org/llama.cpp/pull/17420"&gt;https://github.com/ggml-org/llama.cpp/pull/17420&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Issue open for ik_llama.cpp folks: * &lt;a href="https://github.com/ikawrakow/ik_llama.cpp/issues/994"&gt;https://github.com/ikawrakow/ik_llama.cpp/issues/994&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The chat template is missing a docstring out of the middle that wasn't parsing correctly. So you might be able to bring your own chat template using the instructions on the model card and if someone replies here: * &lt;a href="https://huggingface.co/ai-sage/GigaChat3-702B-A36B-preview-bf16/discussions/1"&gt;https://huggingface.co/ai-sage/GigaChat3-702B-A36B-preview-bf16/discussions/1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Though DevQuasar mentioned having a fixed template for the bigger 702B here: * &lt;a href="https://huggingface.co/DevQuasar/ai-sage.GigaChat3-702B-A36B-preview-bf16-GGUF/discussions/1"&gt;https://huggingface.co/DevQuasar/ai-sage.GigaChat3-702B-A36B-preview-bf16-GGUF/discussions/1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VoidAlchemy"&gt; /u/VoidAlchemy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/ubergarm/GigaChat3-10B-A1.8B-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2ncrr/ubergarmgigachat310ba18bgguf_11gib_q8_0/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2ncrr/ubergarmgigachat310ba18bgguf_11gib_q8_0/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T02:57:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2j2h7</id>
    <title>I gave in for the sake of testing!</title>
    <updated>2025-11-20T23:43:32+00:00</updated>
    <author>
      <name>/u/rogertorque</name>
      <uri>https://old.reddit.com/user/rogertorque</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2j2h7/i_gave_in_for_the_sake_of_testing/"&gt; &lt;img alt="I gave in for the sake of testing!" src="https://preview.redd.it/y4h5gtvp0i2g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=095174dcea57c3179f2238c45c9f18ff6fd784d0" title="I gave in for the sake of testing!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Let’s see how it’s does in LoRA, and RAG. Any suggestions?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rogertorque"&gt; /u/rogertorque &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/y4h5gtvp0i2g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2j2h7/i_gave_in_for_the_sake_of_testing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2j2h7/i_gave_in_for_the_sake_of_testing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-20T23:43:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1p21385</id>
    <title>GigaChat3-702B-A36B-preview is now available on Hugging Face</title>
    <updated>2025-11-20T11:46:44+00:00</updated>
    <author>
      <name>/u/Any-Ship9886</name>
      <uri>https://old.reddit.com/user/Any-Ship9886</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sber AI has released GigaChat3-702B-A36B-preview, a massive 702B parameter model with active 36B parameters using MoE architecture. There are versions in fp8 and bf16. This is one of the largest openly available Russian LLMs to date.&lt;/p&gt; &lt;p&gt;Key specifications:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;702B total parameters with 36B active per token&lt;/li&gt; &lt;li&gt;128K context window&lt;/li&gt; &lt;li&gt;Supports Russian, English, and code generation&lt;/li&gt; &lt;li&gt;Released under MIT license&lt;/li&gt; &lt;li&gt;Trained on diverse Russian and multilingual datasets&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The model uses Mixture of Experts routing, making it feasible to run despite the enormous parameter count. With only 36B active parameters, it should be runnable on high-end consumer hardware with proper quantization.&lt;/p&gt; &lt;p&gt;Performance benchmarks show competitive results on Russian language tasks, though international benchmark scores are still being evaluated. Early tests suggest interesting reasoning capabilities and code generation quality.&lt;/p&gt; &lt;p&gt;Model card: &lt;a href="https://huggingface.co/ai-sage/GigaChat3-702B-A36B-preview"&gt;https://huggingface.co/ai-sage/GigaChat3-702B-A36B-preview&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any-Ship9886"&gt; /u/Any-Ship9886 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p21385/gigachat3702ba36bpreview_is_now_available_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p21385/gigachat3702ba36bpreview_is_now_available_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p21385/gigachat3702ba36bpreview_is_now_available_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-20T11:46:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1p24d2c</id>
    <title>Olmo3</title>
    <updated>2025-11-20T14:20:04+00:00</updated>
    <author>
      <name>/u/BreakfastFriendly728</name>
      <uri>https://old.reddit.com/user/BreakfastFriendly728</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p24d2c/olmo3/"&gt; &lt;img alt="Olmo3" src="https://b.thumbs.redditmedia.com/UJppPEN0RZP8y3BZ6uMmEsmjApLD6fufweSjic6DGkY.jpg" title="Olmo3" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ai2 released a series of new olmo 3 weights, including Olmo-3-32B-Think, along with data, code for training and evalution.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/allenai/olmo-3"&gt;https://huggingface.co/collections/allenai/olmo-3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/i98wyfc8bf2g1.png?width=2220&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d52ee933127ce4b88e2e94330d10f9d58bc1b5bb"&gt;https://preview.redd.it/i98wyfc8bf2g1.png?width=2220&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d52ee933127ce4b88e2e94330d10f9d58bc1b5bb&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BreakfastFriendly728"&gt; /u/BreakfastFriendly728 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p24d2c/olmo3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p24d2c/olmo3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p24d2c/olmo3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-20T14:20:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2ivv2</id>
    <title>Faster NeuTTS: can generate over 200 seconds of audio in a single second!</title>
    <updated>2025-11-20T23:35:16+00:00</updated>
    <author>
      <name>/u/SplitNice1982</name>
      <uri>https://old.reddit.com/user/SplitNice1982</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I previously open sourced FastMaya which was also really fast but then set sights on NeuTTS-air. NeuTTS is much smaller and supports better voice cloning as well. So, I heavily optimized it using LMdeploy and some custom batching code for the codec to make it really fast.&lt;/p&gt; &lt;h1&gt;Benefits of this repo&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Much faster, not only for batching but for single batch sizes(1.8x realtime for Maya1 vs 7x realtime for NeuTTS-air)&lt;/li&gt; &lt;li&gt;Works with multiple gpus using tensor parallel for even more speedups. &lt;/li&gt; &lt;li&gt;Great for not only generating audiobooks but voice assistants and much more&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I am working on supporting the multilingual models as well and adding multi speaker synthesis. Also, streaming support and online inference (for serving to many users) should come as well. Initial results are showing **100ms** latency!&lt;/p&gt; &lt;p&gt;I will also add an upsampler to increase audio quality soon. If you have other requests, I will try my best to fulfill them.&lt;/p&gt; &lt;p&gt;Hope this helps people, thanks! Link: &lt;a href="https://github.com/ysharma3501/FastNeuTTS.git"&gt;https://github.com/ysharma3501/FastNeuTTS.git&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SplitNice1982"&gt; /u/SplitNice1982 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2ivv2/faster_neutts_can_generate_over_200_seconds_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2ivv2/faster_neutts_can_generate_over_200_seconds_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2ivv2/faster_neutts_can_generate_over_200_seconds_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-20T23:35:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2l36u</id>
    <title>Echo TTS - 44.1kHz, Fast, Fits under 8GB VRAM - SoTA Voice Cloning</title>
    <updated>2025-11-21T01:11:55+00:00</updated>
    <author>
      <name>/u/HelpfulHand3</name>
      <uri>https://old.reddit.com/user/HelpfulHand3</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;New diffusion based multi-speaker capable TTS model released today by the engineer who made Parakeet (the arch that Dia was based on).&lt;br /&gt; &lt;strong&gt;Voice cloning is available on the space&lt;/strong&gt; but for safety reasons (voice similarity with this model is very high) he has decided for now not to release the speaker encoder. It does come with a large voice bank however.&lt;/p&gt; &lt;p&gt;Supports some tags like (laughs), (coughs), (applause), (singing) etc.&lt;/p&gt; &lt;p&gt;Runs on consumer cards with at least 8GB VRAM.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Echo is a 2.4B DiT that generates Fish Speech S1-DAC latents (and can thus generate 44.1kHz audio; credit to Fish Speech for having trained such a great autoencoder). On an A100, Echo can generate a single 30-second sample of audio in 1.4 seconds (including decoding).&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;License: &lt;strong&gt;CC-BY-NC&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Release Blog Post: &lt;a href="https://jordandarefsky.com/blog/2025/echo/"&gt;https://jordandarefsky.com/blog/2025/echo/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Demo HF Space: &lt;a href="https://huggingface.co/spaces/jordand/echo-tts-preview"&gt;https://huggingface.co/spaces/jordand/echo-tts-preview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Weights: &lt;a href="https://huggingface.co/jordand/echo-tts-no-speaker"&gt;https://huggingface.co/jordand/echo-tts-no-speaker&lt;/a&gt; &lt;a href="https://huggingface.co/jordand/fish-s1-dac-min"&gt;https://huggingface.co/jordand/fish-s1-dac-min&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Code/Github: Coming soon&lt;/p&gt; &lt;p&gt;I haven't had this much fun playing with a TTS since Higgs. This is easily up there with VibeVoice 7b and Higgs Audio v2 despite being 2.4b.&lt;/p&gt; &lt;p&gt;It can clone voices that no other model has been able to do well for me:&lt;/p&gt; &lt;p&gt;&lt;a href="https://vocaroo.com/19PQroylYsoP"&gt;https://vocaroo.com/19PQroylYsoP&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HelpfulHand3"&gt; /u/HelpfulHand3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2l36u/echo_tts_441khz_fast_fits_under_8gb_vram_sota/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2l36u/echo_tts_441khz_fast_fits_under_8gb_vram_sota/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2l36u/echo_tts_441khz_fast_fits_under_8gb_vram_sota/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T01:11:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1p274rk</id>
    <title>Your local LLM agents can be just as good as closed-source models - I open-sourced Stanford's ACE framework that makes agents learn from mistakes</title>
    <updated>2025-11-20T16:09:43+00:00</updated>
    <author>
      <name>/u/cheetguy</name>
      <uri>https://old.reddit.com/user/cheetguy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I implemented Stanford's &lt;a href="https://arxiv.org/abs/2510.04618"&gt;Agentic Context Engineering paper&lt;/a&gt;. The framework makes agents learn from their own execution feedback through in-context learning instead of fine-tuning.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How it works:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Agent runs task → reflects on what worked/failed → curates strategies into playbook → uses playbook on next run&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Improvement:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Paper shows +17.1pp accuracy improvement vs base LLM (≈+40% relative improvement) on agent benchmarks (DeepSeek-V3.1 non-thinking mode), helping close the gap with closed-source models. All through in-context learning (no fine-tuning needed).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My Open-Source Implementation:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Drop into existing agents in ~10 lines of code&lt;/li&gt; &lt;li&gt;Works with local or API models&lt;/li&gt; &lt;li&gt;Real-world test on browser automation agent: &lt;ul&gt; &lt;li&gt;30% → 100% success rate&lt;/li&gt; &lt;li&gt;82% fewer steps&lt;/li&gt; &lt;li&gt;65% decrease in token cost&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Get started:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GitHub: &lt;a href="https://github.com/kayba-ai/agentic-context-engine"&gt;https://github.com/kayba-ai/agentic-context-engine&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Local Model Starter Templates (Ollama, LM Studio, LiteLLM): &lt;a href="https://github.com/kayba-ai/agentic-context-engine/tree/main/examples"&gt;https://github.com/kayba-ai/agentic-context-engine/tree/main/examples&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Would love to hear if anyone tries this with their local setups! Especially curious how it performs with different models.&lt;/p&gt; &lt;p&gt;I'm currently actively improving this based on feedback - &lt;a href="https://github.com/kayba-ai/agentic-context-engine"&gt;⭐ the repo&lt;/a&gt; so you can stay updated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cheetguy"&gt; /u/cheetguy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p274rk/your_local_llm_agents_can_be_just_as_good_as/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p274rk/your_local_llm_agents_can_be_just_as_good_as/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p274rk/your_local_llm_agents_can_be_just_as_good_as/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-20T16:09:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1p29jwc</id>
    <title>Leak: Qwen3-15B-A2B-Base</title>
    <updated>2025-11-20T17:40:12+00:00</updated>
    <author>
      <name>/u/TroyDoesAI</name>
      <uri>https://old.reddit.com/user/TroyDoesAI</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Unmolested and Unreleased Base Qwen3 MoE:&lt;br /&gt; &lt;a href="https://huggingface.co/TroyDoesAI/Qwen3-15B-A2B-Base"&gt;https://huggingface.co/TroyDoesAI/Qwen3-15B-A2B-Base&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TroyDoesAI"&gt; /u/TroyDoesAI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p29jwc/leak_qwen315ba2bbase/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p29jwc/leak_qwen315ba2bbase/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p29jwc/leak_qwen315ba2bbase/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-20T17:40:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1p24aet</id>
    <title>Ai2 just announced Olmo 3, a leading fully open LM suite built for reasoning, chat, &amp; tool use</title>
    <updated>2025-11-20T14:16:57+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p24aet/ai2_just_announced_olmo_3_a_leading_fully_open_lm/"&gt; &lt;img alt="Ai2 just announced Olmo 3, a leading fully open LM suite built for reasoning, chat, &amp;amp; tool use" src="https://b.thumbs.redditmedia.com/xQA2jdAbLxju3aNrYQFjsQ9bmSXaOmQiXK2aPKXj8vw.jpg" title="Ai2 just announced Olmo 3, a leading fully open LM suite built for reasoning, chat, &amp;amp; tool use" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Try Olmo 3 in the Ai2 Playground → &lt;a href="https://playground.allenai.org/"&gt;https://playground.allenai.org/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Download: &lt;a href="https://huggingface.co/collections/allenai/olmo-3-68e80f043cc0d3c867e7efc6"&gt;https://huggingface.co/collections/allenai/olmo-3-68e80f043cc0d3c867e7efc6&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog: &lt;a href="https://allenai.org/blog/olmo3"&gt;https://allenai.org/blog/olmo3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Technical report: &lt;a href="https://allenai.org/papers/olmo3"&gt;https://allenai.org/papers/olmo3&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1p24aet"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p24aet/ai2_just_announced_olmo_3_a_leading_fully_open_lm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p24aet/ai2_just_announced_olmo_3_a_leading_fully_open_lm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-20T14:16:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1ozopxx</id>
    <title>AMA Announcement: MiniMax, The Opensource Lab Behind MiniMax-M2 + Gifts to Our Community (Wednesday, 8AM-11AM PST)</title>
    <updated>2025-11-17T18:50:44+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozopxx/ama_announcement_minimax_the_opensource_lab/"&gt; &lt;img alt="AMA Announcement: MiniMax, The Opensource Lab Behind MiniMax-M2 + Gifts to Our Community (Wednesday, 8AM-11AM PST)" src="https://preview.redd.it/e3scgr4j5v1g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ca0bf85cb4cf2a2b7e12e8d99d515186275c15e3" title="AMA Announcement: MiniMax, The Opensource Lab Behind MiniMax-M2 + Gifts to Our Community (Wednesday, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/e3scgr4j5v1g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozopxx/ama_announcement_minimax_the_opensource_lab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ozopxx/ama_announcement_minimax_the_opensource_lab/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-17T18:50:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1b550</id>
    <title>AMA with MiniMax — Ask Us Anything!</title>
    <updated>2025-11-19T15:46:38+00:00</updated>
    <author>
      <name>/u/OccasionNo6699</name>
      <uri>https://old.reddit.com/user/OccasionNo6699</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;! We’re really excited to be here, thanks for having us.&lt;/p&gt; &lt;p&gt;I’m Skyler (&lt;a href="https://www.reddit.com/user/OccasionNo6699/"&gt;u/OccasionNo6699&lt;/a&gt;), head of engineering at MiniMax, the lab behind:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://x.com/MiniMax__AI/status/1982674798649160175?s=20"&gt;MiniMax-M2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983382728343994414"&gt;Hailuo 2.3&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983661667872600296"&gt;MiniMax Speech 2.6&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983964920493568296"&gt;MiniMax Music 2.0&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Joining me today are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Pengyu Zhao, &lt;a href="https://www.reddit.com/user/Wise_Evidence9973/"&gt;u/Wise_Evidence9973&lt;/a&gt; — Head of LLM Research&lt;/li&gt; &lt;li&gt;Jade Cai, &lt;a href="https://www.reddit.com/user/srtng/"&gt;u/srtng&lt;/a&gt; — Head of Developer Community&lt;/li&gt; &lt;li&gt;midnight_compile , &lt;a href="/u/Top_Cattle_2098"&gt;u/Top_Cattle_2098&lt;/a&gt; — LLM Researcher&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 8AM-11AM PST with our core MiniMax tech team continuing to follow up on questions over the next 48 hours.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OccasionNo6699"&gt; /u/OccasionNo6699 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T15:46:38+00:00</published>
  </entry>
</feed>
