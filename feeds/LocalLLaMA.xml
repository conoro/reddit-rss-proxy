<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-24T14:24:45+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1p5d3uy</id>
    <title>Question...Mac Studio M2 Ultra 128GB RAM or second RTX 5090 Question | Help</title>
    <updated>2025-11-24T09:57:41+00:00</updated>
    <author>
      <name>/u/ajujox</name>
      <uri>https://old.reddit.com/user/ajujox</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, I have a Ryzen 9 5900X with 64GB of RAM and a 5090. I do data science and have local LLMs for my daily work: Qwen 30b and Gemma 3 27b on Arch Linux. &lt;/p&gt; &lt;p&gt;I wanted to broaden my horizons and was looking at a Mac Studio M2 Ultra with 128GB of RAM to add more context and because it's a higher-quality model. But I'm wondering if I should buy a second 5090 and another PSU to handle both, but I think I'd only benefit from the extra RAM and not the extra power, plus it would generate more heat and consume more power for everyday use. I work mornings and afternoons. I tend to leave the PC on a lot. &lt;/p&gt; &lt;p&gt;I'm wondering if the M2 Ultra would be a better daily workstation and I could leave the PC for tasks with CUDA processing. I'm not sure if my budget would allow me to get an M3 Ultra (which I wouldn't be able to afford) or an M4 Max. &lt;/p&gt; &lt;p&gt;Any suggestions or similar experiences? What would you recommend for a 3k budget?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ajujox"&gt; /u/ajujox &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5d3uy/questionmac_studio_m2_ultra_128gb_ram_or_second/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5d3uy/questionmac_studio_m2_ultra_128gb_ram_or_second/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p5d3uy/questionmac_studio_m2_ultra_128gb_ram_or_second/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T09:57:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5hjft</id>
    <title>Looking for honest feedback on LoreTokens + SAIQL (semantic compression vs JSON / TOON / TONL / CSV)</title>
    <updated>2025-11-24T13:49:46+00:00</updated>
    <author>
      <name>/u/barrphite</name>
      <uri>https://old.reddit.com/user/barrphite</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been building something in the ‚ÄúLLM-native data‚Äù space for a while and I finally need other people to poke at it. Reddit is usually the best place to find out if you‚Äôre onto something or just imagining in your own head.&lt;/p&gt; &lt;p&gt;First, this is boring infra. It's not a shiny new wrapped model downloaded from huggingface that makes cool images or videos.&lt;/p&gt; &lt;p&gt;Very high level:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;LoreTokens&lt;/strong&gt; ‚Äì an AI-native semantic compression format&lt;/li&gt; &lt;li&gt;&lt;strong&gt;SAIQL&lt;/strong&gt; ‚Äì a query/database engine designed to run on top of LoreTokens&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The goal is to stop shoving huge JSON blobs into LLMs, but to do it at the &lt;em&gt;semantic&lt;/em&gt; layer, not just by changing brackets.&lt;/p&gt; &lt;h1&gt;How I see the current landscape&lt;/h1&gt; &lt;p&gt;Happy to be corrected on any of this - this is my working mental model:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;CSV&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Great for simple tables and quick imports.&lt;/li&gt; &lt;li&gt;Falls apart once you need nested structure, evolving schemas, or more expressive semantics.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;JSON&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Great for humans, tooling, and general-purpose APIs.&lt;/li&gt; &lt;li&gt;For LLMs, it‚Äôs expensive: repeated keys, quotes, braces, deep nesting. Models keep re-reading structure instead of meaning.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;TOON / TONL&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Both are real improvements over raw JSON.&lt;/li&gt; &lt;li&gt;They reduce repeated keys, punctuation, and boilerplate.&lt;/li&gt; &lt;li&gt;They‚Äôre ‚ÄúLLM-friendlier JSON‚Äù and can save a lot of tokens, especially for uniform arrays.&lt;/li&gt; &lt;li&gt;They also have plenty of their own issues, especially when nesting.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Where I‚Äôm starting to worry a bit is the &lt;strong&gt;compression arms race&lt;/strong&gt; around syntax:&lt;br /&gt; everyone is trying to shave off more characters and tokens, and some of the newer patterns are getting so dense that the model has to guess what the fields actually &lt;em&gt;mean&lt;/em&gt;. At that point you trade JSON bloat for semantic drift and send your agents wandering off into digital peyote land - the hidden cost of TOON-style compression.&lt;/p&gt; &lt;h1&gt;Where LoreTokens are different&lt;/h1&gt; &lt;p&gt;LoreTokens aim to compress &lt;em&gt;meaning&lt;/em&gt;, not just syntax.&lt;/p&gt; &lt;p&gt;Each LoreToken line is designed to encode things like:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;domain (medical, trading, profile, logs, etc.)&lt;/li&gt; &lt;li&gt;concept (symptoms, order book, skills, events, etc.)&lt;/li&gt; &lt;li&gt;subject / entity&lt;/li&gt; &lt;li&gt;output shape (record, table, explanation, timeline, etc.)&lt;/li&gt; &lt;li&gt;status / flags&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;you send a short semantic line that tells the model &lt;em&gt;what&lt;/em&gt; this is and &lt;em&gt;how&lt;/em&gt; it should be expanded. Modern LLMs already like regular, symbolic patterns, so they tend to recognize and work with LoreToken-style lines very naturally once they‚Äôve seen a few examples.&lt;/p&gt; &lt;p&gt;Here is the same question asked to several models to compare Toon vs LoreToken&lt;br /&gt; &lt;a href="https://claude.ai/share/5d420c55-686e-4750-9859-0fd1a234f1ca"&gt;Asking Claude&lt;/a&gt; - &lt;a href="https://chatgpt.com/share/69236d1e-57c4-800c-be15-2ee4e3617d9a"&gt;Asking ChatGPT&lt;/a&gt; - &lt;a href="https://gemini.google.com/share/0b8951d805a6"&gt;Asking Gemini&lt;/a&gt; - &lt;a href="https://grok.com/share/bGVnYWN5LWNvcHk_349b3941-4fde-4855-9a45-961b8e395b07"&gt;Asking Grok&lt;/a&gt; - &lt;a href="https://chat.deepseek.com/share/gugoduliijsglrows1"&gt;Asking Deepseek&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;ChatGPT, Claude, DeepSeek, Gemini, and Grok all independently picked LoreTokens&lt;/strong&gt;. Their reasoning converged on the same three points: &lt;ul&gt; &lt;li&gt;Fewer tokens overall (20‚Äì60% reductions were typical in their estimates).&lt;/li&gt; &lt;li&gt;Zero or near-zero per-row schema cost, because the LoreToken pattern &lt;em&gt;is&lt;/em&gt; the schema.&lt;/li&gt; &lt;li&gt;More direct semantic mapping once the spec is learned, since each segment (MED, NEURO, etc.) behaves like a stable coordinate in the model‚Äôs internal space, not just a human label.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Gemini was the only one that partially defended TOON (slightly easier initial mapping thanks to named fields, which I admit is true), but even it concluded LoreTokens are the better choice for large-scale workloads.&lt;/p&gt; &lt;p&gt;In practice, I‚Äôm seeing two effects:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Big reductions in tokens / storage (roughly 60‚Äì70% in my own workloads)&lt;/li&gt; &lt;li&gt;Less ‚Äúmystery behavior,‚Äù because the semantics stay explicit instead of being stripped away for the sake of a smaller character count&lt;/li&gt; &lt;li&gt;LoreTokens don‚Äôt fully eliminate hallucinations; but they do they box them in. They make the model‚Äôs job more constrained, the semantics more explicit, and the errors easier to detect ‚Äì which usually means fewer, smaller, and more auditable hallucinations, not magic zero. (sorry everyone, I'm trying lol - we all are)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I‚Äôm not claiming it‚Äôs magic ‚Äì I‚Äôm just trying to keep compression on the safe side where the model doesn‚Äôt have to guess (and hallucinate).&lt;/p&gt; &lt;p&gt;Also to note: &lt;strong&gt;Only LoreTokens seem to do this&lt;/strong&gt;: they act as a lossy-syntax, &lt;em&gt;lossless-semantics&lt;/em&gt; compressor, forcing the LLM into semantic manifold regeneration instead of dumb text reconstruction - a true &lt;strong&gt;semantic clean room&lt;/strong&gt;, where the model rebuilds the &lt;em&gt;intended meaning&lt;/em&gt; in its optimal form instead of replaying our messy human draft. See this paper for extended details &amp;gt; &lt;a href="https://github.com/apolloraines/LoreTokens/blob/main/technical_releases/Loretoken_Emergent_Property_Technical_Paper.md"&gt;Emergent_Property_Technical_Paper&lt;/a&gt; - (which I expect 10% will open it, 2% will finish it, 0.5% will actually &lt;em&gt;grok&lt;/em&gt; it.)&lt;/p&gt; &lt;h1&gt;How SAIQL fits in&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;SAIQL&lt;/strong&gt; is the engine piece:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;An AI-native query language and DB that can store and operate directly on LoreTokens (and/or more traditional structures).&lt;/li&gt; &lt;li&gt;Think ‚ÄúPostgres + JSON + glue‚Äù replaced with a lighter-weight engine that understands the semantic lines it‚Äôs storing.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Main use cases I‚Äôm targeting:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Agent memory and state&lt;/li&gt; &lt;li&gt;Long-term knowledge for LLM systems&lt;/li&gt; &lt;li&gt;Workloads where people are currently paying a lot to stream JSON and vectors back and forth&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;What I‚Äôm asking from Reddit&lt;/h1&gt; &lt;p&gt;I‚Äôm not here to sell anything. I haven‚Äôt even started talking to investors yet - I‚Äôm a deep technical guy trying to sanity-check his own work.&lt;/p&gt; &lt;p&gt;I‚Äôd really appreciate if folks here could:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Tell me if this solves a real pain you have, or if I‚Äôm reinventing the wheel badly&lt;/li&gt; &lt;li&gt;Point out where LoreTokens fall apart (RAG, fine-tuning, multi-agent setups, etc.)&lt;/li&gt; &lt;li&gt;Compare this honestly to TOON / TONL: is semantic encoding worth it, or is ‚Äúcompressed JSON‚Äù already good enough for you?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;And for anyone who has the time/interest, it would be incredibly helpful if you could:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Clone the repos&lt;/li&gt; &lt;li&gt;Run the examples&lt;/li&gt; &lt;li&gt;See how it behaves on your own data or agent workloads&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Repos&lt;/h1&gt; &lt;p&gt;If you want to dig in:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;LoreTokens&lt;/strong&gt; (semantic compression format, symbol sets, examples) &lt;a href="https://github.com/apolloraines/LoreTokens?utm_source=chatgpt.com"&gt;https://github.com/apolloraines/LoreTokens&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;SAIQL Engine&lt;/strong&gt; (AI-native query / DB layer that can run on LoreTokens) &lt;a href="https://github.com/apolloraines/SAIQL-Engine_v0.2.1?utm_source=chatgpt.com"&gt;https://github.com/apolloraines/SAIQL-Engine_v0.2.1&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I got my balls busted on here before over LoreTokens. Maybe I didn‚Äôt explain it well (better this time?), or maybe the cost of JSON just wasn‚Äôt on people‚Äôs radar yet. (I can be appreciative of TOON for bringing more awareness to that at least.) I‚Äôm hoping this round goes a lot better üôÇ&lt;/p&gt; &lt;p&gt;I really do appreciate any help. Thanks in advance. In the meantime, I‚Äôll get my bandages ready in case I need to patch up a few new wounds lol. I‚Äôm here for honest, technical feedback ‚Äì including ‚Äúthis is overcomplicated, here‚Äôs a simpler way.‚Äù&lt;/p&gt; &lt;p&gt;Small disclaimer: I had an LLM help me write this post (well, chunks of it, easy to see). I know what I‚Äôm building, but I‚Äôm not great at explaining it, so I let the AI translate my thoughts into clearer English, helping turn my brain-dump into something readable.&lt;/p&gt; &lt;p&gt;Related note: we also designed the Open Lore License (OLL) to give small teams a way to use and share tech like LoreTokens/SAIQL while still helping protect it from being quietly swallowed up by BigCo. I put together a simple builder at &lt;a href="https://openlorelicense.com/"&gt;https://openlorelicense.com/&lt;/a&gt; so you can generate your own version if you like the idea.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/barrphite"&gt; /u/barrphite &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5hjft/looking_for_honest_feedback_on_loretokens_saiql/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5hjft/looking_for_honest_feedback_on_loretokens_saiql/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p5hjft/looking_for_honest_feedback_on_loretokens_saiql/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T13:49:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5dale</id>
    <title>Local LLM performance on AMD Ryzen AI 9 HX 370 iGPU (Radeon 890M) or NPU</title>
    <updated>2025-11-24T10:09:22+00:00</updated>
    <author>
      <name>/u/_Nitor</name>
      <uri>https://old.reddit.com/user/_Nitor</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello! There are very few recent, properly executed, and detailed benchmarks online for the AMD Ryzen AI 9 HX 370 iGPU or NPU when running LLM. They were either made back when Strix Point support was very weak, or they use the CPU, or they run small models. Owners of mini PCs on the HX 370, can you share your experience of which DeepSeek (70B, 32B, 14B) and gpt-oss (120B, 20B) models generate tokens at a decent rate? I am considering buying a mini PC on the HX 370 for the homelab and would like to know if it is worth considering launching LLM on such hardware? In particular, I'm trying to choose between 64 GB and 96 GB of DDR5-5600 RAM. Without using LLM, 64GB would be enough for me with a large margin.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_Nitor"&gt; /u/_Nitor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5dale/local_llm_performance_on_amd_ryzen_ai_9_hx_370/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5dale/local_llm_performance_on_amd_ryzen_ai_9_hx_370/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p5dale/local_llm_performance_on_amd_ryzen_ai_9_hx_370/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T10:09:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1p58cai</id>
    <title>Recommend Coding model</title>
    <updated>2025-11-24T05:03:59+00:00</updated>
    <author>
      <name>/u/Small_Car6505</name>
      <uri>https://old.reddit.com/user/Small_Car6505</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have Ryzen 7800x3D, 64Gb ram with RTX 5090 which model should I try. At the moment I have tried with llama.cpp with Qwen3-coder-30B-A3B-instruct-Bf16. Any other model is better?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Small_Car6505"&gt; /u/Small_Car6505 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p58cai/recommend_coding_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p58cai/recommend_coding_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p58cai/recommend_coding_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T05:03:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5d288</id>
    <title>Best LLM for mobile? Gemma vs Qwen</title>
    <updated>2025-11-24T09:54:49+00:00</updated>
    <author>
      <name>/u/tonyc1118</name>
      <uri>https://old.reddit.com/user/tonyc1118</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was trying to pick a model for my app to run an LLM on mobile. &lt;/p&gt; &lt;p&gt;So I looked at the performance of Gemma gen 1-3, 1-2B, and Qwen gen 1-3, 0.5B-2B.&lt;/p&gt; &lt;p&gt;An interesting observation is that Gemma had a lead in generation 1, but in the past two years, Qwen has caught up. Now Qwen 3 outperforms Gemma 3. &lt;/p&gt; &lt;p&gt;This also seems to mirror the open-source competition between Google/US and Alibaba/China.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Params&lt;/th&gt; &lt;th align="left"&gt;MMLU&lt;/th&gt; &lt;th align="left"&gt;GSM8K&lt;/th&gt; &lt;th align="left"&gt;MATH&lt;/th&gt; &lt;th align="left"&gt;HumanEval&lt;/th&gt; &lt;th align="left"&gt;MBPP&lt;/th&gt; &lt;th align="left"&gt;BBH&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Gemma 1 PT 2B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;2.0B&lt;/td&gt; &lt;td align="left"&gt;42.3&lt;/td&gt; &lt;td align="left"&gt;17.7&lt;/td&gt; &lt;td align="left"&gt;11.8&lt;/td&gt; &lt;td align="left"&gt;22.0&lt;/td&gt; &lt;td align="left"&gt;29.2&lt;/td&gt; &lt;td align="left"&gt;35.2&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Gemma 2 PT 2B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;2.0B&lt;/td&gt; &lt;td align="left"&gt;51.3&lt;/td&gt; &lt;td align="left"&gt;23.9&lt;/td&gt; &lt;td align="left"&gt;15.0&lt;/td&gt; &lt;td align="left"&gt;17.7&lt;/td&gt; &lt;td align="left"&gt;29.6&lt;/td&gt; &lt;td align="left"&gt;‚Äì&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Gemma 3 IT 1B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;1.0B&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;14.7 (MMLU-Pro)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;62.8&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;48.0&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;41.5&lt;/td&gt; &lt;td align="left"&gt;35.2&lt;/td&gt; &lt;td align="left"&gt;39.1&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Qwen 1.5 ‚Äì 0.5B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;0.5B&lt;/td&gt; &lt;td align="left"&gt;39.2&lt;/td&gt; &lt;td align="left"&gt;22.0&lt;/td&gt; &lt;td align="left"&gt;3.1&lt;/td&gt; &lt;td align="left"&gt;12.2&lt;/td&gt; &lt;td align="left"&gt;6.8&lt;/td&gt; &lt;td align="left"&gt;18.3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Qwen 1.5 ‚Äì 1.8B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;1.8B&lt;/td&gt; &lt;td align="left"&gt;46.8&lt;/td&gt; &lt;td align="left"&gt;38.4&lt;/td&gt; &lt;td align="left"&gt;10.1&lt;/td&gt; &lt;td align="left"&gt;20.1&lt;/td&gt; &lt;td align="left"&gt;18.0&lt;/td&gt; &lt;td align="left"&gt;24.2&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Qwen 2 ‚Äì 0.5B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;0.5B&lt;/td&gt; &lt;td align="left"&gt;45.4&lt;/td&gt; &lt;td align="left"&gt;36.5&lt;/td&gt; &lt;td align="left"&gt;10.7&lt;/td&gt; &lt;td align="left"&gt;22.0&lt;/td&gt; &lt;td align="left"&gt;22.0&lt;/td&gt; &lt;td align="left"&gt;28.4&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Qwen 2 ‚Äì 1.5B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;1.5B&lt;/td&gt; &lt;td align="left"&gt;56.5&lt;/td&gt; &lt;td align="left"&gt;58.5&lt;/td&gt; &lt;td align="left"&gt;21.7&lt;/td&gt; &lt;td align="left"&gt;31.1&lt;/td&gt; &lt;td align="left"&gt;37.4&lt;/td&gt; &lt;td align="left"&gt;37.2&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Qwen 2.5 ‚Äì 0.5B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;0.5B&lt;/td&gt; &lt;td align="left"&gt;47.5&lt;/td&gt; &lt;td align="left"&gt;41.6&lt;/td&gt; &lt;td align="left"&gt;19.5&lt;/td&gt; &lt;td align="left"&gt;‚Äì&lt;/td&gt; &lt;td align="left"&gt;29.8&lt;/td&gt; &lt;td align="left"&gt;20.3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Qwen 3 ‚Äì 0.6B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;0.6B&lt;/td&gt; &lt;td align="left"&gt;52.8&lt;/td&gt; &lt;td align="left"&gt;59.6&lt;/td&gt; &lt;td align="left"&gt;32.4&lt;/td&gt; &lt;td align="left"&gt;‚Äì&lt;/td&gt; &lt;td align="left"&gt;36.6&lt;/td&gt; &lt;td align="left"&gt;41.5&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Qwen 3 ‚Äì 1.7B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;1.7B&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;62.6&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;75.4&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;43.5&lt;/td&gt; &lt;td align="left"&gt;‚Äì&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;55.4&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;54.5&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;References:&lt;/p&gt; &lt;p&gt;- Gemma 1: &lt;a href="https://ai.google.dev/gemma/docs/core/model_card"&gt;https://ai.google.dev/gemma/docs/core/model_card&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- Gemma 2: &lt;a href="https://ai.google.dev/gemma/docs/core/model_card_2"&gt;https://ai.google.dev/gemma/docs/core/model_card_2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- Gemma 3: &lt;a href="https://ai.google.dev/gemma/docs/core/model_card_3"&gt;https://ai.google.dev/gemma/docs/core/model_card_3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- Qwen 1.5: &lt;a href="https://qwen.ai/blog?id=qwen1.5"&gt;https://qwen.ai/blog?id=qwen1.5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- Qwen 2: &lt;a href="https://huggingface.co/Qwen/Qwen2-1.5B"&gt;https://huggingface.co/Qwen/Qwen2-1.5B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- Qwen 3: &lt;a href="https://arxiv.org/pdf/2505.09388"&gt;https://arxiv.org/pdf/2505.09388&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tonyc1118"&gt; /u/tonyc1118 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5d288/best_llm_for_mobile_gemma_vs_qwen/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5d288/best_llm_for_mobile_gemma_vs_qwen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p5d288/best_llm_for_mobile_gemma_vs_qwen/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T09:54:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5h2s7</id>
    <title>This app lets you use your phone as a local server and access all your local models in your other devices</title>
    <updated>2025-11-24T13:29:15+00:00</updated>
    <author>
      <name>/u/Ya_SG</name>
      <uri>https://old.reddit.com/user/Ya_SG</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5h2s7/this_app_lets_you_use_your_phone_as_a_local/"&gt; &lt;img alt="This app lets you use your phone as a local server and access all your local models in your other devices" src="https://external-preview.redd.it/cjI0ZjVlZXJpNzNnMcSzsRbD0UIi6J_Onz1fdRwNZdw9hDAxtV2HZ8rdL5SD.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bddbde7180139b0b58b4323002e89e2776b8c743" title="This app lets you use your phone as a local server and access all your local models in your other devices" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, I've been working on this app for so long - originally it was launched on Android about 8 months ago, but now I finally got it to iOS as well.&lt;/p&gt; &lt;p&gt;It can run language models locally like any other local LLM app + it lets you access those models remotely in your local network through REST API making your phone act as a local server. &lt;/p&gt; &lt;p&gt;Plus, it has Apple Foundation model support, local RAG based file upload support, support for remote models - and a lot more features - more than any other local LLM app on Android &amp;amp; iOS.&lt;/p&gt; &lt;p&gt;Everything is free &amp;amp; open-source: &lt;a href="https://github.com/sbhjt-gr/inferra"&gt;https://github.com/sbhjt-gr/inferra&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Currently it uses llama.cpp, but I'm actively working on integrating MLX and MediaPipe (of AI Edge Gallery) as well. &lt;/p&gt; &lt;p&gt;Looks a bit like self-promotion but LocalLLaMA &amp;amp; LocalLLM were the only communities I found where people would find such stuff relevant and would actually want to use it. Let me know what you think. :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ya_SG"&gt; /u/Ya_SG &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/tmy3njdri73g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5h2s7/this_app_lets_you_use_your_phone_as_a_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p5h2s7/this_app_lets_you_use_your_phone_as_a_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T13:29:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5h7xh</id>
    <title>Planning Multi-RTX 5060 Ti Local LLM Workstation (TRX40 / 32‚Äì64GB VRAM)</title>
    <updated>2025-11-24T13:35:38+00:00</updated>
    <author>
      <name>/u/Special-Art-9369</name>
      <uri>https://old.reddit.com/user/Special-Art-9369</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt;&lt;br /&gt; Building my first multi-GPU workstation for running local LLMs (30B+ models) and RAG on personal datasets. Starting with 2√ó RTX 5060 Ti (16GB) on a used TRX40 Threadripper setup, planning to eventually scale to 4 GPUs. Looking for real-world advice on PCIe stability, multi-GPU thermals, case fitment, PSU headroom, and any TRX40 quirks.&lt;/p&gt; &lt;p&gt;Hey all,&lt;/p&gt; &lt;p&gt;I‚Äôm putting together a workstation mainly for local LLM inference and RAG on personal datasets. I‚Äôm leaning toward a used TRX40 platform because of its PCIe lanes, which should help avoid bottlenecks you sometimes see on more mainstream boards. I‚Äôm fairly new to PC building, so I might be overthinking some things‚Äîbut experimenting with local LLMs looks really fun.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Goals:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Run ~30B parameter models, or multiple smaller models in parallel (e.g., GPT OSS 20B) on personal datasets.&lt;/li&gt; &lt;li&gt;Pool VRAM across GPUs (starting with 32GB, aiming for 64GB eventually).&lt;/li&gt; &lt;li&gt;Scale to 3‚Äì4 GPUs later without major headaches.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Current Build Plan (I/O-focused):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;CPU: Threadripper 3960X (used)&lt;/li&gt; &lt;li&gt;Motherboard: MSI TRX40 PRO 10G (used)&lt;/li&gt; &lt;li&gt;GPUs (initial): 2√ó Palit RTX 5060 Ti 16GB&lt;/li&gt; &lt;li&gt;RAM: 64GB DDR4-3200 CL22 (4√ó16GB)&lt;/li&gt; &lt;li&gt;PSU: 1200W 80+ Platinum (ATX 3.1)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Questions for anyone with TRX40 multi-GPU experience:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TRX40 quirks / platform issues&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;BIOS / PCIe: Any issues on the MSI TRX40 PRO 10G that prevent 3-4 GPU slots from running at full x16 PCIe 4.0?&lt;/li&gt; &lt;li&gt;RAM stability: Any compatibility or quad-channel stability issues with CL22 kits?&lt;/li&gt; &lt;li&gt;Multi-GPU surprises: Any unexpected headaches when building a multi-GPU inference box?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Case / cooling&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Open vs closed cases: What works best for multi-GPU setups?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Power supply / spikes&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Will a 1200W Platinum PSU handle 4√ó RTX 5060 Ti plus a Threadripper 3960X (280W)?&lt;/li&gt; &lt;li&gt;Any issues with transient spikes under heavy LLM workloads?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Basically, I‚Äôm just trying to catch any pitfalls or design mistakes before investing in this set up. I‚Äôd love to hear what worked, what didn‚Äôt, and any lessons learned from your own multi-GPU/TRX40 builds.&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Special-Art-9369"&gt; /u/Special-Art-9369 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5h7xh/planning_multirtx_5060_ti_local_llm_workstation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5h7xh/planning_multirtx_5060_ti_local_llm_workstation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p5h7xh/planning_multirtx_5060_ti_local_llm_workstation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T13:35:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4s7nt</id>
    <title>Drummer's Snowpiercer 15B v4 ¬∑ A strong RP model that punches a pack!</title>
    <updated>2025-11-23T17:17:31+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4s7nt/drummers_snowpiercer_15b_v4_a_strong_rp_model/"&gt; &lt;img alt="Drummer's Snowpiercer 15B v4 ¬∑ A strong RP model that punches a pack!" src="https://external-preview.redd.it/QHcTMS_GK1SpPCsYVSA_d521aSr77tuQOduExaTV8io.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=260671c6b499b9df6965e93453782a12c31d98d9" title="Drummer's Snowpiercer 15B v4 ¬∑ A strong RP model that punches a pack!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;While I have your attention, I'd like to ask: Does anyone here honestly bother with models below 12B? Like 8B, 4B, or 2B? I feel like I might have neglected smaller model sizes for far too long.&lt;/p&gt; &lt;p&gt;Also: &amp;quot;Air 4.6 in two weeks!&amp;quot;&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;Snowpiercer v4 is part of the Gen 4.0 series I'm working on that puts more focus on character adherence. YMMV. You might want to check out Gen 3.5/3.0 if Gen 4.0 isn't doing it for you.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/spaces/TheDrummer/directory"&gt;https://huggingface.co/spaces/TheDrummer/directory&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Snowpiercer-15B-v4"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4s7nt/drummers_snowpiercer_15b_v4_a_strong_rp_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p4s7nt/drummers_snowpiercer_15b_v4_a_strong_rp_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T17:17:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4ln6s</id>
    <title>Computer Manufacturer threw my $ 20000 rig down the stairs and now says everything is fine</title>
    <updated>2025-11-23T12:34:18+00:00</updated>
    <author>
      <name>/u/phwlarxoc</name>
      <uri>https://old.reddit.com/user/phwlarxoc</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I bought a custom built Threadripper Pro water-cooled dual RTX 4090 workstation from a builder and had it updated a couple of times with new hardware so that finally it became a rig worth about $20000.&lt;/p&gt; &lt;p&gt;Upon picking up the machine last week from the builder after another upgrade I asked staff that we check together the upgrade before paying and confirming the order fulfilled.&lt;/p&gt; &lt;p&gt;They lifted the machine (still in its box and secured with two styrofoam blocks), on a table, but the heavy box (30kg) slipped from their hands, the box fell on the floor and from there down a staircase where it cartwheeled several times until it stopped at the end of the stairs.&lt;/p&gt; &lt;p&gt;They sent a mail saying they checked the machine and everything is fine.&lt;/p&gt; &lt;p&gt;Who wouldn't expect otherwise.&lt;/p&gt; &lt;p&gt;Can anyone comment on possible damages such an incident can have on the electronics, PCIe Slots, GPUs, watercooling, mainboard etc, ‚Äî also on what damages might have occurred that are not immediately evident, but could e.g. impact signal quality and therefore speed? Would you accept back such a machine?&lt;/p&gt; &lt;p&gt;Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/phwlarxoc"&gt; /u/phwlarxoc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4ln6s/computer_manufacturer_threw_my_20000_rig_down_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4ln6s/computer_manufacturer_threw_my_20000_rig_down_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p4ln6s/computer_manufacturer_threw_my_20000_rig_down_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T12:34:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4jyrv</id>
    <title>No way kimi gonna release new model !!</title>
    <updated>2025-11-23T10:57:51+00:00</updated>
    <author>
      <name>/u/Independent-Wind4462</name>
      <uri>https://old.reddit.com/user/Independent-Wind4462</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4jyrv/no_way_kimi_gonna_release_new_model/"&gt; &lt;img alt="No way kimi gonna release new model !!" src="https://preview.redd.it/1ezldlbumz2g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8ad6c7d5d0f6a6e2b160c885a08a82d80d71ef81" title="No way kimi gonna release new model !!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Wind4462"&gt; /u/Independent-Wind4462 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1ezldlbumz2g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4jyrv/no_way_kimi_gonna_release_new_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p4jyrv/no_way_kimi_gonna_release_new_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T10:57:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4xscg</id>
    <title>Can an expert chime in and explain what is holding Vulkan back from becoming the standard API for ML?</title>
    <updated>2025-11-23T20:59:05+00:00</updated>
    <author>
      <name>/u/A_Chungus</name>
      <uri>https://old.reddit.com/user/A_Chungus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm just getting into GPGPU programming, and my knowledge is limited. I‚Äôve only written a handful of code and mostly just read examples. I‚Äôm trying to understand whether there are any major downsides or roadblocks to writing or contributing to AI/ML frameworks using Vulkan, or whether I should just stick to CUDA or others.&lt;/p&gt; &lt;p&gt;My understanding is that Vulkan is primarily a graphics-focused API, while CUDA, ROCm, and SYCL are more compute-oriented. However, Vulkan has recently been shown to match or even beat CUDA in performance in projects like llama.cpp. With features like &lt;a href="https://www.vulkan.org/user/pages/09.events/vulkanised-2025/T47-Jeff-Bolz-NVIDIA.pdf"&gt;Vulkan Cooperative Vectors&lt;/a&gt;, it seems it possible to squeeze the most performance out of the hardware and only limited by architecture tuning. The only times I see Vulkan lose to CUDA are in a few specific workloads on Linux or when the model exceeds VRAM. In those cases, Vulkan tends to fail or crash, while CUDA still finishes generation, although very slowly.&lt;/p&gt; &lt;p&gt;Since Vulkan can already reach this level of performance and is improving quickly, it seems like a serious contender to challenge CUDA‚Äôs moat and to offer true cross-vendor, cross-platform support unlike the rest. Even if Vulkan never fully matches CUDA‚Äôs performance in every framework, I can still see it becoming the default backend for many applications. For example, Electron dominates desktop development despite its sub-par performance because it makes cross-platform development so easy.&lt;/p&gt; &lt;p&gt;Setting aside companies‚Äô reluctance to invest in Vulkan as part of their AI/ML ecosystems in order to protect their proprietary platforms:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Are vendors actively doing anything to limit its capabilities?&lt;/li&gt; &lt;li&gt;Could we see more frameworks like &lt;a href="https://docs.pytorch.org/tutorials/unstable/vulkan_workflow.html"&gt;PyTorch&lt;/a&gt; adopting it and eventually making Vulkan a go-to cross-vendor solution?&lt;/li&gt; &lt;li&gt;If more contributions were made to Vulkan ecosystem, could it eventually reach the ecosystem that of CUDA has with libraries and tooling, or will Vulkan always be limited as a permanent ‚Äúsecond source‚Äù backend?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Even with the current downsides, I don't think they‚Äôre significant enough to prevent Vulkan from gaining wider adoption in the AI/ML space. Could I be wrong here?&lt;/p&gt; &lt;p&gt;EDIT:&lt;/p&gt; &lt;p&gt;I guess what I'm really asking is if there are any CUDA/Vulkan devs that can provide some input on where they think Vulkan is lacking other than what I mentioned and if it its doable eventually to be feature parity with CUDA. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/A_Chungus"&gt; /u/A_Chungus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4xscg/can_an_expert_chime_in_and_explain_what_is/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4xscg/can_an_expert_chime_in_and_explain_what_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p4xscg/can_an_expert_chime_in_and_explain_what_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T20:59:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5i4dz</id>
    <title>Last week in Multimodal AI - Local Edition</title>
    <updated>2025-11-24T14:14:17+00:00</updated>
    <author>
      <name>/u/Vast_Yak_4147</name>
      <uri>https://old.reddit.com/user/Vast_Yak_4147</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5i4dz/last_week_in_multimodal_ai_local_edition/"&gt; &lt;img alt="Last week in Multimodal AI - Local Edition" src="https://external-preview.redd.it/CayXH4WCl2Z2-U54jDMovfWBN3_44MX4GcqXEoo-xsM.png?width=216&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9880bfac9ded07ef6ee26f773c3c6a508681457e" title="Last week in Multimodal AI - Local Edition" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I curate a weekly newsletter on multimodal AI. Here are the local/open-source highlights from this week:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;HunyuanVideo 1.5 - Open-Source Video Generation&lt;/strong&gt;&lt;br /&gt; ‚Ä¢ Strongest open-source video generation model built on DiT architecture.&lt;br /&gt; ‚Ä¢ High-quality video generation without commercial licensing fees, optimized for accessibility.&lt;br /&gt; ‚Ä¢ &lt;a href="https://hunyuan.tencent.com/video/zh"&gt;Project Page&lt;/a&gt; | &lt;a href="https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5"&gt;GitHub&lt;/a&gt; | &lt;a href="https://huggingface.co/tencent/HunyuanVideo-1.5"&gt;Hugging Face&lt;/a&gt; | &lt;a href="https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5/blob/main/assets/HunyuanVideo_1_5.pdf"&gt;Technical Report&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1p5i4dz/video/pxsn6y8nq73g1/player"&gt;https://reddit.com/link/1p5i4dz/video/pxsn6y8nq73g1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Supertonic TTS - On-Device Speech Synthesis&lt;/strong&gt;&lt;br /&gt; ‚Ä¢ Fast speech model designed to run on-device with minimal resources.&lt;br /&gt; ‚Ä¢ Enables local text-to-speech without cloud dependencies.&lt;br /&gt; ‚Ä¢ &lt;a href="https://huggingface.co/spaces/Supertone/supertonic"&gt;Demo&lt;/a&gt; | &lt;a href="https://github.com/supertone-inc/supertonic/"&gt;GitHub&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1p5i4dz/video/o85kdyznq73g1/player"&gt;https://reddit.com/link/1p5i4dz/video/o85kdyznq73g1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Jan-v2-VL - Extended Task Execution&lt;/strong&gt;&lt;br /&gt; ‚Ä¢ Executes 49 steps in long-horizon tasks without failure (base model stops at 5 steps).&lt;br /&gt; ‚Ä¢ Handles extended task sequences that break other vision-language models.&lt;br /&gt; ‚Ä¢ &lt;a href="https://huggingface.co/collections/janhq/jan-v2-vl"&gt;Hugging Face&lt;/a&gt; | &lt;a href="https://x.com/jandotai/status/1988916861174710686?s=20"&gt;Announcement&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1p5i4dz/video/w1yu32ooq73g1/player"&gt;https://reddit.com/link/1p5i4dz/video/w1yu32ooq73g1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Step-Audio-R1 - Audio Reasoning Model&lt;/strong&gt;&lt;br /&gt; ‚Ä¢ First audio reasoning model with chain-of-thought capabilities.&lt;br /&gt; ‚Ä¢ Outperforms Gemini 2.5 Pro and matches Gemini 3 Pro on audio tasks.&lt;br /&gt; ‚Ä¢ &lt;a href="https://stepaudiollm.github.io/step-audio-r1/"&gt;Project Page&lt;/a&gt; | &lt;a href="https://huggingface.co/papers/2511.15848"&gt;Paper&lt;/a&gt; | &lt;a href="https://github.com/stepfun-ai/Step-Audio-R1"&gt;GitHub&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;FaceFusion ComfyUI - Local Face Swapping&lt;/strong&gt;&lt;br /&gt; ‚Ä¢ Advanced face swapping tool with local ONNX inference.&lt;br /&gt; ‚Ä¢ Built by huygiatrng for the ComfyUI ecosystem.&lt;br /&gt; ‚Ä¢ &lt;a href="https://github.com/huygiatrng/Facefusion_comfyui"&gt;GitHub&lt;/a&gt; | &lt;a href="https://www.reddit.com/r/comfyui/comments/1p3np7v/facefusion_comfyui_advanced_face_swapping_with/"&gt;Reddit&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1p5i4dz/video/nwfumgwpq73g1/player"&gt;https://reddit.com/link/1p5i4dz/video/nwfumgwpq73g1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Checkout the &lt;a href="https://thelivingedge.substack.com/p/multimodal-monday-33-physical-ai?r=12l7fk"&gt;full newsletter&lt;/a&gt; for more demos, papers, and resources.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/submit/?source_id=t3_1p5hq0g"&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vast_Yak_4147"&gt; /u/Vast_Yak_4147 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5i4dz/last_week_in_multimodal_ai_local_edition/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5i4dz/last_week_in_multimodal_ai_local_edition/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p5i4dz/last_week_in_multimodal_ai_local_edition/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T14:14:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5entm</id>
    <title>Speakr v0.5.9 update - Voice profile embeddings and better local model support</title>
    <updated>2025-11-24T11:29:53+00:00</updated>
    <author>
      <name>/u/hedonihilistic</name>
      <uri>https://old.reddit.com/user/hedonihilistic</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5entm/speakr_v059_update_voice_profile_embeddings_and/"&gt; &lt;img alt="Speakr v0.5.9 update - Voice profile embeddings and better local model support" src="https://b.thumbs.redditmedia.com/VQ5tFeH_smOUr_UDaT5MmB2MuNu2jodx8PnMggCIw2I.jpg" title="Speakr v0.5.9 update - Voice profile embeddings and better local model support" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Quick update on Speakr for those who've been following along. Just released v0.5.9 with some changes that are particularly relevant for local setups.&lt;/p&gt; &lt;p&gt;For anyone who hasn't seen this before: Speakr is a self-hosted transcription app that works with Whisper + local LLMs. You record or upload audio, it transcribes with speaker diarization, then you can chat with the transcript or get summaries using whatever model you point it at. The app runs in Docker.&lt;/p&gt; &lt;p&gt;The big addition is voice profile support using speaker embeddings. If you're running my WhisperX API webservice (see below), it now extracts 256-dimensional voice embeddings during transcription. Once you've identified someone in a recording, the system recognizes their voice automatically in future recordings based on the embedding similarity.&lt;/p&gt; &lt;p&gt;Also added some collaboration features (internal sharing, teams, retention policies) if you're running this for multiple people. All configurable through environment variables.&lt;/p&gt; &lt;p&gt;I put together a &lt;a href="https://github.com/murtaza-nasir/whisperx-asr-service"&gt;companion ASR webservice&lt;/a&gt; for this that runs WhisperX with the latest pyannote models. It's not meant to be production-grade, more of an experimental reference implementation, but it handles the diarization, time alignment, and embedding extraction. You can still use the standard Whisper ASR webservice if you don't need voice profiles.&lt;/p&gt; &lt;p&gt;The voice recognition uses cosine similarity matching against stored profiles and works pretty well in practice. I've been testing it and it's accurate enough that I rarely need to manually select speaker labels anymore. The embeddings are stored locally in your database, nothing leaves your system.&lt;/p&gt; &lt;p&gt;The upgrade path is straightforward but make sure to backup first since there are database schema changes. Everything's opt-in through env vars so your existing setup should not break.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/murtaza-nasir/speakr"&gt;GitHub&lt;/a&gt; | &lt;a href="https://murtaza-nasir.github.io/speakr"&gt;Docs&lt;/a&gt; | &lt;a href="https://murtaza-nasir.github.io/speakr/screenshots/"&gt;Screenshots&lt;/a&gt; | &lt;a href="https://hub.docker.com/r/learnedmachine/speakr"&gt;Docker Hub&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know if you hit any issues upgrading or have questions about the new features.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hedonihilistic"&gt; /u/hedonihilistic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1p5entm"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5entm/speakr_v059_update_voice_profile_embeddings_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p5entm/speakr_v059_update_voice_profile_embeddings_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T11:29:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4t5ix</id>
    <title>I created a llama.cpp fork with the Rockchip NPU integration as an accelerator and the results are already looking great!</title>
    <updated>2025-11-23T17:54:06+00:00</updated>
    <author>
      <name>/u/Inv1si</name>
      <uri>https://old.reddit.com/user/Inv1si</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4t5ix/i_created_a_llamacpp_fork_with_the_rockchip_npu/"&gt; &lt;img alt="I created a llama.cpp fork with the Rockchip NPU integration as an accelerator and the results are already looking great!" src="https://external-preview.redd.it/YWZjazBjYjBwMTNnMfl_KE3bRLUmxUrgo6sq7iH5IJtc0qUYB-cQv58tKBaC.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=59e7a57c308ef96e98b98c0b7dd565ffc1a5aa0c" title="I created a llama.cpp fork with the Rockchip NPU integration as an accelerator and the results are already looking great!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Inv1si"&gt; /u/Inv1si &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9r0ixbb0p13g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4t5ix/i_created_a_llamacpp_fork_with_the_rockchip_npu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p4t5ix/i_created_a_llamacpp_fork_with_the_rockchip_npu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T17:54:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1p552bw</id>
    <title>[Update] Epstein Files dataset stays open and ungated on Hugging Face</title>
    <updated>2025-11-24T02:20:19+00:00</updated>
    <author>
      <name>/u/tensonaut</name>
      <uri>https://old.reddit.com/user/tensonaut</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Thank you to everyone who provided feedback on our previous post. We agree with your comments - public data should stay public.&lt;/p&gt; &lt;p&gt;As for maintaining the data, we kindly request you to go through this &lt;a href="https://huggingface.co/blog/tensonaut/the-epstein-files"&gt;data usage article&lt;/a&gt; and contribute as volunteer in any way you can. Every small contribution is valuable - priority wise adding additional data from official sources while performing data integrity is of utmost importance&lt;/p&gt; &lt;p&gt;We're creating a central hub for all the investigative tools being built on this dataset. We already have 5 projects from this sub. If you are working on any tool to help journalists to search through the documents efficiently or share findings you've made, we request you to submit a PR &lt;a href="https://github.com/EF20K/Projects"&gt;here&lt;/a&gt; so we can update our documentation and have a central index of all the tools that journalists can use.&lt;/p&gt; &lt;p&gt;Thank you again to everyone who provided feedback and support. This dataset exists because of your feedbacks and suggestions, and we look forward to continuing to build this resource with this sub &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tensonaut"&gt; /u/tensonaut &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p552bw/update_epstein_files_dataset_stays_open_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p552bw/update_epstein_files_dataset_stays_open_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p552bw/update_epstein_files_dataset_stays_open_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T02:20:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5eyi6</id>
    <title>Best open-source models alternative to openai realtime models or how to achieve ultra low latency to create a conversational agent</title>
    <updated>2025-11-24T11:46:33+00:00</updated>
    <author>
      <name>/u/Ai_Peep</name>
      <uri>https://old.reddit.com/user/Ai_Peep</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am currently working on a real time voice agent and so far i've been using openai realtime models. Now i want to deploy opensource model instead of openai.&lt;/p&gt; &lt;p&gt;I want to knwo is there any opensource model that are similar to openai realtime models. like asr, llm ,tts in unified realtime arch.&lt;/p&gt; &lt;p&gt;if it is not there, how we can achieve minimal latency?&lt;/p&gt; &lt;p&gt;Thanks in advance&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ai_Peep"&gt; /u/Ai_Peep &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5eyi6/best_opensource_models_alternative_to_openai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5eyi6/best_opensource_models_alternative_to_openai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p5eyi6/best_opensource_models_alternative_to_openai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T11:46:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5afo7</id>
    <title>My chatbot went rogue again‚Ä¶ I think it hates me lol</title>
    <updated>2025-11-24T07:05:17+00:00</updated>
    <author>
      <name>/u/Aggravating_Log9704</name>
      <uri>https://old.reddit.com/user/Aggravating_Log9704</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Trying to fine-tune a bot for customer support but if users nudge it even slightly, it starts rambling conspiracy theories or making up company policies we never created.&lt;/p&gt; &lt;p&gt;I swear it behaves until one guy on the team tries something weird, then bam chaos.&lt;/p&gt; &lt;p&gt;How are y‚Äôall keeping your bots from acting like little internet feral gremlins&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aggravating_Log9704"&gt; /u/Aggravating_Log9704 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5afo7/my_chatbot_went_rogue_again_i_think_it_hates_me/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5afo7/my_chatbot_went_rogue_again_i_think_it_hates_me/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p5afo7/my_chatbot_went_rogue_again_i_think_it_hates_me/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T07:05:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1p56v22</id>
    <title>It been 2 years but why llama 3.1 8B still a popular choice to fine tune?</title>
    <updated>2025-11-24T03:47:07+00:00</updated>
    <author>
      <name>/u/dheetoo</name>
      <uri>https://old.reddit.com/user/dheetoo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;the model is so old now but new fine tuned model with this llama 3.1 8B as base still come out, do you think this trend will shift to olmo3 7B as a newer and more open ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dheetoo"&gt; /u/dheetoo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p56v22/it_been_2_years_but_why_llama_31_8b_still_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p56v22/it_been_2_years_but_why_llama_31_8b_still_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p56v22/it_been_2_years_but_why_llama_31_8b_still_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T03:47:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1p55dbq</id>
    <title>What‚Äôs the best High Parameter (100B+) Local LLM for NSFW RP?</title>
    <updated>2025-11-24T02:34:53+00:00</updated>
    <author>
      <name>/u/LyutsiferSafin</name>
      <uri>https://old.reddit.com/user/LyutsiferSafin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have about 400 GB GPU memory, what would be the best NSFW RP model I can try locally?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LyutsiferSafin"&gt; /u/LyutsiferSafin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p55dbq/whats_the_best_high_parameter_100b_local_llm_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p55dbq/whats_the_best_high_parameter_100b_local_llm_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p55dbq/whats_the_best_high_parameter_100b_local_llm_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T02:34:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5frn9</id>
    <title>[Release] Hypnos i1-8B: I fine-tuned Hermes 3 on REAL IBM Quantum Computer data (133-qubit GHZ states). Beats Llama-70B in Logic.</title>
    <updated>2025-11-24T12:28:42+00:00</updated>
    <author>
      <name>/u/Disastrous_Bid5976</name>
      <uri>https://old.reddit.com/user/Disastrous_Bid5976</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;! üëã&lt;/p&gt; &lt;p&gt;Its my first post here, and I‚Äôm excited to share a weird experiment I have been working on. I wanted to see what happens if we inject &lt;strong&gt;true physical entropy&lt;/strong&gt; from a quantum processor into the SFT stage of an LLM.&lt;/p&gt; &lt;p&gt;So, I got access to IBM Quantum's latest chips (&lt;strong&gt;Heron r2&lt;/strong&gt; &amp;amp; &lt;strong&gt;Heron r1&lt;/strong&gt;, 133+ qubits) and ran some entanglement experiments (GHZ state). I took the raw measurement data ‚Äî which contains true quantum randomness and hardware noise ‚Äî and mixed it into a high-quality reasoning dataset. Meet Hypnos i1-8B!&lt;br /&gt; Results (Benchmarks vs Llama 3.1 Base)&lt;/p&gt; &lt;p&gt;The reasoning capabilities jumped significantly due to the dataset mix:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Logic (BBH):&lt;/strong&gt; &lt;strong&gt;~68.5%&lt;/strong&gt; (Beats base Llama-3-70B in specific logic tasks).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Math (MATH):&lt;/strong&gt; &lt;strong&gt;~60%+&lt;/strong&gt; (Huge improvement over base).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Instruction Following:&lt;/strong&gt; &lt;strong&gt;~85%&lt;/strong&gt; (Very obedient).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Why Quantum Data? &lt;/p&gt; &lt;p&gt;LLMs tend to suffer from mode collapse or become too &amp;quot;robotic&amp;quot; after heavy fine-tuning. My hypothesis was that injecting real-world quantum noise would act as a form of &lt;strong&gt;Data-Driven Stochastic Regularization&lt;/strong&gt;, giving the model a unique &amp;quot;temperature&amp;quot; and preventing it from overfitting to synthetic reasoning patterns.&lt;/p&gt; &lt;p&gt;I've uploaded Q4_K_M and Q8_0 quants.&lt;/p&gt; &lt;p&gt;Check this out on Ollama or LM Studio!&lt;br /&gt; &lt;a href="https://huggingface.co/squ11z1/Hypnos-i1-8B"&gt;https://huggingface.co/squ11z1/Hypnos-i1-8B&lt;/a&gt; or &lt;code&gt;ollama run squ11z1/hypnos-i1-8B&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Disastrous_Bid5976"&gt; /u/Disastrous_Bid5976 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5frn9/release_hypnos_i18b_i_finetuned_hermes_3_on_real/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5frn9/release_hypnos_i18b_i_finetuned_hermes_3_on_real/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p5frn9/release_hypnos_i18b_i_finetuned_hermes_3_on_real/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T12:28:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5fe9u</id>
    <title>Kimi: Wait... I beat Gemini 3? For real?</title>
    <updated>2025-11-24T12:09:46+00:00</updated>
    <author>
      <name>/u/xiaoruhao</name>
      <uri>https://old.reddit.com/user/xiaoruhao</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5fe9u/kimi_wait_i_beat_gemini_3_for_real/"&gt; &lt;img alt="Kimi: Wait... I beat Gemini 3? For real?" src="https://b.thumbs.redditmedia.com/vHmqwt4RbzyHVTmK5Rpwoiu8qk8apNZx0Pd1Q5Gdc7w.jpg" title="Kimi: Wait... I beat Gemini 3? For real?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/3d2q76ci473g1.jpg?width=1194&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=f00ae8be20af807202bfbb40d8cd9e4e18f0a736"&gt;https://preview.redd.it/3d2q76ci473g1.jpg?width=1194&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=f00ae8be20af807202bfbb40d8cd9e4e18f0a736&lt;/a&gt;&lt;/p&gt; &lt;p&gt;gguf when&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xiaoruhao"&gt; /u/xiaoruhao &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5fe9u/kimi_wait_i_beat_gemini_3_for_real/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5fe9u/kimi_wait_i_beat_gemini_3_for_real/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p5fe9u/kimi_wait_i_beat_gemini_3_for_real/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T12:09:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5by1a</id>
    <title>Qwen3-Next support in llama.cpp almost ready!</title>
    <updated>2025-11-24T08:41:57+00:00</updated>
    <author>
      <name>/u/beneath_steel_sky</name>
      <uri>https://old.reddit.com/user/beneath_steel_sky</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5by1a/qwen3next_support_in_llamacpp_almost_ready/"&gt; &lt;img alt="Qwen3-Next support in llama.cpp almost ready!" src="https://external-preview.redd.it/HHVAlPQ4eWe2-qdcom6wml40H6sWoSOnzo2PuJ4icH8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=86d583e47273dca1af24d1e45d57a0f1544aa04c" title="Qwen3-Next support in llama.cpp almost ready!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/beneath_steel_sky"&gt; /u/beneath_steel_sky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/issues/15940#issuecomment-3567006967"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5by1a/qwen3next_support_in_llamacpp_almost_ready/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p5by1a/qwen3next_support_in_llamacpp_almost_ready/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T08:41:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5epot</id>
    <title>The most objectively correct way to abliterate so far - ArliAI/GLM-4.5-Air-Derestricted</title>
    <updated>2025-11-24T11:32:45+00:00</updated>
    <author>
      <name>/u/Arli_AI</name>
      <uri>https://old.reddit.com/user/Arli_AI</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5epot/the_most_objectively_correct_way_to_abliterate_so/"&gt; &lt;img alt="The most objectively correct way to abliterate so far - ArliAI/GLM-4.5-Air-Derestricted" src="https://external-preview.redd.it/8n5MhbkzXEcl9NlvZMbb8GGre-k1VjQ0kDAKe7qQtQM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9334318d3d29cfd953050dfdf981bc10db9cc00b" title="The most objectively correct way to abliterate so far - ArliAI/GLM-4.5-Air-Derestricted" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, this is Owen Arli from &lt;strong&gt;&lt;a href="https://www.arliai.com"&gt;Arli AI&lt;/a&gt;&lt;/strong&gt; and this is the first model release we created in a while. We previously created models finetuned for more creativity with our &lt;a href="https://huggingface.co/collections/ArliAI/rpr-models"&gt;RpR&lt;/a&gt; and &lt;a href="https://huggingface.co/collections/ArliAI/rpmax-v1-models"&gt;RPMax&lt;/a&gt; models.&lt;/p&gt; &lt;p&gt;After seeing the post by Jim Lai on Norm-Preserving Biprojected Abliteration &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1oypwa7/a_more_surgical_approach_to_abliteration/"&gt;here&lt;/a&gt;, I immediately thought that no one has done abliteration this way and that the &amp;quot;norm-preserving&amp;quot; part was a brilliant improvement in the method to abliterate models, and appears to me like it is objectively the best way to abliterate models. You can find the full technical details in his post, but I will explain the gist of it here.&lt;/p&gt; &lt;h1&gt;The problem:&lt;/h1&gt; &lt;p&gt;Typical abliteration methods finds the refusal vector and simply subtracts it from the weights, this causes the &amp;quot;length&amp;quot; (Norm) of the weight vectors to be altered. This is a problem because this &amp;quot;length&amp;quot; usually dictates how &amp;quot;important&amp;quot; a neuron is and how much it contributes, so changing it will cause damage to the model's general intelligence.&lt;/p&gt; &lt;h1&gt;The solution:&lt;/h1&gt; &lt;p&gt;This Norm-Preserving technique modifies the direction the weights point in, but forces them to keep their original length.&lt;/p&gt; &lt;p&gt;Essentially, by removing the refusal in this way you can potentially also improve the model's performance instead of diminishing it. &lt;/p&gt; &lt;p&gt;Trying out the &lt;a href="https://huggingface.co/grimjim/gemma-3-12b-it-norm-preserved-biprojected-abliterated"&gt;Gemma 3 12B&lt;/a&gt; model example, it clearly works extremely well compared to regular abliteration methods that often leaves the model broken until further finetuning. Which explains why the model ranks so high in the UGI leaderboard even though its base was Gemma 3 12B which is a notoriously censored model.&lt;/p&gt; &lt;h1&gt;The result:&lt;/h1&gt; &lt;p&gt;Armed with a new 2xRTX Pro 6000 server I just built for Arli AI model experimentation, I set out to try and apply this abliteration technique to the much larger and smarter GLM-4.5-Air. Which ended up in what I think is undoubtedly one of the most interesting model I have ever used.&lt;/p&gt; &lt;p&gt;Its not that GLM-4.5-Air is usually plagued with refusals, but using this &amp;quot;Derestricted&amp;quot; version feels like the model suddenly becomes free to do anything it wants without trying to &amp;quot;align&amp;quot; to a non-existent guideline either visibly or subconsciously. It's hard to explain without trying it out yourself.&lt;/p&gt; &lt;p&gt;For an visible example, I bet that those of you running models locally or through an API will definitely have tried to add a system prompt that says &amp;quot;You are a person and not an AI&amp;quot; or something along those lines. Usually even with such a system prompt and nothing in the context that suggests it is an AI, the model will stubbornly still insist that it is an AI and it is unable to do &amp;quot;human-like&amp;quot; things. With this model, just adding that prompt immediately allows the model to pretend to act like a human in its response. No hesitation or any coaxing needed. &lt;/p&gt; &lt;p&gt;The most impressive part about this abliteration technique is definitely the fact that it has somehow made the model a better instruction follower instead of just a braindead NSFW-capable model from typical abliteration. As for it's intelligence, it has not been benchmarked but I believe that just using the model and feeling it out to see if it has degraded in capabilities is better than just checking benchmarks. Which in this case, the model does feel like it is just as smart if not better than the original GLM-4.5-Air.&lt;/p&gt; &lt;p&gt;You can find the model available on our API, or you can download them yourself from the HF links below!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Model downloads:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Original: &lt;a href="https://huggingface.co/ArliAI/GLM-4.5-Air-Derestricted"&gt;https://huggingface.co/ArliAI/GLM-4.5-Air-Derestricted&lt;/a&gt;&lt;/li&gt; &lt;li&gt;FP8: &lt;a href="https://huggingface.co/ArliAI/GLM-4.5-Air-Derestricted-FP8"&gt;https://huggingface.co/ArliAI/GLM-4.5-Air-Derestricted-FP8&lt;/a&gt;&lt;/li&gt; &lt;li&gt;INT8: &lt;a href="https://huggingface.co/ArliAI/GLM-4.5-Air-Derestricted-W8A8-INT8"&gt;https://huggingface.co/ArliAI/GLM-4.5-Air-Derestricted-W8A8-INT8&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We will be working to create more of these Derestricted models, along with many new finetuned models too!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Arli_AI"&gt; /u/Arli_AI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/ArliAI/GLM-4.5-Air-Derestricted"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5epot/the_most_objectively_correct_way_to_abliterate_so/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p5epot/the_most_objectively_correct_way_to_abliterate_so/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T11:32:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1ozopxx</id>
    <title>AMA Announcement: MiniMax, The Opensource Lab Behind MiniMax-M2 + Gifts to Our Community (Wednesday, 8AM-11AM PST)</title>
    <updated>2025-11-17T18:50:44+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozopxx/ama_announcement_minimax_the_opensource_lab/"&gt; &lt;img alt="AMA Announcement: MiniMax, The Opensource Lab Behind MiniMax-M2 + Gifts to Our Community (Wednesday, 8AM-11AM PST)" src="https://preview.redd.it/e3scgr4j5v1g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ca0bf85cb4cf2a2b7e12e8d99d515186275c15e3" title="AMA Announcement: MiniMax, The Opensource Lab Behind MiniMax-M2 + Gifts to Our Community (Wednesday, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/e3scgr4j5v1g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozopxx/ama_announcement_minimax_the_opensource_lab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ozopxx/ama_announcement_minimax_the_opensource_lab/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-17T18:50:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1b550</id>
    <title>AMA with MiniMax ‚Äî Ask Us Anything!</title>
    <updated>2025-11-19T15:46:38+00:00</updated>
    <author>
      <name>/u/OccasionNo6699</name>
      <uri>https://old.reddit.com/user/OccasionNo6699</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;! We‚Äôre really excited to be here, thanks for having us.&lt;/p&gt; &lt;p&gt;I‚Äôm Skyler (&lt;a href="https://www.reddit.com/user/OccasionNo6699/"&gt;u/OccasionNo6699&lt;/a&gt;), head of engineering at MiniMax, the lab behind:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://x.com/MiniMax__AI/status/1982674798649160175?s=20"&gt;MiniMax-M2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983382728343994414"&gt;Hailuo 2.3&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983661667872600296"&gt;MiniMax Speech 2.6&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983964920493568296"&gt;MiniMax Music 2.0&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Joining me today are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Pengyu Zhao, &lt;a href="https://www.reddit.com/user/Wise_Evidence9973/"&gt;u/Wise_Evidence9973&lt;/a&gt; ‚Äî Head of LLM Research&lt;/li&gt; &lt;li&gt;Jade Cai, &lt;a href="https://www.reddit.com/user/srtng/"&gt;u/srtng&lt;/a&gt; ‚Äî Head of Developer Community&lt;/li&gt; &lt;li&gt;midnight_compile , &lt;a href="/u/Top_Cattle_2098"&gt;u/Top_Cattle_2098&lt;/a&gt; ‚Äî LLM Researcher&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 8AM-11AM PST with our core MiniMax tech team continuing to follow up on questions over the next 48 hours.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OccasionNo6699"&gt; /u/OccasionNo6699 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T15:46:38+00:00</published>
  </entry>
</feed>
