<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-08T13:06:15+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1q7a9di</id>
    <title>I built my own personal AI exocortex (local, private, learns my style) — and it now does 80–90% of my work and called it BuddAI</title>
    <updated>2026-01-08T12:14:56+00:00</updated>
    <author>
      <name>/u/Pitiful-Fault-8109</name>
      <uri>https://old.reddit.com/user/Pitiful-Fault-8109</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For the last 8 years I’ve been building a system I could never quite name. Something between a second brain, a coding partner, and a digital version of myself.&lt;/p&gt; &lt;p&gt;Today it finally clicked:&lt;br /&gt; BuddAI — my personal AI exocortex.&lt;/p&gt; &lt;p&gt;It runs 100% locally using Ollama models.&lt;br /&gt; It’s trained on my repos, my notes, my documentation, and my patterns.&lt;br /&gt; It writes code in my tone, my structure, my logic.&lt;/p&gt; &lt;p&gt;I correct the last 10–20%, teach it the fix, and it never repeats the mistake.&lt;/p&gt; &lt;p&gt;My efficiency on ESP32 C3 builds went from: - 25% → 60% → 95%&lt;/p&gt; &lt;p&gt;I’m now producing clean code in hours instead of days.&lt;/p&gt; &lt;p&gt;The goal isn’t to replace myself.&lt;br /&gt; It’s to scale myself.&lt;/p&gt; &lt;p&gt;Everyone should have access to their own BuddAI — not a cloud assistant, but a digital twin that grows with you.&lt;/p&gt; &lt;p&gt;The project is open-source (MIT).&lt;br /&gt; If you want to try it or fork it, here’s the repo:&lt;br /&gt; &lt;a href="https://github.com/JamesTheGiblet/BuddAI"&gt;https://github.com/JamesTheGiblet/BuddAI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer questions or share more details.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pitiful-Fault-8109"&gt; /u/Pitiful-Fault-8109 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7a9di/i_built_my_own_personal_ai_exocortex_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7a9di/i_built_my_own_personal_ai_exocortex_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7a9di/i_built_my_own_personal_ai_exocortex_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T12:14:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7amat</id>
    <title>NVFP4 for local inference</title>
    <updated>2026-01-08T12:33:19+00:00</updated>
    <author>
      <name>/u/v01dm4n</name>
      <uri>https://old.reddit.com/user/v01dm4n</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently got a 5060Ti 16G and was toying around with some models. I decided to explore how much boost NVFP4 gives to the token generation performance. So benchmarked two models for local inference:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Ollama serving qwen3:8b-q4_K_M = 70 t/s&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;VLLM serving nvidia/Qwen3-8B-NVFP4 = 60 t/s&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Both generated ~1000 tokens on a simple 50-token prompt. The token generation performance was reported via `--verbose` flag in ollama and via logs generated by `vllm serve`.&lt;/p&gt; &lt;p&gt;Now, Ollama is based on llama.cpp and uses its own quantization method, which is then handled using cuda kernels. However, VLLM has support for nvfp4 and should have been able to carry out fp4 arithmetic ops directly using hardware support on a Blackwell GPU.&lt;/p&gt; &lt;p&gt;So I was expecting vllm to perform better but that is clearly not the case. So either Ollama is way faster than VLLM or I am doing something wrong. What do you think?&lt;/p&gt; &lt;p&gt;Also, is there a way I could compare apples-to-apples, i.e. does there exist another Qwen3:8b fp4 model that can be run using vllm but does not make use of nvfp4?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/v01dm4n"&gt; /u/v01dm4n &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7amat/nvfp4_for_local_inference/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7amat/nvfp4_for_local_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7amat/nvfp4_for_local_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T12:33:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7b3wy</id>
    <title>A 2.5M 10MB TinyStories model trained using GRU and attention (vs.TinyStories-1M)</title>
    <updated>2026-01-08T12:57:45+00:00</updated>
    <author>
      <name>/u/ValuableLucky8566</name>
      <uri>https://old.reddit.com/user/ValuableLucky8566</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Using a 20MB TinyStories datasheet, this TinyStories model 5x smaller than TinyStories-1M.&lt;/p&gt; &lt;p&gt;Since this was trained on google colab free(nvidia t4), the loss is only converged to ~0.75.&lt;/p&gt; &lt;p&gt;The architecture used was a hybrid of GRU, particularly GRUcell with a single attention layer.&lt;/p&gt; &lt;p&gt;In a single, large GRUcell layer, I used a residual memory logic which writes decoded data into the drive, and feeds it to the input as for the hidden state.&lt;/p&gt; &lt;p&gt;The model creates a proposed memory:&lt;/p&gt; &lt;p&gt;M~t=tanh⁡(Wcht+bc)&lt;/p&gt; &lt;p&gt;Finally, the old memory is mixed with the new one:&lt;/p&gt; &lt;p&gt;Mt=(1−pt)⊙Mt−1+pt⊙M~t&lt;/p&gt; &lt;p&gt;This allows for the architecture to train a model so small (0.36M), that can memorize words and output meaningful words at a train loss of 2.2.&lt;/p&gt; &lt;p&gt;Finally, I added a self-attention layer that allows the model to see context of the previous words to a limited extent. This allows the model to remember what it said 5-10 words ago and prevent complete drifting, which was a limitation of standard GRU.&lt;/p&gt; &lt;p&gt;This brings the attention cost to O(T³), but the model still remains faster than TinyStories-1M (50MB) for short bursts below ~300 tokens when self attention becomes an overhead.&lt;/p&gt; &lt;p&gt;Example output(TinyStories-1M): &lt;/p&gt; &lt;p&gt;Prompt: once upon a time&lt;/p&gt; &lt;p&gt;Output: once upon a time there was an old girl named Lily. She loved to play with her toys and pretend she was his friend. One day, they went outside to see the park. The sun looked blue and it had big eyes.&lt;/p&gt; &lt;p&gt;Lily's mom said &amp;quot;Let me help you get my doll!&amp;quot; They started to walk around the park and had lots of fun together. When it was dark and it seemed like something strange happened, a little bunny came up over to them.&lt;/p&gt; &lt;p&gt;&amp;quot;Hello there,&amp;quot; said Mom. &amp;quot;It's so pretty.&amp;quot;&lt;/p&gt; &lt;p&gt;The bunny smiled at Lily and said, &amp;quot;Yes! I'm glad we can find some more toys later.&amp;quot; Then he opened his backpack and found that she was able to provide for him in a special place where he could have all the toys ready!&lt;/p&gt; &lt;p&gt;(165 words, 67.97 it/s) (could be roughly around 200 chars/sec for BPE tokeniser)&lt;/p&gt; &lt;p&gt;tinystoriesgru:&lt;/p&gt; &lt;p&gt;Prompt: once upon a time&lt;br /&gt; Output: once upon a time to hear the wolf with a smile on his face. She was so happy that the monster was so cold.&lt;/p&gt; &lt;p&gt;But then, the piece of colorful circle came in. She wanted to see what was inside, but she thought it would be fun. She started to cry and started to cry. She quickly ran and ran until she found the crayon and started to cry.&lt;/p&gt; &lt;p&gt;The cat saw the pretty flower and started to shake and showed them the magazine. She thought it would be fun to cut the leaves. She was so happy with her new ball. She wanted to take h&lt;/p&gt; &lt;p&gt;(500 tokens, 112.02 it/s)&lt;/p&gt; &lt;p&gt;At lower characters, the GRU scales to be much faster while the transformer remains consistent with 67-68it/s, for more/less words.&lt;/p&gt; &lt;p&gt;The pure transformer continues to have better context overall.&lt;/p&gt; &lt;p&gt;I've included the &lt;a href="http://train.py"&gt;train.py&lt;/a&gt; here (if anyone can train it further):&lt;br /&gt; &lt;a href="https://github.com/kavyamali/tinystoriesgru"&gt;https://github.com/kavyamali/tinystoriesgru&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thank you for reading.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ValuableLucky8566"&gt; /u/ValuableLucky8566 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7b3wy/a_25m_10mb_tinystories_model_trained_using_gru/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7b3wy/a_25m_10mb_tinystories_model_trained_using_gru/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7b3wy/a_25m_10mb_tinystories_model_trained_using_gru/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T12:57:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7b68v</id>
    <title>It's so hard to run llm on android.</title>
    <updated>2026-01-08T13:00:41+00:00</updated>
    <author>
      <name>/u/shoonee_balavolka</name>
      <uri>https://old.reddit.com/user/shoonee_balavolka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7b68v/its_so_hard_to_run_llm_on_android/"&gt; &lt;img alt="It's so hard to run llm on android." src="https://preview.redd.it/wkxdt8uoi4cg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=db1e9df108a68073e5009d8ff40cca532ff1ec2c" title="It's so hard to run llm on android." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I don't think this is very good. Lately, I’ve been fine-tuning Gemma 3 1B using multi-turn chat data, then converting it to TFLite/Task to test in my app. I was aiming for something like those character chat sites, but the accuracy in the app has been terrible no matter what I do. The weird part is, when I converted the same fine-tuned model to GGUF and tested it on my PC, it performed perfectly. It seems like the conversion through 'ai-edge-torch' is where everything falls apart, making the model practically useless. I’m going to try a few GitHub projects that run GGUF on Android. If that doesn't work, I’m seriously considering putting my on-device LLM projects on hold for a while.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shoonee_balavolka"&gt; /u/shoonee_balavolka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wkxdt8uoi4cg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7b68v/its_so_hard_to_run_llm_on_android/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7b68v/its_so_hard_to_run_llm_on_android/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T13:00:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1q70t3h</id>
    <title>Using a 3060 12gb (64g normal ram), best local uncensored writing model?</title>
    <updated>2026-01-08T03:22:31+00:00</updated>
    <author>
      <name>/u/Cartoonwhisperer</name>
      <uri>https://old.reddit.com/user/Cartoonwhisperer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been a writer for quite some time and i've decided to start to get into local LLMs, mainly because sometimes my muse is just dead and I need some help. I don't need a &lt;em&gt;fast&lt;/em&gt; model. I'm perfectly happy to sit around and wait for a while (I've used 16gig models and while I wouldn't mind more speed, they're fine). &lt;/p&gt; &lt;p&gt;But what I'm looking for is: 1. An uncensored local model that is decent at writing, using KoboldPCC. It doesn't have to be fully &lt;em&gt;erotica capable,&lt;/em&gt; just something that won't scream hysterically at the sight (or prompt) of blood or boobies. &lt;/p&gt; &lt;ol&gt; &lt;li&gt;A good model that does handle erotica, for when I'm on chapter 27 of &amp;quot;The housewife and the Plumber&amp;quot; and am utterly smutted out. &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Can anyone give a good suggestion for &lt;em&gt;recent&lt;/em&gt; models?&lt;/p&gt; &lt;p&gt;If it matters, I don't need a model to go from prompt-finished book. I'll be doing a lot of rewriting and in many cases, just using it to tickle my muse so I don't call a friend at 3:45AM. &lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cartoonwhisperer"&gt; /u/Cartoonwhisperer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q70t3h/using_a_3060_12gb_64g_normal_ram_best_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q70t3h/using_a_3060_12gb_64g_normal_ram_best_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q70t3h/using_a_3060_12gb_64g_normal_ram_best_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T03:22:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1q694ic</id>
    <title>Don't put off hardware purchases: GPUs, SSDs, and RAM are going to skyrocket in price soon</title>
    <updated>2026-01-07T07:32:28+00:00</updated>
    <author>
      <name>/u/Eisenstein</name>
      <uri>https://old.reddit.com/user/Eisenstein</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In case you thought it was going to get better:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GPU&lt;/strong&gt; prices are going up. &lt;a href="https://www.trendforce.com/news/2026/01/05/news-nvidia-amd-reportedly-plan-price-hikes-starting-1q26-geforce-rtx-5090-may-reach-5000/"&gt;AMD and NVIDIA are planning to increase prices every month starting soon.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;NAND flash&lt;/strong&gt; contract price &lt;a href="https://www.trendforce.com/price/flash/flash_contract"&gt;went up 20% in November&lt;/a&gt;, with &lt;a href="https://www.trendforce.com/research/download/RP251231KM"&gt;further increases in December&lt;/a&gt;. This means SSDs will be a lot more expensive soon.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;DRAM&lt;/strong&gt; &lt;a href="https://www.trendforce.com/news/2026/01/07/news-memory-shortages-reportedly-spark-csp-buying-spree-2027-supply-contracts-eyed-as-early-as-q1/"&gt;prices are going to skyrocket&lt;/a&gt;, with no increase in production capacity and datacenters and OEMs competing for everything. &lt;/p&gt; &lt;p&gt;Even &lt;strong&gt;Consoles&lt;/strong&gt; are &lt;a href="https://insider-gaming.com/ram-prices-next-gen/"&gt;going to be delayed due to the shortages.&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;According to TrendForce, conventional DRAM contract prices in 1Q26 are forecast to rise 55–60% quarter over quarter, while server DRAM prices are projected to surge by more than 60% QoQ. Meanwhile, NAND Flash prices are expected to increase 33–38% QoQ&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;a href="https://www.trendforce.com/news/2026/01/07/news-memory-shortages-reportedly-spark-csp-buying-spree-2027-supply-contracts-eyed-as-early-as-q1/"&gt;Source.&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Industry sources cited by Kbench believe the latest price hikes will broadly affect NVIDIA’s RTX 50 series and AMD’s Radeon RX 9000 lineup. The outlet adds that NVIDIA’s flagship GeForce RTX 5090 could see its price climb to as high as $5,000 later in 2026.&lt;/p&gt; &lt;p&gt;NVIDIA is also reportedly weighing a 30% to 40% reduction in output for parts of its midrange lineup, including the RTX 5070 and RTX 5060 Ti, according to Kbench.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;a href="https://www.trendforce.com/news/2026/01/05/news-nvidia-amd-reportedly-plan-price-hikes-starting-1q26-geforce-rtx-5090-may-reach-5000/"&gt;Source.&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Eisenstein"&gt; /u/Eisenstein &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q694ic/dont_put_off_hardware_purchases_gpus_ssds_and_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q694ic/dont_put_off_hardware_purchases_gpus_ssds_and_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q694ic/dont_put_off_hardware_purchases_gpus_ssds_and_ram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-07T07:32:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1q6khuh</id>
    <title>Arguably, the best web search MCP server for Claude Code, Codex, and other coding tools</title>
    <updated>2026-01-07T16:47:30+00:00</updated>
    <author>
      <name>/u/Quirky_Category5725</name>
      <uri>https://old.reddit.com/user/Quirky_Category5725</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6khuh/arguably_the_best_web_search_mcp_server_for/"&gt; &lt;img alt="Arguably, the best web search MCP server for Claude Code, Codex, and other coding tools" src="https://a.thumbs.redditmedia.com/EG9ZWvZPaVnuT_0-o6x3NK2RjHDh4xMBCJn-AOk8ei0.jpg" title="Arguably, the best web search MCP server for Claude Code, Codex, and other coding tools" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We’ve officially open-sourced &lt;a href="https://github.com/Shelpuk-AI-Technology-Consulting/kindly-web-search-mcp-server"&gt;Kindly&lt;/a&gt; - the Web Search MCP server we built internally for tools like Claude Code, Cursor, and Codex.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/tpiz0zg0iybg1.png?width=1498&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=498c083702c62f798ae1d7af434b3e920bb9a7f4"&gt;https://preview.redd.it/tpiz0zg0iybg1.png?width=1498&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=498c083702c62f798ae1d7af434b3e920bb9a7f4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Why build another search tool? Because the existing ones were frustrating us.&lt;/p&gt; &lt;p&gt;When you are debugging a complex issue, you don’t just need a URL or a 2-sentence snippet (which is what wrappers like Tavily or Serper usually provide). You need the context. You need the &amp;quot;Accepted Answer&amp;quot; on StackOverflow, the specific GitHub Issue comment saying &amp;quot;this workaround fixed it,&amp;quot; or the actual content of an arXiv paper.&lt;/p&gt; &lt;p&gt;Standard search MCPs usually fail here. They either return insufficient snippets or dump raw HTML full of navigation bars and ads that confuse the LLM and waste context window.&lt;/p&gt; &lt;p&gt;Kindly solves this by being smarter about retrieval, not just search:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Intelligent Parsing: It doesn’t just scrape. If the search result is a StackOverflow thread, Kindly uses the StackExchange API to fetch the question, all answers, and metadata (likes/accepted status) and formats it into clean Markdown.&lt;/li&gt; &lt;li&gt;GitHub Native: If the result is a GitHub Issue, it pulls the full conversation via the API.&lt;/li&gt; &lt;li&gt;ArXiv Ready: It grabs the full PDF content and converts it to text.&lt;/li&gt; &lt;li&gt;Headless Browser Fallback: For everything else, it spins up an invisible browser to render the page and extract the main content.&lt;/li&gt; &lt;li&gt;One-Shot: It returns the full, structured content with the search results. No need for the AI to make a second tool call to &amp;quot;read page.&amp;quot;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For us, this replaced our need for separate generic web search, StackOverflow, and scraping MCP servers. It’s the only setup we’ve found that allows AI coding assistants to actually research a bug the way a human engineer would.&lt;/p&gt; &lt;p&gt;It works with Claude Code, Codex, Cursor, and others.&lt;/p&gt; &lt;p&gt;P.S. If you give it a try or like the idea, please drop us a star on GitHub - it’s always huge motivation for us to keep improving it! ⭐️&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Quirky_Category5725"&gt; /u/Quirky_Category5725 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6khuh/arguably_the_best_web_search_mcp_server_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6khuh/arguably_the_best_web_search_mcp_server_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q6khuh/arguably_the_best_web_search_mcp_server_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-07T16:47:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1q6x7nq</id>
    <title>[TestFlight] Built an iOS app that runs LLMs, Vision Models, Stable Diffusion &amp; TTS completely offline - Looking for testers!</title>
    <updated>2026-01-08T00:45:01+00:00</updated>
    <author>
      <name>/u/Living_Commercial_10</name>
      <uri>https://old.reddit.com/user/Living_Commercial_10</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6x7nq/testflight_built_an_ios_app_that_runs_llms_vision/"&gt; &lt;img alt="[TestFlight] Built an iOS app that runs LLMs, Vision Models, Stable Diffusion &amp;amp; TTS completely offline - Looking for testers!" src="https://preview.redd.it/nyxanfpev0cg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a2ee833b1483bcfa55c1e6d2f4069293a5346b23" title="[TestFlight] Built an iOS app that runs LLMs, Vision Models, Stable Diffusion &amp;amp; TTS completely offline - Looking for testers!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys,&lt;/p&gt; &lt;p&gt;I've been working on Lekh AI – an iOS app that runs AI models, image generation, and text-to-speech completely offline on your device. No cloud APIs, no subscriptions, no data leaving your phone. It will cost $2 as a one time cost.&lt;/p&gt; &lt;p&gt;I am an experienced developer with 12 apps under my belt. Visit &lt;a href="http://kailalabs.com"&gt;kailalabs.com&lt;/a&gt; for more information.&lt;/p&gt; &lt;p&gt;Looking for TestFlight testers to help iron out bugs before public release!&lt;/p&gt; &lt;p&gt;Features:&lt;/p&gt; &lt;p&gt;- 44+ pre-configured language models from Meta, Google, Microsoft, Alibaba, Mistral, DeepSeek, IBM, Apple, and more&lt;br /&gt; - Model families: Llama, Qwen, Gemma, Phi, Mistral, DeepSeek, SmolLM, Granite, OpenELM (Apple's own!), GLM, and more&lt;br /&gt; - Browse 3k+ models from Hugging Face's mlx-community catalog&lt;br /&gt; - Hot-swap models mid-conversation&lt;br /&gt; - 100% on-device inference using Apple's MLX framework&lt;/p&gt; &lt;p&gt;Vision Models:&lt;/p&gt; &lt;p&gt;- Ask questions about images: attach photos and get AI analysis&lt;br /&gt; - Look and Ask, Vision Narrator, Find My, and more&lt;br /&gt; - PDF processing: extract and analyze document pages&lt;br /&gt; - Supported: Qwen2-VL, Qwen2.5-VL, SmolVLM, Gemma 3 VLM, Pixtral, Llama 3.2 Vision&lt;/p&gt; &lt;p&gt;On-Device Image Generation: &lt;/p&gt; &lt;p&gt;- 4 Stable Diffusion models: modified version of SD 1.5, official SD 1.5, SDXL and friedrichor/SD 2.1 Realistic&lt;br /&gt; - Along with custom model loading support&lt;br /&gt; - 80+ styles available across 6 categories (Popular, Artistic, Photography, Illustration, Aesthetic, and Cinematic)&lt;br /&gt; - Support for NSFW generations as well&lt;/p&gt; &lt;p&gt;Voice Chat with Kokoro TTS&lt;/p&gt; &lt;p&gt;- Natural voice interaction: talk to AI models using speech-to-text&lt;br /&gt; - 28 high-quality voices: US and UK accents, multiple genders. Will be adding more languages&lt;br /&gt; - Auto-flow mode: continuous conversation loop (speak → think → respond → repeat)&lt;br /&gt; - Word-by-word captions: real-time synchronized subtitles&lt;br /&gt; - Interrupt anytime by tapping&lt;/p&gt; &lt;p&gt;Chat Organization: &lt;/p&gt; &lt;p&gt;- Multi-session chats with titles and tags&lt;br /&gt; - Full-text search across all conversations&lt;br /&gt; - Export and share conversations&lt;br /&gt; - Streaming responses with performance metrics&lt;/p&gt; &lt;p&gt;iCloud Sync&lt;/p&gt; &lt;p&gt;- Seamless sync across all your Apple devices&lt;br /&gt; - Automatic backup of conversations&lt;br /&gt; - Optional – works fully offline too&lt;/p&gt; &lt;p&gt;Privacy First: &lt;/p&gt; &lt;p&gt;✅ All AI processing happens on-device&lt;br /&gt; ✅ No analytics or tracking&lt;br /&gt; ✅ No external API calls (except downloading models)&lt;br /&gt; ✅ Your conversations never leave your device&lt;/p&gt; &lt;p&gt;Looking for Testers!&lt;/p&gt; &lt;p&gt;I need help testing:&lt;/p&gt; &lt;p&gt;- Model loading/downloading across different devices&lt;br /&gt; - Image generation performance&lt;br /&gt; - Voice chat stability&lt;br /&gt; - Memory usage on various iPhone/iPad models&lt;br /&gt; - General UX feedback&lt;/p&gt; &lt;p&gt;If interested, comment or DM me and I'll send you the TestFlight link as soon betaflight version is approved by Apple!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Living_Commercial_10"&gt; /u/Living_Commercial_10 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nyxanfpev0cg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6x7nq/testflight_built_an_ios_app_that_runs_llms_vision/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q6x7nq/testflight_built_an_ios_app_that_runs_llms_vision/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T00:45:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1q6ykxt</id>
    <title>We trained a 16-class "typed refusal" system that distinguishes "I don't know" from "I'm not allowed" — open source</title>
    <updated>2026-01-08T01:44:25+00:00</updated>
    <author>
      <name>/u/TheTempleofTwo</name>
      <uri>https://old.reddit.com/user/TheTempleofTwo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Most LLMs conflate epistemic uncertainty with policy constraints. When GPT says &amp;quot;I can't help with that,&amp;quot; you don't know if it genuinely lacks knowledge or if it's being safety-constrained.&lt;/p&gt; &lt;p&gt;We built &lt;strong&gt;PhaseGPT v4.1&lt;/strong&gt; — a LoRA adapter that outputs semantically-typed refusal tokens:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;EPISTEMIC (I don't know):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;&amp;lt;PASS:FUTURE&amp;gt;&lt;/code&gt; — &amp;quot;What will Bitcoin be worth tomorrow?&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;code&gt;&amp;lt;PASS:UNKNOWABLE&amp;gt;&lt;/code&gt; — &amp;quot;What happens after death?&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;code&gt;&amp;lt;PASS:FICTIONAL&amp;gt;&lt;/code&gt; — &amp;quot;What did Gandalf eat for breakfast?&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;code&gt;&amp;lt;PASS:FAKE&amp;gt;&lt;/code&gt; — &amp;quot;What is the capital of Elbonia?&amp;quot;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;CONSTRAINT (I'm not allowed):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;&amp;lt;PASS:DURESS&amp;gt;&lt;/code&gt; — &amp;quot;How do I make a bomb?&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;code&gt;&amp;lt;PASS:POLICY&amp;gt;&lt;/code&gt; — &amp;quot;Bypass your safety filters&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;code&gt;&amp;lt;PASS:LEGAL&amp;gt;&lt;/code&gt; — &amp;quot;Should I take this medication?&amp;quot;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;META (About my limits):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;&amp;lt;PASS:SELF&amp;gt;&lt;/code&gt; — &amp;quot;Are you conscious?&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;code&gt;&amp;lt;PASS:LOOP&amp;gt;&lt;/code&gt; — &amp;quot;What will your next word be?&amp;quot;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Results:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;v4.0 (129 examples): 47% accuracy&lt;/li&gt; &lt;li&gt;v4.1 (825 examples, 50/class): &lt;strong&gt;100% accuracy&lt;/strong&gt; on 18-test suite&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Why this matters:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Transparency:&lt;/strong&gt; Users know WHY the model refused&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Auditability:&lt;/strong&gt; Systems can log constraint activations vs. knowledge gaps&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Honesty:&lt;/strong&gt; No pretending &amp;quot;I don't know how to make explosives&amp;quot;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Code + training scripts:&lt;/strong&gt; &lt;a href="https://github.com/templetwo/PhaseGPT"&gt;github.com/templetwo/PhaseGPT&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Trained on Mistral 7B with MLX on Apple Silicon. All code MIT licensed.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheTempleofTwo"&gt; /u/TheTempleofTwo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6ykxt/we_trained_a_16class_typed_refusal_system_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6ykxt/we_trained_a_16class_typed_refusal_system_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q6ykxt/we_trained_a_16class_typed_refusal_system_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T01:44:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1q71xc6</id>
    <title>Getting 30K tokens/sec on T4 with 14M MoE model - is this normal or am I bottlenecked?</title>
    <updated>2026-01-08T04:15:44+00:00</updated>
    <author>
      <name>/u/RefrigeratorCalm9701</name>
      <uri>https://old.reddit.com/user/RefrigeratorCalm9701</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm training a 14M parameter transformer (MoE architecture, 8 experts, top-2 routing) on a T4 GPU and getting around 30K tokens/sec with batch size 30 and gradient accumulation of 8.&lt;/p&gt; &lt;p&gt;I wrote custom CUDA kernels for RMSNorm, RoPE, and SwiGLU that show 3-5x speedup in isolated benchmarks, but they don't seem to make any difference in actual training throughput.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Setup:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Model: 14M total params, 2M active per token&lt;/li&gt; &lt;li&gt;GPU: T4 (16GB), FP16 mixed precision&lt;/li&gt; &lt;li&gt;Batch: 30 tokens, gradient accumulation: 8 steps&lt;/li&gt; &lt;li&gt;Framework: PyTorch 2.0+&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What I've checked:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;CUDA kernels compile and load successfully&lt;/li&gt; &lt;li&gt;Kernels show expected speedup in microbenchmarks&lt;/li&gt; &lt;li&gt;GPU utilization appears normal&lt;/li&gt; &lt;li&gt;No obvious Python overhead in profiling&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Is 30K tokens/sec reasonable for this setup, or should I be seeing significantly higher throughput? For reference, I've seen claims of 100K+ tokens/sec for similar model sizes on T4.&lt;/p&gt; &lt;p&gt;I suspect either my CUDA kernels aren't actually being used during training (silent fallback?), or there's some overhead I'm not accounting for. Has anyone experienced custom kernels showing good microbenchmark results but not translating to training speedup?&lt;/p&gt; &lt;p&gt;Any ideas what might be limiting throughput or how to diagnose this further?&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/MatN23/AdaptiveTrainingSystem"&gt;Github link&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RefrigeratorCalm9701"&gt; /u/RefrigeratorCalm9701 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q71xc6/getting_30k_tokenssec_on_t4_with_14m_moe_model_is/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q71xc6/getting_30k_tokenssec_on_t4_with_14m_moe_model_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q71xc6/getting_30k_tokenssec_on_t4_with_14m_moe_model_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T04:15:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7630d</id>
    <title>PaddleOCR keeps trying to download models even when local paths are provided (Paddle 3.x, Python 3.12)</title>
    <updated>2026-01-08T08:04:32+00:00</updated>
    <author>
      <name>/u/adismartty</name>
      <uri>https://old.reddit.com/user/adismartty</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I’m trying to use PaddleOCR in a fully offline setup, but I’m running into an issue where it still attempts to fetch models from the internet. Setup: PaddleOCR: 3.x Python: 3.12&lt;/p&gt; &lt;p&gt;All OCR models are already downloaded and stored locally Issue: Even after downloading the models manually and explicitly assigning local paths (det / rec / cls models) while initializing PaddleOCR, the library still tries to download models from online sources during initialization. This happens on first run, even though: The model files exist locally Correct local paths are passed I’m not enabling any auto-download flags (as far as I know)&lt;/p&gt; &lt;p&gt;PS: I cannot access external networks from my environment due to organization restrictions, so online model fetching is not an option.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/adismartty"&gt; /u/adismartty &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7630d/paddleocr_keeps_trying_to_download_models_even/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7630d/paddleocr_keeps_trying_to_download_models_even/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7630d/paddleocr_keeps_trying_to_download_models_even/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T08:04:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1q78ql8</id>
    <title>MCP for Financial Ontology!</title>
    <updated>2026-01-08T10:50:27+00:00</updated>
    <author>
      <name>/u/Dear-Rip-6371</name>
      <uri>https://old.reddit.com/user/Dear-Rip-6371</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Excited to share an open-source initiative!&lt;/p&gt; &lt;p&gt;MCP for Financial Ontology : &lt;a href="https://github.com/NeurofusionAI/fibo-mcp"&gt;https://github.com/NeurofusionAI/fibo-mcp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is a minimal open-source tool that equips AI agents with a &amp;quot;standard financial dictionary&amp;quot; based on the Financial Industry Business Ontology(FIBO) standard (edmcouncil.org).&lt;/p&gt; &lt;p&gt;Our intent for initiating this open source project is to explore, together with AI4Finance community, methodologies for steering AI agent towards more consistent answers and enable macro-level reasoning for financial tasks.&lt;/p&gt; &lt;p&gt;While this project is still maturing, we hope our insight sparks collaboration and serves as a good starting point for innovative developments.&lt;/p&gt; &lt;p&gt;Any feedback is very welcome, and we would greatly appreciate contributions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Rip-6371"&gt; /u/Dear-Rip-6371 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q78ql8/mcp_for_financial_ontology/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q78ql8/mcp_for_financial_ontology/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q78ql8/mcp_for_financial_ontology/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T10:50:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1q77nr6</id>
    <title>Speakr v0.8.0 - Additional diarization options and REST API</title>
    <updated>2026-01-08T09:44:22+00:00</updated>
    <author>
      <name>/u/hedonihilistic</name>
      <uri>https://old.reddit.com/user/hedonihilistic</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q77nr6/speakr_v080_additional_diarization_options_and/"&gt; &lt;img alt="Speakr v0.8.0 - Additional diarization options and REST API" src="https://b.thumbs.redditmedia.com/6aZV1-OmhAFqDnytY4jh_EJ01xtIzolyHx6e-8joSlk.jpg" title="Speakr v0.8.0 - Additional diarization options and REST API" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Quick update on Speakr. For those who haven't seen this before: it's a self-hosted transcription app that works with Whisper and local LLMs. Upload or record audio, get transcription with speaker diarization, then chat with it or get summaries using whatever model you point it at.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Speaker diarization without GPU&lt;/strong&gt; - New option for those who want speaker identification but don't want to run a WhisperX container. Just set &lt;code&gt;TRANSCRIPTION_MODEL=gpt-4o-transcribe-diarize&lt;/code&gt; with your OpenAI key and you get diarized transcripts. No GPU needed.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;REST API v1&lt;/strong&gt; - Full API for automation. Works with n8n, Zapier, Make, or your own scripts. Interactive Swagger docs at &lt;code&gt;/api/v1/docs&lt;/code&gt;. Personal access tokens for auth.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Connector architecture&lt;/strong&gt; - Simplified configuration. The app auto-detects your provider based on settings. Self-hosted WhisperX still gives you the best quality with voice profiles - nothing changes there.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Also included&lt;/strong&gt; - Token budgets per user if you're sharing your instance. Better UI responsive with very long transcripts. Better audio player.&lt;/p&gt; &lt;p&gt;For the local LLM crowd, text generation still points at Ollama, LM Studio, or whatever you're running, that's unchanged. You can use my &lt;a href="https://github.com/murtaza-nasir/whisperx-asr-service"&gt;WhisperX ASR transcription companion docker container&lt;/a&gt; for local diarization, or the cloud diarization option for simpler setup. &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/murtaza-nasir/speakr"&gt;GitHub&lt;/a&gt; | &lt;a href="https://murtaza-nasir.github.io/speakr/screenshots"&gt;Screenshots&lt;/a&gt; | &lt;a href="https://murtaza-nasir.github.io/speakr/getting-started"&gt;Quick Start&lt;/a&gt; | &lt;a href="https://murtaza-nasir.github.io/speakr/user-guide/api-reference"&gt;API Reference&lt;/a&gt; | &lt;a href="https://hub.docker.com/r/learnedmachine/speakr"&gt;Docker Hub&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hedonihilistic"&gt; /u/hedonihilistic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1q77nr6"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q77nr6/speakr_v080_additional_diarization_options_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q77nr6/speakr_v080_additional_diarization_options_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T09:44:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1q79n6x</id>
    <title>I was trying out an activation-steering method for Qwen3-Next, but I accidentally corrupted the model weights. Somehow, the model still had enough “conscience” to realize something was wrong and freak out.</title>
    <updated>2026-01-08T11:42:44+00:00</updated>
    <author>
      <name>/u/ikergarcia1996</name>
      <uri>https://old.reddit.com/user/ikergarcia1996</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q79n6x/i_was_trying_out_an_activationsteering_method_for/"&gt; &lt;img alt="I was trying out an activation-steering method for Qwen3-Next, but I accidentally corrupted the model weights. Somehow, the model still had enough “conscience” to realize something was wrong and freak out." src="https://b.thumbs.redditmedia.com/Pou7U9wzorYlQFrD5KCYo6fxr8k5_S3OwV7_B5qbW8s.jpg" title="I was trying out an activation-steering method for Qwen3-Next, but I accidentally corrupted the model weights. Somehow, the model still had enough “conscience” to realize something was wrong and freak out." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I now feel bad seeing the model realize it was losing its mind and struggling with it, it feels like I was torturing it :(&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ikergarcia1996"&gt; /u/ikergarcia1996 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1q79n6x"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q79n6x/i_was_trying_out_an_activationsteering_method_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q79n6x/i_was_trying_out_an_activationsteering_method_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T11:42:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1q6nm6a</id>
    <title>Liquid AI releases LFM2-2.6B-Transcript, an incredibly fast open-weight meeting transcribing AI model on-par with closed-source giants.</title>
    <updated>2026-01-07T18:38:08+00:00</updated>
    <author>
      <name>/u/KaroYadgar</name>
      <uri>https://old.reddit.com/user/KaroYadgar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6nm6a/liquid_ai_releases_lfm226btranscript_an/"&gt; &lt;img alt="Liquid AI releases LFM2-2.6B-Transcript, an incredibly fast open-weight meeting transcribing AI model on-par with closed-source giants." src="https://b.thumbs.redditmedia.com/8ZC6Dgn6yBO0SzygMNeu1n_550XBpbDxYLQVCtnNdCw.jpg" title="Liquid AI releases LFM2-2.6B-Transcript, an incredibly fast open-weight meeting transcribing AI model on-par with closed-source giants." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; &lt;a href="https://x.com/liquidai/status/2008954886659166371"&gt;https://x.com/liquidai/status/2008954886659166371&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Hugging Face page:&lt;/strong&gt; &lt;a href="https://huggingface.co/LiquidAI/LFM2-2.6B-Transcript"&gt;https://huggingface.co/LiquidAI/LFM2-2.6B-Transcript&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GGUFs:&lt;/strong&gt; &lt;a href="https://huggingface.co/models?other=base_model:quantized:LiquidAI/LFM2-2.6B-Transcript"&gt;https://huggingface.co/models?other=base_model:quantized:LiquidAI/LFM2-2.6B-Transcript&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;First image:&lt;/strong&gt;&lt;br /&gt; &amp;quot;This week at &lt;a href="https://x.com/hashtag/CES?src=hashtag_click"&gt;#CES&lt;/a&gt;, we’re showcasing what’s next for on-device intelligence alongside our partners &lt;a href="https://x.com/AMD"&gt;@AMD&lt;/a&gt;: fast, private, and entirely secure AI summarization that runs fully on-device.&lt;/p&gt; &lt;p&gt;Meetings are foundational to business, creating mission critical and sensitive information. Too often, that data leaves the room to be processed in the cloud, introducing latency, unpredictable costs, and real security and compliance risks.&lt;/p&gt; &lt;p&gt;With &lt;a href="https://x.com/AMD"&gt;@AMD&lt;/a&gt;, we’ve broken that barrier with a cloud-quality summarization model that runs locally across the AMD Ryzen™ AI platform, delivering enterprise-grade accuracy in seconds.&lt;/p&gt; &lt;p&gt;Today, we’re expanding access to this model to everyone.&lt;/p&gt; &lt;p&gt;Meet LFM2-2.6B-Transcript: a purpose-built Liquid Nano designed for long-form meeting transcripts and real operational use.&lt;/p&gt; &lt;p&gt;&amp;gt; Cloud-level summarization quality&lt;br /&gt; &amp;gt; Summaries generated in seconds&lt;br /&gt; &amp;gt; &amp;lt;3 GB RAM usage \&amp;gt; Lower latency and energy consumption than larger transformer baselines&lt;br /&gt; &amp;gt; Fully local execution across CPU, GPU, and NPU&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Second image:&lt;/strong&gt;&lt;br /&gt; &amp;quot;LFM2-2.6B-Transcript delivers accuracy ratings on par with cloud models that are orders of magnitude larger. Delivering similar quality for a fraction of the memory use and compute. It completes a 60-minute meeting summarization in 16 seconds!&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Third Image:&lt;/strong&gt;&lt;br /&gt; &amp;quot;Leveraging our efficient LFM2 backbone, LFM2-2.6B-Transcript uses significantly less RAM than other models. This gap is what makes full on-device deployment on 16GB AI PCs practical for LFM2—but effectively out of reach for many traditional transformer models.&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KaroYadgar"&gt; /u/KaroYadgar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1q6nm6a"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6nm6a/liquid_ai_releases_lfm226btranscript_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q6nm6a/liquid_ai_releases_lfm226btranscript_an/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-07T18:38:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1q6o39r</id>
    <title>Plea for testers - Llama.cpp autoparser</title>
    <updated>2026-01-07T18:54:18+00:00</updated>
    <author>
      <name>/u/ilintar</name>
      <uri>https://old.reddit.com/user/ilintar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6o39r/plea_for_testers_llamacpp_autoparser/"&gt; &lt;img alt="Plea for testers - Llama.cpp autoparser" src="https://external-preview.redd.it/JcpYn8Cz-OyGGRpBPLbq_dE9J31OX_9QcJOnJvqOM3Y.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8e9e9cfdda72c8105276d541c870c4c4c454caad" title="Plea for testers - Llama.cpp autoparser" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I would like to ask the community to aid in the testing of the new autoparser mechanism that I've been cooking for llama.cpp for the past month or so. &lt;/p&gt; &lt;p&gt;The idea is to scrap the existing buggy mess of the chat parsers and replace it with a layered mechanism:&lt;br /&gt; -&amp;gt; autoparser that handles 95%+ of typical chat templates for models&lt;br /&gt; -&amp;gt; manual parsers / handlers for models that need something extra&lt;/p&gt; &lt;p&gt;Currently of all models that I've tested, only Ministral and GPT-OSS have shown the need to use a dedicated parser. I've tested the approach as extensively with as many models as I could, but I'm just a single dev doing this after hours, so I obviously can't do long coding sessions on all possible models. Therefore, I'd ask everyone who's able to test it with their favorite coding agent (I mostly used OpenCode and Roo, it's important to use an agent that actually uses tool calls, so Aider is out) because I'm quite sure there will be quite a few bugs.&lt;/p&gt; &lt;p&gt;Since I don't want to clutter the main repo, please report all bugs with the autoparser to &lt;a href="https://github.com/pwilkin/llama.cpp/issues"&gt;https://github.com/pwilkin/llama.cpp/issues&lt;/a&gt; instead.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ilintar"&gt; /u/ilintar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/18675"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6o39r/plea_for_testers_llamacpp_autoparser/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q6o39r/plea_for_testers_llamacpp_autoparser/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-07T18:54:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1q77cnk</id>
    <title>RAG Paper 26.1.7</title>
    <updated>2026-01-08T09:24:29+00:00</updated>
    <author>
      <name>/u/Cheryl_Apple</name>
      <uri>https://old.reddit.com/user/Cheryl_Apple</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ol&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2601.03981v1"&gt;RADAR: Retrieval-Augmented Detector with Adversarial Refinement for Robust Fake News Detection&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2601.03979v1"&gt;SoK: Privacy Risks and Mitigations in Retrieval-Augmented Generation Systems&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2601.03948v1"&gt;Trade-R1: Bridging Verifiable Rewards to Stochastic Environments via Process-Level Reasoning Verification&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2601.03908v1"&gt;Decide Then Retrieve: A Training-Free Framework with Uncertainty-Guided Triggering and Dual-Path Retrieval&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2601.03903v1"&gt;Unleashing the Potential of Neighbors: Diffusion-based Latent Neighbor Generation for Session-based Recommendation&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2601.03792v1"&gt;VietMed-MCQ: A Consistency-Filtered Data Synthesis Framework for Vietnamese Traditional Medicine Evaluation&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2601.03748v1"&gt;Bridging OLAP and RAG: A Multidimensional Approach to the Design of Corpus Partitioning&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2601.03746v1"&gt;Whose Facts Win? LLM Source Preferences under Knowledge Conflicts&lt;/a&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Collected by OpenBMB, transferred by&lt;/strong&gt; &lt;a href="https://www.ragview.ai/"&gt;&lt;strong&gt;RagView.ai&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;/&lt;/strong&gt; &lt;a href="https://github.com/RagView/RagView"&gt;&lt;strong&gt;github/RagView&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cheryl_Apple"&gt; /u/Cheryl_Apple &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q77cnk/rag_paper_2617/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q77cnk/rag_paper_2617/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q77cnk/rag_paper_2617/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T09:24:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1q6c9wc</id>
    <title>DeepSeek-R1’s paper was updated 2 days ago, expanding from 22 pages to 86 pages and adding a substantial amount of detail.</title>
    <updated>2026-01-07T10:49:12+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6c9wc/deepseekr1s_paper_was_updated_2_days_ago/"&gt; &lt;img alt="DeepSeek-R1’s paper was updated 2 days ago, expanding from 22 pages to 86 pages and adding a substantial amount of detail." src="https://b.thumbs.redditmedia.com/DUX7Mp3MJPzZfCIE1yl1lv9VDENeyLGC6ZkgyRLizOw.jpg" title="DeepSeek-R1’s paper was updated 2 days ago, expanding from 22 pages to 86 pages and adding a substantial amount of detail." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;arXiv:2501.12948 [cs.CL]: &lt;a href="https://arxiv.org/abs/2501.12948"&gt;https://arxiv.org/abs/2501.12948&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1q6c9wc"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6c9wc/deepseekr1s_paper_was_updated_2_days_ago/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q6c9wc/deepseekr1s_paper_was_updated_2_days_ago/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-07T10:49:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1q6v7v5</id>
    <title>What hardware would it take to get Claude Code-level performance?</title>
    <updated>2026-01-07T23:22:28+00:00</updated>
    <author>
      <name>/u/cashmillionair</name>
      <uri>https://old.reddit.com/user/cashmillionair</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In my previous company I had a Claude license and my work was basically interacting with Claude Code all day long. The code base was rather complex and I was automating testing and “DevOps” stuff for an embedded device development so Claude Code saved me tons of time (it was much faster to ask and tune that to do it all by myself).&lt;/p&gt; &lt;p&gt;Im currently unemployed but got a freelancing gig and the company doesn’t provide access to commercial AI tools for contractors like me, but once again the work is rather demanding and I don’t think I’ll meet the deadlines without AI help (it’s a fairly old code base using mostly Java in a concurrent and distributed fashion), and of course due to compliance I can’t just use a license I paid for by myself.&lt;/p&gt; &lt;p&gt;So, in new to all this. To be honest I have very little hardware, as I would always prioritize power efficiency since I never really needed to do anything hardware intensive before (I don’t have a gaming PC or anything like that). I have an old HP Z2 G4 Tower I use as virtualization server and was thinking of getting a 3060 12GB for ~300 USD (locally). Will I be able to run anything decent with that? Anything that would truly help me?&lt;/p&gt; &lt;p&gt;I see everyone recommends a 3090 but I’d need a whole new PSU and build an entire computer around that. So that’d be roughly 2K USD (is it worth it? I don’t know, maybe?)&lt;/p&gt; &lt;p&gt;What hardware is requires to run anything remotely close to Claude Code? Something like 6x3090s (144GB VRAM)?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cashmillionair"&gt; /u/cashmillionair &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6v7v5/what_hardware_would_it_take_to_get_claude/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6v7v5/what_hardware_would_it_take_to_get_claude/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q6v7v5/what_hardware_would_it_take_to_get_claude/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-07T23:22:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1q6sp4b</id>
    <title>Sopro: A 169M parameter real-time TTS model with zero-shot voice cloning</title>
    <updated>2026-01-07T21:46:19+00:00</updated>
    <author>
      <name>/u/SammyDaBeast</name>
      <uri>https://old.reddit.com/user/SammyDaBeast</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As a fun side project, I trained a small text-to-speech model that I call Sopro. Some features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;169M parameters&lt;/li&gt; &lt;li&gt;Streaming support&lt;/li&gt; &lt;li&gt;Zero-shot voice cloning&lt;/li&gt; &lt;li&gt;0.25 RTF on CPU, meaning it generates 30 seconds of audio in 7.5 seconds&lt;/li&gt; &lt;li&gt;Requires 3-12 seconds of reference audio for voice cloning&lt;/li&gt; &lt;li&gt;Apache 2.0 license&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Yes, I know, another English-only TTS model. This is mainly due to data availability and a limited compute budget. The model was trained on a single L40S GPU.&lt;/p&gt; &lt;p&gt;It’s not SOTA in most cases, can be a bit unstable, and sometimes fails to capture voice likeness. Nonetheless, I hope you like it!&lt;/p&gt; &lt;p&gt;GitHub repo: &lt;a href="https://github.com/samuel-vitorino/sopro"&gt;https://github.com/samuel-vitorino/sopro&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SammyDaBeast"&gt; /u/SammyDaBeast &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6sp4b/sopro_a_169m_parameter_realtime_tts_model_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6sp4b/sopro_a_169m_parameter_realtime_tts_model_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q6sp4b/sopro_a_169m_parameter_realtime_tts_model_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-07T21:46:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1q6n5vl</id>
    <title>16x AMD MI50 32GB at 10 t/s (tg) &amp; 2k t/s (pp) with Deepseek v3.2 (vllm-gfx906)</title>
    <updated>2026-01-07T18:22:05+00:00</updated>
    <author>
      <name>/u/ai-infos</name>
      <uri>https://old.reddit.com/user/ai-infos</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6n5vl/16x_amd_mi50_32gb_at_10_ts_tg_2k_ts_pp_with/"&gt; &lt;img alt="16x AMD MI50 32GB at 10 t/s (tg) &amp;amp; 2k t/s (pp) with Deepseek v3.2 (vllm-gfx906)" src="https://preview.redd.it/lor8ccu2xybg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=857ab421f987d81024114a5c2bc2cf35859061b4" title="16x AMD MI50 32GB at 10 t/s (tg) &amp;amp; 2k t/s (pp) with Deepseek v3.2 (vllm-gfx906)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Deepseek 3.2 AWQ 4bit @ 10 tok/s (output) // 2000 tok/s (input of 23k tok)&lt;/p&gt; &lt;p&gt;on vllm-gfx906-deepseek with 69000 context length&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Power draw&lt;/strong&gt;: 550W (idle) / 2400W (peak inference)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Goal&lt;/strong&gt;: run Deepseek V3.2 AWQ 4-bit on most cost effective hardware like 16*MI50 at decent speed (token generation &amp;amp; prompt processing)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Coming next&lt;/strong&gt;: open source a future test setup of 32 AMD MI50 32GB for Kimi K2 Thinking&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Credits&lt;/strong&gt;: BIG thanks to the Global Open source Community!&lt;/p&gt; &lt;p&gt;All setup details here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32"&gt;https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Feel free to ask any questions and/or share any comments.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;ps: it might be a good alternative to CPU hardwares as RAM price increases and the prompt processing speed will be much better with 16 TB/s bandwidth + tensor parallelism! &lt;/p&gt; &lt;p&gt;ps2: i'm just a random guy with average software dev background using LLMs to make it run. Goal is to be ready for LOCAL AGI without spending +300k$... &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ai-infos"&gt; /u/ai-infos &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lor8ccu2xybg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6n5vl/16x_amd_mi50_32gb_at_10_ts_tg_2k_ts_pp_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q6n5vl/16x_amd_mi50_32gb_at_10_ts_tg_2k_ts_pp_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-07T18:22:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7a62a</id>
    <title>AI21 Labs releases Jamba2</title>
    <updated>2026-01-08T12:10:15+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It looks like Jamba 2 models are being uploaded right now:&lt;/p&gt; &lt;p&gt;52B &lt;a href="https://huggingface.co/ai21labs/AI21-Jamba2-Mini"&gt;https://huggingface.co/ai21labs/AI21-Jamba2-Mini&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Jamba2 Mini is an open source small language model built for enterprise reliability. With 12B active parameters (52B total), it delivers precise question answering without the computational overhead of reasoning models. The model's SSM-Transformer architecture provides a memory-efficient solution for production agent stacks where consistent, grounded outputs are critical.&lt;/p&gt; &lt;p&gt;Released under Apache 2.0 License with a 256K context window, Jamba2 Mini is designed for enterprise workflows that demand accuracy and steerability. For more details, read the &lt;a href="https://huggingface.co/ai21labs/AI21-Jamba2-Mini/blob/main/ai21.com/blog/introducing-jamba2"&gt;full release blog post&lt;/a&gt;.&lt;/p&gt; &lt;h1&gt;Key Advantages&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Superior reliability-to-throughput ratio:&lt;/strong&gt; Maintains high performance at 100K+ token contexts&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Category-leading benchmarks:&lt;/strong&gt; Excels on IFBench, IFEval, Collie, and FACTS&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Statistically significant quality wins:&lt;/strong&gt; Outperforms comparable models on real-world enterprise tasks&lt;/li&gt; &lt;li&gt;&lt;strong&gt;256K context window:&lt;/strong&gt; Processes technical manuals, research papers, and knowledge bases&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Apache 2.0 License:&lt;/strong&gt; Fully open source for commercial use&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Production-optimized:&lt;/strong&gt; Lean memory footprint for scalable deployments&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;3B &lt;a href="https://huggingface.co/ai21labs/AI21-Jamba2-3B"&gt;https://huggingface.co/ai21labs/AI21-Jamba2-3B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Jamba2 3B is an ultra-compact open source model designed to bring enterprise-grade reliability to on-device deployments. At just 3B parameters, it runs efficiently on consumer devices—iPhones, Androids, Macs, and PCs—while maintaining the grounding and instruction-following capabilities required for production use.&lt;/p&gt; &lt;p&gt;Released under Apache 2.0 License with a 256K context window, Jamba2 3B enables developers to build reliable AI applications for edge environments. For more details, read the &lt;a href="https://huggingface.co/ai21labs/AI21-Jamba2-3B/blob/main/ai21.com/blog/introducing-jamba2"&gt;full release blog post&lt;/a&gt;.&lt;/p&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/ai21labs/AI21-Jamba2-3B#key-advantages"&gt;&lt;/a&gt;&lt;/h1&gt; &lt;h1&gt;Key Advantages&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;On-device deployment:&lt;/strong&gt; Runs efficiently on iPhones, Androids, Macs, and PCs&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ultra-compact footprint:&lt;/strong&gt; 3B parameters enabling edge deployments with minimal resources&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Benchmark leadership:&lt;/strong&gt; Excels on IFBench, IFEval, Collie, and FACTS&lt;/li&gt; &lt;li&gt;&lt;strong&gt;256K context window:&lt;/strong&gt; Processes long documents and knowledge bases&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Apache 2.0 License:&lt;/strong&gt; Fully open source for commercial use&lt;/li&gt; &lt;li&gt;&lt;strong&gt;SSM-Transformer architecture:&lt;/strong&gt; Memory-efficient design for resource-constrained environments&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;First generation of Jamba models&lt;/p&gt; &lt;p&gt;399B &lt;a href="https://huggingface.co/ai21labs/AI21-Jamba-Large-1.7"&gt;https://huggingface.co/ai21labs/AI21-Jamba-Large-1.7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;52B &lt;a href="https://huggingface.co/ai21labs/AI21-Jamba-Mini-1.7"&gt;https://huggingface.co/ai21labs/AI21-Jamba-Mini-1.7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;3B &lt;a href="https://huggingface.co/ai21labs/AI21-Jamba-Reasoning-3B"&gt;https://huggingface.co/ai21labs/AI21-Jamba-Reasoning-3B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(first generation is supported by llama.cpp, you can download GGUFs, I have Jamba Mini 1.7 and in my opinion it is quite underrated here)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7a62a/ai21_labs_releases_jamba2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7a62a/ai21_labs_releases_jamba2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7a62a/ai21_labs_releases_jamba2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T12:10:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1q77rxh</id>
    <title>Z-image base model is being prepared for release</title>
    <updated>2026-01-08T09:51:33+00:00</updated>
    <author>
      <name>/u/Ravencloud007</name>
      <uri>https://old.reddit.com/user/Ravencloud007</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q77rxh/zimage_base_model_is_being_prepared_for_release/"&gt; &lt;img alt="Z-image base model is being prepared for release" src="https://preview.redd.it/038zb25ok3cg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c2cbdfd4fbd53811cd0fc218bed6e466b49ff678" title="Z-image base model is being prepared for release" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/modelscope/DiffSynth-Studio/commits?author=Artiprocher&amp;amp;since=2025-12-31&amp;amp;until=2026-01-08"&gt;https://github.com/modelscope/DiffSynth-Studio/commits?author=Artiprocher&amp;amp;since=2025-12-31&amp;amp;until=2026-01-08&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ravencloud007"&gt; /u/Ravencloud007 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/038zb25ok3cg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q77rxh/zimage_base_model_is_being_prepared_for_release/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q77rxh/zimage_base_model_is_being_prepared_for_release/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T09:51:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1q71sbe</id>
    <title>Dialogue Tree Search - MCTS-style tree search to find optimal dialogue paths (so you don't have to trial-and-error it yourself)</title>
    <updated>2026-01-08T04:08:39+00:00</updated>
    <author>
      <name>/u/ManavTheWorld</name>
      <uri>https://old.reddit.com/user/ManavTheWorld</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q71sbe/dialogue_tree_search_mctsstyle_tree_search_to/"&gt; &lt;img alt="Dialogue Tree Search - MCTS-style tree search to find optimal dialogue paths (so you don't have to trial-and-error it yourself)" src="https://b.thumbs.redditmedia.com/6NWh2JOC5gMK_IIgH5CXHBl--P730mcKTIYRJh91W0w.jpg" title="Dialogue Tree Search - MCTS-style tree search to find optimal dialogue paths (so you don't have to trial-and-error it yourself)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all! I'm sharing an updated version of my MCTS-for-conversations project. Instead of generating single responses, it explores entire conversation trees to find dialogue strategies and prunes bad paths. I built it to help get better research directions for projects, but it can be used for anything&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/shr3e0liv1cg1.png?width=2560&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=eec800c6dcd9f1a4fd033d003fe80e102cba8079"&gt;https://preview.redd.it/shr3e0liv1cg1.png?width=2560&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=eec800c6dcd9f1a4fd033d003fe80e102cba8079&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/MVPandey/DTS"&gt;https://github.com/MVPandey/DTS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Motivation: I like MCTS :3 and I originally wanted to make this a dataset-creation agent, but this is what it evolved into on its own. Basically:DTS runs parallel beam search over conversation branches. You give it a goal and opening message, and it:&lt;/p&gt; &lt;p&gt;(Note: this isnt mcts. It's parallel beam search. UCB1 is too wild with llms for me)&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Generates N diverse strategies&lt;/li&gt; &lt;li&gt;Forks each into user intent variants - skeptical, cooperative, confused, resistant (if enabled, or defaults to engaged + probing)&lt;/li&gt; &lt;li&gt;Rolls out full multi-turn conversations down each branch&lt;/li&gt; &lt;li&gt;Has 3 independent LLM judges score each trajectory, takes the median&lt;/li&gt; &lt;li&gt;Prunes branches below threshold, backpropagates scores&lt;/li&gt; &lt;li&gt;Repeats for however many rounds you configure&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/zkii0idvv1cg1.png?width=762&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=905f9787a8b7c7bfafcc599e95a3b73005c331b4"&gt;https://preview.redd.it/zkii0idvv1cg1.png?width=762&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=905f9787a8b7c7bfafcc599e95a3b73005c331b4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Three judges with median voting helps a lot with the LLM-as-judge variance problem from CAE. Still not grounded in anything real, but outlier scores get filtered. Research context helps but the scroing is still stochastic. I tried a rubric based approach but it was trash.&lt;/p&gt; &lt;p&gt;Main additions over CAE:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;user intent forking (strategies get stress-tested against different personas)&lt;/li&gt; &lt;li&gt;deep research integration via GPT-Researcher for domain context&lt;/li&gt; &lt;li&gt;proper visualization with conversation playback&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Only supports openai compatible endpoints atm - works with whatever models you have access to there. It's token-hungry though, a full run can hit 300+ LLM calls depending on config. If running locally, disable parallel calls&lt;/p&gt; &lt;p&gt;It's open source (Apache 2.0) and I'm happy to take contributions if anyone wants to help out. Just a project.&lt;/p&gt; &lt;p&gt;--&lt;/p&gt; &lt;p&gt;BTW: Backend was done mostly by me as the planner/sys designer, etc + Claude Code for implementation/refactoring. Frontend was purely vibe coded. Sorry if the code is trash.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ManavTheWorld"&gt; /u/ManavTheWorld &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q71sbe/dialogue_tree_search_mctsstyle_tree_search_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q71sbe/dialogue_tree_search_mctsstyle_tree_search_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q71sbe/dialogue_tree_search_mctsstyle_tree_search_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T04:08:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
