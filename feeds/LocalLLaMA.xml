<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-22T16:49:16+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qj2dnd</id>
    <title>A new model from http://Z.ai, "GLM-OCR" has been spotted on Github</title>
    <updated>2026-01-21T16:21:49+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qj2dnd/a_new_model_from_httpzai_glmocr_has_been_spotted/"&gt; &lt;img alt="A new model from http://Z.ai, &amp;quot;GLM-OCR&amp;quot; has been spotted on Github" src="https://preview.redd.it/tduio97daqeg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dd5b258b6d658644e83ddec8f6c475cc131ee93a" title="A new model from http://Z.ai, &amp;quot;GLM-OCR&amp;quot; has been spotted on Github" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/tduio97daqeg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qj2dnd/a_new_model_from_httpzai_glmocr_has_been_spotted/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qj2dnd/a_new_model_from_httpzai_glmocr_has_been_spotted/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-21T16:21:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1qiwm3c</id>
    <title>Fix for GLM 4.7 Flash has been merged into llama.cpp</title>
    <updated>2026-01-21T12:29:19+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qiwm3c/fix_for_glm_47_flash_has_been_merged_into_llamacpp/"&gt; &lt;img alt="Fix for GLM 4.7 Flash has been merged into llama.cpp" src="https://external-preview.redd.it/P0aZfAO5cQnwgz36bD9sAcDcttCXWcTbQBhIkzY76fc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4ac930f1f077b513ae17d07167d50119d0ac69d0" title="Fix for GLM 4.7 Flash has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The world is saved!&lt;/p&gt; &lt;p&gt;FA for CUDA in progress &lt;a href="https://github.com/ggml-org/llama.cpp/pull/18953"&gt;https://github.com/ggml-org/llama.cpp/pull/18953&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/18980"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qiwm3c/fix_for_glm_47_flash_has_been_merged_into_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qiwm3c/fix_for_glm_47_flash_has_been_merged_into_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-21T12:29:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1qjpwbr</id>
    <title>Qwen3-Coder-480B on Mac Studio M3 Ultra 512gb</title>
    <updated>2026-01-22T09:15:42+00:00</updated>
    <author>
      <name>/u/BitXorBit</name>
      <uri>https://old.reddit.com/user/BitXorBit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;i was wondering if anyone use this configuration for daily usage as coding assistant/agentic?&lt;/p&gt; &lt;p&gt;my goal here is to have as much as possible close to claude code opus 4.5 on my local setup, i need 6-10 hours/day of usage for refactoring, research, solve architecture problems, etc &lt;/p&gt; &lt;p&gt;i read on many places that the 30b models are too &amp;quot;dumb&amp;quot; for this case, and i should aim on the higher models, which ofc leads us to the known issue of VRAM, 6000 pro is not an option because of the VRAM requirements and other cluster solutions would cost like my house. &lt;/p&gt; &lt;p&gt;so before going and buying the Mac Studio M3 Ultra with 512gb ram, i would love to hear feedback if any developers using this configuration/alternative on daily basis and what is their feedback. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BitXorBit"&gt; /u/BitXorBit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjpwbr/qwen3coder480b_on_mac_studio_m3_ultra_512gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjpwbr/qwen3coder480b_on_mac_studio_m3_ultra_512gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qjpwbr/qwen3coder480b_on_mac_studio_m3_ultra_512gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-22T09:15:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1qj40er</id>
    <title>VibeVoice-ASR released!</title>
    <updated>2026-01-21T17:20:29+00:00</updated>
    <author>
      <name>/u/k_means_clusterfuck</name>
      <uri>https://old.reddit.com/user/k_means_clusterfuck</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/microsoft/VibeVoice-ASR"&gt;https://huggingface.co/microsoft/VibeVoice-ASR&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/k_means_clusterfuck"&gt; /u/k_means_clusterfuck &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qj40er/vibevoiceasr_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qj40er/vibevoiceasr_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qj40er/vibevoiceasr_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-21T17:20:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1qjwnqh</id>
    <title>GLM 4.7 Quants Recommendations</title>
    <updated>2026-01-22T14:56:27+00:00</updated>
    <author>
      <name>/u/val_in_tech</name>
      <uri>https://old.reddit.com/user/val_in_tech</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For folks who are running GLM 4.7, could you please share your stable quant/vLLM settings and what tps are getting. I've tried QuantTrio/GLM-4.7-GPTQ-Int4-Int8Mix and reap 30 on vLLM 0.14 and nightly, sm120, but they didn't seem intelligent/stable. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/val_in_tech"&gt; /u/val_in_tech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjwnqh/glm_47_quants_recommendations/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjwnqh/glm_47_quants_recommendations/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qjwnqh/glm_47_quants_recommendations/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-22T14:56:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1qjd8dp</id>
    <title>Kimi-Linear-48B-A3B-Instruct-GGUF Support - Any news?</title>
    <updated>2026-01-21T22:58:38+00:00</updated>
    <author>
      <name>/u/Iory1998</name>
      <uri>https://old.reddit.com/user/Iory1998</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Kimi-Linear seems to handle long context pretty well. Do you have any idea why it's still not implemented in llama.cpp? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Iory1998"&gt; /u/Iory1998 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjd8dp/kimilinear48ba3binstructgguf_support_any_news/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjd8dp/kimilinear48ba3binstructgguf_support_any_news/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qjd8dp/kimilinear48ba3binstructgguf_support_any_news/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-21T22:58:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1qjxo0k</id>
    <title>What is the learning path for hosting local ai for total newbie?</title>
    <updated>2026-01-22T15:34:46+00:00</updated>
    <author>
      <name>/u/danuser8</name>
      <uri>https://old.reddit.com/user/danuser8</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What is the learning path of hosting local ai and setup workflows for total newbie?&lt;/p&gt; &lt;p&gt;Where to start for total newbie with 5060 Ti 16GBVRAM and 32GB system RAM?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danuser8"&gt; /u/danuser8 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjxo0k/what_is_the_learning_path_for_hosting_local_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjxo0k/what_is_the_learning_path_for_hosting_local_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qjxo0k/what_is_the_learning_path_for_hosting_local_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-22T15:34:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1qjujqk</id>
    <title>GLM-OCR is coming! A new PR has appeared in Hugging Face Transformers.</title>
    <updated>2026-01-22T13:29:35+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjujqk/glmocr_is_coming_a_new_pr_has_appeared_in_hugging/"&gt; &lt;img alt="GLM-OCR is coming! A new PR has appeared in Hugging Face Transformers." src="https://b.thumbs.redditmedia.com/erXSCkXsMtgCFjfsVbnnzk_VaEtCjnE7XOkXWVhIRLM.jpg" title="GLM-OCR is coming! A new PR has appeared in Hugging Face Transformers." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/huggingface/transformers/pull/43391"&gt;https://github.com/huggingface/transformers/pull/43391&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8mc2nl0bkweg1.png?width=398&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9462570c05402da9d395f12c91b78376fc9b9021"&gt;https://preview.redd.it/8mc2nl0bkweg1.png?width=398&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9462570c05402da9d395f12c91b78376fc9b9021&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/wlj57v1ckweg1.png?width=724&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=aa726ea2de7215e7ba30b0c1e364ef0adcef269e"&gt;https://preview.redd.it/wlj57v1ckweg1.png?width=724&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=aa726ea2de7215e7ba30b0c1e364ef0adcef269e&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjujqk/glmocr_is_coming_a_new_pr_has_appeared_in_hugging/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjujqk/glmocr_is_coming_a_new_pr_has_appeared_in_hugging/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qjujqk/glmocr_is_coming_a_new_pr_has_appeared_in_hugging/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-22T13:29:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1qjc8a2</id>
    <title>Michigan is pushing a Anti Chatbot bill to protect the heckin kiddos</title>
    <updated>2026-01-21T22:19:31+00:00</updated>
    <author>
      <name>/u/PostEasy7183</name>
      <uri>https://old.reddit.com/user/PostEasy7183</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Senate Democrats Call for Improved Safety Measures to Better Protect Michigan Kids from Digital Dangers - Senator Kevin Hertel &lt;a href="https://share.google/ZwmPjEOVP5AcgZnhT"&gt;https://share.google/ZwmPjEOVP5AcgZnhT&lt;/a&gt;&lt;/p&gt; &lt;p&gt;not much information about this yet but they've talked about making sure kids have a harder time to access chat bots. the bill is vague so far and to my knowledge no real text has been released yet. My question is how can they assess what is a teen and not without a Digital ID? I'm so sick of these bullshit laws in the spirit of &amp;quot;Protecting the children.&amp;quot; Give your thoughts below&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PostEasy7183"&gt; /u/PostEasy7183 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjc8a2/michigan_is_pushing_a_anti_chatbot_bill_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjc8a2/michigan_is_pushing_a_anti_chatbot_bill_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qjc8a2/michigan_is_pushing_a_anti_chatbot_bill_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-21T22:19:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1qjxyqb</id>
    <title>Experiences with local coding agents?</title>
    <updated>2026-01-22T15:46:04+00:00</updated>
    <author>
      <name>/u/st8ic88</name>
      <uri>https://old.reddit.com/user/st8ic88</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I decided to play around with Goose as a coding agent using various local models through ollama. I gave it two tasks, one was to create a simple javascript app and the other was to write unit tests for a few simple python functions. It was pretty miserable all around. The only models which did anything remotely useful were qwen3-coder and gpt-oss-20B. Even those had major issues with tool use, often randomly refusing to write the output to a file. Sometimes they would just spin for a while and then randomly quit. No model was able to fix its own bugs even when I explicitly pointed them out. The models seemed to have a real problem understanding their own code, not really being able to make simple changes. My favorite moment was when devstral-small-2 randomly switched to speaking in Dutch for some reason then seemed to have an identity crisis?&lt;/p&gt; &lt;p&gt;For comparison to a free hosted model, I tried gemini 2.5 flash. It did better than the local models, but also made basic syntax mistakes. It also got rate limited very quickly on the free tier.&lt;/p&gt; &lt;p&gt;Has anyone had a better experience using local models for coding? Maybe Goose is the problem and you have better tooling?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/st8ic88"&gt; /u/st8ic88 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjxyqb/experiences_with_local_coding_agents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjxyqb/experiences_with_local_coding_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qjxyqb/experiences_with_local_coding_agents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-22T15:46:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1qjf6ys</id>
    <title>Wrote a guide for running Claude Code with GLM-4.7 Flash locally with llama.cpp</title>
    <updated>2026-01-22T00:17:31+00:00</updated>
    <author>
      <name>/u/tammamtech</name>
      <uri>https://old.reddit.com/user/tammamtech</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Many of ollama features are now support llama.cpp server but aren't well documented. The ollama convenience features can be replicated in llama.cpp now, the main ones I wanted were model swapping, and freeing gpu memory on idle because I run llama.cpp as a docker service exposed to internet with cloudflare tunnels.&lt;/p&gt; &lt;p&gt;The GLM-4.7 flash release and the recent support for Anthropic API in llama.cpp server gave me the motivation to finally make this happen. I basically wanted to run Claude Code from laptop withGLM 4.7 Flash running on my PC.&lt;/p&gt; &lt;p&gt;I wrote a slightly more comprehensive version&lt;a href="https://tammam.io/blog/llama-cpp-setup-with-claude-codex-cli/"&gt;here&lt;/a&gt;&lt;/p&gt; &lt;h3&gt;Install llama.cpp if you don't have it&lt;/h3&gt; &lt;p&gt;I'm going to assume you have llama-cli or llama-server installed or you have ability to run docker containers with gpu. There are many sources for how to do this.&lt;/p&gt; &lt;h3&gt;Running the model&lt;/h3&gt; &lt;p&gt;All you need is the following command if you just want to run GLM 4.7 Flash.&lt;/p&gt; &lt;p&gt;&lt;code&gt;bash llama-server -hf unsloth/GLM-4.7-Flash-GGUF:UD-Q4_K_XL \ --alias glm-4.7-flash \ --jinja --ctx-size 32768 \ --temp 1.0 --top-p 0.95 --min-p 0.01 --fit on \ --sleep-idle-seconds 300 \ --host 0.0.0.0 --port 8080 &lt;/code&gt;&lt;/p&gt; &lt;p&gt;The command above will download the model on first run and cache it locally. The `&lt;code&gt;sleep-idle-seconds 300&lt;/code&gt; frees GPU memory after 5 minutes of idle so you can keep the server running.&lt;/p&gt; &lt;p&gt;The sampling parameters above (&lt;code&gt;--temp 1.0 --top-p 0.95 --min-p 0.01&lt;/code&gt;) are the recommended settings for GLM-4.7 general use. For tool-calling, use &lt;code&gt;--temp 0.7 --top-p 1.0&lt;/code&gt; instead.&lt;/p&gt; &lt;h4&gt;Or With Docker&lt;/h4&gt; &lt;p&gt;&lt;code&gt;bash docker run --gpus all -p 8080:8080 \ ghcr.io/ggml-org/llama.cpp:server-cuda \ -hf unsloth/GLM-4.7-Flash-GGUF:UD-Q4_K_XL \ --jinja --ctx-size 32768 \ --temp 1.0 --top-p 0.95 --min-p 0.01 --fit on \ --sleep-idle-seconds 300 \ --host 0.0.0.0 --port 8080 &lt;/code&gt;&lt;/p&gt; &lt;h3&gt;Multi-Model Setup with Config File&lt;/h3&gt; &lt;p&gt;If you want to run multiple models with router mode, you'll need a config file. This lets the server load models on demand based on what clients request.&lt;/p&gt; &lt;p&gt;First, download your models (or let them download via &lt;code&gt;-hf&lt;/code&gt; on first use):&lt;/p&gt; &lt;p&gt;&lt;code&gt;bash mkdir -p ~/llama-cpp &amp;amp;&amp;amp; touch ~/llama-cpp/config.ini &lt;/code&gt;&lt;/p&gt; &lt;p&gt;In &lt;code&gt;~/llama-cpp/config.ini&lt;/code&gt; put your models settings:&lt;/p&gt; &lt;p&gt;```ini [*] &lt;/p&gt; &lt;h1&gt;Global settings&lt;/h1&gt; &lt;p&gt;[glm-4.7-flash] hf-repo = unsloth/GLM-4.7-Flash-GGUF:UD-Q4_K_XL jinja = true temp = 0.7 ctx-size = 32768 top-p = 1 min-p = 0.01 fit = on&lt;/p&gt; &lt;p&gt;[other-model] ... ```&lt;/p&gt; &lt;h4&gt;Run with Router Mode&lt;/h4&gt; &lt;p&gt;&lt;code&gt;bash llama-server \ --models-preset ~/llama-cpp/config.ini \ --sleep-idle-seconds 300 \ --host 0.0.0.0 --port 8080 --models-max 1 &lt;/code&gt;&lt;/p&gt; &lt;h4&gt;Or with Docker&lt;/h4&gt; &lt;p&gt;&lt;code&gt;bash docker run --gpus all -p 8080:8080 \ -v ~/llama-cpp/config.ini:/config.ini \ ghcr.io/ggml-org/llama.cpp:server-cuda \ --models-preset /config.ini \ --sleep-idle-seconds 300 \ --host 0.0.0.0 --port 8080 \ --models-max 1 &lt;/code&gt;&lt;/p&gt; &lt;h2&gt;Configuring Claude Code&lt;/h2&gt; &lt;p&gt;Claude Code can be pointed at your local server. In your terminal run&lt;/p&gt; &lt;p&gt;&lt;code&gt;bash export ANTHROPIC_BASE_URL=http://localhost:8080 claude --model glm-4.7-flash &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Claude Code will now use your local model instead of hitting Anthropic's servers.&lt;/p&gt; &lt;h2&gt;Configuring Codex CLI&lt;/h2&gt; &lt;p&gt;You can also configure the Codex CLI to use your local server. Modify the &lt;code&gt;~/.codex/config.toml&lt;/code&gt; to look something like this:&lt;/p&gt; &lt;p&gt;```toml model = &amp;quot;glm-4.7-flash&amp;quot; model_reasoning_effort = &amp;quot;medium&amp;quot; model_provider=&amp;quot;llamacpp&amp;quot;&lt;/p&gt; &lt;p&gt;[model_providers.llamacpp] name=&amp;quot;llamacpp&amp;quot; base_url=&amp;quot;http://localhost:8080/v1&amp;quot; ```&lt;/p&gt; &lt;h2&gt;Some Extra Notes&lt;/h2&gt; &lt;p&gt;&lt;strong&gt;Model load time&lt;/strong&gt;: When a model is unloaded (after idle timeout), the next request has to wait for it to load again. For large models this can take some time. Tune &lt;code&gt;--sleep-idle-seconds&lt;/code&gt; based on your usage pattern.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Performance and Memory Tuning&lt;/strong&gt;: There are more flags you can use in llama.cpp for tuning cpu offloading, flash attention, etc that you can use to optimize memory usage and performance. The &lt;code&gt;--fit&lt;/code&gt; flag is a good starting point. Check the &lt;a href="https://github.com/ggml-org/llama.cpp/blob/master/tools/server/README.md"&gt;llama.cpp server docs&lt;/a&gt; for details on all the flags.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Internet Access&lt;/strong&gt;: If you want to use models deployed on your PC from say your laptop, the easiest way is to use something like Cloudflare tunnels, I go over setting this up in &lt;a href="https://tammam.io/blog/access-sd-ui-over-internet"&gt;my Stable Diffusion setup guide&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Auth&lt;/strong&gt;: If exposing the server to the internet, you can use &lt;code&gt;--api-key KEY&lt;/code&gt; to require an API key for authentication.&lt;/p&gt; &lt;p&gt;Edit 1: you should probably not use ctx-size param if using --fit.&lt;/p&gt; &lt;p&gt;Edit 2: replaced llama-cli with llama-server which is what I personally tested&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tammamtech"&gt; /u/tammamtech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjf6ys/wrote_a_guide_for_running_claude_code_with_glm47/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjf6ys/wrote_a_guide_for_running_claude_code_with_glm47/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qjf6ys/wrote_a_guide_for_running_claude_code_with_glm47/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-22T00:17:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1qjt08m</id>
    <title>Sleeping on Engram</title>
    <updated>2026-01-22T12:16:42+00:00</updated>
    <author>
      <name>/u/cravic</name>
      <uri>https://old.reddit.com/user/cravic</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The more I look at it the more I am convinced that the Engram model developed by Deepseek will have a similar impact on AI development as RL and the Transformer.&lt;/p&gt; &lt;p&gt;To expand on why.&lt;/p&gt; &lt;p&gt;1) Grounded fact checking fixing most hallucinations.&lt;/p&gt; &lt;p&gt;2) Vast model knowledge being available for very small models... think 3 billion parameter models that do better on knowledge task than 1 trillion parameter models because they have 1 trillion parameter Engram tables to pull grounded facts from. &lt;/p&gt; &lt;p&gt;3) the biggest reason is the impact it has on RL scaling for small models. We know reasoning benefits from RL more than model size and RL is much cheaper on smaller models... a 3 billion parameter doing the same RL training as a 3 trillion parameter model will cost literally 1000X less compute. &lt;/p&gt; &lt;p&gt;This allows for previously unthinkable RL scaling for small models without risking losing its factual knowledge because the factual knowledge is stored in the Engram table. &lt;/p&gt; &lt;p&gt;We have seen small models match larger models in limited use cases when RL is applied... but this was not scalable before because the small models lose their factual knowledge to make room for reasoning capability because of limited parameter space... Engram fixes that. &lt;/p&gt; &lt;p&gt;Over time this leads to very capable small models that border on AGI capabilities.&lt;/p&gt; &lt;p&gt;Yet the community seems almost silent on Engram.. can anyone say why the odd silence?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cravic"&gt; /u/cravic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjt08m/sleeping_on_engram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjt08m/sleeping_on_engram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qjt08m/sleeping_on_engram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-22T12:16:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1qjxp4p</id>
    <title>VibeVoice LoRAs are a thing</title>
    <updated>2026-01-22T15:36:02+00:00</updated>
    <author>
      <name>/u/llamabott</name>
      <uri>https://old.reddit.com/user/llamabott</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wasn't aware of this until recently, but started experimenting with them for the last couple days. Some learnings below, plus some sample output.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Trainer:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This trainer has worked very well so far: &lt;a href="https://github.com/voicepowered-ai/VibeVoice-finetuning"&gt;https://github.com/voicepowered-ai/VibeVoice-finetuning&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The sample arguments in the README for using a local dataset are fine, but &lt;code&gt;--voice_prompt_drop_rate&lt;/code&gt;should be set to 1 for single-speaker training. Also, lowering gradient accumulation steps to like 4 helps. Training against the 1.5B model fills up the full 24GB of my 4090. I've found all intermediate checkpoints starting from 15 minutes on ('wall clock time') to be very usable. Further training yields incremental improvements, though sometimes hard to tell one way or the other. And it seems pretty difficult to fry the lora, at least with datasets I've been using, which have ranged from 45 minutes to 2 hours' worth of audio.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Pros/cons;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Using loras instead of voice clone samples resolves the most important weaknesses of the 1.5B model:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;No more random music (yes really)&lt;/li&gt; &lt;li&gt;No more chronic truncation of the last word of a prompt&lt;/li&gt; &lt;li&gt;No more occurrences of a reference voice prompt &lt;em&gt;leaking&lt;/em&gt; into the audio output (that's the one that really kills me)&lt;/li&gt; &lt;li&gt;Dramatically lower word error rate all the way around, equaling the 7B model + zero shot voice clone or basically any other open weight TTS model I've tried for that matter.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In terms of raw voice likeness, my loras thus far have ranged from just okay to very good, but can't quite match the results of simple zero shot voice cloning. But the more unique the qualities of the source vocal material are, the better (though I guess that's always the case, regardless). &lt;/p&gt; &lt;p&gt;&lt;strong&gt;How to run:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The gradio demo in the &lt;a href="https://github.com/vibevoice-community/VibeVoice"&gt;VibeVoice Community repo&lt;/a&gt; accepts loras by adding a command line argument `--checkpoint_path path/to/checkpoint`.&lt;/p&gt; &lt;p&gt;And I just added vibevoice lora support to my audiobook creator app &lt;a href="https://github.com/zeropointnine/tts-audiobook-tool"&gt;tts-audiobook-tool&lt;/a&gt; (&lt;code&gt;Voice clone and model settings&lt;/code&gt; &amp;gt; &lt;code&gt;Lora&lt;/code&gt;, and enter either a local path or a huggingface dataset repo id). &lt;/p&gt; &lt;p&gt;CFG matters a lot and should be experimented with whenever testing a new checkpoint. A very low CFG (approaching 1.0) tends to be more raw, more sibilant (which can be good or bad, depending), and sometimes gives a greater likeness but also less stable. ~3.0 is usually my preference: More stable, often yields a fuller sound, and should still maintain good likeness without starting to sound generic if you've cherrypicked the right checkpoint.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Examples:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://zeropointnine.github.io/tts-audiobook-tool/browser_player/?url=https://zeropointnine.github.io/tts-audiobook-tool/browser_player/waves-vibevoice-1.5b-lora-hsrjl.abr.m4a"&gt;Here's some sample output&lt;/a&gt; using a lora I made using the settings described above and generated through tts-audiobook-tool (The web player is a feature of the project).&lt;/p&gt; &lt;p&gt;Not sure I should share the lora itself, but bonus points if you recognize the vocal source material and in which case, you'll be able to form opinions about likeness.&lt;/p&gt; &lt;p&gt;I did, however, create a lora using public domain source material for the purpose of sharing: &lt;a href="https://huggingface.co/vibevoice-community/klett"&gt;vibevoice-community/klett&lt;/a&gt;. Sound quality is somewhat compromised by the source audio and I'm not that crazy about the degree of likeness, but it can still be useful as a point of reference. (&lt;a href="https://zeropointnine.github.io/tts-audiobook-tool/browser_player/?url=https://zeropointnine.github.io/tts-audiobook-tool/browser_player/waves-vibevoice-1.5b-lora-klett.abr.m4a"&gt;sample output&lt;/a&gt;)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/llamabott"&gt; /u/llamabott &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjxp4p/vibevoice_loras_are_a_thing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjxp4p/vibevoice_loras_are_a_thing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qjxp4p/vibevoice_loras_are_a_thing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-22T15:36:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1qjaxfy</id>
    <title>8x AMD MI50 32GB at 26 t/s (tg) with MiniMax-M2.1 and 15 t/s (tg) with GLM 4.7 (vllm-gfx906)</title>
    <updated>2026-01-21T21:30:54+00:00</updated>
    <author>
      <name>/u/ai-infos</name>
      <uri>https://old.reddit.com/user/ai-infos</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjaxfy/8x_amd_mi50_32gb_at_26_ts_tg_with_minimaxm21_and/"&gt; &lt;img alt="8x AMD MI50 32GB at 26 t/s (tg) with MiniMax-M2.1 and 15 t/s (tg) with GLM 4.7 (vllm-gfx906)" src="https://preview.redd.it/16ndtph7treg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=df00bad2dcdf2390a12afaf191c07f1264ae2752" title="8x AMD MI50 32GB at 26 t/s (tg) with MiniMax-M2.1 and 15 t/s (tg) with GLM 4.7 (vllm-gfx906)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ul&gt; &lt;li&gt;&lt;strong&gt;MiniMax-M2.1&lt;/strong&gt; AWQ 4bit @ &lt;strong&gt;26.8 tok/s&lt;/strong&gt; (output) // 3000 tok/s (input of 30k tok) on vllm-gfx906 with MAX context length (196608)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GLM 4.7&lt;/strong&gt; AWQ 4bit @ &lt;strong&gt;15.6 tok/s&lt;/strong&gt; (output) // 3000 tok/s (input of 30k tok) on vllm-gfx906 with context length 95000&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;GPUs cost&lt;/strong&gt;: 880$ for 256GB VRAM (early 2025 prices)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Power draw&lt;/strong&gt;: 280W (idle) / 1200W (inference)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Goal&lt;/strong&gt;: reach one of the most cost effective solution of the world for one of the best fast intelligent local inference setup.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Credits&lt;/strong&gt;: BIG thanks to the Global Open source Community!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;All setup details here:&lt;/strong&gt; &lt;a href="https://github.com/ai-infos/guidances-setup-8-mi50-glm47-minimax-m21/tree/main"&gt;https://github.com/ai-infos/guidances-setup-8-mi50-glm47-minimax-m21/tree/main&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Feel free to ask any questions and/or share any comments.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;PS&lt;/strong&gt;: few weeks ago, I posted here this setup of 16 MI50 with deepeseek v3.2: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1q6n5vl/16x_amd_mi50_32gb_at_10_ts_tg_2k_ts_pp_with/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1q6n5vl/16x_amd_mi50_32gb_at_10_ts_tg_2k_ts_pp_with/&lt;/a&gt; After few more tests/dev on it, I could have reached 14 tok/s but still not stable after ~18k tokens context input (generating garbage output) so almost useless for me. Whereas, the above models (Minimax M2.1 and GLM 4.7) are pretty stable at long context so usable for coding agents usecases etc.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ai-infos"&gt; /u/ai-infos &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/16ndtph7treg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjaxfy/8x_amd_mi50_32gb_at_26_ts_tg_with_minimaxm21_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qjaxfy/8x_amd_mi50_32gb_at_26_ts_tg_with_minimaxm21_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-21T21:30:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1qjotja</id>
    <title>Qwen3 TTS Open Source VLLM-Omni PR</title>
    <updated>2026-01-22T08:07:52+00:00</updated>
    <author>
      <name>/u/jnk_str</name>
      <uri>https://old.reddit.com/user/jnk_str</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Might be coming soon..&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/vllm-project/vllm-omni/pull/895"&gt;https://github.com/vllm-project/vllm-omni/pull/895&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jnk_str"&gt; /u/jnk_str &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjotja/qwen3_tts_open_source_vllmomni_pr/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjotja/qwen3_tts_open_source_vllmomni_pr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qjotja/qwen3_tts_open_source_vllmomni_pr/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-22T08:07:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1qjjrmq</id>
    <title>Fei Fei Li dropped a non-JEPA world model, and the spatial intelligence is insane</title>
    <updated>2026-01-22T03:39:33+00:00</updated>
    <author>
      <name>/u/coloradical5280</name>
      <uri>https://old.reddit.com/user/coloradical5280</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjjrmq/fei_fei_li_dropped_a_nonjepa_world_model_and_the/"&gt; &lt;img alt="Fei Fei Li dropped a non-JEPA world model, and the spatial intelligence is insane" src="https://external-preview.redd.it/dHl1a3MydnZsdGVnMeL77RZtngf3FeBaBzx1OTOSVuPnAYqXhpVZKKbs_rnV.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ce79108f93f379daa30c29208725387fb2320ee4" title="Fei Fei Li dropped a non-JEPA world model, and the spatial intelligence is insane" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Fei-Fei Li, the &amp;quot;godmother of modern AI&amp;quot; and a pioneer in computer vision, founded World Labs a few years ago with a small team and $230 million in funding. Last month, they launched &lt;a href="https://marble.worldlabs.ai/"&gt;https://marble.worldlabs.ai/&lt;/a&gt;, a generative world model that’s not JEPA, but instead built on Neural Radiance Fields (NeRF) and Gaussian splatting. &lt;/p&gt; &lt;p&gt;It’s &lt;em&gt;insanely fast&lt;/em&gt; for what it does, generating explorable 3D worlds in minutes. For example: &lt;a href="https://marble.worldlabs.ai/world/5b850e80-a587-48d7-9340-186e0bcbf46b"&gt;this scene&lt;/a&gt;. &lt;/p&gt; &lt;p&gt;Crucially, it’s not video. The frames aren’t rendered on-the-fly as you move. Instead, it’s a fully stateful 3D environment represented as a dense cloud of Gaussian splats—each with position, scale, rotation, color, and opacity. This means the world is persistent, editable, and supports non-destructive iteration. You can expand regions, modify materials, and even merge multiple worlds together. &lt;/p&gt; &lt;p&gt;You can share your world, others can build on it, and you can build on theirs. It natively supports VR (Vision Pro, Quest 3), and you can export splats or meshes for use in Unreal, Unity, or Blender via USDZ or GLB. &lt;/p&gt; &lt;p&gt;It's early, there are (very literally) rough edges, but it's crazy to think about this in 5 years. For free, you get a few generations to experiment; $20/month unlocks a lot, I just did one month so I could actually play, and definitely didn't max out credits. &lt;/p&gt; &lt;p&gt;Fei-Fei Li is an OG AI visionary, but zero hype. She’s been quiet, especially about this. So Marble hasn’t gotten the attention it deserves. &lt;/p&gt; &lt;p&gt;At first glance, visually, you might think, “meh”... but there’s &lt;strong&gt;no triangle-based geometry here, no real-time rendering pipeline, no frame-by-frame generation.&lt;/strong&gt; Just a solid, exportable, editable, stateful pile of splats. &lt;/p&gt; &lt;p&gt;The breakthrough isn't the image though, it’s the spatial intelligence. Y'all should play around, it's wild. &lt;/p&gt; &lt;p&gt;&lt;em&gt;I know this is a violation of Rule #2 but honestly there just aren't that many subs with people smart enough to appreciate this; no hard feelings if it needs be removed though.&lt;/em&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/coloradical5280"&gt; /u/coloradical5280 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/udsg2ztvlteg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjjrmq/fei_fei_li_dropped_a_nonjepa_world_model_and_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qjjrmq/fei_fei_li_dropped_a_nonjepa_world_model_and_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-22T03:39:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1qjuxq1</id>
    <title>Qwen3-TTS released on Hugging Face</title>
    <updated>2026-01-22T13:46:09+00:00</updated>
    <author>
      <name>/u/paf1138</name>
      <uri>https://old.reddit.com/user/paf1138</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjuxq1/qwen3tts_released_on_hugging_face/"&gt; &lt;img alt="Qwen3-TTS released on Hugging Face" src="https://external-preview.redd.it/7BxNqktM4o5ALuA1kWfED-mIho1h_s0v4BGX5KFnfkQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=998729f1d543b7a06571328a501ebf09c31629b9" title="Qwen3-TTS released on Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paf1138"&gt; /u/paf1138 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/Qwen/qwen3-tts"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjuxq1/qwen3tts_released_on_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qjuxq1/qwen3tts_released_on_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-22T13:46:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1qjnbh8</id>
    <title>Thoughts on LLMs (closed- and open-source) in software development after one year of professional use.</title>
    <updated>2026-01-22T06:38:33+00:00</updated>
    <author>
      <name>/u/grey-seagull</name>
      <uri>https://old.reddit.com/user/grey-seagull</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ul&gt; &lt;li&gt;Chatbots are amazing at codebase exploration.&lt;/li&gt; &lt;li&gt;Chatbots are good at checking regression while going through ideas, especially Codex.&lt;/li&gt; &lt;li&gt;Codex is best at debugging.&lt;/li&gt; &lt;li&gt;Claude is better than others in code quality.&lt;/li&gt; &lt;li&gt;Gemini is bad at instruction following.&lt;/li&gt; &lt;li&gt;Biggest open source LLMs are basically at par with the above models.&lt;/li&gt; &lt;li&gt;Local model aren't much help not even for easier tasks. The models you can run locally using 24-40 GB of VRAM are underwhelming and slow. The agentic flows, especially, can quickly build up big KV caches which are too much and too slow to handle locally. Forget about multiple 100k+ token chat sessions concurrently. Economies of scale win here to bring the best value out of a certain capex spent on hardware. Models like gemini flash are fast, good and cheap.&lt;/li&gt; &lt;li&gt;That said, the biggest open-source models can basically match GPTs and Claudes of the world now and at a fraction of the cost. Since, for most people, they are too big to run locally the only viable option is various 3rd party hosted ones but they are often not trusted enough to be used with internal company codebases. This means we are mostly left with OpenAI, Anthropic or Google’s models.&lt;/li&gt; &lt;li&gt;Since code generation is cheap now (LLMs), going out of the way for thoughtful tests, readability, and PR documentation is the minimum now.&lt;/li&gt; &lt;li&gt;Code cannot be merged at the rate it is produced because you have to own what was generated. The main gain we get is elevation from generation to checking, which is faster but not a substitute for skills.&lt;/li&gt; &lt;li&gt;Because you have to own the work, you have to be competent in that area. Paradoxically, if LLMs are relied on too much, they can hinder your ability to develop enough competence to supervise the work.&lt;/li&gt; &lt;li&gt;On the flip side, LLMs do allow greater exposure to the problem set much faster: fail fast → solve → get better (rapid iteration). In other words, they complement your agency. It remains an open question which of these two wins out for developing competence.&lt;/li&gt; &lt;li&gt;Rapid comprehension appears to be the most standout capability of LLMs over humans. So the longer the longer and the richer the context the most we can get out of LLMs. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://www.ubaada.com/post/a32d3df0"&gt;Original Post&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/grey-seagull"&gt; /u/grey-seagull &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjnbh8/thoughts_on_llms_closed_and_opensource_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjnbh8/thoughts_on_llms_closed_and_opensource_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qjnbh8/thoughts_on_llms_closed_and_opensource_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-22T06:38:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1qjqgnr</id>
    <title>This Week's Hottest Hugging Face Releases: Top Picks by Category!</title>
    <updated>2026-01-22T09:51:15+00:00</updated>
    <author>
      <name>/u/techlatest_net</name>
      <uri>https://old.reddit.com/user/techlatest_net</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hugging Face trending is on fire this week with fresh drops in text generation, image, audio, and more.&lt;/p&gt; &lt;p&gt;Check 'em out and drop your thoughts—which one's getting deployed first?&lt;/p&gt; &lt;h1&gt;Text Generation&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/zai-org/GLM-4.7-Flash"&gt;&lt;strong&gt;zai-org/GLM-4.7-Flash&lt;/strong&gt;&lt;/a&gt;: 31B param model for fast, efficient text gen—updated 2 days ago with 124k downloads and 932 likes. Ideal for real-time apps and agents.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF"&gt;&lt;strong&gt;unsloth/GLM-4.7-Flash-GGUF&lt;/strong&gt;&lt;/a&gt;: Quantized 30B version for easy local inference—hot with 112k downloads in hours. Great for low-resource setups.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Image / Multimodal&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/zai-org/GLM-Image"&gt;&lt;strong&gt;zai-org/GLM-Image&lt;/strong&gt;&lt;/a&gt;: Image-text-to-image powerhouse—10.8k downloads, 938 likes. Excels in creative edits and generation.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/google/translategemma-4b-it"&gt;&lt;strong&gt;google/translategemma-4b-it&lt;/strong&gt;&lt;/a&gt;: 5B vision-language model for multilingual image-text tasks—45.4k downloads, supports translation + vision.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Audio / Speech&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/kyutai/pocket-tts"&gt;&lt;strong&gt;kyutai/pocket-tts&lt;/strong&gt;&lt;/a&gt;: Compact TTS for natural voices—38.8k downloads, 397 likes. Pocket-sized for mobile/edge deployment.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/microsoft/VibeVoice-ASR"&gt;&lt;strong&gt;microsoft/VibeVoice-ASR&lt;/strong&gt;&lt;/a&gt;: 9B ASR for multilingual speech recognition—ultra-low latency, 816 downloads already spiking.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Other Hot Categories (Video/Agentic)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Lightricks/LTX-2"&gt;&lt;strong&gt;Lightricks/LTX-2&lt;/strong&gt;&lt;/a&gt; (Image-to-Video): 1.96M downloads, 1.25k likes—pro-level video from images.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/stepfun-ai/Step3-VL-10B"&gt;&lt;strong&gt;stepfun-ai/Step3-VL-10B&lt;/strong&gt;&lt;/a&gt; (Image-Text-to-Text): 10B VL model for advanced reasoning—28.6k downloads in hours.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;These are dominating trends with massive community traction.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/techlatest_net"&gt; /u/techlatest_net &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjqgnr/this_weeks_hottest_hugging_face_releases_top/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjqgnr/this_weeks_hottest_hugging_face_releases_top/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qjqgnr/this_weeks_hottest_hugging_face_releases_top/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-22T09:51:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1qjrsur</id>
    <title>GLM 4.7 flash FA fix for CUDA has been merged into llama.cpp</title>
    <updated>2026-01-22T11:10:42+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjrsur/glm_47_flash_fa_fix_for_cuda_has_been_merged_into/"&gt; &lt;img alt="GLM 4.7 flash FA fix for CUDA has been merged into llama.cpp" src="https://external-preview.redd.it/TcGseIeP3Z00NB4otbKR8-_fs_ssjxg6HC4Fv_lVbUU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d37072f08acdffcd4c3617847f00d2a9b9bafcf4" title="GLM 4.7 flash FA fix for CUDA has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/18953"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjrsur/glm_47_flash_fa_fix_for_cuda_has_been_merged_into/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qjrsur/glm_47_flash_fa_fix_for_cuda_has_been_merged_into/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-22T11:10:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1qjul2g</id>
    <title>Qwen3 TTS just dropped 🗣️🔈</title>
    <updated>2026-01-22T13:31:10+00:00</updated>
    <author>
      <name>/u/Reasonable-Fun-7078</name>
      <uri>https://old.reddit.com/user/Reasonable-Fun-7078</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/QwenLM/Qwen3-TTS"&gt;https://github.com/QwenLM/Qwen3-TTS&lt;/a&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/collections/Qwen/qwen3-tts"&gt;https://huggingface.co/collections/Qwen/qwen3-tts&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reasonable-Fun-7078"&gt; /u/Reasonable-Fun-7078 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjul2g/qwen3_tts_just_dropped/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjul2g/qwen3_tts_just_dropped/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qjul2g/qwen3_tts_just_dropped/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-22T13:31:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1qjp29u</id>
    <title>So THAT'S why generations take so long sometimes</title>
    <updated>2026-01-22T08:23:01+00:00</updated>
    <author>
      <name>/u/linkcharger</name>
      <uri>https://old.reddit.com/user/linkcharger</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjp29u/so_thats_why_generations_take_so_long_sometimes/"&gt; &lt;img alt="So THAT'S why generations take so long sometimes" src="https://external-preview.redd.it/NDExcmt3bHcxdmVnMbqlCOGHRc5n3_cDftfsgD3DArw7u7f4exLoWTuSZ93a.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=02862344fe7ac8a58e90fba9b10a59c1d8140d34" title="So THAT'S why generations take so long sometimes" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/linkcharger"&gt; /u/linkcharger &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/6p9cu9rw1veg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjp29u/so_thats_why_generations_take_so_long_sometimes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qjp29u/so_thats_why_generations_take_so_long_sometimes/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-22T08:23:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1qjul5t</id>
    <title>Qwen have open-sourced the full family of Qwen3-TTS: VoiceDesign, CustomVoice, and Base, 5 models (0.6B &amp; 1.8B), Support for 10 languages</title>
    <updated>2026-01-22T13:31:16+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjul5t/qwen_have_opensourced_the_full_family_of_qwen3tts/"&gt; &lt;img alt="Qwen have open-sourced the full family of Qwen3-TTS: VoiceDesign, CustomVoice, and Base, 5 models (0.6B &amp;amp; 1.8B), Support for 10 languages" src="https://preview.redd.it/wo9tqflvkweg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=75bf194547e68a1bb648f530175a2ec826899fd0" title="Qwen have open-sourced the full family of Qwen3-TTS: VoiceDesign, CustomVoice, and Base, 5 models (0.6B &amp;amp; 1.8B), Support for 10 languages" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Github: &lt;a href="https://github.com/QwenLM/Qwen3-TTS"&gt;https://github.com/QwenLM/Qwen3-TTS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hugging Face: &lt;a href="https://huggingface.co/collections/Qwen/qwen3-tts"&gt;https://huggingface.co/collections/Qwen/qwen3-tts&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog: &lt;a href="https://qwen.ai/blog?id=qwen3tts-0115"&gt;https://qwen.ai/blog?id=qwen3tts-0115&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Paper: &lt;a href="https://github.com/QwenLM/Qwen3-TTS/blob/main/assets/Qwen3_TTS.pdf"&gt;https://github.com/QwenLM/Qwen3-TTS/blob/main/assets/Qwen3_TTS.pdf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hugging Face Demo: &lt;a href="https://huggingface.co/spaces/Qwen/Qwen3-TTS"&gt;https://huggingface.co/spaces/Qwen/Qwen3-TTS&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wo9tqflvkweg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjul5t/qwen_have_opensourced_the_full_family_of_qwen3tts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qjul5t/qwen_have_opensourced_the_full_family_of_qwen3tts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-22T13:31:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1qjtyw8</id>
    <title>Qwen dev on Twitter!!</title>
    <updated>2026-01-22T13:03:26+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjtyw8/qwen_dev_on_twitter/"&gt; &lt;img alt="Qwen dev on Twitter!!" src="https://preview.redd.it/avu4mhyvfweg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=60312577cba6dc65c74da0313ab4d31252bd6be2" title="Qwen dev on Twitter!!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/avu4mhyvfweg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjtyw8/qwen_dev_on_twitter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qjtyw8/qwen_dev_on_twitter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-22T13:03:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
