<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-23T21:20:32+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1rcqqlz</id>
    <title>Looking for a perfect "Deep Research" app which works with Llama.cpp</title>
    <updated>2026-02-23T19:11:11+00:00</updated>
    <author>
      <name>/u/hackiv</name>
      <uri>https://old.reddit.com/user/hackiv</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have found something like Perplexica but can't get it to work with llamacpp. suggestions appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hackiv"&gt; /u/hackiv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcqqlz/looking_for_a_perfect_deep_research_app_which/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcqqlz/looking_for_a_perfect_deep_research_app_which/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rcqqlz/looking_for_a_perfect_deep_research_app_which/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T19:11:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcqv6b</id>
    <title>A guide to building an ML research cluster</title>
    <updated>2026-02-23T19:15:39+00:00</updated>
    <author>
      <name>/u/OriginalSpread3100</name>
      <uri>https://old.reddit.com/user/OriginalSpread3100</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcqv6b/a_guide_to_building_an_ml_research_cluster/"&gt; &lt;img alt="A guide to building an ML research cluster" src="https://preview.redd.it/nkxg0gwanalg1.png?width=140&amp;amp;height=77&amp;amp;auto=webp&amp;amp;s=72e5f2c810db833ceb28400bf7f3945dfd427331" title="A guide to building an ML research cluster" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/nkxg0gwanalg1.png?width=2784&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e0e5831362fb3c54e940881bcba8a20d71d94f63"&gt;https://preview.redd.it/nkxg0gwanalg1.png?width=2784&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e0e5831362fb3c54e940881bcba8a20d71d94f63&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you‚Äôre doing local training/fine-tuning and you‚Äôre somewhere between ‚Äúone GPU rig‚Äù and ‚Äúwe might add another box soon,‚Äù we wrote up a practical guide that tries to cover that whole progression.&lt;/p&gt; &lt;p&gt;The repo for The Definitive Guide to Building a Machine Learning Research Cluster From Scratch (PRs/Issues welcome):&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/transformerlab/build-a-machine-learning-research-cluster"&gt;https://github.com/transformerlab/build-a-machine-learning-research-cluster&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Includes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Technical blueprint for single ‚Äúunder-the-desk‚Äù GPU server to scaling university-wide cluster for 1,000+ users&lt;/li&gt; &lt;li&gt;Tried and tested configurations for drivers, orchestration, storage, scheduling, and UI with a bias toward modern, simple tooling that is open source and easy to maintain.&lt;/li&gt; &lt;li&gt;Step-by-step install guides (CUDA, ROCm, k3s, Rancher, SLURM/SkyPilot paths)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We‚Äôd appreciate feedback from people who‚Äôve dealt with this.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OriginalSpread3100"&gt; /u/OriginalSpread3100 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcqv6b/a_guide_to_building_an_ml_research_cluster/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcqv6b/a_guide_to_building_an_ml_research_cluster/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rcqv6b/a_guide_to_building_an_ml_research_cluster/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T19:15:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1rccsjg</id>
    <title>I made an interactive timeline of 171 LLMs (2017‚Äì2026)</title>
    <updated>2026-02-23T09:18:16+00:00</updated>
    <author>
      <name>/u/asymortenson</name>
      <uri>https://old.reddit.com/user/asymortenson</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Built a visual timeline tracking every major Large Language Model ‚Äî from the original Transformer paper to GPT-5.3 Codex.&lt;/p&gt; &lt;p&gt;171 models, 54 organizations. Filterable by open/closed source, searchable, with milestones highlighted.&lt;/p&gt; &lt;p&gt;Some stats from the data: - 2024‚Äì2025 was the explosion: 108 models in two years - Open source reached parity with closed in 2025 (29 vs 28) - Chinese labs account for ~20% of all major releases (10 orgs, 32 models)&lt;/p&gt; &lt;p&gt;&lt;a href="https://llm-timeline.com"&gt;https://llm-timeline.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Missing a model? Let me know and I'll add it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/asymortenson"&gt; /u/asymortenson &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rccsjg/i_made_an_interactive_timeline_of_171_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rccsjg/i_made_an_interactive_timeline_of_171_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rccsjg/i_made_an_interactive_timeline_of_171_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T09:18:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1rc97qf</id>
    <title>üåä Wave Field LLM O(n log n) Successfully Scales to 1B Parameters</title>
    <updated>2026-02-23T05:44:29+00:00</updated>
    <author>
      <name>/u/Murky-Sign37</name>
      <uri>https://old.reddit.com/user/Murky-Sign37</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rc97qf/wave_field_llm_on_log_n_successfully_scales_to_1b/"&gt; &lt;img alt="üåä Wave Field LLM O(n log n) Successfully Scales to 1B Parameters" src="https://preview.redd.it/6m7q2vzlm6lg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2ede585956ec96d0434754c49701c58176ad83ad" title="üåä Wave Field LLM O(n log n) Successfully Scales to 1B Parameters" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just completed full pretraining of &lt;strong&gt;Wave Field LLM (v4) at 1B scale&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Training Summary:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Parameters:&lt;/strong&gt; 825M&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Total Tokens:&lt;/strong&gt; 1.33B&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Final PPL:&lt;/strong&gt; 72.2&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Best PPL:&lt;/strong&gt; 72.2&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Final Accuracy:&lt;/strong&gt; 27.1%&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Training Time:&lt;/strong&gt; 13.2 hours&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This isn‚Äôt a small 30M or 124M experiment anymore.&lt;/p&gt; &lt;p&gt;Wave Field is now:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;‚úÖ Stable at near-billion scale&lt;/li&gt; &lt;li&gt;‚úÖ Training cleanly&lt;/li&gt; &lt;li&gt;‚úÖ Converging properly&lt;/li&gt; &lt;li&gt;‚úÖ Saving best checkpoints&lt;/li&gt; &lt;li&gt;‚úÖ Handling &amp;gt;1B tokens&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The key takeaway:&lt;/p&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;This validates that Wave Field‚Äôs field-based interaction mechanism is not just an experimental curiosity ‚Äî it holds up under real model size and real token volume &lt;a href="https://github.com/badaramoni/wave-field-llm"&gt;git&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Murky-Sign37"&gt; /u/Murky-Sign37 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6m7q2vzlm6lg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rc97qf/wave_field_llm_on_log_n_successfully_scales_to_1b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rc97qf/wave_field_llm_on_log_n_successfully_scales_to_1b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T05:44:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1rckcww</id>
    <title>Benchmarked 4 AI Memory Systems on 600-Turn Conversations - Here Are the Results</title>
    <updated>2026-02-23T15:25:00+00:00</updated>
    <author>
      <name>/u/singh_taranjeet</name>
      <uri>https://old.reddit.com/user/singh_taranjeet</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We just completed comprehensive benchmarks comparing memory layers for production AI agents. Tested Mem0 against OpenAI Memory, LangMem, and MemGPT across 10 multi-session conversations with 200 questions each.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key findings:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Mem0&lt;/strong&gt;: 66.9% accuracy, 1.4s p95 latency, ~2K tokens per query&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Mem0 Graph&lt;/strong&gt;: 68.5% accuracy, 2.6s p95 latency, ~4K tokens (superior temporal reasoning)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;OpenAI Memory&lt;/strong&gt;: 52.9% accuracy, 0.9s p95 latency, ~5K tokens&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LangMem&lt;/strong&gt;: 58.1% accuracy, 60s p95 latency, ~130 tokens&lt;/li&gt; &lt;li&gt;&lt;strong&gt;MemGPT&lt;/strong&gt;: Results in appendix&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What stands out:&lt;/strong&gt; Mem0 achieved 14 percentage points higher accuracy than OpenAI Memory while maintaining sub-2s response times. The graph variant excels at temporal queries (58.1% vs OpenAI's 21.7%) and multi-hop reasoning.&lt;/p&gt; &lt;p&gt;LangMem's 60-second latency makes it unusable for interactive applications, despite being open source.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Methodology:&lt;/strong&gt; Used LOCOMO dataset with GPT-4o-mini at temperature 0. Evaluated factual consistency, multi-hop reasoning, temporal understanding, and open-domain recall across 26K+ token conversations.&lt;/p&gt; &lt;p&gt;This matters because production agents need memory that persists beyond context windows while maintaining chat-level responsiveness. Current approaches either sacrifice accuracy for speed or become too slow for real-time use.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/singh_taranjeet"&gt; /u/singh_taranjeet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rckcww/benchmarked_4_ai_memory_systems_on_600turn/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rckcww/benchmarked_4_ai_memory_systems_on_600turn/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rckcww/benchmarked_4_ai_memory_systems_on_600turn/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T15:25:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1rc3naj</id>
    <title>Super New to Godot, used Claude Code/gpt-oss-120b locally to help me vibecode a simple platformer game about a grumpy mage who follows you around making fun of you lmao.</title>
    <updated>2026-02-23T01:13:04+00:00</updated>
    <author>
      <name>/u/swagonflyyyy</name>
      <uri>https://old.reddit.com/user/swagonflyyyy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rc3naj/super_new_to_godot_used_claude_codegptoss120b/"&gt; &lt;img alt="Super New to Godot, used Claude Code/gpt-oss-120b locally to help me vibecode a simple platformer game about a grumpy mage who follows you around making fun of you lmao." src="https://external-preview.redd.it/MmJ6MGRjNjA4NWxnMR3Al36Nr886FX7jQ_P96fNg8PSf4Zsku92kjG2XN_qv.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b8910573e373960eea6962553218ddcd88a9324c" title="Super New to Godot, used Claude Code/gpt-oss-120b locally to help me vibecode a simple platformer game about a grumpy mage who follows you around making fun of you lmao." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Yeah, I was bored so I spent the last two weeks experimenting with vibecoding with local LLMs, namely gpt-oss-120b.&lt;/p&gt; &lt;p&gt;I started with Cline, didn't like it at all because it was overheating my GPU while giving back too little. Codex was even worse, locally, leading to weird CPU switches mid-generation when there was supposed to be enough VRAM to run the model entirely on GPU. Then I tried Claude Code and that's when my expectations were exceeded, &lt;em&gt;big time.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;I first started with pygame, and after successfully one-shotting simple games (snake game, etc.) under the same project with the same model I decided to take it another level and use Claude Code with Godot, which was pretty easy to setup in VSCode and their IDE/extension. &lt;/p&gt; &lt;p&gt;Next thing I know, I spend the last two weeks making this game on Godot out of curiosity and using Claude Code to help me Vibecode parts of it along the way, and I came up with this game where you have a useful, snarky NPC that makes fun of you lmao.&lt;/p&gt; &lt;p&gt;The way it works is that the game is going to be gathering contextual information in real-time, e.g. actions taken, events occurring, etc. You can see that in the logs that are printed under the gameplay loop. &lt;/p&gt; &lt;p&gt;The mage then stores each chain of events in a chat history and comments on it every 10 seconds. The AI behavior is hard-coded but it works really well. However, I do plan on adding a hybrid approach where the LLM uses tool calls to make informed decisions depending on the situations, such as:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Switching equipment&lt;/li&gt; &lt;li&gt;Healing the player or himself&lt;/li&gt; &lt;li&gt;Pointing out objects of interest&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;And so forth. I haven't ruled out a Wizard of Oz worldbuilding AI that vibecodes enemies and obstacles throughout the game with tool calls, but that will be for another time.&lt;/p&gt; &lt;p&gt;I'm enjoying this process so I think I might actually finish this game, but we'll see how far I can get. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/swagonflyyyy"&gt; /u/swagonflyyyy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/jl31wp5085lg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rc3naj/super_new_to_godot_used_claude_codegptoss120b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rc3naj/super_new_to_godot_used_claude_codegptoss120b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T01:13:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1rckqpp</id>
    <title>Hardware requirements for training a ~3B Model From Scratch locally?</title>
    <updated>2026-02-23T15:39:08+00:00</updated>
    <author>
      <name>/u/Any-Cobbler6161</name>
      <uri>https://old.reddit.com/user/Any-Cobbler6161</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all,&lt;/p&gt; &lt;p&gt;I‚Äôm a data science master‚Äôs student who‚Äôs posted on here a couple times before over the last year or 2. Now am working on my senior thesis and I‚Äôm trying to figure out the feasibility of training a ~3B parameter transformer model from scratch. So not fine-tuning. I‚Äôm trying to figure out what‚Äôs realistically doable on a home setup within ~6 months. My school is unfortunately is a very small public school and doesn‚Äôt have their own cluster or anything like that. Prior to this I was at a bigger school that did so I was just planning on booking time using theirs but unfortunately last year I had to transfer because I got really sick as they didn‚Äôt make accommodations for folks with medical disability. &lt;/p&gt; &lt;p&gt;Anyways I was thinking about training something in the ball park of 3B Params, 2k context, 25/50b training tokens, in fp16, probably using AdamW. My current system I have designed based on some napkin math is 2x 3090s over nvlink as I already have a Z690 motherboard that supports x8/x8 bifurcation, 1200W PSU, and 64gb of DDR5 RAM. Prior to this I had a rtx 5090 but even though it was crazy fast the 32gb was not enough to hold all the weights, grads, buffers, optimizer states (AdamW), etc. &lt;/p&gt; &lt;p&gt;Just wanted to hop on here and see if anyone here actually trained a 3B model or slightly smaller from scratch at home and if so what GPUs did you use/how did you do it? If you‚Äôve done anything remotely similar (even 1B‚Äì2B scale), I‚Äôd love to hear your setup and how it went.&lt;/p&gt; &lt;p&gt;Appreciate any real-world data points , thanks üôè&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any-Cobbler6161"&gt; /u/Any-Cobbler6161 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rckqpp/hardware_requirements_for_training_a_3b_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rckqpp/hardware_requirements_for_training_a_3b_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rckqpp/hardware_requirements_for_training_a_3b_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T15:39:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcrzbn</id>
    <title>Strix Halo 128Gb: what models, which quants are optimal?</title>
    <updated>2026-02-23T19:55:21+00:00</updated>
    <author>
      <name>/u/DevelopmentBorn3978</name>
      <uri>https://old.reddit.com/user/DevelopmentBorn3978</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Strix Halo APU should not benefit from running large models that have been quantized using MXFP4 (as on Blackwell GPUs). So which models at which quants have you found that do shine on this architecture in GPU only mode (i.e. runnable with llama.cpp)? Could it benefit as well from usage of formats for models quantization that are closer to the native FP4/FP8 formats of these chips?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DevelopmentBorn3978"&gt; /u/DevelopmentBorn3978 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcrzbn/strix_halo_128gb_what_models_which_quants_are/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcrzbn/strix_halo_128gb_what_models_which_quants_are/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rcrzbn/strix_halo_128gb_what_models_which_quants_are/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T19:55:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1rci9h1</id>
    <title>TinyTeapot (77 million params): Context-grounded LLM running ~40 tok/s on CPU (open-source)</title>
    <updated>2026-02-23T14:03:12+00:00</updated>
    <author>
      <name>/u/zakerytclarke</name>
      <uri>https://old.reddit.com/user/zakerytclarke</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rci9h1/tinyteapot_77_million_params_contextgrounded_llm/"&gt; &lt;img alt="TinyTeapot (77 million params): Context-grounded LLM running ~40 tok/s on CPU (open-source)" src="https://external-preview.redd.it/JxyR2-HPTrb177zTD0smUzhI5l6xLW7EKVY2pYpkHxc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bf1debba24ef624d732718952f5f5127580705f6" title="TinyTeapot (77 million params): Context-grounded LLM running ~40 tok/s on CPU (open-source)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zakerytclarke"&gt; /u/zakerytclarke &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/teapotai/tinyteapot"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rci9h1/tinyteapot_77_million_params_contextgrounded_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rci9h1/tinyteapot_77_million_params_contextgrounded_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T14:03:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1rco9v7</id>
    <title>RWKV-7: O(1) memory inference, 16.39 tok/s on ARM Cortex-A76, beats LLaMA 3.2 3B. The local-first architecture nobody is talking about...</title>
    <updated>2026-02-23T17:45:31+00:00</updated>
    <author>
      <name>/u/Sensitive-Two9732</name>
      <uri>https://old.reddit.com/user/Sensitive-Two9732</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Wrote a deep-dive specifically because the deployment numbers don't get enough attention.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;FREE MEDIUM LINK&lt;/strong&gt;: &lt;a href="https://ai.gopubby.com/rwkv-7-beats-llama-3-2-rnn-constant-memory-46064bbf1f64?sk=c2e60e9b74b726d8697dbabc220cbbf4"&gt;https://ai.gopubby.com/rwkv-7-beats-llama-3-2-rnn-constant-memory-46064bbf1f64?sk=c2e60e9b74b726d8697dbabc220cbbf4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The headline stats for local inference:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;O(1) memory per token, no KV cache at all. Context length does not affect VRAM usage.&lt;/li&gt; &lt;li&gt;16.39 tok/s on ARM Cortex-A76 (7B model). That's a mid-range Android chip.&lt;/li&gt; &lt;li&gt;28.7 tok/s on Snapdragon X Elite (7B). Current-gen Windows on ARM.&lt;/li&gt; &lt;li&gt;RWKV-X hybrid: 1.37x faster than Flash Attention v3 at 128K context.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Microsoft already ships Eagle v5 (RWKV-based) on ~1.5 billion Windows machines for on-device tasks. No cloud round-trip.&lt;/p&gt; &lt;p&gt;The compression stack: 4-bit quantized RWKV-7 0.1B runs on microcontrollers. The state size is fixed regardless of how long the conversation runs. For local-first deployment this is a fundamentally different proposition than fitting a Transformer's growing KV cache into limited VRAM.&lt;/p&gt; &lt;p&gt;Weights (Apache 2.0): &lt;a href="https://huggingface.co/collections/RWKV/rwkv-v7"&gt;https://huggingface.co/collections/RWKV/rwkv-v7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to discuss about this. :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sensitive-Two9732"&gt; /u/Sensitive-Two9732 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://medium.com/ai-advances/rwkv-7-beats-llama-3-2-rnn-constant-memory-46064bbf1f64"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rco9v7/rwkv7_o1_memory_inference_1639_toks_on_arm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rco9v7/rwkv7_o1_memory_inference_1639_toks_on_arm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T17:45:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcu741</id>
    <title>Anthropic today</title>
    <updated>2026-02-23T21:16:10+00:00</updated>
    <author>
      <name>/u/PaceImaginary8610</name>
      <uri>https://old.reddit.com/user/PaceImaginary8610</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcu741/anthropic_today/"&gt; &lt;img alt="Anthropic today" src="https://preview.redd.it/mfd5i5tr8blg1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7fc1f40b60802aadfa7bac6e1db0e8792b2d1979" title="Anthropic today" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;While I generally do not agree with the misuse of others' property, this statement is ironic coming from Anthropic.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PaceImaginary8610"&gt; /u/PaceImaginary8610 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/mfd5i5tr8blg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcu741/anthropic_today/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rcu741/anthropic_today/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T21:16:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1rc6c8m</id>
    <title>Feels like magic. A local gpt-oss 20B is capable of agentic work</title>
    <updated>2026-02-23T03:18:16+00:00</updated>
    <author>
      <name>/u/Vaddieg</name>
      <uri>https://old.reddit.com/user/Vaddieg</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rc6c8m/feels_like_magic_a_local_gptoss_20b_is_capable_of/"&gt; &lt;img alt="Feels like magic. A local gpt-oss 20B is capable of agentic work" src="https://preview.redd.it/b27xdhewq5lg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f9692be692d82dd176bce38aa1cffe88af9406be" title="Feels like magic. A local gpt-oss 20B is capable of agentic work" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I gave a try to &lt;a href="https://github.com/zeroclaw-labs/zeroclaw"&gt;zeroclaw&lt;/a&gt; agent (intstead of the bloated and overhyped one). After few hours of fuckery with configs it's finally useful. Both main and embeddings models are running locally.&lt;br /&gt; I carefully read what it's trying to execute in shell, and permit only [relatively] safe tools in config.&lt;br /&gt; So far it can interact with macOS apps, web pages, and local files while keeping all my data private.&lt;br /&gt; gpt-oss 20B has its limits though, it loses focus after 15-20 steps and often needs direct instructions to use persistent memory. It also starts behaving weirdly if tool access has been denied or tool returned some error.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vaddieg"&gt; /u/Vaddieg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/b27xdhewq5lg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rc6c8m/feels_like_magic_a_local_gptoss_20b_is_capable_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rc6c8m/feels_like_magic_a_local_gptoss_20b_is_capable_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T03:18:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcs9vr</id>
    <title>Talking to my to-do list</title>
    <updated>2026-02-23T20:05:37+00:00</updated>
    <author>
      <name>/u/llo7d</name>
      <uri>https://old.reddit.com/user/llo7d</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcs9vr/talking_to_my_todo_list/"&gt; &lt;img alt="Talking to my to-do list" src="https://external-preview.redd.it/YnFzdm9lejd2YWxnMWY-tuy7HWwE5y0N4mja7xeEwkxeCiovLgSs8XbE5sB8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7139ad7399515809748a8bd26139c3d328ee50f5" title="Talking to my to-do list" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been testing feeding all my to-do list and productivity and having this kinda of desk robot thing as a screen to talk to? all the stuff happens on the pc, the screen is just a display and still for now it is a cloud based ai but I can definitely see this all happening locally in the future &lt;em&gt;(also better for privacy stuff)&lt;/em&gt; man the future is going to be awesome&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/llo7d"&gt; /u/llo7d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/xplqhdz7valg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcs9vr/talking_to_my_todo_list/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rcs9vr/talking_to_my_todo_list/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T20:05:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcc2fa</id>
    <title>An open-source framework to achieve Gemini 3 Deep Think / GPT-5.2 Pro level performance with local models scaffolding</title>
    <updated>2026-02-23T08:33:22+00:00</updated>
    <author>
      <name>/u/Ryoiki-Tokuiten</name>
      <uri>https://old.reddit.com/user/Ryoiki-Tokuiten</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcc2fa/an_opensource_framework_to_achieve_gemini_3_deep/"&gt; &lt;img alt="An open-source framework to achieve Gemini 3 Deep Think / GPT-5.2 Pro level performance with local models scaffolding" src="https://preview.redd.it/wfpxmbhlc7lg1.png?width=140&amp;amp;height=99&amp;amp;auto=webp&amp;amp;s=48d6dec8f7b1fd1a11e8a1b5afbd51040b6a5021" title="An open-source framework to achieve Gemini 3 Deep Think / GPT-5.2 Pro level performance with local models scaffolding" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ryoiki-Tokuiten"&gt; /u/Ryoiki-Tokuiten &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1rcc2fa"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcc2fa/an_opensource_framework_to_achieve_gemini_3_deep/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rcc2fa/an_opensource_framework_to_achieve_gemini_3_deep/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T08:33:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1rc59ze</id>
    <title>Qwen3's most underrated feature: Voice embeddings</title>
    <updated>2026-02-23T02:28:32+00:00</updated>
    <author>
      <name>/u/k_means_clusterfuck</name>
      <uri>https://old.reddit.com/user/k_means_clusterfuck</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rc59ze/qwen3s_most_underrated_feature_voice_embeddings/"&gt; &lt;img alt="Qwen3's most underrated feature: Voice embeddings" src="https://preview.redd.it/zmcs7iysm5lg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=796016e685c536fbab1ce49b5fec35afeb75f40e" title="Qwen3's most underrated feature: Voice embeddings" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Did you know that Qwen3 TTS utilizes voice embedding for voice cloning?&lt;br /&gt; Your voice is turned into a vector of 1024 dimensions (or 2048 for 1.7b), and based on this vector alone you can get your custom voice.&lt;/p&gt; &lt;p&gt;But the coolest part is that this means that you can use math to modify voices, average voices. You can swap gender, pitch, mix and match voices, and even create an emotion space! This also enables semantic voice search!&lt;/p&gt; &lt;p&gt;The voice embedding model is actually just a tiny encoder with just a few million parameters. I've ripped it out of the voice embedding model so you can use the embedding model standalone. Check out my collection! :D I also have onnx models for optimized web / front-end inference.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/marksverdhei/qwen3-voice-embedding"&gt;https://huggingface.co/collections/marksverdhei/qwen3-voice-embedding&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Voice embedings can be used for inference in my vllm-omni fork until it is supported in upstream: &lt;a href="https://github.com/heiervang-technologies/ht-vllm-omni"&gt;https://github.com/heiervang-technologies/ht-vllm-omni&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/k_means_clusterfuck"&gt; /u/k_means_clusterfuck &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zmcs7iysm5lg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rc59ze/qwen3s_most_underrated_feature_voice_embeddings/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rc59ze/qwen3s_most_underrated_feature_voice_embeddings/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T02:28:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcnv9h</id>
    <title>GLM-5 is the new top open-weights model on the Extended NYT Connections benchmark, with a score of 81.8, edging out Kimi K2.5 Thinking (78.3)</title>
    <updated>2026-02-23T17:31:02+00:00</updated>
    <author>
      <name>/u/zero0_one1</name>
      <uri>https://old.reddit.com/user/zero0_one1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcnv9h/glm5_is_the_new_top_openweights_model_on_the/"&gt; &lt;img alt="GLM-5 is the new top open-weights model on the Extended NYT Connections benchmark, with a score of 81.8, edging out Kimi K2.5 Thinking (78.3)" src="https://preview.redd.it/t89mf46o4alg1.png?width=140&amp;amp;height=89&amp;amp;auto=webp&amp;amp;s=6d3e242b7e37ab99694926c9cefb58fae2a90e45" title="GLM-5 is the new top open-weights model on the Extended NYT Connections benchmark, with a score of 81.8, edging out Kimi K2.5 Thinking (78.3)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;More info: &lt;a href="https://github.com/lechmazur/nyt-connections/"&gt;https://github.com/lechmazur/nyt-connections/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zero0_one1"&gt; /u/zero0_one1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1rcnv9h"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcnv9h/glm5_is_the_new_top_openweights_model_on_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rcnv9h/glm5_is_the_new_top_openweights_model_on_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T17:31:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1rclyvf</id>
    <title>Portable Workstation for Inference</title>
    <updated>2026-02-23T16:24:21+00:00</updated>
    <author>
      <name>/u/neintailedfoxx</name>
      <uri>https://old.reddit.com/user/neintailedfoxx</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rclyvf/portable_workstation_for_inference/"&gt; &lt;img alt="Portable Workstation for Inference" src="https://preview.redd.it/j59qyq8sq9lg1.jpg?width=140&amp;amp;height=64&amp;amp;auto=webp&amp;amp;s=f979ae081b50b775630080b53aa9f61d422aa5ed" title="Portable Workstation for Inference" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Built a new portable workstation for gaming/AI workloads. One of the fans is a 12018 fan bought from aliexpress derived from a fan on the 4090FE, allowing it to provide airflow equivalent to normal 25mm thick fans despite only being 18mm in thickness.&lt;/p&gt; &lt;p&gt;Would've loved to get a Threadripper for additional memory bandwidth, but sadly there aren't any itx Threadripper boards :(&lt;/p&gt; &lt;p&gt;Getting around 150-165 tok/sec running GPT OSS 120B with max context length in LM Studio (Using windows, haven't had time to test in linux yet)&lt;/p&gt; &lt;p&gt;CPU is undervolted using the curve optimizer (-25/-30 per CCD CO) with a +200MHz PBO clock offset, RAM is tuned to 6000MT/s CL28-36-35-30 @ 2233MHz FCLK, and the GPU is undervolted to 0.89v@2700MHz and power limited to 500w.&lt;/p&gt; &lt;p&gt;Temps are good, with the cpu reaching a max temp of around 75c and the GPU never going above 80c even during extremely heavy workloads. Top fans are set to intake, providing airflow to the flipped GPU.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Case:&lt;/strong&gt; FormD T1 2.5 Gunmetal w/ Flipped Travel Kit&lt;/p&gt; &lt;p&gt;&lt;strong&gt;CPU:&lt;/strong&gt; AMD Ryzen 9 9950X3D&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GPU:&lt;/strong&gt; NVIDIA RTX PRO 6000 Workstation Edition&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Motherboard:&lt;/strong&gt; MSI MPG X870I EDGE TI EVO WIFI&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Ram:&lt;/strong&gt; TEAMGROUP T-Force Delta RGB 96 GB DDR5-6800 CL36&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Storage:&lt;/strong&gt; Crucial T710 4TB, Samsung 990 Pro 4TB, WD Black SN850X 8TB, TEAMGROUP CX2 2TB (Used drives from my previous build since I definitely won't be able to afford all this storage at current prices)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;PSU:&lt;/strong&gt; Corsair SF1000&lt;/p&gt; &lt;p&gt;&lt;strong&gt;PSU Cables:&lt;/strong&gt; Custom Cables from Dreambigbyray&lt;/p&gt; &lt;p&gt;&lt;strong&gt;CPU Cooler:&lt;/strong&gt; CM Masterliquid 240 ATMOS Stealth&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/neintailedfoxx"&gt; /u/neintailedfoxx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1rclyvf"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rclyvf/portable_workstation_for_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rclyvf/portable_workstation_for_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T16:24:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcrb2k</id>
    <title>Hypocrisy?</title>
    <updated>2026-02-23T19:31:17+00:00</updated>
    <author>
      <name>/u/pmv143</name>
      <uri>https://old.reddit.com/user/pmv143</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcrb2k/hypocrisy/"&gt; &lt;img alt="Hypocrisy?" src="https://preview.redd.it/jxutlq8bqalg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=59d78bab536255787ed1f0bc277f2a7f6d5aea3b" title="Hypocrisy?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmv143"&gt; /u/pmv143 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jxutlq8bqalg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcrb2k/hypocrisy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rcrb2k/hypocrisy/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T19:31:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcr4ju</id>
    <title>Dario Is Scared</title>
    <updated>2026-02-23T19:24:52+00:00</updated>
    <author>
      <name>/u/Doris_Dressy1</name>
      <uri>https://old.reddit.com/user/Doris_Dressy1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcr4ju/dario_is_scared/"&gt; &lt;img alt="Dario Is Scared" src="https://preview.redd.it/kws4m2dtnalg1.png?width=140&amp;amp;height=65&amp;amp;auto=webp&amp;amp;s=03b607ae75f5f49a95e7b7bdff7b77e0c3749c2e" title="Dario Is Scared" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Why did Anthropic choose this exact moment to release that &lt;a href="https://x.com/AnthropicAI/status/2025997928242811253"&gt;statement&lt;/a&gt;? Because he‚Äôs scared. &lt;/p&gt; &lt;p&gt;Ever since OpenClaw launched, token usage from both individuals and model companies has been booming. And yet, on OpenRouter, the top-ranked models are no longer Claude but open-source models like Kimi K2.5 and Minimax M2.5. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/kws4m2dtnalg1.png?width=2076&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e7355e5685d4cafe68bbb0ad1f2deffa69f74a50"&gt;https://preview.redd.it/kws4m2dtnalg1.png?width=2076&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e7355e5685d4cafe68bbb0ad1f2deffa69f74a50&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Everyone can see that agents are the future. But Anthropic is losing market share in this area. &lt;/p&gt; &lt;p&gt;Dario keeps talking about AI safety, while on the other side his company runs &lt;a href="https://www.theverge.com/2024/7/25/24205943/anthropic-ai-web-crawler-claudebot-ifixit-scraping-training-data"&gt;crawlers&lt;/a&gt; that ignore robots.txt and overwhelm independent websites, trains on &lt;a href="https://www.theguardian.com/technology/2025/sep/05/anthropic-settlement-ai-book-lawsuit"&gt;copyrighted material&lt;/a&gt;, and keep trying to ban open-source models, scare people by comparing them to &lt;a href="https://www.axios.com/2026/01/20/anthropic-ceo-admodei-nvidia-chips-china-trump"&gt;nuclear weapons&lt;/a&gt;. His goal is so clear: monopolize the intelligence of the future, and with it, monopolize power. &lt;/p&gt; &lt;p&gt;Yet another Linus moment: fxxk you, Dario!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Doris_Dressy1"&gt; /u/Doris_Dressy1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcr4ju/dario_is_scared/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcr4ju/dario_is_scared/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rcr4ju/dario_is_scared/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T19:24:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcseh1</id>
    <title>Fun fact: Anthropic has never open-sourced any LLMs</title>
    <updated>2026-02-23T20:10:06+00:00</updated>
    <author>
      <name>/u/InternationalAsk1490</name>
      <uri>https://old.reddit.com/user/InternationalAsk1490</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been working on a little side project comparing tokenizer efficiency across different companies‚Äô models for multilingual encoding.&lt;/p&gt; &lt;p&gt;Then I saw Anthropic‚Äôs announcement today and suddenly realized: there‚Äôs no way to analyze claude‚Äôs tokenizer lmao!&lt;/p&gt; &lt;p&gt;edit: Google once mentioned in a paper that Gemma and Gemini share the same tokenizer. OpenAI has already open‚Äësourced their tokenizers (and gpt‚Äëoss). And don‚Äôt even get me started on Llama (Llama 5 pls üò≠). &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InternationalAsk1490"&gt; /u/InternationalAsk1490 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcseh1/fun_fact_anthropic_has_never_opensourced_any_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcseh1/fun_fact_anthropic_has_never_opensourced_any_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rcseh1/fun_fact_anthropic_has_never_opensourced_any_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T20:10:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1rctg3y</id>
    <title>we can't upvote Elon Musk, this is reddit :)</title>
    <updated>2026-02-23T20:47:05+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rctg3y/we_cant_upvote_elon_musk_this_is_reddit/"&gt; &lt;img alt="we can't upvote Elon Musk, this is reddit :)" src="https://preview.redd.it/4sskgcvr3blg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=330c907588a9c7425017a748a1a64c47565259b7" title="we can't upvote Elon Musk, this is reddit :)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4sskgcvr3blg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rctg3y/we_cant_upvote_elon_musk_this_is_reddit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rctg3y/we_cant_upvote_elon_musk_this_is_reddit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T20:47:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcmlwk</id>
    <title>so is OpenClaw local or not</title>
    <updated>2026-02-23T16:47:01+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcmlwk/so_is_openclaw_local_or_not/"&gt; &lt;img alt="so is OpenClaw local or not" src="https://preview.redd.it/5rolok0mw9lg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d0bdebee8fd3b3c91999b3592892a73daf47142e" title="so is OpenClaw local or not" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Reading the comments, I‚Äôm guessing you didn‚Äôt bother to read this:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&amp;quot;Safety and alignment at Meta Superintelligence.&amp;quot;&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5rolok0mw9lg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcmlwk/so_is_openclaw_local_or_not/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rcmlwk/so_is_openclaw_local_or_not/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T16:47:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcpmwn</id>
    <title>Anthropic: "We‚Äôve identified industrial-scale distillation attacks on our models by DeepSeek, Moonshot AI, and MiniMax." üö®</title>
    <updated>2026-02-23T18:32:45+00:00</updated>
    <author>
      <name>/u/KvAk_AKPlaysYT</name>
      <uri>https://old.reddit.com/user/KvAk_AKPlaysYT</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcpmwn/anthropic_weve_identified_industrialscale/"&gt; &lt;img alt="Anthropic: &amp;quot;We‚Äôve identified industrial-scale distillation attacks on our models by DeepSeek, Moonshot AI, and MiniMax.&amp;quot; üö®" src="https://preview.redd.it/94fbimavfalg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c2ad159232448ffd7033d6be4fa96582b674e461" title="Anthropic: &amp;quot;We‚Äôve identified industrial-scale distillation attacks on our models by DeepSeek, Moonshot AI, and MiniMax.&amp;quot; üö®" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KvAk_AKPlaysYT"&gt; /u/KvAk_AKPlaysYT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/94fbimavfalg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcpmwn/anthropic_weve_identified_industrialscale/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rcpmwn/anthropic_weve_identified_industrialscale/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T18:32:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7bsfd</id>
    <title>Best Audio Models - Feb 2026</title>
    <updated>2026-02-17T17:01:37+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;They've been a ton of audio models released of late, the most notable perhaps being Qwen3 TTS. So its time for another &lt;strong&gt;Best Audio Models&lt;/strong&gt; megathread&lt;/p&gt; &lt;p&gt;Share what your favorite ASR, TTS, STT, Text to Music models are right now &lt;strong&gt;and why.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Given the the amount of ambiguity and subjectivity in rating/testing these models, please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks etc. Closed models like Elevenlabs v3 seem to continue to be a few levels above open models especially for production use cases with long lengths/stability requirements, so comparisons, especially empirical ones are welcome.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please use the top level comments to thread your responses.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T17:01:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8snay</id>
    <title>AMA with StepFun AI - Ask Us Anything</title>
    <updated>2026-02-19T07:15:35+00:00</updated>
    <author>
      <name>/u/StepFun_ai</name>
      <uri>https://old.reddit.com/user/StepFun_ai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt; &lt;img alt="AMA with StepFun AI - Ask Us Anything" src="https://external-preview.redd.it/bqLigXK_A-XO-amDfdyeQenKCxR18nhChWpDnQXYmr4.png?width=140&amp;amp;height=75&amp;amp;auto=webp&amp;amp;s=72c79bc533c167d8f77ecf629e3b7af53ee1e6b6" title="AMA with StepFun AI - Ask Us Anything" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/w8274fg1jekg1.png?width=1785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fadbd0ec26a56e60900f9ed667ae808217d70cf2"&gt;https://preview.redd.it/w8274fg1jekg1.png?width=1785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fadbd0ec26a56e60900f9ed667ae808217d70cf2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; !&lt;/p&gt; &lt;p&gt;We are &lt;strong&gt;StepFun&lt;/strong&gt;, the team behind the &lt;strong&gt;Step&lt;/strong&gt; family models, including &lt;a href="https://huggingface.co/collections/stepfun-ai/step-35-flash"&gt;&lt;strong&gt;Step 3.5 Flash&lt;/strong&gt;&lt;/a&gt; and &lt;a href="https://huggingface.co/collections/stepfun-ai/step3-vl-10b"&gt;&lt;strong&gt;Step-3-VL-10B&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;We are super excited to host our first AMA tomorrow in this community. Our participants include CEO, CTO, Chief Scientist, LLM Researchers.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Participants&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Ok_Reach_5122"&gt;u/Ok_Reach_5122&lt;/a&gt; (Co-founder &amp;amp; CEO of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/bobzhuyb"&gt;u/bobzhuyb&lt;/a&gt; (Co-founder &amp;amp; CTO of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/user/Lost-Nectarine1016"&gt;u/Lost-Nectarine1016&lt;/a&gt; (Co-founder &amp;amp; Chief Scientist of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Elegant-Sale-1328"&gt;u/Elegant-Sale-1328&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/SavingsConclusion298"&gt;u/SavingsConclusion298&lt;/a&gt; (Post-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Spirited_Spirit3387"&gt;u/Spirited_Spirit3387&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/These-Nothing-8564/"&gt;u/These-Nothing-8564&lt;/a&gt; (Technical Project Manager)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Either-Beyond-7395"&gt;u/Either-Beyond-7395&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Human_Ad_162"&gt;u/Human_Ad_162&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Icy_Dare_3866"&gt;u/Icy_Dare_3866&lt;/a&gt; (Post-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Big-Employee5595"&gt;u/Big-Employee5595&lt;/a&gt; (Agent Algorithms Lead&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run 8 - 11 AM PST, Feburary 19th. The StepFun team will monitor and answer questions over the 24 hours after the live session.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StepFun_ai"&gt; /u/StepFun_ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T07:15:35+00:00</published>
  </entry>
</feed>
