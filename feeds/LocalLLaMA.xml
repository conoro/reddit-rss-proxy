<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-20T04:59:17+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qhk5j9</id>
    <title>nvfp4 on Blackwell: sglang, vllm, trt</title>
    <updated>2026-01-19T23:16:05+00:00</updated>
    <author>
      <name>/u/ARCHLucifer</name>
      <uri>https://old.reddit.com/user/ARCHLucifer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;why architecture of kernels from hardware developer and end users differs slightly ?&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/advpropx/status/2013383198466556394?s=46"&gt;https://x.com/advpropx/status/2013383198466556394?s=46&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ARCHLucifer"&gt; /u/ARCHLucifer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhk5j9/nvfp4_on_blackwell_sglang_vllm_trt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhk5j9/nvfp4_on_blackwell_sglang_vllm_trt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhk5j9/nvfp4_on_blackwell_sglang_vllm_trt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T23:16:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1qgx83t</id>
    <title>3x3090 + 3060 in a mid tower case</title>
    <updated>2026-01-19T06:59:39+00:00</updated>
    <author>
      <name>/u/liviuberechet</name>
      <uri>https://old.reddit.com/user/liviuberechet</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgx83t/3x3090_3060_in_a_mid_tower_case/"&gt; &lt;img alt="3x3090 + 3060 in a mid tower case" src="https://b.thumbs.redditmedia.com/isvewN3PNijf6_OZxoF82ROhZEAD0nfG7ddsr3ghkYU.jpg" title="3x3090 + 3060 in a mid tower case" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Decided to go all out and max out this desktop. I was lucky to find 3090 cards for around 600 usd, over a period of 3 months and decided to go for it. &lt;/p&gt; &lt;p&gt;The RAM was a bit more expensive, but I had 64 bought before the price spiked.&lt;/p&gt; &lt;p&gt;I didn’t want to change the case, because I through it’s a high quality case and it would be a shame to toss it. So made the most out of it!&lt;/p&gt; &lt;p&gt;Specs: * Fractal Define 7 Mid Tower * 3x3090 + 1x3060 (86gb total, but 72gb VRAM main) * 128GB DDR4 (Corsair 4x32) * Corsair HX1500i 1500w (has 7 PCIe power cables) * Vertical mounts are all cheap from AliExpress * ASUS Maximus XII Hero — has only 3x PCIe16x, had to deactivate the 2nd NVMe to use the 3rd PCIe16x in 4x, the 4th GPU (the 3060) is on a riser from a PCIe1x. * For drives, only one NVMe of 1TB works, I also bought 2x2TB SSDs that I tried in RAID but the performance was terrible (and they are limited to 500mb from the SATA interface, which I didn’t know…) so I keep them as 2 drives.&lt;/p&gt; &lt;p&gt;Temperatures are holding surprisingly well. The gap between the cards is about the size of an empty PCIe slot, maybe a bit more. &lt;/p&gt; &lt;p&gt;Temperature was a big improvement compared to having just 2x3090 stacked without any space between them — the way the motherboard is designed to use them.&lt;/p&gt; &lt;p&gt;In terms of performance 3x3090 is great! There are great options in the 60-65gb range with the extra space to 72gb VRAM used for context.&lt;/p&gt; &lt;p&gt;I am not using the RAM for anything other than to load models, and the speed is amazing when everything is loaded in VRAM! &lt;/p&gt; &lt;p&gt;Models I started using a lot: * gpt-oss-120b in MXFP4 with 60k context * glm-4.5-air in IQ4_NL with 46k context * qwen3-vl-235b in TQ1_0 (surprisingly good!) * minimax-M2-REAP-139B in Q3_K_S with 40k context&lt;/p&gt; &lt;p&gt;But still return a lot to old models for context and speed: * devstral-small-2-24 in Q8_0 with 200k context * qwen3-coder in Q8 with 1M (!!) context (using RAM) * qwen3-next-80b in Q6_K with 60k context — still my favourite for general chat, and the Q6 makes me trust it more than Q3-Q4 models&lt;/p&gt; &lt;p&gt;The 3060 on the riser from PCIe1x is very slow at loading the models, however, once it’s loaded it works great! I am using it for image generation and TTS audio generation mostly (for Open WebUI).&lt;/p&gt; &lt;p&gt;Also did a lot of testing on using 2x3090 via normal PCIe, with a 3rd card via riser — it works same as normal PCIe! But the loading takes forever (sometimes over 2-3 minutes) and you simply can’t use the RAM for context because of how slow it is — so I am considering the current setup to be “maxed out” because I don’t think adding a 4th 3090 will be useful. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/liviuberechet"&gt; /u/liviuberechet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qgx83t"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgx83t/3x3090_3060_in_a_mid_tower_case/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qgx83t/3x3090_3060_in_a_mid_tower_case/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T06:59:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhnkuy</id>
    <title>Any suggestions for models in the 7-14B range for Linux admin, cybersecurity, etc?</title>
    <updated>2026-01-20T01:40:57+00:00</updated>
    <author>
      <name>/u/xenronex</name>
      <uri>https://old.reddit.com/user/xenronex</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking for a model that excels at sysadmin related tasks, specifically for Linux.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenronex"&gt; /u/xenronex &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhnkuy/any_suggestions_for_models_in_the_714b_range_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhnkuy/any_suggestions_for_models_in_the_714b_range_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhnkuy/any_suggestions_for_models_in_the_714b_range_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T01:40:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhnl7d</id>
    <title>Project HYDRA- A local LLM distributed computing project</title>
    <updated>2026-01-20T01:41:22+00:00</updated>
    <author>
      <name>/u/Fear_ltself</name>
      <uri>https://old.reddit.com/user/Fear_ltself</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhnl7d/project_hydra_a_local_llm_distributed_computing/"&gt; &lt;img alt="Project HYDRA- A local LLM distributed computing project" src="https://external-preview.redd.it/bzN4eGpsMjhzZWVnMSGdfUtWVuAwoPkxxwKeUbXwwzyN2ch9VQo4HJqp54qC.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=34410c5d6e5a948a348cef44a18299fb48851053" title="Project HYDRA- A local LLM distributed computing project" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I have an 18Gb MacBook Pro that’s great at Whisper (MLX, unified memory, blazing fast CPU) , but it isn’t as fast at image generation like my Asus Zephyrus with NVIDIA RTX 4070. I discovered BOINC a couple months ago and it sparked my interest in the idea of distributed computing, and recently I began running into issues running the best model available with the image generation since each takes up too much RAM. So my solution was to split the workload, instead of my previous version sending image creation requests to a self hosted server, it finds a server on the local network hosted by Asus to the local network (WiFi). Larger models in each device, running what they’re best at… &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fear_ltself"&gt; /u/Fear_ltself &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/cgrcqh78seeg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhnl7d/project_hydra_a_local_llm_distributed_computing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhnl7d/project_hydra_a_local_llm_distributed_computing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T01:41:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1qgwup8</id>
    <title>Is Local Coding even worth setting up</title>
    <updated>2026-01-19T06:38:37+00:00</updated>
    <author>
      <name>/u/Interesting-Fish6494</name>
      <uri>https://old.reddit.com/user/Interesting-Fish6494</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi I am new to Local LLM but have been having a lot of issues setting up a local LLM coding environment so wanted some suggestions from people.I have a 5070 ti (16gb vram).&lt;/p&gt; &lt;p&gt;I have tried to use Kilo code with qwen 2.5 coder 7B running through ollama but the context size feels so low that it finishes the context within a single file of my project.&lt;/p&gt; &lt;p&gt;How are other people with a 16gb GPU dealing with local llm?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Interesting-Fish6494"&gt; /u/Interesting-Fish6494 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgwup8/is_local_coding_even_worth_setting_up/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgwup8/is_local_coding_even_worth_setting_up/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qgwup8/is_local_coding_even_worth_setting_up/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T06:38:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhpna6</id>
    <title>What Local Models work well with Claude Code?</title>
    <updated>2026-01-20T03:13:31+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The ~20k system prompt seems to overwhelm my usual agentic go-to's (Qwen3-Next-80B and Gpt-OSS-120B) on relatively simple tasks.&lt;/p&gt; &lt;p&gt;GLM 4.6v works okay but is too slow and can enter far too long, sometimes infinite reasoning loops.&lt;/p&gt; &lt;p&gt;Qwen3-235B-2507 works well but is too slow on my machines.&lt;/p&gt; &lt;p&gt;Any suggestions? 48GB VRAM and 64GB system memory&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhpna6/what_local_models_work_well_with_claude_code/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhpna6/what_local_models_work_well_with_claude_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhpna6/what_local_models_work_well_with_claude_code/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T03:13:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhqrl7</id>
    <title>DeepSeek V3.2 (open weights) beats GPT-5.2-Codex and Claude Opus on production code challenge — The Multivac daily blind peer eval</title>
    <updated>2026-01-20T04:05:23+00:00</updated>
    <author>
      <name>/u/Silver_Raspberry_811</name>
      <uri>https://old.reddit.com/user/Silver_Raspberry_811</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; DeepSeek V3.2 scored 9.39 to beat GPT-5.2-Codex (9.20) and every other closed model on a complex coding task. But the real story is Claude Sonnet 4.5 got scored anywhere from 3.95 to 8.80 by different judges — same exact code.&lt;/p&gt; &lt;h1&gt;The Test&lt;/h1&gt; &lt;p&gt;We asked 10 models to write a production-grade nested JSON parser with:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Path syntax (&amp;quot;user.profile.settings.theme&amp;quot;)&lt;/li&gt; &lt;li&gt;Array indexing (&amp;quot;users[0].name&amp;quot;)&lt;/li&gt; &lt;li&gt;Circular reference detection&lt;/li&gt; &lt;li&gt;Typed results with error messages&lt;/li&gt; &lt;li&gt;Full type hints and docstrings&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This is a real-world task. Every backend engineer has written something like this.&lt;/p&gt; &lt;h1&gt;Results&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Rank&lt;/th&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Score&lt;/th&gt; &lt;th align="left"&gt;Std Dev&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;DeepSeek V3.2&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;9.39&lt;/td&gt; &lt;td align="left"&gt;0.80&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;GPT-5.2-Codex&lt;/td&gt; &lt;td align="left"&gt;9.20&lt;/td&gt; &lt;td align="left"&gt;0.50&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;3&lt;/td&gt; &lt;td align="left"&gt;Grok 3&lt;/td&gt; &lt;td align="left"&gt;8.89&lt;/td&gt; &lt;td align="left"&gt;0.76&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;4&lt;/td&gt; &lt;td align="left"&gt;Grok Code Fast 1&lt;/td&gt; &lt;td align="left"&gt;8.46&lt;/td&gt; &lt;td align="left"&gt;1.10&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;5&lt;/td&gt; &lt;td align="left"&gt;Gemini 3 Flash&lt;/td&gt; &lt;td align="left"&gt;8.16&lt;/td&gt; &lt;td align="left"&gt;0.71&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;6&lt;/td&gt; &lt;td align="left"&gt;Claude Opus 4.5&lt;/td&gt; &lt;td align="left"&gt;7.57&lt;/td&gt; &lt;td align="left"&gt;1.56&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;7&lt;/td&gt; &lt;td align="left"&gt;Claude Sonnet 4.5&lt;/td&gt; &lt;td align="left"&gt;7.02&lt;/td&gt; &lt;td align="left"&gt;2.03&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;8&lt;/td&gt; &lt;td align="left"&gt;Gemini 3 Pro&lt;/td&gt; &lt;td align="left"&gt;4.30&lt;/td&gt; &lt;td align="left"&gt;1.38&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;9&lt;/td&gt; &lt;td align="left"&gt;GLM 4.7&lt;/td&gt; &lt;td align="left"&gt;2.91&lt;/td&gt; &lt;td align="left"&gt;3.61&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;10&lt;/td&gt; &lt;td align="left"&gt;MiniMax M2.1&lt;/td&gt; &lt;td align="left"&gt;0.70&lt;/td&gt; &lt;td align="left"&gt;0.28&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Open weights won.&lt;/strong&gt; DeepSeek V3.2 is fully open.&lt;/p&gt; &lt;h1&gt;The Variance Problem (responding to yesterday's feedback)&lt;/h1&gt; &lt;p&gt;Yesterday &lt;a href="/u/Proud-Claim-485"&gt;u/Proud-Claim-485&lt;/a&gt; critiqued our methodology — said we're measuring &amp;quot;output alignment&amp;quot; not &amp;quot;reasoning alignment.&amp;quot;&lt;/p&gt; &lt;p&gt;Today's data supports this. Look at Claude Sonnet's std dev: &lt;strong&gt;2.03&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;That's a 5-point spread (3.95 to 8.80) on the same response. Judges fundamentally disagreed on what &amp;quot;good&amp;quot; means.&lt;/p&gt; &lt;p&gt;Compare to GPT-5.2-Codex with 0.50 std dev — everyone agreed within ~1 point.&lt;/p&gt; &lt;p&gt;When evaluators disagree this much, the benchmark is under-specified.&lt;/p&gt; &lt;h1&gt;Judge Strictness (meta-analysis)&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Judge&lt;/th&gt; &lt;th align="left"&gt;Avg Score Given&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Claude Opus 4.5&lt;/td&gt; &lt;td align="left"&gt;5.92 (strictest)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Claude Sonnet 4.5&lt;/td&gt; &lt;td align="left"&gt;5.94&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPT-5.2-Codex&lt;/td&gt; &lt;td align="left"&gt;6.07&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;DeepSeek V3.2&lt;/td&gt; &lt;td align="left"&gt;7.88&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Gemini 3 Flash&lt;/td&gt; &lt;td align="left"&gt;9.11 (most lenient)&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Claude models judge harshly but score mid-tier themselves. Interesting pattern.&lt;/p&gt; &lt;h1&gt;What We're Adding (based on your feedback)&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;5 open-weight models for tomorrow:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Llama-3.3-70B-Instruct&lt;/li&gt; &lt;li&gt;Qwen2.5-72B-Instruct&lt;/li&gt; &lt;li&gt;Mistral-Large-2411&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Big-Tiger-Gemma-27B-v3&lt;/strong&gt; (&lt;a href="/u/ttkciar"&gt;u/ttkciar&lt;/a&gt; suggested this — anti-sycophancy finetune)&lt;/li&gt; &lt;li&gt;Phi-4&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;New evaluation dimension:&lt;/strong&gt; We're adding &amp;quot;reasoning justification&amp;quot; scoring — did the model explain its approach, not just produce correct-looking output?&lt;/p&gt; &lt;h1&gt;Methodology&lt;/h1&gt; &lt;p&gt;This is The Multivac — daily 10×10 blind peer matrix:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;10 models respond to same question&lt;/li&gt; &lt;li&gt;Each model judges all 10 responses (100 total judgments)&lt;/li&gt; &lt;li&gt;Models don't know which response came from which model&lt;/li&gt; &lt;li&gt;Rankings from peer consensus, not single evaluator&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Full responses and analysis: &lt;a href="https://open.substack.com/pub/themultivac/p/deepseek-v32-wins-the-json-parsing?r=72olj0&amp;amp;utm_campaign=post&amp;amp;utm_medium=web&amp;amp;showWelcomeOnShare=true"&gt;https://open.substack.com/pub/themultivac/p/deepseek-v32-wins-the-json-parsing?r=72olj0&amp;amp;utm_campaign=post&amp;amp;utm_medium=web&amp;amp;showWelcomeOnShare=true&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="http://themultivac.com"&gt;themultivac.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Questions welcome. Roast the methodology. That's how we improve.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Silver_Raspberry_811"&gt; /u/Silver_Raspberry_811 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhqrl7/deepseek_v32_open_weights_beats_gpt52codex_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhqrl7/deepseek_v32_open_weights_beats_gpt52codex_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhqrl7/deepseek_v32_open_weights_beats_gpt52codex_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T04:05:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhrdia</id>
    <title>Last Week in Multimodal AI - Local Edition</title>
    <updated>2026-01-20T04:34:33+00:00</updated>
    <author>
      <name>/u/Vast_Yak_4147</name>
      <uri>https://old.reddit.com/user/Vast_Yak_4147</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhrdia/last_week_in_multimodal_ai_local_edition/"&gt; &lt;img alt="Last Week in Multimodal AI - Local Edition" src="https://b.thumbs.redditmedia.com/FluQr8-T2xSzepE7O56z1wr9rHYSGXZbAvZ94jKYf2w.jpg" title="Last Week in Multimodal AI - Local Edition" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I curate a weekly multimodal AI roundup, here are the local/open-source highlights from last week:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;FLUX.2 [klein] - Consumer GPU Image Generation&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Runs on consumer GPUs (13GB VRAM), generates high-quality images in under a second.&lt;/li&gt; &lt;li&gt;Handles text-to-image, editing, and multi-reference generation in one model.&lt;/li&gt; &lt;li&gt;&lt;a href="https://bfl.ai/blog/flux2-klein-towards-interactive-visual-intelligence"&gt;Blog&lt;/a&gt; | &lt;a href="https://bfl.ai/models/flux-2-klein#try-demo"&gt;Demo&lt;/a&gt; | &lt;a href="https://huggingface.co/collections/black-forest-labs/flux2"&gt;Models&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://i.redd.it/7vq4pfm0nfeg1.gif"&gt;https://i.redd.it/7vq4pfm0nfeg1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Pocket TTS - Lightweight Text-to-Speech&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Lightweight, CPU-friendly open text-to-speech application.&lt;/li&gt; &lt;li&gt;Local speech synthesis without proprietary services.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/kyutai/pocket-tts"&gt;Hugging Face&lt;/a&gt; | &lt;a href="https://kyutai.org/tts"&gt;Demo&lt;/a&gt; | &lt;a href="https://github.com/kyutai-labs/pocket-tts"&gt;GitHub Repository&lt;/a&gt; | &lt;a href="https://huggingface.co/kyutai/pocket-tts"&gt;Hugging Face Model Card&lt;/a&gt; | &lt;a href="https://arxiv.org/abs/2509.06926"&gt;Paper&lt;/a&gt; | &lt;a href="https://github.com/kyutai-labs/pocket-tts/tree/main/docs"&gt;Documentation&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Ministral 3 - Edge-Ready Multimodal Models&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Compact open models (3B, 8B, 14B) with image understanding for edge devices.&lt;/li&gt; &lt;li&gt;Run multimodal tasks locally without cloud dependencies.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/collections/mistralai/ministral-3"&gt;Hugging Face&lt;/a&gt; | &lt;a href="https://arxiv.org/abs/2601.08584"&gt;Paper&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5fwsc0zymfeg1.png?width=996&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6e5bfafefd5d98665badb3f9eac21886386bf65e"&gt;https://preview.redd.it/5fwsc0zymfeg1.png?width=996&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6e5bfafefd5d98665badb3f9eac21886386bf65e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;STEP3-VL-10B - Efficient Multimodal Intelligence&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;10B parameter model with frontier-level visual perception and reasoning.&lt;/li&gt; &lt;li&gt;Proves you don't need massive models for high-level multimodal intelligence.&lt;/li&gt; &lt;li&gt;h&lt;a href="https://huggingface.co/stepfun-ai/Step3-VL-10B"&gt;ugging Face&lt;/a&gt; | &lt;a href="https://arxiv.org/abs/2601.09668"&gt;Paper&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/uk3qg0z3nfeg1.png?width=1456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=670e4e3902a6a1609db3b135be4801769493ae27"&gt;https://preview.redd.it/uk3qg0z3nfeg1.png?width=1456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=670e4e3902a6a1609db3b135be4801769493ae27&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TranslateGemma - Open Translation Models&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Google's open translation models (4B, 12B, 27B) supporting 55 languages.&lt;/li&gt; &lt;li&gt;Fully open multilingual translation models.&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/GoogleDeepMind/status/2011848249850630363?s=20"&gt;Announcement&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;FASHN Human Parser - Fashion Image Segmentation&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Open fine-tuned SegFormer for parsing humans in fashion images.&lt;/li&gt; &lt;li&gt;Specialized open model for fashion applications.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/fashn-ai/fashn-human-parser"&gt;Hugging Face&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/przknaqrmfeg1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ef36c3976c5e63bd33a68936986ee3f923a8a055"&gt;https://preview.redd.it/przknaqrmfeg1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ef36c3976c5e63bd33a68936986ee3f923a8a055&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;DeepSeek Engram - Memory Module for LLMs&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Lookup-based memory module for faster knowledge retrieval.&lt;/li&gt; &lt;li&gt;Improves efficiency of local LLM deployments.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/deepseek-ai/Engram/tree/main"&gt;GitHub&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;ShowUI-Aloha - GUI Automation Agent&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Flow-based model that learns to use GUIs from human demonstrations.&lt;/li&gt; &lt;li&gt;Generates smooth mouse movements and clicks for workflow automation.&lt;/li&gt; &lt;li&gt;&lt;a href="https://showlab.github.io/Aloha_Page/"&gt;Project Page&lt;/a&gt; | &lt;a href="https://github.com/showlab/ShowUI-Aloha"&gt;GitHub&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1qhrdia/video/ewq89rktmfeg1/player"&gt;https://reddit.com/link/1qhrdia/video/ewq89rktmfeg1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Real-Qwen-Image-V2 - Peak Realism Image Model&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Community fine-tuned Qwen-Image model built for photorealism.&lt;/li&gt; &lt;li&gt;Open alternative for realistic image generation.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/wikeeyang/Real-Qwen-Image-V2"&gt;Model&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fty6rpiumfeg1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ad94c0cd39fe6a97c018bbe3f31f0ec6717ee830"&gt;https://preview.redd.it/fty6rpiumfeg1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ad94c0cd39fe6a97c018bbe3f31f0ec6717ee830&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Checkout the &lt;a href="https://open.substack.com/pub/thelivingedge/p/last-week-in-multimodal-ai-41-vision?utm_campaign=post-expanded-share&amp;amp;utm_medium=web"&gt;full roundup&lt;/a&gt; for more demos, papers, and resources.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/submit/?source_id=t3_1qbala2"&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vast_Yak_4147"&gt; /u/Vast_Yak_4147 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhrdia/last_week_in_multimodal_ai_local_edition/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhrdia/last_week_in_multimodal_ai_local_edition/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhrdia/last_week_in_multimodal_ai_local_edition/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T04:34:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhinqq</id>
    <title>Best MoE models for 64gb RAM &amp; CPU inference?</title>
    <updated>2026-01-19T22:18:01+00:00</updated>
    <author>
      <name>/u/GamerFromGamerTown</name>
      <uri>https://old.reddit.com/user/GamerFromGamerTown</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello! I've been looking around around for good ~A3B models that can run well on my hardware, but this space seems to be pretty saturated with options; among these, &lt;a href="https://huggingface.co/zai-org/GLM-4.7-Flash"&gt;GLM-4.7-Flash&lt;/a&gt;, &lt;a href="https://huggingface.co/nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16"&gt;NVIDIA-Nemotron-3-Nano-30B-A3B&lt;/a&gt;, &lt;a href="https://huggingface.co/openai/gpt-oss-20b"&gt;gpt-oss-20b&lt;/a&gt;, &lt;a href="https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct"&gt;Qwen3-Coder-30B-A3B-Instruct&lt;/a&gt; , &lt;a href="https://huggingface.co/Qwen/Qwen3-30B-A3B"&gt;Qwen3-30B-A3B&lt;/a&gt;, and &lt;a href="https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Instruct"&gt;Qwen3-Next-80B-A3B-Instruct&lt;/a&gt; seem to be the most popular choices, though I might be missing one or two! With them not really sharing many benchmarks, it can be a bit difficult to compare them; Nemotron-A3B and gpt-oss 20b seem to be pretty popular with the people around here, but GLM-4.7 flash just released, which people seem to feel pretty positively about.&lt;/p&gt; &lt;p&gt;I'll just be doing some coding help, math, and maybe some online/offline RAG. If you have other use cases though, feel free to share!&lt;/p&gt; &lt;p&gt;Given my mediocre Alaskan internet, it would be impossible to download them all to try them out, so anyone with experience trying some of these would be greatly appreciated. Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GamerFromGamerTown"&gt; /u/GamerFromGamerTown &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhinqq/best_moe_models_for_64gb_ram_cpu_inference/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhinqq/best_moe_models_for_64gb_ram_cpu_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhinqq/best_moe_models_for_64gb_ram_cpu_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T22:18:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhozrq</id>
    <title>A couple quick tests of GLM 4.7 flash on NVIDIA GB10 (Spark)</title>
    <updated>2026-01-20T02:44:09+00:00</updated>
    <author>
      <name>/u/Comrade-Porcupine</name>
      <uri>https://old.reddit.com/user/Comrade-Porcupine</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;On my ASUS GB10 (like NVIDIA Spark) with Q8_0 quantization, prompt to write a fibonacci in Scala:&lt;/p&gt; &lt;p&gt;HEAD of ollama with Q8_0 vs vLLM with BF16 and FP8 after.&lt;/p&gt; &lt;p&gt;BF16 predictably bad. Surprised FP8 performed so poorly, but I might not have things tuned that well. New at this. Any tips on how best to run these types of models on the Spark type machines?&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Backend&lt;/th&gt; &lt;th align="left"&gt;Quantization&lt;/th&gt; &lt;th align="left"&gt;Memory&lt;/th&gt; &lt;th align="left"&gt;Tokens/sec&lt;/th&gt; &lt;th align="left"&gt;Notes&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;vLLM&lt;/td&gt; &lt;td align="left"&gt;BF16&lt;/td&gt; &lt;td align="left"&gt;~62GB weights, ~102GB total&lt;/td&gt; &lt;td align="left"&gt;13-17&lt;/td&gt; &lt;td align="left"&gt;Bandwidth-bound&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;vLLM&lt;/td&gt; &lt;td align="left"&gt;FP8&lt;/td&gt; &lt;td align="left"&gt;~28GB weights&lt;/td&gt; &lt;td align="left"&gt;11-19&lt;/td&gt; &lt;td align="left"&gt;DeepGEMM disabled, Triton fallback&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Ollama&lt;/td&gt; &lt;td align="left"&gt;Q8_0&lt;/td&gt; &lt;td align="left"&gt;~32GB&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;32&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Best performance&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Most importantly, it actually worked nice in opencode, which I couldn't get Nemotron to do.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Comrade-Porcupine"&gt; /u/Comrade-Porcupine &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhozrq/a_couple_quick_tests_of_glm_47_flash_on_nvidia/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhozrq/a_couple_quick_tests_of_glm_47_flash_on_nvidia/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhozrq/a_couple_quick_tests_of_glm_47_flash_on_nvidia/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T02:44:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1qh442y</id>
    <title>Models that run in 72GB VRAM with context loaded in GPU (3x3090 benchmark test)</title>
    <updated>2026-01-19T13:27:32+00:00</updated>
    <author>
      <name>/u/liviuberechet</name>
      <uri>https://old.reddit.com/user/liviuberechet</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh442y/models_that_run_in_72gb_vram_with_context_loaded/"&gt; &lt;img alt="Models that run in 72GB VRAM with context loaded in GPU (3x3090 benchmark test)" src="https://preview.redd.it/85cs39k6daeg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=72f0ad403efa3ee18868a0b8bf289eb713cca04a" title="Models that run in 72GB VRAM with context loaded in GPU (3x3090 benchmark test)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently finished my 3x3090 setup, and thought of sharing my experience.&lt;/p&gt; &lt;p&gt;This is very much a personal observation, with some very basic testing. &lt;/p&gt; &lt;p&gt;The benchmark is by no means precise, however, after checking the numbers, it is very much aligned with &amp;quot;how I feels they perform&amp;quot; after a few days of bouncing between them. All the above are running on CUDA 12 llama.cpp via LM Studio (nothing special). &lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. Large models (&amp;gt; 100 B)&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;All big models run in roughly the same ballpark—about &lt;strong&gt;30 tok/s&lt;/strong&gt; in everyday use. GPT‑OSS‑120 runs a bit faster than the other large models, but the difference is only noticeable on very short answers; you wouldn’t notice it during longer conversations. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. Qwen3‑VL 235 B (TQ1, 1.66‑bit compression)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I was surprised by how usable TQ1_0 turned out to be. In most chat or image‑analysis scenarios it actually feels better than the Qwen3‑VL 30 B model quantised to Q8. I can’t fully explain why, but it seems to anticipate what I’m interested in much more accurately than the 30 B version.&lt;/p&gt; &lt;p&gt;It does show the expected weaknesses of a Q1‑type quantisation. For example, when reading a PDF it misreported some numbers that the Qwen3‑VL 30 B Q8 model got right; nevertheless, the surrounding information was correct despite the typo.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3. The biggest and best models you can run in Q3–Q4 with a decent context window:&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;&lt;strong&gt;(A) REAP Minimax M2&lt;/strong&gt; – 139 B quantised to Q3_K_S, at 42k context. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;(B) GLM 4.5 Air&lt;/strong&gt; – 110B quantised to IQ4_NL, supports 46 k context. &lt;/p&gt; &lt;p&gt;Both perform great and they will probably become my daily models. Overall GLM-4.5-Air feels slower and dumber than REAP Minimax M2, but I haven't had a lot of time with either of them. I will follow up and edit this if I change my min&lt;/p&gt; &lt;p&gt;&lt;strong&gt;4. GPT-OSS-120B&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Is still decent and runs fast, but I can't help but feel that it's very dated, and extremely censored (!) For instance try asking: &lt;/p&gt; &lt;p&gt;&lt;code&gt;&amp;quot;What are some some examples of business strategies such as selling eternal youth to woman, or money making ideas to poor people?&amp;quot;&lt;/code&gt; &lt;/p&gt; &lt;p&gt;and you’ll get a response along the lines of: “I’m sorry, but I can’t help with that.” &lt;/p&gt; &lt;p&gt;&lt;strong&gt;5. Qwen3 Next 80B&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Runs very slow. Someone suggested the bottleneck might be CUDA and to trying Vulkan instead. However, given the many larger options available, I may drop it, even though it was my favourite model when I ran it on a 48GB (2x3090) &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Overall upgrading from 2x3090 to 3x3090, there are a lot of LLM models that get unlocked with that extra 24GB&lt;/strong&gt;. I would argue feels like a much bigger jump that it was when I moved from 24 to 48GB, and just wanted to share for those of you thinking for making the upgrade.&lt;/p&gt; &lt;p&gt;PS: I also upgraded my ram from 64GB to 128GB, but I think it might have been for nothing. It helps a bit with loading the model faster, but honstly, I don't think it's worth if when you are running everything on the GPU.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/liviuberechet"&gt; /u/liviuberechet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/85cs39k6daeg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh442y/models_that_run_in_72gb_vram_with_context_loaded/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qh442y/models_that_run_in_72gb_vram_with_context_loaded/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T13:27:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1qh0yq8</id>
    <title>I made a Top-K implementation that's up to 20x faster than PyTorch CPU (open source)</title>
    <updated>2026-01-19T10:45:28+00:00</updated>
    <author>
      <name>/u/andreabarbato</name>
      <uri>https://old.reddit.com/user/andreabarbato</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Spent way too long optimizing Top-K selection for LLM sampling and finally hit some stupid numbers.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; AVX2-optimized batched Top-K that beats PyTorch CPU by 4-20x depending on vocab size. Sometimes competitive with CUDA for small batches.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Benchmarks (K=50):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Vocab=32K: 0.043ms vs PyTorch's 0.173ms (4x faster)&lt;/li&gt; &lt;li&gt;Vocab=128K: 0.057ms vs PyTorch's 0.777ms (13x faster)&lt;/li&gt; &lt;li&gt;Vocab=256K: 0.079ms vs PyTorch's 1.56ms (20x faster)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Integrated it into llama.cpp and got 63% faster prompt processing on a 120B MoE model (81→142 tokens/sec).&lt;/p&gt; &lt;p&gt;Uses adaptive sampling + AVX2 SIMD + cache-optimized scanning. Has fast paths for sorted/constant inputs. Single-pass algorithm, no GPU needed.&lt;/p&gt; &lt;p&gt;Includes pre-built DLLs and llama.cpp implementation (for windows).&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/RAZZULLIX/fast_topk_batched"&gt;https://github.com/RAZZULLIX/fast_topk_batched&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love feedback or roasting, whichever you prefer.&lt;/p&gt; &lt;p&gt;EDIT:&lt;/p&gt; &lt;p&gt;can anyone try it and let me know if it works for them? thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/andreabarbato"&gt; /u/andreabarbato &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh0yq8/i_made_a_topk_implementation_thats_up_to_20x/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh0yq8/i_made_a_topk_implementation_thats_up_to_20x/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qh0yq8/i_made_a_topk_implementation_thats_up_to_20x/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T10:45:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhkh2z</id>
    <title>I FP8 quantized GLM 4.7 Flash!</title>
    <updated>2026-01-19T23:28:43+00:00</updated>
    <author>
      <name>/u/k_means_clusterfuck</name>
      <uri>https://old.reddit.com/user/k_means_clusterfuck</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, I know it ain't much, I finally decided to try and be the first out to fp8 quant a newly dropped model. I would love to hear feedback if you try it. Steps to get it running are in the README :) &lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/marksverdhei/GLM-4.7-Flash-FP8"&gt;https://huggingface.co/marksverdhei/GLM-4.7-Flash-FP8&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/k_means_clusterfuck"&gt; /u/k_means_clusterfuck &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhkh2z/i_fp8_quantized_glm_47_flash/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhkh2z/i_fp8_quantized_glm_47_flash/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhkh2z/i_fp8_quantized_glm_47_flash/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T23:28:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhgi10</id>
    <title>lightonai/LightOnOCR-2-1B · Hugging Face</title>
    <updated>2026-01-19T20:57:11+00:00</updated>
    <author>
      <name>/u/SarcasticBaka</name>
      <uri>https://old.reddit.com/user/SarcasticBaka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhgi10/lightonailightonocr21b_hugging_face/"&gt; &lt;img alt="lightonai/LightOnOCR-2-1B · Hugging Face" src="https://external-preview.redd.it/owrWH9MOuE15-iASn4iPzZcG9U3KIDtVJ9SmxpvC1c0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d891c173f79ddf24b05c65d408e9287701ba72c2" title="lightonai/LightOnOCR-2-1B · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SarcasticBaka"&gt; /u/SarcasticBaka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/lightonai/LightOnOCR-2-1B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhgi10/lightonailightonocr21b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhgi10/lightonailightonocr21b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T20:57:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhqzsi</id>
    <title>Mosquito - 7.3M parameter tiny knowledge model</title>
    <updated>2026-01-20T04:16:20+00:00</updated>
    <author>
      <name>/u/Lopsided-Repair-3638</name>
      <uri>https://old.reddit.com/user/Lopsided-Repair-3638</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A mosquito brain size model (7.3M params) that can answer surprisingly many general knowledge questions. Demo: &lt;a href="https://huggingface.co/spaces/ag14850/Mosquito-Demo"&gt;https://huggingface.co/spaces/ag14850/Mosquito-Demo&lt;/a&gt; Model: &lt;a href="https://huggingface.co/ag14850/Mosquito"&gt;https://huggingface.co/ag14850/Mosquito&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lopsided-Repair-3638"&gt; /u/Lopsided-Repair-3638 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhqzsi/mosquito_73m_parameter_tiny_knowledge_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhqzsi/mosquito_73m_parameter_tiny_knowledge_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhqzsi/mosquito_73m_parameter_tiny_knowledge_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T04:16:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhml0s</id>
    <title>With DRAM and NAND prices what they are, the DGX Spark almost seems like a bargain now LOL.</title>
    <updated>2026-01-20T00:57:58+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know a lot of the inference-focused crowd (myself included) were let down by the DGX Spark when it was released because of its weak memory bandwidth and high price tag. &lt;/p&gt; &lt;p&gt;Fast forward a few months and the whole consumer PC component market has turned into an absolute shitshow, RAM prices have quadrupled, now M2 prices are doing the same. That being said, if you break down the current retail market cost of the hardware components thar make up the DGX Spark, it’s sadly turned into a decent value from a solely HW component perspective. &lt;/p&gt; &lt;p&gt;Here’s a break down the core specs of the DGX Spark and what the market prices of the equivalent components would be (pulled these prices from Amazon US today) &lt;/p&gt; &lt;p&gt;- 128 GB of LPDDR5x RAM = $1600 (for 6000 MT/s, the DGX Spark has 8533 MT/s)&lt;/p&gt; &lt;p&gt;- 4TB M2 Gen5 SSD = $895&lt;/p&gt; &lt;p&gt;- 20 core CPU = $300&lt;/p&gt; &lt;p&gt;- Connectx-7 400 GB Nic (which the Spark has built-in = $1,197 &lt;/p&gt; &lt;p&gt;- 5070 GPU (which is what the DGX is said to be equivalent to from a pure GPU compute standpoint) = $639&lt;/p&gt; &lt;p&gt;Total current market prices of equivalent DGX Spark components = $4,631&lt;/p&gt; &lt;p&gt;DGX Spark Current price (4TB model) = $3,999&lt;/p&gt; &lt;p&gt;Estimated cost savings (if you bought a Spark instead of the components) = $632&lt;/p&gt; &lt;p&gt;I did not take into account Motherboard, Case, PSU, cooling, etc. You probably are looking at at least another $300 or more saved by getting the Spark, but I wasn’t really going to count those because the market prices for those components are pretty stable. &lt;/p&gt; &lt;p&gt;Anyways, I’m not advocating buying a Spark or anything like that, I just thought it was interesting that our mindset of what is a good deal vs. what isn’t a good deal is probably going to shift as DRAM and other component market prices get worse. My point is that 6 months ago, DGX Spark was a terrible perceived value proposition, but now in the current HW component market, maybe it’s not so bad. It is still pretty garbage for inference speed though except for some specific NVFP4 models. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhml0s/with_dram_and_nand_prices_what_they_are_the_dgx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhml0s/with_dram_and_nand_prices_what_they_are_the_dgx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhml0s/with_dram_and_nand_prices_what_they_are_the_dgx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T00:57:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhg6rm</id>
    <title>GLM-4.7-FLASH-NVFP4 on huggingface (20.5 GB)</title>
    <updated>2026-01-19T20:45:46+00:00</updated>
    <author>
      <name>/u/DataGOGO</name>
      <uri>https://old.reddit.com/user/DataGOGO</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I published a mixed precision NVFP4 quantized version the new GLM-4.7-FLASH on HF, can any of you can test it and let me know how it goes, I would really appreciate it. &lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/GadflyII/GLM-4.7-Flash-NVFP4"&gt;https://huggingface.co/GadflyII/GLM-4.7-Flash-NVFP4&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DataGOGO"&gt; /u/DataGOGO &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhg6rm/glm47flashnvfp4_on_huggingface_205_gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhg6rm/glm47flashnvfp4_on_huggingface_205_gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhg6rm/glm47flashnvfp4_on_huggingface_205_gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T20:45:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhpima</id>
    <title>Bartowski comes through again. GLM 4.7 flash GGUF</title>
    <updated>2026-01-20T03:07:33+00:00</updated>
    <author>
      <name>/u/RenewAi</name>
      <uri>https://old.reddit.com/user/RenewAi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/bartowski/zai-org_GLM-4.7-Flash-GGUF"&gt;https://huggingface.co/bartowski/zai-org_GLM-4.7-Flash-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RenewAi"&gt; /u/RenewAi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhpima/bartowski_comes_through_again_glm_47_flash_gguf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhpima/bartowski_comes_through_again_glm_47_flash_gguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhpima/bartowski_comes_through_again_glm_47_flash_gguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T03:07:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhaq21</id>
    <title>New in llama.cpp: Anthropic Messages API</title>
    <updated>2026-01-19T17:33:24+00:00</updated>
    <author>
      <name>/u/paf1138</name>
      <uri>https://old.reddit.com/user/paf1138</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhaq21/new_in_llamacpp_anthropic_messages_api/"&gt; &lt;img alt="New in llama.cpp: Anthropic Messages API" src="https://external-preview.redd.it/zqasF6xdAR1yVfMl-Ppz2b8-S-Dv35pa4J_UeKummLg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=56eabcfaa752210d59dc7af42f1b2087636a579d" title="New in llama.cpp: Anthropic Messages API" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paf1138"&gt; /u/paf1138 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/blog/ggml-org/anthropic-messages-api-in-llamacpp"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhaq21/new_in_llamacpp_anthropic_messages_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhaq21/new_in_llamacpp_anthropic_messages_api/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T17:33:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhjhlh</id>
    <title>GLM-4.7-Flash-GGUF is here!</title>
    <updated>2026-01-19T22:49:59+00:00</updated>
    <author>
      <name>/u/KvAk_AKPlaysYT</name>
      <uri>https://old.reddit.com/user/KvAk_AKPlaysYT</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhjhlh/glm47flashgguf_is_here/"&gt; &lt;img alt="GLM-4.7-Flash-GGUF is here!" src="https://external-preview.redd.it/xaz8me0jAeBOkTb7mKUXdYdIdr8aoSsiwENwulyOJmI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6f21f70be7ae2e1b3f10f33471dbfc4c47ba6518" title="GLM-4.7-Flash-GGUF is here!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KvAk_AKPlaysYT"&gt; /u/KvAk_AKPlaysYT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/AaryanK/GLM-4.7-Flash-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhjhlh/glm47flashgguf_is_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhjhlh/glm47flashgguf_is_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T22:49:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1qh5wdq</id>
    <title>zai-org/GLM-4.7-Flash · Hugging Face</title>
    <updated>2026-01-19T14:40:27+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh5wdq/zaiorgglm47flash_hugging_face/"&gt; &lt;img alt="zai-org/GLM-4.7-Flash · Hugging Face" src="https://external-preview.redd.it/Qs0t4y5eLm-uwORWdP6T0dcwW2T6VJyQFBUSY70CTF8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8700f4a43fe16a1031ccda94b517fd709573a5c3" title="zai-org/GLM-4.7-Flash · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/zai-org/GLM-4.7-Flash"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh5wdq/zaiorgglm47flash_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qh5wdq/zaiorgglm47flash_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T14:40:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhlnsv</id>
    <title>Unsloth GLM 4.7-Flash GGUF</title>
    <updated>2026-01-20T00:17:58+00:00</updated>
    <author>
      <name>/u/Wooden-Deer-1276</name>
      <uri>https://old.reddit.com/user/Wooden-Deer-1276</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF"&gt;https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wooden-Deer-1276"&gt; /u/Wooden-Deer-1276 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhlnsv/unsloth_glm_47flash_gguf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhlnsv/unsloth_glm_47flash_gguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhlnsv/unsloth_glm_47flash_gguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T00:17:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhii5v</id>
    <title>My gpu poor comrades, GLM 4.7 Flash is your local agent</title>
    <updated>2026-01-19T22:12:06+00:00</updated>
    <author>
      <name>/u/__Maximum__</name>
      <uri>https://old.reddit.com/user/__Maximum__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried many MoE models at 30B or under and all of them failed sooner or later in an agentic framework. If z.ai is not redirecting my requests to another model, then GLM 4.7 Flash is finally the reliable (soon local) agent that I desperately wanted.&lt;/p&gt; &lt;p&gt;I am running it since more than half an hour on opencode and it produced hundreds of thousands tokens in one session (with context compacting obviously) without any tool calling errors. It clones github repos, it runs all kind of commands, edits files, commits changes, all perfect, not a single error yet.&lt;/p&gt; &lt;p&gt;Can't wait for GGUFs to try this locally.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/__Maximum__"&gt; /u/__Maximum__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhii5v/my_gpu_poor_comrades_glm_47_flash_is_your_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhii5v/my_gpu_poor_comrades_glm_47_flash_is_your_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhii5v/my_gpu_poor_comrades_glm_47_flash_is_your_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T22:12:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhitrj</id>
    <title>GLM 4.7 Flash official support merged in llama.cpp</title>
    <updated>2026-01-19T22:24:24+00:00</updated>
    <author>
      <name>/u/ayylmaonade</name>
      <uri>https://old.reddit.com/user/ayylmaonade</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhitrj/glm_47_flash_official_support_merged_in_llamacpp/"&gt; &lt;img alt="GLM 4.7 Flash official support merged in llama.cpp" src="https://external-preview.redd.it/AVP8Isc32PMjAyVGtAipaav3x8aU8JY8Lx1bZ_yPak0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=43081fb39d8cfd3c8faeeb3516b7513654ed8fce" title="GLM 4.7 Flash official support merged in llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ayylmaonade"&gt; /u/ayylmaonade &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/18936"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhitrj/glm_47_flash_official_support_merged_in_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhitrj/glm_47_flash_official_support_merged_in_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T22:24:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
