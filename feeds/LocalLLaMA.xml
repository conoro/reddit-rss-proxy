<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-25T14:23:19+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ofq2g3</id>
    <title>[Open Source] We deployed numerous agents in production and ended up building our own GenAI framework</title>
    <updated>2025-10-25T12:17:13+00:00</updated>
    <author>
      <name>/u/vizsatiz</name>
      <uri>https://old.reddit.com/user/vizsatiz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here‚Äôs what the journey taught us üß†&lt;/p&gt; &lt;p&gt;After building and deploying GenAI solutions in production, we got tired of fighting with bloated frameworks, debugging black boxes, and dealing with vendor lock-in.&lt;/p&gt; &lt;p&gt;So we built Flo AI - a Python framework that actually respects your time.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Problem We Solved&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Most LLM frameworks give you two bad options:&lt;/p&gt; &lt;p&gt;Too much abstraction ‚Üí You have no idea why your agent did what it did&lt;/p&gt; &lt;p&gt;Too little structure ‚Üí You're rebuilding the same patterns over and over.&lt;/p&gt; &lt;p&gt;We wanted something that's predictable, debuggable, customizable, composable and production-ready from day one.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What Makes FloAI Different&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;üîç &lt;strong&gt;Built-in Observability&lt;/strong&gt;: OpenTelemetry tracing out of the box. See exactly what your agents are doing, track token usage, and debug performance issues without adding extra libraries. (pre-release)&lt;/p&gt; &lt;p&gt;ü§ù &lt;strong&gt;Multi-Agent Collaboration (Arium)&lt;/strong&gt;: Agents can call other specialized agents. Build a trip planner that coordinates weather experts and web researchers - it just works.&lt;/p&gt; &lt;p&gt;üìö &lt;strong&gt;Composable by Design&lt;/strong&gt;: Ability to build larger and larger agentic workflows, by composable smaller units&lt;/p&gt; &lt;p&gt;‚öôÔ∏è &lt;strong&gt;Customizable via YAML&lt;/strong&gt;: Design your agents using for YAMLs for easy customizations and prompt changes, as well as flo changes&lt;/p&gt; &lt;p&gt;üîå &lt;strong&gt;Vendor Agnostic&lt;/strong&gt;: Start with OpenAI, switch to Claude, add Gemini - same code. We support OpenAI, Anthropic, Google, Ollama, vLLM and VertextAI. (more coming soon)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why We're Sharing This&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;We believe in less abstraction, more control.&lt;/p&gt; &lt;p&gt;If you‚Äôve ever been frustrated by frameworks that hide too much or make you reinvent the wheel, Flo AI might be exactly what you‚Äôre looking for.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;üêô GitHub: &lt;a href="https://github.com/rootflo/flo-ai"&gt;https://github.com/rootflo/flo-ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üè† Website: &lt;a href="https://rootflo.ai/"&gt;https://rootflo.ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Docs: &lt;a href="https://flo-ai.rootflo.ai"&gt;https://flo-ai.rootflo.ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üôå We Need Your Feedback&lt;/p&gt; &lt;p&gt;We‚Äôre actively building and would love your input:&lt;/p&gt; &lt;p&gt;What features would make this useful for your use case?&lt;/p&gt; &lt;p&gt;What pain points do you face with current LLM frameworks?&lt;/p&gt; &lt;p&gt;Found a bug? We respond fast!&lt;/p&gt; &lt;p&gt;‚≠ê Star us on GitHub if this resonates ‚Äî it really helps us know we‚Äôre solving real problems.&lt;/p&gt; &lt;p&gt;Happy to chat or answer questions in the comments! üöÄ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vizsatiz"&gt; /u/vizsatiz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofq2g3/open_source_we_deployed_numerous_agents_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofq2g3/open_source_we_deployed_numerous_agents_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ofq2g3/open_source_we_deployed_numerous_agents_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-25T12:17:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ofq8jq</id>
    <title>How to make PocketPal inference faster on android?</title>
    <updated>2025-10-25T12:26:01+00:00</updated>
    <author>
      <name>/u/Ok_Warning2146</name>
      <uri>https://old.reddit.com/user/Ok_Warning2146</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have an OnePlus 12 24GB running on LineageOS 22.2 with 6.44GB zram. I ran the PocketPal bench at the default pp=512,tg=128,pl=1 and rep=3.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;pp&lt;/th&gt; &lt;th align="left"&gt;tg&lt;/th&gt; &lt;th align="left"&gt;time&lt;/th&gt; &lt;th align="left"&gt;PeakMem&lt;/th&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;14.18t/s&lt;/td&gt; &lt;td align="left"&gt;6.79t/s&lt;/td&gt; &lt;td align="left"&gt;2m50s&lt;/td&gt; &lt;td align="left"&gt;81.1%&lt;/td&gt; &lt;td align="left"&gt;Qwen3-30B-A3B-Instruct-2507-UD_Q5_K_XL&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;17.42t/s&lt;/td&gt; &lt;td align="left"&gt;4.00t/s&lt;/td&gt; &lt;td align="left"&gt;3m4s&lt;/td&gt; &lt;td align="left"&gt;62.0%&lt;/td&gt; &lt;td align="left"&gt;gemma-3-12b-it-qat-Q4_0&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;The Qwen model is about 21.7GB and the gemma model is 6.9GB. It seems like the PeakMem refers to the Peak Memory used by the whole system as the gemma model shouldn't fill up 62% of 24GB. In that sense, I presume some of the 21.7GB Qwen model went to zram which is like a compressed swap stored in RAM. Would adjusting zram size affect performance? Would it perform much better if I use a 16GB qwen model?&lt;/p&gt; &lt;p&gt;I noticed that PocketPal benchmark doesn't offload anything to the GPU. Does that mean only CPU is used? Is it possible to make PocketPal to use GPU?&lt;/p&gt; &lt;p&gt;Thanks a lot in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Warning2146"&gt; /u/Ok_Warning2146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofq8jq/how_to_make_pocketpal_inference_faster_on_android/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofq8jq/how_to_make_pocketpal_inference_faster_on_android/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ofq8jq/how_to_make_pocketpal_inference_faster_on_android/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-25T12:26:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1ofqe7x</id>
    <title>What are actual verifiable ways we can detect AI?</title>
    <updated>2025-10-25T12:33:57+00:00</updated>
    <author>
      <name>/u/ra4h</name>
      <uri>https://old.reddit.com/user/ra4h</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Social media is now filled with AI content that is fooling people left and right. AI generated short form content goes viral frequently, with lots of people assuming it to be real, along with majority of long write ups being chatGPT‚Äôd. &lt;/p&gt; &lt;p&gt;Most of us already saw this coming years ago, I‚Äôm sure this isn‚Äôt a surprise to most people here. The thing is, do we have any strategies to combat this? Is there any realistic ‚ÄúAI detection‚Äù tool we can develop to be able to easily deem video/audio/text as AI generated? &lt;/p&gt; &lt;p&gt;Personally, I feel that I can spot AI generated text quite consistently. There‚Äôs the obvious tell of em-dashes, but even without that there are some obvious word patterns, sentence structure, etc. I don‚Äôt know how long this will last and how fast standard text generation will become indistinguishable. Even now if people prompt the AI properly and make a few tweaks themselves, most write ups can‚Äôt be spotted as AI. Moreover, we have all seen the unreliability of AI detection tools that universities and such use, so it‚Äôs clearly not even close to being a solved issue. And these AI technologies will only get better. &lt;/p&gt; &lt;p&gt;Video and audio content seems even tougher, at least for me to be able to distinguish. Some of them have obvious tells but a lot of them don‚Äôt. My question is, what is being done to combat this? I would think that this issue of not being able to tell what‚Äôs real vs AI will become one of the most pertinent issues as we continue onwards. As such, there is lots of value in developing ways to detect this and I‚Äôm sure some very smart people are trying to solve this issue. I want to know what is being done and what are the technologies/strategies we could conceivably develop to achieve this task? &lt;/p&gt; &lt;p&gt;The simplest solution is having people do things in a controlled environment where they can be constantly observed. For Uni tests and such, a return to proctored pen and paper exams is quite likely. For people who want art that is verifiably human-made, they could maybe be given a video of the artist going through the entire process, but even this could become AI generated quite soon. Anyhow, these methods aren‚Äôt a general solution for the broader issue. Is there even a way to address the broader issue, or do we just have to accept the new reality with no recourse?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ra4h"&gt; /u/ra4h &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofqe7x/what_are_actual_verifiable_ways_we_can_detect_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofqe7x/what_are_actual_verifiable_ways_we_can_detect_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ofqe7x/what_are_actual_verifiable_ways_we_can_detect_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-25T12:33:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1of0xc1</id>
    <title>GLM 4.6 coding Benchmarks</title>
    <updated>2025-10-24T15:33:10+00:00</updated>
    <author>
      <name>/u/IndependentFresh628</name>
      <uri>https://old.reddit.com/user/IndependentFresh628</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Did they fake Coding benchmarks where it is visible GLM 4.6 is neck to neck with Claude Sonnet 4.5 however, in real world Use it is not even close to Sonnet when it comes Debug or Efficient problem solving.&lt;/p&gt; &lt;p&gt;But yeah, GLM can generate massive amount of Coding tokens in one prompt.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IndependentFresh628"&gt; /u/IndependentFresh628 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1of0xc1/glm_46_coding_benchmarks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1of0xc1/glm_46_coding_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1of0xc1/glm_46_coding_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-24T15:33:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1oeyzxq</id>
    <title>[ü™® Onyx v2.0.0] Self-hosted chat and RAG - now with FOSS repo, SSO, new design/colors, and projects!</title>
    <updated>2025-10-24T14:19:13+00:00</updated>
    <author>
      <name>/u/Weves11</name>
      <uri>https://old.reddit.com/user/Weves11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oeyzxq/onyx_v200_selfhosted_chat_and_rag_now_with_foss/"&gt; &lt;img alt="[ü™® Onyx v2.0.0] Self-hosted chat and RAG - now with FOSS repo, SSO, new design/colors, and projects!" src="https://b.thumbs.redditmedia.com/1g7bw_MAWvmmqQ_XM_zZ3Opapd4T9MrJQmyySlO22qg.jpg" title="[ü™® Onyx v2.0.0] Self-hosted chat and RAG - now with FOSS repo, SSO, new design/colors, and projects!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey friends, I‚Äôve got a big Onyx update for you guys! &lt;/p&gt; &lt;p&gt;I heard your feedback loud and clear last time - and thanks to the great suggestions I‚Äôve 1/ released a fully FOSS, MIT-licensed version of Onyx, 2/ open-sourced OIDC/SAML, and 3/ did a complete makeover of the design and colors. &lt;/p&gt; &lt;p&gt;If you don‚Äôt know - Onyx is an open-source, self-hostable chat UI that has support for every LLM plus built in RAG + connectors + MCP + web search + deep research.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Everything that‚Äôs new:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Open-sourced SSO (OIDC + SAML) &lt;/li&gt; &lt;li&gt;onyx-foss (&lt;a href="https://github.com/onyx-dot-app/onyx-foss"&gt;https://github.com/onyx-dot-app/onyx-foss&lt;/a&gt;), a completely MIT licensed version of Onyx&lt;/li&gt; &lt;li&gt;Brand new design / colors&lt;/li&gt; &lt;li&gt;Projects (think Claude projects, but with any model + self-hosted)&lt;/li&gt; &lt;li&gt;Organization info and personalization&lt;/li&gt; &lt;li&gt;Reworked core tool-calling loop. Uses native tool calling for better adherence, fewer history rewrites for better prompt caching, and less hand-crafted prompts for fewer artifacts in longer runs&lt;/li&gt; &lt;li&gt;OAuth support for OpenAPI-based tools&lt;/li&gt; &lt;li&gt;A bunch of bug fixes&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Really appreciate all the feedback from last time, and looking forward to more of it here. Onyx was briefly #1 python and #2 github trending repo of the day, which is so crazy to me.&lt;/p&gt; &lt;p&gt;If there‚Äôs anything else that you would find useful that‚Äôs NOT part of the MIT license please let me know and I‚Äôll do my best to move it over. All of the core functionality mentioned above is 100% FOSS. I want everything needed for the best open-source chat UI to be completely free and usable by all!&lt;/p&gt; &lt;p&gt;Repo:&lt;a href="https://github.com/onyx-dot-app/onyx"&gt; https://github.com/onyx-dot-app/onyx&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Full release notes:&lt;a href="https://docs.onyx.app/changelog#v2-0-0"&gt; https://docs.onyx.app/changelog#v2-0-0&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Weves11"&gt; /u/Weves11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1oeyzxq"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oeyzxq/onyx_v200_selfhosted_chat_and_rag_now_with_foss/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oeyzxq/onyx_v200_selfhosted_chat_and_rag_now_with_foss/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-24T14:19:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1of8f1i</id>
    <title>First attempt at building a local LLM setup in my mini rack</title>
    <updated>2025-10-24T20:23:54+00:00</updated>
    <author>
      <name>/u/Von_plaf</name>
      <uri>https://old.reddit.com/user/Von_plaf</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1of8f1i/first_attempt_at_building_a_local_llm_setup_in_my/"&gt; &lt;img alt="First attempt at building a local LLM setup in my mini rack" src="https://preview.redd.it/aseoosei84xf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=da5c51332fc5fb2400148e14f00664703a266818" title="First attempt at building a local LLM setup in my mini rack" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I finally got around to attempting to build a local LLM setup.&lt;br /&gt; Got my hands on 3 x Nvidia Jetson Orin nano's and put them into my mini rack and started to see if I could make them into a cluster.&lt;br /&gt; Long story short ... &lt;strong&gt;YES and NOOooo..&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I got all 3 Jetsons running llama.cpp and got them working in a cluster using llama-server on the first Jetson and rpc-server on the two other.&lt;br /&gt; But using llama-bench they produced only about 7 tokens/sec. when working together, but just one Jetson working alone i got about 22 tokens/sec. &lt;/p&gt; &lt;p&gt;Model I was using was Llama-3.2-3B-Instruct-Q4_K_M.gguf I did try out other models but not with any real good results.&lt;br /&gt; But it all comes down to the fact that they LLM really like things fast and for them to having to share over a &amp;quot;slow&amp;quot; 1Gb ethernet connection between each other was one of the factors that slowed everything down. &lt;/p&gt; &lt;p&gt;So I wanted to try something else.&lt;br /&gt; I loaded up the same model all 3 Jetsons and started a llama-server on each node but on different ports.&lt;br /&gt; Then setting up a Raspberry pi 5 4GB with Nginx as a load balancer and having a docker container run open webUI I then got all 3 Jetsons with llama.cpp feeding into the same UI, I still only get about 20-22 tokens/sec pr node, but adding the same model 3 times in one chat then all 3 nodes starts working on the prompt at the same time, then I can either merge the result or have 3 separate results.&lt;br /&gt; So all in all as for a first real try, not great but also not bad and just happy I got it running.&lt;/p&gt; &lt;p&gt;Now I think I will be looking into getting a larger model running to maximize the use of the jetsons.&lt;br /&gt; Still a lot to learn..&lt;/p&gt; &lt;p&gt;&lt;em&gt;The bottom part of the rack has the 3 x Nvidia Jetson Orin nano's and the Raspberry pi 5 for load balancing and running the webUI.&lt;/em&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Von_plaf"&gt; /u/Von_plaf &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/aseoosei84xf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1of8f1i/first_attempt_at_building_a_local_llm_setup_in_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1of8f1i/first_attempt_at_building_a_local_llm_setup_in_my/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-24T20:23:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1ofsn4q</id>
    <title>Looking for best Time-Series Data Model for pump or fan prediction on Hugging Face (Any Suggestions?)</title>
    <updated>2025-10-25T14:16:21+00:00</updated>
    <author>
      <name>/u/Worth-Relation72</name>
      <uri>https://old.reddit.com/user/Worth-Relation72</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I spent hours on hugging face looking for Time Series Data Model for Pump or Fan prediction but couldn't find a good model that could do predictive analysis, fault prediction and what not... Please suggest the best model on hugging face to analyse time series data with LLM... Thank you for the help...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Worth-Relation72"&gt; /u/Worth-Relation72 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofsn4q/looking_for_best_timeseries_data_model_for_pump/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofsn4q/looking_for_best_timeseries_data_model_for_pump/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ofsn4q/looking_for_best_timeseries_data_model_for_pump/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-25T14:16:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1ofezsz</id>
    <title>If you could have one LLM distilled to a smaller size, which would model would you pick and what size(s) would you pick?</title>
    <updated>2025-10-25T01:23:12+00:00</updated>
    <author>
      <name>/u/silenceimpaired</name>
      <uri>https://old.reddit.com/user/silenceimpaired</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Really the question is‚Ä¶ what larger open weight model do you wish you could run on your hardware with some reduced capacity: something large enough where quantization isn‚Äôt an option.&lt;/p&gt; &lt;p&gt;This is a tough choice for me, as I‚Äôve wanted to have a true distillation of Deepseek for the longest time, but I think Kimi-K2 has changed my mind. &lt;/p&gt; &lt;p&gt;I would love to have Kimi-K2 distilled to a 70b dense model‚Ä¶ a more likely size someone might attempt would be 106 billion total parameters and 12 billion active parameters, the same size as GLM 4.5 Air‚Ä¶ though maybe I would even go so large as GLM-4.5 which has 355 billion total parameters with 32 billion active parameters.&lt;/p&gt; &lt;p&gt;I completely forgot about the larger Qwen model! That would be great as well.&lt;/p&gt; &lt;p&gt;How about you? What model would you pick and at what size?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/silenceimpaired"&gt; /u/silenceimpaired &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofezsz/if_you_could_have_one_llm_distilled_to_a_smaller/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofezsz/if_you_could_have_one_llm_distilled_to_a_smaller/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ofezsz/if_you_could_have_one_llm_distilled_to_a_smaller/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-25T01:23:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1oes4ez</id>
    <title>Qwen3 Next support in llama.cpp ready for review</title>
    <updated>2025-10-24T08:18:49+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oes4ez/qwen3_next_support_in_llamacpp_ready_for_review/"&gt; &lt;img alt="Qwen3 Next support in llama.cpp ready for review" src="https://external-preview.redd.it/JWuwM-H5pHYaaKPNtY_8U3LHlrsSjJTNAjLHRGwU5o0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=016d876487cc90150078e6b226c52b29735d5532" title="Qwen3 Next support in llama.cpp ready for review" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Congratulations to Piotr for his hard work, the code is now ready for review.&lt;/p&gt; &lt;p&gt;Please note that this is not the final version, and if you download some quantized models, you will probably need to download them again later. Also, it's not yet optimized for speed.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/16095"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oes4ez/qwen3_next_support_in_llamacpp_ready_for_review/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oes4ez/qwen3_next_support_in_llamacpp_ready_for_review/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-24T08:18:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ofh2ia</id>
    <title>Woke up whole night and still couldn't resolve this one issue</title>
    <updated>2025-10-25T03:10:46+00:00</updated>
    <author>
      <name>/u/thenew_Alex_Bawden</name>
      <uri>https://old.reddit.com/user/thenew_Alex_Bawden</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofh2ia/woke_up_whole_night_and_still_couldnt_resolve/"&gt; &lt;img alt="Woke up whole night and still couldn't resolve this one issue" src="https://preview.redd.it/1v6zgu92d6xf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d0c90f63896b1c5ead31fa8fc2b09e3e0261e75a" title="Woke up whole night and still couldn't resolve this one issue" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Google Collab link :- &lt;a href="https://colab.research.google.com/drive/1gutbsKAiS46PsSoqPG51fHt8VNRrUNB3?usp=sharing#scrollTo=xIPudkKcQeyD"&gt;https://colab.research.google.com/drive/1gutbsKAiS46PsSoqPG51fHt8VNRrUNB3?usp=sharing#scrollTo=xIPudkKcQeyD&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I was fine tuning gpt oss 20B using unsloth on Google Colab and this error kept coming...&lt;/p&gt; &lt;p&gt;I feel i changed my dataset structure many times and still wasnot about to proceed.....&lt;/p&gt; &lt;p&gt;Also i think it is something to which harmony 1&lt;/p&gt; &lt;p&gt;Like do i need build a good json file but everything failed or the error is something else &lt;/p&gt; &lt;p&gt;Please please help me&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thenew_Alex_Bawden"&gt; /u/thenew_Alex_Bawden &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1v6zgu92d6xf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofh2ia/woke_up_whole_night_and_still_couldnt_resolve/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ofh2ia/woke_up_whole_night_and_still_couldnt_resolve/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-25T03:10:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1oeuiev</id>
    <title>Is OpenAI afraid of Kimi?</title>
    <updated>2025-10-24T10:50:59+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oeuiev/is_openai_afraid_of_kimi/"&gt; &lt;img alt="Is OpenAI afraid of Kimi?" src="https://b.thumbs.redditmedia.com/apnbgQLHwz7nx79BJoKIRI6eUEamaMguXqyI2K8LM8M.jpg" title="Is OpenAI afraid of Kimi?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;roon from OpenAI posted this earlier&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5hqotg83i1xf1.jpg?width=1190&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=f1396023a25350b27a94a3e4225bf38eb4ae86c3"&gt;https://preview.redd.it/5hqotg83i1xf1.jpg?width=1190&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=f1396023a25350b27a94a3e4225bf38eb4ae86c3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Then he instantly deleted the tweet&lt;/strong&gt; lol&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oeuiev/is_openai_afraid_of_kimi/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oeuiev/is_openai_afraid_of_kimi/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oeuiev/is_openai_afraid_of_kimi/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-24T10:50:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1ofpcje</id>
    <title>Unable to setup Cline in VScode with LM studio. Cant set context window.</title>
    <updated>2025-10-25T11:37:37+00:00</updated>
    <author>
      <name>/u/Pack_Commercial</name>
      <uri>https://old.reddit.com/user/Pack_Commercial</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofpcje/unable_to_setup_cline_in_vscode_with_lm_studio/"&gt; &lt;img alt="Unable to setup Cline in VScode with LM studio. Cant set context window." src="https://preview.redd.it/sgtruy7iv8xf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8e8e670e8943850816cfa1042f926ce0fea083ac" title="Unable to setup Cline in VScode with LM studio. Cant set context window." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Would anyone with some Cline setup experience help me üôÇ&lt;/p&gt; &lt;p&gt;I just installed and setting up cline extension in VScode with my local llm on LM studio. But after installing I started the below steps.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;When I clicked LM studio provider it did not show list of models, So I did manually typed the model ID (seen on left from LM studio)&lt;/li&gt; &lt;li&gt;Next I was unable to set Context window length. It has a hard value 0, I can't modify.&lt;/li&gt; &lt;li&gt;Then I proceeded in chat asking simple question, and checking bg status on LM studio, Nothing happened even there.. &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Did I miss anything ? PS: I skipped signin process, everything is on my Win11 machine. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pack_Commercial"&gt; /u/Pack_Commercial &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sgtruy7iv8xf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofpcje/unable_to_setup_cline_in_vscode_with_lm_studio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ofpcje/unable_to_setup_cline_in_vscode_with_lm_studio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-25T11:37:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1ofboir</id>
    <title>Strix Halo + RTX 3090 Achieved! Interesting Results...</title>
    <updated>2025-10-24T22:42:49+00:00</updated>
    <author>
      <name>/u/JayTheProdigy16</name>
      <uri>https://old.reddit.com/user/JayTheProdigy16</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofboir/strix_halo_rtx_3090_achieved_interesting_results/"&gt; &lt;img alt="Strix Halo + RTX 3090 Achieved! Interesting Results..." src="https://b.thumbs.redditmedia.com/b6nRUaAihZDoN0MlWEmQXboPVf1abHo_e9s727G4WNU.jpg" title="Strix Halo + RTX 3090 Achieved! Interesting Results..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Specs: Fedora 43 Server (bare metal, tried via Proxmox but to reduce complexity went BM, will try again), Bosgame M5 128gb AI Max+ 395 (identical board to GMKtek EVO-X2), EVGA FTW3 3090, MinisForum DEG1 eGPU dock with generic m.2 to Oculink adapter + 850w PSU.&lt;/p&gt; &lt;p&gt;Compiled the latest version of llama.cpp with Vulkan RADV (NO CUDA), things are still very wonky but it does work. I was able to get GPT OSS 120b to run on llama-bench but running into weird OOM and VlkDeviceLost errors specifically in llama-bench when trying GLM 4.5 Air even though the rig has served all models perfectly fine thus far. KV cache quant also seems to be bugged out and throws context errors with llama-bench but again works fine with llama-server. Tried the strix-halo-toolbox build of llama.cpp but could never get memory allocation to function properly with the 3090.&lt;/p&gt; &lt;p&gt;Saw a ~30% increase in PP at 12k context no quant going from 312 TPS on Strix Halo only to 413 TPS with SH + 3090, but a ~20% decrease in TG from 50 TPS on SH only to 40 on SH + 3090 which i thought was pretty interesting, and a part of me wonders if that was an anomaly or not but will confirm at a later date with more data.&lt;/p&gt; &lt;p&gt;Going to do more testing with it but after banging my head into a wall for 4 days to get it serving properly im taking a break and enjoying my vette. Let me know if yall have any ideas or benchmarks yall might be interested in&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ly9ey0wr05xf1.jpg?width=3060&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=cc073c67f6d2bd5f976f53679d8de83215fb4697"&gt;https://preview.redd.it/ly9ey0wr05xf1.jpg?width=3060&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=cc073c67f6d2bd5f976f53679d8de83215fb4697&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/gv0terms05xf1.jpg?width=3060&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=8ccc70fb59e9e7a0771274e15e67bc18a36ac624"&gt;https://preview.redd.it/gv0terms05xf1.jpg?width=3060&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=8ccc70fb59e9e7a0771274e15e67bc18a36ac624&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/0ohsyz23z4xf1.png?width=1654&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9e0d122713096d181026c3a160b381e2db1333c6"&gt;https://preview.redd.it/0ohsyz23z4xf1.png?width=1654&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9e0d122713096d181026c3a160b381e2db1333c6&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JayTheProdigy16"&gt; /u/JayTheProdigy16 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofboir/strix_halo_rtx_3090_achieved_interesting_results/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofboir/strix_halo_rtx_3090_achieved_interesting_results/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ofboir/strix_halo_rtx_3090_achieved_interesting_results/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-24T22:42:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1of7gcb</id>
    <title>MiniMax-M2 Info (from OpenRouter discord)</title>
    <updated>2025-10-24T19:45:21+00:00</updated>
    <author>
      <name>/u/nuclearbananana</name>
      <uri>https://old.reddit.com/user/nuclearbananana</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1of7gcb/minimaxm2_info_from_openrouter_discord/"&gt; &lt;img alt="MiniMax-M2 Info (from OpenRouter discord)" src="https://b.thumbs.redditmedia.com/xndOTEovtiDsZXO2jNXpyX138_t8q84HFR2H6Bffzts.jpg" title="MiniMax-M2 Info (from OpenRouter discord)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/6s6d9ykc54xf1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=51b274177a62bfec585d7f06c2fe7649bc9aa5c9"&gt;https://preview.redd.it/6s6d9ykc54xf1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=51b274177a62bfec585d7f06c2fe7649bc9aa5c9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;MiniMax M2 ‚Äî A Gift for All Developers on the 1024 Festival&amp;quot;&lt;/p&gt; &lt;p&gt;Top 5 globally, surpassing Claude Opus 4.1 and second only to Sonnet 4.5; state-of-the-art among open-source models. Reengineered for coding and agentic use‚Äîopen-source SOTA, highly intelligent, with low latency and cost. We believe it's one of the best choices for agent products and the most suitable open-source alternative to Claude Code.&lt;/p&gt; &lt;p&gt;We are very proud to have participated in the model‚Äôs development; this is our gift to all developers.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;MiniMax-M2 is coming on Oct 27&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6s6d9ykc54xf1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=51b274177a62bfec585d7f06c2fe7649bc9aa5c9"&gt;https://preview.redd.it/6s6d9ykc54xf1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=51b274177a62bfec585d7f06c2fe7649bc9aa5c9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xfjadh5e54xf1.png?width=1380&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=21b792491113f10c43e492b252bc87176a3f7f53"&gt;https://preview.redd.it/xfjadh5e54xf1.png?width=1380&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=21b792491113f10c43e492b252bc87176a3f7f53&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/gb7mat4f54xf1.png?width=636&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=59eed9df7b9c485ce21b5cb41d9707e72ba5d39b"&gt;https://preview.redd.it/gb7mat4f54xf1.png?width=636&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=59eed9df7b9c485ce21b5cb41d9707e72ba5d39b&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nuclearbananana"&gt; /u/nuclearbananana &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1of7gcb/minimaxm2_info_from_openrouter_discord/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1of7gcb/minimaxm2_info_from_openrouter_discord/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1of7gcb/minimaxm2_info_from_openrouter_discord/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-24T19:45:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1of769j</id>
    <title>You can turn off the cloud, this + solar panel will suffice:</title>
    <updated>2025-10-24T19:34:13+00:00</updated>
    <author>
      <name>/u/JLeonsarmiento</name>
      <uri>https://old.reddit.com/user/JLeonsarmiento</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1of769j/you_can_turn_off_the_cloud_this_solar_panel_will/"&gt; &lt;img alt="You can turn off the cloud, this + solar panel will suffice:" src="https://preview.redd.it/a0svyfed34xf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f50f6ea7d9b1152c45428baa79fd791529dae389" title="You can turn off the cloud, this + solar panel will suffice:" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JLeonsarmiento"&gt; /u/JLeonsarmiento &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/a0svyfed34xf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1of769j/you_can_turn_off_the_cloud_this_solar_panel_will/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1of769j/you_can_turn_off_the_cloud_this_solar_panel_will/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-24T19:34:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ofb7mu</id>
    <title>4B fp16 or 8B q4?</title>
    <updated>2025-10-24T22:21:45+00:00</updated>
    <author>
      <name>/u/Ok-Internal9317</name>
      <uri>https://old.reddit.com/user/Ok-Internal9317</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofb7mu/4b_fp16_or_8b_q4/"&gt; &lt;img alt="4B fp16 or 8B q4?" src="https://preview.redd.it/ukn1akunw4xf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dd900eb86aa1c749649781d6ebd92067020e1209" title="4B fp16 or 8B q4?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys, &lt;/p&gt; &lt;p&gt;For my 8GB GPU schould I go for &lt;strong&gt;fp16 but 4B&lt;/strong&gt; or &lt;strong&gt;q4 version of 8B&lt;/strong&gt;? Any model you particularly want to recommend me? Requirement: basic ChatGPT replacement&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Internal9317"&gt; /u/Ok-Internal9317 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ukn1akunw4xf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofb7mu/4b_fp16_or_8b_q4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ofb7mu/4b_fp16_or_8b_q4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-24T22:21:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1ofqyhc</id>
    <title>Is GLM 4.5 / 4.6 really sensitive to quantisation? Or is vLLM stupifying the models?</title>
    <updated>2025-10-25T13:00:59+00:00</updated>
    <author>
      <name>/u/_supert_</name>
      <uri>https://old.reddit.com/user/_supert_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am trying to run GLM 4.6 or GLM 4.5 Air locally with vLLM. I've tried:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;zai-org/GLM-4.5-Air-FP8&lt;/li&gt; &lt;li&gt;vkerkez/GLM-4.6-REAP-266B-A32B-awq-sym&lt;/li&gt; &lt;li&gt;bullpoint/GLM-4.6-AWQ&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;but the answers just aren't as smart or deep as the full-fat deepinfra zai-org/GLM-4.6. I'm using OpenWebUI with the same parameters and system prompt.&lt;/p&gt; &lt;p&gt;For example, with prompt &amp;quot;Explain Girsanov's theorem&amp;quot; and my (long) system prompt,&lt;/p&gt; &lt;p&gt;&lt;strong&gt;vllm zai-org/GLM-4.5-Air-FP8&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Girsanov's theorem is a cornerstone in stochastic calculus and financial mathematics, particularly in the context of risk-neutral pricing. It states that if there exists a risk-free interest rate $ r $, then the discounted stock price can be transformed into a process with the same drift but a different volatility. Specifically, if $ X_t $ is a stochastic process with drift $ \mu $ and volatility $ \sigma $, then under the risk-free rate $ r $, the discounted process $ \frac{X_t}{r} $ follows a Gaussian distribution with mean $ \mu $ and variance $ \sigma^2 $. This transformation allows for the pricing of options by adjusting the volatility to account for the risk-free rate. The theorem is pivotal in proving the no-arbitrage principle and is widely used in options pricing to ensure that the expected value of a derivative is equal to its payoff, adjusted for the risk-free rate. The proof typically involves changing the measure to account for the risk-free rate, which may involve integrating over the Brownian motion under the risk-neutral measure. This ensures that the expected value of the discounted process remains unchanged, but its distribution is adjusted to reflect the risk-free interest rate. In summary, Girsanov's theorem enables the conversion of a stochastic process into one that is risk-neutral, allowing for more accurate pricing of financial instruments. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;deepinfra GLM 4.5 Air&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Girsanov‚Äôs theorem is a result in stochastic calculus that describes how the dynamics of a Brownian motion (or, more generally, a continuous local martingale) change when we pass to an equivalent probability measure. It is the analytical foundation for changing the drift of a stochastic differential equation by moving to a new measure, the central tool for constructing risk‚Äëneutral measures in mathematical finance. -------------------------------------------------------------------- ### 1. Setting Let \((\Omega,\mathcal F,\{\mathcal F_t\}_{t\ge 0},\mathbb P)\) be a filtered probability space satisfying the usual conditions. Assume that \(W=(W_t)_{t\ge0}\) is an \(\mathcal F_t\)-standard Brownian motion under \(\mathbb P\). Let \(\theta=(\theta_t)_{t\ge0}\) be an \(\mathcal F_t\)-adapted process such that \[ \mathbb E_{\mathbb P}\!\left[\exp\!\Bigl(\frac12\int_0^T\theta_s^2\,ds\Bigr)\right] &amp;lt;\infty \qquad\text{for all } T\ge0 . \tag{1} \] Condition (1) is known as the **Novikov condition**; it guarantees that a certain stochastic exponential is a true martingale. -------------------------------------------------------------------- ### 2. Exponential martingale and new measure Define the **stochastic exponential** \[ Z_t := \exp\!\left(-\int_0^t \theta_s\, dW_s -\frac12\int_0^t \theta_s^2\, ds\right), \qquad t\ge0 . \] Under (1), \((Z_t)_{t\ge0}\) is a strictly positive martingale with \(Z_0=1\). Using \(Z_T\) as a Radon‚ÄìNikodym derivative we introduce a new probability measure \(\mathbb Q\) on \(\mathcal F_T\) by \[ \frac{d\mathbb Q}{d\mathbb P}\bigg|_{\mathcal F_T}=Z_T . \] The family \(\{\mathbb Q\}\) obtained in this way is equivalent to \(\mathbb P\) (i.e., no null set of \(\mathbb P\) is null under \(\mathbb Q\) and vice versa). -------------------------------------------------------------------- ### 3. Statement of the theorem Under the new measure \(\mathbb Q\) the process \[ \widetilde W_t := W_t + \int_0^t \theta_s\, ds, \qquad t\ge0 , \] is an \(\mathcal F_t\)-standard Brownian motion. Equivalently, \[ W_t = \widetilde W_t - \int_0^t \theta_s\, ds , \] so that the drift of \(W\) is shifted by \(-\theta\) when viewed under \(\mathbb Q\). -------------------------------------------------------------------- ### 4. Interpretation The theorem gives a concrete recipe for turning a Brownian motion with drift into a true Brownian motion by changing probability measures. In the language of differential equations, suppose a process \(X\) satisfies \[ dX_t = \mu_t\,dt + \sigma_t\,dW_t , \] where \(\mu_t,\sigma_t\) are adapted processes. Choose \(\theta_t = \sigma_t^{-1}\mu_t\) (assuming \(\sigma_t\neq0\)). Then the new measure \(\mathbb Q\) defined above eliminates the drift term: under \(\mathbb Q\), \[ dX_t = \sigma_t\, d\widetilde W_t , \] so \(X\) behaves like a Brownian motion scaled by \(\sigma\). This is precisely the device used in financial mathematics to construct an equivalent martingale measure (‚Äúrisk‚Äëneutral measure‚Äù) under which the discounted price of a tradable asset becomes a martingale. -------------------------------------------------------------------- ### 5. Extensions * **General local martingales**: The result extends to continuous local martingales that are not necessarily Brownian, provided the Novikov condition holds for the integrand against that martingale. * **Processes with jumps**: A Girsanov theorem for semimartingales with jumps requires an additional Radon‚ÄìNikodym derivative relating the jump measures of the two processes. * **Multidimensional setting**: If \(W\) is an \(n\)-dimensional Brownian motion and \(\theta\) an \(\mathbb R^n\)-valued adapted process satisfying a Novikov-type condition, the same exponential formula changes the drift vector and preserves the covariance matrix. -------------------------------------------------------------------- ### 6. Practical checklist * Verify that the integrand \(\theta_t\) is adapted and satisfies a Novikov‚Äìtype integrability condition. * Compute the stochastic exponential \(Z_t\); ensure it is a martingale (this is the key check). * Define the new measure by \(d\mathbb Q = Z_T\, d\mathbb P\). * Conclude that shifted process \(\widetilde W_t = W_t + \int_0^t \theta_s\,ds\) is a Brownian motion under \(\mathbb Q\). -------------------------------------------------------------------- **Reference** The original proof appears in Girsanov (1960), ‚ÄúOn transforming a certain class of stochastic processes by absolutely continuous change of measure‚Äù; see also standard texts such as √òksendal (2003) ‚ÄúStochastic Differential Equations‚Äù, Chapter II, or Karatzas &amp;amp; Shreve (1991) ‚ÄúBrownian Motion and Stochastic Calculus‚Äù, Chapter IV. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This (qualitative) difference is repeatable and I notice it for the 4.6 quants as well.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_supert_"&gt; /u/_supert_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofqyhc/is_glm_45_46_really_sensitive_to_quantisation_or/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofqyhc/is_glm_45_46_really_sensitive_to_quantisation_or/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ofqyhc/is_glm_45_46_really_sensitive_to_quantisation_or/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-25T13:00:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1ofggny</id>
    <title>Kimi k2 image generation</title>
    <updated>2025-10-25T02:38:13+00:00</updated>
    <author>
      <name>/u/Used-Nectarine5541</name>
      <uri>https://old.reddit.com/user/Used-Nectarine5541</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofggny/kimi_k2_image_generation/"&gt; &lt;img alt="Kimi k2 image generation" src="https://preview.redd.it/vimkvz5976xf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=202c4cfa6cd438c6e2e103ec6ec094a5b596d49a" title="Kimi k2 image generation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am so confused because I can‚Äôt find any information on Kimi k2 image generation abilities. When I asked Kimi to generate an image it said it couldn‚Äôt. But I‚Äôm having it code a tarot reading project and it‚Äôs generating all these images‚Ä¶when I asked about it Kimi still said it couldn‚Äôt generate images. What‚Äôs going on and how are these images being generated??&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Used-Nectarine5541"&gt; /u/Used-Nectarine5541 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vimkvz5976xf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofggny/kimi_k2_image_generation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ofggny/kimi_k2_image_generation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-25T02:38:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ofq6m7</id>
    <title>which model has the best world knowledge? Open weights and proprietary.</title>
    <updated>2025-10-25T12:23:16+00:00</updated>
    <author>
      <name>/u/z_3454_pfk</name>
      <uri>https://old.reddit.com/user/z_3454_pfk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I am looking for models with great general world knowledge and application of this. Open weights are preferred (I have access to H200s, so anything below 1.8TB VRAM) but API can be used if necessary. I am finding world knowledge really sucks for open models, even Kimi which can just get things wrong.&lt;/p&gt; &lt;p&gt;For example, knowing how much medication is wasted when you draw it up from a vial, based of the type of needle (since you get something called dead space - medication that stays in the tip o the syringe and needle). A lot of this is in nursing text books, so they know the content, but when asking models about it (such as Gemini flash) they really suck when it comes to applying this knowledge.&lt;/p&gt; &lt;p&gt;Any suggestions?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/z_3454_pfk"&gt; /u/z_3454_pfk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofq6m7/which_model_has_the_best_world_knowledge_open/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofq6m7/which_model_has_the_best_world_knowledge_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ofq6m7/which_model_has_the_best_world_knowledge_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-25T12:23:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1oextwc</id>
    <title>GLM-4.6-Air is not forgotten!</title>
    <updated>2025-10-24T13:31:12+00:00</updated>
    <author>
      <name>/u/codys12</name>
      <uri>https://old.reddit.com/user/codys12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oextwc/glm46air_is_not_forgotten/"&gt; &lt;img alt="GLM-4.6-Air is not forgotten!" src="https://preview.redd.it/z5dduynua2xf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b43f43a244e84de5bb07a0bc9e4c16127860c9a4" title="GLM-4.6-Air is not forgotten!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/codys12"&gt; /u/codys12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/z5dduynua2xf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oextwc/glm46air_is_not_forgotten/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oextwc/glm46air_is_not_forgotten/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-24T13:31:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1of8xl2</id>
    <title>Apple Foundation is dumb</title>
    <updated>2025-10-24T20:45:00+00:00</updated>
    <author>
      <name>/u/PM_ME_UR_COFFEE_CUPS</name>
      <uri>https://old.reddit.com/user/PM_ME_UR_COFFEE_CUPS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1of8xl2/apple_foundation_is_dumb/"&gt; &lt;img alt="Apple Foundation is dumb" src="https://b.thumbs.redditmedia.com/YZ_noIfhIcr5Qu2dEET4ho0GRm1U2R0wUE1Mn8YjpWQ.jpg" title="Apple Foundation is dumb" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Like the other poster, I‚Äôve found Apple Foundational model to disapprove of lots of content. It‚Äôs too safe. Too corporate.&lt;/p&gt; &lt;p&gt;This is the most innocuous example I could come up with. Also attached proof that it even indirectly avoids the word. Google‚Äôs model gives me accurate info. &lt;/p&gt; &lt;p&gt;(FYI in case you are not in a region that has chiggers‚Ä¶ they are little red bugs that bite you, no relation to a word that it rhymes with at all)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PM_ME_UR_COFFEE_CUPS"&gt; /u/PM_ME_UR_COFFEE_CUPS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1of8xl2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1of8xl2/apple_foundation_is_dumb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1of8xl2/apple_foundation_is_dumb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-24T20:45:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1ofasus</id>
    <title>MiniMax M2 is 230B-A10B</title>
    <updated>2025-10-24T22:03:31+00:00</updated>
    <author>
      <name>/u/codys12</name>
      <uri>https://old.reddit.com/user/codys12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofasus/minimax_m2_is_230ba10b/"&gt; &lt;img alt="MiniMax M2 is 230B-A10B" src="https://preview.redd.it/f45v1dx7u4xf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9c77e931b8bddbffa454a9542abfb528e2be6fc4" title="MiniMax M2 is 230B-A10B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/codys12"&gt; /u/codys12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/f45v1dx7u4xf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofasus/minimax_m2_is_230ba10b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ofasus/minimax_m2_is_230ba10b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-24T22:03:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1ofrbcy</id>
    <title>VSORA Launches Europe‚Äôs Most Powerful AI Inference Chip</title>
    <updated>2025-10-25T13:17:00+00:00</updated>
    <author>
      <name>/u/RG54415</name>
      <uri>https://old.reddit.com/user/RG54415</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofrbcy/vsora_launches_europes_most_powerful_ai_inference/"&gt; &lt;img alt="VSORA Launches Europe‚Äôs Most Powerful AI Inference Chip" src="https://external-preview.redd.it/yZMJwzqfo-1KodiPMX_oqs-WZlqWdJqQO_vA61_E2Yg.png?width=216&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f9bde9afeea2ecc09d17d3b9a496407c3e11f9c3" title="VSORA Launches Europe‚Äôs Most Powerful AI Inference Chip" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Some of its features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Fully programmable&lt;/li&gt; &lt;li&gt;Algorithm agnostic&lt;/li&gt; &lt;li&gt;Host processor agnostic&lt;/li&gt; &lt;li&gt;RISC-V cores to offload host &amp;amp; run AI completely on-chip&lt;/li&gt; &lt;li&gt;Tensorcore (dense) &lt;ul&gt; &lt;li&gt;fp8: 3200 Tflops &lt;/li&gt; &lt;li&gt;fp16: 800 Tflops&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;General Purpose &lt;ul&gt; &lt;li&gt;fp8/int8: 100 Tflops&lt;/li&gt; &lt;li&gt;fp16/int16: 50 Tflops&lt;/li&gt; &lt;li&gt;fp32/int32: 25 Tflops&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Capacity HBM: 288GB&lt;/li&gt; &lt;li&gt;Throughput HBM: 8 TB/s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Seems like a big win for local AI models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RG54415"&gt; /u/RG54415 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://finance.yahoo.com/news/vsora-launches-europe-most-powerful-121700744.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofrbcy/vsora_launches_europes_most_powerful_ai_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ofrbcy/vsora_launches_europes_most_powerful_ai_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-25T13:17:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1ofn0nb</id>
    <title>meituan-longcat/LongCat-Video ¬∑ Hugging Face</title>
    <updated>2025-10-25T09:11:57+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofn0nb/meituanlongcatlongcatvideo_hugging_face/"&gt; &lt;img alt="meituan-longcat/LongCat-Video ¬∑ Hugging Face" src="https://external-preview.redd.it/Yt-ii7zJ14OIVDQLAiu4mswoqHj6Da2dRjpAXgOm1a4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a68e500343ff77272afd8ad706e2ac971ab2d083" title="meituan-longcat/LongCat-Video ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A foundational video generation model with 13.6B parameters, delivering strong performance across Text-to-Video, Image-to-Video, and Video-Continuation generation tasks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/meituan-longcat/LongCat-Video"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofn0nb/meituanlongcatlongcatvideo_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ofn0nb/meituanlongcatlongcatvideo_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-25T09:11:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1of5ywl</id>
    <title>What‚Äôs even the goddamn point?</title>
    <updated>2025-10-24T18:47:20+00:00</updated>
    <author>
      <name>/u/ChockyBlox</name>
      <uri>https://old.reddit.com/user/ChockyBlox</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1of5ywl/whats_even_the_goddamn_point/"&gt; &lt;img alt="What‚Äôs even the goddamn point?" src="https://preview.redd.it/9fjtexb9v3xf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0d398d419e5a3d539c3f2c82b07408ef22f90899" title="What‚Äôs even the goddamn point?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;To be fair I will probably never use this model for any real use cases, but these corporations do need to go a little easy on the restrictions and be less paranoid.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ChockyBlox"&gt; /u/ChockyBlox &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9fjtexb9v3xf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1of5ywl/whats_even_the_goddamn_point/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1of5ywl/whats_even_the_goddamn_point/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-24T18:47:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1obqkpe</id>
    <title>Best Local LLMs - October 2025</title>
    <updated>2025-10-20T19:06:06+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Welcome to the first monthly &amp;quot;Best Local LLMs&amp;quot; post!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Share what your favorite models are right now &lt;strong&gt;and why.&lt;/strong&gt; Given the nature of the beast in evaluating LLMs (untrustworthiness of benchmarks, immature tooling, intrinsic stochasticity), please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Applications&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;General&lt;/li&gt; &lt;li&gt;Agentic/Tool Use&lt;/li&gt; &lt;li&gt;Coding&lt;/li&gt; &lt;li&gt;Creative Writing/RP&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;(look for the top level comments for each Application and please thread your responses under that)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obqkpe/best_local_llms_october_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obqkpe/best_local_llms_october_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1obqkpe/best_local_llms_october_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T19:06:06+00:00</published>
  </entry>
</feed>
