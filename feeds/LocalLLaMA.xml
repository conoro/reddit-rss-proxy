<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-03T13:46:49+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1pcp927</id>
    <title>Qwen 3 0.6B, 1.7B and 4B</title>
    <updated>2025-12-03T00:16:53+00:00</updated>
    <author>
      <name>/u/SlowFail2433</name>
      <uri>https://old.reddit.com/user/SlowFail2433</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What have people been building with the small Qwens?&lt;/p&gt; &lt;p&gt;Which tasks have you found them to perform well on or poorly?&lt;/p&gt; &lt;p&gt;So far my experience has been that you really need the 1.7B most of the time over the 0.6B, as it gains the ability to handle simple text tasks more reliably, but in a basic manner. The jump up to 4B makes it much more robust with broader knowledge and has almost a 7B feel. The 1.7B is adequate enough for simple tasks though.&lt;/p&gt; &lt;p&gt;Does this experience track?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SlowFail2433"&gt; /u/SlowFail2433 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcp927/qwen_3_06b_17b_and_4b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcp927/qwen_3_06b_17b_and_4b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pcp927/qwen_3_06b_17b_and_4b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T00:16:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1pd3uvy</id>
    <title>Can we expect better LLM hardware in 2026?</title>
    <updated>2025-12-03T13:16:07+00:00</updated>
    <author>
      <name>/u/Bitter-College8786</name>
      <uri>https://old.reddit.com/user/Bitter-College8786</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I mean with a lot of fast(!) VRAM.&lt;/p&gt; &lt;p&gt;DGX spark and AMD AI Max have really low memory speeds.&lt;/p&gt; &lt;p&gt;China is releasing so many open source models, when will they come with cheap hardware that we can run them?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bitter-College8786"&gt; /u/Bitter-College8786 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd3uvy/can_we_expect_better_llm_hardware_in_2026/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd3uvy/can_we_expect_better_llm_hardware_in_2026/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pd3uvy/can_we_expect_better_llm_hardware_in_2026/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T13:16:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1pchuvk</id>
    <title>Qwen3 VL built from scratch with PyTorch</title>
    <updated>2025-12-02T19:26:13+00:00</updated>
    <author>
      <name>/u/No-Compote-6794</name>
      <uri>https://old.reddit.com/user/No-Compote-6794</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pchuvk/qwen3_vl_built_from_scratch_with_pytorch/"&gt; &lt;img alt="Qwen3 VL built from scratch with PyTorch" src="https://preview.redd.it/m7gqtnm2du4g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=693e3c8c4453fc8e36a6a9ec7a0c64542089aeab" title="Qwen3 VL built from scratch with PyTorch" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I updated my &lt;a href="https://github.com/Emericen/tiny-qwen"&gt;Tiny-Qwen repo&lt;/a&gt; to support Qwen3 VL. It is a minimal PyTorch re-implementation of the open source model served behind a fancy CLI.&lt;/p&gt; &lt;p&gt;The code is IMO quite simple and easy to follow and hack around. If you are looking to learn how multi-modal LLM's work and find Hugging Face Transformers code verbose, then this repo is for you :)&lt;/p&gt; &lt;p&gt;This line of work is heavily inspired by Andrej Karpathy's nanoGPT (albeit I'm not nearly as good as he is). I always wished that style could be used on open source models so I did it myself. You can also find older versions of Qwen in the same repo as well as DeepSeek R1. I've linked them in the repo's readme.&lt;/p&gt; &lt;p&gt;If you find this helpful, please please please star the repo ü§ó&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Compote-6794"&gt; /u/No-Compote-6794 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/m7gqtnm2du4g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pchuvk/qwen3_vl_built_from_scratch_with_pytorch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pchuvk/qwen3_vl_built_from_scratch_with_pytorch/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-02T19:26:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1pd2x8u</id>
    <title>I built an offline AI chat app that automatically pulls Wikipedia articles for factual answers - runs completely locally with Ollama</title>
    <updated>2025-12-03T12:32:49+00:00</updated>
    <author>
      <name>/u/Smart-Competition200</name>
      <uri>https://old.reddit.com/user/Smart-Competition200</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/imDelivered/WikiRAG"&gt;https://github.com/imDelivered/WikiRAG&lt;/a&gt;&lt;/p&gt; &lt;p&gt;‚Ä¢ Runs 100% offline - no internet needed after initial setup&lt;/p&gt; &lt;p&gt;‚Ä¢ Automatically injects relevant Wikipedia articles into AI responses (RAG)&lt;/p&gt; &lt;p&gt;‚Ä¢ Privacy-focused - everything runs locally on your machine&lt;/p&gt; &lt;p&gt;‚Ä¢ Works with any Ollama model (llama3.2, dolphin-llama3, etc.)&lt;/p&gt; &lt;p&gt;‚Ä¢ Simple setup script handles everything automatically&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Smart-Competition200"&gt; /u/Smart-Competition200 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd2x8u/i_built_an_offline_ai_chat_app_that_automatically/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd2x8u/i_built_an_offline_ai_chat_app_that_automatically/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pd2x8u/i_built_an_offline_ai_chat_app_that_automatically/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T12:32:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1pctktp</id>
    <title>Open Source Alternative to NotebookLM</title>
    <updated>2025-12-03T03:34:26+00:00</updated>
    <author>
      <name>/u/Uiqueblhats</name>
      <uri>https://old.reddit.com/user/Uiqueblhats</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those of you who aren't familiar with SurfSense, it aims to be the &lt;strong&gt;open-source alternative to NotebookLM, Perplexity, or Glean.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;In short, it's a Highly Customizable AI Research Agent that connects to your personal external sources and Search Engines (SearxNG, Tavily, LinkUp), Slack, Linear, Jira, ClickUp, Confluence, Gmail, Notion, YouTube, GitHub, Discord, Airtable, Google Calendar and more to come.&lt;/p&gt; &lt;p&gt;I'm looking for contributors. If you're interested in AI agents, RAG, browser extensions, or building open-source research tools, this is a great place to jump in.&lt;/p&gt; &lt;p&gt;Here‚Äôs a quick look at what SurfSense offers right now:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;RBAC (Role Based Access for Teams)&lt;/li&gt; &lt;li&gt;Notion Like Document Editing experience&lt;/li&gt; &lt;li&gt;Supports 100+ LLMs&lt;/li&gt; &lt;li&gt;Supports local Ollama or vLLM setups&lt;/li&gt; &lt;li&gt;6000+ Embedding Models&lt;/li&gt; &lt;li&gt;50+ File extensions supported (Added Docling recently)&lt;/li&gt; &lt;li&gt;Podcasts support with local TTS providers (Kokoro TTS)&lt;/li&gt; &lt;li&gt;Connects with 15+ external sources such as Search Engines, Slack, Notion, Gmail, Notion, Confluence etc&lt;/li&gt; &lt;li&gt;Cross-Browser Extension to let you save any dynamic webpage you want, including authenticated content.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Upcoming Planned Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Note Management (Like Notion)&lt;/li&gt; &lt;li&gt;Multi Collaborative Chats.&lt;/li&gt; &lt;li&gt;Multi Collaborative Documents.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Interested in contributing?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;SurfSense is completely open source, with an active roadmap. Whether you want to pick up an existing feature, suggest something new, fix bugs, or help improve docs, you're welcome to join in.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/MODSetter/SurfSense"&gt;https://github.com/MODSetter/SurfSense&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Uiqueblhats"&gt; /u/Uiqueblhats &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pctktp/open_source_alternative_to_notebooklm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pctktp/open_source_alternative_to_notebooklm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pctktp/open_source_alternative_to_notebooklm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T03:34:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1pcrc1j</id>
    <title>apple/starflow ¬∑ Hugging Face</title>
    <updated>2025-12-03T01:50:16+00:00</updated>
    <author>
      <name>/u/ab2377</name>
      <uri>https://old.reddit.com/user/ab2377</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcrc1j/applestarflow_hugging_face/"&gt; &lt;img alt="apple/starflow ¬∑ Hugging Face" src="https://external-preview.redd.it/9MKZtuMzr_COQCqOF3P3AvNoeDUxBXmS4TeVDsiZt8c.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=00b6a986e0fe8fa76f89639e776eee93811f8828" title="apple/starflow ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;STARFlow introduces a novel transformer autoregressive flow architecture that combines the expressiveness of autoregressive models with the efficiency of normalizing flows. The model achieves state-of-the-art results in both text-to-image and text-to-video generation tasks.&lt;/p&gt; &lt;p&gt;STARFlow: Scaling Latent Normalizing Flows for High-resolution Image Synthesis (NeurIPS 2025 Spotlight) STARFlow-V: End-to-End Video Generative Modeling with Normalizing Flows (Arxiv)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ab2377"&gt; /u/ab2377 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/apple/starflow"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcrc1j/applestarflow_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pcrc1j/applestarflow_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T01:50:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1pc6i8v</id>
    <title>Only the real ones remember (he is still the contributor with the most likes for his models)</title>
    <updated>2025-12-02T11:52:35+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pc6i8v/only_the_real_ones_remember_he_is_still_the/"&gt; &lt;img alt="Only the real ones remember (he is still the contributor with the most likes for his models)" src="https://b.thumbs.redditmedia.com/jy0GG6iG37_fVgalRs58BxRL8j79pwfEfAMtQhgLw_c.jpg" title="Only the real ones remember (he is still the contributor with the most likes for his models)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hugging Face space by TCTF: Top Contributors To Follow - November 2025: &lt;a href="https://huggingface.co/spaces/TCTF/TCTF"&gt;https://huggingface.co/spaces/TCTF/TCTF&lt;/a&gt;&lt;br /&gt; Team mradermacher and Bartowski on the podium, legends.&lt;br /&gt; From Yaƒüƒ±z √áalƒ±k on ùïè: &lt;a href="https://x.com/Weyaxi/status/1995814979543371869"&gt;https://x.com/Weyaxi/status/1995814979543371869&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pc6i8v"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pc6i8v/only_the_real_ones_remember_he_is_still_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pc6i8v/only_the_real_ones_remember_he_is_still_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-02T11:52:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1pcjjsk</id>
    <title>minimax m2 tops official SWE-bench leaderboard, followed by deepseek v3.2 and glm 4.6 [details on step limits, cost efficiency, etc. in post]</title>
    <updated>2025-12-02T20:29:33+00:00</updated>
    <author>
      <name>/u/klieret</name>
      <uri>https://old.reddit.com/user/klieret</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcjjsk/minimax_m2_tops_official_swebench_leaderboard/"&gt; &lt;img alt="minimax m2 tops official SWE-bench leaderboard, followed by deepseek v3.2 and glm 4.6 [details on step limits, cost efficiency, etc. in post]" src="https://a.thumbs.redditmedia.com/ys-1wDn79XSFknvv5ToJiU1nmLysxbP3_tau9tGK2u4.jpg" title="minimax m2 tops official SWE-bench leaderboard, followed by deepseek v3.2 and glm 4.6 [details on step limits, cost efficiency, etc. in post]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! I'm from the SWE-bench team. We've just finished evaluating the new deepseek &amp;amp; GLM, and minimax using a minimal agent&lt;/p&gt; &lt;p&gt;Minimax M2 is best open source model (but expensive!). Deepseek v3.2 reasoning close behind, very cheap, but very slow. GLM 4.6 reaches good performance (same as qwen3 coder 480b a35b) fast and cheap. Compared to the non-open source models, the performance is still relatively low with Gemini 3 pro and Claude 4.5 Opus medium being around 74%&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/uaudqnr5nu4g1.png?width=3593&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=27e997e9af142ed512db76961d22ded868a857ba"&gt;https://preview.redd.it/uaudqnr5nu4g1.png?width=3593&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=27e997e9af142ed512db76961d22ded868a857ba&lt;/a&gt;&lt;/p&gt; &lt;p&gt;All costs are calculated with the official API cost at the time of release.&lt;/p&gt; &lt;p&gt;Models take different amount of steps, with minimax taking the most and deepseek taking comparatively few. This is probably a big factor in minimax being pretty pricy at the moment.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ggrkjftfnu4g1.png?width=2345&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=574c785d616c94a168dac9476f7b57bfb927186e"&gt;https://preview.redd.it/ggrkjftfnu4g1.png?width=2345&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=574c785d616c94a168dac9476f7b57bfb927186e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;However, you also cannot just stop minimax early by setting a low step limit, because it actually still solves quite a few instances at high step counts (&amp;gt; 150 and some even &amp;gt;200 steps). That definitely speaks to the ability to do long horizon tasks, though of course most people want to have results earlier. For deepseek you can already stop at around 100 steps, there's a very clear flattening effect there.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/zghs1m94ou4g1.png?width=2092&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6b7256b37e6c935e92ca5b783d91cbec9b2924e0"&gt;https://preview.redd.it/zghs1m94ou4g1.png?width=2092&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6b7256b37e6c935e92ca5b783d91cbec9b2924e0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In terms of cost efficiency (again, official API cost), you can trade off performance vs cost if you reduce the step limit. Here's the resulting cost-performance lines that you can get. If you don't mind the very long reasoning times of deepseek, clearly this is your most cost efficient bet at the moment. Otherwise, GLM seems very cost efficient.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/rt6rgt26ou4g1.png?width=2092&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cc0bd71026f671143894e52685d984dc332525ec"&gt;https://preview.redd.it/rt6rgt26ou4g1.png?width=2092&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cc0bd71026f671143894e52685d984dc332525ec&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Some small evaluation notes: We used T=0 for all models except GLM (T=1). We don't want to tune temperature for this eval, so it's either T=0 or T=1 for all. To parse the action from the agent we use &amp;quot;triple backticks&amp;quot; except for minimax that really didn't like that, so we used &amp;quot;xml style&amp;quot; parsing.&lt;/p&gt; &lt;p&gt;You can find the full config/prompts here: &lt;a href="https://github.com/SWE-agent/mini-swe-agent/blob/main/src/minisweagent/config/extra/swebench.yaml"&gt;https://github.com/SWE-agent/mini-swe-agent/blob/main/src/minisweagent/config/extra/swebench.yaml&lt;/a&gt; (resp &lt;a href="https://github.com/SWE-agent/mini-swe-agent/blob/main/src/minisweagent/config/extra/swebench_xml.yaml"&gt;https://github.com/SWE-agent/mini-swe-agent/blob/main/src/minisweagent/config/extra/swebench_xml.yaml&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;The full leaderboard is at &lt;a href="http://swebench.com"&gt;swebench.com&lt;/a&gt; (I'll update it very soon, at which point you can create your own plots &amp;amp; browse the trajectories from your browser). The trajectories are already available in our s3 container.&lt;/p&gt; &lt;p&gt;mini-swe-agent is open source at &lt;a href="https://github.com/SWE-agent/mini-swe-agent/"&gt;https://github.com/SWE-agent/mini-swe-agent/&lt;/a&gt;. The docs contain the full example of how to evaluate on SWE-bench (it only takes 2 commands and $15 for deepseek)&lt;/p&gt; &lt;p&gt;Let us know what models to evaluate next (we hope to add more open source models soon)!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/klieret"&gt; /u/klieret &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcjjsk/minimax_m2_tops_official_swebench_leaderboard/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcjjsk/minimax_m2_tops_official_swebench_leaderboard/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pcjjsk/minimax_m2_tops_official_swebench_leaderboard/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-02T20:29:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1pcwffb</id>
    <title>Llama 3.1 70B + one prompt now beats Claude 3.5 Sonnet (96.9% on Arena-Hard-Auto, 4% refusals)</title>
    <updated>2025-12-03T06:00:57+00:00</updated>
    <author>
      <name>/u/NoSir261</name>
      <uri>https://old.reddit.com/user/NoSir261</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I spent the last few weeks iterating a single system prompt until stock Llama-3.1-70B-Instruct started outperforming Claude 3.5 Sonnet on the hardest blind arena benchmark. Results (100% reproducible):&lt;/p&gt; &lt;p&gt;‚Ä¢ 96.4‚Äì96.9% win rate on Arena-Hard-Auto (vs Sonnet‚Äôs 94.7%) ‚Ä¢ Only 4% refusals (base model is ~25‚Äì30%) ‚Ä¢ Dense, creative, actually useful output&lt;/p&gt; &lt;p&gt;No fine-tune, no LoRA, no quantization tricks. Just one prompt.&lt;/p&gt; &lt;p&gt;Full X thread with JSONL proof + evals: &lt;a href="https://x.com/BrSanch/status/1864123456789012345"&gt;https://x.com/BrSanch/status/1864123456789012345&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I think prompt engineering can do a lot more than most people think it can.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NoSir261"&gt; /u/NoSir261 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcwffb/llama_31_70b_one_prompt_now_beats_claude_35/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcwffb/llama_31_70b_one_prompt_now_beats_claude_35/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pcwffb/llama_31_70b_one_prompt_now_beats_claude_35/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T06:00:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1pd36b7</id>
    <title>LLM Council - Multi-model AI with democratic voting (Enhanced fork with 5 production features)</title>
    <updated>2025-12-03T12:44:59+00:00</updated>
    <author>
      <name>/u/Distinct_Site_3462</name>
      <uri>https://old.reddit.com/user/Distinct_Site_3462</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been working on an enhanced version of Karpathy's LLM Council that adds production-ready features while keeping the original vision intact.&lt;/p&gt; &lt;h1&gt;What is LLM Council?&lt;/h1&gt; &lt;p&gt;Instead of asking one LLM, you ask multiple models simultaneously. They each respond, then anonymously rank each other's answers, and a chairman synthesizes the final response. Think of it as &amp;quot;democratic AI decision-making.&amp;quot;&lt;/p&gt; &lt;h1&gt;What I Added (5 Features):&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;üéØ &lt;strong&gt;TOON Integration&lt;/strong&gt; - 30-60% token savings with optimized data format&lt;/li&gt; &lt;li&gt;üíæ &lt;strong&gt;Multi-Database Support&lt;/strong&gt; - JSON (default), PostgreSQL, or MySQL&lt;/li&gt; &lt;li&gt;üí¨ &lt;strong&gt;Context &amp;amp; Follow-ups&lt;/strong&gt; - Natural multi-turn conversations with memory&lt;/li&gt; &lt;li&gt;üõ†Ô∏è &lt;strong&gt;AI Tools&lt;/strong&gt; - Calculator, Wikipedia, ArXiv, DuckDuckGo, Yahoo Finance \&lt;/li&gt; &lt;li&gt;‚öôÔ∏è &lt;strong&gt;Conversation Management&lt;/strong&gt; - Delete, edit titles, temporary chat mode&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Tech Stack:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Backend&lt;/strong&gt;: FastAPI, LangChain, SQLAlchemy, ChromaDB&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Frontend&lt;/strong&gt;: React + Vite with Server-Sent Events&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Models&lt;/strong&gt;: Any via OpenRouter (GPT-5.1, Gemini, Claude, Grok, etc.)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Free Features:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;All 5 free tools enabled by default&lt;/li&gt; &lt;li&gt;Local embeddings with HuggingFace (no API needed)&lt;/li&gt; &lt;li&gt;JSON storage (zero setup)&lt;/li&gt; &lt;li&gt;Optional: Tavily search, OpenAI embeddings&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;GitHub&lt;/strong&gt;: &lt;a href="https://github.com/Reeteshrajesh/llm-council"&gt;https://github.com/Reeteshrajesh/llm-council&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Original concept by &lt;a href="https://github.com/karpathy/llm-council"&gt;@karpathy&lt;/a&gt;. This is an enhanced fork with professional features for production use.&lt;/p&gt; &lt;p&gt;Happy to answer questions! üöÄ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Distinct_Site_3462"&gt; /u/Distinct_Site_3462 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd36b7/llm_council_multimodel_ai_with_democratic_voting/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd36b7/llm_council_multimodel_ai_with_democratic_voting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pd36b7/llm_council_multimodel_ai_with_democratic_voting/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T12:44:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1pcbr10</id>
    <title>Ministral WebGPU: Run Mistral's new multimodal models 100% locally in your browser.</title>
    <updated>2025-12-02T15:43:30+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcbr10/ministral_webgpu_run_mistrals_new_multimodal/"&gt; &lt;img alt="Ministral WebGPU: Run Mistral's new multimodal models 100% locally in your browser." src="https://external-preview.redd.it/a2FpOGJodms5dDRnMVOJ9FmD9w2-LMCVXdFIiBg8ZPjaS6tgqxX1OyhMPvmT.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f21be91171f3f1c63baa540518e8447e2d1bdca9" title="Ministral WebGPU: Run Mistral's new multimodal models 100% locally in your browser." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Today, Mistral released &lt;strong&gt;Mistral 3&lt;/strong&gt;, a family of multimodal models, including three start-of-the-art dense models (3B, 8B, and 14B) and Mistral Large 3 (675B, 41B active). All Apache 2.0! ü§ó Surprisingly, the 3B is small enough to run 100% locally in your browser with WebGPU acceleration, powered by Transformers.js.&lt;/p&gt; &lt;p&gt;Link to demo: &lt;a href="https://huggingface.co/spaces/mistralai/Ministral_3B_WebGPU"&gt;https://huggingface.co/spaces/mistralai/Ministral_3B_WebGPU&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/vwrcg6vk9t4g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcbr10/ministral_webgpu_run_mistrals_new_multimodal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pcbr10/ministral_webgpu_run_mistrals_new_multimodal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-02T15:43:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1pcv0kd</id>
    <title>Maaza Orchestrator v1.2 ‚Äî 9.6M params, 62.9 % on hard adversarial tool-calling, 39 ms latency</title>
    <updated>2025-12-03T04:45:28+00:00</updated>
    <author>
      <name>/u/CycleCore_Tech</name>
      <uri>https://old.reddit.com/user/CycleCore_Tech</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just shipped v1.2 of Maaza Orchestrator (9.6 M params). &lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Split&lt;/th&gt; &lt;th align="left"&gt;v1.0&lt;/th&gt; &lt;th align="left"&gt;v1.2&lt;/th&gt; &lt;th align="left"&gt;Œî&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;In-distribution accuracy&lt;/td&gt; &lt;td align="left"&gt;88.0%&lt;/td&gt; &lt;td align="left"&gt;86.0%&lt;/td&gt; &lt;td align="left"&gt;‚àí2.0%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Adversarial tool-calling&lt;/td&gt; &lt;td align="left"&gt;26.6%&lt;/td&gt; &lt;td align="left"&gt;62.9%&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;+36.3%&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;p50 latency (CPU)&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;33.4ms&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;39.4ms&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;+6.0ms&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;The adversarial set is 124 held-out examples across 36 tools. A few representative ones so you can judge the difficulty:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;‚Äúlmao just text that to them‚Äù ‚Üí email_send&lt;/li&gt; &lt;li&gt;‚Äúturn this into spokenshit‚Äù ‚Üí voice_mcp&lt;/li&gt; &lt;li&gt;‚Äútime to rip and tear‚Äù ‚Üí doom_mcp&lt;/li&gt; &lt;li&gt;‚Äúwassup with my ethereum val‚Äù ‚Üí crypto_lookup&lt;/li&gt; &lt;li&gt;‚Äúplz execcute dis py code, gr8 tnx‚Äù ‚Üí code_execute_python&lt;/li&gt; &lt;li&gt;‚Äúweather or not?‚Äù ‚Üí weather_lookup (pun + typo)&lt;/li&gt; &lt;li&gt;‚Äúwiggle to &lt;a href="http://www.example.com%E2%80%9D"&gt;www.example.com‚Äù&lt;/a&gt; ‚Üí puppeteer_navigate&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Most examples stack 2‚Äì3 perturbations (slang + typos + abbreviations + cultural references). A vanilla 9.6 M model would probably sit below 30 % here.&lt;/p&gt; &lt;p&gt;The +36% came from one data-centric fine-tune: ~500 diverse adversarial seeds ‚Üí 10√ó upsampled ‚Üí 5 epochs. &lt;/p&gt; &lt;p&gt;‚Ä¢ HF: &lt;a href="https://huggingface.co/CycleCoreTechnologies/maaza-nlm-orchestrator-9.6m-v1.2"&gt;https://huggingface.co/CycleCoreTechnologies/maaza-nlm-orchestrator-9.6m-v1.2&lt;/a&gt;&lt;br /&gt; ‚Ä¢ Full 124-example held-out adversarial set (JSONL)&lt;br /&gt; ‚Ä¢ Training split &amp;amp; exact upsampling script&lt;br /&gt; ‚Ä¢ Apache 2.0&lt;/p&gt; &lt;p&gt;Happy to share the seed adversarial list. (v1.3 with 18√ó upsampling is already training).&lt;/p&gt; &lt;p&gt;Thanks for reading. Feedback always welcome. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CycleCore_Tech"&gt; /u/CycleCore_Tech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcv0kd/maaza_orchestrator_v12_96m_params_629_on_hard/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcv0kd/maaza_orchestrator_v12_96m_params_629_on_hard/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pcv0kd/maaza_orchestrator_v12_96m_params_629_on_hard/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T04:45:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1pcurp8</id>
    <title>Q: When will there be fast and competent SLMs for laptops?</title>
    <updated>2025-12-03T04:32:47+00:00</updated>
    <author>
      <name>/u/TomLucidor</name>
      <uri>https://old.reddit.com/user/TomLucidor</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It has been a whole year since Qwen2.5-32B been published for people to self-host their coding models. Similar models for RP probably exists before then, but the ideal of a general purpose portable model is still here. Yet, the news kept showing more techniques!&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Qwen3-30B-A3B and GPT-OSS-20B both uses Mixture-of-Experts instead of dense layers for their SLM&lt;/li&gt; &lt;li&gt;Kimi-Linear and Qwen3-Next-80B-A3B moved along to use &amp;quot;mixed attention&amp;quot; (majority of layers with linear attention) to speed things up AND have longer contexts&lt;/li&gt; &lt;li&gt;Not enough people getting into ternary attention like &lt;strong&gt;BitNet a4.8&lt;/strong&gt; / &lt;strong&gt;BitNet v2&lt;/strong&gt; &lt;a href="https://arxiv.org/html/2504.18415v2"&gt;https://arxiv.org/html/2504.18415v2&lt;/a&gt; or ternary quantization (PTQ) &lt;a href="https://arxiv.org/html/2509.23809v2"&gt;https://arxiv.org/html/2509.23809v2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Whatever layer routing is to reduce the amount of RAM needed, including &lt;strong&gt;Ouro-2.6B-Thinking&lt;/strong&gt; these days and &lt;strong&gt;Mixture-of-Depths&lt;/strong&gt; back in 2024&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Are all of these different techniques conflicting with one another? If it is just a lack of funding for fine-tuning/modding an existing SLM into something fast (assuming QAFT and RL), how much would it cost to crowdfund a project like this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TomLucidor"&gt; /u/TomLucidor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcurp8/q_when_will_there_be_fast_and_competent_slms_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcurp8/q_when_will_there_be_fast_and_competent_slms_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pcurp8/q_when_will_there_be_fast_and_competent_slms_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T04:32:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1pcb50r</id>
    <title>Ministral-3 has been released</title>
    <updated>2025-12-02T15:20:25+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcb50r/ministral3_has_been_released/"&gt; &lt;img alt="Ministral-3 has been released" src="https://b.thumbs.redditmedia.com/a0DyjW1DyWh-ddE3J9WOyZjKJiBbmcXRGjqX2TH__QM.jpg" title="Ministral-3 has been released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/mistralai/Ministral-3-14B-Reasoning-2512"&gt;https://huggingface.co/mistralai/Ministral-3-14B-Reasoning-2512&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mistralai/Ministral-3-14B-Instruct-2512"&gt;https://huggingface.co/mistralai/Ministral-3-14B-Instruct-2512&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mistralai/Ministral-3-14B-Base-2512"&gt;https://huggingface.co/mistralai/Ministral-3-14B-Base-2512&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The largest model in the Ministral 3 family, &lt;strong&gt;Ministral 3 14B&lt;/strong&gt; offers frontier capabilities and performance comparable to its larger &lt;a href="https://huggingface.co/mistralai/Mistral-Small-3.2-Instruct-2506"&gt;Mistral Small 3.2 24B&lt;/a&gt; counterpart. A powerful and efficient language model with vision capabilities.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mistralai/Ministral-3-8B-Reasoning-2512"&gt;https://huggingface.co/mistralai/Ministral-3-8B-Reasoning-2512&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mistralai/Ministral-3-8B-Instruct-2512"&gt;https://huggingface.co/mistralai/Ministral-3-8B-Instruct-2512&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mistralai/Ministral-3-8B-Base-2512"&gt;https://huggingface.co/mistralai/Ministral-3-8B-Base-2512&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A balanced model in the Ministral 3 family, &lt;strong&gt;Ministral 3 8B&lt;/strong&gt; is a powerful, efficient tiny language model with vision capabilities.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mistralai/Ministral-3-3B-Reasoning-2512"&gt;https://huggingface.co/mistralai/Ministral-3-3B-Reasoning-2512&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mistralai/Ministral-3-3B-Instruct-2512"&gt;https://huggingface.co/mistralai/Ministral-3-3B-Instruct-2512&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mistralai/Ministral-3-3B-Base-2512"&gt;https://huggingface.co/mistralai/Ministral-3-3B-Base-2512&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The smallest model in the Ministral 3 family, &lt;strong&gt;Ministral 3 3B&lt;/strong&gt; is a powerful, efficient tiny language model with vision capabilities.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/471e4lma6t4g1.png?width=1078&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c23d37e6a361041132ccec451c0a03921acc6e13"&gt;https://preview.redd.it/471e4lma6t4g1.png?width=1078&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c23d37e6a361041132ccec451c0a03921acc6e13&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/c2szd14b6t4g1.png?width=1210&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3d97fc5e8626f25f8c13a5b159e6351976f45de5"&gt;https://preview.redd.it/c2szd14b6t4g1.png?width=1210&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3d97fc5e8626f25f8c13a5b159e6351976f45de5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/unsloth/Ministral-3-14B-Reasoning-2512-GGUF"&gt;https://huggingface.co/unsloth/Ministral-3-14B-Reasoning-2512-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/unsloth/Ministral-3-14B-Instruct-2512-GGUF"&gt;https://huggingface.co/unsloth/Ministral-3-14B-Instruct-2512-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/unsloth/Ministral-3-8B-Reasoning-2512-GGUF"&gt;https://huggingface.co/unsloth/Ministral-3-8B-Reasoning-2512-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/unsloth/Ministral-3-8B-Instruct-2512-GGUF"&gt;https://huggingface.co/unsloth/Ministral-3-8B-Instruct-2512-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/unsloth/Ministral-3-3B-Reasoning-2512-GGUF"&gt;https://huggingface.co/unsloth/Ministral-3-3B-Reasoning-2512-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/unsloth/Ministral-3-3B-Instruct-2512-GGUF"&gt;https://huggingface.co/unsloth/Ministral-3-3B-Instruct-2512-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcb50r/ministral3_has_been_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcb50r/ministral3_has_been_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pcb50r/ministral3_has_been_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-02T15:20:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1pcjqjs</id>
    <title>Ministral 3 models were pruned from Mistral Small 3.1</title>
    <updated>2025-12-02T20:36:32+00:00</updated>
    <author>
      <name>/u/brown2green</name>
      <uri>https://old.reddit.com/user/brown2green</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcjqjs/ministral_3_models_were_pruned_from_mistral_small/"&gt; &lt;img alt="Ministral 3 models were pruned from Mistral Small 3.1" src="https://preview.redd.it/bte4gtp1qu4g1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bec6f7045ad754997a36d5294eedaa2112246178" title="Ministral 3 models were pruned from Mistral Small 3.1" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/brown2green"&gt; /u/brown2green &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bte4gtp1qu4g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcjqjs/ministral_3_models_were_pruned_from_mistral_small/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pcjqjs/ministral_3_models_were_pruned_from_mistral_small/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-02T20:36:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1pcia1t</id>
    <title>DeepSeek V3.2 Speciale dominates my math bench while being ~15√ó cheaper than GPT-5.1 High</title>
    <updated>2025-12-02T19:41:57+00:00</updated>
    <author>
      <name>/u/kyousukegum</name>
      <uri>https://old.reddit.com/user/kyousukegum</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcia1t/deepseek_v32_speciale_dominates_my_math_bench/"&gt; &lt;img alt="DeepSeek V3.2 Speciale dominates my math bench while being ~15√ó cheaper than GPT-5.1 High" src="https://a.thumbs.redditmedia.com/TJzNTRI6aFSLhjDdzBZtSFW1nl-mDnldDNZ8ONcsRV0.jpg" title="DeepSeek V3.2 Speciale dominates my math bench while being ~15√ó cheaper than GPT-5.1 High" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/cwyrxaxneu4g1.png?width=3600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=02ddc7b09fb3264f23ada56612af21febfe93bad"&gt;https://preview.redd.it/cwyrxaxneu4g1.png?width=3600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=02ddc7b09fb3264f23ada56612af21febfe93bad&lt;/a&gt;&lt;/p&gt; &lt;p&gt;for context on how impressive this is, I couldn't believe my eyes and had to double-check the results multiple times. The problems in this category are very hard like in the same ballpark as IMO P6.&lt;br /&gt; &lt;a href="https://x.com/gum1h0x/status/1995915458612953419"&gt;https://x.com/gum1h0x/status/1995915458612953419&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kyousukegum"&gt; /u/kyousukegum &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcia1t/deepseek_v32_speciale_dominates_my_math_bench/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcia1t/deepseek_v32_speciale_dominates_my_math_bench/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pcia1t/deepseek_v32_speciale_dominates_my_math_bench/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-02T19:41:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1pcayfs</id>
    <title>Mistral 3 Blog post</title>
    <updated>2025-12-02T15:13:14+00:00</updated>
    <author>
      <name>/u/rerri</name>
      <uri>https://old.reddit.com/user/rerri</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcayfs/mistral_3_blog_post/"&gt; &lt;img alt="Mistral 3 Blog post" src="https://external-preview.redd.it/QLQU1soiMTzFAm8GzW6EPDbaX5jrcYYFqy1ql5NYoiQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4c3b97e1405ebb7916bf71d7b9a3da9a44efaea7" title="Mistral 3 Blog post" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rerri"&gt; /u/rerri &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://mistral.ai/news/mistral-3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcayfs/mistral_3_blog_post/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pcayfs/mistral_3_blog_post/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-02T15:13:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1pd1yqc</id>
    <title>Hot take: We‚Äôre overselling 'semantic search' in RAG.</title>
    <updated>2025-12-03T11:42:02+00:00</updated>
    <author>
      <name>/u/Raisin_False</name>
      <uri>https://old.reddit.com/user/Raisin_False</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been building some RAG stuff and 'semantic search' feels way more magical in marketing than in reality.&lt;/p&gt; &lt;p&gt;Embeddings are great &lt;strong&gt;fuzzy matchers in meaning space&lt;/strong&gt; - they shine on paraphrases, synonyms, 'something like this' queries. But whenever I need sharper behavior (logic, constraints, dates, 'papers using X on Y after 2019'), plain bi-encoder vector search starts to fall over unless I add extra machinery.&lt;/p&gt; &lt;p&gt;In practice my setups end up looking more like:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;BM25 or dense (or hybrid) &lt;/li&gt; &lt;li&gt;Reranker and/or LLM query rewrite &lt;/li&gt; &lt;li&gt;LLM reasoning also maybe graphs/filters&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;At that point, calling just the first stage 'semantic search' feels a bit misleading, cause it's more like 'dense/vector retrieval' plus a bunch of stuff on top that actually does the reasoning.&lt;/p&gt; &lt;p&gt;So i have 2 questions for you:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Is 'semantic search' a fair name for plain vector similarity, or do you avoid that term?&lt;/li&gt; &lt;li&gt;How far did you get with just embeddings before needing reranking / query rewriting / graphs / filters?&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Raisin_False"&gt; /u/Raisin_False &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd1yqc/hot_take_were_overselling_semantic_search_in_rag/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd1yqc/hot_take_were_overselling_semantic_search_in_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pd1yqc/hot_take_were_overselling_semantic_search_in_rag/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T11:42:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1pd3mdw</id>
    <title>Intel Arc Pro B60 Battlematrix Preview: 192GB of VRAM for On-Premise AI</title>
    <updated>2025-12-03T13:05:36+00:00</updated>
    <author>
      <name>/u/reps_up</name>
      <uri>https://old.reddit.com/user/reps_up</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd3mdw/intel_arc_pro_b60_battlematrix_preview_192gb_of/"&gt; &lt;img alt="Intel Arc Pro B60 Battlematrix Preview: 192GB of VRAM for On-Premise AI" src="https://external-preview.redd.it/0mZ7_HvOTkdLgtq4s_qT3vry9cE_RWRALKiuljZ3Fl8.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d01be5fde96c8bded5f16d12f17d20ed686c5e29" title="Intel Arc Pro B60 Battlematrix Preview: 192GB of VRAM for On-Premise AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/reps_up"&gt; /u/reps_up &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.storagereview.com/review/intel-arc-pro-b60-battlematrix-preview-192gb-of-vram-for-on-premise-ai"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd3mdw/intel_arc_pro_b60_battlematrix_preview_192gb_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pd3mdw/intel_arc_pro_b60_battlematrix_preview_192gb_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T13:05:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1pcomhi</id>
    <title>I'm surprised how simple Qwen3 VL's architecture is.</title>
    <updated>2025-12-02T23:50:06+00:00</updated>
    <author>
      <name>/u/No-Compote-6794</name>
      <uri>https://old.reddit.com/user/No-Compote-6794</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcomhi/im_surprised_how_simple_qwen3_vls_architecture_is/"&gt; &lt;img alt="I'm surprised how simple Qwen3 VL's architecture is." src="https://preview.redd.it/bfrh4xf5nv4g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2643d2d6457fb6e3adfc09a5cf9e18b995e4219f" title="I'm surprised how simple Qwen3 VL's architecture is." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;the new 3D position id logic really got a lot more intuitive compared to qwen2.5 vl. it basically index image patches on width and height dimension in addition to the regular token sequence / temporal dimension (while treating text as one same number across all 3 dimensions). &lt;/p&gt; &lt;p&gt;in addition to this, they added deepstack, which essentially is just some residual connections between vision encoder blocks and downstream LLM blocks.&lt;/p&gt; &lt;p&gt;here's the full repo if you want to read more: &lt;a href="https://github.com/Emericen/tiny-qwen"&gt;https://github.com/Emericen/tiny-qwen&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Compote-6794"&gt; /u/No-Compote-6794 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bfrh4xf5nv4g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcomhi/im_surprised_how_simple_qwen3_vls_architecture_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pcomhi/im_surprised_how_simple_qwen3_vls_architecture_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-02T23:50:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1pceipb</id>
    <title>Mistral just released Mistral 3 ‚Äî a full open-weight model family from 3B all the way up to 675B parameters.</title>
    <updated>2025-12-02T17:26:06+00:00</updated>
    <author>
      <name>/u/InternationalToe2678</name>
      <uri>https://old.reddit.com/user/InternationalToe2678</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;All models are Apache 2.0 and fully usable for research + commercial work.&lt;/p&gt; &lt;p&gt;Quick breakdown:&lt;/p&gt; &lt;p&gt;‚Ä¢ Ministral 3 (3B / 8B / 14B) ‚Äì compact, multimodal, and available in base, instruct, and reasoning variants. Surprisingly strong for their size.&lt;/p&gt; &lt;p&gt;‚Ä¢ Mistral Large 3 (675B MoE) ‚Äì their new flagship. Strong multilingual performance, high efficiency, and one of the most capable open-weight instruct models released so far.&lt;/p&gt; &lt;p&gt;Why it matters: You now get a full spectrum of open models that cover everything from on-device reasoning to large enterprise-scale intelligence. The release pushes the ecosystem further toward distributed, open AI instead of closed black-box APIs.&lt;/p&gt; &lt;p&gt;Full announcement: &lt;a href="https://mistral.ai/news/mistral-3"&gt;https://mistral.ai/news/mistral-3&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InternationalToe2678"&gt; /u/InternationalToe2678 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pceipb/mistral_just_released_mistral_3_a_full_openweight/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pceipb/mistral_just_released_mistral_3_a_full_openweight/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pceipb/mistral_just_released_mistral_3_a_full_openweight/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-02T17:26:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1pcp8z3</id>
    <title>Who‚Äôs got them Q_001_X_S_REAP Mistral Large 3 GGUFs?</title>
    <updated>2025-12-03T00:16:47+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcp8z3/whos_got_them_q_001_x_s_reap_mistral_large_3_ggufs/"&gt; &lt;img alt="Who‚Äôs got them Q_001_X_S_REAP Mistral Large 3 GGUFs?" src="https://preview.redd.it/buxyht7ltv4g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ed225e778fb3ebb1d3e4ff9ac401e09c3aced65e" title="Who‚Äôs got them Q_001_X_S_REAP Mistral Large 3 GGUFs?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm looking at you, Unsloth üòÅ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/buxyht7ltv4g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcp8z3/whos_got_them_q_001_x_s_reap_mistral_large_3_ggufs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pcp8z3/whos_got_them_q_001_x_s_reap_mistral_large_3_ggufs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T00:16:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1pd2wjt</id>
    <title>DeepSeek V3.2 Technical Report</title>
    <updated>2025-12-03T12:31:51+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd2wjt/deepseek_v32_technical_report/"&gt; &lt;img alt="DeepSeek V3.2 Technical Report" src="https://preview.redd.it/q3rjrhs0gz4g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7d2e078ce099142771b5d3999cbb9670fbfc18d8" title="DeepSeek V3.2 Technical Report" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here is a brief summary of &lt;strong&gt;key breakthroughs of DeepSeek V3.2&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. DeepSeek Sparse Attention (DSA)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;A new efficient attention mechanism that dramatically reduces computational complexity while preserving performance in long-context scenarios. &lt;/p&gt; &lt;p&gt;It uses a lightning indexer with fine-grained top-k token selection to achieve sparse but effective attention.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. Scalable and Stable Reinforcement Learning Framework&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Implements a heavily scaled post-training RL pipeline, with compute exceeding 10% of pretraining cost. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;3. Large-Scale Agentic Task Synthesis Pipeline&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Provides a novel pipeline that programmatically generates large numbers of tool-use environments (1,800+ environments, 85,000+ complex prompts). &lt;/p&gt; &lt;p&gt;This boosts generalization, tool-use ability, and instruction-following in interactive settings.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;4. Unified Reasoning + Agentic RL Training&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Merges reasoning, tool-use, and human-alignment RL into a single stage rather than multi-stage pipelines. &lt;/p&gt; &lt;p&gt;This avoids catastrophic forgetting and improves cross-domain performance simultaneously.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;DeepSeek-V3.2-Speciale&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;A high-compute variant trained with relaxed length penalties and enhanced mathematical-reasoning rewards. &lt;/p&gt; &lt;p&gt;This model even surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI).&lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/abs/2512.02556"&gt;Arxiv paper &lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/q3rjrhs0gz4g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd2wjt/deepseek_v32_technical_report/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pd2wjt/deepseek_v32_technical_report/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T12:31:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1pd04cn</id>
    <title>Chinese startup founded by Google engineer claims to have developed its own tpu reportedly 1.5 times faster than nvidia a100.</title>
    <updated>2025-12-03T09:51:40+00:00</updated>
    <author>
      <name>/u/Turbulent_Pin7635</name>
      <uri>https://old.reddit.com/user/Turbulent_Pin7635</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.tomshardware.com/tech-industry/chinese-startup-founded-by-google-engineer-claims-to-have-developed-its-own-tpu-reportedly-1-5-times-faster-than-nvidias-a100-gpu-from-2020-42-percent-more-efficient"&gt;https://www.tomshardware.com/tech-industry/chinese-startup-founded-by-google-engineer-claims-to-have-developed-its-own-tpu-reportedly-1-5-times-faster-than-nvidias-a100-gpu-from-2020-42-percent-more-efficient&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Turbulent_Pin7635"&gt; /u/Turbulent_Pin7635 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd04cn/chinese_startup_founded_by_google_engineer_claims/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd04cn/chinese_startup_founded_by_google_engineer_claims/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pd04cn/chinese_startup_founded_by_google_engineer_claims/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T09:51:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
