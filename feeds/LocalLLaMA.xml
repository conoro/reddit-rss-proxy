<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-04T09:06:02+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI. Hey, Tony</subtitle>
  <entry>
    <id>t3_1nxnq77</id>
    <title>best coding model under 40b parameters? preferably moe</title>
    <updated>2025-10-04T07:37:00+00:00</updated>
    <author>
      <name>/u/Odd-Ordinary-5922</name>
      <uri>https://old.reddit.com/user/Odd-Ordinary-5922</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;preferably moe&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Odd-Ordinary-5922"&gt; /u/Odd-Ordinary-5922 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxnq77/best_coding_model_under_40b_parameters_preferably/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxnq77/best_coding_model_under_40b_parameters_preferably/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxnq77/best_coding_model_under_40b_parameters_preferably/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T07:37:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxnszs</id>
    <title>Where do you think we'll be at for home inference in 2 years?</title>
    <updated>2025-10-04T07:41:51+00:00</updated>
    <author>
      <name>/u/TumbleweedDeep825</name>
      <uri>https://old.reddit.com/user/TumbleweedDeep825</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I suppose we'll never see any big price reduction jumps? Especially with inflation rising globally?&lt;/p&gt; &lt;p&gt;I'd love to be able to have a home SOTA tier model for under $15k. Like GLM 4.6, etc. But wouldn't we all?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TumbleweedDeep825"&gt; /u/TumbleweedDeep825 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxnszs/where_do_you_think_well_be_at_for_home_inference/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxnszs/where_do_you_think_well_be_at_for_home_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxnszs/where_do_you_think_well_be_at_for_home_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T07:41:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxlq7t</id>
    <title>Unsure which ollama model to use? Here's a tool I built to help</title>
    <updated>2025-10-04T05:36:03+00:00</updated>
    <author>
      <name>/u/h3xzur7</name>
      <uri>https://old.reddit.com/user/h3xzur7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;Iâ€™m fairly new to working with local LLMs, and like many, I wondered which model(s) I should use. To help answer that, I put together a tool that:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Automates running multiple models on custom prompts&lt;/li&gt; &lt;li&gt;Outputs everything into a clean, easy-to-read HTML report&lt;/li&gt; &lt;li&gt;Lets you quickly compare results side by side&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;While there might be similar tools out there, I wanted something lightweight and straightforward for my own workflow. I figured Iâ€™d share in case others find it useful too.&lt;/p&gt; &lt;p&gt;Iâ€™d love any constructive feedbackâ€”whether you think this fills a gap, how it could be improved, or if you know of alternatives I should check out.&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Spectral-Knight-Ops/local-llm-evaluator"&gt;https://github.com/Spectral-Knight-Ops/local-llm-evaluator&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/h3xzur7"&gt; /u/h3xzur7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxlq7t/unsure_which_ollama_model_to_use_heres_a_tool_i/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxlq7t/unsure_which_ollama_model_to_use_heres_a_tool_i/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxlq7t/unsure_which_ollama_model_to_use_heres_a_tool_i/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T05:36:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1nx71mm</id>
    <title>Looks like the ASUS Ascent GX10 release is imminent</title>
    <updated>2025-10-03T18:25:32+00:00</updated>
    <author>
      <name>/u/noco-ai</name>
      <uri>https://old.reddit.com/user/noco-ai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nx71mm/looks_like_the_asus_ascent_gx10_release_is/"&gt; &lt;img alt="Looks like the ASUS Ascent GX10 release is imminent" src="https://preview.redd.it/4b1db2z8vxsf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3c85c12e9af6ec2b65feed208efacca05e4f18db" title="Looks like the ASUS Ascent GX10 release is imminent" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/noco-ai"&gt; /u/noco-ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4b1db2z8vxsf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nx71mm/looks_like_the_asus_ascent_gx10_release_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nx71mm/looks_like_the_asus_ascent_gx10_release_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T18:25:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwukd5</id>
    <title>Granite4 -1M context window, and no one even noticed?</title>
    <updated>2025-10-03T09:38:28+00:00</updated>
    <author>
      <name>/u/Western_Courage_6563</name>
      <uri>https://old.reddit.com/user/Western_Courage_6563</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How is it, when IBM drops a model, no one notice?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Western_Courage_6563"&gt; /u/Western_Courage_6563 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwukd5/granite4_1m_context_window_and_no_one_even_noticed/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwukd5/granite4_1m_context_window_and_no_one_even_noticed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwukd5/granite4_1m_context_window_and_no_one_even_noticed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T09:38:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxosfs</id>
    <title>Can't get Granite 4 maximum context window size...</title>
    <updated>2025-10-04T08:44:57+00:00</updated>
    <author>
      <name>/u/Fade78</name>
      <uri>https://old.reddit.com/user/Fade78</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I'm using ollama 0.12.3 and OpenWebui 0.6.32 and I have a rig with 3x 4060 TI 16GB. I can run 32b models with context size that allow to fill up to 48GB VRAM.&lt;/p&gt; &lt;p&gt;When I'm using granite4:tiny-h, I can put a context of 290000 tokens, which takes 12GB in the VRAM but I have a memory error for 300000 tokens.&lt;/p&gt; &lt;p&gt;With granite4:small-h, I can put a context of 40000 tokens, which takes 30GB in VRAM but have memory error for 50000 tokens.&lt;/p&gt; &lt;p&gt;The error is like : 500: llama runner process has terminated: cudaMalloc failed: out of memory ggml_gallocr_reserve_n: failed to allocate CUDA1 buffer of size 7112647168&lt;/p&gt; &lt;p&gt;Does anyone could get the maximum 1000000 tokens context window?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fade78"&gt; /u/Fade78 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxosfs/cant_get_granite_4_maximum_context_window_size/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxosfs/cant_get_granite_4_maximum_context_window_size/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxosfs/cant_get_granite_4_maximum_context_window_size/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T08:44:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxfpqd</id>
    <title>Tips for getting OSS-120B to run faster at longer context?</title>
    <updated>2025-10-04T00:21:40+00:00</updated>
    <author>
      <name>/u/Acceptable_Adagio_91</name>
      <uri>https://old.reddit.com/user/Acceptable_Adagio_91</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;UPDATE - Swapping to the Q4_K_XL unsloth GGUF and removing the KV quantization seems to have done the trick! Getting much higher speeds now across the board and at longer context lengths.&lt;/p&gt; &lt;p&gt;I'm running OSS 120B (f16 GGUF from unsloth) in llama.cpp using the llamacpp-gptoss-120b container, on 3x 3090s, on linux. i9 7900x CPU with 64GB system ram.&lt;/p&gt; &lt;p&gt;Weights and cache fully offloaded to GPU. Llama settings are:&lt;/p&gt; &lt;p&gt;--ctx-size 131k (max)&lt;/p&gt; &lt;p&gt;--flash-attn&lt;/p&gt; &lt;p&gt;-- K &amp;amp; V cache Q8&lt;/p&gt; &lt;p&gt;--batch 512&lt;/p&gt; &lt;p&gt;--ubatch-size 128&lt;/p&gt; &lt;p&gt;--threads 10&lt;/p&gt; &lt;p&gt;--threads_batch 10&lt;/p&gt; &lt;p&gt;--tensor-split 0.30,0.34,0.36&lt;/p&gt; &lt;p&gt;--jinja&lt;/p&gt; &lt;p&gt;--verbose&lt;/p&gt; &lt;p&gt;--main-gpu 2&lt;/p&gt; &lt;p&gt;--split-mode layer&lt;/p&gt; &lt;p&gt;At short prompts (less than 1k) I get like 30-40tps, but as soon as I put more than 2-3k of context in, it grinds way down to like 10-tps or less. Token ingestion takes ages too, like 30s to 1 minute for 3-4k tokens.&lt;/p&gt; &lt;p&gt;I feel like this can't be right, I'm not even getting anywhere close to max context length (at this rate it would be unusably slow anyway).. There must be a way to get this working better/faster&lt;/p&gt; &lt;p&gt;Anyone else running this model on a similar setup that can share their settings and experience with getting the most out of this model?&lt;/p&gt; &lt;p&gt;I haven't tried ex_lllama yet but I have heard it might be better/faster than llama so I could try that&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Acceptable_Adagio_91"&gt; /u/Acceptable_Adagio_91 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxfpqd/tips_for_getting_oss120b_to_run_faster_at_longer/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxfpqd/tips_for_getting_oss120b_to_run_faster_at_longer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxfpqd/tips_for_getting_oss120b_to_run_faster_at_longer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T00:21:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwzq6p</id>
    <title>GLM-4.6 now on artificial analysis</title>
    <updated>2025-10-03T13:50:50+00:00</updated>
    <author>
      <name>/u/Professional-Bear857</name>
      <uri>https://old.reddit.com/user/Professional-Bear857</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://artificialanalysis.ai/models/glm-4-6-reasoning"&gt;https://artificialanalysis.ai/models/glm-4-6-reasoning&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Tldr, it benchmarks slightly worse than Qwen 235b 2507. In my use I have found it to also perform worse than the Qwen model, glm 4.5 also didn't benchmark well so it might just be the benchmarks. Although it looks to be slightly better with agent / tool use.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Professional-Bear857"&gt; /u/Professional-Bear857 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwzq6p/glm46_now_on_artificial_analysis/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwzq6p/glm46_now_on_artificial_analysis/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwzq6p/glm46_now_on_artificial_analysis/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T13:50:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwv4q0</id>
    <title>A list of models released or udpated last week on this sub, in case you missed any (3rd Oct)</title>
    <updated>2025-10-03T10:12:59+00:00</updated>
    <author>
      <name>/u/aifeed-fyi</name>
      <uri>https://old.reddit.com/user/aifeed-fyi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We had an interesting week in releases this week (Open &amp;amp; Closed).&lt;/p&gt; &lt;p&gt;Here is the weekly list of models, I found discussed on LocalLlama &lt;em&gt;this week.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Please update or let me know in the comments if there are any mistakes or misses. Good Friday!&lt;/p&gt; &lt;h1&gt;Model Releases &amp;amp; Updates&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Description&lt;/th&gt; &lt;th align="left"&gt;Reddit&lt;/th&gt; &lt;th align="left"&gt;HF / GH&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nu75l3/glm_46_is_out_and_its_going_against_claude_45/"&gt;GLM-4.6&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;LLM 200k ctx&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nu75l3/glm_46_is_out_and_its_going_against_claude_45/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/zai-org/GLM-4.6"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nte4j1/deepseekaideepseekv32exp_and/"&gt;DeepSeek-V3.2-Exp&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;LLM exp/base&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nte4j1/deepseekaideepseekv32exp_and/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nw2wd6/granite_40_language_models_a_ibmgranite_collection"&gt;Granite 4.0&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;IBM LLM collection&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nw2wd6/granite_40_language_models_a_ibmgranite_collection"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/collections/ibm-granite/granite-quantized-models-67f944eddd16ff8e057f115c"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nwibsb/ming_v2_is_out/"&gt;Ming V2&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Multimodal collection&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nwibsb/ming_v2_is_out/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/collections/inclusionAI/ming-v2-68ddea4954413c128d706630"&gt;HF Collection&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/gallery/1nvltym"&gt;LFM2-Audio-1.5&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Audio&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/gallery/1nvltym"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/LiquidAI/LFM2-Audio-1.5B"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.liquid.ai/blog/introducing-liquid-nanos-frontier-grade-performance-on-everyday-devices"&gt;LiquidAI&lt;/a&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nuyjp9/liquidai_bet_on_small_but_mighty_model/"&gt; &lt;/a&gt;nanos&lt;/td&gt; &lt;td align="left"&gt;Small task LLM&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nuyjp9/liquidai_bet_on_small_but_mighty_model/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/collections/LiquidAI/liquid-nanos-68b98d898414dd94d4d5f99a"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nt2l57/qwen3_omni_awq_released/"&gt;Qwen3 Omni AWQ&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;30B 4bit AWQ&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nt2l57/qwen3_omni_awq_released/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/cpatonn/Qwen3-Omni-30B-A3B-Instruct-AWQ-4bit"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nu44n4/1t_open_source_reasoning_model_with_50b_activation/"&gt;Ring-1T-preview&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;1T reasoning 50B Active&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nu44n4/1t_open_source_reasoning_model_with_50b_activation/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/inclusionAI/Ring-1T-preview"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nw8jbn/ring_flash_20_104b_a6b_with_linear_attention"&gt;RingFlash linea r 2&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;LLM 104B MOE&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nw8jbn/ring_flash_20_104b_a6b_with_linear_attention"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/inclusionAI/Ring-flash-linear-2.0"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nu1rul/lingmini20_finally_almost_here_lets_push_context/"&gt;Ling-mini-2.0&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;16B LLM&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nu1rul/lingmini20_finally_almost_here_lets_push_context/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/inclusionAI/Ling-mini-2.0-GGUF"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nrvo9g/finally_internvl3_5_flash_versions_coming/"&gt;InternVL3_5 Flash&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Vision-language&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nrvo9g/finally_internvl3_5_flash_versions_coming/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/OpenGVLab/InternVL3_5-8B-Flash"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nrhr13/k2think_32b_reasoning_model_from_uae/"&gt;K2-Think 32B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;32B reasoning&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nrhr13/k2think_32b_reasoning_model_from_uae/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/LLM360/K2-Think-32B"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nv5uw8/dont_sleep_on_apriel1515bthinker_and_snowpiercer/"&gt;Apriel-1.5-15b-Thinker&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;15B multimodal&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nv5uw8/dont_sleep_on_apriel1515bthinker_and_snowpiercer/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/ServiceNow-AI/Apriel-1.5-15b-Thinker"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nuxdd4/release_finally_a_working_8bit_quantized/"&gt;VibeVoice 1.8.0 (8-bit)&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;8-bit speech&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nuxdd4/release_finally_a_working_8bit_quantized/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/FabioSarracino/VibeVoice-Large-Q8"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;N&lt;a href="https://huggingface.co/neuphonic/neutts-air"&gt;eutts-air&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;TTS model&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nw60fj/open_source_speech_foundation_model_that_runs/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/neuphonic/neutts-air"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;ðŸ§° Resources &amp;amp; Tools&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Name&lt;/th&gt; &lt;th align="left"&gt;Type&lt;/th&gt; &lt;th align="left"&gt;Reddit&lt;/th&gt; &lt;th align="left"&gt;Link&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://v.redd.it/vklzqk9bipsf1"&gt;Onyx&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Open-source Chat UI&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://v.redd.it/vklzqk9bipsf1"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;â€“&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ntiua9/we_just_opensourced_kroko_asr_a_fast_streaming/"&gt;Kroko ASR&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Speech recognition&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ntiua9/we_just_opensourced_kroko_asr_a_fast_streaming/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://kroko.ai"&gt;kroko.ai&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nu3slg/an_opensource_omni_chatbot_for_long_speech_and/"&gt;MGM-Omni&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Omni chatbot&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nu3slg/an_opensource_omni_chatbot_for_long_speech_and/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/dvlab-research/MGM-Omni"&gt;GitHub&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nrrgoy/monkesearch_technical_report_out_now/"&gt;monkeSearch Report&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Research/benchmark&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nrrgoy/monkesearch_technical_report_out_now/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://monkesearch.github.io/"&gt;monkesearch.github.io&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aifeed-fyi"&gt; /u/aifeed-fyi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwv4q0/a_list_of_models_released_or_udpated_last_week_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwv4q0/a_list_of_models_released_or_udpated_last_week_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwv4q0/a_list_of_models_released_or_udpated_last_week_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T10:12:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxibik</id>
    <title>Whereâ€™s the lip reading ai?</title>
    <updated>2025-10-04T02:31:46+00:00</updated>
    <author>
      <name>/u/Trustingmeerkat</name>
      <uri>https://old.reddit.com/user/Trustingmeerkat</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Iâ€™m sure there are some projects out there making real progress on this, but given how quickly tech has advanced in recent years, Iâ€™m honestly surprised nothing has surfaced with strong accuracy in converting video to transcript purely through lip reading.&lt;/p&gt; &lt;p&gt;From what Iâ€™ve seen, personalized models trained on specific individuals do quite well with front facing footage, but whereâ€™s the model that can take any video and give a reasonably accurate idea of what was said? Putting privacy concerns aside for a second, it feels like we should already be 80 percent of the way there. With the amount of spoken video data that already has transcripts, a solid model paired with a standard LLM technique could fill in the blanks with high confidence.&lt;/p&gt; &lt;p&gt;If that doesnâ€™t exist yet, letâ€™s make it, Iâ€™m down to even spin it up as a DAO, which is something Iâ€™ve wanted to experiment with.&lt;/p&gt; &lt;p&gt;Bonus question: what historical videos would be the most fascinating or valuable to finally understand what was said on camera?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Trustingmeerkat"&gt; /u/Trustingmeerkat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxibik/wheres_the_lip_reading_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxibik/wheres_the_lip_reading_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxibik/wheres_the_lip_reading_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T02:31:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1nx8igd</id>
    <title>Best LLMs for writing (not coding)</title>
    <updated>2025-10-03T19:21:05+00:00</updated>
    <author>
      <name>/u/FrequentHelp2203</name>
      <uri>https://old.reddit.com/user/FrequentHelp2203</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It seems most of the LLMs I see are being ranked on coding ability and I understand why I think but for the rest of us, what are some of best LLM for writing. Not writing for you but analysis and critique to better develop your writing such as an essay or story. &lt;/p&gt; &lt;p&gt;Thank you for your time. &lt;/p&gt; &lt;p&gt;Update: thanks for all the help. Appreciate it&lt;/p&gt; &lt;p&gt;Update: Iâ€™m writing my own stuff. Essays mostly. I need LLMs that can improve it with discussion and analysis. I write far better than the LLMs Iâ€™ve tried so hoping to hear whatâ€™s really good out there. Again appreciate your time and tips. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FrequentHelp2203"&gt; /u/FrequentHelp2203 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nx8igd/best_llms_for_writing_not_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nx8igd/best_llms_for_writing_not_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nx8igd/best_llms_for_writing_not_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T19:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwxxya</id>
    <title>Bought a used 5090 only to find out it was tampered with</title>
    <updated>2025-10-03T12:36:48+00:00</updated>
    <author>
      <name>/u/a201905</name>
      <uri>https://old.reddit.com/user/a201905</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just a angry/disappointment/frustration post from someone who was very excited at the opportunity to upgrade from 3080 to a 5090 at a discount to run local LLM.&lt;/p&gt; &lt;p&gt;A MSI rtx 5090 came up at my local, trustworthy auction house and I won it for around $2k. It was a stretch on my budget but it was too good of an opportunity so I jumped on it. I was extremely excited and upgraded the PSU but when I tried to put everything together, the system would not boot. I tried everything for hours until I remembered reading the article about people stealing GPU cores. &lt;/p&gt; &lt;p&gt;So I looked at the back and noticed the warranty tamper sticker was voided. i looked back at the auction site and I can see the image they posted with the screw tampered. I was blinded by the potential happiness this was going to bring me and I just didn't pay attention.&lt;/p&gt; &lt;p&gt;What a disappointment. Why do people do this garbage to others. I hope karma bites you in the ass. &lt;/p&gt; &lt;p&gt;Edit: I should have been clearer, i opened it and it's missing the core. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/a201905"&gt; /u/a201905 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwxxya/bought_a_used_5090_only_to_find_out_it_was/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwxxya/bought_a_used_5090_only_to_find_out_it_was/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwxxya/bought_a_used_5090_only_to_find_out_it_was/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T12:36:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxmq9b</id>
    <title>The Missing Link between the Transformer and Models of the Brain</title>
    <updated>2025-10-04T06:35:16+00:00</updated>
    <author>
      <name>/u/ramzeez88</name>
      <uri>https://old.reddit.com/user/ramzeez88</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A group of scientists at Pathway claim to have found a missing link . 'The massively parallel post-Transformer reasoning architecture which opens the door to generalization over time' Link to the paper : &lt;a href="https://arxiv.org/abs/2509.26507"&gt;https://arxiv.org/abs/2509.26507&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ramzeez88"&gt; /u/ramzeez88 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxmq9b/the_missing_link_between_the_transformer_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxmq9b/the_missing_link_between_the_transformer_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxmq9b/the_missing_link_between_the_transformer_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T06:35:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxh2n8</id>
    <title>Paper | Apriel-1.5-15B-Thinker: Mid-training is all you need</title>
    <updated>2025-10-04T01:28:45+00:00</updated>
    <author>
      <name>/u/touhidul002</name>
      <uri>https://old.reddit.com/user/touhidul002</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;(1) &lt;strong&gt;Integrated Multimodal Architecture&lt;/strong&gt;: Beginning with Pixtral-12B [9] as our foundation, we expand it to a model size capable of advanced reasoning across modalities, without requiring pretraining from scratch. &lt;/p&gt; &lt;p&gt;(2) &lt;strong&gt;Staged Multimodal Continual Pretraining (CPT)&lt;/strong&gt;: We adopt a two-phase CPT strategy. The first phase develops foundational text reasoning and broad multimodal capabilities, while the second enhances visual reasoning through synthetic data targeting spatial structure, compositional understanding, and fine-grained perception. This staged progression enables balanced strengthening of both modalities and provides a stable foundation for subsequent training stages, even when later stages emphasize a narrower set of modalities. &lt;/p&gt; &lt;p&gt;(3) &lt;strong&gt;High-Quality Supervised Fine-Tuning (SFT):&lt;/strong&gt; We curate a diverse, high-quality, and high-signal set of samples for supervised fine-tuning. Each response includes explicit reasoning traces, enabling the model to learn transparent thought processes. Coupled with the strong base model, this yields frontier-level performance across a broad range of reasoning benchmarks without requiring additional post-training. &lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2510.01141"&gt;https://arxiv.org/pdf/2510.01141&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/touhidul002"&gt; /u/touhidul002 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxh2n8/paper_apriel1515bthinker_midtraining_is_all_you/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxh2n8/paper_apriel1515bthinker_midtraining_is_all_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxh2n8/paper_apriel1515bthinker_midtraining_is_all_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T01:28:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxo3ao</id>
    <title>Best local model for open code?</title>
    <updated>2025-10-04T08:00:16+00:00</updated>
    <author>
      <name>/u/LastCulture3768</name>
      <uri>https://old.reddit.com/user/LastCulture3768</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Which LLM gives you satisfaction for tasks under open code with 12Go vram ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LastCulture3768"&gt; /u/LastCulture3768 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxo3ao/best_local_model_for_open_code/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxo3ao/best_local_model_for_open_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxo3ao/best_local_model_for_open_code/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T08:00:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1nx1ot4</id>
    <title>Qwen3-VL-30B-A3B-Instruct &amp; Thinking (Now Hidden)</title>
    <updated>2025-10-03T15:06:48+00:00</updated>
    <author>
      <name>/u/TKGaming_11</name>
      <uri>https://old.reddit.com/user/TKGaming_11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nx1ot4/qwen3vl30ba3binstruct_thinking_now_hidden/"&gt; &lt;img alt="Qwen3-VL-30B-A3B-Instruct &amp;amp; Thinking (Now Hidden)" src="https://a.thumbs.redditmedia.com/iNETafBex6Qpbyi8P087geXMh_aBmkILehL6E7qn-m4.jpg" title="Qwen3-VL-30B-A3B-Instruct &amp;amp; Thinking (Now Hidden)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TKGaming_11"&gt; /u/TKGaming_11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nx1ot4"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nx1ot4/qwen3vl30ba3binstruct_thinking_now_hidden/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nx1ot4/qwen3vl30ba3binstruct_thinking_now_hidden/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T15:06:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1nx8e2l</id>
    <title>Is this expected behaviour from Granite 4 32B? (Unsloth Q4XL, no system prompt)</title>
    <updated>2025-10-03T19:16:29+00:00</updated>
    <author>
      <name>/u/IonizedRay</name>
      <uri>https://old.reddit.com/user/IonizedRay</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nx8e2l/is_this_expected_behaviour_from_granite_4_32b/"&gt; &lt;img alt="Is this expected behaviour from Granite 4 32B? (Unsloth Q4XL, no system prompt)" src="https://preview.redd.it/uq9t3il85ysf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=86fe0496ab662fb43abd450fc0e2e5a75018e96b" title="Is this expected behaviour from Granite 4 32B? (Unsloth Q4XL, no system prompt)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IonizedRay"&gt; /u/IonizedRay &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/uq9t3il85ysf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nx8e2l/is_this_expected_behaviour_from_granite_4_32b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nx8e2l/is_this_expected_behaviour_from_granite_4_32b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T19:16:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxjh4c</id>
    <title>GitHub - huawei-csl/SINQ: Welcome to the official repository of SINQ! A novel, fast and high-quality quantization method designed to make any Large Language Model smaller while preserving accuracy.</title>
    <updated>2025-10-04T03:30:30+00:00</updated>
    <author>
      <name>/u/Aiochedolor</name>
      <uri>https://old.reddit.com/user/Aiochedolor</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxjh4c/github_huaweicslsinq_welcome_to_the_official/"&gt; &lt;img alt="GitHub - huawei-csl/SINQ: Welcome to the official repository of SINQ! A novel, fast and high-quality quantization method designed to make any Large Language Model smaller while preserving accuracy." src="https://external-preview.redd.it/yP0CnjxBFJCXTVacHixSvy4H_F7MTnOAVtKcV29Lggk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c413349140863192c8413b0f7b8e7f32ec48822c" title="GitHub - huawei-csl/SINQ: Welcome to the official repository of SINQ! A novel, fast and high-quality quantization method designed to make any Large Language Model smaller while preserving accuracy." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aiochedolor"&gt; /u/Aiochedolor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/huawei-csl/SINQ"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxjh4c/github_huaweicslsinq_welcome_to_the_official/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxjh4c/github_huaweicslsinq_welcome_to_the_official/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T03:30:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxbbxe</id>
    <title>GLM 4.6 new best open weight overall on lmarena</title>
    <updated>2025-10-03T21:11:39+00:00</updated>
    <author>
      <name>/u/r3m8sh</name>
      <uri>https://old.reddit.com/user/r3m8sh</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Third on code after Qwen 235b (lmarena isn't agent based). #3 on hard prompts and #1 on creative writing.&lt;/p&gt; &lt;p&gt;Edit : in thinking mode (default).&lt;/p&gt; &lt;p&gt;&lt;a href="https://lmarena.ai/leaderboard/text/overall"&gt;https://lmarena.ai/leaderboard/text/overall&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/r3m8sh"&gt; /u/r3m8sh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxbbxe/glm_46_new_best_open_weight_overall_on_lmarena/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxbbxe/glm_46_new_best_open_weight_overall_on_lmarena/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxbbxe/glm_46_new_best_open_weight_overall_on_lmarena/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T21:11:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1nx18ax</id>
    <title>GLM 4.6 IS A FUKING AMAZING MODEL AND NOBODY CAN TELL ME OTHERWISE</title>
    <updated>2025-10-03T14:49:34+00:00</updated>
    <author>
      <name>/u/boneMechBoy69420</name>
      <uri>https://old.reddit.com/user/boneMechBoy69420</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Especially fuckin artificial analysis and their bullshit ass benchmark &lt;/p&gt; &lt;p&gt;Been using GLM 4.5 it on prod for a month now and I've got nothing but good feedback from the users , it's got way better autonomy than any other proprietary model I've tried (sonnet , gpt 5 and grok code) and it's probably the best ever model for tool call accuracy &lt;/p&gt; &lt;p&gt;One benchmark id recommend yall follow is the berkley function calling benchmark (v4 ig) &lt;a href="https://gorilla.cs.berkeley.edu/leaderboard.html"&gt;bfcl v4&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/boneMechBoy69420"&gt; /u/boneMechBoy69420 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nx18ax/glm_46_is_a_fuking_amazing_model_and_nobody_can/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nx18ax/glm_46_is_a_fuking_amazing_model_and_nobody_can/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nx18ax/glm_46_is_a_fuking_amazing_model_and_nobody_can/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T14:49:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxjhnj</id>
    <title>Behold, the jankiest setup ever</title>
    <updated>2025-10-04T03:31:15+00:00</updated>
    <author>
      <name>/u/T-VIRUS999</name>
      <uri>https://old.reddit.com/user/T-VIRUS999</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxjhnj/behold_the_jankiest_setup_ever/"&gt; &lt;img alt="Behold, the jankiest setup ever" src="https://b.thumbs.redditmedia.com/twOOoKU5XbRq6uFRGfXj_XqIEzieTWVvhWE3zg-T_qA.jpg" title="Behold, the jankiest setup ever" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I plan to get an open test bench, after I get my second P40 in a week or two (which will fit nicely on the other side of that fan) &lt;/p&gt; &lt;p&gt;Performance is as shown, Qwen 3 32B Q4 5.9T/sec&lt;/p&gt; &lt;p&gt;The fan is one of those stupidly powerful Delta electronics server fans that pushes out like 250cfm, so I needed to add a PWM controller to slow it down, and it wouldn't run without that giant capacitor, and it's powered by a Li-ion battery instead of the PSU (for now) &lt;/p&gt; &lt;p&gt;It's not stable at all, the whole system BSODs if a program tries to query the GPU while something else is using it (such as if I try to run GPUZ while LM Studio is running), but if only 1 thing touches the GPU at a time, it works &lt;/p&gt; &lt;p&gt;It has a Ryzen 5 5500GT, 16GB of DDR4, a 1000w PSU, a 512GB SSD, and 1 Nvidia P40 (soon to be 2) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/T-VIRUS999"&gt; /u/T-VIRUS999 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nxjhnj"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxjhnj/behold_the_jankiest_setup_ever/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxjhnj/behold_the_jankiest_setup_ever/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T03:31:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxi82t</id>
    <title>Why do private companies release open source models?</title>
    <updated>2025-10-04T02:26:58+00:00</updated>
    <author>
      <name>/u/desudesu15</name>
      <uri>https://old.reddit.com/user/desudesu15</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I love open source models. I feel they are an alternative for general knowledge, and since I started in this world, I stopped paying for subscriptions and started running models locally.&lt;/p&gt; &lt;p&gt;However, I don't understand the business model of companies like OpenAI launching an open source model. &lt;/p&gt; &lt;p&gt;How do they make money by launching an open source model? &lt;/p&gt; &lt;p&gt;Isn't it counterproductive to their subscription model?&lt;/p&gt; &lt;p&gt;Thank you, and forgive my ignorance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/desudesu15"&gt; /u/desudesu15 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxi82t/why_do_private_companies_release_open_source/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxi82t/why_do_private_companies_release_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxi82t/why_do_private_companies_release_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T02:26:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxjzbn</id>
    <title>Distributed Inference over wifi with 8x 3090 egpus performance</title>
    <updated>2025-10-04T03:57:59+00:00</updated>
    <author>
      <name>/u/Only_Situation_4713</name>
      <uri>https://old.reddit.com/user/Only_Situation_4713</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I smoked some really good weed recently and decided it was a good idea to buy more 3090s.&lt;/p&gt; &lt;p&gt;Naturally I didn't want to use a real build with server parts, put 8 3090s in one build on home depot racks? No thanks I'm lazy.&lt;/p&gt; &lt;p&gt;I got 4 3090 egpus from a guy on facebook. He's cool, sold them to me for 650 each with the egpu. &lt;/p&gt; &lt;p&gt;&lt;a href="https://www.gigabyte.com/Graphics-Card/GV-N3090IXEB-24GD"&gt;https://www.gigabyte.com/Graphics-Card/GV-N3090IXEB-24GD&lt;/a&gt; &amp;lt;--- these are the EGPUs&lt;/p&gt; &lt;p&gt;Then I got 4 other random 3090s of different brands and put them in 3 spare Pcs I have lying around.&lt;/p&gt; &lt;p&gt;Node #1&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Z390 Prime&lt;/li&gt; &lt;li&gt;9900K&lt;/li&gt; &lt;li&gt;64gb of DDR4&lt;/li&gt; &lt;li&gt;3090 (duh)&lt;/li&gt; &lt;li&gt;850W.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Node #2&lt;/p&gt; &lt;ul&gt; &lt;li&gt;MSI Unify ITX z690&lt;/li&gt; &lt;li&gt;12400K&lt;/li&gt; &lt;li&gt;64gb of DDR5&lt;/li&gt; &lt;li&gt;3090 (duh) &lt;/li&gt; &lt;li&gt;650W&lt;/li&gt; &lt;li&gt;2X 3090 EGPUs attached&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Node #3 (Host)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Z790 Maximus Hero&lt;/li&gt; &lt;li&gt;13700k&lt;/li&gt; &lt;li&gt;64gb of DDR5&lt;/li&gt; &lt;li&gt;1200W PSU&lt;/li&gt; &lt;li&gt;2x 3090s &lt;/li&gt; &lt;li&gt;2x 3090 EGPUs attached&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I ran all of it over VLLM with Ray to distribute the load. It's connected over Wifi, I got a good router so speed is about only 10% slower than ethernet from across the house. For now it's all pipeline parallel until the parts arrive then I'll do a 2 node system with 4 gpu each.&lt;/p&gt; &lt;p&gt;&lt;a href="https://rog.asus.com/us/networking/rog-rapture-gt-axe16000-model/"&gt;https://rog.asus.com/us/networking/rog-rapture-gt-axe16000-model/&lt;/a&gt; &amp;lt;--- my router(s).&lt;/p&gt; &lt;p&gt;Results:&lt;/p&gt; &lt;p&gt;At 128k context limit running GLM 4.5 Air AWQ 8 bit (that's Q8 for you gguf folks)&lt;/p&gt; &lt;p&gt;I get 5500 tokens/s prompt processing and 24 tokens a second for a 50k~ ish token prompt. &lt;/p&gt; &lt;p&gt;It works great over Roo.&lt;/p&gt; &lt;p&gt;Ray has a very annoying overhead cost so just assume that each system has like 1gb less vram. Running all my node in headless helps alot too.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Only_Situation_4713"&gt; /u/Only_Situation_4713 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxjzbn/distributed_inference_over_wifi_with_8x_3090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxjzbn/distributed_inference_over_wifi_with_8x_3090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxjzbn/distributed_inference_over_wifi_with_8x_3090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T03:57:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwx1rx</id>
    <title>The most important AI paper of the decade. No debate</title>
    <updated>2025-10-03T11:55:32+00:00</updated>
    <author>
      <name>/u/PumpkinNarrow6339</name>
      <uri>https://old.reddit.com/user/PumpkinNarrow6339</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwx1rx/the_most_important_ai_paper_of_the_decade_no/"&gt; &lt;img alt="The most important AI paper of the decade. No debate" src="https://preview.redd.it/d2rcvb6nyvsf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0051a67e9886e507e2b0a35679f4d469050fda91" title="The most important AI paper of the decade. No debate" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PumpkinNarrow6339"&gt; /u/PumpkinNarrow6339 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/d2rcvb6nyvsf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwx1rx/the_most_important_ai_paper_of_the_decade_no/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwx1rx/the_most_important_ai_paper_of_the_decade_no/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T11:55:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxhfcq</id>
    <title>Qwen3-VL-30B-A3B-Instruct &amp; Thinking are here</title>
    <updated>2025-10-04T01:46:34+00:00</updated>
    <author>
      <name>/u/AlanzhuLy</name>
      <uri>https://old.reddit.com/user/AlanzhuLy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxhfcq/qwen3vl30ba3binstruct_thinking_are_here/"&gt; &lt;img alt="Qwen3-VL-30B-A3B-Instruct &amp;amp; Thinking are here" src="https://external-preview.redd.it/NiG8S3h1NHl5GO5rtLVQY3YBzVNQNV4xvZR0hFbk_vE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=88770649ad1f1c425c3a22e1502363d18f9727dc" title="Qwen3-VL-30B-A3B-Instruct &amp;amp; Thinking are here" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/xwkuqkkt20tf1.png?width=2994&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=16a4068b96a7c20f55817cc29987345c287c76a7"&gt;https://preview.redd.it/xwkuqkkt20tf1.png?width=2994&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=16a4068b96a7c20f55817cc29987345c287c76a7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-VL-30B-A3B-Instruct"&gt;https://huggingface.co/Qwen/Qwen3-VL-30B-A3B-Instruct&lt;/a&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/Qwen/Qwen3-VL-30B-A3B-Thinking"&gt;https://huggingface.co/Qwen/Qwen3-VL-30B-A3B-Thinking&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlanzhuLy"&gt; /u/AlanzhuLy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxhfcq/qwen3vl30ba3binstruct_thinking_are_here/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxhfcq/qwen3vl30ba3binstruct_thinking_are_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxhfcq/qwen3vl30ba3binstruct_thinking_are_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T01:46:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvryo4</id>
    <title>AMA Announcement: Prime Intellect â€” The Openâ€‘Source Distributed Training Lab (Thu, Oct 2 â€¢ 10 AM â€“ 1 PM PDT)</title>
    <updated>2025-10-02T02:31:01+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"&gt; &lt;img alt="AMA Announcement: Prime Intellect â€” The Openâ€‘Source Distributed Training Lab (Thu, Oct 2 â€¢ 10 AM â€“ 1 PM PDT)" src="https://preview.redd.it/222kj50x0msf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c1c747c42690f74b8d08690dcdbb1ef23aece13b" title="AMA Announcement: Prime Intellect â€” The Openâ€‘Source Distributed Training Lab (Thu, Oct 2 â€¢ 10 AM â€“ 1 PM PDT)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/222kj50x0msf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T02:31:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwaoyd</id>
    <title>AMA with Prime Intellect â€” Ask Us Anything!</title>
    <updated>2025-10-02T17:47:44+00:00</updated>
    <author>
      <name>/u/kindacognizant</name>
      <uri>https://old.reddit.com/user/kindacognizant</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;AMA with Prime Intellect â€” Ask Us Anything!&lt;/h1&gt; &lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;! Weâ€™re excited for this AMA, thank you for having us.&lt;/p&gt; &lt;p&gt;Iâ€™m Kalomaze (&lt;a href="/u/kindacognizant"&gt;u/kindacognizant&lt;/a&gt;), a researcher at Prime Intellect, the lab behind:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Distributed training &lt;a href="https://www.primeintellect.ai/#research"&gt;efforts&lt;/a&gt; including INTELLECT-1 + INTELLECT-2&lt;/li&gt; &lt;li&gt;Open-source RL efforts including &lt;a href="https://github.com/PrimeIntellect-ai/verifiers"&gt;verifiers&lt;/a&gt;, &lt;a href="https://github.com/PrimeIntellect-ai/prime-rl"&gt;prime-rl&lt;/a&gt;, and the &lt;a href="https://app.primeintellect.ai/dashboard/environments"&gt;Environments Hub&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Our other participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Sami Jaghouar, &lt;a href="/u/samsja19"&gt;u/samsja19&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Will Brown, &lt;a href="/u/willccbb"&gt;u/willccbb&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Jack Min Ong, &lt;a href="/u/Cinamic"&gt;u/Cinamic&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Mika Senghaas, &lt;a href="/u/mikasenghaas"&gt;u/mikasenghaas&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 11:00 AM â€“ 2:00 PM PST, with the Prime Intellect team continuing to follow up on questions over the next 48 hours.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kindacognizant"&gt; /u/kindacognizant &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwaoyd/ama_with_prime_intellect_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwaoyd/ama_with_prime_intellect_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwaoyd/ama_with_prime_intellect_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T17:47:44+00:00</published>
  </entry>
</feed>
