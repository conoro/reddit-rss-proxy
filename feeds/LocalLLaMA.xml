<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-23T14:51:25+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ptd60q</id>
    <title>Stop wasting your MCP context window. LTP (Lazy Tool Protocol) reduces tool-calling overhead by up to 93 percent.</title>
    <updated>2025-12-22T22:37:38+00:00</updated>
    <author>
      <name>/u/song-junhyeong</name>
      <uri>https://old.reddit.com/user/song-junhyeong</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptd60q/stop_wasting_your_mcp_context_window_ltp_lazy/"&gt; &lt;img alt="Stop wasting your MCP context window. LTP (Lazy Tool Protocol) reduces tool-calling overhead by up to 93 percent." src="https://b.thumbs.redditmedia.com/ZmFYqL2BAkBid2YXZuXdBnn96S3WsyuZUJ2eKu54gbg.jpg" title="Stop wasting your MCP context window. LTP (Lazy Tool Protocol) reduces tool-calling overhead by up to 93 percent." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been working on a solution for a problem that has been bothering me with AI agents: the massive hidden cost of tool definitions.&lt;/p&gt; &lt;p&gt;Current implementations of the Model Context Protocol (MCP) typically require loading full tool schemas into the AI's context at the start. If you are using a large library of tools, you can easily burn through 60,000 to 300,000 tokens just to define what the tools do before any actual work begins.&lt;/p&gt; &lt;p&gt;I built LTP (Lazy Tool Protocol) to solve this through a Lazy Loading pattern.&lt;/p&gt; &lt;p&gt;Instead of bloating the context window, LTP uses a CLI bridge that allows the AI to discover and fetch tool information only when necessary.&lt;/p&gt; &lt;p&gt;Key Benchmarks from v0.1.0:&lt;/p&gt; &lt;p&gt;93 Percent Token Reduction: In tests with 100 tool calls, LTP reduced token consumption from 300,000 to just 20,000.&lt;/p&gt; &lt;p&gt;Efficiency at Scale: While traditional MCP usage grows linearly with the number of calls, LTP maintains a near-fixed discovery cost.&lt;/p&gt; &lt;p&gt;The --schema Flag: This new feature provides compact function signatures to the AI at the start of a session. It eliminates the need for repeated metadata calls while keeping the context footprint minimal.&lt;/p&gt; &lt;p&gt;Features:&lt;/p&gt; &lt;p&gt;Unlimited Tools: You can connect hundreds or thousands of MCP tools without degrading reasoning performance or hitting context limits.&lt;/p&gt; &lt;p&gt;Executable Crafts: We are moving beyond static instructions. A &amp;quot;Craft&amp;quot; is a package containing precise AI prompts and executable automation scripts to ensure reliability.&lt;/p&gt; &lt;p&gt;Security-First Design: It includes a built-in whitelist, sandbox path restrictions, and mandatory confirmation for high-risk operations like file deletions.&lt;/p&gt; &lt;p&gt;How to use it: The protocol works by giving your AI a system prompt that teaches it how to interact with the LTP CLI. The AI can then search for tools, read schemas on-demand, and execute them as needed.&lt;/p&gt; &lt;p&gt;I have released this as an open-source project and am running the registry on my own infrastructure to support the community.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/JuN-B-official/ltp"&gt;https://github.com/JuN-B-official/ltp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Url: &lt;a href="https://ltp.jun-b.com"&gt;https://ltp.jun-b.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Efficiency Analysis: &lt;a href="https://ltp.jun-b.com/docs/effect"&gt;https://ltp.jun-b.com/docs/effect&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/song-junhyeong"&gt; /u/song-junhyeong &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ptd60q"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptd60q/stop_wasting_your_mcp_context_window_ltp_lazy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptd60q/stop_wasting_your_mcp_context_window_ltp_lazy/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T22:37:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptjmb0</id>
    <title>MiniMax M2.1 benchmark</title>
    <updated>2025-12-23T03:34:38+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptjmb0/minimax_m21_benchmark/"&gt; &lt;img alt="MiniMax M2.1 benchmark" src="https://b.thumbs.redditmedia.com/OHdACrr3ilm8bDAbmlW2TC91DBmLNVmeavTzyzCbMgM.jpg" title="MiniMax M2.1 benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/lxif8yh0jv8g1.jpg?width=1280&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=f1f787941640a15bee52988c35f35c0c1e5c3eca"&gt;https://preview.redd.it/lxif8yh0jv8g1.jpg?width=1280&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=f1f787941640a15bee52988c35f35c0c1e5c3eca&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- Multi-language Coding (beyond Python) SOTA across Rust, Java, Go, C++, Kotlin, Obj-C, TS &amp;amp; JS, scoring 72.5% for SWE-bench Multilingual and exceeding Gemini 3 Pro and Claude Sonnet 4.5.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;AppDev &amp;amp; WebDev Major upgrades for native Android &amp;amp; iOS, plus stronger web aesthetics + realistic scientific simulations. Not only vibe WebDev, but also vibe AppDev.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Lightning Fast with Concise Reasoning Faster responses, more concise reasoning, and significantly reduced token consumption.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Advanced Interleaved Thinking &amp;amp; Instruction Following Excels at integrating &amp;quot;composite instruction constraints&amp;quot; (as seen in OctoCodingBench), ready for office automation tasks.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptjmb0/minimax_m21_benchmark/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptjmb0/minimax_m21_benchmark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptjmb0/minimax_m21_benchmark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T03:34:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptn2lq</id>
    <title>Batch OCR: Dockerized PaddleOCR pipeline to convert thousands of PDFs into clean text (GPU/CPU, Windows + Linux)</title>
    <updated>2025-12-23T06:39:03+00:00</updated>
    <author>
      <name>/u/QuanstScientist</name>
      <uri>https://old.reddit.com/user/QuanstScientist</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptn2lq/batch_ocr_dockerized_paddleocr_pipeline_to/"&gt; &lt;img alt="Batch OCR: Dockerized PaddleOCR pipeline to convert thousands of PDFs into clean text (GPU/CPU, Windows + Linux)" src="https://external-preview.redd.it/jtfq7WMsQ6MVADp_C19uPunr_Ib9UQe2B12piwoyxvY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e39d273b2df321cf09ad739dc6906b9cc539ebfc" title="Batch OCR: Dockerized PaddleOCR pipeline to convert thousands of PDFs into clean text (GPU/CPU, Windows + Linux)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Dear All,&lt;/p&gt; &lt;p&gt;I just open-sourced Batch OCR — a Dockerized, PaddleOCR-based pipeline for turning large collections of PDFs into clean text files. After testing many OCR/model options from Hugging Face, I settled on PaddleOCR for its speed and accuracy.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/94wg0beyfw8g1.png?width=2740&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1f9ac2791c12f525cc1bf1b5f16cbf6f2731fb7c"&gt;https://preview.redd.it/94wg0beyfw8g1.png?width=2740&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1f9ac2791c12f525cc1bf1b5f16cbf6f2731fb7c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A simple Gradio UI lets you choose a folder and recursively process PDFs into .txt files for indexing, search, or LLM training.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/BoltzmannEntropy/batch-ocr"&gt;https://github.com/BoltzmannEntropy/batch-ocr&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/2u3cptgfgw8g1.png?width=1058&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3d6398e16609884a5bd2aca850e294b83c921f75"&gt;https://preview.redd.it/2u3cptgfgw8g1.png?width=1058&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3d6398e16609884a5bd2aca850e294b83c921f75&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Highlights:&lt;/p&gt; &lt;p&gt;- Process hundreds or thousands of PDFs reliably&lt;/p&gt; &lt;p&gt;- Extract embedded text when available; fall back to OCR when needed&lt;/p&gt; &lt;p&gt;- Produce consistent, clean text with a lightweight quality filter&lt;/p&gt; &lt;p&gt;- Mirror the input folder structure and write results under ocr_results&lt;/p&gt; &lt;p&gt;- GPU or CPU: Uses PaddlePaddle CUDA when available; CPU fallback&lt;/p&gt; &lt;p&gt;- Simple UI: Select folder, list PDFs, initialize OCR, run batch&lt;/p&gt; &lt;p&gt;- Clean output: Writes &amp;lt;name&amp;gt;_ocr.txt per PDF; errors as &amp;lt;name&amp;gt;_ERROR.txt&lt;/p&gt; &lt;p&gt;- Cross‑platform: Windows and Linux/macOS via Docker&lt;/p&gt; &lt;p&gt;- Privacy: Everything runs locally; no cloud calls&lt;/p&gt; &lt;p&gt;Feedback and contributions welcome. If you try it on a large dataset or different languages, I’d love to hear how it goes.&lt;/p&gt; &lt;p&gt;Best,&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/QuanstScientist"&gt; /u/QuanstScientist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptn2lq/batch_ocr_dockerized_paddleocr_pipeline_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptn2lq/batch_ocr_dockerized_paddleocr_pipeline_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptn2lq/batch_ocr_dockerized_paddleocr_pipeline_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T06:39:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptd1nc</id>
    <title>GLM-4.7 FP8 on 4x6000 pro blackwells</title>
    <updated>2025-12-22T22:32:26+00:00</updated>
    <author>
      <name>/u/getfitdotus</name>
      <uri>https://old.reddit.com/user/getfitdotus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1ptd1nc/video/oueyacty0u8g1/player"&gt;https://reddit.com/link/1ptd1nc/video/oueyacty0u8g1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GLM-4.7 FP8 sglang mtp fp8 e4m3fn KVCache on 4x6000 Blackwell pro max can get 140k context and mtp is faster then last time I had this with 4.6. May be due to using new sglang with newer jit flashinfer for sm120. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/getfitdotus"&gt; /u/getfitdotus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptd1nc/glm47_fp8_on_4x6000_pro_blackwells/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptd1nc/glm47_fp8_on_4x6000_pro_blackwells/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptd1nc/glm47_fp8_on_4x6000_pro_blackwells/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T22:32:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptu5wm</id>
    <title>MNIST handwritten digit recognition, independently completed by Kimi K2</title>
    <updated>2025-12-23T13:39:12+00:00</updated>
    <author>
      <name>/u/InternationalAsk1490</name>
      <uri>https://old.reddit.com/user/InternationalAsk1490</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptu5wm/mnist_handwritten_digit_recognition_independently/"&gt; &lt;img alt="MNIST handwritten digit recognition, independently completed by Kimi K2" src="https://b.thumbs.redditmedia.com/YNvEBYAla2JfnLDWXxUER9ioiwdnK4UqxZiISrugo-g.jpg" title="MNIST handwritten digit recognition, independently completed by Kimi K2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As a beginner in machine learning, it feels amazing that a neural network has implemented another neural network by itself.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3y4nu42wiy8g1.png?width=949&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3651c1cfbba9adef613055b3406da64c51615059"&gt;https://preview.redd.it/3y4nu42wiy8g1.png?width=949&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3651c1cfbba9adef613055b3406da64c51615059&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://5vbqmgatvcymq.ok.kimi.link/"&gt;Demo&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InternationalAsk1490"&gt; /u/InternationalAsk1490 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptu5wm/mnist_handwritten_digit_recognition_independently/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptu5wm/mnist_handwritten_digit_recognition_independently/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptu5wm/mnist_handwritten_digit_recognition_independently/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T13:39:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptrbc1</id>
    <title>I tested GPT-5.2 Codex vs Gemini 3 Pro vs Claude Opus on real dev tasks</title>
    <updated>2025-12-23T11:09:00+00:00</updated>
    <author>
      <name>/u/shricodev</name>
      <uri>https://old.reddit.com/user/shricodev</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Okay, so we have three AI models leading the coding leaderboards and they are the talk of the town on Twitter and literally everywhere.&lt;/p&gt; &lt;p&gt;The names are pretty obvious: Claude Opus, Gemini 3 Pro, and OpenAI's GPT-5.2 (Codex).&lt;/p&gt; &lt;p&gt;They're also the most recent &amp;quot;agentic&amp;quot; models, and given that they have pretty much the same benchmark compared to the others, I decided to test these head-on &lt;strong&gt;in coding (not agentic)&lt;/strong&gt; (of course!)&lt;/p&gt; &lt;p&gt;So instead of some basic tests, I gave them 3 real tasks, mostly on UI and a logic question that I actually care about:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Build a simple Minecraft clone in Python (Pygame)&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Clone a real Figma dashboard (with Figma MCP access)&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Solve a LeetCode Hard (10.6% acceptance)&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;TL;DR (my results)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Gemini 3 Pro&lt;/strong&gt;: Best for &lt;strong&gt;UI/frontend&lt;/strong&gt;. Best Figma clone and even made the best “Minecraft” by going 3D. But it fell short on the LeetCode Hard (failed immediately).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPT-5.2 Codex&lt;/strong&gt;: Most consistent all-rounder. Solid Pygame Minecraft, decent Figma clone, and a correct LeetCode solution that still &lt;strong&gt;TLEs&lt;/strong&gt; on bigger cases.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Claude Opus&lt;/strong&gt;: Rough day. UI work was messy (Minecraft + Figma), and the LeetCode solution also &lt;strong&gt;TLEs&lt;/strong&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If your day-to-day is mostly frontend/UI, Gemini 3 Pro is the winner from this small test. If you want something steady across random coding tasks, GPT-5.2 Codex felt like the safest pick. Opus honestly didn’t justify the cost for me here.&lt;/p&gt; &lt;h1&gt;Quick notes from each test&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;1) Pygame Minecraft&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Gemini 3 Pro&lt;/strong&gt; was the standout. It went &lt;strong&gt;3D&lt;/strong&gt;, looked polished, and actually felt like a mini game.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPT-5.2 Codex&lt;/strong&gt; was surprisingly good. Functional, different block types, smooth movement, even FPS.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Opus&lt;/strong&gt; was basically broken for me. Weird rotation, controls didn’t work, high CPU, then crash.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;2) Figma clone&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Gemini 3 Pro&lt;/strong&gt; nailed the UI. Spacing, layout, typography were closest.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPT-5.2 Codex&lt;/strong&gt; was solid, but a bit flat and some sizing felt off compared to Gemini.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Opus&lt;/strong&gt; was way off. Layout didn’t match, text didn’t match, feels like some random dashboard.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;3) LeetCode Hard&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GPT-5.2 Codex&lt;/strong&gt; produced a correct solution but &lt;strong&gt;not optimized enough&lt;/strong&gt; so it &lt;strong&gt;TLEs&lt;/strong&gt; on larger cases.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Opus&lt;/strong&gt; also correct on smaller tests, but again &lt;strong&gt;TLE&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Gemini 3 Pro&lt;/strong&gt; didn’t just TLE, it was &lt;strong&gt;incorrect&lt;/strong&gt; and failed early cases.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Now, if you're curious, I’ve got the videos + full breakdown in the blog post (and gists for each output): &lt;a href="https://www.tensorlake.ai/blog/gpt5.2-gemini3-opus4.5-coding"&gt;OpenAI GPT-5.2 Codex vs. Gemini 3 Pro vs Opus 4.5: Coding comparison&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you’re using any of these as your daily driver, what are you seeing in real work?&lt;/p&gt; &lt;p&gt;Especially curious if Opus is doing good for people in non-UI workflows, because for frontend it was not for me.&lt;/p&gt; &lt;p&gt;Let me know if you want quick agentic coding tests in the comments!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shricodev"&gt; /u/shricodev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptrbc1/i_tested_gpt52_codex_vs_gemini_3_pro_vs_claude/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptrbc1/i_tested_gpt52_codex_vs_gemini_3_pro_vs_claude/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptrbc1/i_tested_gpt52_codex_vs_gemini_3_pro_vs_claude/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T11:09:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptkqee</id>
    <title>2025 LLM's vs 2007 AI</title>
    <updated>2025-12-23T04:30:31+00:00</updated>
    <author>
      <name>/u/Rombodawg</name>
      <uri>https://old.reddit.com/user/Rombodawg</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptkqee/2025_llms_vs_2007_ai/"&gt; &lt;img alt="2025 LLM's vs 2007 AI" src="https://b.thumbs.redditmedia.com/GuKc03QNuV73YqFB8sWLS9E5-uR0Eq9JKINJ0eokOPI.jpg" title="2025 LLM's vs 2007 AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;2025: Gpt 5.2, Gemini 3.0, Claude 4.5 opus: 20% fail rate on most tasks &lt;/p&gt; &lt;p&gt;2007: Akinator: 100% success rate literally reading your mind&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/i90p0gh0tv8g1.jpg?width=1400&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=5f7a16e77dc19aa9ae5386495880045aebd70e15"&gt;https://preview.redd.it/i90p0gh0tv8g1.jpg?width=1400&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=5f7a16e77dc19aa9ae5386495880045aebd70e15&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rombodawg"&gt; /u/Rombodawg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptkqee/2025_llms_vs_2007_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptkqee/2025_llms_vs_2007_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptkqee/2025_llms_vs_2007_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T04:30:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptva10</id>
    <title>Runtime optimizing llama.cpp</title>
    <updated>2025-12-23T14:28:13+00:00</updated>
    <author>
      <name>/u/go-nz-ale-s</name>
      <uri>https://old.reddit.com/user/go-nz-ale-s</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptva10/runtime_optimizing_llamacpp/"&gt; &lt;img alt="Runtime optimizing llama.cpp" src="https://preview.redd.it/0kehku3moy8g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2ce8fd6c567dba927f40c4eee5076ecd6654d108" title="Runtime optimizing llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;You often hear the criticism that AI consumes too much energy and that a bunch of new nuclear power plants will have to be built to operate the many AI models.&lt;br /&gt; One approach to refute this is to optimize the algorithms so that they run faster on the same hardware.&lt;br /&gt; And I have now shown that llama.cpp and ggml also have potential when it comes to runtime optimization.&lt;/p&gt; &lt;p&gt;I optimized 2 of the AVX2 functions inside &amp;quot;ggml\src\ggml-cpu\arch\x86\repack.cpp&amp;quot; and now the performance of the llama_bench tests is &lt;strong&gt;up to 20% better&lt;/strong&gt; (than the implementation on master).&lt;br /&gt; I think there is a lot more potential for optimizations in ggml. First I didn't spend too much time for these examples and second, there are many more cpu/gpu architectures and model types.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/go-nz-ale-s"&gt; /u/go-nz-ale-s &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0kehku3moy8g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptva10/runtime_optimizing_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptva10/runtime_optimizing_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T14:28:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptt4uf</id>
    <title>I integrated llama.cpp's new router mode into llamactl with web UI support</title>
    <updated>2025-12-23T12:49:34+00:00</updated>
    <author>
      <name>/u/RealLordMathis</name>
      <uri>https://old.reddit.com/user/RealLordMathis</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've shared my project &lt;a href="https://github.com/lordmathis/llamactl"&gt;llamactl&lt;/a&gt; here a few times, and wanted to update you on some major new features, especially the integration of llama.cpp's recently released router mode.&lt;/p&gt; &lt;p&gt;Llamactl is a unified management system for running local LLMs across llama.cpp, MLX, and vLLM backends. It provides a web dashboard for managing instances along with an OpenAI-compatible API.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Router mode integration&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;llama.cpp recently introduced router mode for dynamic model management, and I've now integrated it into llamactl. You can now:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Create a llama.cpp instance without specifying a model&lt;/li&gt; &lt;li&gt;Load/unload models on-demand through the dashboard&lt;/li&gt; &lt;li&gt;Route requests using &lt;code&gt;&amp;lt;instance_name&amp;gt;/&amp;lt;model_name&amp;gt;&lt;/code&gt; syntax in your chat completion calls&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Current limitations&lt;/strong&gt; (both planned for future releases):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Model preset configuration (.ini files) must be done manually for now&lt;/li&gt; &lt;li&gt;Model downloads aren't available through the UI yet (there's a hacky workaround)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Other recent additions&lt;/strong&gt; :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Multi-node support - Deploy instances across different hosts for distributed setups&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Granular API key permissions - Create inference API keys with per-instance access control&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Docker support, log rotation, improved health checks, and more&lt;br /&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://github.com/lordmathis/llamactl"&gt;GitHub&lt;/a&gt;&lt;br /&gt; &lt;a href="https://llamactl.org/stable/"&gt;Docs&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Always looking for feedback and contributions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RealLordMathis"&gt; /u/RealLordMathis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptt4uf/i_integrated_llamacpps_new_router_mode_into/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptt4uf/i_integrated_llamacpps_new_router_mode_into/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptt4uf/i_integrated_llamacpps_new_router_mode_into/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T12:49:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1pt18x4</id>
    <title>NVIDIA made a beginner's guide to fine-tuning LLMs with Unsloth!</title>
    <updated>2025-12-22T14:42:56+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pt18x4/nvidia_made_a_beginners_guide_to_finetuning_llms/"&gt; &lt;img alt="NVIDIA made a beginner's guide to fine-tuning LLMs with Unsloth!" src="https://preview.redd.it/k20itq6cpr8g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=28c0951b3081aafea1e185ecbd82f4f0fc54726f" title="NVIDIA made a beginner's guide to fine-tuning LLMs with Unsloth!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Blog Link: &lt;a href="https://blogs.nvidia.com/blog/rtx-ai-garage-fine-tuning-unsloth-dgx-spark/"&gt;https://blogs.nvidia.com/blog/rtx-ai-garage-fine-tuning-unsloth-dgx-spark/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You'll learn about: - Training methods: LoRA, FFT, RL - When to fine-tune and why + use-cases - Amount of data and VRAM needed - How to train locally on DGX Spark, RTX GPUs &amp;amp; more&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/k20itq6cpr8g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pt18x4/nvidia_made_a_beginners_guide_to_finetuning_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pt18x4/nvidia_made_a_beginners_guide_to_finetuning_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T14:42:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1pt5jfn</id>
    <title>GLM 4.7 released!</title>
    <updated>2025-12-22T17:32:39+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pt5jfn/glm_47_released/"&gt; &lt;img alt="GLM 4.7 released!" src="https://b.thumbs.redditmedia.com/gArPotffsWB1U2wwy4EN2C6LihhGp5xRmDZVZabOdLE.jpg" title="GLM 4.7 released!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GLM-4.7 is here!&lt;/p&gt; &lt;p&gt;GLM-4.7 surpasses GLM-4.6 with substantial improvements in coding, complex reasoning, and tool usage, setting new open-source SOTA standards. It also boosts performance in chat, creative writing, and role-play scenarios.&lt;/p&gt; &lt;p&gt;Weights: &lt;a href="http://huggingface.co/zai-org/GLM-4.7"&gt;http://huggingface.co/zai-org/GLM-4.7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Tech Blog: &lt;a href="http://z.ai/blog/glm-4.7"&gt;http://z.ai/blog/glm-4.7&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pt5jfn"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pt5jfn/glm_47_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pt5jfn/glm_47_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T17:32:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptifqq</id>
    <title>MiniMax M2.1 released on openrouter!</title>
    <updated>2025-12-23T02:37:16+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://openrouter.ai/minimax/minimax-m2.1"&gt;https://openrouter.ai/minimax/minimax-m2.1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.minimax.io/news/minimax-m21"&gt;https://www.minimax.io/news/minimax-m21&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://platform.minimax.io/docs/api-reference/text-intro"&gt;https://platform.minimax.io/docs/api-reference/text-intro&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptifqq/minimax_m21_released_on_openrouter/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptifqq/minimax_m21_released_on_openrouter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptifqq/minimax_m21_released_on_openrouter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T02:37:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptb4jj</id>
    <title>GLM-4.7 GGUF is here!</title>
    <updated>2025-12-22T21:12:09+00:00</updated>
    <author>
      <name>/u/KvAk_AKPlaysYT</name>
      <uri>https://old.reddit.com/user/KvAk_AKPlaysYT</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptb4jj/glm47_gguf_is_here/"&gt; &lt;img alt="GLM-4.7 GGUF is here!" src="https://external-preview.redd.it/TGrfpTKzENR2d9AEv3GwEXRPg1BfQXmQleE_vDAQ5qs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2c04adace1535fe995e55aae71ce5fa9e94f80d8" title="GLM-4.7 GGUF is here!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Still in the process of quantizing, it's a big model :)&lt;br /&gt; HF: &lt;a href="https://huggingface.co/AaryanK/GLM-4.7-GGUF"&gt;https://huggingface.co/AaryanK/GLM-4.7-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KvAk_AKPlaysYT"&gt; /u/KvAk_AKPlaysYT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/AaryanK/GLM-4.7-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptb4jj/glm47_gguf_is_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptb4jj/glm47_gguf_is_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T21:12:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptt5xj</id>
    <title>[PROJECT] I updated EntropyGuard a CLI tool to deduplicate RAG data locally on CPU before embedding. Saves ~40% tokens, handles 100GB+ files, and just got Checkpointing. (Open Source)</title>
    <updated>2025-12-23T12:51:02+00:00</updated>
    <author>
      <name>/u/Low-Flow-6572</name>
      <uri>https://old.reddit.com/user/Low-Flow-6572</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptt5xj/project_i_updated_entropyguard_a_cli_tool_to/"&gt; &lt;img alt="[PROJECT] I updated EntropyGuard a CLI tool to deduplicate RAG data locally on CPU before embedding. Saves ~40% tokens, handles 100GB+ files, and just got Checkpointing. (Open Source)" src="https://preview.redd.it/j1j9f7j6ay8g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=365953652a5dbae69ef2b33ac18bd26caabdf93e" title="[PROJECT] I updated EntropyGuard a CLI tool to deduplicate RAG data locally on CPU before embedding. Saves ~40% tokens, handles 100GB+ files, and just got Checkpointing. (Open Source)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;Like many of you, I've been building local RAG pipelines and got tired of the &amp;quot;garbage in, garbage out&amp;quot; problem. I noticed my vector database (and context window) was often bloated with duplicate chunks, things like recurring headers/footers in PDFs, identical error logs, or scraped pages that are 99% the same.&lt;/p&gt; &lt;p&gt;This does two bad things:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Pollutes Retrieval:&lt;/strong&gt; Your &lt;code&gt;top-k&lt;/code&gt; slots get filled with 5 variations of the same sentence, pushing out unique/relevant info.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Wastes Compute:&lt;/strong&gt; You end up embedding (and storing) junk.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I didn't want to spin up a heavy vector DB cluster just to clean data, and I definitely didn't want to send my raw data to an external API for processing. I needed something that runs on my CPU so my GPU is free for inference.&lt;/p&gt; &lt;p&gt;So I built &lt;strong&gt;EntropyGuard&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;It’s a standalone CLI tool designed to filter your datasets &lt;em&gt;before&lt;/em&gt; ingestion.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How it works (The &amp;quot;Hybrid&amp;quot; approach):&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Stage 1 (Fast):&lt;/strong&gt; It runs a fast hash (&lt;code&gt;xxhash&lt;/code&gt;) on the normalized text. This kills 100% identical duplicates instantly without touching neural networks.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Stage 2 (Smart):&lt;/strong&gt; The survivors go through a lightweight embedding model (default: &lt;code&gt;all-MiniLM-L6-v2&lt;/code&gt;) and FAISS to find &lt;em&gt;semantic&lt;/em&gt; duplicates.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;I just pushed v1.22 today with features for larger local datasets:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;OOM Safe:&lt;/strong&gt; It uses chunked processing and Polars LazyFrames. I’ve tested it on datasets larger than my RAM, and it doesn't crash.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Checkpoint &amp;amp; Resume:&lt;/strong&gt; If you're processing a massive dataset (e.g., 50GB) and your script dies at 90%, you can run &lt;code&gt;--resume&lt;/code&gt;. It picks up exactly where it left off.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Unix Pipes:&lt;/strong&gt; It plays nice with bash. You can just: &lt;code&gt;cat data.jsonl | entropyguard --dedup-threshold 0.95 &amp;gt; clean.jsonl&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Stats:&lt;/strong&gt; On my machine, I'm seeing about ~6k rows/sec for the hashing stage. It tells you exactly how many &amp;quot;Tokens&amp;quot; you saved at the end of the run, which is satisfying to watch.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;License:&lt;/strong&gt; MIT. It's open source and runs entirely offline.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Link:&lt;/strong&gt;&lt;a href="https://github.com/DamianSiuta/entropyguard"&gt;https://github.com/DamianSiuta/entropyguard&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I’d love some feedback on the logic or performance. If you manage to break it with a weird dataset, let me know in the issues. If you find it useful for your local stack, a star on GitHub is always appreciated!&lt;/p&gt; &lt;p&gt;Cheers!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Low-Flow-6572"&gt; /u/Low-Flow-6572 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/j1j9f7j6ay8g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptt5xj/project_i_updated_entropyguard_a_cli_tool_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptt5xj/project_i_updated_entropyguard_a_cli_tool_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T12:51:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1pt3sco</id>
    <title>I made Soprano-80M: Stream ultra-realistic TTS in &lt;15ms, up to 2000x realtime, and &lt;1 GB VRAM, released under Apache 2.0!</title>
    <updated>2025-12-22T16:24:36+00:00</updated>
    <author>
      <name>/u/eugenekwek</name>
      <uri>https://old.reddit.com/user/eugenekwek</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pt3sco/i_made_soprano80m_stream_ultrarealistic_tts_in/"&gt; &lt;img alt="I made Soprano-80M: Stream ultra-realistic TTS in &amp;lt;15ms, up to 2000x realtime, and &amp;lt;1 GB VRAM, released under Apache 2.0!" src="https://external-preview.redd.it/NzJvcm5nM3g1czhnMTuHKMiW0LLPLmT-UAsj3QPgelU3LLUn7ZzaJN_zFkuW.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=92e3fd36cfab3db61e8dc90b692c7a0d1918ce30" title="I made Soprano-80M: Stream ultra-realistic TTS in &amp;lt;15ms, up to 2000x realtime, and &amp;lt;1 GB VRAM, released under Apache 2.0!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! I’m Eugene, and I’ve been working on &lt;strong&gt;Soprano&lt;/strong&gt;: a new state-of-the-art TTS model I designed for voice chatbots. Voice applications require very low latency and natural speech generation to sound convincing, and I created Soprano to deliver on both of these goals.&lt;/p&gt; &lt;p&gt;Soprano is the world’s fastest TTS by an enormous margin. It is optimized to stream audio playback with &lt;strong&gt;&amp;lt;15 ms latency&lt;/strong&gt;, 10x faster than any other realtime TTS model like Chatterbox Turbo, VibeVoice-Realtime, GLM TTS, or CosyVoice3. It also natively supports batched inference, benefiting greatly from long-form speech generation. &lt;strong&gt;I was able to generate a 10-hour audiobook in under 20 seconds, achieving ~2000x realtime!&lt;/strong&gt; This is multiple orders of magnitude faster than any other TTS model, making ultra-fast, ultra-natural TTS a reality for the first time.&lt;/p&gt; &lt;p&gt;I owe these gains to the following design choices:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Higher sample rate:&lt;/strong&gt; most TTS models use a sample rate of 24 kHz, which can cause s and z sounds to be muffled. In contrast, Soprano natively generates 32 kHz audio, which sounds much sharper and clearer. In fact, 32 kHz speech sounds indistinguishable from 44.1/48 kHz speech, so I found it to be the best choice.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Vocoder-based audio decoder:&lt;/strong&gt; Most TTS designs use diffusion models to convert LLM outputs into audio waveforms. However, this comes at the cost of slow generation. To fix this, I trained a vocoder-based decoder instead, which uses a Vocos model to perform this conversion. My decoder runs several orders of magnitude faster than diffusion-based decoders (~6000x realtime!), enabling extremely fast audio generation.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Seamless Streaming:&lt;/strong&gt; Streaming usually requires generating multiple audio chunks and applying crossfade. However, this causes streamed output to sound worse than nonstreamed output. I solve this by using a Vocos-based decoder. Because Vocos has a finite receptive field. I can exploit its input locality to completely skip crossfading, producing streaming output that is identical to unstreamed output. Furthermore, I modified the Vocos architecture to reduce the receptive field, allowing Soprano to start streaming audio after generating just five audio tokens with the LLM.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;State-of-the-art Neural Audio Codec:&lt;/strong&gt; Speech is represented using a novel neural codec that compresses audio to ~15 tokens/sec at just 0.2 kbps. This helps improve generation speed, as only 15 tokens need to be generated to synthesize 1 second of audio, compared to 25, 50, or other commonly used token rates. To my knowledge, this is the highest bitrate compression achieved by any audio codec.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Infinite generation length:&lt;/strong&gt; Soprano automatically generates each sentence independently, and then stitches the results together. Theoretically, this means that sentences can no longer influence each other, but in practice I found that this doesn’t really happen anyway. Splitting by sentences allows for batching on long inputs, dramatically improving inference speed. &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I’m a second-year undergrad who’s just started working on TTS models, so I wanted to start small. Soprano was only pretrained on 1000 hours of audio (~100x less than other TTS models), so its stability and quality will improve tremendously as I train it on more data. Also, I optimized Soprano purely for speed, which is why it lacks bells and whistles like voice cloning, style control, and multilingual support. Now that I have experience creating TTS models, I have a lot of ideas for how to make Soprano even better in the future, so stay tuned for those!&lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/ekwek1/soprano"&gt;https://github.com/ekwek1/soprano&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Huggingface Demo: &lt;a href="https://huggingface.co/spaces/ekwek/Soprano-TTS"&gt;https://huggingface.co/spaces/ekwek/Soprano-TTS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Model Weights: &lt;a href="https://huggingface.co/ekwek/Soprano-80M"&gt;https://huggingface.co/ekwek/Soprano-80M&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- Eugene&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eugenekwek"&gt; /u/eugenekwek &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/htwi2n2x5s8g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pt3sco/i_made_soprano80m_stream_ultrarealistic_tts_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pt3sco/i_made_soprano80m_stream_ultrarealistic_tts_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T16:24:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptom2s</id>
    <title>exllamav3 adds support for GLM 4.7 (and 4.6V, + Ministral &amp; OLMO 3)</title>
    <updated>2025-12-23T08:13:58+00:00</updated>
    <author>
      <name>/u/Unstable_Llama</name>
      <uri>https://old.reddit.com/user/Unstable_Llama</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Lots of updates this month to exllamav3. Support added for &lt;a href="https://github.com/turboderp-org/exllamav3/commit/4d4992a8b82ae13edf86db2bb19e2de1c522c054"&gt;GLM 4.6V&lt;/a&gt;, &lt;a href="https://github.com/turboderp-org/exllamav3/commit/9b75bc5f58a70cb0e73c45f0bcd7d5959e124aa4"&gt;Ministral&lt;/a&gt;, and &lt;a href="https://github.com/turboderp-org/exllamav3/commit/104268521cdd1b24d19bcf92e5289b10219af5bd"&gt;OLMO 3&lt;/a&gt; (on the dev branch).&lt;/p&gt; &lt;p&gt;As GLM 4.7 is the same architecture as 4.6, it is already supported.&lt;/p&gt; &lt;p&gt;Several models from these families haven't been quantized and uploaded to HF yet, so if you can't find the one you are looking for, now is your chance to contribute to local AI!&lt;/p&gt; &lt;p&gt;Questions? Ask here or at the &lt;a href="https://discord.gg/wmrxvpdd"&gt;exllama discord&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Unstable_Llama"&gt; /u/Unstable_Llama &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptom2s/exllamav3_adds_support_for_glm_47_and_46v/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptom2s/exllamav3_adds_support_for_glm_47_and_46v/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptom2s/exllamav3_adds_support_for_glm_47_and_46v/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T08:13:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1pt5heq</id>
    <title>GLM 4.7 is out on HF!</title>
    <updated>2025-12-22T17:30:33+00:00</updated>
    <author>
      <name>/u/KvAk_AKPlaysYT</name>
      <uri>https://old.reddit.com/user/KvAk_AKPlaysYT</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pt5heq/glm_47_is_out_on_hf/"&gt; &lt;img alt="GLM 4.7 is out on HF!" src="https://external-preview.redd.it/gR0grxFGZc9MSnGGFWbK39DsDkjKEI-u2jMcygDp6Nc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cb92abe1e270f4c3e9d804bcff4653d2d0d7cc74" title="GLM 4.7 is out on HF!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KvAk_AKPlaysYT"&gt; /u/KvAk_AKPlaysYT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/zai-org/GLM-4.7"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pt5heq/glm_47_is_out_on_hf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pt5heq/glm_47_is_out_on_hf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T17:30:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptm3n4</id>
    <title>GLM 4.7 top the chart at Rank #6 in WebDev</title>
    <updated>2025-12-23T05:43:37+00:00</updated>
    <author>
      <name>/u/GeLaMi-Speaker</name>
      <uri>https://old.reddit.com/user/GeLaMi-Speaker</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptm3n4/glm_47_top_the_chart_at_rank_6_in_webdev/"&gt; &lt;img alt="GLM 4.7 top the chart at Rank #6 in WebDev" src="https://b.thumbs.redditmedia.com/s0EknbN86ZnaumUiKb5KC7z3WQ1YE737LQCzHo-j3pg.jpg" title="GLM 4.7 top the chart at Rank #6 in WebDev" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/zai-org/GLM-4.7"&gt;https://huggingface.co/zai-org/GLM-4.7&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GeLaMi-Speaker"&gt; /u/GeLaMi-Speaker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ptm3n4"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptm3n4/glm_47_top_the_chart_at_rank_6_in_webdev/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptm3n4/glm_47_top_the_chart_at_rank_6_in_webdev/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T05:43:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptq7rc</id>
    <title>GLM 4.7 vs. Minimax M2.1. My test &amp; subscription decision</title>
    <updated>2025-12-23T09:59:41+00:00</updated>
    <author>
      <name>/u/Psychological_Box406</name>
      <uri>https://old.reddit.com/user/Psychological_Box406</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been really excited about these two releases since I subscribed to both as potential offloads for my Claude Pro subscription.&lt;/p&gt; &lt;p&gt;I grabbed the GLM 4.7 subscription in early October on the quarterly plan (expires in ~2 weeks), and the Minimax M2.1 $2/month plan about 3 weeks ago to test it out. With both subscriptions ending soon, I needed to figure out which one to renew.&lt;/p&gt; &lt;p&gt;Since subscribing to Minimax M2.1, it's been my go-to model. But I wanted to see if GLM 4.7 had improved enough to make me switch back.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Test&lt;/strong&gt;&lt;br /&gt; I ran both models on the same prompt (in Claude Code) to generate e2e tests for a new feature I'm implementing in an application I'm building. Nothing complicated, two tables (1:N relationship), model, repo, service, controller, validator, routes. Pretty standard stuff.&lt;/p&gt; &lt;p&gt;I set up an agent with all the project's patterns, examples, and context for e2e testing. The models' job was to review the implementation done and instruct the agent to generate the new e2e.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GLM 4.7&lt;/strong&gt;: Ran for 70 minutes straight without finishing. Tests kept failing. I've had enough and stopped it.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Minimax M2.1&lt;/strong&gt;: Finished in 40 minutes with clean, working tests.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;But&lt;/strong&gt;&lt;br /&gt; The interesting part is, even though GLM 4.7 failed to finish, it actually caught a flaw in my implementation during testing. Minimax M2.1, on the other hand, just bent the tests to make them pass without flagging the design issue.&lt;/p&gt; &lt;p&gt;I’ll be sticking with Minimax for now, but I’m going to update my agent’s docs and constraints so it catches that kind of design flaw in the future.&lt;/p&gt; &lt;p&gt;I'm thinking about grabbing the GLM yearly promo at $29 just to have it on hand in case they drop a significantly faster and more capable version (GLM 5?). But for now, Minimax M2.1 wins on speed and reliability for me.&lt;/p&gt; &lt;p&gt;Also, Minimax, where is the Christmas promo like others are doing ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Psychological_Box406"&gt; /u/Psychological_Box406 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptq7rc/glm_47_vs_minimax_m21_my_test_subscription/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptq7rc/glm_47_vs_minimax_m21_my_test_subscription/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptq7rc/glm_47_vs_minimax_m21_my_test_subscription/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T09:59:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptttcm</id>
    <title>How to run the GLM-4.7 model locally on your own device (guide)</title>
    <updated>2025-12-23T13:23:03+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptttcm/how_to_run_the_glm47_model_locally_on_your_own/"&gt; &lt;img alt="How to run the GLM-4.7 model locally on your own device (guide)" src="https://preview.redd.it/b995ei5mfy8g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f4519336dd309d77b0ea2caf5ea5cb6af6df8bf4" title="How to run the GLM-4.7 model locally on your own device (guide)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ul&gt; &lt;li&gt;GLM-4.7 is Z.ai’s latest thinking model, delivering stronger coding, agent, and chat performance than GLM-4.6 &lt;/li&gt; &lt;li&gt;It achieves SOTA performance on on SWE-bench (73.8%, +5.8), SWE-bench Multilingual (66.7%, +12.9), and Terminal Bench 2.0 (41.0%, +16.5).&lt;/li&gt; &lt;li&gt;The full 355B parameter model requires &lt;strong&gt;400GB&lt;/strong&gt; of disk space, while the Unsloth Dynamic 2-bit GGUF reduces the size to &lt;strong&gt;134GB&lt;/strong&gt; (-&lt;strong&gt;75%)&lt;/strong&gt;. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Official blog post - &lt;a href="https://docs.unsloth.ai/models/glm-4.7"&gt;https://docs.unsloth.ai/models/glm-4.7&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/b995ei5mfy8g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptttcm/how_to_run_the_glm47_model_locally_on_your_own/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptttcm/how_to_run_the_glm47_model_locally_on_your_own/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T13:23:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptk5fs</id>
    <title>Unsloth GLM-4.7 GGUF</title>
    <updated>2025-12-23T04:01:15+00:00</updated>
    <author>
      <name>/u/Wooden-Deer-1276</name>
      <uri>https://old.reddit.com/user/Wooden-Deer-1276</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/unsloth/GLM-4.7-GGUF"&gt;https://huggingface.co/unsloth/GLM-4.7-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wooden-Deer-1276"&gt; /u/Wooden-Deer-1276 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptk5fs/unsloth_glm47_gguf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptk5fs/unsloth_glm47_gguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptk5fs/unsloth_glm47_gguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T04:01:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptr3lv</id>
    <title>r/LocalLLaMA - a year in review</title>
    <updated>2025-12-23T10:56:05+00:00</updated>
    <author>
      <name>/u/Everlier</name>
      <uri>https://old.reddit.com/user/Everlier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptr3lv/rlocalllama_a_year_in_review/"&gt; &lt;img alt="r/LocalLLaMA - a year in review" src="https://external-preview.redd.it/JtVvJz_3p9gkpLWB6adlC3-5M7zJIyvCc23zaC-6JV0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d4f54df711befdabb904b0d3f68bc21eb350d5a4" title="r/LocalLLaMA - a year in review" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm the same guy that made &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1hov3y9/rlocalllama_a_year_in_review/"&gt;2024 edition&lt;/a&gt;, here we are again.&lt;/p&gt; &lt;p&gt;This community has been the central hub for open-source AI for another year, and what a year 2025 has been. Let me take you back to the most notable things happened here during this time. This isn't really a list of model releases or papers, rather posts that were discussed and upvoted by the people here. So notable things missing is also an indication of what was going on. From the rise of Chinese open-source dominance to the hardware hacks, here is what happened in &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; in 2025.&lt;/p&gt; &lt;p&gt;The year started with a splash. The &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ho27fr/the_whale_has_landed/"&gt;arrival of &amp;quot;The Whale&amp;quot;&lt;/a&gt; (2121 upvotes, by &lt;a href="/u/fourDnet"&gt;u/fourDnet&lt;/a&gt;) marked the release of DeepSeek V3, setting the tone for what would become the &amp;quot;Year of the Open Source Strike Back.&amp;quot; It wasn't long before we saw &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1hphlz7/sam_altman_is_taking_veiled_shots_at_deepseek_and/"&gt;Sam Altman taking veiled shots&lt;/a&gt; (1959 upvotes) at the new competition, a clear sign that the market was changing.&lt;/p&gt; &lt;p&gt;We were all trying to figure out how to run these new beasts. Nvidia teased us with the &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1hvj4wn/nvidia_announces_3000_personal_ai_supercomputer/"&gt;Digits personal AI supercomputer&lt;/a&gt; (1663 upvotes, by &lt;a href="/u/DubiousLLM"&gt;u/DubiousLLM&lt;/a&gt;), while others were just trying to understand the sheer scale of what was happening. The realization that &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1i80cwf/deepseek_is_a_side_project/"&gt;DeepSeek was essentially a side project&lt;/a&gt; (2861 upvotes, by &lt;a href="/u/ParsaKhaz"&gt;u/ParsaKhaz&lt;/a&gt;) for a hedge fund only made it even more interesting.&lt;/p&gt; &lt;p&gt;By late January, the narrative was clear: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1i88g4y/meta_panicked_by_deepseek/"&gt;Meta was panicked&lt;/a&gt; (2779 upvotes, by &lt;a href="/u/Optimal_Hamster5789"&gt;u/Optimal_Hamster5789&lt;/a&gt;), reportedly &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ibk9us/meta_is_reportedly_scrambling_multiple_war_rooms/"&gt;scrambling &amp;quot;war rooms&amp;quot;&lt;/a&gt; (2117 upvotes, by &lt;a href="/u/FullstackSensei"&gt;u/FullstackSensei&lt;/a&gt;) to catch up. The community was buzzing with benchmarks, with &lt;a href="/u/kyazoglu"&gt;u/kyazoglu&lt;/a&gt; &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1i8tx5z/i_benchmarked_almost_every_model_that_can_fit_in/"&gt;testing almost every model that fits in 24GB VRAM&lt;/a&gt; (1861 upvotes) - a hero's work for the GPU-poor among us.&lt;/p&gt; &lt;p&gt;The &amp;quot;DeepSeek effect&amp;quot; was everywhere. &lt;a href="/u/Porespellar"&gt;u/Porespellar&lt;/a&gt; summed it up perfectly: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1iji47x/all_deepseek_all_the_time/"&gt;&amp;quot;All DeepSeek, all the time&amp;quot;&lt;/a&gt; (4116 upvotes). But it wasn't just about models; it was about what we could &lt;em&gt;do&lt;/em&gt; with them. We saw inspiring projects like &lt;a href="/u/Dry_Steak30"&gt;u/Dry_Steak30&lt;/a&gt;'s &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ij5yf2/how_i_built_an_open_source_ai_tool_to_find_my/"&gt;open source tool to find their autoimmune disease&lt;/a&gt; (2488 upvotes), proving that local AI is more than just a hobby.&lt;/p&gt; &lt;p&gt;Of course, it wouldn't be 2025 without some drama. The threat of &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1igc6r0/20_yrs_in_jail_or_1_million_for_downloading/"&gt;20 years in jail for downloading Chinese models&lt;/a&gt; (2092 upvotes, by &lt;a href="/u/segmond"&gt;u/segmond&lt;/a&gt;) worried us, but that didn't stop the innovation. We laughed when &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1iwb5nu/groks_think_mode_leaks_system_prompt/"&gt;Grok's think mode leaked its system prompt&lt;/a&gt; (6465 upvotes, by &lt;a href="/u/onil_gova"&gt;u/onil_gova&lt;/a&gt;), and cheered when DeepSeek announced they would &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1iui6nk/starting_next_week_deepseek_will_opensource_5/"&gt;open-source 5 repos&lt;/a&gt; (4560 upvotes, by &lt;a href="/u/Nunki08"&gt;u/Nunki08&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;Hardware remained a constant obsession. We drooled over &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1iy2t7c/frameworks_new_ryzen_max_desktop_with_128gb/"&gt;Framework's new Ryzen Max desktop&lt;/a&gt; (2004 upvotes, by &lt;a href="/u/sobe3249"&gt;u/sobe3249&lt;/a&gt;) and marveled at the monstrosity that was &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1j67bxt/16x_3090s_its_alive/"&gt;16x 3090s&lt;/a&gt; (1797 upvotes, by &lt;a href="/u/Conscious_Cut_6144"&gt;u/Conscious_Cut_6144&lt;/a&gt;). &amp;quot;It's alive!&amp;quot; indeed.&lt;/p&gt; &lt;p&gt;Spring brought the highly anticipated Llama 4. Mark Zuckerberg &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1jsampe/mark_presenting_four_llama_4_models_even_a_2/"&gt;presented the models&lt;/a&gt; (2645 upvotes, by &lt;a href="/u/LarDark"&gt;u/LarDark&lt;/a&gt;), but the community felt it &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1jt7hlc/metas_llama_4_fell_short/"&gt;fell short&lt;/a&gt; (2175 upvotes, by &lt;a href="/u/Rare-Site"&gt;u/Rare-Site&lt;/a&gt;). The community was let down, especially when compared to the relentless release schedule from the East.&lt;/p&gt; &lt;p&gt;Open Weight releases continued, though, we got &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1juni3t/deepcoder_a_fully_opensource_14b_coder_at_o3mini/"&gt;DeepCoder&lt;/a&gt; (1609 upvotes, by &lt;a href="/u/TKGaming_11"&gt;u/TKGaming_11&lt;/a&gt;) and saw &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1jytw62/deepseek_is_about_to_opensource_their_inference/"&gt;DeepSeek open-sourcing their inference engine&lt;/a&gt; (1760 upvotes, by &lt;a href="/u/Dr_Karminski"&gt;u/Dr_Karminski&lt;/a&gt;). There was also a moment of collective frustration when &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1jzocoo/finally_someone_noticed_this_unfair_situation/"&gt;llama.cpp was snubbed&lt;/a&gt; (1742 upvotes, by &lt;a href="/u/nekofneko"&gt;u/nekofneko&lt;/a&gt;) in favor of shinier wrappers.&lt;/p&gt; &lt;p&gt;Then came &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ka6mic/qwen_3/"&gt;Qwen 3&lt;/a&gt; (1940 upvotes, by &lt;a href="/u/ResearchCrafty1804"&gt;u/ResearchCrafty1804&lt;/a&gt;). The excitement was back. We were running &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1klx9q2/realtime_webcam_demo_with_smolvlm_using_llamacpp/"&gt;real-time webcam demos with SmolVLM&lt;/a&gt; (2762 upvotes, by &lt;a href="/u/dionisioalcaraz"&gt;u/dionisioalcaraz&lt;/a&gt;) and building &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ktx15j/guys_i_managed_to_build_a_100_fully_local_voice/"&gt;fully local voice AIs&lt;/a&gt; (2447 upvotes, by &lt;a href="/u/RoyalCities"&gt;u/RoyalCities&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;The reality of our hardware addiction hit hard with the question: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ktlz3w/96gb_vram_what_should_run_first/"&gt;&amp;quot;96GB VRAM! What should run first?&amp;quot;&lt;/a&gt; (1745 upvotes, by &lt;a href="/u/Mother_Occasion_8076"&gt;u/Mother_Occasion_8076&lt;/a&gt;). And as &lt;a href="/u/TheLogiqueViper"&gt;u/TheLogiqueViper&lt;/a&gt; noted, &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1kzsa70/china_is_leading_open_source/"&gt;China is leading open source&lt;/a&gt; (2618 upvotes).&lt;/p&gt; &lt;p&gt;We found humor in the absurdity of it all. &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1l6ibwg/when_you_figure_out_its_all_just_math/"&gt;&amp;quot;When you figure out it’s all just math&amp;quot;&lt;/a&gt; (4123 upvotes, by &lt;a href="/u/Current-Ticket4214"&gt;u/Current-Ticket4214&lt;/a&gt;) was a top post, and we all related to &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1l1qqdx/at_the_airport_people_watching_while_i_run_models/"&gt;running models at the airport&lt;/a&gt; (2378 upvotes, by &lt;a href="/u/Current-Ticket4214"&gt;u/Current-Ticket4214&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;Summer was a season of delays and parodies. &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1lxyvto/we_have_to_delay_it/"&gt;&amp;quot;We have to delay it&amp;quot;&lt;/a&gt; (3574 upvotes, by &lt;a href="/u/ILoveMy2Balls"&gt;u/ILoveMy2Balls&lt;/a&gt;) became the catchphrase for Western labs. We poked fun with a &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1laee7q/got_a_tester_version_of_the_openweight_openai/"&gt;tester version of the &amp;quot;open-weight&amp;quot; OpenAI model&lt;/a&gt; (1639 upvotes, by &lt;a href="/u/Firepal64"&gt;u/Firepal64&lt;/a&gt;) and a &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1lx5awq/friendly_reminder_that_grok_3_should_be_now/"&gt;friendly reminder about Grok 3&lt;/a&gt; (1447 upvotes, by &lt;a href="/u/Wrong_User_Logged"&gt;u/Wrong_User_Logged&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;But the community kept building. &lt;a href="/u/hotroaches4liferz"&gt;u/hotroaches4liferz&lt;/a&gt; made a &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1m39uqi/i_made_a_1000_hour_nsfw_tts_dataset/"&gt;1000 hour NSFW TTS dataset&lt;/a&gt; (1516 upvotes)-because of course they did. &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1m6qdet/qwen3coder_is_here/"&gt;Qwen3-Coder arrived&lt;/a&gt; (1925 upvotes, by &lt;a href="/u/ResearchCrafty1804"&gt;u/ResearchCrafty1804&lt;/a&gt;), followed by the blazing fast &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1me31d8/qwen3coderflash_released/"&gt;Qwen3-Coder-Flash&lt;/a&gt; (1694 upvotes).&lt;/p&gt; &lt;p&gt;The sentiment shifted as Meta seemingly bowed out of open source: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1md6t2h/bye_bye_meta_ai_it_was_good_while_it_lasted/"&gt;&amp;quot;Bye bye, Meta AI&amp;quot;&lt;/a&gt; (1492 upvotes, by &lt;a href="/u/absolooot1"&gt;u/absolooot1&lt;/a&gt;). Meanwhile, we got the adorable &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1mhyzp7/kitten_tts_sota_supertiny_tts_model_less_than_25/"&gt;Kitten TTS&lt;/a&gt; (2460 upvotes, by &lt;a href="/u/ElectricalBar7464"&gt;u/ElectricalBar7464&lt;/a&gt;) and continued to dream of &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1mllt5x/imagine_an_open_source_code_model_that_in_the/"&gt;open source code models rivaling Claude&lt;/a&gt; (2304 upvotes, by &lt;a href="/u/Severe-Awareness829"&gt;u/Severe-Awareness829&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;&lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; remained &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1mnxodk/localllama_is_the_last_sane_place_to_discuss_llms/"&gt;&amp;quot;the last sane place to discuss LLMs&amp;quot;&lt;/a&gt; (2181 upvotes, by &lt;a href="/u/ForsookComparison"&gt;u/ForsookComparison&lt;/a&gt;). Even if we did have to vent about &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1mncrqp/ollama/"&gt;Ollama&lt;/a&gt; (1906 upvotes, by &lt;a href="/u/jacek2023"&gt;u/jacek2023&lt;/a&gt;) occasionally.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1n46ify/finally_china_entering_the_gpu_market_to_destroy/"&gt;China entering the GPU market&lt;/a&gt; (4171 upvotes, by &lt;a href="/u/CeFurkan"&gt;u/CeFurkan&lt;/a&gt;) with 96GB cards for under $2000 was a game-changer. Some of us even went to Shenzhen to &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nifajh/i_bought_a_modded_4090_48gb_in_shenzhen_this_is/"&gt;buy modded 4090s&lt;/a&gt; (1924 upvotes, by &lt;a href="/u/king_priam_of_Troy"&gt;u/king_priam_of_Troy&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;We celebrated the &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nz722n/biggest_provider_for_the_community_for_at_moment/"&gt;biggest providers for the community&lt;/a&gt; (2918 upvotes, by &lt;a href="/u/dead-supernova"&gt;u/dead-supernova&lt;/a&gt;)-mostly Chinese labs now-and devoured &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1oakwgs/stanford_just_dropped_55hrs_worth_of_lectures_on/"&gt;Stanford's 5.5hrs of lectures&lt;/a&gt; (2731 upvotes, by &lt;a href="/u/igorwarzocha"&gt;u/igorwarzocha&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;The year ended with a mix of high-level tools and deep-dive resources. We got &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1oymku1/heretic_fully_automatic_censorship_removal_for/"&gt;Heretic for automatic censorship removal&lt;/a&gt; (3008 upvotes, by &lt;a href="/u/-p-e-w-"&gt;u/-p-e-w-&lt;/a&gt;) and &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ok3xie/200_pages_of_hugging_face_secrets_on_how_to_train/"&gt;200+ pages of Hugging Face secrets&lt;/a&gt; (2204 upvotes, by &lt;a href="/u/eliebakk"&gt;u/eliebakk&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;And finally, the memes kept us grounded. The &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1pqegcr/realist_meme_of_the_year/"&gt;Realist meme of the year&lt;/a&gt; (1926 upvotes, by &lt;a href="/u/Slight_Tone_2188"&gt;u/Slight_Tone_2188&lt;/a&gt;) reminded us that no matter how advanced the models get, we'll always be RAM poor from now on.&lt;/p&gt; &lt;p&gt;That's it, folks. 2025 was the year the open-source torch passed to the East, the year our hardware dreams got a little wilder (and insanely more expensive). Here's to another year of local LLMs!&lt;/p&gt; &lt;p&gt;P.S. I wasn't going to make a recap this year, but &lt;a href="https://gist.github.com/qingy1337"&gt;qingy1337&lt;/a&gt; kindly asked on GitHub if I would which touched me. So here it is!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Everlier"&gt; /u/Everlier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptr3lv/rlocalllama_a_year_in_review/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptr3lv/rlocalllama_a_year_in_review/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptr3lv/rlocalllama_a_year_in_review/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T10:56:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptdtmz</id>
    <title>DGX Spark: an unpopular opinion</title>
    <updated>2025-12-22T23:05:29+00:00</updated>
    <author>
      <name>/u/emdblc</name>
      <uri>https://old.reddit.com/user/emdblc</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptdtmz/dgx_spark_an_unpopular_opinion/"&gt; &lt;img alt="DGX Spark: an unpopular opinion" src="https://preview.redd.it/cktkoyb16u8g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ddc1aaf35931031505022ffcc5838d1fb7a1a8ea" title="DGX Spark: an unpopular opinion" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know there has been a lot of criticism about the DGX Spark here, so I want to share some of my personal experience and opinion:&lt;/p&gt; &lt;p&gt;I’m a doctoral student doing data science in a small research group that doesn’t have access to massive computing resources. We only have a handful of V100s and T4s in our local cluster, and limited access to A100s and L40s on the university cluster (two at a time). Spark lets us prototype and train foundation models, and (at last) compete with groups that have access to high performance GPUs like the H100s or H200s.&lt;/p&gt; &lt;p&gt;I want to be clear: Spark is NOT faster than an H100 (or even a 5090). But its all-in-one design and its massive amount of memory (all sitting on your desk) enable us — a small group with limited funding, to do more research.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/emdblc"&gt; /u/emdblc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/cktkoyb16u8g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptdtmz/dgx_spark_an_unpopular_opinion/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptdtmz/dgx_spark_an_unpopular_opinion/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T23:05:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1pt50mt</id>
    <title>AMA Announcement: Z.ai, The Opensource Lab Behind GLM-4.7 (Tuesday, 8AM-11AM PST)</title>
    <updated>2025-12-22T17:12:21+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pt50mt/ama_announcement_zai_the_opensource_lab_behind/"&gt; &lt;img alt="AMA Announcement: Z.ai, The Opensource Lab Behind GLM-4.7 (Tuesday, 8AM-11AM PST)" src="https://preview.redd.it/r06ch4zyfs8g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e0a10789f393f350618520fcb81174f3a3dae1c7" title="AMA Announcement: Z.ai, The Opensource Lab Behind GLM-4.7 (Tuesday, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/r06ch4zyfs8g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pt50mt/ama_announcement_zai_the_opensource_lab_behind/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pt50mt/ama_announcement_zai_the_opensource_lab_behind/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T17:12:21+00:00</published>
  </entry>
</feed>
