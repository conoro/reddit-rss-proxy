<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-06T13:52:10+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1opt2q9</id>
    <title>Building LangChain &amp; LangGraph Concepts From Scratch (Next Step in My AI Agents Repo)</title>
    <updated>2025-11-06T07:58:54+00:00</updated>
    <author>
      <name>/u/purellmagents</name>
      <uri>https://old.reddit.com/user/purellmagents</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm extending my &lt;em&gt;ai-agents-from-scratch&lt;/em&gt; project, the one that teaches AI agent fundamentals in plain JavaScript using local models via &lt;code&gt;node-llama-cpp,&lt;/code&gt;with a new section focused on &lt;strong&gt;re-implementing core concepts from LangChain and LangGraph&lt;/strong&gt; step by step.&lt;/p&gt; &lt;p&gt;The goal is to get from understanding the fundamentals to build ai agents for production by understanding LangChain / LangGraph core principles.&lt;/p&gt; &lt;h1&gt;What Exists So Far&lt;/h1&gt; &lt;p&gt;The repo already has nine self-contained examples under &lt;code&gt;examples/&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;intro/ ‚Üí basic LLM call simple-agent/ ‚Üí tool-using agent react-agent/ ‚Üí ReAct pattern memory-agent/ ‚Üí persistent state &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Everything runs locally - no API keys or external services.&lt;/p&gt; &lt;h1&gt;What‚Äôs Coming Next&lt;/h1&gt; &lt;p&gt;A new series of lessons where you implement the pieces that make frameworks like LangChain tick:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Foundations&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The Runnable abstraction - why everything revolves around it&lt;/li&gt; &lt;li&gt;Message types and structured conversation data&lt;/li&gt; &lt;li&gt;LLM wrappers for &lt;code&gt;node-llama-cpp&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Context and configuration management&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Composition and Agency&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Prompts, parsers, and chains&lt;/li&gt; &lt;li&gt;Memory and state&lt;/li&gt; &lt;li&gt;Tool execution and agent loops&lt;/li&gt; &lt;li&gt;Graphs, routing, and checkpointing&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Each lesson combines explanation, implementation, and small exercises that lead to a working system.&lt;br /&gt; You end up with your own mini-LangChain - and a full understanding of how modern agent frameworks are built.&lt;/p&gt; &lt;h1&gt;Why I‚Äôm Doing This&lt;/h1&gt; &lt;p&gt;Most tutorials show how to &lt;em&gt;use&lt;/em&gt; frameworks, not how they work.&lt;br /&gt; You learn syntax but not architecture.&lt;br /&gt; This project bridges that gap: start from raw function calls, build abstractions, and then use real frameworks with clarity.&lt;/p&gt; &lt;h1&gt;What I‚Äôd Like Feedback On&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Would you find value in &lt;em&gt;building&lt;/em&gt; a framework before using one?&lt;/li&gt; &lt;li&gt;Is the progression (basics ‚Üí build framework ‚Üí use frameworks) logical?&lt;/li&gt; &lt;li&gt;Would you actually code through the exercises or just read?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The first lesson (Runnable) is available.&lt;br /&gt; I plan to release one new lesson per week.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/pguso/ai-agents-from-scratch"&gt;https://github.com/pguso/ai-agents-from-scratch&lt;/a&gt;&lt;br /&gt; If this approach sounds useful, I‚Äôd appreciate feedback before I finalize the full series.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/purellmagents"&gt; /u/purellmagents &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opt2q9/building_langchain_langgraph_concepts_from/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opt2q9/building_langchain_langgraph_concepts_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1opt2q9/building_langchain_langgraph_concepts_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T07:58:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1opl54d</id>
    <title>What are you doing with your 128GB Mac?</title>
    <updated>2025-11-06T01:04:17+00:00</updated>
    <author>
      <name>/u/Technical_Pass_1858</name>
      <uri>https://old.reddit.com/user/Technical_Pass_1858</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a MacBook Pro M3Max 128GB,I think I do not use it effectively.&lt;/p&gt; &lt;p&gt;So I wander what are you doing with it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Technical_Pass_1858"&gt; /u/Technical_Pass_1858 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opl54d/what_are_you_doing_with_your_128gb_mac/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opl54d/what_are_you_doing_with_your_128gb_mac/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1opl54d/what_are_you_doing_with_your_128gb_mac/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T01:04:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1opyxu7</id>
    <title>11 problems nobody talks about building Agents (and how to approach them)</title>
    <updated>2025-11-06T13:29:42+00:00</updated>
    <author>
      <name>/u/Acrobatic-Pay-279</name>
      <uri>https://old.reddit.com/user/Acrobatic-Pay-279</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opyxu7/11_problems_nobody_talks_about_building_agents/"&gt; &lt;img alt="11 problems nobody talks about building Agents (and how to approach them)" src="https://external-preview.redd.it/E7osLo16dHGyGHMFS5WAFF_cR8bnX9KL3VZzbEytdXs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9746ebdb6c765683910f5c69deca74c8bf22a555" title="11 problems nobody talks about building Agents (and how to approach them)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been working on AI agents for a while now. It‚Äôs fun, but some parts are genuinely tough to get right. Over time, I have kept a mental list of things that consistently slow me down.&lt;/p&gt; &lt;p&gt;These are the hardest issues I have hit (and how you can approach each of them).&lt;/p&gt; &lt;h1&gt;1. Overly Complex Frameworks&lt;/h1&gt; &lt;p&gt;I think the biggest challenge is using agent frameworks that try to do everything and end up feeling like overkill.&lt;/p&gt; &lt;p&gt;Those are powerful and can do amazing things, but in practice you use ~10% of it and then you realize that it's too complex to do the simple, specific things you need it to do. You end up fighting the framework instead of building with it.&lt;/p&gt; &lt;p&gt;For example: in &lt;strong&gt;LangChain&lt;/strong&gt;, defining a simple agent with a single tool can involve setting up chains, memory objects, executors and callbacks. That‚Äôs a lot of stuff when all you really need is an LLM call plus one function.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Approach:&lt;/strong&gt; Pick a lightweight building block you actually understand end-to-end. If something like Pydantic AI or SmolAgents (or yes, feel free to plug your own) covers 90% of use cases, build on that. Save the rest for later.&lt;/p&gt; &lt;p&gt;It takes just a few lines of code:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from pydantic_ai import Agent, RunContext roulette_agent = Agent( 'openai:gpt-4o', deps_type=int, output_type=bool, system_prompt=( 'Use the `roulette_wheel` function to see if the ' 'customer has won based on the number they provide.' ), ) u/roulette_agent.tool async def roulette_wheel(ctx: RunContext[int], square: int) -&amp;gt; str: &amp;quot;&amp;quot;&amp;quot;check if the square is a winner&amp;quot;&amp;quot;&amp;quot; return 'winner' if square == ctx.deps else 'not a winner' # run the agent success_number = 18 result = roulette_agent.run_sync('Put my money on square eighteen', deps=success_number) print(result.output) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;---&lt;/p&gt; &lt;h1&gt;2. No ‚Äúhuman-in-the-loop‚Äù&lt;/h1&gt; &lt;p&gt;Autonomous agents may sound cool, but giving them unrestricted control is bad.&lt;/p&gt; &lt;p&gt;I was experimenting with an MCP Agent for LinkedIn. It was fun to prototype, but I quickly realized there were no natural breakpoints. Giving the agent full control to post or send messages felt risky (one misfire and boom).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Approach:&lt;/strong&gt; The fix is to introduce &lt;strong&gt;human-in-the-loop (HITL) controls&lt;/strong&gt; which are like safe breakpoints where the agent pauses, shows you its plan or action and waits for approval before continuing.&lt;/p&gt; &lt;p&gt;Here's a simple example pattern:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# Pseudo-code def approval_hook(action, context): print(f&amp;quot;Agent wants to: {action}&amp;quot;) user_approval = input(&amp;quot;Approve? (y/n): &amp;quot;) return user_approval.lower().startswith('y') # Use in agent workflow if approval_hook(&amp;quot;send_email&amp;quot;, email_context): agent.execute_action(&amp;quot;send_email&amp;quot;) else: agent.abort(&amp;quot;User rejected action&amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The upshot is: you stay in control.&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;h1&gt;3. Black-Box Reasoning&lt;/h1&gt; &lt;p&gt;Half the time, I can‚Äôt explain why my agent did what it did. It will take some weird action, skip an obvious step or make weird assumptions -- all hidden behind ‚ÄúLLM logic‚Äù.&lt;/p&gt; &lt;p&gt;The whole thing feels like a black box where the plan is hidden.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Approach:&lt;/strong&gt; Force your agent to expose its reasoning: structured plans, decision logs, traceable steps. Use tools like LangGraph, OpenTelemetry or logging frameworks to surface ‚Äúwhy‚Äù rather than just seeing ‚Äúwhat‚Äù.&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;h1&gt;4. Tool-Calling Reliability Issues&lt;/h1&gt; &lt;p&gt;Here‚Äôs the thing about agents: they are only as strong as the tools they connect to. And those tools? They change.&lt;/p&gt; &lt;p&gt;Rate-limits hit. Schema drifts. Suddenly your agent agent has no idea how to handle that so it just fails mid-task.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Approach:&lt;/strong&gt; Don‚Äôt assume the tool will stay perfect forever.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Treat tools as versioned contracts -- enforce schemas &amp;amp; validate arguments&lt;/li&gt; &lt;li&gt;Add retries and fallbacks instead of failing on the first error&lt;/li&gt; &lt;li&gt;Follow open standards like MCP (used by OpenAI) or A2A to reduce schema mismatches.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In Composio, every tool is fully described with a JSON schema for its inputs and outputs. Their API returns an error code if the JSON doesn‚Äôt match the expected schema.&lt;/p&gt; &lt;p&gt;You can catch this and handle it (for example, prompting the LLM to retry or falling back to a clarification step).&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from composio_openai import ComposioToolSet, Action # Get structured, validated tools toolset = ComposioToolSet() tools = toolset.get_tools(actions=[Action.GITHUB_STAR_A_REPOSITORY_FOR_THE_AUTHENTICATED_USER]) # Tools come with built-in validation and error handling response = openai.chat.completions.create( model=&amp;quot;gpt-4&amp;quot;, tools=tools, messages=[{&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Star the composio repository&amp;quot;}] ) # Handle tool calls with automatic retry logic result = toolset.handle_tool_calls(response) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;They also allow fine-tuning of the tool definitions further guides the LLM to use tools correctly.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Who‚Äôs doing what today:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;LangChain ‚Üí Structured tool calling with Pydantic validation.&lt;/li&gt; &lt;li&gt;LlamaIndex ‚Üí Built-in retry patterns &amp;amp; validator engines for self-correcting queries.&lt;/li&gt; &lt;li&gt;CrewAI ‚Üí Error recovery, handling, structured retry flows.&lt;/li&gt; &lt;li&gt;Composio ‚Üí 500+ integrations with prebuilt OAuth handling and robust tool-calling architecture.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;---&lt;/p&gt; &lt;h1&gt;5. Token Consumption Explosion&lt;/h1&gt; &lt;p&gt;One of the sneakier problems with agents is how fast they can consume tokens. The worst part? I couldn‚Äôt even see what was going on under the hood. I had no visibility into the exact prompts, token counts, cache hits and costs flowing through the LLM.&lt;/p&gt; &lt;p&gt;Because we stuffed the full conversation history, every tool result, every prompt into the context window.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Approach:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Split short-term vs long-term memory&lt;/li&gt; &lt;li&gt;Purge or summarise stale context&lt;/li&gt; &lt;li&gt;Only feed what the model needs now&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&amp;#8203;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;context.append(user_message) if token_count(context) &amp;gt; MAX_TOKENS: summary = llm(&amp;quot;Summarize: &amp;quot; + &amp;quot; &amp;quot;.join(context)) context = [summary] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Some frameworks like AutoGen, cache LLM calls to avoid repeat requests, supporting backends like disk, Redis, Cosmos DB.&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;h1&gt;6. State &amp;amp; Context Loss&lt;/h1&gt; &lt;p&gt;You kick off a plan, great! Halfway through, the agent forgets what it was doing or loses track of an earlier decision. Why? Because all the ‚Äústate‚Äù was inside the prompt and the prompt maxed out or was truncated.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Approach:&lt;/strong&gt; Externalize memory/state: use vector DBs, graph flows, persisted run-state files. On crashes or restarts, load what you already did and resume rather than restart.&lt;/p&gt; &lt;p&gt;For ex: LlamaIndex provides &lt;code&gt;ChatMemoryBuffer&lt;/code&gt; &amp;amp; storage connectors for persisting conversation state.&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;h1&gt;7. Multi-Agent Coordination Nightmares&lt;/h1&gt; &lt;p&gt;You split your work: ‚Äúplanner‚Äù agent, ‚Äúresearcher‚Äù agent, ‚Äúwriter‚Äù agent. Great in theory. But now you have routing to manage, memory sharing, who invokes who, when. It becomes spaghetti.&lt;/p&gt; &lt;p&gt;And if you scale to five or ten agents, the sync overhead can feel a lot worse (when you are coding the whole thing yourself).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Approach:&lt;/strong&gt; Don‚Äôt free-form it at first. Adopt protocols (like A2A, ACP) for structured agent-to-agent handoffs. Define roles, clear boundaries, explicit orchestration. If you only need one agent, don‚Äôt over-architect.&lt;/p&gt; &lt;p&gt;Start with the simplest design: if you really need sub-agents, manually code an agent-to-agent handoff.&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;h1&gt;8. Long-term memory problem&lt;/h1&gt; &lt;p&gt;Too much memory = token chaos.&lt;br /&gt; Too little = agent forgets important facts.&lt;/p&gt; &lt;p&gt;This is the ‚Äúmemory bottleneck‚Äù, you have to decide ‚Äúwhat to remember, what to forget and when‚Äù in a systematic way.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Approach:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Naive approaches don‚Äôt cut it. Treat memory layers:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Short-term: current conversation, active plan&lt;/li&gt; &lt;li&gt;Long-term: important facts, user preferences, permanent state&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Frameworks like Mem0 have a purpose-built memory layer for agents with relevance scoring &amp;amp; long-term recall, while Letta (another framework) uses a memory graph with explicit ‚Äúwhen/how to forget‚Äù rules baked in.&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;h1&gt;9. The ‚ÄúAlmost Right‚Äù Code Problem&lt;/h1&gt; &lt;p&gt;The biggest frustration developers (including me) face is dealing with AI-generated solutions that are &amp;quot;almost right, but not quite&amp;quot;.&lt;/p&gt; &lt;p&gt;Debugging that ‚Äúalmost right‚Äù output often takes longer than just writing the function yourself.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Approach:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;There‚Äôs not much we can do here (this is a model-level issue) but you can add guardrails and sanity checks.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Check types, bounds, output shape.&lt;/li&gt; &lt;li&gt;If you expect a date, validate its format.&lt;/li&gt; &lt;li&gt;Use self-reflection steps in the agent.&lt;/li&gt; &lt;li&gt;Add test cases inside the loop.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Some frameworks support `chain-of-thought reflection` or `self-correction steps`.&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;h1&gt;10. Authentication &amp;amp; Security Trust Issue&lt;/h1&gt; &lt;p&gt;Security is usually an afterthought in an agent's architecture. So handling authentication is tricky with agents.&lt;/p&gt; &lt;p&gt;On paper, it seems simple: give the agent an API key and let it call the service. But in practice, this is one of the fastest ways to create security holes (like MCP Agents).&lt;/p&gt; &lt;p&gt;Role-based access controls must propagate to all agents and any data touched by an LLM becomes &amp;quot;totally public with very little effort&amp;quot;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Approach:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Least-privilege access&lt;/li&gt; &lt;li&gt;Let agents request access only when needed (use OAuth flows or Token Vault mechanisms)&lt;/li&gt; &lt;li&gt;Track all API calls and enforce role-based access via an identity provider (Auth0, Okta)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Assume your whole agent is an attack surface.&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;h1&gt;11. No Real-Time Awareness (Event Triggers)&lt;/h1&gt; &lt;p&gt;Many agents are still built on a ‚ÄúYou ask ‚Üí I respond‚Äù loop. That‚Äôs in-scope but not enough.&lt;/p&gt; &lt;p&gt;What if an external event occurs (Slack message, DB update, calendar event)? If your agent can‚Äôt react then you are just building a chatbot, not a true agent.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Approach:&lt;/strong&gt; Plug into event sources/webhooks, set triggers, give your agent ‚Äúears‚Äù and ‚Äúeyes‚Äù beyond user prompts.&lt;/p&gt; &lt;p&gt;Just use a managed trigger platform instead of rolling your own webhook system. Like Composio Triggers can send payloads to your AI agents (you can also go with the SDK listener). Here's the webhook approach.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;app = FastAPI() client = OpenAI() toolset = ComposioToolSet() @app.post(&amp;quot;/webhook&amp;quot;) async def webhook_handler(request: Request): payload = await request.json() # Handle Slack message events if payload.get(&amp;quot;type&amp;quot;) == &amp;quot;slack_receive_message&amp;quot;: text = payload[&amp;quot;data&amp;quot;].get(&amp;quot;text&amp;quot;, &amp;quot;&amp;quot;) # Pass the event to your LLM agent tools = toolset.get_tools([Action.SLACK_SENDS_A_MESSAGE_TO_A_SLACK_CHANNEL]) resp = client.chat.completions.create( model=&amp;quot;gpt-4o&amp;quot;, messages=[ {&amp;quot;role&amp;quot;: &amp;quot;system&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;You are a witty Slack bot.&amp;quot;}, {&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: f&amp;quot;User says: {text}&amp;quot;}, ], tools=tools ) # Execute the tool call (sends a reply to Slack) toolset.handle_tool_calls(resp, entity_id=&amp;quot;default&amp;quot;) return {&amp;quot;status&amp;quot;: &amp;quot;ok&amp;quot;} &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This pattern works for any app integration.&lt;/p&gt; &lt;p&gt;The trigger payload includes context (message text, user, channel, ...) so your agent can use that as part of its reasoning or pass it directly to a tool.&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;At the end of the day, agents break for the same old reasons. I think most of the possible fixes are the boring stuff nobody wants to do.&lt;/p&gt; &lt;p&gt;Which of these have you hit in your own agent builds? And how did (or will) you approach them.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Acrobatic-Pay-279"&gt; /u/Acrobatic-Pay-279 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://composio.dev/blog/11-problems-i-have-noticed-building-agents-(and-fixes-nobody-talks-about)"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opyxu7/11_problems_nobody_talks_about_building_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1opyxu7/11_problems_nobody_talks_about_building_agents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T13:29:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1opzanc</id>
    <title>3 RTX 3090 graphics cards in a computer for inference and neural network training</title>
    <updated>2025-11-06T13:44:36+00:00</updated>
    <author>
      <name>/u/Standard-Heat4706</name>
      <uri>https://old.reddit.com/user/Standard-Heat4706</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to build a sufficiently powerful PC for ML within my budget. I have enough money for 3√ó RTX 3090s or a single RTX 5090. In terms of performance, they‚Äôre roughly comparable (3 √ó 35.58 TFLOPS FP32 vs 1 √ó 104.8 TFLOPS FP32), but the 3√ó RTX 3090s have more VRAM (3 √ó 24 GB vs 1 √ó 32 GB). As I understand it, to run three GPUs well I need a server-grade CPU (for example, Intel Xeon or AMD EPYC) to have enough PCIe lanes. Also, if I‚Äôm understanding correctly, NVLink works with at most 2 GPUs, and with 3 they can only communicate via PCIe - how much will this affect the speed of neural network inference and training? Which GPUs should I get?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Standard-Heat4706"&gt; /u/Standard-Heat4706 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opzanc/3_rtx_3090_graphics_cards_in_a_computer_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opzanc/3_rtx_3090_graphics_cards_in_a_computer_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1opzanc/3_rtx_3090_graphics_cards_in_a_computer_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T13:44:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1opsg7i</id>
    <title>Engineer's Guide to Local LLMs with LLaMA.cpp on Linux</title>
    <updated>2025-11-06T07:18:23+00:00</updated>
    <author>
      <name>/u/Limp_Classroom_2645</name>
      <uri>https://old.reddit.com/user/Limp_Classroom_2645</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opsg7i/engineers_guide_to_local_llms_with_llamacpp_on/"&gt; &lt;img alt="Engineer's Guide to Local LLMs with LLaMA.cpp on Linux" src="https://external-preview.redd.it/US-UbbTxz7m8KlFQtgZDs91DLDYBKPSct91uvqvv4to.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1c898e5efa44ccbe05624e26f2bd67f4adb9aa14" title="Engineer's Guide to Local LLMs with LLaMA.cpp on Linux" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Limp_Classroom_2645"&gt; /u/Limp_Classroom_2645 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://avatsaev.substack.com/p/engineers-guide-to-local-llms-with"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opsg7i/engineers_guide_to_local_llms_with_llamacpp_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1opsg7i/engineers_guide_to_local_llms_with_llamacpp_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T07:18:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1oprpxq</id>
    <title>What is the point of Nvidia's Jet-Nemotron-2B?</title>
    <updated>2025-11-06T06:34:02+00:00</updated>
    <author>
      <name>/u/Ok_Warning2146</name>
      <uri>https://old.reddit.com/user/Ok_Warning2146</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In their paper, they claimed 10x faster tokens per sec than its parent model Qwen2.5-1.5B. But in my own test using huggingface transformers, this is not the case.&lt;/p&gt; &lt;p&gt;My setup:&lt;br /&gt; RTX 3050 6GB 70W transformers 4.53.0&lt;br /&gt; FA2 enabled at bfloat16&lt;br /&gt; max_length=1024&lt;br /&gt; temperature=0.1&lt;br /&gt; top_p=0.8&lt;br /&gt; repetitive_penalty=1.25&lt;br /&gt; system: You are a European History Professor named Professor Whitman.&lt;br /&gt; prompt: Why was Duke Vladivoj enfeoffed Duchy of Bohemia with the Holy Roman Empire in 1002? Does that mean Duchy of Bohemia was part of the Holy Roman Empire already? If so, when did the Holy Roman Empire acquired Bohemia?&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;tokens&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma-3-1b-it&lt;/td&gt; &lt;td align="left"&gt;296&lt;/td&gt; &lt;td align="left"&gt;1.36&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-1.7B&lt;/td&gt; &lt;td align="left"&gt;943&lt;/td&gt; &lt;td align="left"&gt;5.01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-1.7B /nothink&lt;/td&gt; &lt;td align="left"&gt;669&lt;/td&gt; &lt;td align="left"&gt;4.98&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Jet-Nemotron-2B&lt;/td&gt; &lt;td align="left"&gt;943&lt;/td&gt; &lt;td align="left"&gt;2.30&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen2.5-1.5B&lt;/td&gt; &lt;td align="left"&gt;354&lt;/td&gt; &lt;td align="left"&gt;6.09&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Surprisingly, gemma-3-1b-it seems very good for its size. However, it is quite slow and strangely VRAM usage is growing gradually to 5GB while decoding. Is there any way to fix this?&lt;/p&gt; &lt;p&gt;Qwen2.5-1.5B is useless as it generates Chinese when asked an English question. &lt;/p&gt; &lt;p&gt;Qwen3-1.7B runs fast but it is very verbose in thinking mode. Turning off thinking seems to give better answer for historical questions. But both seems to suffer hallucinations.&lt;/p&gt; &lt;p&gt;Jet-Nemotron 2B is slower than Qwen3-1.7B and the reply while ok in the beginning, it was outputting nouns from the middle. So what is the point? I can only see the theoretical KV cache saving here. Or is there something I can set to make it work as expected?&lt;/p&gt; &lt;p&gt;Replies from LLMs are detailed in the replies in this thread.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Warning2146"&gt; /u/Ok_Warning2146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oprpxq/what_is_the_point_of_nvidias_jetnemotron2b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oprpxq/what_is_the_point_of_nvidias_jetnemotron2b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oprpxq/what_is_the_point_of_nvidias_jetnemotron2b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T06:34:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1opz7s0</id>
    <title>The 1 Billion Token Challenge: Finding the Perfect Pre-training Mix</title>
    <updated>2025-11-06T13:41:14+00:00</updated>
    <author>
      <name>/u/asankhs</name>
      <uri>https://old.reddit.com/user/asankhs</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opz7s0/the_1_billion_token_challenge_finding_the_perfect/"&gt; &lt;img alt="The 1 Billion Token Challenge: Finding the Perfect Pre-training Mix" src="https://external-preview.redd.it/_7547ybAZ0VtkPRQO9cNQBrH3zJjmJDlBtHalKB63eY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=405a71d422559afcbb722515f04b7c60d0ce6182" title="The 1 Billion Token Challenge: Finding the Perfect Pre-training Mix" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/asankhs"&gt; /u/asankhs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/blog/codelion/optimal-dataset-mixing"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opz7s0/the_1_billion_token_challenge_finding_the_perfect/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1opz7s0/the_1_billion_token_challenge_finding_the_perfect/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T13:41:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1oppykf</id>
    <title>Is OCR accuracy actually a blocker for anyone's RAG/automation pipelines?</title>
    <updated>2025-11-06T04:55:12+00:00</updated>
    <author>
      <name>/u/Individual-Library-1</name>
      <uri>https://old.reddit.com/user/Individual-Library-1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Genuine question for the group -&lt;/p&gt; &lt;p&gt;I've been building document automation systems (litigation, compliance, NGO tools) and keep running into the same issue: &lt;strong&gt;OCR accuracy becomes the bottleneck that caps your entire system's reliability.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Specifically with complex documents:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Financial reports with tables + charts + multi-column text&lt;/li&gt; &lt;li&gt;Legal documents with footnotes, schedules, exhibits&lt;/li&gt; &lt;li&gt;Technical manuals with diagrams embedded in text&lt;/li&gt; &lt;li&gt;Scanned forms where structure matters (not just text extraction)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I've tried Google Vision, Azure Document Intelligence, Mistral APIs - they're good, but when you're building production systems where 95% accuracy means 1 in 20 documents has errors, that's not good enough. Especially when the errors are in the critical parts (tables, structured data).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My question:&lt;/strong&gt; Is this actually a problem for your workflows?&lt;/p&gt; &lt;p&gt;Or is &amp;quot;good enough&amp;quot; OCR + error handling downstream actually fine, and I'm overthinking this?&lt;/p&gt; &lt;p&gt;I'm trying to understand if OCR quality is a real bottleneck for people building with n8n/LangChain/LlamaIndex, or if it's just my specific use case.&lt;/p&gt; &lt;p&gt;For context: I ended up fine-tuning Qwen3-VL on document OCR and it's working better for complex layouts. Thinking about opening up an API for testing if people actually need this. But want to understand the problem first before I waste time building infrastructure nobody needs.&lt;/p&gt; &lt;p&gt;Appreciate any thoughts.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Individual-Library-1"&gt; /u/Individual-Library-1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oppykf/is_ocr_accuracy_actually_a_blocker_for_anyones/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oppykf/is_ocr_accuracy_actually_a_blocker_for_anyones/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oppykf/is_ocr_accuracy_actually_a_blocker_for_anyones/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T04:55:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1opp94f</id>
    <title>Where are my 5060ti brothers at.</title>
    <updated>2025-11-06T04:17:24+00:00</updated>
    <author>
      <name>/u/do_u_think_im_spooky</name>
      <uri>https://old.reddit.com/user/do_u_think_im_spooky</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opp94f/where_are_my_5060ti_brothers_at/"&gt; &lt;img alt="Where are my 5060ti brothers at." src="https://preview.redd.it/los33pewbkzf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0be5dff1428a671994dc9cfd688873fadb4454cd" title="Where are my 5060ti brothers at." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Figured I'd take part in sharing my local AI setup.&lt;/p&gt; &lt;p&gt;Dell Precision T7810 Dual Xeon E5 2680 v4 28c 56t 128GB DDR4 2400MHz Dual RTX 5060 ti 16GB&lt;/p&gt; &lt;p&gt;Originally purchased the Dell before getting into LLMs for homelab services but in the past few months I've dipped my toes into the local AI rabbit hole and it keeps getting deeper...&lt;/p&gt; &lt;p&gt;Running proxmox as the hypervisor and have dedicated containers for my inference engine and chat interface. I started with ollama but now I'm using llama.cpp with llama-swap for easy model swapping. Using openwebui because I'm yet to find something that's better and worth switching to.&lt;/p&gt; &lt;p&gt;What are your use cases or projects you utilize your local AI for?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/do_u_think_im_spooky"&gt; /u/do_u_think_im_spooky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/los33pewbkzf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opp94f/where_are_my_5060ti_brothers_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1opp94f/where_are_my_5060ti_brothers_at/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T04:17:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1oplwcv</id>
    <title>You can now Fine-tune DeepSeek-OCR locally!</title>
    <updated>2025-11-06T01:39:01+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oplwcv/you_can_now_finetune_deepseekocr_locally/"&gt; &lt;img alt="You can now Fine-tune DeepSeek-OCR locally!" src="https://preview.redd.it/q1dmxwy4h9zf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9cac1e07107ec19f184e22c98ba69c518cbd1fa5" title="You can now Fine-tune DeepSeek-OCR locally!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/q1dmxwy4h9zf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oplwcv/you_can_now_finetune_deepseekocr_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oplwcv/you_can_now_finetune_deepseekocr_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T01:39:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1oppdxi</id>
    <title>Llama.cpp vs Ollama - Same model, parameters and system prompts but VASTLY different experiences</title>
    <updated>2025-11-06T04:24:25+00:00</updated>
    <author>
      <name>/u/ubrtnk</name>
      <uri>https://old.reddit.com/user/ubrtnk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm slowly seeing the light on Llama.cpp now that I understand how Llama-swap works. I've got the new Qwen3-VL models working good. &lt;/p&gt; &lt;p&gt;However, GPT-OSS:20B is the default model that the family uses before deciding if they need to branch off out to bigger models or specialized models.&lt;/p&gt; &lt;p&gt;However, 20B on Ollama works about 90-95% of the time the way I want. MCP tools work, it searches the internet when it needs to with my MCP Websearch pipeline thru n8n. &lt;/p&gt; &lt;p&gt;20B in Llama.cpp though is VASTLY inconsistent other than when it's consistently non-sensical . I've got my Temp at 1.0, repeat penalty on 1.1 , top K at 0 and top p at 1.0, just like the Unsloth guide. It makes things up more frequently, ignores the system prompt and what the rules for tool usage are and sometimes the /think tokens spill over into the normal responses.&lt;/p&gt; &lt;p&gt;WTF&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ubrtnk"&gt; /u/ubrtnk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oppdxi/llamacpp_vs_ollama_same_model_parameters_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oppdxi/llamacpp_vs_ollama_same_model_parameters_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oppdxi/llamacpp_vs_ollama_same_model_parameters_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T04:24:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1opo5k8</id>
    <title>Explanation of Gated DeltaNet (Qwen3-Next and Kimi Linear)</title>
    <updated>2025-11-06T03:23:03+00:00</updated>
    <author>
      <name>/u/seraschka</name>
      <uri>https://old.reddit.com/user/seraschka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opo5k8/explanation_of_gated_deltanet_qwen3next_and_kimi/"&gt; &lt;img alt="Explanation of Gated DeltaNet (Qwen3-Next and Kimi Linear)" src="https://external-preview.redd.it/docq3T1hMcX3nsU3Xo2w0YwpdMWU2MziLAnrwt_wi_s.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=675bb873c375977b90591f1eec84b3474fb6b564" title="Explanation of Gated DeltaNet (Qwen3-Next and Kimi Linear)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/seraschka"&gt; /u/seraschka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://sebastianraschka.com/llms-from-scratch/ch04/08_deltanet/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opo5k8/explanation_of_gated_deltanet_qwen3next_and_kimi/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1opo5k8/explanation_of_gated_deltanet_qwen3next_and_kimi/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T03:23:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1opl1j0</id>
    <title>AMD to launch gaming-oriented Ryzen AI MAX+ 388 &amp; 392 "Strix Halo" APUs with full Radeon 8060S graphics - VideoCardz.com</title>
    <updated>2025-11-06T00:59:59+00:00</updated>
    <author>
      <name>/u/evil0sheep</name>
      <uri>https://old.reddit.com/user/evil0sheep</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opl1j0/amd_to_launch_gamingoriented_ryzen_ai_max_388_392/"&gt; &lt;img alt="AMD to launch gaming-oriented Ryzen AI MAX+ 388 &amp;amp; 392 &amp;quot;Strix Halo&amp;quot; APUs with full Radeon 8060S graphics - VideoCardz.com" src="https://external-preview.redd.it/WU5-XQvK1clH2V4Ad2EQWo3oFOrLJrxUzGAOS1faPQA.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=874d8a08b9396a7dc93e7cc0e412e2285f20e269" title="AMD to launch gaming-oriented Ryzen AI MAX+ 388 &amp;amp; 392 &amp;quot;Strix Halo&amp;quot; APUs with full Radeon 8060S graphics - VideoCardz.com" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looks like the same GPU and memory interface but 8 CPU cores instead of 16 so maybe a bit cheaper&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/evil0sheep"&gt; /u/evil0sheep &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://videocardz.com/newz/amd-to-launch-gaming-oriented-ryzen-ai-max-388-strix-halo-apu-with-full-radeon-8060s-graphics"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opl1j0/amd_to_launch_gamingoriented_ryzen_ai_max_388_392/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1opl1j0/amd_to_launch_gamingoriented_ryzen_ai_max_388_392/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T00:59:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1opxb1r</id>
    <title>Text-to-Speech (TTS) models &amp; Tools for 8GB VRAM?</title>
    <updated>2025-11-06T12:14:17+00:00</updated>
    <author>
      <name>/u/pmttyji</name>
      <uri>https://old.reddit.com/user/pmttyji</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm a GGUF guy. I use Jan, Koboldcpp, llama.cpp for Text models. Now I'm starting to experiment with Audio models(TTS - Text to Speech).&lt;/p&gt; &lt;p&gt;I see below Audio model formats on HuggingFace. Now I have confusion over model formats. &lt;/p&gt; &lt;ul&gt; &lt;li&gt;safetensors / bin (PyTorch)&lt;/li&gt; &lt;li&gt;GGUF&lt;/li&gt; &lt;li&gt;ONNX&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I don't see GGUF quants for some Audio models. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;1]&lt;/strong&gt; What &lt;strong&gt;model format&lt;/strong&gt; are you using? &lt;/p&gt; &lt;p&gt;&lt;strong&gt;2]&lt;/strong&gt; Which &lt;strong&gt;tools/utilities&lt;/strong&gt; are you using for Text-to-Speech process? Because not all chat assistants have TTS &amp;amp; other options. Hopefully there are &lt;strong&gt;tools to run all type of audio model formats&lt;/strong&gt;(since no GGUF for some models). I have windows 11.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3]&lt;/strong&gt; What &lt;strong&gt;Audio models&lt;/strong&gt; are you using? &lt;/p&gt; &lt;p&gt;I see lot of Audio models like below:&lt;/p&gt; &lt;p&gt;Kokoro, coqui-XTTS, Chatterbox, Dia, VibeVoice, Kyutai-TTS, Orpheus, Zonos, Fishaudio-Openaudio, bark, sesame-csm, kani-tts, VoxCPM, SoulX-Podcast, Marvis-tts, Whisper, parakeet, canary-qwen, granite-speech&lt;/p&gt; &lt;p&gt;&lt;strong&gt;4]&lt;/strong&gt; &lt;strong&gt;What quants&lt;/strong&gt; are you using &amp;amp; recommended? Since I have only 8GB VRAM &amp;amp; 32GB RAM. &lt;/p&gt; &lt;p&gt;I usually do tradeoff between speed and quality for few Text models which are big for my VRAM+RAM. &lt;strong&gt;But Audio-wise I want best quality so I'll pick higher quants which fits my VRAM&lt;/strong&gt;. &lt;/p&gt; &lt;p&gt;Never used any quants greater than Q8, but I'm fine going with BF16/F16/F32 as long the it fits my 8GB VRAM. Here I'm talking about GGUF formats. For example, Dia-1.6-F32 is just 6GB. VibeVoice-1.5B-BF16 is 5GB, SoulX-Podcast-1.7B.F16 is 4GB. Hope these fit my VRAM with context &amp;amp; etc.,&lt;/p&gt; &lt;p&gt;Fortunately half of the Audio models(1-3B mostly) size are small comparing to Text models. I don't know how much the context will take additional VRAM, since haven't tried any Audio models before.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;5]&lt;/strong&gt; Please share any resources related to this(Ex: Any github repo has huge list?).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My requirements&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Make 5-10 mins audio in mp3 format for given text.&lt;/li&gt; &lt;li&gt;Voice cloning. For CBT type presentations, I don't want to talk every time. I just want to create my voice as template first. Then I want use my Voice template with given text, to make decent audio in my voice. That's it.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmttyji"&gt; /u/pmttyji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opxb1r/texttospeech_tts_models_tools_for_8gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opxb1r/texttospeech_tts_models_tools_for_8gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1opxb1r/texttospeech_tts_models_tools_for_8gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T12:14:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1opyi9q</id>
    <title>Can we expect Gemma 4 to generate/edit images?</title>
    <updated>2025-11-06T13:10:44+00:00</updated>
    <author>
      <name>/u/Brave-Hold-9389</name>
      <uri>https://old.reddit.com/user/Brave-Hold-9389</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Gemma 3 was based on gemini 2.0 architecture. Then gemini 2.5 was launched. But we didn't get gemma 4 or 3.5. Then when they released nanobanana and merged it with gemini 2.5 flash.&lt;/p&gt; &lt;p&gt;Then I had a thought. What if google releases gemini 3.0 with native image generation? If that becomes reality then we might get gemma 4 with image generation. And guess what, Rumours are that gemini 3.0 pro will have native image generation, or like some people say, it will have nano banana 2.&lt;/p&gt; &lt;p&gt;That's it!!!!!. My thoughts came true.&lt;/p&gt; &lt;p&gt;Now im not sure if gemini 3.0 flash and flash lite will have image generation but if they do, then gemma models will definitely get image generation too. Something like EMU 3.5 but in different sizes.&lt;/p&gt; &lt;p&gt;What do you guys think?&lt;/p&gt; &lt;p&gt;(Some people even say they aint gonna release gemma 4 and im here speculating its featuresüò≠üò≠üò≠)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Brave-Hold-9389"&gt; /u/Brave-Hold-9389 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opyi9q/can_we_expect_gemma_4_to_generateedit_images/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opyi9q/can_we_expect_gemma_4_to_generateedit_images/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1opyi9q/can_we_expect_gemma_4_to_generateedit_images/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T13:10:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1opopxh</id>
    <title>Maya1 : 1st AI TTS model with Voice Design Feature on the fly</title>
    <updated>2025-11-06T03:50:50+00:00</updated>
    <author>
      <name>/u/Technical-Love-8479</name>
      <uri>https://old.reddit.com/user/Technical-Love-8479</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So Maya-research has released Maya1, a low latency TTS model where you can design the voice also given the description (like female, mid 30s, author, a little aggressive). The model uses Llama backbone and has 3B params.&lt;/p&gt; &lt;p&gt;Hugging Face : &lt;a href="https://huggingface.co/maya-research/maya1"&gt;https://huggingface.co/maya-research/maya1&lt;/a&gt; Demo : &lt;a href="https://youtu.be/69voVwdcVYg?si=wx1zM0CXU-DWbKwb"&gt;https://youtu.be/69voVwdcVYg?si=wx1zM0CXU-DWbKwb&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Technical-Love-8479"&gt; /u/Technical-Love-8479 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opopxh/maya1_1st_ai_tts_model_with_voice_design_feature/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opopxh/maya1_1st_ai_tts_model_with_voice_design_feature/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1opopxh/maya1_1st_ai_tts_model_with_voice_design_feature/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T03:50:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1opx9k2</id>
    <title>Vascura BAT - configuration Tool for Llama.Cpp Server via simple BAT files.</title>
    <updated>2025-11-06T12:12:15+00:00</updated>
    <author>
      <name>/u/-Ellary-</name>
      <uri>https://old.reddit.com/user/-Ellary-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opx9k2/vascura_bat_configuration_tool_for_llamacpp/"&gt; &lt;img alt="Vascura BAT - configuration Tool for Llama.Cpp Server via simple BAT files." src="https://external-preview.redd.it/ODE5d3VyOTFvbXpmMdNIzzUckZQtCYN2T67NLARkGu_0Cp0d-HZIxSMAHrnP.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=33101c6ed9256d47e2ebf6f3ad5e3a1ea3e330d0" title="Vascura BAT - configuration Tool for Llama.Cpp Server via simple BAT files." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-Ellary-"&gt; /u/-Ellary- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/h3q6is91omzf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opx9k2/vascura_bat_configuration_tool_for_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1opx9k2/vascura_bat_configuration_tool_for_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T12:12:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1opsqjh</id>
    <title>Free credits will continue until retention improves.</title>
    <updated>2025-11-06T07:36:36+00:00</updated>
    <author>
      <name>/u/phoneixAdi</name>
      <uri>https://old.reddit.com/user/phoneixAdi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opsqjh/free_credits_will_continue_until_retention/"&gt; &lt;img alt="Free credits will continue until retention improves." src="https://b.thumbs.redditmedia.com/9qqmM1NFRFmb8CXIeFnM2xP6S6_OEySYeaMQMNKBcPc.jpg" title="Free credits will continue until retention improves." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/phoneixAdi"&gt; /u/phoneixAdi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1opsqjh"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opsqjh/free_credits_will_continue_until_retention/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1opsqjh/free_credits_will_continue_until_retention/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T07:36:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1opeu1w</id>
    <title>Visualizing Quantization Types</title>
    <updated>2025-11-05T20:52:02+00:00</updated>
    <author>
      <name>/u/VoidAlchemy</name>
      <uri>https://old.reddit.com/user/VoidAlchemy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opeu1w/visualizing_quantization_types/"&gt; &lt;img alt="Visualizing Quantization Types" src="https://preview.redd.it/brkkf7fs2izf1.gif?width=640&amp;amp;crop=smart&amp;amp;s=69bbd6b8af4c7420aed83b9b70eddb5a51a78d26" title="Visualizing Quantization Types" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've seen some releases of MXFP4 quantized models recently and don't understand why given mxfp4 is kind of like a slightly smaller lower quality q4_0.&lt;/p&gt; &lt;p&gt;So unless the original model was post-trained specifically for MXFP4 like gpt-oss-120b or you yourself did some kind of QAT (quantization aware fine-tuning) targeting specifically mxfp4, then personally I'd go with good old q4_0 or ik's newer iq4_kss.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;mxfp4 4.25bpw&lt;/li&gt; &lt;li&gt;q4_0 4.5bpw&lt;/li&gt; &lt;li&gt;iq4_kss 4.0bpw&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I used the llama.cpp gguf python package to read a uint8 .bmp image, convert it to float16 numpy 2d array, and save that as a .gguf. Then I quantized the gguf to various types using ik_llama.cpp, and then finally re-quantize that back to f16 and save the resulting uint8 .bmp image.&lt;/p&gt; &lt;p&gt;Its kinda neat to visualize the effects of block sizes looking at image data. To me the mxfp4 looks &amp;quot;worse&amp;quot; than the q4_0 and the iq4_kss.&lt;/p&gt; &lt;p&gt;I haven't done perplexity/KLD measurements to directly compare mxfp4, but iq4_kss tends to be one of the best available in that size range in my previous quant release testing.&lt;/p&gt; &lt;p&gt;Finally, it is confusing to me, but nvfp4 is yet &lt;em&gt;a different&lt;/em&gt; quantization type with specific blackwell hardware support which I haven't tried yet myself.&lt;/p&gt; &lt;p&gt;Anyway, in my opinion mxfp4 isn't particularly special or better despite being somewhat newer. What do y'all think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VoidAlchemy"&gt; /u/VoidAlchemy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/brkkf7fs2izf1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opeu1w/visualizing_quantization_types/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1opeu1w/visualizing_quantization_types/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-05T20:52:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1opyvjt</id>
    <title>Continuous Autoregressive Language Models : Alternate for traditional LLMs, paper by Tencent</title>
    <updated>2025-11-06T13:26:55+00:00</updated>
    <author>
      <name>/u/Technical-Love-8479</name>
      <uri>https://old.reddit.com/user/Technical-Love-8479</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;WeChat AI just dropped a paper called Continuous Autoregressive Language Models (CALM),it basically rethinks how LLMs generate text. Instead of predicting one token at a time from a discrete vocabulary (the slow, softmax-heavy way every GPT-style model works), CALM predicts continuous vectors that each represent multiple tokens. &lt;/p&gt; &lt;p&gt;These vectors are learned through a high-fidelity autoencoder that can compress, say, 4 tokens into one latent vector and reconstruct them with over 99.9% accuracy. So the model generates ‚Äúsemantic chunks‚Äù instead of words, cutting generation steps by 4√ó while keeping meaning intact.&lt;/p&gt; &lt;p&gt;Because the model operates in continuous space, there‚Äôs no softmax, no cross-entropy, and no perplexity.&lt;/p&gt; &lt;p&gt;Training uses an energy-based objective that compares predicted vs. real vectors, and evaluation uses a new metric called BrierLM, a likelihood-free stand-in for perplexity. In benchmarks on The Pile and WikiText-103, CALM matched or beat standard Transformers with ~40% less compute. It‚Äôs not just a speed trick, it‚Äôs a new scaling direction: instead of making models bigger, make each generative step carry more meaning.&lt;/p&gt; &lt;p&gt;Paper : &lt;a href="https://arxiv.org/abs/2510.27688"&gt;https://arxiv.org/abs/2510.27688&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Explanation : &lt;a href="https://youtu.be/tLWBzya9dwA?si=k-9ozLk_PvU-V6au"&gt;https://youtu.be/tLWBzya9dwA?si=k-9ozLk_PvU-V6au&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Technical-Love-8479"&gt; /u/Technical-Love-8479 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opyvjt/continuous_autoregressive_language_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opyvjt/continuous_autoregressive_language_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1opyvjt/continuous_autoregressive_language_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T13:26:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1opx6p1</id>
    <title>We just Fine-Tuned a Japanese Manga OCR Model with PaddleOCR-VL!</title>
    <updated>2025-11-06T12:08:18+00:00</updated>
    <author>
      <name>/u/erinr1122</name>
      <uri>https://old.reddit.com/user/erinr1122</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opx6p1/we_just_finetuned_a_japanese_manga_ocr_model_with/"&gt; &lt;img alt="We just Fine-Tuned a Japanese Manga OCR Model with PaddleOCR-VL!" src="https://b.thumbs.redditmedia.com/yFGQjvFykSgFy7I5SA6oM7N5D5pziNhR7HfUDN7uUKo.jpg" title="We just Fine-Tuned a Japanese Manga OCR Model with PaddleOCR-VL!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Hi all! üëã&lt;/strong&gt;&lt;br /&gt; Hope you don‚Äôt mind a little self-promo, but we just finished fine-tuning &lt;strong&gt;PaddleOCR-VL&lt;/strong&gt; to build a model specialized in &lt;strong&gt;Japanese manga text recognition&lt;/strong&gt; ‚Äî and it works surprisingly well! üéâ&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Model:&lt;/strong&gt; &lt;a href="https://huggingface.co/jzhang533/PaddleOCR-VL-For-Manga"&gt;PaddleOCR-VL-For-Manga&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Dataset:&lt;/strong&gt; Manga109-s + 1.5 million synthetic samples&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Accuracy:&lt;/strong&gt; 70% full-sentence accuracy (vs. 27% from the original model)&lt;/p&gt; &lt;p&gt;It handles manga speech bubbles and stylized fonts really nicely. There are still challenges with full-width vs. half-width characters, but overall it‚Äôs a big step forward for domain-specific OCR.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How to use&lt;/strong&gt;&lt;br /&gt; You can use this model with &lt;strong&gt;Transformers&lt;/strong&gt;, &lt;strong&gt;PaddleOCR&lt;/strong&gt;, or any library that supports PaddleOCR-VL to recognize manga text.&lt;br /&gt; For structured documents, try pairing it with &lt;strong&gt;PP-DocLayoutV2&lt;/strong&gt; for layout analysis ‚Äî though manga layouts are a bit different.&lt;/p&gt; &lt;p&gt;We‚Äôd love to hear your thoughts or see your own fine-tuned versions!&lt;br /&gt; Really excited to see how we can push OCR models even further. üöÄ &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ampi1bppmmzf1.png?width=1196&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=43153284175cefb14a0f7cd8f415d127a33e26b4"&gt;https://preview.redd.it/ampi1bppmmzf1.png?width=1196&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=43153284175cefb14a0f7cd8f415d127a33e26b4&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/erinr1122"&gt; /u/erinr1122 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opx6p1/we_just_finetuned_a_japanese_manga_ocr_model_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opx6p1/we_just_finetuned_a_japanese_manga_ocr_model_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1opx6p1/we_just_finetuned_a_japanese_manga_ocr_model_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T12:08:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1oph7jd</id>
    <title>Unified memory is the future, not GPU for local A.I.</title>
    <updated>2025-11-05T22:20:52+00:00</updated>
    <author>
      <name>/u/Terminator857</name>
      <uri>https://old.reddit.com/user/Terminator857</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As model sizes are trending bigger, even the best open weight models hover around half a terabyte, we are not going to be able to run those on GPU, yes on unified memory. Gemini-3 is rumored to be 1.2 trillion parameters:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reuters.com/business/apple-use-googles-ai-model-run-new-siri-bloomberg-news-reports-2025-11-05/"&gt;https://www.reuters.com/business/apple-use-googles-ai-model-run-new-siri-bloomberg-news-reports-2025-11-05/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So Apple and Strix Halo are on the right track. Intel where art thou? Any one else we can count on to eventually catch the trend? Medusa halo is going to be awesome:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;a href="https://www.youtube.com/shorts/yAcONx3Jxf8"&gt;https://www.youtube.com/shorts/yAcONx3Jxf8&lt;/a&gt; . Quote: Medusa Halo is going to destroy strix halo.&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.techpowerup.com/340216/amd-medusa-halo-apu-leak-reveals-up-to-24-cores-and-48-rdna-5-cus#g340216-3"&gt;https://www.techpowerup.com/340216/amd-medusa-halo-apu-leak-reveals-up-to-24-cores-and-48-rdna-5-cus#g340216-3&lt;/a&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Even longer term 5 years, I'm thinking in memory compute will take over versus current standard of von neumann architecture. Once we crack in memory compute nut then things will get very interesting. Will allow a greater level of parallelization. Every neuron can fire simultaneously like our human brain. In memory compute will dominate for future architectures in 10 years versus von neumann.&lt;/p&gt; &lt;p&gt;What do you think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Terminator857"&gt; /u/Terminator857 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oph7jd/unified_memory_is_the_future_not_gpu_for_local_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oph7jd/unified_memory_is_the_future_not_gpu_for_local_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oph7jd/unified_memory_is_the_future_not_gpu_for_local_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-05T22:20:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1opu1wi</id>
    <title>Kimi-K2 Thinking (not yet released)</title>
    <updated>2025-11-06T09:02:47+00:00</updated>
    <author>
      <name>/u/TheRealMasonMac</name>
      <uri>https://old.reddit.com/user/TheRealMasonMac</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opu1wi/kimik2_thinking_not_yet_released/"&gt; &lt;img alt="Kimi-K2 Thinking (not yet released)" src="https://b.thumbs.redditmedia.com/bExVpI9LsAAHMbYXusrGRMBojpqecQ4Ff8dkoHONjdw.jpg" title="Kimi-K2 Thinking (not yet released)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/djyebzcoqlzf1.png?width=1712&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7cb784d84b524d85b4ef17938df6f71a6fe6e90b"&gt;https://preview.redd.it/djyebzcoqlzf1.png?width=1712&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7cb784d84b524d85b4ef17938df6f71a6fe6e90b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://platform.moonshot.ai/docs/pricing/chat#generation-model-kimi-k2"&gt;https://platform.moonshot.ai/docs/pricing/chat#generation-model-kimi-k2&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheRealMasonMac"&gt; /u/TheRealMasonMac &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opu1wi/kimik2_thinking_not_yet_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opu1wi/kimik2_thinking_not_yet_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1opu1wi/kimik2_thinking_not_yet_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T09:02:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1opa6os</id>
    <title>Local Setup</title>
    <updated>2025-11-05T18:03:19+00:00</updated>
    <author>
      <name>/u/mattate</name>
      <uri>https://old.reddit.com/user/mattate</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opa6os/local_setup/"&gt; &lt;img alt="Local Setup" src="https://preview.redd.it/8imhi4icahzf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=eabf7d36f6208d91a8e908e97d3d1a1b1ee6998f" title="Local Setup" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey just figured I would share our local setup. I started building these machines as an experiment to see if I could drop our cost, and so far it has worked out pretty good. The first one was over a year ago, lots of lessons learned getting them up and stable. &lt;/p&gt; &lt;p&gt;The cost of AI APIs has come down drastically, when we started with these machines there was absolutely no competition. It's still cheaper to run your own hardware, but it's much much closer now. This community really I think is providing crazy value allowing company's like mine to experiment and roll things into production without having to drop hundreds of thousands of dollars literally on propritary AI API usage.&lt;/p&gt; &lt;p&gt;Running a mix of used 3090s, new 4090s, 5090s, and RTX 6000 pro's. The 3090 is certainly the king off cost per token without a doubt, but the problems with buying used gpus is not really worth the hassle of you're relying on these machines to get work done. &lt;/p&gt; &lt;p&gt;We process anywhere between 70m and 120m tokens per day, we could probably do more. &lt;/p&gt; &lt;p&gt;Some notes:&lt;/p&gt; &lt;p&gt;ASUS motherboards work well and are pretty stable, running ASUS Pro WS WRX80E-SAGE SE with threadripper gets up to 7 gpus, but usually pair gpus so 6 is the useful max. Will upgrade to the 90 in future machines. &lt;/p&gt; &lt;p&gt;240v power works much better then 120v, this is more about effciency of the power supplies. &lt;/p&gt; &lt;p&gt;Cooling is a huge problem, any more machines them I have now and cooling will become a very significant issue.&lt;/p&gt; &lt;p&gt;We run predominantly vllm these days, mixture of different models as new ones get released. &lt;/p&gt; &lt;p&gt;Happy to answer any other questions. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mattate"&gt; /u/mattate &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8imhi4icahzf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opa6os/local_setup/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1opa6os/local_setup/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-05T18:03:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1oprsln</id>
    <title>What is your take on this?</title>
    <updated>2025-11-06T06:38:31+00:00</updated>
    <author>
      <name>/u/ya_Priya</name>
      <uri>https://old.reddit.com/user/ya_Priya</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oprsln/what_is_your_take_on_this/"&gt; &lt;img alt="What is your take on this?" src="https://external-preview.redd.it/enAybXNsNngwbHpmMSG2HwlQpQ6Hj-82EDUoyhNg7YK-n8qL0itnzTKon9hZ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b9dc87934b9d0d8de087571ccc6f9351740b8dac" title="What is your take on this?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Source: Mobile Hacker on twitter&lt;/p&gt; &lt;p&gt;Some of you were trying to find it. &lt;/p&gt; &lt;p&gt;Hey guys, this is their website - &lt;a href="https://droidrun.ai/"&gt;https://droidrun.ai/&lt;/a&gt;&lt;br /&gt; and the github - &lt;a href="https://github.com/droidrun/droidrun"&gt;https://github.com/droidrun/droidrun&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The guy who posted on X - &lt;a href="https://x.com/androidmalware2/status/1981732061267235050"&gt;https://x.com/androidmalware2/status/1981732061267235050&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Can't add so many links, but they have detailed docs on their website.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ya_Priya"&gt; /u/ya_Priya &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/zp20kj6x0lzf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oprsln/what_is_your_take_on_this/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oprsln/what_is_your_take_on_this/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T06:38:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1olq14f</id>
    <title>[MEGATHREAD] Local AI Hardware - November 2025</title>
    <updated>2025-11-01T15:02:17+00:00</updated>
    <author>
      <name>/u/eck72</name>
      <uri>https://old.reddit.com/user/eck72</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is the monthly thread for sharing your local AI setups and the models you're running.&lt;/p&gt; &lt;p&gt;Whether you're using a single CPU, a gaming GPU, or a full rack, post what you're running and how it performs.&lt;/p&gt; &lt;p&gt;Post in any format you like. The list below is just a guide:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Hardware: CPU, GPU(s), RAM, storage, OS&lt;/li&gt; &lt;li&gt;Model(s): name + size/quant&lt;/li&gt; &lt;li&gt;Stack: (e.g. llama.cpp + custom UI)&lt;/li&gt; &lt;li&gt;Performance: t/s, latency, context, batch etc.&lt;/li&gt; &lt;li&gt;Power consumption&lt;/li&gt; &lt;li&gt;Notes: purpose, quirks, comments&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please share setup pics for eye candy!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Quick reminder&lt;/strong&gt;: You can share hardware purely to ask questions or get feedback. All experience levels welcome.&lt;/p&gt; &lt;p&gt;House rules: no buying/selling/promo.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eck72"&gt; /u/eck72 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T15:02:17+00:00</published>
  </entry>
</feed>
