<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-29T10:38:42+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ntc6ix</id>
    <title>Has anyone used GDB-MCP</title>
    <updated>2025-09-29T07:59:27+00:00</updated>
    <author>
      <name>/u/Comfortable-Soft336</name>
      <uri>https://old.reddit.com/user/Comfortable-Soft336</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/Chedrian07/gdb-mcp"&gt;https://github.com/Chedrian07/gdb-mcp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Just as the title says. I came across an interesting repository&lt;br /&gt; has anyone tried it? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Comfortable-Soft336"&gt; /u/Comfortable-Soft336 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntc6ix/has_anyone_used_gdbmcp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntc6ix/has_anyone_used_gdbmcp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ntc6ix/has_anyone_used_gdbmcp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-29T07:59:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1nt7z3f</id>
    <title>What are your go to VL models?</title>
    <updated>2025-09-29T03:40:17+00:00</updated>
    <author>
      <name>/u/segmond</name>
      <uri>https://old.reddit.com/user/segmond</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen2.5-VL seems to be the best so far for me.&lt;/p&gt; &lt;p&gt;Gemma3-27B and MistralSmall24B have also been solid.&lt;/p&gt; &lt;p&gt;I keep giving InternVL a try, but it's not living up. I downloaded InternVL3.5-38B Q8 this weekend and it was garbage with so much hallucination. &lt;/p&gt; &lt;p&gt;Currently downloading KimiVL and moondream3. If you have a favorite please do share, Qwen3-235B-VL looks like it would be the real deal, but I broke down most of my rigs, and might be able to give it a go at Q4. I hate running VL models on anything besides Q8. If anyone has given it a go, please share if it's really the SOTA it seems to be.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/segmond"&gt; /u/segmond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nt7z3f/what_are_your_go_to_vl_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nt7z3f/what_are_your_go_to_vl_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nt7z3f/what_are_your_go_to_vl_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-29T03:40:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1nsszob</id>
    <title>The MoE tradeoff seems bad for local hosting</title>
    <updated>2025-09-28T16:40:58+00:00</updated>
    <author>
      <name>/u/upside-down-number</name>
      <uri>https://old.reddit.com/user/upside-down-number</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I think I understand this right, but somebody tell me where I'm wrong here.&lt;/p&gt; &lt;p&gt;Overly simplified explanation of how an LLM works: for a dense model, you take the context, stuff it through the whole neural network, sample a token, add it to the context, and do it again. The way an MoE model works, instead of the context getting processed by the entire model, there's a router network and then the model is split into a set of &amp;quot;experts&amp;quot;, and only some subset of those get used to compute the next output token. But you need more total parameters in the model for this, there's a rough rule of thumb that an MoE model is equivalent to a dense model of size sqrt(total_params × active_params), all else equal. (and all else usually isn't equal, we've all seen wildly different performance from models of the same size, but never mind that).&lt;/p&gt; &lt;p&gt;So the tradeoff is, the MoE model uses more VRAM, uses less compute, and is probably more efficient at batch processing because when it's processing contexts from multiple users those are (hopefully) going to activate different experts in the model. This all works out very well if VRAM is abundant, compute (and electricity) is the big bottleneck, and you're trying to maximize throughput to a large number of users; i.e. the use case for a major AI company.&lt;/p&gt; &lt;p&gt;Now, consider the typical local LLM use case. Probably most local LLM users are in this situation: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;VRAM is not abundant, because you're using consumer grade GPUs where VRAM is kept low for market segmentation reasons&lt;/li&gt; &lt;li&gt;Compute is &lt;em&gt;relatively&lt;/em&gt; more abundant than VRAM, consider that the compute in an RTX 4090 isn't that far off from what you get from an H100; the H100's advantanges are that it has more VRAM and better memory bandwidth and so on&lt;/li&gt; &lt;li&gt;You are serving one user at a time at home, or a small number for some weird small business case&lt;/li&gt; &lt;li&gt;The incremental benefit of higher token throughput above some usability threshold of 20-30 tok/sec is not very high&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Given all that, it seems like for our use case you're going to want the best dense model you can fit in consumer-grade hardware (one or two consumer GPUs in the neighborhood of 24GB size), right? Unfortunately the major labs are going to be optimizing mostly for the largest MoE model they can fit in a 8xH100 server or similar because that's increasingly important for their own use case. Am I missing anything here? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/upside-down-number"&gt; /u/upside-down-number &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsszob/the_moe_tradeoff_seems_bad_for_local_hosting/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsszob/the_moe_tradeoff_seems_bad_for_local_hosting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nsszob/the_moe_tradeoff_seems_bad_for_local_hosting/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-28T16:40:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1nsz6ss</id>
    <title>Local multimodal RAG: search &amp; summarize screenshots/photos fully offline</title>
    <updated>2025-09-28T20:46:24+00:00</updated>
    <author>
      <name>/u/AlanzhuLy</name>
      <uri>https://old.reddit.com/user/AlanzhuLy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsz6ss/local_multimodal_rag_search_summarize/"&gt; &lt;img alt="Local multimodal RAG: search &amp;amp; summarize screenshots/photos fully offline" src="https://external-preview.redd.it/OXk5MDdtY2h3eXJmMbRJ2b6sMPmHJiOrbj5FYV3hs8t-hezd4gT3rSFztsqf.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=766b7b8d1974e0c636d5623a7a7843becb301d95" title="Local multimodal RAG: search &amp;amp; summarize screenshots/photos fully offline" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;One of the strongest use cases I’ve found for local LLMs + vision is turning my messy screenshot/photo library into something queryable.&lt;/p&gt; &lt;p&gt;Half my “notes” are just images — slides from talks, whiteboards, book pages, receipts, chat snippets. Normally they rot in a folder. Now I can:&lt;br /&gt; – Point a local multimodal agent (&lt;a href="https://hyperlink.nexa.ai/"&gt;Hyperlink&lt;/a&gt;) at my screenshots folder&lt;br /&gt; – Ask in plain English → &lt;em&gt;“Summarize what I saved about the future of AI”&lt;/em&gt;&lt;br /&gt; – It runs OCR + embeddings locally, pulls the right images, and gives a short summary with the source image linked&lt;/p&gt; &lt;p&gt;No cloud, no quotas. 100% on-device. My own storage is the only limit.&lt;/p&gt; &lt;p&gt;Feels like the natural extension of RAG: not just text docs, but &lt;em&gt;vision + text together&lt;/em&gt;.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Imagine querying screenshots, PDFs, and notes in one pass&lt;/li&gt; &lt;li&gt;Summaries grounded in the actual images&lt;/li&gt; &lt;li&gt;Completely private, runs on consumer hardware&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I’m using Hyperlink to prototype this flow. Curious if anyone else here is building multimodal local RAG — what have you managed to get working, and what’s been most useful?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlanzhuLy"&gt; /u/AlanzhuLy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/lnkwumchwyrf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsz6ss/local_multimodal_rag_search_summarize/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nsz6ss/local_multimodal_rag_search_summarize/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-28T20:46:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1ntcr1k</id>
    <title>For local models, has anyone benchmarked tool calling protocols performance?</title>
    <updated>2025-09-29T08:38:25+00:00</updated>
    <author>
      <name>/u/NoSound1395</name>
      <uri>https://old.reddit.com/user/NoSound1395</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been researching tool-calling protocols and came across comparisons claiming UTCP is 30–40% faster than MCP. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Quick overview:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;UTCP: Direct tool calls; native support for WebSocket, gRPC, CLI&lt;/li&gt; &lt;li&gt;MCP: All calls go through a JSON-RPC server (extra overhead, but adds control)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I’m planning to process a large volume of documents locally with llama.cpp, so I’m curious:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Anyone tested UTCP or MCP with llama.cpp’s tool-calling features?&lt;/li&gt; &lt;li&gt;Has anyone run these protocols against Qwen or Llama locally? What performance differences did you see?&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NoSound1395"&gt; /u/NoSound1395 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntcr1k/for_local_models_has_anyone_benchmarked_tool/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntcr1k/for_local_models_has_anyone_benchmarked_tool/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ntcr1k/for_local_models_has_anyone_benchmarked_tool/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-29T08:38:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1nsyu7b</id>
    <title>Do you think that &lt;4B models has caught up with good old GPT3?</title>
    <updated>2025-09-28T20:32:15+00:00</updated>
    <author>
      <name>/u/Ok-Internal9317</name>
      <uri>https://old.reddit.com/user/Ok-Internal9317</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsyu7b/do_you_think_that_4b_models_has_caught_up_with/"&gt; &lt;img alt="Do you think that &amp;lt;4B models has caught up with good old GPT3?" src="https://b.thumbs.redditmedia.com/dqXrXaddxjMsBxXL3FLNfmNHICqqjQ2Qhphy8PaccHw.jpg" title="Do you think that &amp;lt;4B models has caught up with good old GPT3?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/a76qyhd1uyrf1.png?width=807&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=35fbb5e302f260d4c57ab6ad41ce0d4d770906fc"&gt;https://preview.redd.it/a76qyhd1uyrf1.png?width=807&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=35fbb5e302f260d4c57ab6ad41ce0d4d770906fc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I think it was up to 3.5 that it stopped hallusinating like hell, so what do you think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Internal9317"&gt; /u/Ok-Internal9317 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsyu7b/do_you_think_that_4b_models_has_caught_up_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsyu7b/do_you_think_that_4b_models_has_caught_up_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nsyu7b/do_you_think_that_4b_models_has_caught_up_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-28T20:32:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1nta39d</id>
    <title>torn between GPU, Mini PC for local LLM</title>
    <updated>2025-09-29T05:42:56+00:00</updated>
    <author>
      <name>/u/jussey-x-poosi</name>
      <uri>https://old.reddit.com/user/jussey-x-poosi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm contemplating on buying a Mac Mini M4 Pro 128gb or Beelink GTR9 128gb (ryzen AI Max 395) vs a dedicated GPU (atleast 2x 3090).&lt;/p&gt; &lt;p&gt;I know that running a dedicated GPU requires more power, but I want to understand what's the advantage i'll have for dedicated GPU if I only do Inference and rag. I plan to host my own IT Service enabled by AI at the back, so I'll prolly need a machine to do a lot of processing.&lt;/p&gt; &lt;p&gt;some of you might wonder why macmini, I think the edge for me is the warranty and support in my country. Beelink or any china made MiniPC doesn't have a warranty here, and RTX 3090 as well since i'll be sourcing it in secondary market.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jussey-x-poosi"&gt; /u/jussey-x-poosi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nta39d/torn_between_gpu_mini_pc_for_local_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nta39d/torn_between_gpu_mini_pc_for_local_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nta39d/torn_between_gpu_mini_pc_for_local_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-29T05:42:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1nt3aqc</id>
    <title>Update got dual b580 working in LM studio</title>
    <updated>2025-09-28T23:45:52+00:00</updated>
    <author>
      <name>/u/hasanismail_</name>
      <uri>https://old.reddit.com/user/hasanismail_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nt3aqc/update_got_dual_b580_working_in_lm_studio/"&gt; &lt;img alt="Update got dual b580 working in LM studio" src="https://b.thumbs.redditmedia.com/MowvbWoRvfurgrczxvwqAazNnm6d58cEc3FQ4dkn90k.jpg" title="Update got dual b580 working in LM studio" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have 4 Intel b580 GPUs I wanted to test 2 of them in this system dual Xeon v3 32gb ram and dual b580 GPUs first I tried Ubuntu that didn't work out them I tried fedora that also didn't work out them I tried win10 with LM studio and finally I got it working its doing 40b parameter models at around 37 tokens per second is there anything else I can do ti enhance this setup before I install 2 more Intel arc b580 GPUs ( I'm gonna use a different motherboard for all 4 GPUs)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hasanismail_"&gt; /u/hasanismail_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nt3aqc"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nt3aqc/update_got_dual_b580_working_in_lm_studio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nt3aqc/update_got_dual_b580_working_in_lm_studio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-28T23:45:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1ntdy1r</id>
    <title>What are your thoughts about Cerebras?</title>
    <updated>2025-09-29T09:58:57+00:00</updated>
    <author>
      <name>/u/robertpiosik</name>
      <uri>https://old.reddit.com/user/robertpiosik</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What's the deal with them? If they're so efficient why big labs are not using/buying them? Is China trying to replicate their tech? &lt;/p&gt; &lt;p&gt;They claim to be 3x more energy efficient than GPUs and just imagine they offering Wafer Scale Engine Mini for blazing fast inference at home...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/robertpiosik"&gt; /u/robertpiosik &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntdy1r/what_are_your_thoughts_about_cerebras/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntdy1r/what_are_your_thoughts_about_cerebras/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ntdy1r/what_are_your_thoughts_about_cerebras/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-29T09:58:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1ntcnqi</id>
    <title>Which samplers at this point are outdated</title>
    <updated>2025-09-29T08:32:06+00:00</updated>
    <author>
      <name>/u/Long_comment_san</name>
      <uri>https://old.reddit.com/user/Long_comment_san</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Which samplers would you say at this moment are superceded by other samplers/combos and why? IMHO: temperature has not been replaced as a baseline sampler. Min p seems like a common pick from what I can see on the sub. So what about: typical p, top a, top K, smooth sampling, XTC, mirostat (1,2), dynamic temperature. Would you say some are outright better pick over the others? Personally I feel &amp;quot;dynamic samplers&amp;quot; are a more interesting alternative but have some weird tendencies to overshoot, but feel a lot less &amp;quot;robotic&amp;quot; over min p + top k.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Long_comment_san"&gt; /u/Long_comment_san &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntcnqi/which_samplers_at_this_point_are_outdated/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntcnqi/which_samplers_at_this_point_are_outdated/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ntcnqi/which_samplers_at_this_point_are_outdated/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-29T08:32:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1nsu3og</id>
    <title>Drummer's Cydonia R1 24B v4.1 · A less positive, less censored, better roleplay, creative finetune with reasoning!</title>
    <updated>2025-09-28T17:25:35+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsu3og/drummers_cydonia_r1_24b_v41_a_less_positive_less/"&gt; &lt;img alt="Drummer's Cydonia R1 24B v4.1 · A less positive, less censored, better roleplay, creative finetune with reasoning!" src="https://external-preview.redd.it/pROfhPfXKnMC7ws-LUjL0JI_7ZtgvBAu9hx7L06rI9c.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fc893240ec75e9bd482665fa32ded5d1badc97c0" title="Drummer's Cydonia R1 24B v4.1 · A less positive, less censored, better roleplay, creative finetune with reasoning!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Backlog:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Cydonia v4.2.0,&lt;/li&gt; &lt;li&gt;Snowpiercer 15B v3,&lt;/li&gt; &lt;li&gt;Anubis Mini 8B v1&lt;/li&gt; &lt;li&gt;Behemoth ReduX 123B v1.1 (v4.2.0 treatment)&lt;/li&gt; &lt;li&gt;RimTalk Mini (showcase)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I can't wait to release v4.2.0. I think it's proof that I still have room to grow. You can test it out here: &lt;a href="https://huggingface.co/BeaverAI/Cydonia-24B-v4o-GGUF"&gt;https://huggingface.co/BeaverAI/Cydonia-24B-v4o-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;and I went ahead and gave Largestral 2407 the same treatment here: &lt;a href="https://huggingface.co/BeaverAI/Behemoth-ReduX-123B-v1b-GGUF"&gt;https://huggingface.co/BeaverAI/Behemoth-ReduX-123B-v1b-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Cydonia-R1-24B-v4.1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsu3og/drummers_cydonia_r1_24b_v41_a_less_positive_less/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nsu3og/drummers_cydonia_r1_24b_v41_a_less_positive_less/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-28T17:25:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1nt2c38</id>
    <title>Llama.cpp MoE models find best --n-cpu-moe value</title>
    <updated>2025-09-28T23:00:55+00:00</updated>
    <author>
      <name>/u/tabletuser_blogspot</name>
      <uri>https://old.reddit.com/user/tabletuser_blogspot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Being able to run larger LLM on consumer equipment keeps getting better. Running MoE models is a big step and now with CPU offloading it's an even bigger step.&lt;/p&gt; &lt;p&gt;Here is what is working for me on my RX 7900 GRE 16GB GPU running the Llama4 Scout 108B parameter beast. I use &lt;em&gt;--n-cpu-moe 30,40,50,60&lt;/em&gt; to find my focus range. &lt;/p&gt; &lt;p&gt;&lt;code&gt;./llama-bench -m /meta-llama_Llama-4-Scout-17B-16E-Instruct-IQ3_XXS.gguf --n-cpu-moe 30,40,50,60&lt;/code&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;n_cpu_moe&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;llama4 17Bx16E (Scout) IQ3_XXS - 3.0625 bpw&lt;/td&gt; &lt;td align="left"&gt;41.86 GiB&lt;/td&gt; &lt;td align="left"&gt;107.77 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;30&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;22.50 ± 0.10&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama4 17Bx16E (Scout) IQ3_XXS - 3.0625 bpw&lt;/td&gt; &lt;td align="left"&gt;41.86 GiB&lt;/td&gt; &lt;td align="left"&gt;107.77 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;30&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;6.58 ± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama4 17Bx16E (Scout) IQ3_XXS - 3.0625 bpw&lt;/td&gt; &lt;td align="left"&gt;41.86 GiB&lt;/td&gt; &lt;td align="left"&gt;107.77 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;40&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;150.33 ± 0.88&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama4 17Bx16E (Scout) IQ3_XXS - 3.0625 bpw&lt;/td&gt; &lt;td align="left"&gt;41.86 GiB&lt;/td&gt; &lt;td align="left"&gt;107.77 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;40&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;8.30 ± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama4 17Bx16E (Scout) IQ3_XXS - 3.0625 bpw&lt;/td&gt; &lt;td align="left"&gt;41.86 GiB&lt;/td&gt; &lt;td align="left"&gt;107.77 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;50&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;136.62 ± 0.45&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama4 17Bx16E (Scout) IQ3_XXS - 3.0625 bpw&lt;/td&gt; &lt;td align="left"&gt;41.86 GiB&lt;/td&gt; &lt;td align="left"&gt;107.77 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;50&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;7.36 ± 0.03&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama4 17Bx16E (Scout) IQ3_XXS - 3.0625 bpw&lt;/td&gt; &lt;td align="left"&gt;41.86 GiB&lt;/td&gt; &lt;td align="left"&gt;107.77 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;60&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;137.33 ± 1.10&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama4 17Bx16E (Scout) IQ3_XXS - 3.0625 bpw&lt;/td&gt; &lt;td align="left"&gt;41.86 GiB&lt;/td&gt; &lt;td align="left"&gt;107.77 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;60&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;7.33 ± 0.05&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Here we figured out where to start. 30 didn't have boost but 40 did so lets try around those values.&lt;/p&gt; &lt;p&gt;&lt;code&gt;./llama-bench -m /meta-llama_Llama-4-Scout-17B-16E-Instruct-IQ3_XXS.gguf --n-cpu-moe 31,32,33,34,35,36,37,38,39,41,42,43&lt;/code&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;n_cpu_moe&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;llama4 17Bx16E (Scout) IQ3_XXS - 3.0625 bpw&lt;/td&gt; &lt;td align="left"&gt;41.86 GiB&lt;/td&gt; &lt;td align="left"&gt;107.77 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;31&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;22.52 ± 0.15&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama4 17Bx16E (Scout) IQ3_XXS - 3.0625 bpw&lt;/td&gt; &lt;td align="left"&gt;41.86 GiB&lt;/td&gt; &lt;td align="left"&gt;107.77 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;31&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;6.82 ± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama4 17Bx16E (Scout) IQ3_XXS - 3.0625 bpw&lt;/td&gt; &lt;td align="left"&gt;41.86 GiB&lt;/td&gt; &lt;td align="left"&gt;107.77 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;32&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;22.92 ± 0.24&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama4 17Bx16E (Scout) IQ3_XXS - 3.0625 bpw&lt;/td&gt; &lt;td align="left"&gt;41.86 GiB&lt;/td&gt; &lt;td align="left"&gt;107.77 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;32&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;7.09 ± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama4 17Bx16E (Scout) IQ3_XXS - 3.0625 bpw&lt;/td&gt; &lt;td align="left"&gt;41.86 GiB&lt;/td&gt; &lt;td align="left"&gt;107.77 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;33&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;22.95 ± 0.18&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama4 17Bx16E (Scout) IQ3_XXS - 3.0625 bpw&lt;/td&gt; &lt;td align="left"&gt;41.86 GiB&lt;/td&gt; &lt;td align="left"&gt;107.77 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;33&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;7.35 ± 0.03&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama4 17Bx16E (Scout) IQ3_XXS - 3.0625 bpw&lt;/td&gt; &lt;td align="left"&gt;41.86 GiB&lt;/td&gt; &lt;td align="left"&gt;107.77 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;34&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;23.06 ± 0.24&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama4 17Bx16E (Scout) IQ3_XXS - 3.0625 bpw&lt;/td&gt; &lt;td align="left"&gt;41.86 GiB&lt;/td&gt; &lt;td align="left"&gt;107.77 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;34&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;7.47 ± 0.22&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama4 17Bx16E (Scout) IQ3_XXS - 3.0625 bpw&lt;/td&gt; &lt;td align="left"&gt;41.86 GiB&lt;/td&gt; &lt;td align="left"&gt;107.77 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;35&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;22.89 ± 0.35&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama4 17Bx16E (Scout) IQ3_XXS - 3.0625 bpw&lt;/td&gt; &lt;td align="left"&gt;41.86 GiB&lt;/td&gt; &lt;td align="left"&gt;107.77 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;35&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;7.96 ± 0.04&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama4 17Bx16E (Scout) IQ3_XXS - 3.0625 bpw&lt;/td&gt; &lt;td align="left"&gt;41.86 GiB&lt;/td&gt; &lt;td align="left"&gt;107.77 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;36&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;23.09 ± 0.34&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama4 17Bx16E (Scout) IQ3_XXS - 3.0625 bpw&lt;/td&gt; &lt;td align="left"&gt;41.86 GiB&lt;/td&gt; &lt;td align="left"&gt;107.77 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;36&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;7.96 ± 0.05&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama4 17Bx16E (Scout) IQ3_XXS - 3.0625 bpw&lt;/td&gt; &lt;td align="left"&gt;41.86 GiB&lt;/td&gt; &lt;td align="left"&gt;107.77 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;37&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;22.95 ± 0.19&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama4 17Bx16E (Scout) IQ3_XXS - 3.0625 bpw&lt;/td&gt; &lt;td align="left"&gt;41.86 GiB&lt;/td&gt; &lt;td align="left"&gt;107.77 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;37&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;8.28 ± 0.03&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama4 17Bx16E (Scout) IQ3_XXS - 3.0625 bpw&lt;/td&gt; &lt;td align="left"&gt;41.86 GiB&lt;/td&gt; &lt;td align="left"&gt;107.77 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;38&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;22.46 ± 0.39&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama4 17Bx16E (Scout) IQ3_XXS - 3.0625 bpw&lt;/td&gt; &lt;td align="left"&gt;41.86 GiB&lt;/td&gt; &lt;td align="left"&gt;107.77 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;38&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;8.41 ± 0.22&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama4 17Bx16E (Scout) IQ3_XXS - 3.0625 bpw&lt;/td&gt; &lt;td align="left"&gt;41.86 GiB&lt;/td&gt; &lt;td align="left"&gt;107.77 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;39&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;153.23 ± 0.94&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama4 17Bx16E (Scout) IQ3_XXS - 3.0625 bpw&lt;/td&gt; &lt;td align="left"&gt;41.86 GiB&lt;/td&gt; &lt;td align="left"&gt;107.77 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;39&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;8.42 ± 0.04&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama4 17Bx16E (Scout) IQ3_XXS - 3.0625 bpw&lt;/td&gt; &lt;td align="left"&gt;41.86 GiB&lt;/td&gt; &lt;td align="left"&gt;107.77 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;41&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;148.07 ± 1.28&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama4 17Bx16E (Scout) IQ3_XXS - 3.0625 bpw&lt;/td&gt; &lt;td align="left"&gt;41.86 GiB&lt;/td&gt; &lt;td align="left"&gt;107.77 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;41&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;8.15 ± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama4 17Bx16E (Scout) IQ3_XXS - 3.0625 bpw&lt;/td&gt; &lt;td align="left"&gt;41.86 GiB&lt;/td&gt; &lt;td align="left"&gt;107.77 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;42&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;144.90 ± 0.71&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama4 17Bx16E (Scout) IQ3_XXS - 3.0625 bpw&lt;/td&gt; &lt;td align="left"&gt;41.86 GiB&lt;/td&gt; &lt;td align="left"&gt;107.77 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;42&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;8.01 ± 0.05&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama4 17Bx16E (Scout) IQ3_XXS - 3.0625 bpw&lt;/td&gt; &lt;td align="left"&gt;41.86 GiB&lt;/td&gt; &lt;td align="left"&gt;107.77 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;43&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;144.11 ± 1.14&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama4 17Bx16E (Scout) IQ3_XXS - 3.0625 bpw&lt;/td&gt; &lt;td align="left"&gt;41.86 GiB&lt;/td&gt; &lt;td align="left"&gt;107.77 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;43&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;7.87 ± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;So for best performance I can run: &lt;code&gt;./llama-server -m /meta-llama_Llama-4-Scout-17B-16E-Instruct-IQ3_XXS.gguf --n-cpu-moe 39&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Huge improvements!&lt;/p&gt; &lt;p&gt;pp512 = 20.67, tg128 = 4.00 t/s no moe &lt;/p&gt; &lt;p&gt;pp512 = 153.23, tg128 = 8.42 t.s with --n-cpu-moe 39&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tabletuser_blogspot"&gt; /u/tabletuser_blogspot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nt2c38/llamacpp_moe_models_find_best_ncpumoe_value/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nt2c38/llamacpp_moe_models_find_best_ncpumoe_value/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nt2c38/llamacpp_moe_models_find_best_ncpumoe_value/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-28T23:00:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1ntas8u</id>
    <title>KoboldCpp &amp; Croco.Cpp - Updated versions</title>
    <updated>2025-09-29T06:26:37+00:00</updated>
    <author>
      <name>/u/pmttyji</name>
      <uri>https://old.reddit.com/user/pmttyji</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TLDR .... &lt;a href="https://github.com/LostRuins/koboldcpp"&gt;KoboldCpp&lt;/a&gt; for llama.cpp &amp;amp; &lt;a href="https://github.com/Nexesenex/croco.cpp"&gt;Croco.Cpp&lt;/a&gt; for ik_llama.cpp&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;KoboldCpp is an easy-to-use AI text-generation software for GGML and GGUF models, inspired by the original KoboldAI. It's a single self-contained distributable that builds off &lt;strong&gt;llama.cpp&lt;/strong&gt; and adds many additional powerful features. &lt;/p&gt; &lt;p&gt;Croco.Cpp is fork of KoboldCPP infering GGML/GGUF models on CPU/Cuda with KoboldAI's UI. It's powered partly by &lt;strong&gt;IK_LLama.cpp&lt;/strong&gt;, and compatible with most of Ikawrakow's quants except Bitnet.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Though I'm using KoboldCpp for sometime(along with Jan), I haven't tried Croco.Cpp yet &amp;amp; I was waiting for latest version which is ready now. Both are so useful for people who doesn't prefer command line stuff.&lt;/p&gt; &lt;p&gt;I see KoboldCpp's &lt;a href="https://github.com/LostRuins/koboldcpp/releases/tag/v1.99.4"&gt;current version&lt;/a&gt; is so nice due to changes like QOL change &amp;amp; UI design.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmttyji"&gt; /u/pmttyji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntas8u/koboldcpp_crococpp_updated_versions/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntas8u/koboldcpp_crococpp_updated_versions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ntas8u/koboldcpp_crococpp_updated_versions/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-29T06:26:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1ntedx1</id>
    <title>Official: DeepSeek-V3.2-Exp</title>
    <updated>2025-09-29T10:26:18+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntedx1/official_deepseekv32exp/"&gt; &lt;img alt="Official: DeepSeek-V3.2-Exp" src="https://preview.redd.it/4acvj1zry2sf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0d0be4f54fc4f141e32496ea57718012d97d0ed7" title="Official: DeepSeek-V3.2-Exp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Model: &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp"&gt;https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp&lt;/a&gt;&lt;br /&gt; Tech report: &lt;a href="https://github.com/deepseek-ai/DeepSeek-V3.2-Exp/blob/main/DeepSeek_V3_2.pdf"&gt;https://github.com/deepseek-ai/DeepSeek-V3.2-Exp/blob/main/DeepSeek_V3_2.pdf&lt;/a&gt;&lt;br /&gt; DeepSeek on 𝕏: &lt;a href="https://x.com/deepseek_ai/status/1972604768309871061"&gt;https://x.com/deepseek_ai/status/1972604768309871061&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4acvj1zry2sf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntedx1/official_deepseekv32exp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ntedx1/official_deepseekv32exp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-29T10:26:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1nsy2ak</id>
    <title>GLM4.6 soon ?</title>
    <updated>2025-09-28T20:01:14+00:00</updated>
    <author>
      <name>/u/Angel-Karlsson</name>
      <uri>https://old.reddit.com/user/Angel-Karlsson</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsy2ak/glm46_soon/"&gt; &lt;img alt="GLM4.6 soon ?" src="https://b.thumbs.redditmedia.com/dskyWZciazzyTx2dXG0mytJV0Yq5yzQ46bgrpLsQ1-s.jpg" title="GLM4.6 soon ?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/usggbqdmoyrf1.png?width=567&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a1677630b65f1d4dee3a6776247a0a5f31be050d"&gt;https://preview.redd.it/usggbqdmoyrf1.png?width=567&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a1677630b65f1d4dee3a6776247a0a5f31be050d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;While browsing the &lt;a href="http://z.ai"&gt;z.ai&lt;/a&gt; website, I noticed this... maybe GLM4.6 is coming soon? Given the digital shift, I don't expect major changes... I ear some context lenght increase&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Angel-Karlsson"&gt; /u/Angel-Karlsson &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsy2ak/glm46_soon/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsy2ak/glm46_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nsy2ak/glm46_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-28T20:01:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1nsmksq</id>
    <title>What are Kimi devs smoking</title>
    <updated>2025-09-28T12:02:02+00:00</updated>
    <author>
      <name>/u/Thechae9</name>
      <uri>https://old.reddit.com/user/Thechae9</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsmksq/what_are_kimi_devs_smoking/"&gt; &lt;img alt="What are Kimi devs smoking" src="https://preview.redd.it/t8wfkk09bwrf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7ec5f0e9de05cf0aafc8bc507d4950ca47e8ef09" title="What are Kimi devs smoking" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Strangee&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thechae9"&gt; /u/Thechae9 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/t8wfkk09bwrf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsmksq/what_are_kimi_devs_smoking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nsmksq/what_are_kimi_devs_smoking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-28T12:02:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1nt0kn3</id>
    <title>Someone pinch me .! 🤣 Am I seeing this right ?.🙄</title>
    <updated>2025-09-28T21:43:22+00:00</updated>
    <author>
      <name>/u/sub_RedditTor</name>
      <uri>https://old.reddit.com/user/sub_RedditTor</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nt0kn3/someone_pinch_me_am_i_seeing_this_right/"&gt; &lt;img alt="Someone pinch me .! 🤣 Am I seeing this right ?.🙄" src="https://b.thumbs.redditmedia.com/uZSQcPB3gvUjO5vlPxt8AX_gBhOg-2Up3FDO2_kjoZE.jpg" title="Someone pinch me .! 🤣 Am I seeing this right ?.🙄" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A what looks like 4080S with 32GB vRam ..! 🧐 . I just got 2X 3080 20GB 😫&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sub_RedditTor"&gt; /u/sub_RedditTor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nt0kn3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nt0kn3/someone_pinch_me_am_i_seeing_this_right/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nt0kn3/someone_pinch_me_am_i_seeing_this_right/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-28T21:43:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1nt2l57</id>
    <title>Qwen3 Omni AWQ released</title>
    <updated>2025-09-28T23:12:33+00:00</updated>
    <author>
      <name>/u/No_Information9314</name>
      <uri>https://old.reddit.com/user/No_Information9314</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/cpatonn/Qwen3-Omni-30B-A3B-Instruct-AWQ-4bit"&gt;https://huggingface.co/cpatonn/Qwen3-Omni-30B-A3B-Instruct-AWQ-4bit&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Information9314"&gt; /u/No_Information9314 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nt2l57/qwen3_omni_awq_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nt2l57/qwen3_omni_awq_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nt2l57/qwen3_omni_awq_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-28T23:12:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1nt1jaa</id>
    <title>Good ol gpu heat</title>
    <updated>2025-09-28T22:24:45+00:00</updated>
    <author>
      <name>/u/animal_hoarder</name>
      <uri>https://old.reddit.com/user/animal_hoarder</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nt1jaa/good_ol_gpu_heat/"&gt; &lt;img alt="Good ol gpu heat" src="https://preview.redd.it/94k8168cezrf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7614d55e91893f545ea06888d5bbbc047c1ec146" title="Good ol gpu heat" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I live at 9600ft in a basement with extremely inefficient floor heaters, so it’s usually 50-60F inside year round. I’ve been fine tuning Mistral 7B for a dungeons and dragons game I’ve been working on and oh boy does my 3090 pump out some heat. Popped the front cover off for some more airflow. My cat loves my new hobby, he just waits for me to run another training script so he can soak it in. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/animal_hoarder"&gt; /u/animal_hoarder &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/94k8168cezrf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nt1jaa/good_ol_gpu_heat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nt1jaa/good_ol_gpu_heat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-28T22:24:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1nte4j1</id>
    <title>Deepseek-Ai/DeepSeek-V3.2-Exp and Deepseek-ai/DeepSeek-V3.2-Exp-Base • HuggingFace</title>
    <updated>2025-09-29T10:09:56+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nte4j1/deepseekaideepseekv32exp_and/"&gt; &lt;img alt="Deepseek-Ai/DeepSeek-V3.2-Exp and Deepseek-ai/DeepSeek-V3.2-Exp-Base • HuggingFace" src="https://external-preview.redd.it/VTfzQkd7AA6Y5gHEsOqxIa7Bzf1OPlvJrdnEYbmotnQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ba6d5dd6d41a5218ce7ddbe2cce44e354d9f63ea" title="Deepseek-Ai/DeepSeek-V3.2-Exp and Deepseek-ai/DeepSeek-V3.2-Exp-Base • HuggingFace" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp"&gt;https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp-Base"&gt;https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp-Base&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/jrpylyzjx2sf1.png?width=714&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0f8df476ca80a0fa1d01539f59049856b2e15979"&gt;https://preview.redd.it/jrpylyzjx2sf1.png?width=714&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0f8df476ca80a0fa1d01539f59049856b2e15979&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/d0hamcb6x2sf1.jpg?width=1280&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=5abcd9c9073ae9a56e24b51c4c3fc7a7d13d5c33"&gt;https://preview.redd.it/d0hamcb6x2sf1.jpg?width=1280&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=5abcd9c9073ae9a56e24b51c4c3fc7a7d13d5c33&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/celk1fb6x2sf1.png?width=552&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6713f8594e8a66987bade6a83ca404877e077646"&gt;https://preview.redd.it/celk1fb6x2sf1.png?width=552&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6713f8594e8a66987bade6a83ca404877e077646&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5xnbatckw2sf1.png?width=939&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fa2e365d129160081fc292cbac7c1c5b2dc50814"&gt;https://preview.redd.it/5xnbatckw2sf1.png?width=939&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fa2e365d129160081fc292cbac7c1c5b2dc50814&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nte4j1/deepseekaideepseekv32exp_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nte4j1/deepseekaideepseekv32exp_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nte4j1/deepseekaideepseekv32exp_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-29T10:09:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1nt8jf0</id>
    <title>I have discovered DeepSeeker V3.2-Base</title>
    <updated>2025-09-29T04:10:45+00:00</updated>
    <author>
      <name>/u/ReceptionExternal344</name>
      <uri>https://old.reddit.com/user/ReceptionExternal344</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nt8jf0/i_have_discovered_deepseeker_v32base/"&gt; &lt;img alt="I have discovered DeepSeeker V3.2-Base" src="https://external-preview.redd.it/2DgE6Nx11cfl0KA4q_jdWtEOsZKhXgwGdD7Iw7jyvX8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e4787c26efff2156fccbd5d67ab061987d38be00" title="I have discovered DeepSeeker V3.2-Base" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I discovered the deepseek-3.2-base repository on Hugging Face just half an hour ago, but within minutes it returned a 404 error. Another model is on its way!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/al21vk9t31sf1.png?width=2690&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=067b5daef487efac4fba9699c13a24294088dc42"&gt;https://preview.redd.it/al21vk9t31sf1.png?width=2690&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=067b5daef487efac4fba9699c13a24294088dc42&lt;/a&gt;&lt;/p&gt; &lt;p&gt;unfortunately, I forgot to check the config.json file and only took a screenshot of the repository. I'll just wait for the release now.&lt;/p&gt; &lt;p&gt;Now we have discovered：&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2/"&gt;https://huggingface.co/deepseek-ai/DeepSeek-V3.2/&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ReceptionExternal344"&gt; /u/ReceptionExternal344 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nt8jf0/i_have_discovered_deepseeker_v32base/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nt8jf0/i_have_discovered_deepseeker_v32base/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nt8jf0/i_have_discovered_deepseeker_v32base/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-29T04:10:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1ntcy33</id>
    <title>DeepSeek online model updated</title>
    <updated>2025-09-29T08:51:48+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntcy33/deepseek_online_model_updated/"&gt; &lt;img alt="DeepSeek online model updated" src="https://preview.redd.it/9is51ljzh2sf1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a258b226765fa5eefbb3d3e92ca70908860d33ed" title="DeepSeek online model updated" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sender: DeepSeek Assistant DeepSeek&lt;br /&gt; Message: The DeepSeek online model has been updated to a new version. Everyone is welcome to test it and provide feedback~&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9is51ljzh2sf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntcy33/deepseek_online_model_updated/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ntcy33/deepseek_online_model_updated/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-29T08:51:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1ntb5ab</id>
    <title>deepseek-ai/DeepSeek-V3.2 · Hugging Face</title>
    <updated>2025-09-29T06:49:56+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntb5ab/deepseekaideepseekv32_hugging_face/"&gt; &lt;img alt="deepseek-ai/DeepSeek-V3.2 · Hugging Face" src="https://external-preview.redd.it/2DgE6Nx11cfl0KA4q_jdWtEOsZKhXgwGdD7Iw7jyvX8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e4787c26efff2156fccbd5d67ab061987d38be00" title="deepseek-ai/DeepSeek-V3.2 · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Empty readme and no files yet&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntb5ab/deepseekaideepseekv32_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ntb5ab/deepseekaideepseekv32_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-29T06:49:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1nte1kr</id>
    <title>DeepSeek-V3.2 released</title>
    <updated>2025-09-29T10:04:40+00:00</updated>
    <author>
      <name>/u/Leather-Term-30</name>
      <uri>https://old.reddit.com/user/Leather-Term-30</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/collections/deepseek-ai/deepseek-v32-68da2f317324c70047c28f66"&gt;https://huggingface.co/collections/deepseek-ai/deepseek-v32-68da2f317324c70047c28f66&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Leather-Term-30"&gt; /u/Leather-Term-30 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nte1kr/deepseekv32_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nte1kr/deepseekv32_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nte1kr/deepseekv32_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-29T10:04:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1nt99fp</id>
    <title>GLM-4.6 now accessible via API</title>
    <updated>2025-09-29T04:52:14+00:00</updated>
    <author>
      <name>/u/Mysterious_Finish543</name>
      <uri>https://old.reddit.com/user/Mysterious_Finish543</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nt99fp/glm46_now_accessible_via_api/"&gt; &lt;img alt="GLM-4.6 now accessible via API" src="https://preview.redd.it/yrpnx9o7b1sf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=034a4f48d831abff53602bf4adefb90e5b28a82b" title="GLM-4.6 now accessible via API" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Using the official API, I was able to access GLM 4.6. Looks like release is imminent.&lt;/p&gt; &lt;p&gt;On a side note, the reasoning traces look very different from previous Chinese releases, much more like Gemini models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mysterious_Finish543"&gt; /u/Mysterious_Finish543 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/yrpnx9o7b1sf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nt99fp/glm46_now_accessible_via_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nt99fp/glm46_now_accessible_via_api/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-29T04:52:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
