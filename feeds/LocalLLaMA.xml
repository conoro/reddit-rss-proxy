<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-19T02:20:37+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1p0bbrl</id>
    <title>RTX 3080 20GB - A comprehensive review of Chinese card</title>
    <updated>2025-11-18T13:01:53+00:00</updated>
    <author>
      <name>/u/No-Refrigerator-1672</name>
      <uri>https://old.reddit.com/user/No-Refrigerator-1672</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0bbrl/rtx_3080_20gb_a_comprehensive_review_of_chinese/"&gt; &lt;img alt="RTX 3080 20GB - A comprehensive review of Chinese card" src="https://b.thumbs.redditmedia.com/uoDJmmq5yOFZZRZbOcx_sIHxzvP57lpJQZH7OtIEARk.jpg" title="RTX 3080 20GB - A comprehensive review of Chinese card" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello! Recently, RTX 3080 20GB became available on Chinese sites like Alibaba. In light of rising prices for RTX3090, I've decided to give those cards a try, and ordered a pair of them. In this post I'll feature lots performance benchmarks, compare it to 3090, share my ordering experience, and discuss the feasibility of this purchase.&lt;/p&gt; &lt;h1&gt;Overview of the card&lt;/h1&gt; &lt;p&gt;The cards feature blower-style cooling. Physical dimensions matches that of a server card, like Mi50 or Tesla series. It takes 2 PCIe slots and features power connector on the shorter side. The power is supplied by 2x regular gpu connector (not EPS12V like on Tesla cards), with default power limit of 320W. The card is clearly prepared for installation inside server enclosures. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9blx4dgsrk1g1.jpg?width=4000&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=4be4f96266ff97bdb929d0a9d1db38970d4b0388"&gt;https://preview.redd.it/9blx4dgsrk1g1.jpg?width=4000&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=4be4f96266ff97bdb929d0a9d1db38970d4b0388&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It looks like the card is based on a custom PCB. This PCB features NVLink connector, however, it is taped over with capton tape, and at this moment I can't verify if it is operational. The card also has video connectors (1 HDMI, 3 DisplayPort) and can function like a regular GPU. Card's enclosure is fully made out of metal. From the side, a full copper heatsink is visible, with thermal pads connecting it both to PCB and external shroud. The card feels heavy, sturdy, and well-built.&lt;/p&gt; &lt;h1&gt;Test bench&lt;/h1&gt; &lt;p&gt;I will test the cards in my personal inference server based on consumer motherboard. Due to this, the upper card gets PCIe 3.0 x16 link, while the lower card only gets PCIe 2.0 x2. This leads to degraded performance in tensor parallel mode, however, pipeline parallel mode and single card benchmarks remain largely unaffected. I've opted to install proprietary Nvidia drivers in my system; the cards were instantly recognized by the drivers and worked out of the box. Despite being unofficial mods, they don't require any software modifications on PC side. Full system specs are featured below:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;root@proxmox:~# neofetch .://:` `://:. root@proxmox `hMMMMMMd/ /dMMMMMMh` ------------ `sMMMMMMMd: :mMMMMMMMs` OS: Proxmox VE 8.4.14 x86_64 `-/+oo+/:`.yMMMMMMMh- -hMMMMMMMy.`:/+oo+/-` Host: AX370-Gaming 3 `:oooooooo/`-hMMMMMMMyyMMMMMMMh-`/oooooooo:` Kernel: 6.8.12-16-pve `/oooooooo:`:mMMMMMMMMMMMMm:`:oooooooo/` Uptime: 3 days, 13 hours, 53 mins ./ooooooo+- +NMMMMMMMMN+ -+ooooooo/. Packages: 1348 (dpkg) .+ooooooo+-`oNMMMMNo`-+ooooooo+. Shell: bash 5.2.15 -+ooooooo/.`sMMs`./ooooooo+- Terminal: /dev/pts/6 :oooooooo/`..`/oooooooo: CPU: AMD Ryzen 5 5600G with Radeon Graphics (12) @ 4.464GHz :oooooooo/`..`/oooooooo: GPU: NVIDIA GeForce RTX 3080 -+ooooooo/.`sMMs`./ooooooo+- GPU: AMD ATI Radeon Vega Series / Radeon Vega Mobile Series .+ooooooo+-`oNMMMMNo`-+ooooooo+. GPU: NVIDIA GeForce RTX 3080 ./ooooooo+- +NMMMMMMMMN+ -+ooooooo/. GPU: NVIDIA P102-100 `/oooooooo:`:mMMMMMMMMMMMMm:`:oooooooo/` Memory: 18843MiB / 31458MiB `:oooooooo/`-hMMMMMMMyyMMMMMMMh-`/oooooooo:` `-/+oo+/:`.yMMMMMMMh- -hMMMMMMMy.`:/+oo+/-` `sMMMMMMMm: :dMMMMMMMs` `hMMMMMMd/ /dMMMMMMh` `://:` `://:` root@proxmox:~# nvidia-smi +-----------------------------------------------------------------------------------------+ | NVIDIA-SMI 580.105.08 Driver Version: 580.105.08 CUDA Version: 13.0 | +-----------------------------------------+------------------------+----------------------+ | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |=========================================+========================+======================| | 0 NVIDIA GeForce RTX 3080 On | 00000000:01:00.0 Off | N/A | | 50% 47C P8 14W / 320W | 18781MiB / 20480MiB | 0% Default | | | | N/A | +-----------------------------------------+------------------------+----------------------+ | 1 NVIDIA P102-100 On | 00000000:05:00.0 Off | N/A | | 0% 30C P8 6W / 125W | 8393MiB / 10240MiB | 0% Default | | | | N/A | +-----------------------------------------+------------------------+----------------------+ | 2 NVIDIA GeForce RTX 3080 On | 00000000:08:00.0 Off | N/A | | 50% 53C P8 16W / 320W | 19001MiB / 20480MiB | 0% Default | | | | N/A | +-----------------------------------------+------------------------+----------------------+ +-----------------------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=========================================================================================| | 0 N/A N/A 641329 C VLLM::Worker_PP0 18772MiB | | 1 N/A N/A 753366 C ./llama-server 8386MiB | | 2 N/A N/A 641331 C VLLM::Worker_PP1 18992MiB | +-----------------------------------------------------------------------------------------+ &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;All performance measurements will be performed by &lt;code&gt;vllm bench serve&lt;/code&gt;. Any test was run without KV cache quantization.&lt;/p&gt; &lt;h1&gt;Single card: performance in various inference engines&lt;/h1&gt; &lt;p&gt;For this test, I've chosen two models that a person could run on a single card without CPU offloading: one dense (&lt;a href="https://huggingface.co/Qwen/Qwen3-14B-AWQ"&gt;Qwen3 14B AWQ&lt;/a&gt;) and one MoE (&lt;a href="https://huggingface.co/openai/gpt-oss-20b"&gt;GPT-OSS 20B&lt;/a&gt;). In case of llama.cpp, I've used &lt;a href="https://huggingface.co/unsloth/Qwen3-14B-GGUF"&gt;unsloth/Qwen3-14B-GGUF:Q4_K_XL&lt;/a&gt; and &lt;a href="https://huggingface.co/ggml-org/gpt-oss-20b-GGUF"&gt;ggml-org/gpt-oss-20b-GGUF&lt;/a&gt;. I've also wanted to test HuggingFace TGI, but as&lt;a href="https://huggingface.co/docs/text-generation-inference/supported_models"&gt; it has no support&lt;/a&gt; for neither of test models (or even any of the newer ones for that matter), I decided to skip it.&lt;/p&gt; &lt;p&gt;Engine launch commands:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;vLLM: vllm serve /models/mxfp4/gpt-oss-20b/ --max-model-len 65536 --max-num-seqs 1 llama.cpp: ./llama-server -ngl 999 --no-mmap -fa on --no-webui -c 65536 --parallel 1 -m /models/gguf/gpt-oss-20b-mxfp4.gguf SGLang: python3 -m sglang.launch_server --model-path /models/mxfp4/gpt-oss-20b/ --log-level info --max-running-requests 1 --max-total-tokens 65536 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note: For GPT-OSS, SGLang refused to allocate more KV cache than 59k tokens even when explicitly said to. Therefore, 64k long test for SGLang failed. During initial runs, vLLM asked me to install FlashInfer for speedup in it's output log, so I did. All engines installed in full accordance to their official docs, and no other optimization actions were taken.&lt;/p&gt; &lt;p&gt;For this test, I've used the following command with various input lengths: &lt;/p&gt; &lt;pre&gt;&lt;code&gt;vllm bench serve --dataset-name random --backend openai --host vllm_host --port 8000 --endpoint &amp;quot;/v1/completions&amp;quot; --model &amp;quot;openai/gpt-oss-20b&amp;quot; --max-concurrency 1 --num-prompts 20 --random-input-len 16000 --random-output-len 512 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Prompt Processing speed is calculated as time to first token divided by prompt length.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6uxumaf7dz1g1.png?width=3307&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=70e80fbfa7165eb6cd16f3a67a4c7fce694ebb15"&gt;https://preview.redd.it/6uxumaf7dz1g1.png?width=3307&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=70e80fbfa7165eb6cd16f3a67a4c7fce694ebb15&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ejt10lx7tz1g1.png?width=3307&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6dd53b38889e9714fb5b76eec12309ecac9b2db3"&gt;https://preview.redd.it/ejt10lx7tz1g1.png?width=3307&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6dd53b38889e9714fb5b76eec12309ecac9b2db3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We can see, that for mxfp4 MoE model vLLM outperforms other engines on Prompt Processing (PP) by huge amount. For whatever reason Llama.cpp is very efficient in Token Generation (TG) for short sequences, however this edge is not enough to compensate very slow PP. SGLang lags behind significantly, however, this is to be expected, as SGLang itself states that mxpf4 support is not optimized yet. &lt;/p&gt; &lt;p&gt;For more traditional quantization types, SGLang maintains an edge over vLLM in TG, while matching it for PP for sequences longer than 4k tokens. Llama.cpp loses all across the board in this test. I can conclude that for single card and singe user case, SGLang is probably the best choice for this particular card, if you have compatible model.&lt;/p&gt; &lt;h1&gt;Single card: available KV cache in vLLM&lt;/h1&gt; &lt;p&gt;openai/gpt-oss-20b:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;(EngineCore_DP0 pid=1874) INFO 11-16 08:01:36 [gpu_worker.py:298] Available KV cache memory: 3.65 GiB (EngineCore_DP0 pid=1874) INFO 11-16 08:01:37 [kv_cache_utils.py:1087] GPU KV cache size: 79,744 tokens (EngineCore_DP0 pid=1874) INFO 11-16 08:01:37 [kv_cache_utils.py:1091] Maximum concurrency for 65,536 tokens per request: 2.36x &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;cpatonn/Devstral-Small-2507-AWQ-4bit (cache manually set to 5GB):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;(EngineCore_DP0 pid=1451) INFO 11-16 20:07:47 [kv_cache_utils.py:1087] GPU KV cache size: 32,768 tokens (EngineCore_DP0 pid=1451) INFO 11-16 20:07:47 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 1.00x &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Qwen/Qwen3-14B-AWQ:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;(EngineCore_DP0 pid=1796) INFO 11-16 20:55:30 [gpu_worker.py:298] Available KV cache memory: 7.94 GiB (EngineCore_DP0 pid=1796) INFO 11-16 20:55:30 [kv_cache_utils.py:1087] GPU KV cache size: 52,032 tokens (EngineCore_DP0 pid=1796) INFO 11-16 20:55:30 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 1.59x &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Amounts of available cache memory are reasonable. Personally, I would've liked to have more, but 30k is usable amount, with GPT-OSS 20B having enough to cover most typical use cases.&lt;/p&gt; &lt;h1&gt;Single card: Performance vs power limit&lt;/h1&gt; &lt;p&gt;In some circumstances, people would want to limit power usage of a card to maintain cooler temperatures, lower noise, save up on electrical bill, or install multiple GPUs with a limited power supply. To investigate this, I've measured single card performance vs power limit imposed via nvidia-smi. All tests are done with single requests to GPT-OSS 20B with 16k long prompts.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ozzli6fctz1g1.png?width=2572&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0c5446fb35fd1b0cc88f34d12eae48040b189d3b"&gt;https://preview.redd.it/ozzli6fctz1g1.png?width=2572&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0c5446fb35fd1b0cc88f34d12eae48040b189d3b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We can see that card maintains relatively good performance down to 220W. When power limit is lowered by 30%, card's performance degrades only by 10%, making power limitation a viable option for reducing fan noise and power bill.&lt;/p&gt; &lt;h1&gt;Dual cards: pipeline parallel performance for single user&lt;/h1&gt; &lt;p&gt;As I've stated previously, due to consumer motherboard, I only get PCIe 2.0 x2 to the second card. Preliminary testing showed that in tensor parallel mode, the second card maxes out PCIe bandwidth and plummets PP speeds to completely unacceptable numbers. Pipeline parallel mode, however, seems to stay mostly unaffected, thus I've decided to feature only it in this review. For this test, I've chosen much more popular options for models: &lt;a href="https://huggingface.co/cpatonn/Qwen3-VL-32B-Instruct-AWQ-4bit"&gt;cpatonn/Qwen3-VL-32B-Instruct-AWQ-4bit&lt;/a&gt; to test dense model, and &lt;a href="https://huggingface.co/cpatonn/Qwen3-VL-30B-A3B-Instruct-AWQ-4bit"&gt;cpatonn/Qwen3-VL-30B-A3B-Instruct-AWQ-4bit&lt;/a&gt; to test MoE. For llama.cpp, I've chosen &lt;a href="https://huggingface.co/unsloth/Qwen3-VL-32B-Instruct-GGUF"&gt;unsloth/Qwen3-VL-32B-Instruct-GGUF:Q4_K_XL&lt;/a&gt; and &lt;a href="https://huggingface.co/unsloth/Qwen3-VL-30B-A3B-Instruct-GGUF"&gt;unsloth/Qwen3-VL-30B-A3B-Instruct-GGUF:Q4_K_XL&lt;/a&gt;. SGLang, despite advertising support for Qwen3 VL, threw out errors when I've made requests for both of the models, so I decided that it isn't worth the time.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ymepasv6b02g1.png?width=3307&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8e1480ccd19cdbcd044d9e115a62d7ea42a159be"&gt;https://preview.redd.it/ymepasv6b02g1.png?width=3307&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8e1480ccd19cdbcd044d9e115a62d7ea42a159be&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/dbcrvzc1b02g1.png?width=3307&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=285e0d38b50ef7d07ffe309566fb9b93cec735fc"&gt;https://preview.redd.it/dbcrvzc1b02g1.png?width=3307&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=285e0d38b50ef7d07ffe309566fb9b93cec735fc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So, we can see that those cards perform very well for 30B MoE model. Prompt processing for 32B dense looks very weird, probably hindered by narrow PCIe of the second card. I would conclude that if you want to go for multiple card setup, either go with MoE models, or use threadripper/epyc platform to get proper PCIe connectivity. llama.cpp seems to perform really bad, which isn't a big surprise. It is a shame that SGLang failed to do inference on those models, maybe I will revisit this test after a few updates.&lt;/p&gt; &lt;h1&gt;Dual cards: available KV cache in vLLM&lt;/h1&gt; &lt;p&gt;cpatonn/Qwen3-VL-30B-A3B-Instruct-AWQ-4bit:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;(EngineCore_DP0 pid=566) INFO 11-17 13:11:03 [kv_cache_utils.py:1087] GPU KV cache size: 152,912 tokens (EngineCore_DP0 pid=566) INFO 11-17 13:11:03 [kv_cache_utils.py:1091] Maximum concurrency for 131,072 tokens per request: 1.17x &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;cpatonn/Qwen3-VL-32B-Instruct-AWQ-4bit:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;(EngineCore_DP0 pid=810) INFO 11-17 14:08:46 [kv_cache_utils.py:1087] GPU KV cache size: 53,248 tokens (EngineCore_DP0 pid=810) INFO 11-17 14:08:46 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 1.62x &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Cache situation looks similar to single card case. MoE models get lots of cache that probably covers any use case, dense models get enough cache to be decent for single requests.&lt;/p&gt; &lt;h1&gt;Dual cards: multi-user performance scaling&lt;/h1&gt; &lt;p&gt;Systems like RAG or agentic automation like n8n really like to make parallel requests, so even if you're buying those cards for yourself, you may still be interested in serving multiple parallel requests. To investigate that, I've chosen Qwen3 VL 30B, and have set maximum concurrency up to 16 in vllm, then have launched &lt;code&gt;vllm bench serve&lt;/code&gt; with various concurrency numbers, using this command:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;vllm bench serve --dataset-name random --backend openai --host vllm_host --port 8000 --endpoint &amp;quot;/v1/completions&amp;quot; --model &amp;quot;cpatonn/Qwen3-VL-30B-A3B-Instruct-AWQ-4bit&amp;quot; --max-concurrency 4 --num-prompts 100 --random-input-len 8000 --random-output-len 512 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;By design of this test, there were no requests in the queue on inference engine side, so I'm defining combined PP speed as prompt length divided by time to first token and multiplied by number of parallel requests.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/c0kmx35ch02g1.png?width=3307&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7c03acb238409832dd59a0193d8d395ab03d2a44"&gt;https://preview.redd.it/c0kmx35ch02g1.png?width=3307&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7c03acb238409832dd59a0193d8d395ab03d2a44&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Those GPUs are very good at processing simultaneous requests at their price. It seems like the sweet spot for Qwen3 30B MoE is 12 requests. You can easily run a heavy-duty rag solution like RAG Flow or create a cheap private AI setup for small company.&lt;/p&gt; &lt;h1&gt;Dual cards: comparison against 3090&lt;/h1&gt; &lt;p&gt;Of course, you would want to know how well this card stacks up against 3090. To answer this question, I've rented a runpod with dual 3090, and ran identical test on it. Also, this test serves a second purpose: if performance curves are similar, then we can be sure that my dual-card measurements aren't heavily affected by limited second card connectivity.&lt;/p&gt; &lt;p&gt;This test was run with &lt;a href="https://huggingface.co/cpatonn/Qwen3-VL-30B-A3B-Instruct-AWQ-4bit"&gt;cpatonn/Qwen3-VL-30B-A3B-Instruct-AWQ-4bit&lt;/a&gt;, vllm 0.11.0, in pipeline parallel mode.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/jdbfny3ntz1g1.png?width=3307&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fcb37da4c8287f6f31bcf41f5e27feb21c6ec401"&gt;https://preview.redd.it/jdbfny3ntz1g1.png?width=3307&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fcb37da4c8287f6f31bcf41f5e27feb21c6ec401&lt;/a&gt;&lt;/p&gt; &lt;p&gt;During my testing, I've noticed that time to first token is consistently 300-400ms more for Runpod's 3090s vs mine 3080s, which has made 3090 results for sequences shorter than 16k unrealistically low. Due to this, I've decided to subtract 350ms from Runpod's 3090 measurements before processing the data for the graph. As we can see, 3090 offers 30% more TG performance, but PP performance is equal to 3080. &lt;/p&gt; &lt;h1&gt;Purchasing experience and pricing&lt;/h1&gt; &lt;p&gt;At this moment, I was unable to find any source for those GPUs other than Alibaba. This platform has more of customer-personalized flow: you're supposed to message the supplier you choose, negotiate, then the supplier will send you an offer. Typically, you'll get the first response within half a day. To request a shipping cost estimate, you'll need to tell them your country, city, and postal code. Once all order details are finalized, I had to send them my shipping address, and recieved official offer. In my case, within 24 hours from payment via PayPal, the seller sent me a video of my cards running FurMark and GPU-Z in test benches. Within the next day, they have sent me pictures of the package and shipping paperwork, and asked to verify the credentials. After that the shipping was handed to DHL. Overall, it took 6 days from the moment of me paying for the package to me receiving the parcel. I would rate the experience as good.&lt;/p&gt; &lt;p&gt;People report that this site has a number of scammers. Alibaba itself provides customer protection, but it only works if all your communication and transactions are done via the platform. Therefore, if the supplier asks you to switch to Whatsapp, or pay via wire transfer - refuse and find another one. If you would open supplier's profile on Alibaba, there will be a &amp;quot;Company Overview&amp;quot; page, where Alibaba will openly state the amount of transactions that was done by that supplier - try to find the one with the biggest number, that guarantees that they deal within the platform and your customer protection will be in place. My GPU supplier had 300+ transactions, and a storefront full of PC components.&lt;/p&gt; &lt;p&gt;My bill for the GPUs was structured in a following way: $415 x2 for cards, $80 for shipping, $25 for shipping insurance (applied by Alibaba), $25 Paypal transaction fees,160 EUR for import customs. In total, I've paid 1008.53 EUR, so the final price is 500 EUR per card.&lt;/p&gt; &lt;h1&gt;Was this a good purchase, and should you get one?&lt;/h1&gt; &lt;p&gt;Let's talk about the price. At the moment of writing, the cheapest 3090 in Europe on Ebay is 730 EUR including shipping. This makes 3080 20GB a better value: it costs 25 EUR per GB of VRAM, versus 30 EUR/GB for 3090. From performance comparison we can see that price/performance ratio of those two cards is roughly equal. Given that physically this card is prepared to fit workstations and servers very nicely, it also has an edge over 3090 and other gaming cards for multi-gpu setups. However, there are some caveats: as we can see from single card KV cache measurements, those missing 4GB significantly limit available prompt lengths, limiting long-context-prompt usecases to only MoE models. On the other hand, at the moment of writing, for 500 EUR only 16GB Nvidia cards are available, so when price-per-card is considered, 3080 20GB has an edge over any other option. &lt;/p&gt; &lt;p&gt;Also, there are some concerns about longevity: this 3080 is most likely build from salvaged GPU cores and VRAM out of some mining cards, so the reliability of such product is unknown. Over this sub, I've seen some people claiming that modded 2080Ti 22GB worked very long for them, while other claimed that it failed within a month, so we can draw the conclusion that a modded card can be reliable, but this isn't guaranteed. I've decided to take this risk, and at this moment I'm happy with my purchase. Those cards will work 24/7 in my personal inference server, and I oblige to update this post if they would ever fail in upcoming years.&lt;/p&gt; &lt;p&gt;I hope that you found this set of benchmarks useful, and this post will spark more discussion about those Chinese-made Nvidia cards, as at the moment those options seem to stay out of sight from the majority of this subreddit. Later, when I would have some more spare time, I'll also benchmark those cards in ComfyUI for image/video generation.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Refrigerator-1672"&gt; /u/No-Refrigerator-1672 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0bbrl/rtx_3080_20gb_a_comprehensive_review_of_chinese/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0bbrl/rtx_3080_20gb_a_comprehensive_review_of_chinese/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0bbrl/rtx_3080_20gb_a_comprehensive_review_of_chinese/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-18T13:01:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1p06byo</id>
    <title>Another Reflection 70B Movement: "Momentum" model at movementlabs.ai is just GLM 4.6</title>
    <updated>2025-11-18T08:08:32+00:00</updated>
    <author>
      <name>/u/Broad_Travel_1825</name>
      <uri>https://old.reddit.com/user/Broad_Travel_1825</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p06byo/another_reflection_70b_movement_momentum_model_at/"&gt; &lt;img alt="Another Reflection 70B Movement: &amp;quot;Momentum&amp;quot; model at movementlabs.ai is just GLM 4.6" src="https://b.thumbs.redditmedia.com/X42mpw8E4fXegIsm9hPHq-_TInjxRF4CfSnEz0VYIsQ.jpg" title="Another Reflection 70B Movement: &amp;quot;Momentum&amp;quot; model at movementlabs.ai is just GLM 4.6" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/445ltlss1z1g1.png?width=794&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=824b68302441151b9f84af3cc4916af115268a77"&gt;Front-end token substitution&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/qe9um1fe3z1g1.png?width=731&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a3587782dc490d5eebd96cae0e5107a7efa3349a"&gt;A glitch token specific to GLM 4.6&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Well, well, well... What are you trying to hide?&lt;/p&gt; &lt;p&gt;Also, someone &lt;a href="https://linux.do/t/topic/1182920"&gt;here&lt;/a&gt; observed&lt;code&gt;{&amp;quot;chat&amp;quot;:&amp;quot;Celebras Error : 403&amp;quot;}&lt;/code&gt; response. The super-fast MPU+Momentum model is actually a router to cerebras/glm-4.6.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Broad_Travel_1825"&gt; /u/Broad_Travel_1825 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p06byo/another_reflection_70b_movement_momentum_model_at/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p06byo/another_reflection_70b_movement_momentum_model_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p06byo/another_reflection_70b_movement_momentum_model_at/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-18T08:08:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0f4r8</id>
    <title>Gemini 3 Pro vs Kimi K2 Thinking</title>
    <updated>2025-11-18T15:38:27+00:00</updated>
    <author>
      <name>/u/SlowFail2433</name>
      <uri>https://old.reddit.com/user/SlowFail2433</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone done some initial comparisons between the new Gemini 3 Pro and Kimi K2 Thinking?&lt;/p&gt; &lt;p&gt;What are their strengths/weaknesses relative to each other?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SlowFail2433"&gt; /u/SlowFail2433 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0f4r8/gemini_3_pro_vs_kimi_k2_thinking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0f4r8/gemini_3_pro_vs_kimi_k2_thinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0f4r8/gemini_3_pro_vs_kimi_k2_thinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-18T15:38:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0s608</id>
    <title>Stop guessing RAG chunk sizes</title>
    <updated>2025-11-18T23:54:29+00:00</updated>
    <author>
      <name>/u/InstanceSignal5153</name>
      <uri>https://old.reddit.com/user/InstanceSignal5153</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;Last week, I shared a small tool I built to solve a personal frustration: guessing chunk sizes for RAG pipelines.&lt;/p&gt; &lt;p&gt;The feedback here was incredibly helpful. Several of you pointed out that word-based chunking wasn't accurate enough for LLM context windows and that cloning a repo is annoying.&lt;/p&gt; &lt;p&gt;I spent the weekend fixing those issues. I just updated the project (&lt;code&gt;rag-chunk&lt;/code&gt;) with:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;True Token Chunking:&lt;/strong&gt; I integrated &lt;code&gt;tiktoken&lt;/code&gt;, so now you can chunk documents based on exact token counts (matching OpenAI's encoding) rather than just whitespace/words.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Easier Install:&lt;/strong&gt; It's now packaged properly, so you can install it directly via pip.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Visuals:&lt;/strong&gt; Added a demo GIF in the repo so you can see the evaluation table before trying it.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The goal remains the same: a simple CLI to &lt;strong&gt;measure&lt;/strong&gt; recall for different chunking strategies on your own Markdown files, rather than guessing.&lt;/p&gt; &lt;p&gt;It is 100% open-source. I'd love to know if the token-based logic works better for your use cases.&lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/messkan/rag-chunk"&gt;https://github.com/messkan/rag-chunk&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/submit/?source_id=t3_1p0s3cd"&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InstanceSignal5153"&gt; /u/InstanceSignal5153 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0s608/stop_guessing_rag_chunk_sizes/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0s608/stop_guessing_rag_chunk_sizes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0s608/stop_guessing_rag_chunk_sizes/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-18T23:54:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0okh8</id>
    <title>Nvidia Parakeet-Realtime-EOU-120m-v1</title>
    <updated>2025-11-18T21:30:09+00:00</updated>
    <author>
      <name>/u/nuclearbananana</name>
      <uri>https://old.reddit.com/user/nuclearbananana</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0okh8/nvidia_parakeetrealtimeeou120mv1/"&gt; &lt;img alt="Nvidia Parakeet-Realtime-EOU-120m-v1" src="https://external-preview.redd.it/zVPL4n_nWpqoPYqwS2dM60dbdwGWNNEtSCu33kDP7a0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=595bf506e0637ff1f4f22d1c370ac082639d0dbd" title="Nvidia Parakeet-Realtime-EOU-120m-v1" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Parakeet-Realtime-EOU-120m-v1 is a streaming speech recognition model that also performs end-of-utterance (EOU) detection. It achieves low latency (80ms~160 ms) and signals EOU by emitting an &amp;lt;EOU&amp;gt; token at the end of each utterance. The model supports only English and does not output punctuation or capitalization.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nuclearbananana"&gt; /u/nuclearbananana &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/nvidia/parakeet_realtime_eou_120m-v1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0okh8/nvidia_parakeetrealtimeeou120mv1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0okh8/nvidia_parakeetrealtimeeou120mv1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-18T21:30:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0ro7q</id>
    <title>Apple M5 news - LLM boost &amp; clustering</title>
    <updated>2025-11-18T23:33:08+00:00</updated>
    <author>
      <name>/u/Secure_Archer_1529</name>
      <uri>https://old.reddit.com/user/Secure_Archer_1529</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Secure_Archer_1529"&gt; /u/Secure_Archer_1529 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://appleinsider.com/articles/25/11/18/macos-tahoe-262-will-give-m5-macs-a-giant-machine-learning-speed-boost"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0ro7q/apple_m5_news_llm_boost_clustering/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0ro7q/apple_m5_news_llm_boost_clustering/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-18T23:33:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0d18g</id>
    <title>Cloudfare down = ChatGPT down. Local LLM gang for the win!</title>
    <updated>2025-11-18T14:14:53+00:00</updated>
    <author>
      <name>/u/satireplusplus</name>
      <uri>https://old.reddit.com/user/satireplusplus</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0d18g/cloudfare_down_chatgpt_down_local_llm_gang_for/"&gt; &lt;img alt="Cloudfare down = ChatGPT down. Local LLM gang for the win!" src="https://external-preview.redd.it/AGL421fF8rguq6HfntHEktFb_6D8E61a63BOf9nljqw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c0b9fcd248f726ab4f10022b9235b7e02a4c61c6" title="Cloudfare down = ChatGPT down. Local LLM gang for the win!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/satireplusplus"&gt; /u/satireplusplus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://imgur.com/a/B1K8M3f"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0d18g/cloudfare_down_chatgpt_down_local_llm_gang_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0d18g/cloudfare_down_chatgpt_down_local_llm_gang_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-18T14:14:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0tko5</id>
    <title>Running the latest LLMs like Granite-4.0 and Qwen3 fully on ANE (Apple NPU)</title>
    <updated>2025-11-19T00:56:41+00:00</updated>
    <author>
      <name>/u/Different-Effect-724</name>
      <uri>https://old.reddit.com/user/Different-Effect-724</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Last year, our two co-founders were invited by the Apple Data &amp;amp; Machine Learning Innovation (DMLI) team to share our work on on-device multimodal models for local AI agents. One of the questions that came up in that discussion was: Can the latest LLMs actually run end-to-end on the Apple Neural Engine?&lt;/p&gt; &lt;p&gt;After months of experimenting and building, NexaSDK now runs the latest LLMs like Granite-4.0, Qwen3, Gemma3, and Parakeet-v3, fully on ANE (Apple's NPU), powered by the NexaML engine. &lt;/p&gt; &lt;p&gt;For developers building local AI apps on Apple devices, this unlocks low-power, always-on, fast inference across Mac and iPhone (iOS SDK coming very soon). &lt;/p&gt; &lt;p&gt;Video shows performance running directly on ANE&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1p0tko5/video/ur014yfw342g1/player"&gt;https://reddit.com/link/1p0tko5/video/ur014yfw342g1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Links in comment.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Different-Effect-724"&gt; /u/Different-Effect-724 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0tko5/running_the_latest_llms_like_granite40_and_qwen3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0tko5/running_the_latest_llms_like_granite40_and_qwen3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0tko5/running_the_latest_llms_like_granite40_and_qwen3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T00:56:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0kdcc</id>
    <title>DR Tulu: An open, end-to-end training recipe for long-form deep research</title>
    <updated>2025-11-18T18:51:23+00:00</updated>
    <author>
      <name>/u/ai2_official</name>
      <uri>https://old.reddit.com/user/ai2_official</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0kdcc/dr_tulu_an_open_endtoend_training_recipe_for/"&gt; &lt;img alt="DR Tulu: An open, end-to-end training recipe for long-form deep research" src="https://preview.redd.it/6z12rgxba22g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=44824e149eda9e20a1c7b45b09ec52f394824e96" title="DR Tulu: An open, end-to-end training recipe for long-form deep research" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;What Ai2 is releasing&lt;/h1&gt; &lt;p&gt;We‚Äôre making available the entirety of our DR Tulu research and training stack under a permissive license.&lt;/p&gt; &lt;p&gt;Releasing all of DR Tulu‚Äôs components serves three goals. First, it enables reproducibility and transparency: we release our curated prompt datasets, training and evaluation code (including our RLER implementation), and our 8B model checkpoint so others can replicate our results and study how reward functions and tool configurations shape behavior. Second, it provides deployment flexibility‚Äîyou can run the agent with your own MCP tool stack, infrastructure, and privacy constraints. Third, it supports extensibility: the dr-agent-lib agent library lets you plug in domain-specific tools and retrieval systems without retraining by simply describing new tools to the model. Taken together, these artifacts make DR Tulu the first fully open, end-to-end deep research framework.&lt;/p&gt; &lt;p&gt;We encourage you to experiment with different tool configurations, audit the agent‚Äôs research steps, and test how DR Tulu handles your domain's research questions. If you find issues or ways to improve the approach, we'd love to hear about them.&lt;/p&gt; &lt;p&gt;üìö Blog: &lt;a href="https://allenai.org/blog/dr-tulu"&gt;https://allenai.org/blog/dr-tulu&lt;/a&gt;&lt;/p&gt; &lt;p&gt;‚úèÔ∏è Paper: &lt;a href="http://allenai.org/papers/drtulu"&gt;http://allenai.org/papers/drtulu&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üíª Models: &lt;a href="https://huggingface.co/collections/rl-research/dr-tulu"&gt;https://huggingface.co/collections/rl-research/dr-tulu&lt;/a&gt;&lt;/p&gt; &lt;p&gt;‚å®Ô∏è Code: &lt;a href="https://github.com/rlresearch/DR-Tulu"&gt;https://github.com/rlresearch/DR-Tulu&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ai2_official"&gt; /u/ai2_official &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6z12rgxba22g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0kdcc/dr_tulu_an_open_endtoend_training_recipe_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0kdcc/dr_tulu_an_open_endtoend_training_recipe_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-18T18:51:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0kdqf</id>
    <title>That jump in ARC-AGI-2 score from Gemini 3</title>
    <updated>2025-11-18T18:51:48+00:00</updated>
    <author>
      <name>/u/jd_3d</name>
      <uri>https://old.reddit.com/user/jd_3d</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0kdqf/that_jump_in_arcagi2_score_from_gemini_3/"&gt; &lt;img alt="That jump in ARC-AGI-2 score from Gemini 3" src="https://b.thumbs.redditmedia.com/nZ1Acy54HFuRvJailXsBX9aW7Ms7ZFdoAHCltnW5c0Y.jpg" title="That jump in ARC-AGI-2 score from Gemini 3" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jd_3d"&gt; /u/jd_3d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1p0kdqf"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0kdqf/that_jump_in_arcagi2_score_from_gemini_3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0kdqf/that_jump_in_arcagi2_score_from_gemini_3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-18T18:51:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0dxns</id>
    <title>If the bubble bursts, what's gonna happen to all those chips?</title>
    <updated>2025-11-18T14:51:51+00:00</updated>
    <author>
      <name>/u/freecodeio</name>
      <uri>https://old.reddit.com/user/freecodeio</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Will they become cheap? Here's hoping I can have an H200 in my garage for $1500. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/freecodeio"&gt; /u/freecodeio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0dxns/if_the_bubble_bursts_whats_gonna_happen_to_all/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0dxns/if_the_bubble_bursts_whats_gonna_happen_to_all/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0dxns/if_the_bubble_bursts_whats_gonna_happen_to_all/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-18T14:51:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1ozu5v4</id>
    <title>20,000 Epstein Files in a single text file available to download (~100 MB)</title>
    <updated>2025-11-17T22:14:12+00:00</updated>
    <author>
      <name>/u/tensonaut</name>
      <uri>https://old.reddit.com/user/tensonaut</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've processed all the text and image files (~25,000 document pages/emails) within individual folders released last friday into a two column text file. I used Googles tesseract OCR library to convert jpg to text.&lt;/p&gt; &lt;p&gt;You can download it here: &lt;a href="https://huggingface.co/datasets/tensonaut/EPSTEIN_FILES_20K"&gt;https://huggingface.co/datasets/tensonaut/EPSTEIN_FILES_20K&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I uploaded it yesterday, but some of files were incomplete. This version is full. For each document, I've included the full path to the original google drive folder from House oversight committee so you can link and verify contents.&lt;/p&gt; &lt;p&gt;I used mistral 7b to extract entities and relationships and build a basic Graph RAG. There are some new &amp;quot;associations&amp;quot; that have not been reported in the news but couldn't find any breakthrough content. Also my entity/relationship extraction was quick and dirty. Sharing this dataset for people interested in getting into RAG and digging deeper to get more insight that what meets the eye.&lt;/p&gt; &lt;p&gt;In using this dataset, please be sensitive to the privacy of the people involved (and remember that many of these people were certainly not involved in any of the actions which precipitated the investigation.) - Quoted from Enron Email Dataset release&lt;/p&gt; &lt;p&gt;EDIT (NOV 18 Update): These files were released last friday by the &lt;a href="https://oversight.house.gov/release/oversight-committee-releases-additional-epstein-estate-documents/"&gt;house oversight committee&lt;/a&gt;. I will post an update as soon as todays files are released and processed&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tensonaut"&gt; /u/tensonaut &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozu5v4/20000_epstein_files_in_a_single_text_file/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozu5v4/20000_epstein_files_in_a_single_text_file/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ozu5v4/20000_epstein_files_in_a_single_text_file/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-17T22:14:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0euvd</id>
    <title>The world‚Äôs fastest open-source TTS: Supertonic</title>
    <updated>2025-11-18T15:27:47+00:00</updated>
    <author>
      <name>/u/ANLGBOY</name>
      <uri>https://old.reddit.com/user/ANLGBOY</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0euvd/the_worlds_fastest_opensource_tts_supertonic/"&gt; &lt;img alt="The world‚Äôs fastest open-source TTS: Supertonic" src="https://external-preview.redd.it/YTdlbmtuc2FhMTJnMeUni0jQysE8S8tC5OeTL5WYLlemOmlkeCkLZq86D7UU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=35f3083d0ba442d2daf2de8bc05be89e611809d9" title="The world‚Äôs fastest open-source TTS: Supertonic" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Demo &lt;a href="https://huggingface.co/spaces/Supertone/supertonic#interactive-demo"&gt;https://huggingface.co/spaces/Supertone/supertonic#interactive-demo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Code &lt;a href="https://github.com/supertone-inc/supertonic"&gt;https://github.com/supertone-inc/supertonic&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hello!&lt;/p&gt; &lt;p&gt;I want to share Supertonic, a newly open-sourced TTS engine that focuses on extreme speed, lightweight deployment, and real-world text understanding.&lt;/p&gt; &lt;p&gt;It‚Äôs available in 8+ programming languages: C++, C#, Java, JavaScript, Rust, Go, Swift, and Python, so you can plug it almost anywhere ‚Äî from native apps to browsers to embedded/edge devices.&lt;/p&gt; &lt;p&gt;Technical highlights are&lt;/p&gt; &lt;p&gt;(1) Lightning-speed ‚Äî Real-time factor:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;‚Ä¢&lt;/strong&gt; 0.001 on RTX4090&lt;/p&gt; &lt;p&gt;&lt;strong&gt;‚Ä¢&lt;/strong&gt; 0.006 on M4 Pro&lt;/p&gt; &lt;p&gt;(2) Ultra lightweight ‚Äî 66M parameters&lt;/p&gt; &lt;p&gt;(3) On-device TTS ‚Äî Complete privacy and zero network latency&lt;/p&gt; &lt;p&gt;(4) Advanced text understanding ‚Äî Handles complex, real-world inputs naturally&lt;/p&gt; &lt;p&gt;(5) Flexible deployment ‚Äî Works in browsers, mobile apps, and small edge devices&lt;/p&gt; &lt;p&gt;Regarding (4), one of my favorite test sentences is: &lt;/p&gt; &lt;p&gt;&lt;strong&gt;‚Ä¢&lt;/strong&gt; He spent 10,000 JPY to buy tickets for a JYP concert.&lt;/p&gt; &lt;p&gt;Here, ‚ÄúJPY‚Äù refers to Japanese yen, while ‚ÄúJYP‚Äù refers to a name ‚Äî Supertonic handles the difference seamlessly.&lt;/p&gt; &lt;p&gt;Hope it's useful for you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ANLGBOY"&gt; /u/ANLGBOY &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/w8c1bnsaa12g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0euvd/the_worlds_fastest_opensource_tts_supertonic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0euvd/the_worlds_fastest_opensource_tts_supertonic/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-18T15:27:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0r5ww</id>
    <title>GLM 4.6 on 128 GB RAM with llama.cpp</title>
    <updated>2025-11-18T23:11:48+00:00</updated>
    <author>
      <name>/u/ilintar</name>
      <uri>https://old.reddit.com/user/ilintar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Recently I got my hands on a new box at work with 128 GB RAM and 32 GB VRAM (it's a semi-budget option, with 2x5070, but it performs really well). I decided I'm going to try a few of the bigger models. Obviously, a very good model to run on this is GPT-OSS-120B and it's been the default model, but I've set my eyes on the big ones. The GLM 4.6 REAP was a bit overwhelming, but then I though &amp;quot;what if I could get my hands on a good low quant that fits&amp;quot;?&lt;/p&gt; &lt;p&gt;So, with the help of &lt;a href="https://huggingface.co/AesSedai"&gt;https://huggingface.co/AesSedai&lt;/a&gt; I've obtained a really nice mixed quant: &lt;a href="https://huggingface.co/AesSedai/GLM-4.6-GGUF/tree/main/llama.cpp/GLM-4.6-Q6_K-IQ2_XS-IQ2_XS-IQ3_S"&gt;https://huggingface.co/AesSedai/GLM-4.6-GGUF/tree/main/llama.cpp/GLM-4.6-Q6_K-IQ2_XS-IQ2_XS-IQ3_S&lt;/a&gt; - it's tuned to *just barely* fit in 128GB. What's surprising is how good quality it retains even at such low quant sizes - here's its analysis when I fed it the `modeling_kimi.py` file from Kimi Linear: &lt;a href="https://gist.github.com/pwilkin/7ee5672422bd30afdb47d3898680626b"&gt;https://gist.github.com/pwilkin/7ee5672422bd30afdb47d3898680626b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And on top of that, llama.cpp just merged the results of a few weeks of hard work of new contributor &lt;strong&gt;hksdpc255&lt;/strong&gt; on XML tool calling, including GLM 4.6: &lt;a href="https://github.com/ggml-org/llama.cpp/commit/1920345c3bcec451421bb6abc4981678cc721154"&gt;https://github.com/ggml-org/llama.cpp/commit/1920345c3bcec451421bb6abc4981678cc721154&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Feel free to give it a try - on my box it's getting around 40 t/s prompt processing and about 5 t/s generation, which is not lightning fast, but still a HUGE upgrade from the 5 t/s pp and 3 t/s tg when I tried just a slightly bigger quant.&lt;/p&gt; &lt;p&gt;Edit: forgot to mention, the deployment has 80k context at quite good Q8_0 K/V quantization, so not a gimmick build.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ilintar"&gt; /u/ilintar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0r5ww/glm_46_on_128_gb_ram_with_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0r5ww/glm_46_on_128_gb_ram_with_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0r5ww/glm_46_on_128_gb_ram_with_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-18T23:11:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0q3z1</id>
    <title>Offline Epstein File Ranker Using GPT-OSS-120B (Built on tensonaut‚Äôs dataset)</title>
    <updated>2025-11-18T22:29:30+00:00</updated>
    <author>
      <name>/u/onil_gova</name>
      <uri>https://old.reddit.com/user/onil_gova</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0q3z1/offline_epstein_file_ranker_using_gptoss120b/"&gt; &lt;img alt="Offline Epstein File Ranker Using GPT-OSS-120B (Built on tensonaut‚Äôs dataset)" src="https://preview.redd.it/nkktzj83y22g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a55f8da7446aedd9d2f482226b72c19b4e4ebbf9" title="Offline Epstein File Ranker Using GPT-OSS-120B (Built on tensonaut‚Äôs dataset)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been playing with the new 25k-page Epstein Files drop that &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ozu5v4/20000_epstein_files_in_a_single_text_file"&gt;tensonaut posted&lt;/a&gt;. Instead of reading 100MB of chaotic OCR myself like a medieval scribe, I threw an open-source model at it and built a local tool that &lt;strong&gt;ranks every document by ‚Äúinvestigative usefulness.‚Äù&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Everything runs on a single M3 Max MacBook Pro with &lt;strong&gt;open-source&lt;/strong&gt; models only. No cloud, no API calls, no data leaving the machine.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it does&lt;/strong&gt;&lt;br /&gt; ‚Ä¢ Streams the entire House Oversight release through &lt;strong&gt;openai/gpt-oss-120b&lt;/strong&gt; running locally via LM Studio.&lt;br /&gt; ‚Ä¢ Scores each passage based on actionable leads, controversy, novelty, and power-linkage.&lt;br /&gt; ‚Ä¢ Outputs a fully structured JSONL dataset with headline, score, key insights, implicated actors, financial-flow notes, etc.&lt;br /&gt; ‚Ä¢ Ships with an interactive local viewer so you can filter by score, read full source text, explore lead types, and inspect charts.&lt;br /&gt; ‚Ä¢ Designed for investigative triage, RAG, IR experiments, or academic analysis.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why it matters&lt;/strong&gt;&lt;br /&gt; This corpus is massive, messy, and full of OCR noise. Doing a systematic pass manually is impossible. Doing it with cloud models would be expensive and slow. Doing it locally means it‚Äôs cheap, private, and reproducible.&lt;/p&gt; &lt;p&gt;A full run costs about &lt;strong&gt;$1.50 in electricity&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Tech details&lt;/strong&gt;&lt;br /&gt; ‚Ä¢ Model: openai/gpt-oss-120b served at &lt;code&gt;localhost:5002/v1&lt;/code&gt;&lt;br /&gt; ‚Ä¢ Hardware: M3 Max, 128 GB RAM&lt;br /&gt; ‚Ä¢ Viewer: simple JS dashboard with AG Grid, charts, and chunked JSONL loading&lt;br /&gt; ‚Ä¢ Input dataset: &lt;a href="https://huggingface.co/datasets/tensonaut/EPSTEIN_FILES_20K"&gt;tensonaut‚Äôs EPSTEIN_FILES_20K on Hugging Face&lt;/a&gt;&lt;br /&gt; ‚Ä¢ Output: ranked chunks in &lt;code&gt;contrib/&lt;/code&gt;, auto-indexed by the viewer&lt;br /&gt; ‚Ä¢ Prompt: optimized for investigative lead scoring, with a consistent numerical scale (0‚Äì100)&lt;/p&gt; &lt;p&gt;Repo:&lt;br /&gt; &lt;a href="https://github.com/latent-variable/epstein-ranker"&gt;https://github.com/latent-variable/epstein-ranker&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So far I‚Äôve processed the first 5,000 rows myself and published the scored chunks in the repo. If anyone wants to help triage more of the dataset, the GitHub includes simple instructions for claiming a slice and submitting it as a contrib chunk. The workflow supports clean collaboration with automatic deduping.&lt;/p&gt; &lt;p&gt;If you‚Äôd rather build your own tools on top of the scored output or adapt the ranking method for other document dumps, go for it. Everything is MIT-licensed, fully local, and easy to extend.&lt;/p&gt; &lt;p&gt;Contributions, forks, or experiments are all welcome.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/onil_gova"&gt; /u/onil_gova &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nkktzj83y22g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0q3z1/offline_epstein_file_ranker_using_gptoss120b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0q3z1/offline_epstein_file_ranker_using_gptoss120b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-18T22:29:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0r7uw</id>
    <title>CodeMode vs Traditional MCP benchmark</title>
    <updated>2025-11-18T23:13:56+00:00</updated>
    <author>
      <name>/u/juanviera23</name>
      <uri>https://old.reddit.com/user/juanviera23</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0r7uw/codemode_vs_traditional_mcp_benchmark/"&gt; &lt;img alt="CodeMode vs Traditional MCP benchmark" src="https://preview.redd.it/js0ua9ikl32g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=66015efe5e2a45a7817dad1da12469ede8d60d0b" title="CodeMode vs Traditional MCP benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/juanviera23"&gt; /u/juanviera23 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/js0ua9ikl32g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0r7uw/codemode_vs_traditional_mcp_benchmark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0r7uw/codemode_vs_traditional_mcp_benchmark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-18T23:13:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0jr1f</id>
    <title>Mistral removing ton of old models from API (preparing for a new launch?)</title>
    <updated>2025-11-18T18:28:50+00:00</updated>
    <author>
      <name>/u/mpasila</name>
      <uri>https://old.reddit.com/user/mpasila</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0jr1f/mistral_removing_ton_of_old_models_from_api/"&gt; &lt;img alt="Mistral removing ton of old models from API (preparing for a new launch?)" src="https://preview.redd.it/tg4zaa7b622g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=879c9f3922693c16a694f6bce7604bb1dd61da54" title="Mistral removing ton of old models from API (preparing for a new launch?)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;They are going to be removing 9 (screenshot is missing one) models from their API at the end of this month. So I wonder if that means they are preparing to release something early December? I sure hope I finally get Nemo 2.0 or something... (it's been over a year since that released).&lt;br /&gt; Source: &lt;a href="https://docs.mistral.ai/getting-started/models#legacy-models"&gt;https://docs.mistral.ai/getting-started/models#legacy-models&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mpasila"&gt; /u/mpasila &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/tg4zaa7b622g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0jr1f/mistral_removing_ton_of_old_models_from_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0jr1f/mistral_removing_ton_of_old_models_from_api/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-18T18:28:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0bql2</id>
    <title>My local AI server is up and running, while ChatGPT and Claude are down due to Cloudflare's outage. Take that, big tech corps!</title>
    <updated>2025-11-18T13:20:14+00:00</updated>
    <author>
      <name>/u/alex_bit_</name>
      <uri>https://old.reddit.com/user/alex_bit_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Local servers for the win!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/alex_bit_"&gt; /u/alex_bit_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0bql2/my_local_ai_server_is_up_and_running_while/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0bql2/my_local_ai_server_is_up_and_running_while/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0bql2/my_local_ai_server_is_up_and_running_while/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-18T13:20:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0sisn</id>
    <title>I replicated Anthropic‚Äôs "Introspection" paper on DeepSeek-7B. It works.</title>
    <updated>2025-11-19T00:09:39+00:00</updated>
    <author>
      <name>/u/Specialist_Bad_4465</name>
      <uri>https://old.reddit.com/user/Specialist_Bad_4465</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Specialist_Bad_4465"&gt; /u/Specialist_Bad_4465 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://joshfonseca.com/blogs/introspection"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0sisn/i_replicated_anthropics_introspection_paper_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0sisn/i_replicated_anthropics_introspection_paper_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T00:09:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0iayb</id>
    <title>Google Antigravity is a cursor clone</title>
    <updated>2025-11-18T17:36:03+00:00</updated>
    <author>
      <name>/u/Terminator857</name>
      <uri>https://old.reddit.com/user/Terminator857</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you love vibe coding: &lt;a href="https://antigravity.google/"&gt;https://antigravity.google/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Supports models other than gemini such as GPT-OSS. Hopefully we will get instructions for running local models soon.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Terminator857"&gt; /u/Terminator857 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0iayb/google_antigravity_is_a_cursor_clone/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0iayb/google_antigravity_is_a_cursor_clone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0iayb/google_antigravity_is_a_cursor_clone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-18T17:36:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0lnlo</id>
    <title>Make your AI talk like a caveman and decrease token usage</title>
    <updated>2025-11-18T19:39:38+00:00</updated>
    <author>
      <name>/u/RegionCareful7282</name>
      <uri>https://old.reddit.com/user/RegionCareful7282</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0lnlo/make_your_ai_talk_like_a_caveman_and_decrease/"&gt; &lt;img alt="Make your AI talk like a caveman and decrease token usage" src="https://preview.redd.it/7g67ftgti22g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d7d9207d83386575ef61218ed4c0a30301826b10" title="Make your AI talk like a caveman and decrease token usage" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been working on a little side project to help LLMs talk like‚Ä¶ cavemen.&lt;br /&gt; Why? To save tokens, of course. &lt;/p&gt; &lt;p&gt;It works because LLMs can easily fill in grammar and connectives on their own. So we strip what‚Äôs predictable, keep what‚Äôs meaningful, and the model still understands everything perfectly. &lt;/p&gt; &lt;p&gt;Store RAG documents in caveman-compressed form so each chunk carries more valuable data, fits more context, and gives better retrieval quality.&lt;/p&gt; &lt;p&gt;Thought I'd share it here as it might be beneficial in order to not waste tokens on unnecessary words :)&lt;/p&gt; &lt;p&gt;Feel free to contribute if you have any additions!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/wilpel/caveman-compression"&gt;https://github.com/wilpel/caveman-compression&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RegionCareful7282"&gt; /u/RegionCareful7282 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7g67ftgti22g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0lnlo/make_your_ai_talk_like_a_caveman_and_decrease/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0lnlo/make_your_ai_talk_like_a_caveman_and_decrease/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-18T19:39:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0u8hd</id>
    <title>ollama's enshitification has begun! open-source is not their priority anymore, because they're YC-backed and must become profitable for VCs... Meanwhile llama.cpp remains free, open-source, and easier-than-ever to run! No more ollama</title>
    <updated>2025-11-19T01:26:53+00:00</updated>
    <author>
      <name>/u/nderstand2grow</name>
      <uri>https://old.reddit.com/user/nderstand2grow</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0u8hd/ollamas_enshitification_has_begun_opensource_is/"&gt; &lt;img alt="ollama's enshitification has begun! open-source is not their priority anymore, because they're YC-backed and must become profitable for VCs... Meanwhile llama.cpp remains free, open-source, and easier-than-ever to run! No more ollama" src="https://preview.redd.it/2zt7d6q0942g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6d69898cd41ba5897e02dd650de189c04e2b1fbb" title="ollama's enshitification has begun! open-source is not their priority anymore, because they're YC-backed and must become profitable for VCs... Meanwhile llama.cpp remains free, open-source, and easier-than-ever to run! No more ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nderstand2grow"&gt; /u/nderstand2grow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2zt7d6q0942g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0u8hd/ollamas_enshitification_has_begun_opensource_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0u8hd/ollamas_enshitification_has_begun_opensource_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T01:26:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0gjcu</id>
    <title>Gemini 3 is launched</title>
    <updated>2025-11-18T16:31:01+00:00</updated>
    <author>
      <name>/u/Several-Republic-609</name>
      <uri>https://old.reddit.com/user/Several-Republic-609</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0gjcu/gemini_3_is_launched/"&gt; &lt;img alt="Gemini 3 is launched" src="https://external-preview.redd.it/Jcgyato32sPSUDLsqQhcsyfnhHKEryk97hJ_EjIMDyU.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dc3edcd8902e26525ff2ad02160747ab3d46316e" title="Gemini 3 is launched" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Several-Republic-609"&gt; /u/Several-Republic-609 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://blog.google/products/gemini/gemini-3/#note-from-ceo"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0gjcu/gemini_3_is_launched/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0gjcu/gemini_3_is_launched/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-18T16:31:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1ozopxx</id>
    <title>AMA Announcement: MiniMax, The Opensource Lab Behind MiniMax-M2 + Gifts to Our Community (Wednesday, 8AM-11AM PST)</title>
    <updated>2025-11-17T18:50:44+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozopxx/ama_announcement_minimax_the_opensource_lab/"&gt; &lt;img alt="AMA Announcement: MiniMax, The Opensource Lab Behind MiniMax-M2 + Gifts to Our Community (Wednesday, 8AM-11AM PST)" src="https://preview.redd.it/e3scgr4j5v1g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ca0bf85cb4cf2a2b7e12e8d99d515186275c15e3" title="AMA Announcement: MiniMax, The Opensource Lab Behind MiniMax-M2 + Gifts to Our Community (Wednesday, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/e3scgr4j5v1g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozopxx/ama_announcement_minimax_the_opensource_lab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ozopxx/ama_announcement_minimax_the_opensource_lab/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-17T18:50:44+00:00</published>
  </entry>
</feed>
