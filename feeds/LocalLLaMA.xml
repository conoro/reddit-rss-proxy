<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-10T09:24:42+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1q8mwqy</id>
    <title>Fable: Canon + Loop hybrid beats baseline and pure Canon at toy scale (1.1m)</title>
    <updated>2026-01-09T22:47:11+00:00</updated>
    <author>
      <name>/u/Kasidra</name>
      <uri>https://old.reddit.com/user/Kasidra</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8mwqy/fable_canon_loop_hybrid_beats_baseline_and_pure/"&gt; &lt;img alt="Fable: Canon + Loop hybrid beats baseline and pure Canon at toy scale (1.1m)" src="https://b.thumbs.redditmedia.com/7DBr9vAfHkqoxlrLV5v6UxuEzeJ1kptTcVBJ60H6fhI.jpg" title="Fable: Canon + Loop hybrid beats baseline and pure Canon at toy scale (1.1m)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello! I enjoy playing with toy models. A quick disclaimer: I am a noob that's only been playing with models for a few months. Opus 4.5 wrote all my code, but the ideas were my own. Have mercy.&lt;/p&gt; &lt;p&gt;I present **Fable**, a toy-size language model (128dim, 4 layer, 2 head, ~1.1m parameters) where the first two layers use Canon, and the final two layers loop twice.&lt;/p&gt; &lt;p&gt;I was inspired by these two papers:&lt;/p&gt; &lt;p&gt;*Physics of Language Models: Part 4.1, Architecture Design and the Magic of Canon Layers*&lt;br /&gt; &lt;a href="https://arxiv.org/abs/2512.17351"&gt;https://arxiv.org/abs/2512.17351&lt;/a&gt;&lt;/p&gt; &lt;p&gt;*Scaling Latent Reasoning via Looped Language Models*&lt;br /&gt; &lt;a href="https://arxiv.org/abs/2510.25741"&gt;https://arxiv.org/abs/2510.25741&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I trained 4 models simultaneously with the same hyperparameters:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;A baseline model (1,104,640 params) ‚Äì just standard transformer architecture.&lt;/li&gt; &lt;li&gt;A loop model (1,104,640 params) - a baseline model that loops through itself once, so it sees layers a total of 8 times instead of 4.&lt;/li&gt; &lt;li&gt;A canon model (1,106,176 params) - all 4 layers have the canon layers, which allow the previous two tokens to influence the current.&lt;/li&gt; &lt;li&gt;My hybrid, Fable (1,105,408 params) - the first two layers utilize canon, and the last two layers loop twice (0 1 2 3 2 3 2 3), so it sees layers 8 times instead of 4.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;While I was playing around with canon layers and loops separately, I made a few observations:&lt;/p&gt; &lt;p&gt;- The early canon layers seemed a lot more important when I was messing with ablating them. This makes sense to me, because by the end it's just trying to make the next token, whereas the canon information is more useful earlier on in the decision-making. With small models the difference between looking back 2 tokens vs 3 was negligible for me, so I left it at 2.&lt;br /&gt; - Loops only beat baseline when the model had to &amp;quot;try&amp;quot;. Like training a big model on TinyStories, the looping was just unneeded and harmed loss. But when the model was very small and struggling with the training data, they seemed to help.&lt;/p&gt; &lt;p&gt;So after these observations I decided to try combining them! Canon layers early on for that extra information, and then looping the final two layers for that extra thinking power.&lt;/p&gt; &lt;p&gt;My final loss numbers were:&lt;br /&gt; Baseline: 2.2027&lt;br /&gt; Loop: 2.1447&lt;br /&gt; Canon: 2.0897&lt;br /&gt; **Fable: 2.0492**&lt;/p&gt; &lt;p&gt;Fable actually won! Not by a lot, but consistently. It always started behind pure canon, but then caught up and never gave up the lead. This also occurred when I originally tried this with 64 dim models.&lt;/p&gt; &lt;p&gt;I have no idea if this scales.&lt;/p&gt; &lt;p&gt;**Training**&lt;/p&gt; &lt;p&gt;I trained the model on modified TinyStories. I wanted to make it into a rudimentary chatbot because it's cooler. An example training line:&lt;/p&gt; &lt;p&gt;`‚Äã`‚Äã`&lt;br /&gt; {&amp;quot;text&amp;quot;: &amp;quot;&amp;lt;|user|&amp;gt;Will you make a story, please?&amp;lt;|EOT|&amp;gt;&amp;lt;|fable|&amp;gt;Once upon a time‚Ä¶[insert TinyStories line here]&amp;lt;|EOT|&amp;gt;&amp;quot;}&lt;br /&gt; `‚Äã`‚Äã`&lt;/p&gt; &lt;p&gt;Outside of making stories, it knows its name, its hyperparameters, and how to say hello and goodbye. Its conversational skills are not generalized, so don't expect much. Mostly‚Ä¶ just say hello and ask it &amp;quot;Tell me a story.&amp;quot; üòÇ&lt;/p&gt; &lt;p&gt;An example conversation I had with Fable:&lt;/p&gt; &lt;p&gt;**User:** What's your name?&lt;br /&gt; **Fable:** My name is Fable, I am a small language model trained to write short stories.&lt;br /&gt; **User:** Write me a story, please.&lt;br /&gt; **Fable:** Once upon a time there was a little girl named Lily who loved to play outside in the garden. One day, she found a shiny ship on it and picked it up. It was so big that it started to twist it up.&lt;br /&gt; Lily ran inside her house and saw something wonderful - some other children playing together! She picked it up and showed it to them and thanked him for his fun and decided to go back home. Lily watched as all of her friends played together all day long afternoon, laughing and doing lots of tries to fly.&lt;br /&gt; When they were done the end of their adventure, Lily felt jealous because she had been able to keep the warm blue ship safe. She was very happy she hugged her parents and they both smiled. The moral of the story is that not matter how beautiful you can use things we will get lost or safe again.&lt;br /&gt; **User:** Thanks.&lt;br /&gt; **Fable:** You're welcome.&lt;/p&gt; &lt;p&gt;‚Ä¶She tried her best xD&lt;/p&gt; &lt;p&gt;If you want to mess with this all, you can find the files on GitHub:&lt;br /&gt; &lt;a href="https://github.com/xkasidra/Fable"&gt;https://github.com/xkasidra/Fable&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I've included the python for each model, the training script I used, and the final weights/tokenizer for Fable specifically, along with a simple Gradio interface to chat with it.&lt;/p&gt; &lt;p&gt;Thanks for reading üñ§&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/gtaf9e1rjecg1.png?width=878&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0127412fa549bb247fdc498d8301b386904f903b"&gt;Pretty loss :D&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kasidra"&gt; /u/Kasidra &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8mwqy/fable_canon_loop_hybrid_beats_baseline_and_pure/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8mwqy/fable_canon_loop_hybrid_beats_baseline_and_pure/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q8mwqy/fable_canon_loop_hybrid_beats_baseline_and_pure/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-09T22:47:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1q8u2gj</id>
    <title>AMD AI Lemonade Server - Community Mobile App</title>
    <updated>2026-01-10T04:04:30+00:00</updated>
    <author>
      <name>/u/TheOriginalG2</name>
      <uri>https://old.reddit.com/user/TheOriginalG2</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello all Strix Halo users I am a contributor to lemonade-server and Lemonade Mobile an AMD sponsored local llm server. We have released a mobile app specific for the lemonade-server that is also free. We would like to invite any Android users to send me a message directly to be a tester as it's a requirement from Google before we may submit it for review for release.&lt;/p&gt; &lt;p&gt;Android Test Url: &lt;a href="https://play.google.com/apps/internaltest/4700912028661106414/join?hl=en-US"&gt;https://play.google.com/apps/internaltest/4700912028661106414/join?hl=en-US&lt;/a&gt;&lt;/p&gt; &lt;p&gt;iOS Store Url: &lt;a href="https://apps.apple.com/us/app/lemonade-mobile/id6757372210"&gt;https://apps.apple.com/us/app/lemonade-mobile/id6757372210&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Repository: &lt;a href="https://github.com/lemonade-sdk/lemonade-mobile"&gt;https://github.com/lemonade-sdk/lemonade-mobile&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheOriginalG2"&gt; /u/TheOriginalG2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8u2gj/amd_ai_lemonade_server_community_mobile_app/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8u2gj/amd_ai_lemonade_server_community_mobile_app/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q8u2gj/amd_ai_lemonade_server_community_mobile_app/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-10T04:04:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1q8wp6f</id>
    <title>GRPO on SFT model: reward hacking with blank / gibberish reasoning ‚Äî how are you designing robust rewards?</title>
    <updated>2026-01-10T06:20:15+00:00</updated>
    <author>
      <name>/u/Pale-Box-3470</name>
      <uri>https://old.reddit.com/user/Pale-Box-3470</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I‚Äôve been experimenting with &lt;strong&gt;GRPO on top of an SFT-trained model&lt;/strong&gt; where the model produces reasoning inside &lt;code&gt;&amp;lt;think&amp;gt;&lt;/code&gt; tags and a final answer outside of them.&lt;/p&gt; &lt;p&gt;In one experiment, I &lt;strong&gt;had the KL divergence term, beta to 0.001 (unsloth's default)&lt;/strong&gt;, and the model slowly started &lt;strong&gt;reward hacking&lt;/strong&gt;. Specifically:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It would output &lt;strong&gt;only blank spaces or invisible tokens inside the&lt;/strong&gt; &lt;code&gt;&amp;lt;think&amp;gt;&lt;/code&gt; &lt;strong&gt;tags&lt;/strong&gt;, then produce the correct final answer.&lt;/li&gt; &lt;li&gt;In other runs, if the reward function encouraged longer reasoning, the model would generate &lt;strong&gt;unique gibberish tokens or nonsensical words&lt;/strong&gt; purely to inflate the reasoning length.&lt;/li&gt; &lt;li&gt;The final answer was often still correct, but the ‚Äúreasoning‚Äù was obviously meaningless.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I understand &lt;em&gt;why&lt;/em&gt; this is happening, but I‚Äôm trying to avoid heavy-handed solutions. In particular:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I &lt;strong&gt;don‚Äôt want to use another LLM to judge whether the reasoning ‚Äúmakes sense‚Äù&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;I also don‚Äôt want brittle heuristics that just shift the exploit elsewhere.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;My question:&lt;br /&gt; &lt;strong&gt;How are you all designing reward functions that are robust to this kind of behavior and have worked well in practice?&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Are you relying mainly on KL + light shaping?&lt;/li&gt; &lt;li&gt;Using structural constraints instead of semantic checks?&lt;/li&gt; &lt;li&gt;Penalizing entropy collapse or token repetition?&lt;/li&gt; &lt;li&gt;Moving away from reasoning-length-based rewards entirely?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I‚Äôd really appreciate hearing what has &lt;em&gt;actually worked&lt;/em&gt; for you in GRPO / RLHF-style setups, especially when dealing with explicit reasoning traces.&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pale-Box-3470"&gt; /u/Pale-Box-3470 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8wp6f/grpo_on_sft_model_reward_hacking_with_blank/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8wp6f/grpo_on_sft_model_reward_hacking_with_blank/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q8wp6f/grpo_on_sft_model_reward_hacking_with_blank/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-10T06:20:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1q82zdm</id>
    <title>Minimax also live on Hong Kong Stock Exchange</title>
    <updated>2026-01-09T08:33:27+00:00</updated>
    <author>
      <name>/u/No_Conversation9561</name>
      <uri>https://old.reddit.com/user/No_Conversation9561</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q82zdm/minimax_also_live_on_hong_kong_stock_exchange/"&gt; &lt;img alt="Minimax also live on Hong Kong Stock Exchange" src="https://preview.redd.it/999goi9xbacg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5d20235c10219672401efcf3df3bcdf3da53b9a5" title="Minimax also live on Hong Kong Stock Exchange" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Conversation9561"&gt; /u/No_Conversation9561 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/999goi9xbacg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q82zdm/minimax_also_live_on_hong_kong_stock_exchange/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q82zdm/minimax_also_live_on_hong_kong_stock_exchange/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-09T08:33:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1q8t5ie</id>
    <title>Which open-weights model should I use for health, career, and relationship advice with reliable citations?</title>
    <updated>2026-01-10T03:19:38+00:00</updated>
    <author>
      <name>/u/baldyogi</name>
      <uri>https://old.reddit.com/user/baldyogi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone ‚Äî I‚Äôm choosing an open-weights LLM to run locally / self-host (budget varies) and want it to: Match Anthropic Opus 4.5‚Äìlevel general-knowledge performance (high accuracy across general QA/knowledge benchmarks). Provide clear, verifiable citations (articles, DOIs, books) for health analysis, career guidance, and relationship/psychology discussions. Answers should include numbered in-text references and a bibliography with working links/DOIs and short rationale for recommended books. Be easy to integrate with a retrieval + vector DB pipeline (Weaviate/Milvus/Elasticsearch) and have community examples for citation-aware prompting or instruction/fine-tuning. Include safeguards for medical content (clear disclaimers, no prescribing dosages) and a policy for flagging low-quality sources. Questions: Which open-weights models (specific checkpoints) in Jan 2026 are closest to Opus 4.5 for general knowledge and also have good community support for retrieval/citation pipelines? (I‚Äôm currently considering Qwen3, GPT OSS, and DeepSeek ‚Äî pros/cons for citation-heavy use.) Which models/variants have existing citation-focused forks, instruction-tuned checkpoints, or verified community templates that reliably produce numbered references + bibliographies? Practical recommendation by scale: If I have (a) a single high-end GPU or small server, (b) a mid-size local cluster / cloud budget, or (c) ample cloud budget ‚Äî which specific model + retrieval stack would you run for best citation reliability? Any ready prompts/templates or minimal fine-tuning tips to force bibliography-style outputs and to verify cited links automatically? Known pitfalls: hallucination patterns around citations, broken links, or unsafe health advice with these models. Thank you in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/baldyogi"&gt; /u/baldyogi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8t5ie/which_openweights_model_should_i_use_for_health/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8t5ie/which_openweights_model_should_i_use_for_health/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q8t5ie/which_openweights_model_should_i_use_for_health/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-10T03:19:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1q8aypi</id>
    <title>Tested GLM 4.7 vs MiniMax M2.1 - impressed with the performance of both</title>
    <updated>2026-01-09T15:17:53+00:00</updated>
    <author>
      <name>/u/alokin_09</name>
      <uri>https://old.reddit.com/user/alokin_09</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8aypi/tested_glm_47_vs_minimax_m21_impressed_with_the/"&gt; &lt;img alt="Tested GLM 4.7 vs MiniMax M2.1 - impressed with the performance of both" src="https://b.thumbs.redditmedia.com/_Ob25yWKAikztI0zBuNqnB-jnl-5yva82tvIgausepQ.jpg" title="Tested GLM 4.7 vs MiniMax M2.1 - impressed with the performance of both" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Full transparency, I work closely with the Kilo Code team, so take this with appropriate context. That said, I think the results are genuinely interesting for anyone running local/open-weight models.&lt;/p&gt; &lt;p&gt;We ran GLM 4.7 and MiniMax M2.1 through a real coding benchmark, building a CLI task runner with 20 features (dependency management, parallel execution, caching, YAML parsing, etc.). The kind of task that would take a senior dev a day or two.&lt;/p&gt; &lt;p&gt;How it was actually tested:&lt;/p&gt; &lt;p&gt;- Phase 1: Architecture planning (Architect mode)&lt;/p&gt; &lt;p&gt;- Phase 2: Full implementation (Code mode)&lt;/p&gt; &lt;p&gt;- Both models ran uninterrupted with zero human intervention&lt;/p&gt; &lt;p&gt;Overall performance summary &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/c636beit7ccg1.png?width=1456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0e175e42659bcbee51d9f66d5d29ec79958a2b00"&gt;https://preview.redd.it/c636beit7ccg1.png?width=1456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0e175e42659bcbee51d9f66d5d29ec79958a2b00&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Phase 1 results&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;GLM 4.7:&lt;/em&gt;&lt;/p&gt; &lt;p&gt;- 741-line architecture doc with 3 Mermaid diagrams&lt;/p&gt; &lt;p&gt;- Nested structure: 18 files across 8 directories &lt;/p&gt; &lt;p&gt;- Kahn's algorithm with pseudocode, security notes, 26-step roadmap&lt;/p&gt; &lt;p&gt;&lt;em&gt;MiniMax M2.1:&lt;/em&gt;&lt;/p&gt; &lt;p&gt;- 284-line plan, 2 diagrams - leaner but covered everything&lt;/p&gt; &lt;p&gt;- Flat structure: 9 files&lt;/p&gt; &lt;p&gt;- Used Commander.js (smart library choice vs rolling your own) &lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Plan Scoring&lt;/em&gt;&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/cw1fvloq9ccg1.png?width=1014&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=af5febf64d3d28f170bf693d58257c386865c814"&gt;https://preview.redd.it/cw1fvloq9ccg1.png?width=1014&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=af5febf64d3d28f170bf693d58257c386865c814&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Phase 2 Results: Implementation&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Both models successfully implemented all 20 requirements. The code compiles, runs, and handles the test cases correctly without any major issues or errors.&lt;/p&gt; &lt;p&gt;Implementations include:&lt;/p&gt; &lt;p&gt;- Working topological sort with cycle detection&lt;/p&gt; &lt;p&gt;- Parallel execution with concurrency limits&lt;/p&gt; &lt;p&gt;GLM 4.7‚Äôs is more responsive to individual task completion. MiniMax M2.1‚Äôs is simpler to understand.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Implementation Scoring&lt;/em&gt;&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/a1g7d8ul9ccg1.png?width=1426&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7891b07de8642aac887a1acb44a432e02c5b2c58"&gt;https://preview.redd.it/a1g7d8ul9ccg1.png?width=1426&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7891b07de8642aac887a1acb44a432e02c5b2c58&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Code Quality Differences&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;While both implementations are functional, they differ in structure and style.&lt;/p&gt; &lt;p&gt;For example, for the architecture test, GLM 4.7 created a deeply modular structure, while MiniMax M2.1 created a flat structure. &lt;/p&gt; &lt;p&gt;For error handling, GLM 4.7 created custom error classes. On the other hand, MiniMax M2.1 used standard Error objects with descriptive messages:&lt;/p&gt; &lt;p&gt;&lt;a href="https://substackcdn.com/image/fetch/$s_!9AeR!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F155ec0e4-5b77-4398-a7aa-87af0f2395e6_1629x652.png"&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Regarding CLI Parsing, GLM 4.7 implemented argument parsing manually, &lt;a href="https://substackcdn.com/image/fetch/$s_!J5xk!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a945a88-dfa1-4f9a-b264-070994e52806_1629x600.png"&gt;&lt;/a&gt;MiniMax M2.1 used commander.js:&lt;/p&gt; &lt;p&gt;&lt;a href="https://substackcdn.com/image/fetch/$s_!v0un!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc4d599b7-4ff0-48a9-8a6e-12701c009262_1629x276.png"&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GLM 4.7‚Äôs approach has no external dependency. MiniMax M2.1‚Äôs approach is more maintainable and handles edge cases automatically.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Documentation&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;GLM 4.7 generated a 363-line README.md with installation instructions, configuration reference, CLI options, multiple examples, and exit code documentation.&lt;/p&gt; &lt;p&gt;Both models demonstrated genuine agentic behavior. After finishing the implementation, each model tested its own work by running the CLI with Bash and verified the output.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Cost Analysis&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://substackcdn.com/image/fetch/$s_!VUYs!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa32c27b-b49d-4704-b8be-6332d4875217_794x386.png"&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9pesc5s0bccg1.png?width=794&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=980ef4aacd34f33d1aa9917126a2745fde950acd"&gt;https://preview.redd.it/9pesc5s0bccg1.png?width=794&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=980ef4aacd34f33d1aa9917126a2745fde950acd&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Tradeoffs&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Based on our testing, GLM 4.7 is better if you want comprehensive documentation and modular architecture out of the box. It generated a full README, detailed error classes, and organized code across 18 well-separated files. The tradeoff is higher cost and some arguably over-engineered patterns like manual CLI parsing when a library would do.&lt;/p&gt; &lt;p&gt;MiniMax M2.1 is better if you prefer simpler code and lower cost. Its 9-file structure is easier to navigate, and it used established libraries like Commander.js instead of rolling its own. The tradeoff is no documentation. You‚Äôll need to add a README and inline comments yourself. &lt;/p&gt; &lt;p&gt;If you want the full breakdown with code snippets and deeper analysis, you can read it here: &lt;a href="https://blog.kilo.ai/p/open-weight-models-are-getting-serious"&gt;https://blog.kilo.ai/p/open-weight-models-are-getting-serious&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/alokin_09"&gt; /u/alokin_09 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8aypi/tested_glm_47_vs_minimax_m21_impressed_with_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8aypi/tested_glm_47_vs_minimax_m21_impressed_with_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q8aypi/tested_glm_47_vs_minimax_m21_impressed_with_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-09T15:17:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1q8b82f</id>
    <title>Ministral-3-14B-Reasoning: High Intelligence on Low VRAM ‚Äì A Benchmark-Comparison</title>
    <updated>2026-01-09T15:27:49+00:00</updated>
    <author>
      <name>/u/Snail_Inference</name>
      <uri>https://old.reddit.com/user/Snail_Inference</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Below you‚Äôll find a benchmark comparison of Ministral-3-14B-Reasoning-2512 against 10 other large language models.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;LiveCodeBench:&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;LiveCodeBench&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;GLM-4.5-Air&lt;/td&gt; &lt;td align="left"&gt;70.7%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Gemini 2.5 Pro Preview&lt;/td&gt; &lt;td align="left"&gt;69.0%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama 3.1 Nemotron Ultra&lt;/td&gt; &lt;td align="left"&gt;66.3%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 32B&lt;/td&gt; &lt;td align="left"&gt;65.7%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MiniMax M1 80K&lt;/td&gt; &lt;td align="left"&gt;65.0%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Ministral 3 (14B Reasoning)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;64.6%&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;QwQ-32B&lt;/td&gt; &lt;td align="left"&gt;63.4%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 30B A3B&lt;/td&gt; &lt;td align="left"&gt;62.6%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MiniMax M1 40K&lt;/td&gt; &lt;td align="left"&gt;62.3%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Ministral 3 (8B Reasoning)&lt;/td&gt; &lt;td align="left"&gt;61.6%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;DeepSeek R1 Distill Llama&lt;/td&gt; &lt;td align="left"&gt;57.5%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;GPQA:&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;GPQA&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;o1-preview&lt;/td&gt; &lt;td align="left"&gt;73.3%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 VL 32B Thinking&lt;/td&gt; &lt;td align="left"&gt;73.1%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Claude Haiku 4.5&lt;/td&gt; &lt;td align="left"&gt;73.0%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-Next-80B-A3B-Instruct&lt;/td&gt; &lt;td align="left"&gt;72.9%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPT OSS 20B&lt;/td&gt; &lt;td align="left"&gt;71.5%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Ministral 3 (14B Reasoning)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;71.2%&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPT-5 nano&lt;/td&gt; &lt;td align="left"&gt;71.2%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Magistral Medium&lt;/td&gt; &lt;td align="left"&gt;70.8%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 VL 30B A3B Instruct&lt;/td&gt; &lt;td align="left"&gt;70.4%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPT-4o&lt;/td&gt; &lt;td align="left"&gt;70.1%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MiniMax M1 80K&lt;/td&gt; &lt;td align="left"&gt;70.0%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;AIME 2024:&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;&lt;strong&gt;Model&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;AIME 2024&lt;/strong&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Grok-3&lt;/td&gt; &lt;td align="left"&gt;93.3%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Gemini 2.5 Pro&lt;/td&gt; &lt;td align="left"&gt;92.0%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;o3&lt;/td&gt; &lt;td align="left"&gt;91.6%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;DeepSeek-R1-0528&lt;/td&gt; &lt;td align="left"&gt;91.4%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GLM-4.5&lt;/td&gt; &lt;td align="left"&gt;91.0%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Ministral 3 (14B Reasoning 2512)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;89.8%&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GLM-4.5-Air&lt;/td&gt; &lt;td align="left"&gt;89.4%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Gemini 2.5 Flash&lt;/td&gt; &lt;td align="left"&gt;88.0%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;o3-mini&lt;/td&gt; &lt;td align="left"&gt;87.3%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;DeepSeek R1 Zero&lt;/td&gt; &lt;td align="left"&gt;86.7%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;DeepSeek R1 Distill Llama 70B&lt;/td&gt; &lt;td align="left"&gt;86.7%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;AIME 2025:&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;&lt;strong&gt;Model&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;AIME 2025&lt;/strong&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-Next-80B-A3B-Thinking&lt;/td&gt; &lt;td align="left"&gt;87.8%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;DeepSeek-R1-0528&lt;/td&gt; &lt;td align="left"&gt;87.5%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Claude Sonnet 4.5&lt;/td&gt; &lt;td align="left"&gt;87.0%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;o3&lt;/td&gt; &lt;td align="left"&gt;86.4%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPT-5 nano&lt;/td&gt; &lt;td align="left"&gt;85.2%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Ministral 3 (14B Reasoning 2512)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;85.0%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 VL 32B Thinking&lt;/td&gt; &lt;td align="left"&gt;83.7%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 VL 30B A3B Thinking&lt;/td&gt; &lt;td align="left"&gt;83.1%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Gemini 2.5 Pro&lt;/td&gt; &lt;td align="left"&gt;83.0%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 Max&lt;/td&gt; &lt;td align="left"&gt;81.6%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 235B A22B&lt;/td&gt; &lt;td align="left"&gt;81.5%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;All benchmark results are sourced from this page: &lt;a href="https://llm-stats.com/benchmarks/llm-leaderboard-full"&gt;https://llm-stats.com/benchmarks/llm-leaderboard-full&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Snail_Inference"&gt; /u/Snail_Inference &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8b82f/ministral314breasoning_high_intelligence_on_low/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8b82f/ministral314breasoning_high_intelligence_on_low/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q8b82f/ministral314breasoning_high_intelligence_on_low/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-09T15:27:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1q8xj0m</id>
    <title>Qwen3-VL Emb and Reranker are supported by chatllm.cpp now</title>
    <updated>2026-01-10T07:07:01+00:00</updated>
    <author>
      <name>/u/foldl-li</name>
      <uri>https://old.reddit.com/user/foldl-li</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Note that, in Qwen3-Emb, queries and documents are processing differently by using dedicated prompts. Qwen3-VL-Emb does not do this anymore.&lt;/p&gt; &lt;p&gt;Although it's possible to use different prompts for Qwen3-VL-Emb, the official example does not do this.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/foldl/chatllm.cpp"&gt;https://github.com/foldl/chatllm.cpp&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/foldl-li"&gt; /u/foldl-li &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8xj0m/qwen3vl_emb_and_reranker_are_supported_by/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8xj0m/qwen3vl_emb_and_reranker_are_supported_by/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q8xj0m/qwen3vl_emb_and_reranker_are_supported_by/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-10T07:07:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1q8oa60</id>
    <title>Which of the &lt;= 32B models has the best reasoning?</title>
    <updated>2026-01-09T23:43:41+00:00</updated>
    <author>
      <name>/u/Robert__Sinclair</name>
      <uri>https://old.reddit.com/user/Robert__Sinclair</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I mean reasoning on problem solving with &amp;quot;not common&amp;quot; problems.&lt;/p&gt; &lt;p&gt;The deepseek distillation seems quite good. But perhaps there are better ones.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Robert__Sinclair"&gt; /u/Robert__Sinclair &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8oa60/which_of_the_32b_models_has_the_best_reasoning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8oa60/which_of_the_32b_models_has_the_best_reasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q8oa60/which_of_the_32b_models_has_the_best_reasoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-09T23:43:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1q8m89n</id>
    <title>Hermit-AI: Chat with 100GB+ of Wikipedia/Docs offline using a Multi-Joint RAG pipeline</title>
    <updated>2026-01-09T22:20:02+00:00</updated>
    <author>
      <name>/u/Smart-Competition200</name>
      <uri>https://old.reddit.com/user/Smart-Competition200</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8m89n/hermitai_chat_with_100gb_of_wikipediadocs_offline/"&gt; &lt;img alt="Hermit-AI: Chat with 100GB+ of Wikipedia/Docs offline using a Multi-Joint RAG pipeline" src="https://preview.redd.it/2l4v4kpedecg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6da0d5b9083950377ca03f5636085a070944dde2" title="Hermit-AI: Chat with 100GB+ of Wikipedia/Docs offline using a Multi-Joint RAG pipeline" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Hermit-AI&lt;/strong&gt; because I was frustrated with the state of offline RAG.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Headache:&lt;/strong&gt; I wanted to use Local AI along side my collection of ZIM files (Wikipedia, StackExchange, etc.) entirely offline. But every tool I tried had the same issues:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;&amp;quot;Needle in a Haystack&amp;quot;&lt;/strong&gt;: Traditional vector search kept retrieving irrelevant chunks when the dataset was this huge.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hallucinations&lt;/strong&gt;: The AI would confidently agree with false premises just to be helpful.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;So I built a &amp;quot;Multi-Joint&amp;quot; Reasoning Pipeline.&lt;/strong&gt; Instead of just doing one big search and hoping for the best, Hermit breaks the process down. while not perfect i am happy with the results. I can only imagine it getting better as the efficiency and intelligence of local models improve over time. &lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Joint 1 (Extraction)&lt;/strong&gt;: It stops to ask &amp;quot;Who/What specifically is this user asking about?&amp;quot; before touching the database.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Joint 2 (JIT Indexing)&lt;/strong&gt;: It builds a tiny, ephemeral search index &lt;em&gt;just for that query&lt;/em&gt; on the fly. This keeps it fast and accurate without needing 64GB of RAM.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Joint 3 (Verification)&lt;/strong&gt;: This is the cool part. It has a specific &amp;quot;Fact-Check&amp;quot; stage that reads the retrieved text and effectively says, &amp;quot;Wait, does this text actually support what the user is claiming?&amp;quot; If not, it corrects you.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Who is this for?&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Data hoarders (like me) with terabytes of ZIMs.&lt;/li&gt; &lt;li&gt;Researchers working in air-gapped environments.&lt;/li&gt; &lt;li&gt;Privacy advocates who want zero data leakage.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Tech Stack:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Pure Python + &lt;code&gt;llama-cpp-python&lt;/code&gt; (GGUF models)&lt;/li&gt; &lt;li&gt;Native ZIM file support (no conversion needed)&lt;/li&gt; &lt;li&gt;FAISS for the JIT indexing&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I've also included a tool called &lt;strong&gt;&amp;quot;Forge&amp;quot;&lt;/strong&gt; so you can turn your own PDF/Markdown folders into ZIM files and treat them like Wikipedia.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Repo:&lt;/strong&gt; &lt;a href="https://github.com/0nspaceshipearth/Hermit-AI"&gt;https://github.com/0nspaceshipearth/Hermit-AI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'd love to hear if anyone else has hit these &amp;quot;needle in a haystack&amp;quot; limits with local RAG and how you solved them!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Smart-Competition200"&gt; /u/Smart-Competition200 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2l4v4kpedecg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8m89n/hermitai_chat_with_100gb_of_wikipediadocs_offline/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q8m89n/hermitai_chat_with_100gb_of_wikipediadocs_offline/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-09T22:20:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1q8x9yp</id>
    <title>Minisforum BD395i MAX motherboard at CES 2026: built-in AMD Strix Halo APU, use your own GPU</title>
    <updated>2026-01-10T06:52:19+00:00</updated>
    <author>
      <name>/u/noiserr</name>
      <uri>https://old.reddit.com/user/noiserr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8x9yp/minisforum_bd395i_max_motherboard_at_ces_2026/"&gt; &lt;img alt="Minisforum BD395i MAX motherboard at CES 2026: built-in AMD Strix Halo APU, use your own GPU" src="https://external-preview.redd.it/KG0Myw_6qN7QIlQJb4mBoOF1kj92CgMaeoC8dUJj7QQ.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0ad0e2ebc40d196f43927457a486ec1a5070565a" title="Minisforum BD395i MAX motherboard at CES 2026: built-in AMD Strix Halo APU, use your own GPU" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/noiserr"&gt; /u/noiserr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.tweaktown.com/news/109642/minisforum-bd395i-max-motherboard-at-ces-2026-built-in-amd-strix-halo-apu-use-your-own-gpu/index.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8x9yp/minisforum_bd395i_max_motherboard_at_ces_2026/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q8x9yp/minisforum_bd395i_max_motherboard_at_ces_2026/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-10T06:52:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1q8f242</id>
    <title>After 8 years building cloud infrastructure, I'm betting on local-first AI</title>
    <updated>2026-01-09T17:48:38+00:00</updated>
    <author>
      <name>/u/PandaAvailable2504</name>
      <uri>https://old.reddit.com/user/PandaAvailable2504</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sold my Saas company last year and we used to process everything in the cloud. Now, after a few realisations, I'm doing the opposite. As I watch the AI space evolve, I can‚Äôt help but wonder how there‚Äôs a growing sentiment of wanting capable models that run on hardware they control. More people seem to be moving towards local inference: whether for privacy, cost, latency, or just independence from API rate limits. &lt;/p&gt; &lt;p&gt;Curious if anyone else is thinking about this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PandaAvailable2504"&gt; /u/PandaAvailable2504 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8f242/after_8_years_building_cloud_infrastructure_im/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8f242/after_8_years_building_cloud_infrastructure_im/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q8f242/after_8_years_building_cloud_infrastructure_im/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-09T17:48:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1q84u82</id>
    <title>Big tech companies, now "DRAM beggars," are staying in Pangyo and Pyeongtaek, demanding "give us some supplies."</title>
    <updated>2026-01-09T10:28:56+00:00</updated>
    <author>
      <name>/u/FullstackSensei</name>
      <uri>https://old.reddit.com/user/FullstackSensei</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q84u82/big_tech_companies_now_dram_beggars_are_staying/"&gt; &lt;img alt="Big tech companies, now &amp;quot;DRAM beggars,&amp;quot; are staying in Pangyo and Pyeongtaek, demanding &amp;quot;give us some supplies.&amp;quot;" src="https://external-preview.redd.it/bSPrxxtNL1oMDlmeG0HktX0ZjOAtRM_In15JbYuAojA.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=81d28b6e2751adbaced23bb325679e9970b53426" title="Big tech companies, now &amp;quot;DRAM beggars,&amp;quot; are staying in Pangyo and Pyeongtaek, demanding &amp;quot;give us some supplies.&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Not a Korean speaker. Came across this in another sub. The TLDR is that everyone is scrambling to buy as much as they can as soon as they can, because &amp;quot;demanding a 50-60% increase in server DRAM supply prices from the previous quarter during their first-quarter negotiations with customers&amp;quot;.&lt;/p&gt; &lt;p&gt;Per the article, DDR4 prices went up from $1.40 last January to $9.30 in December (my interpretation is $/GB). If they're increasing by another 50%, that's almost $14/GB!!! So, 1TB of DDR4-3200 will cost north of $14k by Q2 if this is true ü§Ø&lt;/p&gt; &lt;p&gt;In case anyone thought things weren't already bad, it's going to get much much worse this year.&lt;/p&gt; &lt;p&gt;Here's the full Google translate of the article:&lt;/p&gt; &lt;p&gt;DRAM, a type of memory semiconductor, was the key driver behind Samsung Electronics' first-quarter operating profit surpassing 20 trillion won. DRAM products, including high-bandwidth memory (HBM), are a core component of the computing infrastructure supporting the artificial intelligence (AI) era. The semiconductor industry predicts that the DRAM shortage, which began in earnest in the second half of last year, will continue until the end of this year, with prices also expected to continue rising.&lt;/p&gt; &lt;p&gt;Samsung Electronics and SK Hynix, major suppliers of DRAM, are reportedly demanding a 50-60% increase in server DRAM supply prices from the previous quarter during their first-quarter negotiations with customers. A semiconductor industry insider reported, &amp;quot;Even with significantly higher prices, the prevailing sentiment is 'let's buy as much as we can before it gets more expensive.'&amp;quot; Recently, semiconductor purchasing managers from Silicon Valley tech companies, nicknamed &amp;quot;DRAM Beggars,&amp;quot; have been reportedly competing fiercely to secure remaining DRAM inventory at hotels in the Pangyo and Pyeongtaek areas.&lt;/p&gt; &lt;p&gt;The semiconductor industry analyzes that &amp;quot;the demand that was initially focused on HBM in the early days of the AI ‚Äã‚Äãcraze is now spreading to server DRAM, creating an unprecedented semiconductor boom.&amp;quot; DRAM is a semiconductor that manages a computer's &amp;quot;short-term memory.&amp;quot; It stores and quickly transmits necessary data when the central processing unit (CPU), the brain, performs tasks. HBM is specialized for seamlessly delivering the massive data required for AI by increasing the data transmission path (bandwidth) dozens of times compared to conventional DRAM. However, HBM is extremely expensive and has limitations in increasing capacity. This explains why big tech companies are scrambling to secure server DRAM products to store more data.&lt;/p&gt; &lt;p&gt;The average contract price of DRAM soared from $1.40 (based on 8GB DDR4) in January last year to $9.30 in December. This marks the first time in seven years and four months that DRAM prices have surpassed the $9 threshold. Kim Dong-won, head of the research center at KB Securities, said, &amp;quot;Due to this price increase, the operating profit margin (the ratio of operating profit to sales) of some general-purpose memories (widely used standard memories) is expected to reach 70%, and DDR5 may even surpass the margin of HBM3E. This year, semiconductor companies' performance is expected to be determined by general-purpose memories.&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullstackSensei"&gt; /u/FullstackSensei &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.chosun.com/economy/tech_it/2026/01/09/MZNIFPCMTZGHHPV5757NJC5QW4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q84u82/big_tech_companies_now_dram_beggars_are_staying/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q84u82/big_tech_companies_now_dram_beggars_are_staying/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-09T10:28:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1q8xd6f</id>
    <title>Higgs Audio v2 GUI with many features</title>
    <updated>2026-01-10T06:57:37+00:00</updated>
    <author>
      <name>/u/Mar00ned</name>
      <uri>https://old.reddit.com/user/Mar00ned</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been obsessed with Higgs v2 as it's been incredible for my use case. I couldn't find a good GUI so I've been creating one.&lt;/p&gt; &lt;p&gt;While I originally used ComfyUI with TTS-Suite, there were still a few parameters that couldn't be tweaked easily that I needed, which lead to this piece of work.&lt;/p&gt; &lt;p&gt;If you're someone who wants to be able to adjust a lot of the parameters that are available in the Higgs generate.py but from a GUI, hopefully this will work for you.&lt;/p&gt; &lt;p&gt;The only thing it requires is to install Gradio in your python environment, it goes right into your higgs-audio install directory under the &amp;quot;examples&amp;quot; folder, so it should be simple to implement.&lt;/p&gt; &lt;p&gt;Please note, this is my first publishing experience on GitHub and I'm still learning Gradio, so please try to be kind.&lt;/p&gt; &lt;p&gt;If you're interested or have feedback, please check out the repository.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Tenidus/Higgs-Audio-v2-Gradio-Interface"&gt;https://github.com/Tenidus/Higgs-Audio-v2-Gradio-Interface&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mar00ned"&gt; /u/Mar00ned &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8xd6f/higgs_audio_v2_gui_with_many_features/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8xd6f/higgs_audio_v2_gui_with_many_features/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q8xd6f/higgs_audio_v2_gui_with_many_features/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-10T06:57:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1q8uyhj</id>
    <title>I built a 100% local Audio RAG pipeline to index 4-hour city council meetings. Runs on an RTX 2060. (Whisper + Ollama + ChromaDB)</title>
    <updated>2026-01-10T04:48:34+00:00</updated>
    <author>
      <name>/u/alias454</name>
      <uri>https://old.reddit.com/user/alias454</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm a bit of a late-comer with LLMs for personal use. I'm sharing this to document that a lot can be done with limited hardware resources.&lt;/p&gt; &lt;p&gt;I‚Äôve spent 4 weeks building a tool I named YATSEE. It is a local-first pipeline designed to turn unstructured audio (think 4-hour jargon-filled city council meetings) into clean searchable summaries. &lt;/p&gt; &lt;p&gt;The Tech Stack (100% Offline):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Ingestion: yt-dlp for automated retrieval.&lt;/li&gt; &lt;li&gt;Audio Prep: ffmpeg for conversion/chunking (16kHz mono).&lt;/li&gt; &lt;li&gt;Transcription: faster-whisper (or standard OpenAI whisper).&lt;/li&gt; &lt;li&gt;Normalization: spaCy (used for clean up of raw transcripts produce.&lt;/li&gt; &lt;li&gt;Summarization: Ollama (running local LLMs like Llama 3 or Mistral).&lt;/li&gt; &lt;li&gt;RAG/Search: ChromaDB for vector storage + Streamlit for the UI.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Hardware:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Lenovo Legion 5, RTX 2060, 32GB RAM (Fedora Linux)&lt;/li&gt; &lt;li&gt;Base M4 Mac mini, 16GB unified RAM&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This was a fun project to get my feet wet with local LLMs. You can check out the code on github &lt;a href="https://github.com/alias454/YATSEE"&gt;https://github.com/alias454/YATSEE&lt;/a&gt;. &lt;/p&gt; &lt;p&gt;I'm interested in exploring smaller models vs larger ones. Any feedback on that would be great.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/alias454"&gt; /u/alias454 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8uyhj/i_built_a_100_local_audio_rag_pipeline_to_index/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8uyhj/i_built_a_100_local_audio_rag_pipeline_to_index/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q8uyhj/i_built_a_100_local_audio_rag_pipeline_to_index/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-10T04:48:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1q8scwx</id>
    <title>I built an open-source tool to analyze spine MRI scans locally.</title>
    <updated>2026-01-10T02:42:54+00:00</updated>
    <author>
      <name>/u/Erdeem</name>
      <uri>https://old.reddit.com/user/Erdeem</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been working on a project to democratize medical imaging analysis and wanted to share it with the community. &lt;a href="https://github.com/NeoNogin/MRI-GPT"&gt;MRI-GPT&lt;/a&gt; allows you to drag-and-drop spine MRI (DICOM) files and generates a detailed pathology report that you can chat with, running entirely on your local machine.&lt;/p&gt; &lt;p&gt;The biggest challenge with using Vision Language Models for medical imaging has always been localization. General models are smart, but they get lost easily‚Äîoften hallucinating a herniation at L4 because they are actually looking at L3.&lt;/p&gt; &lt;p&gt;I solved this by decoupling the &amp;quot;eyes&amp;quot; (segmentation) from the &amp;quot;brain&amp;quot; (Qwen3).&lt;/p&gt; &lt;p&gt;How it works: 3D Localization (The Eyes): Uses nnU-Net to map every vertebra in 3D space with high precision. This ensures we know exactly where L4, L5, and S1 are before the LLM even gets involved.&lt;/p&gt; &lt;p&gt;Smart Sampling: Calculates the geometric center of each disc to grab the &amp;quot;sweet spot&amp;quot; slice (mid-sagittal). This drastically reduces context window usage and noise.&lt;/p&gt; &lt;p&gt;Vision Analysis (The Brain): Feeds a 3-slice montage to a local Qwen3-VL:8b (via Ollama) with anatomy-specific dynamic prompts.&lt;/p&gt; &lt;p&gt;Chat: You can chat with the report to ask follow-up questions.&lt;/p&gt; &lt;p&gt;Why Qwen3-VL:8b + Segmentation? We chose the newly released Qwen3-VL:8b over previous iterations (like Qwen2.5) because of a critical synergy with our segmentation pipeline:&lt;/p&gt; &lt;p&gt;Solving the &amp;quot;Localization Gap&amp;quot;: Benchmarks (like SpineBench) showed that older models like Qwen2.5-VL had terrible localization accuracy (~12-15%) on their own. They knew what a herniation looked like, but not where it was. By handling localization with TotalSpineSeg, we feed Qwen3 the exact right image slice.&lt;/p&gt; &lt;p&gt;Reduced Hallucination: Qwen3-VL features significantly improved instruction-following capabilities over 2.5. When we prompt it with specific anatomical context (&amp;quot;Analyze the L4-L5 disc space in this crop&amp;quot;), it adheres to that constraint much better, reducing the &amp;quot;negative transfer&amp;quot; where models hallucinate diseases based on general training data rather than the actual pixel data.&lt;/p&gt; &lt;p&gt;Efficiency: The 8b model is lightweight enough to run locally on consumer GPUs but, when focused on a pre-segmented image, rivals the diagnostic accuracy of much larger 70B+ models.&lt;/p&gt; &lt;p&gt;A one click (more like 3 click) installer is available &lt;a href="https://github.com/NeoNogin/MRI-GPT/releases/tag/MRI-GPT_v1.0.0-beta"&gt;here.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I made this for my personal use. I am not a medical doctor. It is far from perfect and has gone through VERY limited number of tests, however, it was over 90% accurate with edge cases throwing it off (prior surgeries that let to hardware being installed) and it can be a little over sensitive where it would for example label a mild issue as a moderate one. I have not tested for fractures. I have not tested the thoracic spin due to limited availability of that dataset (apparently its not common to get thoracic spine MRI). For those reasons and more I added the option to include context with your images- which can be anything from &amp;quot;I slept funny&amp;quot;, to an entire MRI report from your doctor. The context will improve accuracy. &lt;/p&gt; &lt;p&gt;Future plans are to include support MRIs of the entire body. &lt;/p&gt; &lt;p&gt;Let me know if you have any questions or requests.&lt;/p&gt; &lt;p&gt;THIS SOFTWARE IS FOR RESEARCH AND EDUCATIONAL PURPOSES ONLY. NOT FOR CLINICAL DIAGNOSIS.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Erdeem"&gt; /u/Erdeem &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8scwx/i_built_an_opensource_tool_to_analyze_spine_mri/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8scwx/i_built_an_opensource_tool_to_analyze_spine_mri/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q8scwx/i_built_an_opensource_tool_to_analyze_spine_mri/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-10T02:42:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1q8lt9c</id>
    <title>PSA: HF seems to be removing grandfathered limits on private storage and billing people on it.</title>
    <updated>2026-01-09T22:03:39+00:00</updated>
    <author>
      <name>/u/FullOf_Bad_Ideas</name>
      <uri>https://old.reddit.com/user/FullOf_Bad_Ideas</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;HF is twisting the screw on their storage billing. I believe than when they announced changes, they grandfathered in storage limits for people who were over a 1 TB limit. I got 1.34TB limit.&lt;/p&gt; &lt;p&gt;Well, now this is over and I got billed additional $25 for keeping my files as is - anything over the first 1TB is counted as another 1TB bought, at $25/TB rate. I uploaded just around 20GB since November 30th, and I wasn't billed for that 1.34TB earlier.&lt;/p&gt; &lt;p&gt;Watch out for surprise bills!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullOf_Bad_Ideas"&gt; /u/FullOf_Bad_Ideas &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8lt9c/psa_hf_seems_to_be_removing_grandfathered_limits/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8lt9c/psa_hf_seems_to_be_removing_grandfathered_limits/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q8lt9c/psa_hf_seems_to_be_removing_grandfathered_limits/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-09T22:03:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1q8fagh</id>
    <title>RTX Blackwell Pro 6000 wholesale pricing has dropped by $150-200</title>
    <updated>2026-01-09T17:57:11+00:00</updated>
    <author>
      <name>/u/TastesLikeOwlbear</name>
      <uri>https://old.reddit.com/user/TastesLikeOwlbear</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Obviously the RTX Blackwell Pro 6000 cards are of great interest to the people here. I see them come up a lot. And we all ooh and ahh over the people that have 8 of them lined up in a nice row.&lt;/p&gt; &lt;p&gt;It also seems to me like the market is suffering from lack of transparency on these.&lt;/p&gt; &lt;p&gt;My employer buys these cards wholesale, and I can see current pricing and stock in our distributors' systems. (And I &lt;strong&gt;may have&lt;/strong&gt; slipped in an order for one for myself...) It's eye-opening.&lt;/p&gt; &lt;p&gt;I'm probably not supposed to disclose the exact price we buy these at. But I wanted people to know that unlike everything else with RAM in it, the wholesale price of these has &lt;strong&gt;dropped&lt;/strong&gt; by about ~$150-200 from December to January.&lt;/p&gt; &lt;p&gt;I will also say that the wholesale price for the 6000 Pro is only about $600 higher than the wholesale price for the new 72GiB 5000 Pro. So, for the love of god, please don't buy that!&lt;/p&gt; &lt;p&gt;(And no, this is &lt;strong&gt;not&lt;/strong&gt; marketing or an ad; I &lt;strong&gt;cannot&lt;/strong&gt; sell &lt;strong&gt;anyone&lt;/strong&gt; these cards at &lt;strong&gt;any&lt;/strong&gt; price. I would be fired immediately. I just want people to have the best available information when they're looking to buy something this expensive.)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TastesLikeOwlbear"&gt; /u/TastesLikeOwlbear &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8fagh/rtx_blackwell_pro_6000_wholesale_pricing_has/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8fagh/rtx_blackwell_pro_6000_wholesale_pricing_has/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q8fagh/rtx_blackwell_pro_6000_wholesale_pricing_has/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-09T17:57:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1q88hdc</id>
    <title>(The Information): DeepSeek To Release Next Flagship AI Model With Strong Coding Ability</title>
    <updated>2026-01-09T13:39:02+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q88hdc/the_information_deepseek_to_release_next_flagship/"&gt; &lt;img alt="(The Information): DeepSeek To Release Next Flagship AI Model With Strong Coding Ability" src="https://a.thumbs.redditmedia.com/V0cq5JgXRMYN60IWmZOgeWuSilWb4Gxub72PzqtA_08.jpg" title="(The Information): DeepSeek To Release Next Flagship AI Model With Strong Coding Ability" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;(paywall): &lt;a href="https://www.theinformation.com/articles/deepseek-release-next-flagship-ai-model-strong-coding-ability"&gt;https://www.theinformation.com/articles/deepseek-release-next-flagship-ai-model-strong-coding-ability&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1q88hdc"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q88hdc/the_information_deepseek_to_release_next_flagship/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q88hdc/the_information_deepseek_to_release_next_flagship/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-09T13:39:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1q89g1i</id>
    <title>DeepSeek V4 Coming</title>
    <updated>2026-01-09T14:18:56+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;According to two people with direct knowledge, DeepSeek is expected to roll out a next‚Äëgeneration flagship AI model in the coming weeks that focuses on strong code‚Äëgeneration capabilities.&lt;/p&gt; &lt;p&gt;The two sources said the model, codenamed V4, is an iteration of the V3 model DeepSeek released in December 2024. Preliminary internal benchmark tests conducted by DeepSeek employees indicate the model outperforms existing mainstream models in code generation, including Anthropic‚Äôs Claude and the OpenAI GPT family.&lt;/p&gt; &lt;p&gt;The sources said the V4 model achieves a technical breakthrough in handling and parsing very long code prompts, a significant practical advantage for engineers working on complex software projects. They also said the model‚Äôs ability to understand data patterns across the full training pipeline has been improved and that no degradation in performance has been observed.&lt;/p&gt; &lt;p&gt;One of the insiders said users may find that V4‚Äôs outputs are more logically rigorous and clear, a trait that indicates the model has stronger reasoning ability and will be much more reliable when performing complex tasks.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.theinformation.com/articles/deepseek-release-next-flagship-ai-model-strong-coding-ability"&gt;https://www.theinformation.com/articles/deepseek-release-next-flagship-ai-model-strong-coding-ability&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q89g1i/deepseek_v4_coming/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q89g1i/deepseek_v4_coming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q89g1i/deepseek_v4_coming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-09T14:18:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1q8tdcz</id>
    <title>Introducing "UITPSDT" a novel approach to runtime efficiency in organic agents</title>
    <updated>2026-01-10T03:30:19+00:00</updated>
    <author>
      <name>/u/reto-wyss</name>
      <uri>https://old.reddit.com/user/reto-wyss</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8tdcz/introducing_uitpsdt_a_novel_approach_to_runtime/"&gt; &lt;img alt="Introducing &amp;quot;UITPSDT&amp;quot; a novel approach to runtime efficiency in organic agents" src="https://preview.redd.it/73wv4f66yfcg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=87519585cbcaf77c9e5c007fc061d1e497362a78" title="Introducing &amp;quot;UITPSDT&amp;quot; a novel approach to runtime efficiency in organic agents" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It is a proof of concept and application outside of the proposed domain may yield unexpected results, we hope the community can contribute to the token efficiency.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/reto-wyss"&gt; /u/reto-wyss &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/73wv4f66yfcg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8tdcz/introducing_uitpsdt_a_novel_approach_to_runtime/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q8tdcz/introducing_uitpsdt_a_novel_approach_to_runtime/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-10T03:30:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1q8wv24</id>
    <title>GLM 5 Is Being Trained!</title>
    <updated>2026-01-10T06:28:58+00:00</updated>
    <author>
      <name>/u/Few_Painter_5588</name>
      <uri>https://old.reddit.com/user/Few_Painter_5588</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8wv24/glm_5_is_being_trained/"&gt; &lt;img alt="GLM 5 Is Being Trained!" src="https://b.thumbs.redditmedia.com/moDsJ3Fi93wAh8NCr_tIBITFa49S-yRZWrZUjXB4v-A.jpg" title="GLM 5 Is Being Trained!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/lc29bfu0ugcg1.png?width=696&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=881458479675304548c6a39e72ed0a8b90f5b54a"&gt;https://preview.redd.it/lc29bfu0ugcg1.png?width=696&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=881458479675304548c6a39e72ed0a8b90f5b54a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Announced after their IPO&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Few_Painter_5588"&gt; /u/Few_Painter_5588 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8wv24/glm_5_is_being_trained/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8wv24/glm_5_is_being_trained/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q8wv24/glm_5_is_being_trained/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-10T06:28:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1q8hqgd</id>
    <title>I clustered 3 DGX Sparks that NVIDIA said couldn't be clustered yet...took 1500 lines of C to make it work</title>
    <updated>2026-01-09T19:27:29+00:00</updated>
    <author>
      <name>/u/Ok-Pomegranate1314</name>
      <uri>https://old.reddit.com/user/Ok-Pomegranate1314</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8hqgd/i_clustered_3_dgx_sparks_that_nvidia_said_couldnt/"&gt; &lt;img alt="I clustered 3 DGX Sparks that NVIDIA said couldn't be clustered yet...took 1500 lines of C to make it work" src="https://preview.redd.it/dban4j25kdcg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b4a2061b8c9510f486ad4475d7a5f9b8d3a666f7" title="I clustered 3 DGX Sparks that NVIDIA said couldn't be clustered yet...took 1500 lines of C to make it work" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;NVIDIA officially supports clustering &lt;em&gt;two&lt;/em&gt; DGX Sparks together. I wanted three.&lt;/p&gt; &lt;p&gt;The problem: each Spark has two 100Gbps ConnectX-7 ports. In a 3-node triangle mesh, each link ends up on a different subnet. NCCL's built-in networking assumes all peers are reachable from a single NIC. It just... doesn't work.&lt;/p&gt; &lt;p&gt;So I wrote a custom NCCL network plugin from scratch.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it does:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Subnet-aware NIC selection (picks the right NIC for each peer)&lt;/li&gt; &lt;li&gt;Raw RDMA verbs implementation (QP state machines, memory registration, completion queues)&lt;/li&gt; &lt;li&gt;Custom TCP handshake protocol to avoid deadlocks&lt;/li&gt; &lt;li&gt;~1500 lines of C&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The result:&lt;/strong&gt; Distributed inference across all 3 nodes at 8+ GB/s over RDMA. &lt;strong&gt;The NVIDIA support tier I'm currently on:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;‚îú‚îÄ‚îÄ Supported configs ‚úì ‚îú‚îÄ‚îÄ &amp;quot;Should work&amp;quot; configs ‚îú‚îÄ‚îÄ &amp;quot;You're on your own&amp;quot; configs ‚îú‚îÄ‚îÄ &amp;quot;Please don't call us&amp;quot; configs ‚îú‚îÄ‚îÄ &amp;quot;How did you even...&amp;quot; configs ‚îî‚îÄ‚îÄ You are here ‚Üí &amp;quot;Writing custom NCCL plugins to cluster standalone workstations over a hand-wired RDMA mesh&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;GitHub link: &lt;a href="https://github.com/autoscriptlabs/nccl-mesh-plugin"&gt;https://github.com/autoscriptlabs/nccl-mesh-plugin&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer questions about the implementation. This was a mass of low-level debugging (segfaults, RDMA state machine issues, GID table problems) but it works.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Pomegranate1314"&gt; /u/Ok-Pomegranate1314 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/dban4j25kdcg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8hqgd/i_clustered_3_dgx_sparks_that_nvidia_said_couldnt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q8hqgd/i_clustered_3_dgx_sparks_that_nvidia_said_couldnt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-09T19:27:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1q8ckz0</id>
    <title>The reason why RAM has become so expensive</title>
    <updated>2026-01-09T16:18:22+00:00</updated>
    <author>
      <name>/u/InvadersMustLive</name>
      <uri>https://old.reddit.com/user/InvadersMustLive</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8ckz0/the_reason_why_ram_has_become_so_expensive/"&gt; &lt;img alt="The reason why RAM has become so expensive" src="https://preview.redd.it/sgbhubsomccg1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=57d847c1b9a2d5786b0a888b5d0d25fe5ede9e12" title="The reason why RAM has become so expensive" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InvadersMustLive"&gt; /u/InvadersMustLive &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sgbhubsomccg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8ckz0/the_reason_why_ram_has_become_so_expensive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q8ckz0/the_reason_why_ram_has_become_so_expensive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-09T16:18:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
