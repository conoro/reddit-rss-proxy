<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-08T07:26:16+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1mkfahe</id>
    <title>LiveBench now has GPT OSS 120b, and it's below ChatGPT-4o.</title>
    <updated>2025-08-07T23:18:21+00:00</updated>
    <author>
      <name>/u/chibop1</name>
      <uri>https://old.reddit.com/user/chibop1</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chibop1"&gt; /u/chibop1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://livebench.ai"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkfahe/livebench_now_has_gpt_oss_120b_and_its_below/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mkfahe/livebench_now_has_gpt_oss_120b_and_its_below/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-07T23:18:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1mkk6o2</id>
    <title>Qwen-Image quantization and GPU parallelization code</title>
    <updated>2025-08-08T03:08:02+00:00</updated>
    <author>
      <name>/u/Ok_Helicopter_2294</name>
      <uri>https://old.reddit.com/user/Ok_Helicopter_2294</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve uploaded the code for Qwen-Image quantization and GPU parallelization on GitHub.&lt;/p&gt; &lt;p&gt;Since I‚Äôm working full-time as an office worker, I wrote it roughly for now ‚Äî but feel free to take a look, and let me know if you have any questions or suggestions!&lt;/p&gt; &lt;p&gt;The environment I used to run this code includes:&lt;br /&gt; 2√ó RTX 3090 GPUs,&lt;br /&gt; a Ryzen 7 7700 CPU,&lt;br /&gt; 128GB of DDR5 RAM,&lt;br /&gt; and a WSL (Windows Subsystem for Linux) setup.&lt;/p&gt; &lt;p&gt;github :&lt;br /&gt; &lt;a href="https://github.com/zc142365/qwen-image-diffusers-patch"&gt;https://github.com/zc142365/qwen-image-diffusers-patch&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Helicopter_2294"&gt; /u/Ok_Helicopter_2294 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkk6o2/qwenimage_quantization_and_gpu_parallelization/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkk6o2/qwenimage_quantization_and_gpu_parallelization/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mkk6o2/qwenimage_quantization_and_gpu_parallelization/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-08T03:08:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1mk95w6</id>
    <title>Qwen3-8b-2508 anyone? ü§ûü§ûü§û Where are you? Are you coming?</title>
    <updated>2025-08-07T19:14:22+00:00</updated>
    <author>
      <name>/u/JLeonsarmiento</name>
      <uri>https://old.reddit.com/user/JLeonsarmiento</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;that's it. Big fan of smaller yet ultra performant LLMs.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JLeonsarmiento"&gt; /u/JLeonsarmiento &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mk95w6/qwen38b2508_anyone_where_are_you_are_you_coming/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mk95w6/qwen38b2508_anyone_where_are_you_are_you_coming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mk95w6/qwen38b2508_anyone_where_are_you_are_you_coming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-07T19:14:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjju67</id>
    <title>No, no, no, wait - on a second thought, I KNOW the answer!</title>
    <updated>2025-08-06T23:11:24+00:00</updated>
    <author>
      <name>/u/Final_Wheel_7486</name>
      <uri>https://old.reddit.com/user/Final_Wheel_7486</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjju67/no_no_no_wait_on_a_second_thought_i_know_the/"&gt; &lt;img alt="No, no, no, wait - on a second thought, I KNOW the answer!" src="https://preview.redd.it/zs8aeebxdhhf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fb8196976261024587d9462ed2ceb999cbda98af" title="No, no, no, wait - on a second thought, I KNOW the answer!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Yes, I know my prompt itself is flawed - let me clarify that I don't side with any country in this regard and just wanted to test for the extent of &amp;quot;SAFETY!!1&amp;quot; in OpenAI's new model. I stumbled across this funny reaction here.&lt;/p&gt; &lt;p&gt;Model: GPT-OSS 120b (High reasoning mode), default system prompt, no further context on the official GPT-OSS website.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Final_Wheel_7486"&gt; /u/Final_Wheel_7486 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zs8aeebxdhhf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjju67/no_no_no_wait_on_a_second_thought_i_know_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjju67/no_no_no_wait_on_a_second_thought_i_know_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T23:11:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1mkaxrx</id>
    <title>On the topic of graphs</title>
    <updated>2025-08-07T20:22:12+00:00</updated>
    <author>
      <name>/u/onil_gova</name>
      <uri>https://old.reddit.com/user/onil_gova</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkaxrx/on_the_topic_of_graphs/"&gt; &lt;img alt="On the topic of graphs" src="https://external-preview.redd.it/djdqd2lmNHZvbmhmMT9yaG9NVPSK_ESe4YNSeWTlgNsDK6lCMTdSd23R-vXk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f8651ed9c62cd9f04f57ff3ed7c3f0971fcdc97f" title="On the topic of graphs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/onil_gova"&gt; /u/onil_gova &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/kdhwce4vonhf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkaxrx/on_the_topic_of_graphs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mkaxrx/on_the_topic_of_graphs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-07T20:22:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1mk92k4</id>
    <title>gabriellarson/Huihui-gpt-oss-20b-BF16-abliterated-GGUF ¬∑ Hugging Face</title>
    <updated>2025-08-07T19:10:48+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mk92k4/gabriellarsonhuihuigptoss20bbf16abliteratedgguf/"&gt; &lt;img alt="gabriellarson/Huihui-gpt-oss-20b-BF16-abliterated-GGUF ¬∑ Hugging Face" src="https://external-preview.redd.it/dM2syJ0lh5qODrveCus4LDlR8L4f9r_ltO_PMWMUbDA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d8ef5dae58a1931f159f19948400500dc5e8110f" title="gabriellarson/Huihui-gpt-oss-20b-BF16-abliterated-GGUF ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/gabriellarson/Huihui-gpt-oss-20b-BF16-abliterated-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mk92k4/gabriellarsonhuihuigptoss20bbf16abliteratedgguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mk92k4/gabriellarsonhuihuigptoss20bbf16abliteratedgguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-07T19:10:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1mk7r1g</id>
    <title>Trained an 41M HRM-Based Model to generate semi-coherent text!</title>
    <updated>2025-08-07T18:20:52+00:00</updated>
    <author>
      <name>/u/random-tomato</name>
      <uri>https://old.reddit.com/user/random-tomato</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mk7r1g/trained_an_41m_hrmbased_model_to_generate/"&gt; &lt;img alt="Trained an 41M HRM-Based Model to generate semi-coherent text!" src="https://b.thumbs.redditmedia.com/5IXZKHsgxD2_snxB5qYDZsSXRsrSDWyvbqoNOIrkjvM.jpg" title="Trained an 41M HRM-Based Model to generate semi-coherent text!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/random-tomato"&gt; /u/random-tomato &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mk7r1g"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mk7r1g/trained_an_41m_hrmbased_model_to_generate/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mk7r1g/trained_an_41m_hrmbased_model_to_generate/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-07T18:20:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1mkdu9r</id>
    <title>GPT-5 results on EQ-Bench + Opus 4.1 takes top spot on longform writing</title>
    <updated>2025-08-07T22:16:44+00:00</updated>
    <author>
      <name>/u/_sqrkl</name>
      <uri>https://old.reddit.com/user/_sqrkl</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkdu9r/gpt5_results_on_eqbench_opus_41_takes_top_spot_on/"&gt; &lt;img alt="GPT-5 results on EQ-Bench + Opus 4.1 takes top spot on longform writing" src="https://b.thumbs.redditmedia.com/jRjd_Zh5thfqHHyXwOn68BMwlkIkCBQx3w8W6QpsShA.jpg" title="GPT-5 results on EQ-Bench + Opus 4.1 takes top spot on longform writing" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://eqbench.com/creative_writing_longform.html"&gt;https://eqbench.com/creative_writing_longform.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Performance for gpt-5 is very similar to horizon-alpha &amp;amp; horizon-beta, those being earlier checkpoints.&lt;/p&gt; &lt;p&gt;Gpt-5-chat-latest (the chat-tuned version that you get on chatgpt.com) performs a little differently, scoring lower than gpt-5 and writing much less verbosely. Less than half the length of gpt-5 outputs on average.&lt;/p&gt; &lt;p&gt;Longform writing update: I added new instructions to help the judge notice &amp;amp; punish overuse of incoherent metaphors, &amp;amp; re-ran the leaderboard. It was becoming a problem with many frontier models converging on this slop.&lt;/p&gt; &lt;p&gt;Some rank changes; now &lt;strong&gt;Opus 4.1 is #1&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;### Samples&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Creative writing:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://eqbench.com/results/creative-writing-v3/gpt-5-2025-08-07.html"&gt;https://eqbench.com/results/creative-writing-v3/gpt-5-2025-08-07.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Longform writing:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://eqbench.com/results/creative-writing-longform/claude-opus-4.1_longform_report.html"&gt;https://eqbench.com/results/creative-writing-longform/claude-opus-4.1_longform_report.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://eqbench.com/results/creative-writing-longform/gpt-5-2025-08-07_longform_report.html"&gt;https://eqbench.com/results/creative-writing-longform/gpt-5-2025-08-07_longform_report.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://eqbench.com/results/creative-writing-longform/gpt-5-chat-latest_longform_report.html"&gt;https://eqbench.com/results/creative-writing-longform/gpt-5-chat-latest_longform_report.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://eqbench.com/results/creative-writing-longform/gpt-5-mini-2025-08-07_longform_report.html"&gt;https://eqbench.com/results/creative-writing-longform/gpt-5-mini-2025-08-07_longform_report.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://eqbench.com/results/creative-writing-longform/gpt-5-nano-2025-08-07_longform_report.html"&gt;https://eqbench.com/results/creative-writing-longform/gpt-5-nano-2025-08-07_longform_report.html&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_sqrkl"&gt; /u/_sqrkl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mkdu9r"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkdu9r/gpt5_results_on_eqbench_opus_41_takes_top_spot_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mkdu9r/gpt5_results_on_eqbench_opus_41_takes_top_spot_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-07T22:16:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1mk4kt0</id>
    <title>Be careful in selecting providers on openrouter</title>
    <updated>2025-08-07T16:22:25+00:00</updated>
    <author>
      <name>/u/Charuru</name>
      <uri>https://old.reddit.com/user/Charuru</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mk4kt0/be_careful_in_selecting_providers_on_openrouter/"&gt; &lt;img alt="Be careful in selecting providers on openrouter" src="https://preview.redd.it/o9dqe3l9imhf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=63498a33a88373227cb3e4dd804ff112b545e323" title="Be careful in selecting providers on openrouter" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charuru"&gt; /u/Charuru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/o9dqe3l9imhf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mk4kt0/be_careful_in_selecting_providers_on_openrouter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mk4kt0/be_careful_in_selecting_providers_on_openrouter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-07T16:22:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1mknjzx</id>
    <title>[Showoff] I made an AI that understands where things are, not just what they are ‚Äì live demo on Hugging Face üöÄ</title>
    <updated>2025-08-08T06:11:55+00:00</updated>
    <author>
      <name>/u/scheitelpunk1337</name>
      <uri>https://old.reddit.com/user/scheitelpunk1337</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;You know how most LLMs can tell you what a &amp;quot;keyboard&amp;quot; is, but if you ask &lt;em&gt;&amp;quot;where‚Äôs the keyboard relative to the monitor?&amp;quot;&lt;/em&gt; you get‚Ä¶ ü§∑?&lt;br /&gt; That‚Äôs the &lt;strong&gt;Spatial Intelligence Gap&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;I‚Äôve been working for months on &lt;strong&gt;GASM&lt;/strong&gt; (Geometric Attention for Spatial &amp;amp; Mathematical Understanding) ‚Äî and yesterday I finally ran the example that‚Äôs been stuck in my head:&lt;/p&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;strong&gt;Raw output:&lt;/strong&gt;&lt;br /&gt; üìç Sensor: &lt;code&gt;(-1.25, -0.68, -1.27)&lt;/code&gt; m&lt;br /&gt; üìç Conveyor: &lt;code&gt;(-0.76, -1.17, -0.78)&lt;/code&gt; m&lt;br /&gt; üìê 45¬∞ angle: Extracted &amp;amp; encoded ‚úì&lt;br /&gt; üîó Spatial relationships: 84.7% confidence ‚úì&lt;/p&gt; &lt;p&gt;No simulation. No smoke. Just &lt;strong&gt;plain English ‚Üí 3D coordinates&lt;/strong&gt;, all CPU.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why it‚Äôs cool:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;First &lt;em&gt;public&lt;/em&gt; SE(3)-invariant AI for natural language ‚Üí geometry&lt;/li&gt; &lt;li&gt;Works for robotics, AR/VR, engineering, scientific modeling&lt;/li&gt; &lt;li&gt;Optimized for curvature calculations so it runs on CPU (because I like the planet)&lt;/li&gt; &lt;li&gt;Mathematically correct spatial relationships under rotations/translations&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Live demo here:&lt;/strong&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/spaces/scheitelpunk/GASM"&gt;huggingface.co/spaces/scheitelpunk/GASM&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Drop &lt;em&gt;any&lt;/em&gt; spatial description in the comments (&amp;quot;put the box between the two red chairs next to the window&amp;quot;) ‚Äî I‚Äôll run it and post the raw coordinates + visualization.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/scheitelpunk1337"&gt; /u/scheitelpunk1337 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mknjzx/showoff_i_made_an_ai_that_understands_where/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mknjzx/showoff_i_made_an_ai_that_understands_where/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mknjzx/showoff_i_made_an_ai_that_understands_where/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-08T06:11:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjub4z</id>
    <title>llama.cpp HQ</title>
    <updated>2025-08-07T08:14:09+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjub4z/llamacpp_hq/"&gt; &lt;img alt="llama.cpp HQ" src="https://preview.redd.it/d15gp2d33khf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=356bf4bfc9f7c3e2c9fc089431a35c0a3300f0d2" title="llama.cpp HQ" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/d15gp2d33khf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjub4z/llamacpp_hq/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjub4z/llamacpp_hq/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-07T08:14:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1mk5n89</id>
    <title>HuggingFace has been on a deletion spree and has already removed 16TB worth of files. dets in screenshots slide</title>
    <updated>2025-08-07T17:02:37+00:00</updated>
    <author>
      <name>/u/Tango-Down766</name>
      <uri>https://old.reddit.com/user/Tango-Down766</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mk5n89/huggingface_has_been_on_a_deletion_spree_and_has/"&gt; &lt;img alt="HuggingFace has been on a deletion spree and has already removed 16TB worth of files. dets in screenshots slide" src="https://b.thumbs.redditmedia.com/-NXaX5EHmxxb7GBcWRIp5vrkgUBJaiSRrpm9inzqlEM.jpg" title="HuggingFace has been on a deletion spree and has already removed 16TB worth of files. dets in screenshots slide" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://civitaiarchive.com/"&gt;https://civitaiarchive.com/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tango-Down766"&gt; /u/Tango-Down766 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mk5n89"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mk5n89/huggingface_has_been_on_a_deletion_spree_and_has/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mk5n89/huggingface_has_been_on_a_deletion_spree_and_has/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-07T17:02:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1mk74wq</id>
    <title>Can we focus more on LOCAL models? We were excited for Qwen3 30b/3b but now were comparing to not local models</title>
    <updated>2025-08-07T17:58:02+00:00</updated>
    <author>
      <name>/u/agentcubed</name>
      <uri>https://old.reddit.com/user/agentcubed</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mk74wq/can_we_focus_more_on_local_models_we_were_excited/"&gt; &lt;img alt="Can we focus more on LOCAL models? We were excited for Qwen3 30b/3b but now were comparing to not local models" src="https://b.thumbs.redditmedia.com/TJcHxiajHwTxrtNoLptpQfkKcoLceehEHpbxyUdzAnI.jpg" title="Can we focus more on LOCAL models? We were excited for Qwen3 30b/3b but now were comparing to not local models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;People need to stop expecting a 5b model to outperform 30b models. Like do they think OpenAI is god?&lt;/p&gt; &lt;p&gt;R1 670b, &lt;strong&gt;37b activ&lt;/strong&gt;e&lt;br /&gt; Kimi K2 1t, &lt;strong&gt;32b active&lt;/strong&gt;&lt;br /&gt; Qwen3 235b, &lt;strong&gt;22b active&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;-- limit of a single average gpu --&lt;/em&gt;&lt;br /&gt; GLM 4.5 Air 106b, &lt;strong&gt;12b active&lt;/strong&gt; (very pushing it but fine)&lt;br /&gt; Qwen3 14b&lt;br /&gt; oss 120b, &lt;strong&gt;5b active&lt;/strong&gt;&lt;br /&gt; Qwen3 30b, &lt;strong&gt;3b active&lt;/strong&gt;&lt;br /&gt; oss 20b, &lt;strong&gt;3b active&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I would rather have a model that I can actually run locally than a good model that needs providers. To be clear, I hate and won't use gpt-oss, but because it's censored and not because models many times larger are better.&lt;/p&gt; &lt;p&gt;I LOVED Qwen3 30b/3b was local-friendly and fast and nobody compared it to bigger models, but when OpenAI releases a local model and suddenly everyone is comparing it to non-local models.&lt;/p&gt; &lt;p&gt;It's an expected model for it's size. It's not beating models 4x larger, but it's not garbage compared to similar sizes.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/aj58dutqzmhf1.png?width=1435&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=47a39c27091e2beb9233c783989cf7305269027e"&gt;Graph of all local-friendly models (GLM Air would be tough)&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/agentcubed"&gt; /u/agentcubed &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mk74wq/can_we_focus_more_on_local_models_we_were_excited/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mk74wq/can_we_focus_more_on_local_models_we_were_excited/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mk74wq/can_we_focus_more_on_local_models_we_were_excited/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-07T17:58:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1mk3rj1</id>
    <title>Jeff Geerling does what Jeff Geerling does best: Quad Strix Halo cluster using Framework Desktop</title>
    <updated>2025-08-07T15:52:01+00:00</updated>
    <author>
      <name>/u/FullstackSensei</name>
      <uri>https://old.reddit.com/user/FullstackSensei</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mk3rj1/jeff_geerling_does_what_jeff_geerling_does_best/"&gt; &lt;img alt="Jeff Geerling does what Jeff Geerling does best: Quad Strix Halo cluster using Framework Desktop" src="https://external-preview.redd.it/igoznQW2BgxL6-V1AGb7GkvH_UJSMnHqmBqwd9fNVKM.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9f53b202537c26df370b20f3e2c66f92c5b25828" title="Jeff Geerling does what Jeff Geerling does best: Quad Strix Halo cluster using Framework Desktop" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;While the setup looks √ºber cool, the software is still not ready to make good use of the hardware.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullstackSensei"&gt; /u/FullstackSensei &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/N5xhOqlvRh4"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mk3rj1/jeff_geerling_does_what_jeff_geerling_does_best/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mk3rj1/jeff_geerling_does_what_jeff_geerling_does_best/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-07T15:52:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1mkg7m7</id>
    <title>GPT-5 removed logprob support from the API - technical breakdown and implications</title>
    <updated>2025-08-07T23:59:34+00:00</updated>
    <author>
      <name>/u/davernow</name>
      <uri>https://old.reddit.com/user/davernow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GPT 4.1/4o and other models always supported logprobs via the API, but with GPT-5 that capability seems to be gone! Try it yourself and you'll get the error &lt;code&gt;You are not allowed to request logprobs from this model&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What are logprobs?&lt;/strong&gt; Logprobs expose the probability distribution for each generated token. For the example ‚ÄúThe dog chased the‚Äù, the next token might be ‚Äúcat‚Äù (70%), ‚Äúball‚Äù (25%), or ‚Äúsquirrel‚Äù (5%). All LLMs have this capability internally, but closed providers like OpenAI and Anthropic sometimes choose to hide these from API users.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why did OpenAI remove them?&lt;/strong&gt; Most likely to prevent model distillation. There's rumours other labs train on the outputs of OpenAI models (Anthropic even cut off OpenAI's API key, they don't seem to trust each other). While you can distill without logprobs, having access to the full probability distribution significantly improves distillation quality and training efficiency. I‚Äôd guess this is a move to prevent competitors from training on their GPT-5 model outputs.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Technical impact: Evals and G-Eval&lt;/strong&gt; The biggest loss (for me at least) is for evaluation workflows. G-Eval (Liu et al.) uses logprobs to weight judge outputs based on model confidence. Instead of binary pass/fail, you get calibrated scores. Consider a eval where the model is uncertain: 51% chance of pass and 49% chance of failure:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Classic LLM-judge: 51% confident ‚Üí &amp;quot;pass&amp;quot; (binary)&lt;/li&gt; &lt;li&gt;G-Eval: 51% pass, 49% fail ‚Üí 0.51 score (calibrated)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In the G-Eval paper consistently outperforms other eval techniques, and logprobs are required.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How we detected this&lt;/strong&gt; I build &lt;a href="https://github.com/Kiln-AI/Kiln"&gt;Kiln&lt;/a&gt; - and open and free tool for evals, synthetic data gen, and fine-tuning. We run automated capability tests on every model before adding them. This makes it much easier to select the right model for a given task. Our logprobs/evals tests immediately caught this change. As far as I'm aware, this wasn't mentioned in any release notes (but I might have missed it).&lt;/p&gt; &lt;p&gt;Here are details on the testing we run on every model to catch issues like this: &lt;a href="https://getkiln.ai/blog/i_wrote_2000_llm_test_cases_so_you_dont_have_to"&gt;https://getkiln.ai/blog/i_wrote_2000_llm_test_cases_so_you_dont_have_to&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And here's our full model library with the results: &lt;a href="https://getkiln.ai/model_library"&gt;https://getkiln.ai/model_library&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/davernow"&gt; /u/davernow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkg7m7/gpt5_removed_logprob_support_from_the_api/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkg7m7/gpt5_removed_logprob_support_from_the_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mkg7m7/gpt5_removed_logprob_support_from_the_api/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-07T23:59:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mkon92</id>
    <title>Half of the models in the top 10 on Design Arena are OW/OS, and they're all from China</title>
    <updated>2025-08-08T07:18:22+00:00</updated>
    <author>
      <name>/u/Accomplished-Copy332</name>
      <uri>https://old.reddit.com/user/Accomplished-Copy332</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkon92/half_of_the_models_in_the_top_10_on_design_arena/"&gt; &lt;img alt="Half of the models in the top 10 on Design Arena are OW/OS, and they're all from China" src="https://preview.redd.it/u7fdqw6zwqhf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3114187ca53c4b97153190460034166fc59eaccc" title="Half of the models in the top 10 on Design Arena are OW/OS, and they're all from China" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Since I started &lt;a href="https://www.designarena.ai/"&gt;my benchmark&lt;/a&gt; just about a month and a half ago, it has been interesting to see just how well the open weight / open source models are competing with their proprietary counterparts when evaluated on how user comparisons of different generations from each model. &lt;/p&gt; &lt;p&gt;Based on the benchmark, Qwen3 Coder, DeepSeek R1-0528, DeepSeek V3-2024, Qwen3 Instruct 2507, and GLM 4.5 could all be considered to be SOTA. I do think this ranking will change slightly though with one of the OS models being pushed out for GPT-5 (which was recently added, so sample size is too small). &lt;/p&gt; &lt;p&gt;That said, it really feels like we're in a golden age of open source models right now. We're also see a good amount of stagnation right now in the improvements being made by the proprietary models. Do you think OS will continue to keep pace? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Accomplished-Copy332"&gt; /u/Accomplished-Copy332 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/u7fdqw6zwqhf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkon92/half_of_the_models_in_the_top_10_on_design_arena/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mkon92/half_of_the_models_in_the_top_10_on_design_arena/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-08T07:18:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1mk26rk</id>
    <title>Llama.cpp now supports GLM 4.5 Air</title>
    <updated>2025-08-07T14:52:12+00:00</updated>
    <author>
      <name>/u/Freonr2</name>
      <uri>https://old.reddit.com/user/Freonr2</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mk26rk/llamacpp_now_supports_glm_45_air/"&gt; &lt;img alt="Llama.cpp now supports GLM 4.5 Air" src="https://b.thumbs.redditmedia.com/jawkehNzIT0a-enbiD4fQc_KPJ-dSoMI8t5allPhBfU.jpg" title="Llama.cpp now supports GLM 4.5 Air" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/14939"&gt;https://github.com/ggml-org/llama.cpp/pull/14939&lt;/a&gt;&lt;/p&gt; &lt;p&gt;from our hero sammcj&lt;/p&gt; &lt;p&gt;Pictured, Cuda v1.45 engine in LM Studio. (the cuda 12 1.44 runtime still not working--the GLM 4.5 PR was merged in the past 8 hours or so).&lt;/p&gt; &lt;p&gt;As an aside, my initial vibe is it is far too wordy and overthinks, though, and gpt oss 120b is better and also faster in pure t/s but that's very much early vibe so take with a heavy dose of salt.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Freonr2"&gt; /u/Freonr2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mk26rk"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mk26rk/llamacpp_now_supports_glm_45_air/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mk26rk/llamacpp_now_supports_glm_45_air/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-07T14:52:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjxx6j</id>
    <title>GPT-OSS is Another Example Why Companies Must Build a Strong Brand Name</title>
    <updated>2025-08-07T11:49:08+00:00</updated>
    <author>
      <name>/u/Iory1998</name>
      <uri>https://old.reddit.com/user/Iory1998</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Please, for the love of God, convince me that GPT-OSS is the best open-source model that exists today. I dare you to convince me. There's no way the GPT-OSS 120B is better than Qwen-235B-A22B-2507, let alone DeepSeek R1. So why do 90% of YouTubers, and even Two Minute Papers (a guy I respect), praise GPT-OSS as the most beautiful gift to humanity any company ever gave? &lt;/p&gt; &lt;p&gt;It's not even multimodal, and they're calling it a gift? WTF for? Isn't that the same coriticim when Deepseek-R1 was released, that it was text-based only? In about 2 weeks, Alibaba released a video model (Wan2.2) , an image model (Qwen-Image) that are the best open-source models in their categories, two amazing 30B models that are super fast and punch above their weight, and two incredible 4B models ‚Äì yet barely any YouTubers covered them. Meanwhile, OpenAI launches a rather OK model and hell broke loose everywhere. How do you explain this? I can't find any rational explanation except OpenAI built a powerful brand name.&lt;/p&gt; &lt;p&gt;When DeepSeek-R1 was released, real innovation became public ‚Äì innovation GPT-OSS clearly built upon. How can a model have 120 Experts all stable without DeepSeek's paper? And to make matters worse, OpenAI dared to show their 20B model trained for under $500K! As if that's an achievement when DeepSeek R1 cost just $5.58 million ‚Äì 89x cheaper than OpenAI's rumored budgets. &lt;/p&gt; &lt;p&gt;Remember when every outlet (especially American ones) criticized DeepSeek: 'Look, the model is censored by the Communist Party. Do you want to live in a world of censorship?' Well, ask GPT-OSS about the Ukraine war and see if it answers you. The hypocrisy is rich. User &lt;a href="/u/Final_Wheel_7486"&gt;u/Final_Wheel_7486&lt;/a&gt; posted about this.&lt;/p&gt; &lt;p&gt;I'm not a coder or mathematician, and even if I were, these models wouldn't help much ‚Äì they're too limited. So I DON'T CARE ABOUT CODING SCORES ON BENCHMARKS. Don't tell me 'these models are very good at coding' as if a 20B model can actually code. Coders are a niche group. We need models that help average people.&lt;/p&gt; &lt;p&gt;This whole situation reminds me of that greedy guy who rarely gives to charity, then gets praised for doing the bare minimum when he finally does.&lt;/p&gt; &lt;p&gt;I am notsaying the models OpenAI released are bad, they simply aren't. But, what I am saying is that the hype is through the roof for an OK product. I want to hear your thoughts. &lt;/p&gt; &lt;p&gt;P.S. OpenAI fanboys, please keep it objective and civil!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Iory1998"&gt; /u/Iory1998 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjxx6j/gptoss_is_another_example_why_companies_must/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjxx6j/gptoss_is_another_example_why_companies_must/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjxx6j/gptoss_is_another_example_why_companies_must/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-07T11:49:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1mkhbs9</id>
    <title>OpenAI new open-source model is basically Phi-5</title>
    <updated>2025-08-08T00:50:55+00:00</updated>
    <author>
      <name>/u/ik-when-that-hotline</name>
      <uri>https://old.reddit.com/user/ik-when-that-hotline</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ik-when-that-hotline"&gt; /u/ik-when-that-hotline &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://news.ycombinator.com/item?id=44828884"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkhbs9/openai_new_opensource_model_is_basically_phi5/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mkhbs9/openai_new_opensource_model_is_basically_phi5/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-08T00:50:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1mkngs6</id>
    <title>I had to try the ‚Äúblueberry‚Äù thing myself with GPT5. I merely report the results.</title>
    <updated>2025-08-08T06:06:41+00:00</updated>
    <author>
      <name>/u/Trilogix</name>
      <uri>https://old.reddit.com/user/Trilogix</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkngs6/i_had_to_try_the_blueberry_thing_myself_with_gpt5/"&gt; &lt;img alt="I had to try the ‚Äúblueberry‚Äù thing myself with GPT5. I merely report the results." src="https://preview.redd.it/n3tapryqkqhf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ef40b2d7394bb222878d6008796d540bdf41673e" title="I had to try the ‚Äúblueberry‚Äù thing myself with GPT5. I merely report the results." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GPT5 keep saying it is the real deal lol. Is working but still far from the real deal in my opinion. &lt;/p&gt; &lt;p&gt;Credit: Kieran Healy‚Ä™@kjhealy.co‚Ä¨&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Trilogix"&gt; /u/Trilogix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/n3tapryqkqhf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkngs6/i_had_to_try_the_blueberry_thing_myself_with_gpt5/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mkngs6/i_had_to_try_the_blueberry_thing_myself_with_gpt5/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-08T06:06:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1mke7ef</id>
    <title>120B runs awesome on just 8GB VRAM!</title>
    <updated>2025-08-07T22:32:04+00:00</updated>
    <author>
      <name>/u/Wrong-Historian</name>
      <uri>https://old.reddit.com/user/Wrong-Historian</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here is the thing, the expert layers run amazing on CPU (~17T/s on a 14900K) and you can force that with this new llama-cpp option: --cpu-moe .&lt;/p&gt; &lt;p&gt;You can offload just the attention layers to GPU (requiring about 5GB of VRAM) for fast prefill.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;KV cache for the sequence&lt;/li&gt; &lt;li&gt;Attention weights &amp;amp; activations&lt;/li&gt; &lt;li&gt;Routing tables&lt;/li&gt; &lt;li&gt;LayerNorms and other ‚Äúnon-expert‚Äù parameters&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;No giant MLP weights are resident on the GPU, so memory use stays low.&lt;/p&gt; &lt;p&gt;This yields an amazing snappy system for a 120B model! Even something like a 3060Ti would be amazing! GPU with BF16 support would be best (RTX3000+) because all layers except the MOE layers (which are mxfp4) are BF16.&lt;/p&gt; &lt;p&gt;64GB of system ram would be minimum, and 96GB would be ideal. (linux uses mmap so will keep the 'hot' experts in memory even if the whole model doesn't fit in memory)&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;prompt eval time = 28044.75 ms / 3440 tokens ( 8.15 ms per token, 122.66 tokens per second)&lt;/p&gt; &lt;p&gt;eval time = 5433.28 ms / 98 tokens ( 55.44 ms per token, 18.04 tokens per second)&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;with 5GB of vram usage!&lt;/p&gt; &lt;p&gt;Honestly, I think this is the biggest win of this 120B model. This seems an amazing model to run fast for GPU-poor people. You can do this on a 3060Ti and 64GB of system ram is cheap.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wrong-Historian"&gt; /u/Wrong-Historian &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-07T22:32:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1mkcwiv</id>
    <title>OpenAI open washing</title>
    <updated>2025-08-07T21:38:35+00:00</updated>
    <author>
      <name>/u/gwyngwynsituation</name>
      <uri>https://old.reddit.com/user/gwyngwynsituation</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I think OpenAI released GPT-OSS, a barely usable model, fully aware it would generate backlash once freely tested. But they also had in mind that releasing GPT-5 immediately afterward would divert all attention away from their low-effort model. In this way, they can defend themselves against criticism that they‚Äôre not committed to the open-source space, without having to face the consequences of releasing a joke of a model. Classic corporate behavior. And that concludes my rant.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gwyngwynsituation"&gt; /u/gwyngwynsituation &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkcwiv/openai_open_washing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkcwiv/openai_open_washing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mkcwiv/openai_open_washing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-07T21:38:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1mki5in</id>
    <title>I'm disappointed with GPT-5</title>
    <updated>2025-08-08T01:29:43+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mki5in/im_disappointed_with_gpt5/"&gt; &lt;img alt="I'm disappointed with GPT-5" src="https://external-preview.redd.it/NXllb2Q4MG83cGhmMQC4BtXOIWa-7tqDev-Ylnu5AZH3avPIE_-Ap5hLG7c5.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=93854ef4afc407c45ae6200f03d7b2d2fb799170" title="I'm disappointed with GPT-5" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Leaving aside the magical bar charts at the launch event, I've been testing GPT-5 on openrouter and found that it fails 100% of the time when trying to complete demos based on three.js. &lt;/p&gt; &lt;p&gt;It seems incapable of writing importmap. Furthermore, when generating complex demos exceeding 600 lines, it runs into variable initialization/scope issues, where variable declarations appear after they are called. &lt;/p&gt; &lt;p&gt;My impression is that this is a problem typically seen only in models with a very small context length. I'm not sure if this is just my individual experience.&lt;/p&gt; &lt;p&gt;I also ran a particle test with Python and pygame, and the results were far from ideal. Of course, these test cases are just for reference and aren't comprehensive.&lt;/p&gt; &lt;p&gt;Finally, Opus-4.1 was a bit of a surprise; its performance has indeed improved. It's just that the pricing is terrifying.&lt;/p&gt; &lt;p&gt;What does everyone think of GPT-5? Is my understanding incorrect, and is GPT-5 a base model like GPT-4.1?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ci66880o7phf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mki5in/im_disappointed_with_gpt5/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mki5in/im_disappointed_with_gpt5/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-08T01:29:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1mkavhy</id>
    <title>random bar chart made by Qwen3-235B-A22B-2507</title>
    <updated>2025-08-07T20:19:55+00:00</updated>
    <author>
      <name>/u/tengo_harambe</name>
      <uri>https://old.reddit.com/user/tengo_harambe</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkavhy/random_bar_chart_made_by_qwen3235ba22b2507/"&gt; &lt;img alt="random bar chart made by Qwen3-235B-A22B-2507" src="https://preview.redd.it/rka3lhpnonhf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dd853635222d78299767b459957da8a9ae9f30b5" title="random bar chart made by Qwen3-235B-A22B-2507" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;had it render the chart on HTML canvas&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tengo_harambe"&gt; /u/tengo_harambe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rka3lhpnonhf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkavhy/random_bar_chart_made_by_qwen3235ba22b2507/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mkavhy/random_bar_chart_made_by_qwen3235ba22b2507/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-07T20:19:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1mkf543</id>
    <title>To all GPT-5 posts</title>
    <updated>2025-08-07T23:11:59+00:00</updated>
    <author>
      <name>/u/Danny_Davitoe</name>
      <uri>https://old.reddit.com/user/Danny_Davitoe</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkf543/to_all_gpt5_posts/"&gt; &lt;img alt="To all GPT-5 posts" src="https://preview.redd.it/8v08gwidjohf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a549b4a6f64e891d2fe2035565f6d9915347c9d1" title="To all GPT-5 posts" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Please. I don‚Äôt care about pricing. The only API teir I care about is which model gets port 8000 or 8080. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Danny_Davitoe"&gt; /u/Danny_Davitoe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8v08gwidjohf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkf543/to_all_gpt5_posts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mkf543/to_all_gpt5_posts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-07T23:11:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjf5ol</id>
    <title>r/LocalLlama is looking for moderators</title>
    <updated>2025-08-06T20:06:34+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/application/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T20:06:34+00:00</published>
  </entry>
</feed>
