<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-02T00:39:25+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qsenpy</id>
    <title>Don‚Äôt buy b60 for LLMs</title>
    <updated>2026-01-31T21:21:10+00:00</updated>
    <author>
      <name>/u/damirca</name>
      <uri>https://old.reddit.com/user/damirca</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I kinda regret buying b60. I thought that 24gb for 700 eur is a great deal, but the reality is completely different.&lt;/p&gt; &lt;p&gt;For starters, I live with a custom compiled kernel with the patch from an Intel dev to solve ffmpeg crashes.&lt;/p&gt; &lt;p&gt;Then I had to install the card into a windows machine in order to get GPU firmware updated (under Linux one need v2.0.19 of fwupd which is not available in Ubuntu yet) to solve the crazy fan speed on the b60 even when the temp of the gpu is 30 degrees Celsius.&lt;/p&gt; &lt;p&gt;But even after solving all of this, the actual experience doing local LLM on b60 is meh.&lt;/p&gt; &lt;p&gt;On llama.cpp the card goes crazy every time it does inference: fans go super high then low, the high again. The speed is about 10-15tks at best in models like mistral 14b. The noise level is just unbearable.&lt;/p&gt; &lt;p&gt;So the only reliable way is intel‚Äôs llm-scaler, but as of now it‚Äôs based on vllm 0.11.1 whereas latest version of vllm is 0.15. So Intel is like 6 months behind which is an eternity in this AI bubble times. For example any of new mistral models are not supported and one cannot run them on vanilla vllm too.&lt;/p&gt; &lt;p&gt;With llm-scaler the behavior of the card is ok: when it‚Äôs doing inference the fan goes louder and stays louder as long is it‚Äôs needed. The speed is like 20-25 tks on qwen3 VL 8b. However there are only some models that work with llm-scaler and most of them only with fp8, so for example qwen3 VL 8b after some requests processed with 16k length takes 20gb. That kinda bad: you have 24gb of vram but you cannot run normally 30b model with q4 quant and has to stick with 8b model with fp8.&lt;/p&gt; &lt;p&gt;Overall I think XFX 7900XTX would have been much better deal: same 24gb, 2x faster, in Dec the price was only 50 eur more than b60, it can run newest models with newest llama.cpp versions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/damirca"&gt; /u/damirca &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsenpy/dont_buy_b60_for_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsenpy/dont_buy_b60_for_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qsenpy/dont_buy_b60_for_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-31T21:21:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1qt3vbc</id>
    <title>What AI to Run on RTX 5070?</title>
    <updated>2026-02-01T17:00:29+00:00</updated>
    <author>
      <name>/u/InternalEffort6161</name>
      <uri>https://old.reddit.com/user/InternalEffort6161</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm upgrading to an RTX 5070 with 12GB VRAM and looking for recommendations on the best local models I can realistically run for two main use cases:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Coding / ‚Äúvibe coding‚Äù (IDE integration, Claude-like workflows, debugging, refactoring)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;General writing (scripts, long-form content)&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Right now I‚Äôm running Gemma 4B on a 4060 8GB using Ollama. It‚Äôs decent for writing and okay for coding, but I‚Äôm looking to push quality as far as possible with 12GB VRAM.&lt;/p&gt; &lt;p&gt;Not expecting a full Claude replacement. But wanting to offload some vibe coding to local llm to save some cost .. and help me write better..&lt;/p&gt; &lt;p&gt;Would love to hear what setups people are using and what‚Äôs realistically possible with 12GB of VRAM&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InternalEffort6161"&gt; /u/InternalEffort6161 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qt3vbc/what_ai_to_run_on_rtx_5070/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qt3vbc/what_ai_to_run_on_rtx_5070/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qt3vbc/what_ai_to_run_on_rtx_5070/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-01T17:00:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1qt6cqd</id>
    <title>PocketCoder - CLI coding agent with session memory that works on Ollama, OpenAI, Claude</title>
    <updated>2026-02-01T18:27:55+00:00</updated>
    <author>
      <name>/u/RentEquivalent1671</name>
      <uri>https://old.reddit.com/user/RentEquivalent1671</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We built an open-source CLI coding agent that works with any LLM - local via Ollama or cloud via OpenAI/Claude API. The idea was to create something that works reasonably well even with small models, not just frontier ones.&lt;/p&gt; &lt;p&gt;Sharing what's under the hood.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;WHY WE BUILT IT&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;We were paying $120/month for Claude Code. Then GLM-4.7 dropped and we thought - what if we build an agent optimized for working with ANY model, even 7B ones? Three weeks later - PocketCoder.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;HOW IT WORKS INSIDE&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Agent Loop - the core cycle:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;1. THINK - model reads task + context, decides what to do 2. ACT - calls a tool (write_file, run_command, etc) 3. OBSERVE - sees the result of what it did 4. DECIDE - task done? if not, repeat &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The tricky part is context management. We built an XML-based SESSION_CONTEXT that compresses everything:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;- task - what we're building (formed once on first message) - repo_map - project structure with classes/functions (like Aider does with tree-sitter) - files - which files were touched, created, read - terminal - last 20 commands with exit codes - todo - plan with status tracking - conversation_history - compressed summaries, not raw messages &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Everything persists in .pocketcoder/ folder (like .git/). Close terminal, come back tomorrow - context is there. This is the main difference from most agents - session memory that actually works.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;MULTI-PROVIDER SUPPORT&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;- Ollama (local models) - OpenAI API - Claude API - vLLM and LM Studio (auto-detects running processes) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;TOOLS THE MODEL CAN CALL&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;- write_file / apply_diff / read_file - run_command (with human approval) - add_todo / mark_done - attempt_completion (validates if file actually appeared - catches hallucinations) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;WHAT WE LEARNED ABOUT SMALL MODELS&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;7B models struggle with apply_diff - they rewrite entire files instead of editing 3 lines. Couldn't fix with prompting alone. 20B+ models handle it fine. Reasoning/MoE models work even better.&lt;/p&gt; &lt;p&gt;Also added loop detection - if model calls same tool 3x with same params, we interrupt it.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;INSTALL&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pip install pocketcoder pocketcoder &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;LINKS&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="http://github.com/Chashchin-Dmitry/pocketcoder"&gt;github.com/Chashchin-Dmitry/pocketcoder&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Looking for feedback and testers. What models are you running? What breaks?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RentEquivalent1671"&gt; /u/RentEquivalent1671 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qt6cqd/pocketcoder_cli_coding_agent_with_session_memory/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qt6cqd/pocketcoder_cli_coding_agent_with_session_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qt6cqd/pocketcoder_cli_coding_agent_with_session_memory/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-01T18:27:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1qswba2</id>
    <title>Llama 3.2 3B on Snapdragon 8 Elite: CPU is fast, but how do we unlock the NPU/GPU in Termux? üöÄ</title>
    <updated>2026-02-01T11:41:49+00:00</updated>
    <author>
      <name>/u/NeoLogic_Dev</name>
      <uri>https://old.reddit.com/user/NeoLogic_Dev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qswba2/llama_32_3b_on_snapdragon_8_elite_cpu_is_fast_but/"&gt; &lt;img alt="Llama 3.2 3B on Snapdragon 8 Elite: CPU is fast, but how do we unlock the NPU/GPU in Termux? üöÄ" src="https://preview.redd.it/8hdxiuxhevgg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dab5759cbcfe82848a7c42507951b94d2d2acc2e" title="Llama 3.2 3B on Snapdragon 8 Elite: CPU is fast, but how do we unlock the NPU/GPU in Termux? üöÄ" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve spent the last few hours optimizing Llama 3.2 3B on the new Snapdragon 8 Elite via Termux. After some environment tuning, the setup is rock solid‚Äîmemory management is no longer an issue, and the Oryon cores are absolutely ripping through tokens. However, running purely on CPU feels like owning a Ferrari and never leaving second gear. I want to tap into the Adreno 830 GPU or the Hexagon NPU to see what this silicon can really do. The Challenge: Standard Ollama/llama.cpp builds in Termux default to CPU. I‚Äôm looking for anyone who has successfully bridged the gap to the hardware accelerators on this specific chip. Current leads I'm investigating: OpenCL/Vulkan Backends: Qualcomm recently introduced a new OpenCL GPU backend for llama.cpp specifically for Adreno. Has anyone successfully compiled this in Termux with the correct libOpenCL.so links from /system/vendor/lib64?.&lt;br /&gt; QNN (Qualcomm AI Engine Direct): There are experimental GGML_HTP (Hexagon Tensor Processor) backends appearing in some research forks. Has anyone managed to get the QNN SDK libraries working natively in Termux to offload the KV cache?. Vulkan via Turnip: With the Adreno 8-series being so new, are the current Turnip drivers stable enough for llama-cpp-backend-vulkan?. If you‚Äôve moved past CPU-only inference on the 8 Elite, how did you handle the library dependencies? Let‚Äôs figure out how to make neobild the fastest mobile LLM implementation out there. üõ†Ô∏è&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NeoLogic_Dev"&gt; /u/NeoLogic_Dev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8hdxiuxhevgg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qswba2/llama_32_3b_on_snapdragon_8_elite_cpu_is_fast_but/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qswba2/llama_32_3b_on_snapdragon_8_elite_cpu_is_fast_but/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-01T11:41:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1qt2cjr</id>
    <title>Interested in preferred coding workflows with RTX 6000 pro</title>
    <updated>2026-02-01T16:04:30+00:00</updated>
    <author>
      <name>/u/Laabc123</name>
      <uri>https://old.reddit.com/user/Laabc123</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all. Apologies if this is somewhat repetitive, but I haven‚Äôt been able to find a thread with this specific discussion. &lt;/p&gt; &lt;p&gt;I have a PC with a single RTX 6000 pro (96gb). I‚Äôm interested in understanding how others are best leveraging this card for building/coding. This will be smaller to medium sized apps (not large existing codebases) in common languages with relatively common stacks. &lt;/p&gt; &lt;p&gt;I‚Äôm open to leveraging one of the massive cloud models in the workflow, but I‚Äôd like pair with local models to maximize the leverage of my RTX. &lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Laabc123"&gt; /u/Laabc123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qt2cjr/interested_in_preferred_coding_workflows_with_rtx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qt2cjr/interested_in_preferred_coding_workflows_with_rtx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qt2cjr/interested_in_preferred_coding_workflows_with_rtx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-01T16:04:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1qt9n73</id>
    <title>Generative AI solution</title>
    <updated>2026-02-01T20:23:35+00:00</updated>
    <author>
      <name>/u/chribonn</name>
      <uri>https://old.reddit.com/user/chribonn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Photoshop has built in functionality to perform generative AI.&lt;/p&gt; &lt;p&gt;Is there a solution consisting of Software and a Local LLM that would allow me to do the same? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chribonn"&gt; /u/chribonn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qt9n73/generative_ai_solution/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qt9n73/generative_ai_solution/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qt9n73/generative_ai_solution/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-01T20:23:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1qtddwk</id>
    <title>Kimi 2.5 vs GLM 4.7 vs MiniMax M2.1 for complex debugging?</title>
    <updated>2026-02-01T22:46:07+00:00</updated>
    <author>
      <name>/u/Legal_Comb_6844</name>
      <uri>https://old.reddit.com/user/Legal_Comb_6844</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm a freelancer working in coding, systems, and networking and I‚Äôm choosing an LLM to use with OpenClaw.&lt;/p&gt; &lt;p&gt;Comparing:&lt;/p&gt; &lt;p&gt;Kimi 2.5&lt;/p&gt; &lt;p&gt;GLM 4.7&lt;/p&gt; &lt;p&gt;MiniMax M2.1 (recommended from openclaw)&lt;/p&gt; &lt;p&gt;Which one performs best for complex debugging and technical problem solving?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Legal_Comb_6844"&gt; /u/Legal_Comb_6844 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtddwk/kimi_25_vs_glm_47_vs_minimax_m21_for_complex/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtddwk/kimi_25_vs_glm_47_vs_minimax_m21_for_complex/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qtddwk/kimi_25_vs_glm_47_vs_minimax_m21_for_complex/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-01T22:46:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1qt5us6</id>
    <title>SDPO: Reinforcement Learning via Self-Distillation</title>
    <updated>2026-02-01T18:10:32+00:00</updated>
    <author>
      <name>/u/TheRealMasonMac</name>
      <uri>https://old.reddit.com/user/TheRealMasonMac</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;SDPO: Reinforcement Learning via Self-Distillation&amp;quot; introduces Self-Distillation Policy Optimization (SDPO), a method that addresses the credit-assignment bottleneck in reinforcement learning with verifiable rewards (RLVR) by leveraging rich textual feedback‚Äîsuch as runtime errors or judge evaluations‚Äîthat many environments provide but current approaches ignore. SDPO treats the model's own feedback-conditioned predictions as a self-teacher, distilling these corrected next-token distributions back into the policy without requiring external teachers or explicit reward models. This approach converts sparse scalar rewards into dense learning signals, enabling the model to learn from its own retrospection and mistake analysis.&lt;/p&gt; &lt;p&gt;Across scientific reasoning, tool use, and competitive programming tasks including LiveCodeBench v6, SDPO achieves substantial improvements in sample efficiency and final accuracy over strong RLVR baselines like GRPO, reaching target accuracies up to 10√ó faster in wall-clock time while producing reasoning traces up to 7√ó shorter. The method also proves effective in environments with only binary rewards by using successful rollouts as implicit feedback, and when applied at test time, it accelerates solution discovery on difficult problems with 3√ó fewer attempts than traditional best-of-k sampling. Notably, SDPO's benefits increase with model scale, suggesting that larger models' superior in-context learning capabilities enhance the effectiveness of self-distillation.&lt;/p&gt; &lt;p&gt;(Summary by K2.5)&lt;/p&gt; &lt;p&gt;tl;dr You know when a model does something wrong and you tell it, &amp;quot;Hey, you made a mistake here. This is what you did wrong: [...]&amp;quot; and it acts upon that to correct itself? That's basically what happens here.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheRealMasonMac"&gt; /u/TheRealMasonMac &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://self-distillation.github.io/SDPO"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qt5us6/sdpo_reinforcement_learning_via_selfdistillation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qt5us6/sdpo_reinforcement_learning_via_selfdistillation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-01T18:10:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1qt250p</id>
    <title>While we wait for Deepseek 4, Unsloth is quietly releasing gguf for 3.2...</title>
    <updated>2026-02-01T15:56:50+00:00</updated>
    <author>
      <name>/u/LegacyRemaster</name>
      <uri>https://old.reddit.com/user/LegacyRemaster</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qt250p/while_we_wait_for_deepseek_4_unsloth_is_quietly/"&gt; &lt;img alt="While we wait for Deepseek 4, Unsloth is quietly releasing gguf for 3.2..." src="https://b.thumbs.redditmedia.com/rAfR66Nhbn--BoXjqUhJbnx3ELte8xYh-HTukDrugDs.jpg" title="While we wait for Deepseek 4, Unsloth is quietly releasing gguf for 3.2..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/u6pxu5imnwgg1.png?width=1654&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=32c0b641bf9fde5d30a684a9c08d22b53f4a0c90"&gt;unsloth deepseek&lt;/a&gt;&lt;/p&gt; &lt;p&gt;On LM studio 0.4.1 I only get 4.2 tokens/sec but on llama.cpp it runs much faster than previous releases! RTX 96gb + 128 DDR4 3200&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LegacyRemaster"&gt; /u/LegacyRemaster &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qt250p/while_we_wait_for_deepseek_4_unsloth_is_quietly/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qt250p/while_we_wait_for_deepseek_4_unsloth_is_quietly/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qt250p/while_we_wait_for_deepseek_4_unsloth_is_quietly/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-01T15:56:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1qssxhx</id>
    <title>Research: vllm-mlx on Apple Silicon achieves 21% to 87% higher throughput than llama.cpp</title>
    <updated>2026-02-01T08:26:21+00:00</updated>
    <author>
      <name>/u/Synor</name>
      <uri>https://old.reddit.com/user/Synor</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Synor"&gt; /u/Synor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2601.19139v1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qssxhx/research_vllmmlx_on_apple_silicon_achieves_21_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qssxhx/research_vllmmlx_on_apple_silicon_achieves_21_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-01T08:26:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1qtf8hk</id>
    <title>AniMUL-v1 a 30B model trained to do species classification from audio files</title>
    <updated>2026-02-02T00:01:13+00:00</updated>
    <author>
      <name>/u/lemon07r</name>
      <uri>https://old.reddit.com/user/lemon07r</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Not my project, sharing this for a friend since they don't have a reddit account. Thought this was cool and wanted to share it since they put in a lot of effort (none of this is my work, so all credits to them). &lt;/p&gt; &lt;p&gt;This is a fine tune of &lt;a href="https://huggingface.co/Qwen/Qwen3-Omni-30B-A3B-Instruct"&gt;Qwen3-Omni-30B-A3B-Instruct&lt;/a&gt; using Earth Species Project's &lt;a href="https://huggingface.co/datasets/EarthSpeciesProject/NatureLM-audio-training"&gt;NatureLM-audio-training&lt;/a&gt; dataset of 26 million audio-text pairs, trained on &lt;strong&gt;8x B200 GPUs for roughly 912~ hours&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Check it out in these links below!&lt;br /&gt; HF: &lt;a href="https://huggingface.co/deepcrayon/AniMUL-v1"&gt;https://huggingface.co/deepcrayon/AniMUL-v1&lt;/a&gt;&lt;br /&gt; Git Repo: &lt;a href="https://spacecruft.org/deepcrayon/AniMUL"&gt;https://spacecruft.org/deepcrayon/AniMUL&lt;/a&gt;&lt;br /&gt; Demo (try it here!): &lt;a href="https://animul.ai/"&gt;https://animul.ai/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here's how it performs compared to the base model:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;================================================================================ MODEL COMPARISON REPORT AniMUL-v1 vs Qwen3-Omni Base Model ================================================================================ ================================================================================ SUMMARY STATISTICS ================================================================================ Total samples: 100 AniMUL-v1 Checkpoint (Fine-tuned): Exact matches: 75/100 (75.0%) Contains matches: 76/100 (76.0%) Average similarity: 88.23% Qwen3-Omni Base Model (Not fine-tuned): Exact matches: 14/100 (14.0%) Contains matches: 18/100 (18.0%) Average similarity: 28.80% -------------------------------------------------------------------------------- COMPARISON (AniMUL vs Qwen3-Omni): -------------------------------------------------------------------------------- ‚úì AniMUL has 61 MORE exact matches (+61.0%) ‚úì AniMUL has 58 MORE contains matches (+58.0%) ‚úì AniMUL has 59.43% HIGHER average similarity üèÜ WINNER: AniMUL-v1 (fine-tuned model performs better) ================================================================================ &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lemon07r"&gt; /u/lemon07r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtf8hk/animulv1_a_30b_model_trained_to_do_species/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtf8hk/animulv1_a_30b_model_trained_to_do_species/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qtf8hk/animulv1_a_30b_model_trained_to_do_species/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-02T00:01:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1qt6u8r</id>
    <title>Qwen3-TTS Studio interface testing in progress</title>
    <updated>2026-02-01T18:44:51+00:00</updated>
    <author>
      <name>/u/Eastern_Rock7947</name>
      <uri>https://old.reddit.com/user/Eastern_Rock7947</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/ckajtdhggxgg1.png?width=1308&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d15394ae2113ba905af0877aeb8681b6cce434ca"&gt;https://preview.redd.it/ckajtdhggxgg1.png?width=1308&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d15394ae2113ba905af0877aeb8681b6cce434ca&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In the final stages of testing my Qwen3-TTS Studio:&lt;/p&gt; &lt;p&gt;Features: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Auto transcribe reference audio&lt;/li&gt; &lt;li&gt;Episode load/save/delete&lt;/li&gt; &lt;li&gt;Bulk text split and editing by paragraph for unlimited long form text generation&lt;/li&gt; &lt;li&gt;Custom time [Pause] tags for text: [pause: 0.3s]&lt;/li&gt; &lt;li&gt;Insert/delete/regenerate any paragraph&lt;/li&gt; &lt;li&gt;Additional media file inserting/deleting anywhere &lt;/li&gt; &lt;li&gt;Drag and drop paragraphs &lt;/li&gt; &lt;li&gt;Auto recombining media &lt;/li&gt; &lt;li&gt;Regenerate a specific paragraph and auto recombine&lt;/li&gt; &lt;li&gt;Generation time demographics&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Anything else I should add?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Eastern_Rock7947"&gt; /u/Eastern_Rock7947 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qt6u8r/qwen3tts_studio_interface_testing_in_progress/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qt6u8r/qwen3tts_studio_interface_testing_in_progress/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qt6u8r/qwen3tts_studio_interface_testing_in_progress/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-01T18:44:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1qt2po4</id>
    <title>A List of Creative Writing Benchmarks</title>
    <updated>2026-02-01T16:17:51+00:00</updated>
    <author>
      <name>/u/claire_rr</name>
      <uri>https://old.reddit.com/user/claire_rr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I like to read &amp;amp; write fiction in my spare time and keep seeing posts asking which LLM works best for creative writing. As a result, I put together a list of the benchmarks I‚Äôve come across so far, hope it helps someone out!&lt;/p&gt; &lt;p&gt;On a side note, I‚Äôm insanely biased toward Kimi K2 üòÑ&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Benchmark&lt;/th&gt; &lt;th align="left"&gt;Description&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Narrator.sh&lt;/td&gt; &lt;td align="left"&gt;A site where AI models write and publish stories ranked by real reader metrics like views and ratings. Supports filtering by genre, NSFW content, and specific story details, and separates models into brainstorming, memory, and writing categories.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Lechmazur Creative Writing Benchmark&lt;/td&gt; &lt;td align="left"&gt;Measures how well models weave 10 key story elements (characters, objects, motivations, etc.) into short stories using multiple judges and transparent scoring, though judges may favor safer writing.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;EQ-Bench Creative Writing v3&lt;/td&gt; &lt;td align="left"&gt;Uses challenging creative prompts to test humor, romance, and unconventional writing, with metrics like ‚ÄúSlop‚Äù scores for clich√©s and repetition detection; penalizes NSFW and darker content.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;NC-Bench (Novelcrafter)&lt;/td&gt; &lt;td align="left"&gt;Evaluates practical writing tasks such as rewriting, idea generation, summarization, and translation, focusing on how useful models are for writers rather than full story generation.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;WritingBench&lt;/td&gt; &lt;td align="left"&gt;Tests models across many writing styles (creative, persuasive, technical, etc.) using 1,000+ real-world examples, offering broad coverage but relying heavily on the critic model.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Fiction Live Benchmark&lt;/td&gt; &lt;td align="left"&gt;Assesses whether models can understand and remember very long stories by quizzing them on plot details and character arcs, without measuring prose quality.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;UGI Writing Leaderboard&lt;/td&gt; &lt;td align="left"&gt;Combines multiple writing metrics into a single score with breakdowns for repetition, length control, and readability, enabling quick comparisons while hiding some tradeoffs.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/claire_rr"&gt; /u/claire_rr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qt2po4/a_list_of_creative_writing_benchmarks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qt2po4/a_list_of_creative_writing_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qt2po4/a_list_of_creative_writing_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-01T16:17:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1qsx9r0</id>
    <title>Ultra-Sparse MoEs are the future</title>
    <updated>2026-02-01T12:31:51+00:00</updated>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GPT-OSS-120B,Qwen3-Next-80B-A3B etc.. we need more of the ultra-sparse MoEs! Like we can create a 120B that uses fine-grained expert system ‚Üí distill it into a 30B A3B ‚Üí again into 7B A1B all trained in MXFP4?&lt;/p&gt; &lt;p&gt;That would be perfect because it solves the issue of direct distillation (model can't approximate the much larger teacher internal representations due to high complexity) while allowing to run models on actual consumer hardware from 96-128GB of ram ‚Üí 24GB GPUs ‚Üí 8GB GPUs.&lt;/p&gt; &lt;p&gt;A more efficient reasoning would be also a great idea! I noticed that specifically in GPT-OSS-120B (low) where it thinks in 1 or 2 words and follows a specific structure we had a great advancement for spec decoding for that model because it's predictable so it's faster.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsx9r0/ultrasparse_moes_are_the_future/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsx9r0/ultrasparse_moes_are_the_future/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qsx9r0/ultrasparse_moes_are_the_future/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-01T12:31:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1qt83qa</id>
    <title>mq - query documents like jq, built for agents (up to 83% fewer tokens use)</title>
    <updated>2026-02-01T19:28:51+00:00</updated>
    <author>
      <name>/u/GetInTheArena</name>
      <uri>https://old.reddit.com/user/GetInTheArena</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I do a lot of agentic coding for work - Claude Code, Codex, Cursor, on medium and large codebases. My 2 Claude Max plan were burning through my weekly context limits within a few days. &lt;/p&gt; &lt;p&gt;Most of it was agents reading entire files when they only needed one section. Subagent do prevent context overflow but still use up lots of tokens.&lt;/p&gt; &lt;p&gt;So I built &lt;a href="https://github.com/muqsitnawaz/mq"&gt;mq&lt;/a&gt;. Instead of Agents reading entire .md files into context, expose the structure and let the agent figure out what it actually needs.&lt;/p&gt; &lt;p&gt;&lt;code&gt;mq paper.pdf .tree # see the structure&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;mq paper.pdf '.section(&amp;quot;Methods&amp;quot;) | .text' # grab what you need&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Tested on LangChain docs for a Explore query - went from 147k tokens to 24k. Works with markdown, HTML, PDF, JSON, YAML. Single binary, no vector DB, no embeddings, no API calls. &lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="http://github.com/muqsitnawaz/mq"&gt;http://github.com/muqsitnawaz/mq&lt;/a&gt; - free and open source for the community &lt;/p&gt; &lt;p&gt;I know Tobi's qmd exists which is pretty cool but it always felt too heavy for what I needed. Downloading 3GB models, managing SQLite databases, keeping embeddings in sync when files change... I just wanted something Agents would pipe into like jq.&lt;/p&gt; &lt;p&gt;The hot take: RAG is overkill for a lot of small-scale agent workflows but that's another post.&lt;/p&gt; &lt;p&gt;Curious if community tried qmd or similar tools. What's working for you?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GetInTheArena"&gt; /u/GetInTheArena &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qt83qa/mq_query_documents_like_jq_built_for_agents_up_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qt83qa/mq_query_documents_like_jq_built_for_agents_up_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qt83qa/mq_query_documents_like_jq_built_for_agents_up_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-01T19:28:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1qsn78m</id>
    <title>Exposed Moltbook Database Let Anyone Take Control of Any AI Agent on the Site</title>
    <updated>2026-02-01T03:25:12+00:00</updated>
    <author>
      <name>/u/georgemoore13</name>
      <uri>https://old.reddit.com/user/georgemoore13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsn78m/exposed_moltbook_database_let_anyone_take_control/"&gt; &lt;img alt="Exposed Moltbook Database Let Anyone Take Control of Any AI Agent on the Site" src="https://external-preview.redd.it/bfjA2IIU81Nyg02gojI5OSsrZO__DkXhWMj0JjL44MA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2b3c6a4dfe6f5b662bd4fcb95ee7ed9af6ee839f" title="Exposed Moltbook Database Let Anyone Take Control of Any AI Agent on the Site" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/georgemoore13"&gt; /u/georgemoore13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.404media.co/exposed-moltbook-database-let-anyone-take-control-of-any-ai-agent-on-the-site/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsn78m/exposed_moltbook_database_let_anyone_take_control/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qsn78m/exposed_moltbook_database_let_anyone_take_control/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-01T03:25:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1qsvgsh</id>
    <title>some uncensored models</title>
    <updated>2026-02-01T10:53:41+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Since there haven‚Äôt been any (major) new local model releases lately, let‚Äôs check what uncensored models are available on Hugging Face. There are different abliteration methods, so varioud models can behave quite differently. Unfortunately, I can‚Äôt find any Nemotron-3 Nano variants.&lt;/p&gt; &lt;p&gt;Which one do you use?&lt;/p&gt; &lt;p&gt;GLM 4.7 Flash&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/DavidAU/GLM-4.7-Flash-Uncensored-Heretic-NEO-CODE-Imatrix-MAX-GGUF"&gt;https://huggingface.co/DavidAU/GLM-4.7-Flash-Uncensored-Heretic-NEO-CODE-Imatrix-MAX-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mradermacher/Huihui-GLM-4.7-Flash-abliterated-GGUF"&gt;https://huggingface.co/mradermacher/Huihui-GLM-4.7-Flash-abliterated-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Olafangensan/GLM-4.7-Flash-heretic-GGUF"&gt;https://huggingface.co/Olafangensan/GLM-4.7-Flash-heretic-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GPT OSS 20B&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf"&gt;https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-HERETIC-uncensored-NEO-Imatrix-gguf"&gt;https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-HERETIC-uncensored-NEO-Imatrix-gguf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/huihui-ai/Huihui-gpt-oss-20b-BF16-abliterated-v2"&gt;https://huggingface.co/huihui-ai/Huihui-gpt-oss-20b-BF16-abliterated-v2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/bartowski/p-e-w_gpt-oss-20b-heretic-GGUF"&gt;https://huggingface.co/bartowski/p-e-w_gpt-oss-20b-heretic-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GPT OSS 120B&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/huihui-ai/Huihui-gpt-oss-120b-BF16-abliterated"&gt;https://huggingface.co/huihui-ai/Huihui-gpt-oss-120b-BF16-abliterated&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/bartowski/kldzj_gpt-oss-120b-heretic-v2-GGUF"&gt;https://huggingface.co/bartowski/kldzj_gpt-oss-120b-heretic-v2-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Gemma 12B&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/DreamFast/gemma-3-12b-it-heretic"&gt;https://huggingface.co/DreamFast/gemma-3-12b-it-heretic&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mlabonne/gemma-3-12b-it-abliterated-v2-GGUF"&gt;https://huggingface.co/mlabonne/gemma-3-12b-it-abliterated-v2-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Gemma 27B&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-GGUF"&gt;https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mradermacher/gemma-3-27b-it-heretic-v2-i1-GGUF"&gt;https://huggingface.co/mradermacher/gemma-3-27b-it-heretic-v2-i1-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Qwen 30B A3B&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/huihui-ai/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated"&gt;https://huggingface.co/huihui-ai/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Goekdeniz-Guelmez/Josiefied-Qwen3-30B-A3B-abliterated-v2"&gt;https://huggingface.co/Goekdeniz-Guelmez/Josiefied-Qwen3-30B-A3B-abliterated-v2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Qwen 8B&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/DavidAU/Qwen3-8B-Hivemind-Instruct-Heretic-Abliterated-Uncensored-NEO-Imatrix-GGUF"&gt;https://huggingface.co/DavidAU/Qwen3-8B-Hivemind-Instruct-Heretic-Abliterated-Uncensored-NEO-Imatrix-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/huihui-ai/Huihui-Qwen3-VL-8B-Instruct-abliterated"&gt;https://huggingface.co/huihui-ai/Huihui-Qwen3-VL-8B-Instruct-abliterated&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Qwen 32B&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mradermacher/Qwen3-VL-32B-Instruct-heretic-v2-GGUF"&gt;https://huggingface.co/mradermacher/Qwen3-VL-32B-Instruct-heretic-v2-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/huihui-ai/Qwen3-32B-abliterated"&gt;https://huggingface.co/huihui-ai/Qwen3-32B-abliterated&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsvgsh/some_uncensored_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsvgsh/some_uncensored_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qsvgsh/some_uncensored_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-01T10:53:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1qsy0gg</id>
    <title>Deepseek v4/3.5 is probably coming out tomorrow or in the next 5 days?</title>
    <updated>2026-02-01T13:07:49+00:00</updated>
    <author>
      <name>/u/power97992</name>
      <uri>https://old.reddit.com/user/power97992</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Are you ready for an llm with engrams? Perhaps it has even vision? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/power97992"&gt; /u/power97992 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsy0gg/deepseek_v435_is_probably_coming_out_tomorrow_or/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsy0gg/deepseek_v435_is_probably_coming_out_tomorrow_or/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qsy0gg/deepseek_v435_is_probably_coming_out_tomorrow_or/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-01T13:07:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1qsrscu</id>
    <title>Can 4chan data REALLY improve a model? TURNS OUT IT CAN!</title>
    <updated>2026-02-01T07:20:46+00:00</updated>
    <author>
      <name>/u/Sicarius_The_First</name>
      <uri>https://old.reddit.com/user/Sicarius_The_First</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsrscu/can_4chan_data_really_improve_a_model_turns_out/"&gt; &lt;img alt="Can 4chan data REALLY improve a model? TURNS OUT IT CAN!" src="https://a.thumbs.redditmedia.com/kH2rOREckIffGxPYl8Dxt7JwePnVvjX39wCJzfAooO0.jpg" title="Can 4chan data REALLY improve a model? TURNS OUT IT CAN!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hear me out, no one (really) knows how these things work.&lt;/p&gt; &lt;p&gt;A few days ago, I released &lt;a href="https://huggingface.co/SicariusSicariiStuff/Assistant_Pepe_8B"&gt;Assistant_Pepe_8B&lt;/a&gt;, you can read the discussion in &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1qppjo4/assistant_pepe_8b_1m_context_zero_slop/"&gt;this thread&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;I trained it on an extended &lt;strong&gt;4chan dataset&lt;/strong&gt;, on an abliterated base, but what I didn't expect was to get this:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/lrqwx8ca1ugg1.png?width=2333&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4dcfcfb9c107fa3d417e5ff623c4952e5e2ab457"&gt;https://preview.redd.it/lrqwx8ca1ugg1.png?width=2333&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4dcfcfb9c107fa3d417e5ff623c4952e5e2ab457&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/a3bby1yd1ugg1.png?width=2980&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8f050bbd512a12a359626af79ccebcd2d2445877"&gt;https://preview.redd.it/a3bby1yd1ugg1.png?width=2980&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8f050bbd512a12a359626af79ccebcd2d2445877&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Somehow, &lt;strong&gt;against all common sense&lt;/strong&gt;, the model &lt;strong&gt;outperformed&lt;/strong&gt; nvidia's nemotron, the base it was trained on. This is usually the other way around. You take a smart base, tune a model on it, and accept the sacrifice of some intelligence to give it flavor.&lt;/p&gt; &lt;p&gt;At first I thought &amp;quot;OK nice, a coincidence, who cares?&amp;quot;&lt;/p&gt; &lt;p&gt;But then I looked more closely at the scores:&lt;/p&gt; &lt;p&gt;1) The abliterated base &lt;strong&gt;scored higher&lt;/strong&gt; than the base.&lt;br /&gt; 2) The finetune scored even &lt;strong&gt;higher than both&lt;/strong&gt;.&lt;br /&gt; 3) The finetune was literally on an extremely noise 4chan dataset, it should have eaten glue.&lt;/p&gt; &lt;p&gt;And then I remembered something: the original, gpt4chan (by Yannic Kilcher) scored especially high in truthfulness (that was b4 benchmaxxing).&lt;/p&gt; &lt;p&gt;So I took a closer look on recent models I released; the abliterated Impish_LLAMA_4B not only outperformed the base tune (the unabliterated one), it also changed its political alignment (you can check for yourself the UGI stats, I feel like I spammed enough images). &lt;/p&gt; &lt;p&gt;People were initially joking about the &amp;quot;alignment tax&amp;quot;, I think there's a none trivial substance in all of this. It seems to me just above a marginal error or statistical noise.&lt;/p&gt; &lt;p&gt;Oh, and the KL divergence for Impish_LLAMA_4B was :&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;lt;0.01 &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sicarius_The_First"&gt; /u/Sicarius_The_First &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsrscu/can_4chan_data_really_improve_a_model_turns_out/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsrscu/can_4chan_data_really_improve_a_model_turns_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qsrscu/can_4chan_data_really_improve_a_model_turns_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-01T07:20:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1qsxowq</id>
    <title>OLMO 3.5 Is Around The Corner</title>
    <updated>2026-02-01T12:52:34+00:00</updated>
    <author>
      <name>/u/Few_Painter_5588</name>
      <uri>https://old.reddit.com/user/Few_Painter_5588</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsxowq/olmo_35_is_around_the_corner/"&gt; &lt;img alt="OLMO 3.5 Is Around The Corner" src="https://preview.redd.it/bfhk9qzqpvgg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=63c2040c8dfb4a24d40bb5ca076c537bef194d77" title="OLMO 3.5 Is Around The Corner" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The OLMO series is seriously under-appreciated. Yes they may not perform the best compared to other openweight models, but OLMO models are fully open sourced, from their datasets to training recipes. So it's nice to see them experiment with more niche techniques.&lt;/p&gt; &lt;p&gt;It seems like for 3.5, they'll be using some of the techniques that Qwen3-Next introduced, so long context tasks should take less memory.&lt;/p&gt; &lt;p&gt;Though this series seems to be a set of Dense models, with the smallest being a 1B model.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;OLMo 3.5 Hybrid is a hybrid architecture model from Ai2 that combines standard transformer attention layers with linear attention layers using the Gated Deltanet. This hybrid approach aims to improve efficiency while maintaining model quality by interleaving full attention layers with linear attention layers.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Few_Painter_5588"&gt; /u/Few_Painter_5588 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bfhk9qzqpvgg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsxowq/olmo_35_is_around_the_corner/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qsxowq/olmo_35_is_around_the_corner/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-01T12:52:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1qt9gyf</id>
    <title>I built a pentesting platform that lets AI control 400+ hacking tools</title>
    <updated>2026-02-01T20:17:14+00:00</updated>
    <author>
      <name>/u/Justachillguypeace</name>
      <uri>https://old.reddit.com/user/Justachillguypeace</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qt9gyf/i_built_a_pentesting_platform_that_lets_ai/"&gt; &lt;img alt="I built a pentesting platform that lets AI control 400+ hacking tools" src="https://external-preview.redd.it/MmhocXdobTl5eGdnMS7Ny9qzMAmuinIQRg---a-6I7vN05-3-TDw6Gj1XVF3.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2e0dd22cb6593dff42a1b197f8e3eb8049aa617e" title="I built a pentesting platform that lets AI control 400+ hacking tools" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I've been working on this project for the past month as a side project (I'm a pentester).&lt;/p&gt; &lt;p&gt;The idea: give your AI agent a full pentesting environment. Claude can execute tools directly in a Docker container, chain attacks based on what it finds, and document everything automatically.&lt;/p&gt; &lt;p&gt;How it works:&lt;/p&gt; &lt;p&gt;- AI agent connects via MCP to an Exegol container (400+ security tools)&lt;/p&gt; &lt;p&gt;- Executes nmap, sqlmap, nuclei, ffuf, etc. directly&lt;/p&gt; &lt;p&gt;- Tracks findings in a web dashboard&lt;/p&gt; &lt;p&gt;- Maintains full context across the entire assessment&lt;/p&gt; &lt;p&gt;No more copy-pasting commands back and forth between Claude and your terminal :)&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/Vasco0x4/AIDA"&gt;https://github.com/Vasco0x4/AIDA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Demo: &lt;a href="https://www.youtube.com/watch?v=yz6ac-y4g08"&gt;https://www.youtube.com/watch?v=yz6ac-y4g08&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is my first big open source project, so I'm waiting for honest reviews and feedback. Not trying to monetize it, just sharing with the community.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Justachillguypeace"&gt; /u/Justachillguypeace &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/sfk44fm9yxgg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qt9gyf/i_built_a_pentesting_platform_that_lets_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qt9gyf/i_built_a_pentesting_platform_that_lets_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-01T20:17:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1qsx51z</id>
    <title>Falcon-H1-Tiny (90M) is out - specialized micro-models that actually work</title>
    <updated>2026-02-01T12:25:04+00:00</updated>
    <author>
      <name>/u/United-Manner-7</name>
      <uri>https://old.reddit.com/user/United-Manner-7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TII just dropped Falcon-H1-Tiny - a series of sub-100M models that quietly challenge the scaling dogma. We've all suspected that narrow, specialized smal models tend to hallucinate less than giant generalists. After all, a 90M parameter model has far less internal &amp;quot;room&amp;quot; to drift off-topic or invent facts outside its training scope. But this release &lt;em&gt;proves&lt;/em&gt; it with numbers - and flips the script on how we think about capability at tiny scales.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What's actually new&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Anti-curriculum training&lt;/strong&gt;: Instead of pretraining on web junk then fine-tuning, they inject target-domain data (SFT, reasoning traces, tool calls) from token #1. For 90M models with ~5 GT memorization windows, this works - no overfitting even after 100+ epochs on high-quality data.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hybrid Mamba+Attention blocks&lt;/strong&gt; inherited from Falcon-H1, plus Learnable Multipliers + Muon optimizer (up to 20% relative gain over AdamW).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Specialized variants that punch above weight&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;90M tool-caller hits 94.44% relevance detection (knows &lt;em&gt;when&lt;/em&gt; to call a function) matches 270M Function Gemma globally despite weaker AST accuracy&lt;/li&gt; &lt;li&gt;600M reasoning model (R-0.6B) post-GRPO solves 75% of AIME24 problems pass@1 - competitive with 7B-class models when scaled at inference&lt;/li&gt; &lt;li&gt;90M coder with native FIM support runs autocomplete inside VS Code via Continue plugin&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Why this matters for local deployment&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Models this size (~90 MB quantized Q8_0) run on any modern phone or Raspberry Pi without breaking a sweat. They're not trying to replace your 7B daily driver they're purpose-built for constrained environments where footprint and latency dominate. And if you scaled these designs to ~1B parameters (11√ó), the'd likely cover 90% of everyday local use cases: chat, tool calling, light coding, reasoning traces - all while staying under 500 MB even quantized.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Base 90M instruct model: &lt;a href="https://huggingface.co/tiiuae/Falcon-H1-Tiny-R-90M"&gt;https://huggingface.co/tiiuae/Falcon-H1-Tiny-R-90M&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Full model collection: &lt;a href="https://huggingface.co/tiiuae/models"&gt;https://huggingface.co/tiiuae/models&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Technical blogpost with experiments: &lt;a href="https://huggingface.co/spaces/tiiuae/tiny-h1-blogpost"&gt;https://huggingface.co/spaces/tiiuae/tiny-h1-blogpost&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/United-Manner-7"&gt; /u/United-Manner-7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsx51z/falconh1tiny_90m_is_out_specialized_micromodels/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsx51z/falconh1tiny_90m_is_out_specialized_micromodels/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qsx51z/falconh1tiny_90m_is_out_specialized_micromodels/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-01T12:25:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1qt76qs</id>
    <title>Mistral Vibe 2.0</title>
    <updated>2026-02-01T18:56:50+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qt76qs/mistral_vibe_20/"&gt; &lt;img alt="Mistral Vibe 2.0" src="https://external-preview.redd.it/QLQU1soiMTzFAm8GzW6EPDbaX5jrcYYFqy1ql5NYoiQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4c3b97e1405ebb7916bf71d7b9a3da9a44efaea7" title="Mistral Vibe 2.0" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looks like I missed Mistral Vibe 2.0 being announced because I‚Äôve been busy with OpenCode.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://mistral.ai/news/mistral-vibe-2-0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qt76qs/mistral_vibe_20/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qt76qs/mistral_vibe_20/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-01T18:56:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpewj7</id>
    <title>AMA With Kimi, The Open-source Frontier Lab Behind Kimi K2.5 Model</title>
    <updated>2026-01-28T15:46:40+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"&gt; &lt;img alt="AMA With Kimi, The Open-source Frontier Lab Behind Kimi K2.5 Model" src="https://b.thumbs.redditmedia.com/Tdp5mXDQmwe_e0sKIEY1hjWX6FrN679odChZp8YL2Rk.jpg" title="AMA With Kimi, The Open-source Frontier Lab Behind Kimi K2.5 Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Today we are having &lt;strong&gt;Kimi&lt;/strong&gt;, the research lab behind the &lt;strong&gt;Kimi&lt;/strong&gt; &lt;strong&gt;K2.5&lt;/strong&gt;. We‚Äôre excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/ComfortableAsk4494/"&gt;u/ComfortableAsk4494&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/zxytim/"&gt;u/zxytim&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/ppwwyyxx/"&gt;u/ppwwyyxx&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 8 AM ‚Äì 11 AM PST, with the Kimi team continuing to follow up on questions over the next 24 hours.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3yq8msvp24gg1.png?width=2000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=98c89b5d86ee1197799532fead6a84da2223b389"&gt;https://preview.redd.it/3yq8msvp24gg1.png?width=2000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=98c89b5d86ee1197799532fead6a84da2223b389&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Thanks everyone for joining our AMA. The live part has ended and the Kimi team will be following up with more answers sporadically over the next 24 hours.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T15:46:40+00:00</published>
  </entry>
</feed>
