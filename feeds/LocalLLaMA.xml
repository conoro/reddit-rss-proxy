<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-28T13:31:09+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1pwybg6</id>
    <title>llama.cpp, experimental native mxfp4 support for blackwell (25% preprocessing speedup!)</title>
    <updated>2025-12-27T13:52:16+00:00</updated>
    <author>
      <name>/u/bfroemel</name>
      <uri>https://old.reddit.com/user/bfroemel</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/17906"&gt;https://github.com/ggml-org/llama.cpp/pull/17906&lt;/a&gt;&lt;/p&gt; &lt;p&gt;love that kind of evolution:&lt;/p&gt; &lt;p&gt;&amp;gt; at the moment this PR is &lt;del&gt;10%&lt;/del&gt; &lt;em&gt;&lt;del&gt;slower&lt;/del&gt;&lt;/em&gt; &lt;del&gt;than master&lt;/del&gt; 25% faster than master on PP.&lt;/p&gt; &lt;p&gt;&amp;gt; To compile &lt;code&gt;-DCMAKE_CUDA_ARCHITECTURES=&amp;quot;120f&amp;quot;&lt;/code&gt; is required.&lt;/p&gt; &lt;p&gt;probably/currently most useful for gpt-oss models! (also while reading the PR it seems that we might see more native nvfp4 support soon!)&lt;/p&gt; &lt;p&gt;Thanks to &lt;a href="/u/am17an"&gt;u/am17an&lt;/a&gt; (PR author) &amp;amp; llama.cpp devs!!&lt;/p&gt; &lt;p&gt;/edit: better point that also out (although, so far I am not noticing any quality degradation with gpt-oss-120b!):&lt;br /&gt; &amp;gt; [..] we quantize activation to mxfp4 instead of q8, which lead to failures in &lt;code&gt;test-backend-ops&lt;/code&gt;, however PPL tests are okay with this change (though not ruling out correctness issues)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bfroemel"&gt; /u/bfroemel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwybg6/llamacpp_experimental_native_mxfp4_support_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwybg6/llamacpp_experimental_native_mxfp4_support_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwybg6/llamacpp_experimental_native_mxfp4_support_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-27T13:52:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1pxl38x</id>
    <title>New to this - are dual 3090s w/ Nvlink worth it?</title>
    <updated>2025-12-28T07:08:58+00:00</updated>
    <author>
      <name>/u/Cferra</name>
      <uri>https://old.reddit.com/user/Cferra</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking to build a local ai box - I had a 3090 and I bought another one locally.&lt;/p&gt; &lt;p&gt;Right now my original 3090 is water cooled and I planned to do the same for the other for an available Nvlink bridge (four slot is like impossible to find for a reasonable price)&lt;/p&gt; &lt;p&gt;Is this the right road to go down or should I look at something else?&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cferra"&gt; /u/Cferra &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxl38x/new_to_this_are_dual_3090s_w_nvlink_worth_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxl38x/new_to_this_are_dual_3090s_w_nvlink_worth_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pxl38x/new_to_this_are_dual_3090s_w_nvlink_worth_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-28T07:08:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1pxflg0</id>
    <title>Trillions parameters models ?</title>
    <updated>2025-12-28T02:22:25+00:00</updated>
    <author>
      <name>/u/Highwaytothebeach</name>
      <uri>https://old.reddit.com/user/Highwaytothebeach</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just found out that Linux supports 178 terabytes of RAM if such CPU that supports that much RAM would ever exist ( with some linux versions even in petabytes). Therefore , theoretically 100 trillion parameters MOE model would be possible to run with quite a lot of t/s speed ( if just some experts activated). I just wonder, what would 100 trillion parameters MOE model be useful for? Would it be 150 times more powerful or smarter in some way than 600 billion parameters models like Deepseek or similar?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Highwaytothebeach"&gt; /u/Highwaytothebeach &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxflg0/trillions_parameters_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxflg0/trillions_parameters_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pxflg0/trillions_parameters_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-28T02:22:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1pxmt38</id>
    <title>What are the new approach on the multi modal RAG currently?</title>
    <updated>2025-12-28T08:56:49+00:00</updated>
    <author>
      <name>/u/ExchangePersonal1384</name>
      <uri>https://old.reddit.com/user/ExchangePersonal1384</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;What are the new approach on the multi modal RAG currently?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;What are the new approach on the multi modal RAG currently? That includes Images, Audio, Videos and Texts.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ExchangePersonal1384"&gt; /u/ExchangePersonal1384 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxmt38/what_are_the_new_approach_on_the_multi_modal_rag/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxmt38/what_are_the_new_approach_on_the_multi_modal_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pxmt38/what_are_the_new_approach_on_the_multi_modal_rag/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-28T08:56:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1pxai05</id>
    <title>RPC-server llama.cpp benchmarks</title>
    <updated>2025-12-27T22:28:25+00:00</updated>
    <author>
      <name>/u/tabletuser_blogspot</name>
      <uri>https://old.reddit.com/user/tabletuser_blogspot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Running a few LLM benchmarks to see how RPC-server is doing.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;The llama.cpp RPC server is a tool that allows for distributed inference of large language models (LLMs) across multiple machines or GPUs by offloading computations to remote instances.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Benchmarks done on local gigabit network across 3 systems and 5 GPUs.&lt;/p&gt; &lt;p&gt;System 1: AMD FX-8350 CPU, 32GB DDR3, CachyOS Kernel 6.18.2-1 on KDE, &lt;a href="https://www.techpowerup.com/gpu-specs/geforce-gtx-1080-ti.c2877"&gt;GTX 1080Ti&lt;/a&gt; (11GB) and&lt;a href="https://www.techpowerup.com/gpu-specs/p102-101.c3284"&gt; Nvidia P102-100&lt;/a&gt; (10GB) 21GB VRAM GPUs.&lt;/p&gt; &lt;p&gt;System 2: Intel i7 7800X CPU, 48GB DDR4, Kubuntu 26.04 Kernel 6.17, GTX 1080Ti (11GB) and Nvidia P102-100 (10GB) 21GB VRAM GPUs.&lt;/p&gt; &lt;p&gt;System 3: AMD Ryzen 5 5600X, 64GB DDR4, Kubuntu 24.04 Kernel 6.14, Radeon &lt;a href="https://www.techpowerup.com/gpu-specs/radeon-rx-7900-gre.c4166"&gt;RX 7900 GRE&lt;/a&gt; (16GB)&lt;/p&gt; &lt;p&gt;Total 57GB of VRAM available on local gigabit network.&lt;/p&gt; &lt;p&gt;Llama.cpp Ubuntu Vulkan build: 06705fdcb (&lt;a href="https://github.com/ggml-org/llama.cpp/releases/download/b7552/llama-b7552-bin-ubuntu-vulkan-x64.tar.gz"&gt;7552&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;&lt;code&gt;time /llama-bench --rpc 10.0.0.51:50051,10.0.0.173:50053 -m /&lt;/code&gt;&lt;br /&gt; &lt;code&gt;Nemotron-3-Nano-30B-A3B-Q6_K.gguf&lt;/code&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;load_backend: loaded RPC backend from /media/czar33/x_2tb/vulkan/llama-b7552/libggml-rpc.so ggml_vulkan: Found 1 Vulkan devices: ggml_vulkan: 0 = AMD Radeon RX 7900 GRE (RADV NAVI31) (radv) | uma: 0 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 1 | matrix cores: KHR_coopmat load_backend: loaded Vulkan backend from /media/czar33/x_2tb/vulkan/llama-b7552/libggml-vulkan.so load_backend: loaded CPU backend from /media/czar33/x_2tb/vulkan/llama-b7552/libggml-cpu-haswell.so &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Nemotron-3-Nano-30B-A3B-Q6_K&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;nemotron_h_moe 31B.A3.5B Q6_K&lt;/td&gt; &lt;td align="left"&gt;31.20 GiB&lt;/td&gt; &lt;td align="left"&gt;31.58 B&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;165.15 ± 12.19&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;nemotron_h_moe 31B.A3.5B Q6_K&lt;/td&gt; &lt;td align="left"&gt;31.20 GiB&lt;/td&gt; &lt;td align="left"&gt;31.58 B&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;51.05 ± 0.87&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Phi-3.5-MoE-instruct-Q6_K_L&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;phimoe 16x3.8B Q6_K&lt;/td&gt; &lt;td align="left"&gt;32.06 GiB&lt;/td&gt; &lt;td align="left"&gt;41.87 B&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;79.18 ± 4.69&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;phimoe 16x3.8B Q6_K&lt;/td&gt; &lt;td align="left"&gt;32.06 GiB&lt;/td&gt; &lt;td align="left"&gt;41.87 B&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;37.83 ± 2.20&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;DeepSeek-R1-Distill-Llama-70B-UD-Q3_K_XL&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 70B Q3_K - Medium&lt;/td&gt; &lt;td align="left"&gt;32.47 GiB&lt;/td&gt; &lt;td align="left"&gt;70.55 B&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;37.30 ± 0.66&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 70B Q3_K - Medium&lt;/td&gt; &lt;td align="left"&gt;32.47 GiB&lt;/td&gt; &lt;td align="left"&gt;70.55 B&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;3.80 ± 0.06&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Qwen3-Next-80B-A3B-Instruct-UD-Q4_K_XL&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3next 80B.A3B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;42.01 GiB&lt;/td&gt; &lt;td align="left"&gt;79.67 B&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;44.95 ± 0.81&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3next 80B.A3B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;42.01 GiB&lt;/td&gt; &lt;td align="left"&gt;79.67 B&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;9.00 ± 3.65&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Cerebras_GLM-4.5-Air-REAP-82B-A12B-IQ4_XS&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;glm4moe 106B.A12B IQ4_XS - 4.25 bpw&lt;/td&gt; &lt;td align="left"&gt;43.75 GiB&lt;/td&gt; &lt;td align="left"&gt;84.99 B&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;45.05 ± 1.28&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;glm4moe 106B.A12B IQ4_XS - 4.25 bpw&lt;/td&gt; &lt;td align="left"&gt;43.75 GiB&lt;/td&gt; &lt;td align="left"&gt;84.99 B&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;19.84 ± 0.50&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Llama-4-Scout-17B-16E-Instruct-IQ3_XXS&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;llama4 17Bx16E (Scout) IQ3_XXS - 3.0625 bpw&lt;/td&gt; &lt;td align="left"&gt;41.86 GiB&lt;/td&gt; &lt;td align="left"&gt;107.77 B&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;88.90 ± 5.17&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama4 17Bx16E (Scout) IQ3_XXS - 3.0625 bpw&lt;/td&gt; &lt;td align="left"&gt;41.86 GiB&lt;/td&gt; &lt;td align="left"&gt;107.77 B&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;4.67 ± 1.15&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Mixtral-8x22B-v0.1.i1-IQ2_M&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 8x22B IQ2_M - 2.7 bpw&lt;/td&gt; &lt;td align="left"&gt;43.50 GiB&lt;/td&gt; &lt;td align="left"&gt;140.62 B&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;23.70 ± 0.55&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 8x22B IQ2_M - 2.7 bpw&lt;/td&gt; &lt;td align="left"&gt;43.50 GiB&lt;/td&gt; &lt;td align="left"&gt;140.62 B&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;9.14 ± 0.79&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tabletuser_blogspot"&gt; /u/tabletuser_blogspot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxai05/rpcserver_llamacpp_benchmarks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxai05/rpcserver_llamacpp_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pxai05/rpcserver_llamacpp_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-27T22:28:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwwsag</id>
    <title>The Infinite Software Crisis: We're generating complex, unmaintainable code faster than we can understand it. Is 'vibe-coding' the ultimate trap?</title>
    <updated>2025-12-27T12:33:15+00:00</updated>
    <author>
      <name>/u/madSaiyanUltra_9789</name>
      <uri>https://old.reddit.com/user/madSaiyanUltra_9789</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwwsag/the_infinite_software_crisis_were_generating/"&gt; &lt;img alt="The Infinite Software Crisis: We're generating complex, unmaintainable code faster than we can understand it. Is 'vibe-coding' the ultimate trap?" src="https://b.thumbs.redditmedia.com/CWbH9aocZDksMFttENvwXtHE-6zGD_eJPUb93Q-jv0M.jpg" title="The Infinite Software Crisis: We're generating complex, unmaintainable code faster than we can understand it. Is 'vibe-coding' the ultimate trap?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, &lt;/p&gt; &lt;p&gt;I just watched &lt;a href="https://www.youtube.com/watch?v=eIoohUmYpGI"&gt;The Infinite Software Crisis – Jake Nations&lt;/a&gt; on YouTube and it got me thinking... the limitations of software development has never been typing speed, but rather our ability to comprehend and design the system correctly in the first place. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Highlights from the talk:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Every developer has shipped code they didn't completely understand. it passed the tests and that was enough validation to deploy it.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The hard part is timeless:&lt;/strong&gt; The hard part isn't the mechanics of coding; it's the conceptual difficulty of designing a solution. Every tool, including AI, just makes implementation easier.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;AI amplifies the problem:&lt;/strong&gt; We can now generate code as fast as we can describe it. The scale is infinite, but our comprehension isn't. The core challenge of understanding &lt;em&gt;what&lt;/em&gt; to build remains.&lt;/li&gt; &lt;li&gt;The real trap we fall into is confusing easy with simple. &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Easy&lt;/strong&gt; is what's within reach. What can you access without effort? Generate it with AI, copy-paste, or install a framework. It's about speed.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Simple&lt;/strong&gt; is about structure. It means one fold, one braid, no entanglement. It requires thought and design.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;LLMs do not understand logic, they merely relate language and substitute those relations as &amp;quot;code&amp;quot;, so the importance of &lt;em&gt;patterns and architectural decisions&lt;/em&gt; in your codebase are lost. &lt;/li&gt; &lt;li&gt;when &amp;quot;vibe-coding&amp;quot; technical debt doesn't register as debt; it's just more code to preserve. &lt;/li&gt; &lt;li&gt;The result? Complex, highly-coupled, and error-prone code generated in minutes that could take you weeks to understand (if ever).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The real danger here is that we're accumulating complexity faster than we can comprehend it because we're not doing the hard work of understanding our systems.&lt;/p&gt; &lt;p&gt;The proposed solution: SLOW DOWN, DO EVERYTHING MANUALLY; architectural design + scaffolding, etc and only let the LLM in at the last step of filling in the scaffolding. &lt;/p&gt; &lt;p&gt;What's your take, Is 'vibe-coding' a trap, or is there a way to use these tools without losing the ability to understand our systems? &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/c4mknoudlq9g1.png?width=553&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=28a6f37623fb0e0725f5b603f4b3a8ce51653ac9"&gt;https://preview.redd.it/c4mknoudlq9g1.png?width=553&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=28a6f37623fb0e0725f5b603f4b3a8ce51653ac9&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/madSaiyanUltra_9789"&gt; /u/madSaiyanUltra_9789 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwwsag/the_infinite_software_crisis_were_generating/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwwsag/the_infinite_software_crisis_were_generating/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwwsag/the_infinite_software_crisis_were_generating/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-27T12:33:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1pxbgyf</id>
    <title>How to get SOTA opensource models (GLM 4.7, Kimi K2) to do multistep coding automatically? On Claude Code? They keep stopping after 2 or 3 steps...</title>
    <updated>2025-12-27T23:11:47+00:00</updated>
    <author>
      <name>/u/FigZestyclose7787</name>
      <uri>https://old.reddit.com/user/FigZestyclose7787</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Honest question: How do you guys get K2, GLM 4.7 models to work automatically on Coding AGents such as Claude Code? I know these models are plenty powerful, and may work well on other harnesses (roocode, cline, etc) but on Claude Code they hang... on need me to type continue after every 2 or 3 interactions, making it completely unusable. What has your experience been like? Is there a configuration I'm missing? (I'm using Claude Code Router, and or a custom built endpoint/ api key injector). &lt;/p&gt; &lt;p&gt;I really want to give these models a fair try but I simply can't make it work well enough. Glm 4.7 is slightly better than k2 for multistep iteration, but it also stops after a few steps. &lt;/p&gt; &lt;p&gt;So far, only minimax m2.1 (not 2) has worked well enough to get to completion of tasks on its own. &lt;/p&gt; &lt;p&gt;But I'm sure there's wisdom out there that I might be missing. Please share your tips and experience. Tks. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FigZestyclose7787"&gt; /u/FigZestyclose7787 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxbgyf/how_to_get_sota_opensource_models_glm_47_kimi_k2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxbgyf/how_to_get_sota_opensource_models_glm_47_kimi_k2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pxbgyf/how_to_get_sota_opensource_models_glm_47_kimi_k2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-27T23:11:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1pxb6oo</id>
    <title>China issues draft rules to regulate AI with human-like interaction.</title>
    <updated>2025-12-27T22:59:09+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxb6oo/china_issues_draft_rules_to_regulate_ai_with/"&gt; &lt;img alt="China issues draft rules to regulate AI with human-like interaction." src="https://external-preview.redd.it/UOe8-3-UzVIzXY9pBpuX6xxhZKsMl0kFOZlkgCirjwM.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9193d747727ddaf97e85a229c20398a79d44528d" title="China issues draft rules to regulate AI with human-like interaction." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wonder if this will have any impact on all the models coming out of China.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reuters.com/world/asia-pacific/china-issues-drafts-rules-regulate-ai-with-human-like-interaction-2025-12-27/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxb6oo/china_issues_draft_rules_to_regulate_ai_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pxb6oo/china_issues_draft_rules_to_regulate_ai_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-27T22:59:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwyw36</id>
    <title>MiniMaxAI/MiniMax-M2.1 seems to be the strongest model per param</title>
    <updated>2025-12-27T14:19:07+00:00</updated>
    <author>
      <name>/u/SlowFail2433</name>
      <uri>https://old.reddit.com/user/SlowFail2433</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Going by the Artifical Analysis benchaes, MiniMaxAI/MiniMax-M2.1 can compete with Kimi K2 Thinking, Deepseek 3.2 and GLM 4.7 in performance.&lt;/p&gt; &lt;p&gt;But what feels especially notable is that MiniMaxAI/MiniMax-M2.1 is only 229B param which is around half of GLM 4.7, around a third of Deepseek 3.2 and around a fifth of Kimi K2 Thinking&lt;/p&gt; &lt;p&gt;What this means is that MiniMaxAI/MiniMax-M2.1 seems to be the best value model now&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SlowFail2433"&gt; /u/SlowFail2433 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwyw36/minimaxaiminimaxm21_seems_to_be_the_strongest/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwyw36/minimaxaiminimaxm21_seems_to_be_the_strongest/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwyw36/minimaxaiminimaxm21_seems_to_be_the_strongest/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-27T14:19:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1pxo97r</id>
    <title>Llama.cpp (or lmstudio) in LXC (proxmox) on 395 (framework desktop)</title>
    <updated>2025-12-28T10:29:48+00:00</updated>
    <author>
      <name>/u/El_90</name>
      <uri>https://old.reddit.com/user/El_90</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Does anyone know if this works?&lt;/p&gt; &lt;p&gt;I see:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;llama.cpp works in LXC&lt;/li&gt; &lt;li&gt;llama.cpp works in LXC using Nvidia/CUDA&lt;/li&gt; &lt;li&gt;llama.cpp works on AMD 395 (there's a whole thread about vulcan vs rcom which I'm still reading upon)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;But I don't see threads/content about them working in this specific combination in the title (and my years in IT have taught me that the devil is always in the detail when combining tech)&lt;/p&gt; &lt;p&gt;I saw some content about the 'llama.cpp in LXC using Nvidia/CUDA' requiring additional drivers on both the host AND in the container that I didn't understand. I assumed you would have it in kernel OR userspace, I'm confused by that, and thus I thought it worth checking the field if anyone has knowledge on my combo.&lt;/p&gt; &lt;p&gt;Many thanks!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/El_90"&gt; /u/El_90 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxo97r/llamacpp_or_lmstudio_in_lxc_proxmox_on_395/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxo97r/llamacpp_or_lmstudio_in_lxc_proxmox_on_395/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pxo97r/llamacpp_or_lmstudio_in_lxc_proxmox_on_395/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-28T10:29:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1pxr47l</id>
    <title>MCP servers are hard to debug and impossible to test, so I built Syrin</title>
    <updated>2025-12-28T13:16:33+00:00</updated>
    <author>
      <name>/u/hack_the_developer</name>
      <uri>https://old.reddit.com/user/hack_the_developer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, &lt;/p&gt; &lt;p&gt;I’ve been building MCP servers and kept running into the same issues:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;No visibility into why an LLM picked a tool&lt;/li&gt; &lt;li&gt;Tool calls looping or failing silently&lt;/li&gt; &lt;li&gt;No deterministic way to test MCP behaviour&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So I built &lt;strong&gt;Syrin,&lt;/strong&gt; a local-first &lt;strong&gt;CLI debugger and test runner for MCP servers&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it does (v1.0.0):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;CLI commands: &lt;code&gt;syrin init&lt;/code&gt;, &lt;code&gt;doctor&lt;/code&gt;, &lt;code&gt;test&lt;/code&gt;, &lt;code&gt;list&lt;/code&gt;, &lt;code&gt;dev&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Full MCP protocol support (tools, resources, prompts, validation)&lt;/li&gt; &lt;li&gt;Multi-LLM support: OpenAI, Claude, Ollama (auto-manages Ollama)&lt;/li&gt; &lt;li&gt;Safe-by-default execution (preview mode + full event tracing)&lt;/li&gt; &lt;li&gt;YAML config, HTTP + stdio transport&lt;/li&gt; &lt;li&gt;TypeScript, npm package, npx-friendly&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What I’m working on next:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Deterministic &lt;strong&gt;unit tests for tools&lt;/strong&gt; (was it called? with what args?)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Workflow testing&lt;/strong&gt; for multi-step tool chains with dependencies&lt;/li&gt; &lt;li&gt;Assertions on runtime events, not model text&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/ankan-labs/syrin"&gt;&lt;strong&gt;https://github.com/ankan-labs/syrin&lt;/strong&gt;&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;NPM:&lt;/strong&gt; &lt;a href="https://www.npmjs.com/package/@ankan-ai/syrin"&gt;&lt;strong&gt;https://www.npmjs.com/package/@ankan-ai/syrin&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you’re building MCP servers, I’d love feedback or contributors.&lt;br /&gt; If this is the wrong approach, tell me why.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hack_the_developer"&gt; /u/hack_the_developer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxr47l/mcp_servers_are_hard_to_debug_and_impossible_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxr47l/mcp_servers_are_hard_to_debug_and_impossible_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pxr47l/mcp_servers_are_hard_to_debug_and_impossible_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-28T13:16:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1pxp9rg</id>
    <title>Seeking advice from developers building apps with ML/DL integration</title>
    <updated>2025-12-28T11:32:29+00:00</updated>
    <author>
      <name>/u/hemahariharansamson</name>
      <uri>https://old.reddit.com/user/hemahariharansamson</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I am planning to build apps and websites that solve real-world problems. My goal is not just to create normal CRUD or UI-focused apps, but also to gradually integrate my own machine learning and deep learning models into these products and services.&lt;/p&gt; &lt;p&gt;I’ve been experimenting with AI-assisted development tools like Cursor to speed up design and coding, but I want to learn from the community about what works best in practice.&lt;/p&gt; &lt;p&gt;I’d love to hear from you:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;What is your go-to AI tool for development like Cursor?&lt;/li&gt; &lt;li&gt;What subscription plan or setup do you use?&lt;/li&gt; &lt;li&gt;Any tips for integrating custom ML/DL models into real apps?&lt;/li&gt; &lt;li&gt;Recommended tech stacks, workflows, or common pitfalls for beginners building production-ready apps&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Looking forward to your advice. Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hemahariharansamson"&gt; /u/hemahariharansamson &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxp9rg/seeking_advice_from_developers_building_apps_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxp9rg/seeking_advice_from_developers_building_apps_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pxp9rg/seeking_advice_from_developers_building_apps_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-28T11:32:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1pxl6c9</id>
    <title>[Strix Halo] Unable to load 120B model on Ryzen AI Max+ 395 (128GB RAM) - "Unable to allocate ROCm0 buffer"</title>
    <updated>2025-12-28T07:14:06+00:00</updated>
    <author>
      <name>/u/Wrong-Policy-5612</name>
      <uri>https://old.reddit.com/user/Wrong-Policy-5612</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxl6c9/strix_halo_unable_to_load_120b_model_on_ryzen_ai/"&gt; &lt;img alt="[Strix Halo] Unable to load 120B model on Ryzen AI Max+ 395 (128GB RAM) - &amp;quot;Unable to allocate ROCm0 buffer&amp;quot;" src="https://b.thumbs.redditmedia.com/BfYcjvJg4-Xby41gb5LwhAkWAi-0QHFoSnniKNs2jAY.jpg" title="[Strix Halo] Unable to load 120B model on Ryzen AI Max+ 395 (128GB RAM) - &amp;quot;Unable to allocate ROCm0 buffer&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I am running a Ryzen AI Max+ 395 (Strix Halo) with 128 GB of RAM. I have set my BIOS/Driver &amp;quot;Variable Graphics Memory&amp;quot; (VGM) to High, so Windows reports 96 GB Dedicated VRAM and ~32 GB System RAM.&lt;/p&gt; &lt;p&gt;I am trying to load gpt-oss-120b-Q4_K_M.gguf (approx 64 GB) in LM Studio 0.3.36.&lt;/p&gt; &lt;p&gt;The Issue: No matter what settings I try, I get an allocation error immediately upon loading: error loading model: unable to allocate ROCm0 buffer (I also tried Vulkan and got unable to allocate Vulkan0 buffer).&lt;/p&gt; &lt;p&gt;My Settings:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Model: gpt-oss-120b-Q4_K_M.gguf (63.66 GB)&lt;/li&gt; &lt;li&gt;Engine: ROCm / Vulkan (Tried both)&lt;/li&gt; &lt;li&gt;Context Length: Reduced to 8192 (and even 2048)&lt;/li&gt; &lt;li&gt;GPU Offload: Max (36/36) and Partial (30/36)&lt;/li&gt; &lt;li&gt;mmap: OFF (Crucial, otherwise it checks system RAM)&lt;/li&gt; &lt;li&gt;Flash Attention: OFF&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/t06q2wcoaw9g1.png?width=1038&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0e118bd60a96faac9195d52d02b158fde0e39fab"&gt;https://preview.redd.it/t06q2wcoaw9g1.png?width=1038&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0e118bd60a96faac9195d52d02b158fde0e39fab&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Observations:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The VRAM usage graph shows it loads about 25% (24GB) and then crashes.&lt;/li&gt; &lt;li&gt;It seems like the Windows driver refuses to allocate a single large contiguous chunk, even though I have 96 GB empty VRAM.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Has anyone with Strix Halo or high-VRAM AMD cards (7900 XTX) encountered this buffer limit on Windows? Do I need a specific boot flag or driver setting to allow &amp;gt;24GB allocations?&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wrong-Policy-5612"&gt; /u/Wrong-Policy-5612 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxl6c9/strix_halo_unable_to_load_120b_model_on_ryzen_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxl6c9/strix_halo_unable_to_load_120b_model_on_ryzen_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pxl6c9/strix_halo_unable_to_load_120b_model_on_ryzen_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-28T07:14:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1pxbg4x</id>
    <title>SOCAMM2 - new(ish), screwable (replaceable, non soldered) LPDDR5X RAM standard intended for AI data centers.</title>
    <updated>2025-12-27T23:10:47+00:00</updated>
    <author>
      <name>/u/-InformalBanana-</name>
      <uri>https://old.reddit.com/user/-InformalBanana-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Samsung introduces SOCAMM2 LPDDR5X memory module for AI data centers — new standard set to offer reduced power consumption and &lt;strong&gt;double the bandwidth&lt;/strong&gt; versus DDR5 RDIMMs.&lt;/p&gt; &lt;p&gt;The SOCAMM2 LPDDR5X-based module is being positioned as a standardized, serviceable alternative to soldered memory as AI servers chase higher bandwidth.&lt;/p&gt; &lt;p&gt;Hopefully this gets represented and used more in the consumer market.&lt;/p&gt; &lt;p&gt;More info:&lt;/p&gt; &lt;p&gt;&lt;a href="https://semiconductor.samsung.com/news-events/tech-blog/introducing-samsungs-socamm2-new-lpddr-memory-module-empowering-next-generation-ai-infrastructure/"&gt;https://semiconductor.samsung.com/news-events/tech-blog/introducing-samsungs-socamm2-new-lpddr-memory-module-empowering-next-generation-ai-infrastructure/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.tomshardware.com/tech-industry/samsung-introduces-socamm2-lpddr5x-memory-module-for-ai-data-centers"&gt;https://www.tomshardware.com/tech-industry/samsung-introduces-socamm2-lpddr5x-memory-module-for-ai-data-centers&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-InformalBanana-"&gt; /u/-InformalBanana- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxbg4x/socamm2_newish_screwable_replaceable_non_soldered/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxbg4x/socamm2_newish_screwable_replaceable_non_soldered/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pxbg4x/socamm2_newish_screwable_replaceable_non_soldered/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-27T23:10:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1px940g</id>
    <title>Running MiniMax-M2.1 Locally with Claude Code and vLLM on Dual RTX Pro 6000</title>
    <updated>2025-12-27T21:28:12+00:00</updated>
    <author>
      <name>/u/zmarty</name>
      <uri>https://old.reddit.com/user/zmarty</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Run Claude Code with your own local MiniMax-M2.1 model using vLLM's native Anthropic API endpoint support.&lt;/p&gt; &lt;h2&gt;Hardware Used&lt;/h2&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Component&lt;/th&gt; &lt;th&gt;Specification&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;CPU&lt;/td&gt; &lt;td&gt;AMD Ryzen 9 7950X3D 16-Core Processor&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Motherboard&lt;/td&gt; &lt;td&gt;ROG CROSSHAIR X670E HERO&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;GPU&lt;/td&gt; &lt;td&gt;Dual NVIDIA RTX Pro 6000 (96 GB VRAM each)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RAM&lt;/td&gt; &lt;td&gt;192 GB DDR5 5200 (note the model does not use the RAM, it fits into VRAM entirely)&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;hr /&gt; &lt;h2&gt;Install vLLM Nightly&lt;/h2&gt; &lt;p&gt;&lt;strong&gt;Prerequisite:&lt;/strong&gt; &lt;a href="https://forum.level1techs.com/t/wip-blackwell-rtx-6000-pro-max-q-quickie-setup-guide-on-ubuntu-24-04-lts-25-04/230521"&gt;Ubuntu 24.04 and the proper NVIDIA drivers&lt;/a&gt;&lt;/p&gt; &lt;p&gt;```bash mkdir vllm-nightly cd vllm-nightly uv venv --python 3.12 --seed source .venv/bin/activate&lt;/p&gt; &lt;p&gt;uv pip install -U vllm \ --torch-backend=auto \ --extra-index-url &lt;a href="https://wheels.vllm.ai/nightly"&gt;https://wheels.vllm.ai/nightly&lt;/a&gt; ```&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Download MiniMax-M2.1&lt;/h2&gt; &lt;p&gt;Set up a separate environment for downloading models:&lt;/p&gt; &lt;p&gt;```bash mkdir /models cd /models uv venv --python 3.12 --seed source .venv/bin/activate&lt;/p&gt; &lt;p&gt;pip install huggingface_hub ```&lt;/p&gt; &lt;p&gt;Download the AWQ-quantized MiniMax-M2.1 model:&lt;/p&gt; &lt;p&gt;&lt;code&gt;bash mkdir /models/awq huggingface-cli download cyankiwi/MiniMax-M2.1-AWQ-4bit \ --local-dir /models/awq/cyankiwi-MiniMax-M2.1-AWQ-4bit &lt;/code&gt;&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Start vLLM Server&lt;/h2&gt; &lt;p&gt;From your vLLM environment, launch the server with the Anthropic-compatible endpoint:&lt;/p&gt; &lt;p&gt;```bash cd ~/vllm-nightly source .venv/bin/activate&lt;/p&gt; &lt;p&gt;vllm serve \ /models/awq/cyankiwi-MiniMax-M2.1-AWQ-4bit \ --served-model-name MiniMax-M2.1-AWQ \ --max-num-seqs 10 \ --max-model-len 128000 \ --gpu-memory-utilization 0.95 \ --tensor-parallel-size 2 \ --pipeline-parallel-size 1 \ --enable-auto-tool-choice \ --tool-call-parser minimax_m2 \ --reasoning-parser minimax_m2_append_think \ --trust-remote-code \ --host 0.0.0.0 \ --port 8000 ```&lt;/p&gt; &lt;p&gt;The server exposes &lt;code&gt;/v1/messages&lt;/code&gt; (Anthropic-compatible) at &lt;code&gt;http://localhost:8000&lt;/code&gt;.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Install Claude Code&lt;/h2&gt; &lt;p&gt;Install Claude Code on macOS, Linux, or WSL:&lt;/p&gt; &lt;p&gt;&lt;code&gt;bash curl -fsSL https://claude.ai/install.sh | bash &lt;/code&gt;&lt;/p&gt; &lt;p&gt;See the &lt;a href="https://code.claude.com/docs/en/overview"&gt;official Claude Code documentation&lt;/a&gt; for more details.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Configure Claude Code&lt;/h2&gt; &lt;h3&gt;Create settings.json&lt;/h3&gt; &lt;p&gt;Create or edit &lt;code&gt;~/.claude/settings.json&lt;/code&gt;:&lt;/p&gt; &lt;p&gt;&lt;code&gt;json { &amp;quot;env&amp;quot;: { &amp;quot;ANTHROPIC_BASE_URL&amp;quot;: &amp;quot;http://localhost:8000&amp;quot;, &amp;quot;ANTHROPIC_AUTH_TOKEN&amp;quot;: &amp;quot;dummy&amp;quot;, &amp;quot;API_TIMEOUT_MS&amp;quot;: &amp;quot;3000000&amp;quot;, &amp;quot;CLAUDE_CODE_DISABLE_NONESSENTIAL_TRAFFIC&amp;quot;: &amp;quot;1&amp;quot;, &amp;quot;ANTHROPIC_MODEL&amp;quot;: &amp;quot;MiniMax-M2.1-AWQ&amp;quot;, &amp;quot;ANTHROPIC_SMALL_FAST_MODEL&amp;quot;: &amp;quot;MiniMax-M2.1-AWQ&amp;quot;, &amp;quot;ANTHROPIC_DEFAULT_SONNET_MODEL&amp;quot;: &amp;quot;MiniMax-M2.1-AWQ&amp;quot;, &amp;quot;ANTHROPIC_DEFAULT_OPUS_MODEL&amp;quot;: &amp;quot;MiniMax-M2.1-AWQ&amp;quot;, &amp;quot;ANTHROPIC_DEFAULT_HAIKU_MODEL&amp;quot;: &amp;quot;MiniMax-M2.1-AWQ&amp;quot; } } &lt;/code&gt;&lt;/p&gt; &lt;h3&gt;Skip Onboarding (Workaround for Bug)&lt;/h3&gt; &lt;p&gt;Due to a &lt;a href="https://github.com/anthropics/claude-code/issues/13827"&gt;known bug in Claude Code 2.0.65+&lt;/a&gt;, fresh installs may ignore &lt;code&gt;settings.json&lt;/code&gt; during onboarding. Add &lt;code&gt;hasCompletedOnboarding&lt;/code&gt; to &lt;code&gt;~/.claude.json&lt;/code&gt;:&lt;/p&gt; &lt;p&gt;```bash&lt;/p&gt; &lt;h1&gt;If ~/.claude.json doesn't exist, create it:&lt;/h1&gt; &lt;p&gt;echo '{&amp;quot;hasCompletedOnboarding&amp;quot;: true}' &amp;gt; ~/.claude.json&lt;/p&gt; &lt;h1&gt;If it exists, add the field manually or use jq:&lt;/h1&gt; &lt;p&gt;jq '. + {&amp;quot;hasCompletedOnboarding&amp;quot;: true}' ~/.claude.json &amp;gt; tmp.json &amp;amp;&amp;amp; mv tmp.json ~/.claude.json ```&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Run Claude Code&lt;/h2&gt; &lt;p&gt;With vLLM running in one terminal, open another and run:&lt;/p&gt; &lt;p&gt;&lt;code&gt;bash claude &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Claude Code will now use your local MiniMax-M2.1 model! If you also want to configure the Claude Code VSCode extension, see &lt;a href="https://platform.minimax.io/docs/guides/text-ai-coding-tools#use-m2-1-in-claude-code-extension-for-vs-code"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;References&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/vllm-project/vllm/issues/21313"&gt;vLLM Anthropic API Support (GitHub Issue #21313)&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://platform.minimax.io/docs/guides/text-ai-coding-tools"&gt;MiniMax M2.1 for AI Coding Tools&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/cyankiwi/MiniMax-M2.1-AWQ-4bit"&gt;cyankiwi/MiniMax-M2.1-AWQ-4bit on Hugging Face&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Cross-posted from my blog: &lt;a href="https://www.ovidiudan.com/2025/12/27/running-claude-code-with-minimax-m2-1.html"&gt;Running MiniMax-M2.1 Locally with Claude Code on Dual RTX Pro 6000&lt;/a&gt; (I am not selling or promoting anything)&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zmarty"&gt; /u/zmarty &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1px940g/running_minimaxm21_locally_with_claude_code_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1px940g/running_minimaxm21_locally_with_claude_code_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1px940g/running_minimaxm21_locally_with_claude_code_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-27T21:28:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1px1c41</id>
    <title>Head of Engineering @MiniMax__AI on MiniMax M2 int4 QAT</title>
    <updated>2025-12-27T16:06:19+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1px1c41/head_of_engineering_minimax_ai_on_minimax_m2_int4/"&gt; &lt;img alt="Head of Engineering @MiniMax__AI on MiniMax M2 int4 QAT" src="https://preview.redd.it/1e9anmnmsr9g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7456cd2a6f5b63217ca62ea494cdbf87700184fa" title="Head of Engineering @MiniMax__AI on MiniMax M2 int4 QAT" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1e9anmnmsr9g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1px1c41/head_of_engineering_minimax_ai_on_minimax_m2_int4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1px1c41/head_of_engineering_minimax_ai_on_minimax_m2_int4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-27T16:06:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1pxl1k1</id>
    <title>I built a frontend for stable-diffusion.cpp for local image generation</title>
    <updated>2025-12-28T07:06:16+00:00</updated>
    <author>
      <name>/u/fabricio3g</name>
      <uri>https://old.reddit.com/user/fabricio3g</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built a Front End of stable-diffusion-cpp to run localy Z-Image Turbo on my old vulkan compatible integrated GPU using stable-diffusion.cpp. &lt;/p&gt; &lt;p&gt;The code is a messy but works for my needs. Some features aren’t fully tested due to my weak GPU. The project is open source and open to contributions.&lt;/p&gt; &lt;p&gt;Currently: Run with npm start&lt;/p&gt; &lt;p&gt;Windows build not working &lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/fabricio3g/FlaxeoUI"&gt;https://github.com/fabricio3g/FlaxeoUI&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fabricio3g"&gt; /u/fabricio3g &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxl1k1/i_built_a_frontend_for_stablediffusioncpp_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxl1k1/i_built_a_frontend_for_stablediffusioncpp_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pxl1k1/i_built_a_frontend_for_stablediffusioncpp_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-28T07:06:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1pxo9y5</id>
    <title>What non-Asian based models do you recommend at the end of 2025?</title>
    <updated>2025-12-28T10:31:00+00:00</updated>
    <author>
      <name>/u/thealliane96</name>
      <uri>https://old.reddit.com/user/thealliane96</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Background:&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;Building agentic stuff so tool calling has to be good (gpt oss has been the most reliable one in my, admittedly anecdotal, experience)&lt;/li&gt; &lt;li&gt;Work with and do work for certain organizations where I can’t:&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;- Use frontier models (or any hosted models for that matter)&lt;/p&gt; &lt;p&gt;- Use models released by Chinese, Taiwanese, etc based companies (maybe it’s dumb, okay it’s probably dumb, but unfortunately I don’t make the rules lol)&lt;/p&gt; &lt;p&gt;So I come to yall ask for your recommendations going into 2026.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note 1:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;I’m aware there’s some other similar posts but since they’re somewhat dated and a lot has happened since, I figured it wouldn’t be&lt;/em&gt; &lt;strong&gt;&lt;em&gt;too&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;egregious to throw mine up. Hope it’s okay &amp;lt;3&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note 2:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;While I am hoping to get recs for models I haven’t considered that will actually be effective, I’m also hoping just to find some new stuff to try regardless &amp;lt;3&lt;/em&gt;&lt;/p&gt; &lt;h1&gt;Models Tried&lt;/h1&gt; &lt;p&gt;- llama3.1 8B&lt;/p&gt; &lt;p&gt;- mistral Nemo&lt;/p&gt; &lt;p&gt;- Nemo fine tuned on my dataset&lt;/p&gt; &lt;p&gt;- mistral small 3.1 / 3.2 24b&lt;/p&gt; &lt;p&gt;- gpt-oss 20b and 120b&lt;/p&gt; &lt;p&gt;- several other mistral and devstral variants&lt;/p&gt; &lt;p&gt;- some phi models&lt;/p&gt; &lt;p&gt;- Gemma 3 27B (been so long and didn’t try it as much as the others)&lt;/p&gt; &lt;h1&gt;Unorganized Thoughts Regarding Models Tried&lt;/h1&gt; &lt;p&gt;From my experience testing them:&lt;/p&gt; &lt;p&gt;- All are generally good with raw text output (except Nemo, Nemo just sucks ass in my opinion)&lt;/p&gt; &lt;p&gt;- Tool calling wise **gpt-oss** is leagues ahead of all the others, at least in my experience using them&lt;/p&gt; &lt;p&gt;- llama3.1 8B is surprising good for raw text output and summarization and it has a oddly pleasing writing style? Maybe that’s just me.&lt;/p&gt; &lt;p&gt;- Mistral models in general never fail to be underwhelming for me. Quite liked Small 3.2, but when I slotted it into a (honestly) quite simple agent setup it got stuck in loops and would fuck up tool calls whereas gpt-oss-20b did it perfectly fine.&lt;/p&gt; &lt;p&gt;- devstral, mixtral, all those mistral variants I’ve found to also be incredibly underwhelming&lt;/p&gt; &lt;p&gt;- Phi models were, in my experience, utterly useless&lt;/p&gt; &lt;p&gt;- Gemma 3 honestly don’t remember, planning to try it out again soon&lt;/p&gt; &lt;h1&gt;On GPT-OSS&lt;/h1&gt; &lt;p&gt;While the answer is somewhat obviously “just use gpt oss”, there’s 2 negatives I find with it, neither are really deal breaking but they can be annoying plus sometimes you just want to try different stuff.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Negative 1:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I sometimes find it can maybe be a bit &lt;em&gt;too good&lt;/em&gt; at following instructions?&lt;/p&gt; &lt;p&gt;It’ll kind of, well, follow them to the letter including making things up to produce an output I’ve asked for.&lt;/p&gt; &lt;p&gt;I’ve gotten around this by instructing it to only output things it’s seen directly in tool results or directly from some external context it was given and that’s worked quite well but still.&lt;/p&gt; &lt;p&gt;It also suffers from what I like to call &lt;em&gt;context window snowballing&lt;/em&gt; where it gets stuck on one path and becomes very narrow minded (all the previous tokens influencing the next token basically, so without some type of intervention it’ll snowball down that same path).&lt;/p&gt; &lt;p&gt;Again I have ways getting around this where I’ll intentionally stop it after a certain percentage of the context window is full and then have it break down what it did and what the next steps should be and then I’ll throw that into a new run with a clear context window and instructing to rethink through the task and what it’s next steps should be. It’s a lot of work around but it works decently well.&lt;/p&gt; &lt;p&gt;I also haven’t found 120b to really be all that better than 20b, honestly sometimes 120b… kinda performs &lt;em&gt;worse&lt;/em&gt;?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Negative Number 2:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;For the work I’m doing I have to abliterate it (de-censor it).&lt;/p&gt; &lt;p&gt;It’d get stuck in a reasoning loop of trying to decide whether it could answer or not until eventually it’d just time out or I’d kill it. And what I’m asking it to do is not even against policy, it’s just been so heavily censored.&lt;/p&gt; &lt;p&gt;This isn’t that big of a deal as it’s been made quite easy by heretic, but still one of those annoyances where you just kind of wish you didn’t have to do it.&lt;/p&gt; &lt;p&gt;—-&lt;/p&gt; &lt;p&gt;Anyway enough of my rambling, anyone who read through it all, you’re a real one!&lt;/p&gt; &lt;h1&gt;TL;DR&lt;/h1&gt; &lt;p&gt;Can’t use models from either Chinese or other Asia-based companies/orgs.&lt;/p&gt; &lt;p&gt;Looking for recommendations for American/Canadian/European models that are good at tool calling that aren’t within the list of ones I’ve already tried.&lt;/p&gt; &lt;p&gt;Edit:&lt;/p&gt; &lt;p&gt;Guess markdown formatting isn’t supported on mobile lol&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thealliane96"&gt; /u/thealliane96 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxo9y5/what_nonasian_based_models_do_you_recommend_at/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxo9y5/what_nonasian_based_models_do_you_recommend_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pxo9y5/what_nonasian_based_models_do_you_recommend_at/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-28T10:31:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1pxj4lv</id>
    <title>Day 20: 21 Days of Building a Small Language Model: Activation Functions</title>
    <updated>2025-12-28T05:18:48+00:00</updated>
    <author>
      <name>/u/Prashant-Lakhera</name>
      <uri>https://old.reddit.com/user/Prashant-Lakhera</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Welcome to Day 20 of 21 Days of Building a Small Language Model. The topic for today is activation functions, the components that give neural networks their ability to learn complex, non-linear patterns. Yesterday we explored residual connections and how they enable deep networks. Today, we'll discover how activation functions work, why they're essential, and how modern choices like SwiGLU have become the standard in state-of-the-art language models.&lt;/p&gt; &lt;h1&gt;Why Activation Functions matter&lt;/h1&gt; &lt;p&gt;Before we dive into specific activation functions, let's understand why they're essential. A neural network without activation functions is just a series of matrix multiplications. No matter how many layers you stack, the result is always a linear transformation. This means the network can only learn linear relationships, which is extremely limiting.&lt;/p&gt; &lt;p&gt;Activation functions introduce non-linearity, allowing networks to learn complex patterns. They're what enable neural networks to approximate any function, recognize images, understand language, and solve problems that linear models cannot. Without activation functions, neural networks would be no more powerful than simple linear regression.&lt;/p&gt; &lt;p&gt;But not all activation functions are created equal. The choice of activation function affects training stability, convergence speed, gradient flow, and final model performance. This is why understanding activation functions is crucial for building effective language models.&lt;/p&gt; &lt;h1&gt;Evolution: From ReLU to SwiGLU&lt;/h1&gt; &lt;p&gt;The history of activation functions in deep learning shows a clear evolution toward smoother, more effective functions. Let's trace this journey:&lt;/p&gt; &lt;h1&gt;ReLU&lt;/h1&gt; &lt;p&gt;ReLU (Rectified Linear Unit) was the breakthrough that made deep learning practical. It's defined as:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;ReLU(x) = max(0, x)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;ReLU is simple: if the input is positive, output the input; if negative, output zero. This simplicity made it fast to compute and helped with the vanishing gradient problem that plagued earlier activation functions like sigmoid and tanh.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why ReLU worked:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Fast computation (just a max operation)&lt;/li&gt; &lt;li&gt;Helps with vanishing gradients (gradient is 1 for positive inputs)&lt;/li&gt; &lt;li&gt;Sparse activations (many neurons output zero, creating sparsity)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Limitations:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Dead neurons: Once a neuron outputs zero, it may never recover (dying ReLU problem)&lt;/li&gt; &lt;li&gt;Not smooth: The function has a sharp corner at zero, which can cause issues&lt;/li&gt; &lt;li&gt;Zero gradient for negative inputs: No learning happens for negative values&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;GELU&lt;/h1&gt; &lt;p&gt;GELU (Gaussian Error Linear Unit) addressed some of ReLU's limitations by being smooth and differentiable everywhere. It's defined as:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GELU(x) = x × Φ(x)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;where Φ(x) is the cumulative distribution function of the standard normal distribution. GELU is smooth, meaning it has no sharp corners, which helps with gradient flow.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why GELU became popular:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Smooth and differentiable everywhere&lt;/li&gt; &lt;li&gt;Better gradient flow than ReLU&lt;/li&gt; &lt;li&gt;Works well in transformers (used in BERT, GPT-2)&lt;/li&gt; &lt;li&gt;More stable training, especially for language models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;GELU's characteristics:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Smooth transition instead of sharp cutoff&lt;/li&gt; &lt;li&gt;Allows small negative values to pass through (unlike ReLU)&lt;/li&gt; &lt;li&gt;Better for tasks requiring fine-grained control&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Swish/SiLU&lt;/h1&gt; &lt;p&gt;Swish (also called SiLU, Sigmoid Linear Unit) is defined as:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Swish(x) = x × sigmoid(x) = x × (1 / (1 + e^(-x)))&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Swish is smooth like GELU but has been shown to work better in many applications. It's non-monotonic (can decrease for negative inputs), which gives it more flexibility than ReLU.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why Swish works well:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Smooth and differentiable everywhere&lt;/li&gt; &lt;li&gt;Non-monotonic behavior provides more expressiveness&lt;/li&gt; &lt;li&gt;Better gradient flow than ReLU&lt;/li&gt; &lt;li&gt;Proven effective in modern language models&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;SwiGLU&lt;/h1&gt; &lt;p&gt;SwiGLU (Swish-Gated Linear Unit) takes Swish and adds a gating mechanism. Instead of just applying Swish to a transformation, SwiGLU uses two parallel paths:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;SwiGLU(x) = Swish(W_gate × x) ⊙ (W_up × x)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;where ⊙ is element-wise multiplication. The key innovation is the gating mechanism: one path gets activated (the gate), and the other doesn't (the up projection). The gate controls how much of the unactivated path passes through.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why SwiGLU is powerful:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Gating mechanism allows more complex transformations&lt;/li&gt; &lt;li&gt;The gate can selectively pass or block information&lt;/li&gt; &lt;li&gt;More expressive than simple activation functions&lt;/li&gt; &lt;li&gt;Has become the standard in modern models like Qwen, LLaMA, and GPT&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;My Experience&lt;/h1&gt; &lt;p&gt;From working with different activation functions in practice, here's what I've learned:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;For small models:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GELU is often the safe, reliable choice. It provides good stability and performance without the extra parameters of gated variants.&lt;/li&gt; &lt;li&gt;SwiGLU can provide better performance but requires more parameters. For small models where every parameter counts, the trade-off isn't always worth it.&lt;/li&gt; &lt;li&gt;ReLU can work but is less stable, especially early in training. I avoid it unless I have a specific reason.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;For Larger models:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;SwiGLU has become the standard. The extra parameters are worth it for the improved expressiveness and performance.&lt;/li&gt; &lt;li&gt;The gating mechanism provides significant benefits in larger models where parameter count is less constrained.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Training Stability:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I've discovered that activation function choice can dramatically affect training stability. GELU and Swish provide better stability than ReLU, especially for small models.&lt;/li&gt; &lt;li&gt;The smoothness of these functions helps with gradient flow, which is critical for stable training.&lt;/li&gt; &lt;li&gt;I've had training runs that failed with ReLU but succeeded with GELU, even with identical architectures and hyperparameters.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;My Decision Framework:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;For most small models, I use GELU it's the safe, reliable choice that works well.&lt;/li&gt; &lt;li&gt;If I have parameter budget and want to maximize performance, I use SwiGLU.&lt;/li&gt; &lt;li&gt;I only consider alternatives like ReLU if I have a specific reason or constraint.&lt;/li&gt; &lt;li&gt;Activation function isn't usually the bottleneck for small models, so I don't spend too much time optimizing it. GELU works, and that's often enough.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Summary&lt;/h1&gt; &lt;p&gt;Today we explored activation functions, the components that give neural networks their non-linear power. We learned how activation functions evolved from simple ReLU to sophisticated SwiGLU, and how they affect training stability and model performance.&lt;/p&gt; &lt;p&gt;Understanding activation functions is crucial because they're fundamental to how neural networks learn. The choice of activation function can mean the difference between a model that trains stably and one that fails, between a model that converges quickly and one that struggles. In the context of language models, activation functions work together with normalization, residual connections, and attention mechanisms to create the powerful architectures we use today.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prashant-Lakhera"&gt; /u/Prashant-Lakhera &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxj4lv/day_20_21_days_of_building_a_small_language_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxj4lv/day_20_21_days_of_building_a_small_language_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pxj4lv/day_20_21_days_of_building_a_small_language_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-28T05:18:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1pxm29c</id>
    <title>Unsloth GLM-4.7-GGUF?</title>
    <updated>2025-12-28T08:08:13+00:00</updated>
    <author>
      <name>/u/UnknownDude360</name>
      <uri>https://old.reddit.com/user/UnknownDude360</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all! I’m really excited to test out GLM-4.7 and I’ve been specifically waiting for Unsloth’s quants because they always cook!&lt;/p&gt; &lt;p&gt;Well, I’m a little confused. Which is “technically” better? I mean higher average bits? Less lossy. &lt;/p&gt; &lt;p&gt;Q3_K_M @ 171GB vs Q3_K_XL @ 159GB?&lt;/p&gt; &lt;p&gt;I have 48GB VRAM + 128GB DDR4 = 176GB absolute maximum ideally. &lt;/p&gt; &lt;p&gt;I would expect it be obvious, the _XL should be better than the _M… right?&lt;/p&gt; &lt;p&gt;However the more lossy quant is somehow bigger? Can someone help me reconcile this discrepancy? I feel kinda dumb overthinking this…&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/UnknownDude360"&gt; /u/UnknownDude360 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxm29c/unsloth_glm47gguf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxm29c/unsloth_glm47gguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pxm29c/unsloth_glm47gguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-28T08:08:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1pxqbk4</id>
    <title>Uh, has anyone seen PR #10305 on TensorRT-LLM?</title>
    <updated>2025-12-28T12:33:41+00:00</updated>
    <author>
      <name>/u/melowdramtic</name>
      <uri>https://old.reddit.com/user/melowdramtic</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/NVIDIA/TensorRT-LLM/pull/10305"&gt;Feat/aether sparse attention by teerthsharma · Pull Request #10305 · NVIDIA/TensorRT-LLM&lt;/a&gt;&lt;/p&gt; &lt;p&gt;it’s titled: &lt;strong&gt;&amp;quot;Implementation of AETHER-X: Adaptive POVM Kernels for 4.9x Inference Speedup.&amp;quot;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I clicked it expecting some readme typo fix or maybe a 2% gain from quantization. &lt;strong&gt;It is not a typo.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/melowdramtic"&gt; /u/melowdramtic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxqbk4/uh_has_anyone_seen_pr_10305_on_tensorrtllm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxqbk4/uh_has_anyone_seen_pr_10305_on_tensorrtllm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pxqbk4/uh_has_anyone_seen_pr_10305_on_tensorrtllm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-28T12:33:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1px55wg</id>
    <title>GLM 4.7 IS NOW THE #1 OPEN SOURCE MODEL IN ARTIFICIAL ANALYSIS</title>
    <updated>2025-12-27T18:42:04+00:00</updated>
    <author>
      <name>/u/ZeeleSama</name>
      <uri>https://old.reddit.com/user/ZeeleSama</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1px55wg/glm_47_is_now_the_1_open_source_model_in/"&gt; &lt;img alt="GLM 4.7 IS NOW THE #1 OPEN SOURCE MODEL IN ARTIFICIAL ANALYSIS" src="https://preview.redd.it/9wzn809jks9g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a4de55096d77fed72c8e49191b815e4f735ff388" title="GLM 4.7 IS NOW THE #1 OPEN SOURCE MODEL IN ARTIFICIAL ANALYSIS" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ZeeleSama"&gt; /u/ZeeleSama &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9wzn809jks9g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1px55wg/glm_47_is_now_the_1_open_source_model_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1px55wg/glm_47_is_now_the_1_open_source_model_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-27T18:42:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1pxad0k</id>
    <title>NVIDIA Drops Pascal Support On Linux, Causing Chaos On Arch Linux</title>
    <updated>2025-12-27T22:22:21+00:00</updated>
    <author>
      <name>/u/HumanDrone8721</name>
      <uri>https://old.reddit.com/user/HumanDrone8721</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxad0k/nvidia_drops_pascal_support_on_linux_causing/"&gt; &lt;img alt="NVIDIA Drops Pascal Support On Linux, Causing Chaos On Arch Linux" src="https://external-preview.redd.it/Z1W-jCS5853m4eyzALlzqsbFjQ8v2fOj2tdMfCsU0J8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=232d0328fa3a116bc0a1917deae0e1763f4b6c47" title="NVIDIA Drops Pascal Support On Linux, Causing Chaos On Arch Linux" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HumanDrone8721"&gt; /u/HumanDrone8721 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://hackaday.com/2025/12/26/nvidia-drops-pascal-support-on-linux-causing-chaos-on-arch-linux/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxad0k/nvidia_drops_pascal_support_on_linux_causing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pxad0k/nvidia_drops_pascal_support_on_linux_causing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-27T22:22:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptxm3x</id>
    <title>AMA With Z.AI, The Lab Behind GLM-4.7</title>
    <updated>2025-12-23T16:04:23+00:00</updated>
    <author>
      <name>/u/zixuanlimit</name>
      <uri>https://old.reddit.com/user/zixuanlimit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi r/LocalLLaMA&lt;/p&gt; &lt;p&gt;Today we are having &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt;, the research lab behind the GLM 4.7. We’re excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Yuxuan Zhang, &lt;a href="/u/YuxuanZhangzR"&gt;u/YuxuanZhangzR&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Qinkai Zheng, &lt;a href="/u/QinkaiZheng"&gt;u/QinkaiZheng&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Aohan Zeng, &lt;a href="/u/Sengxian"&gt;u/Sengxian&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Zhenyu Hou, &lt;a href="/u/ZhenyuHou"&gt;u/ZhenyuHou&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Xin Lv, &lt;a href="/u/davidlvxin"&gt;u/davidlvxin&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The AMA will run from 8 AM – 11 AM PST, with the &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt; team continuing to follow up on questions over the next 48 hours.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zixuanlimit"&gt; /u/zixuanlimit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T16:04:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwh0q9</id>
    <title>Best Local LLMs - 2025</title>
    <updated>2025-12-26T22:31:28+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Year end thread for the best LLMs of 2025!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;2025 is almost done! Its been &lt;strong&gt;a wonderful year&lt;/strong&gt; for us Open/Local AI enthusiasts. And its looking like Xmas time brought some great gifts in the shape of Minimax M2.1 and GLM4.7 that are touting frontier model performance. Are we there already? are we at parity with proprietary models?!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The standard spiel:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Share what your favorite models are right now &lt;strong&gt;and why.&lt;/strong&gt; Given the nature of the beast in evaluating LLMs (untrustworthiness of benchmarks, immature tooling, intrinsic stochasticity), please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Only open weights models&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;em&gt;Please thread your responses in the top level comments for each Application below to enable readability&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Applications&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;General&lt;/strong&gt;: Includes practical guidance, how to, encyclopedic QnA, search engine replacement/augmentation&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agentic/Agentic Coding/Tool Use/Coding&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Creative Writing/RP&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Speciality&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;If a category is missing, please create a top level comment under the Speciality comment&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Notes&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Useful breakdown of how folk are using LLMs: &lt;a href="https://preview.redd.it/i8td7u8vcewf1.png?width=1090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d"&gt;https://preview.redd.it/i8td7u8vcewf1.png?width=1090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d&lt;/a&gt; &lt;/p&gt; &lt;p&gt;A good suggestion for last time, breakdown/classify your recommendation by model memory footprint: (you can and should be using multiple models in each size range for different tasks)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Unlimited: &amp;gt;128GB VRAM &lt;/li&gt; &lt;li&gt;Medium: 8 to 128GB VRAM&lt;/li&gt; &lt;li&gt;Small: &amp;lt;8GB VRAM&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T22:31:28+00:00</published>
  </entry>
</feed>
