<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-16T16:57:59+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qdok2i</id>
    <title>google/translategemma</title>
    <updated>2026-01-15T16:42:58+00:00</updated>
    <author>
      <name>/u/BreakfastFriendly728</name>
      <uri>https://old.reddit.com/user/BreakfastFriendly728</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/collections/google/translategemma"&gt;https://huggingface.co/collections/google/translategemma&lt;/a&gt;&lt;/p&gt; &lt;p&gt;tech report: &lt;a href="https://arxiv.org/abs/2601.09012"&gt;https://arxiv.org/abs/2601.09012&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BreakfastFriendly728"&gt; /u/BreakfastFriendly728 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdok2i/googletranslategemma/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdok2i/googletranslategemma/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qdok2i/googletranslategemma/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-15T16:42:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1qefbst</id>
    <title>I tried Prompt Repetition on Gemma 3.</title>
    <updated>2026-01-16T13:01:04+00:00</updated>
    <author>
      <name>/u/SrijSriv211</name>
      <uri>https://old.reddit.com/user/SrijSriv211</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qefbst/i_tried_prompt_repetition_on_gemma_3/"&gt; &lt;img alt="I tried Prompt Repetition on Gemma 3." src="https://preview.redd.it/u5l099t3jpdg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ae5ec703ca85bbff3b3d515b0ce06cded2be7b23" title="I tried Prompt Repetition on Gemma 3." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was reading this &lt;a href="https://arxiv.org/abs/2512.14982"&gt;paper&lt;/a&gt; and decided to give it a try with a simple Rs in Strawberry test with Gemma 3 4B, I tried with Gemma 3 270M as well but it didn't work. I didn't expect it to work with 4B version either tbh.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SrijSriv211"&gt; /u/SrijSriv211 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/u5l099t3jpdg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qefbst/i_tried_prompt_repetition_on_gemma_3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qefbst/i_tried_prompt_repetition_on_gemma_3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-16T13:01:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1qeia4h</id>
    <title>Tiny, 500MB Spam Detection model to flag spam content automatically. Can be used locally or self-hosted easily and fine-tuned to any language or definition of "spam"</title>
    <updated>2026-01-16T15:02:10+00:00</updated>
    <author>
      <name>/u/Ok_Hold_5385</name>
      <uri>https://old.reddit.com/user/Ok_Hold_5385</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/tanaos/tanaos-spam-detection-v1"&gt;https://huggingface.co/tanaos/tanaos-spam-detection-v1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A small (500Mb, 0.1B params) but efficient Spam Detection model which identifies spam content in any piece of text.&lt;/p&gt; &lt;h1&gt;How to use&lt;/h1&gt; &lt;p&gt;Use it with the &lt;a href="https://github.com/tanaos/artifex"&gt;Artifex python library&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from artifex import Artifex spam_detection = Artifex().spam_detection print(spam_detection(&amp;quot;You won an IPhone 16! Click here to claim your prize.&amp;quot;)) # &amp;gt;&amp;gt;&amp;gt; [{'label': 'spam', 'score': 0.9989}] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;or with the transformers library&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from transformers import pipeline clf = pipeline(&amp;quot;text-classification&amp;quot;, model=&amp;quot;tanaos/tanaos-spam-detection-v1&amp;quot;) print(clf(&amp;quot;You won an IPhone 16! Click here to claim your prize.&amp;quot;)) # &amp;gt;&amp;gt;&amp;gt; [{'label': 'spam', 'score': 0.9989}] &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;How to fine-tune to any language and definition of &amp;quot;spam&amp;quot;&lt;/h1&gt; &lt;p&gt;Use the &lt;a href="https://github.com/tanaos/artifex"&gt;Artifex library&lt;/a&gt; to fine-tune the spam detection model to a language other than English or to your own spam-definition criteria.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from artifex import Artifex spam_detection = Artifex().spam_detection spam_detection.train( spam_content=[ &amp;quot;Unsolicited commercial advertisement or non-commercial proselytizing&amp;quot;, &amp;quot;Fraudulent schemes, including get-rich-quick and pyramid schemes&amp;quot;, &amp;quot;Phishing attempts, unrealistic offers or announcements&amp;quot;, &amp;quot;Content with deceptive or misleading information&amp;quot;, &amp;quot;Malware or harmful links&amp;quot;, &amp;quot;Adult content or explicit material&amp;quot;, &amp;quot;Excessive use of capitalization or punctuation to grab attention&amp;quot;, ], language=&amp;quot;spanish&amp;quot; ) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Hold_5385"&gt; /u/Ok_Hold_5385 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qeia4h/tiny_500mb_spam_detection_model_to_flag_spam/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qeia4h/tiny_500mb_spam_detection_model_to_flag_spam/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qeia4h/tiny_500mb_spam_detection_model_to_flag_spam/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-16T15:02:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1qeg3vr</id>
    <title>Extending the Context of Pretrained LLMs by Dropping Their Positional Embeddings</title>
    <updated>2026-01-16T13:35:38+00:00</updated>
    <author>
      <name>/u/Aaaaaaaaaeeeee</name>
      <uri>https://old.reddit.com/user/Aaaaaaaaaeeeee</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://arxiv.org/abs/2512.12167"&gt;https://arxiv.org/abs/2512.12167&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So far, expensive finetuning beyond the pretraining sequence length has been a requirement for effectively extending the context of language models (LM). In this work, we break this key bottleneck by Dropping the Positional Embeddings of LMs after training (DroPE). Our simple method is motivated by three key theoretical and empirical observations. First, positional embeddings (PEs) serve a crucial role during pretraining, providing an important inductive bias that significantly facilitates convergence. Second, over-reliance on this explicit positional information is also precisely what prevents test-time generalization to sequences of unseen length, even when using popular PE-scaling methods. Third, positional embeddings are not an inherent requirement of effective language modeling and can be safely removed after pretraining, following a short recalibration phase. Empirically, DroPE yields seamless zero-shot context extension without any long-context finetuning, quickly adapting pretrained LMs without compromising their capabilities in the original training context. Our findings hold across different models and dataset sizes, far outperforming previous specialized architectures and established rotary positional embedding scaling methods.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aaaaaaaaaeeeee"&gt; /u/Aaaaaaaaaeeeee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2512.12167"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qeg3vr/extending_the_context_of_pretrained_llms_by/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qeg3vr/extending_the_context_of_pretrained_llms_by/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-16T13:35:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1qdna3t</id>
    <title>7x Longer Context Reinforcement Learning in Unsloth</title>
    <updated>2026-01-15T15:56:40+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdna3t/7x_longer_context_reinforcement_learning_in/"&gt; &lt;img alt="7x Longer Context Reinforcement Learning in Unsloth" src="https://preview.redd.it/nmkee12vbjdg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3cd97dbf853be6596556f70c467d1dccc0cc22a1" title="7x Longer Context Reinforcement Learning in Unsloth" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLlama"&gt;r/LocalLlama&lt;/a&gt;! We're excited to show how Unsloth now enables &lt;strong&gt;7x longer context lengths&lt;/strong&gt; (up to 12x) for Reinforcement Learning! By using 3 new techniques we developed, we enable you to train gpt-oss 20b QLoRA up to &lt;strong&gt;20K context on a 24Gb card&lt;/strong&gt; - all with &lt;strong&gt;no accuracy degradation&lt;/strong&gt;. Unsloth GitHub: &lt;a href="https://github.com/unslothai/unsloth"&gt;https://github.com/unslothai/unsloth&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;For larger GPUs, Unsloth now trains gpt-oss QLoRA with &lt;strong&gt;380K context&lt;/strong&gt; on a single 192GB NVIDIA B200 GPU&lt;/li&gt; &lt;li&gt;Qwen3-8B GRPO reaches &lt;strong&gt;110K context&lt;/strong&gt; on an 80GB VRAM H100 via vLLM and QLoRA, and &lt;strong&gt;65K&lt;/strong&gt; for gpt-oss with BF16 LoRA.&lt;/li&gt; &lt;li&gt;Unsloth GRPO RL runs with Llama, Gemma &amp;amp; all models auto support longer contexts&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Also, all features in Unsloth can be combined together and work well together:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Unsloth's &lt;a href="https://unsloth.ai/docs/get-started/reinforcement-learning-rl-guide/memory-efficient-rl"&gt;weight-sharing&lt;/a&gt; feature with vLLM and our Standby Feature in &lt;a href="https://unsloth.ai/docs/get-started/reinforcement-learning-rl-guide/memory-efficient-rl"&gt;Memory Efficient RL&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Unsloth's &lt;a href="https://unsloth.ai/docs/models/gpt-oss-how-to-run-and-fine-tune/long-context-gpt-oss-training"&gt;Flex Attention&lt;/a&gt; for long context gpt-oss and our &lt;a href="https://unsloth.ai/docs/new/500k-context-length-fine-tuning"&gt;500K Context Training&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Float8 training in &lt;a href="https://unsloth.ai/docs/get-started/reinforcement-learning-rl-guide/fp8-reinforcement-learning"&gt;FP8 RL&lt;/a&gt; and Unsloth's &lt;a href="https://unsloth.ai/blog/long-context"&gt;async gradient checkpointing&lt;/a&gt; and much more&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;You can read our educational blogpost for detailed analysis, benchmarks and more: &lt;a href="https://unsloth.ai/docs/new/grpo-long-context"&gt;https://unsloth.ai/docs/new/grpo-long-context&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And you can of course train any model using our new features and kernels via our free fine-tuning notebooks: &lt;a href="https://docs.unsloth.ai/get-started/unsloth-notebooks"&gt;https://docs.unsloth.ai/get-started/unsloth-notebooks&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Some free Colab notebooks below which has the 7x longer context support backed in:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-(20B"&gt;gpt-oss-20b&lt;/a&gt;-GRPO.ipynb) GSPO Colab&lt;/th&gt; &lt;th align="left"&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_VL_(8B"&gt;Qwen3-VL-8B&lt;/a&gt;-Vision-GRPO.ipynb) Vision RL&lt;/th&gt; &lt;th align="left"&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_8B_FP8_GRPO.ipynb"&gt;Qwen3-8B - FP8&lt;/a&gt; L4 GPU&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;To update Unsloth to automatically make training faster, do:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pip install --upgrade --force-reinstall --no-cache-dir --no-deps unsloth pip install --upgrade --force-reinstall --no-cache-dir --no-deps unsloth_zoo &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And to enable GRPO runs in Unsloth, do&lt;/p&gt; &lt;pre&gt;&lt;code&gt;import os os.environ[&amp;quot;UNSLOTH_VLLM_STANDBY&amp;quot;] = &amp;quot;1&amp;quot; # Standby = extra 30% context lengths! from unsloth import FastLanguageModel import torch max_seq_length = 20000 # Can increase for longer reasoning traces lora_rank = 32 # Larger rank = smarter, but slower model, tokenizer = FastLanguageModel.from_pretrained( model_name = &amp;quot;unsloth/Qwen3-4B-Base&amp;quot;, max_seq_length = max_seq_length, load_in_4bit = False, # False for LoRA 16bit fast_inference = True, # Enable vLLM fast inference max_lora_rank = lora_rank, ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Hope you all have a great rest of the week and thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nmkee12vbjdg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdna3t/7x_longer_context_reinforcement_learning_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qdna3t/7x_longer_context_reinforcement_learning_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-15T15:56:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1qehopv</id>
    <title>Which LLM is best for Q&amp;A sessions?</title>
    <updated>2026-01-16T14:39:17+00:00</updated>
    <author>
      <name>/u/Psyko38</name>
      <uri>https://old.reddit.com/user/Psyko38</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I'm developing a little automatic response agent for email.&lt;/p&gt; &lt;p&gt;My system is not very powerful, but it can run models up to ~1 billion parameters.&lt;/p&gt; &lt;p&gt;So I'm looking for an effective LLM to give simple answers from a text document, which is a model that can read a text and respond to it in a meaningful way while remaining under 1 billion parameters.&lt;/p&gt; &lt;p&gt;Would you have any recommendations for models adapted to this use case?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Psyko38"&gt; /u/Psyko38 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qehopv/which_llm_is_best_for_qa_sessions/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qehopv/which_llm_is_best_for_qa_sessions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qehopv/which_llm_is_best_for_qa_sessions/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-16T14:39:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1qdtvgs</id>
    <title>Not as impressive as most here, but really happy I made it in time!</title>
    <updated>2026-01-15T19:53:15+00:00</updated>
    <author>
      <name>/u/Kahvana</name>
      <uri>https://old.reddit.com/user/Kahvana</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdtvgs/not_as_impressive_as_most_here_but_really_happy_i/"&gt; &lt;img alt="Not as impressive as most here, but really happy I made it in time!" src="https://preview.redd.it/g997soqdgkdg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=572838999e31352821862a1f71f5bd35cb328147" title="Not as impressive as most here, but really happy I made it in time!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm in the Netherlands, I apologize in advance for my grammar (Happy to be corrected!), not using AI for translation.&lt;/p&gt; &lt;p&gt;Over here, getting cards is increasingly more difficult and prices are quite steep.&lt;/p&gt; &lt;p&gt;It was a bit of a gamble to get the second GPU; I had the RTX 5060 Ti on order for 509EU by Paradigit but it wasn't delivered for 2 weeks straight, and they still aren't sure when supply will arrive. Cancelled the order and payed the premium for Azerty's model in stock (600EU), but it arrived the next day!&lt;/p&gt; &lt;p&gt;So if you're in the Netherlands, I recommend calling up the store to ask about stock availability in advance. The listings on Tweakers wasn't accurate for this card.&lt;/p&gt; &lt;p&gt;Today the announcement from HardwareUnboxed came that the RTX 5060 Ti 16GB is becoming unavailable. Really happy it arrived just in time.&lt;/p&gt; &lt;p&gt;Specs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;AMD Ryzen 5 9600X&lt;/li&gt; &lt;li&gt;Crosair Vengence 96GB (2x48) DDR5-6000 CL30&lt;/li&gt; &lt;li&gt;ASUS ProArt X870E-Creator Wifi&lt;/li&gt; &lt;li&gt;2x ASUS Prime RTX 5060 Ti 16GB&lt;/li&gt; &lt;li&gt;BeQuiet! Dark Power 13 850W&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Notes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I don't use the CPU for inference much (embeddings) and the PCI lanes are the same across all models, so I went with the lowest TDP.&lt;/li&gt; &lt;li&gt;Wished I had more (192GB) for dataset generation / RAG but I can hold off.&lt;/li&gt; &lt;li&gt;Picked the motherboad specifically for it's PCI-E 5.0 splitting to get the most out of the GPUs.&lt;/li&gt; &lt;li&gt;Power draw during inference is ~300W.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kahvana"&gt; /u/Kahvana &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/g997soqdgkdg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdtvgs/not_as_impressive_as_most_here_but_really_happy_i/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qdtvgs/not_as_impressive_as_most_here_but_really_happy_i/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-15T19:53:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1qdrf3o</id>
    <title>Nemotron-3-nano:30b is a spectacular general purpose local LLM</title>
    <updated>2026-01-15T18:24:08+00:00</updated>
    <author>
      <name>/u/DrewGrgich</name>
      <uri>https://old.reddit.com/user/DrewGrgich</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just want to sing the praises of this model. I am stunned at how intelligent it is for a 30b model. Comparing it to Llama 3.3:70b, I have yet to find a general purpose question that Nemotron hasn't answered better. It is quite robotic so I won't be using it for creative or chat purposes. Everything else though has been stellar. &lt;/p&gt; &lt;p&gt;If you have the capacity to give it a try, I highly recommend it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DrewGrgich"&gt; /u/DrewGrgich &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdrf3o/nemotron3nano30b_is_a_spectacular_general_purpose/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdrf3o/nemotron3nano30b_is_a_spectacular_general_purpose/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qdrf3o/nemotron3nano30b_is_a_spectacular_general_purpose/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-15T18:24:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1qe7a3m</id>
    <title>Will the AI bubble bursting be good or bad for open-weights? What do you think?</title>
    <updated>2026-01-16T05:21:03+00:00</updated>
    <author>
      <name>/u/RandumbRedditor1000</name>
      <uri>https://old.reddit.com/user/RandumbRedditor1000</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I could see it both ways. On one hand, RAM, GPUs, and SSDs could see their prices return to normal, but on the other hand, it could lead to less AI being developed and released overall, especially from the major tech companies such as Google or Meta.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RandumbRedditor1000"&gt; /u/RandumbRedditor1000 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qe7a3m/will_the_ai_bubble_bursting_be_good_or_bad_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qe7a3m/will_the_ai_bubble_bursting_be_good_or_bad_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qe7a3m/will_the_ai_bubble_bursting_be_good_or_bad_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-16T05:21:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1qegs63</id>
    <title>Automating illustration for the Conan story "Tower of the Elephant"--Llama and Mistral for prompt generation, Qwen3-VL for image scoring, and image models.</title>
    <updated>2026-01-16T14:03:19+00:00</updated>
    <author>
      <name>/u/RobertTetris</name>
      <uri>https://old.reddit.com/user/RobertTetris</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qegs63/automating_illustration_for_the_conan_story_tower/"&gt; &lt;img alt="Automating illustration for the Conan story &amp;quot;Tower of the Elephant&amp;quot;--Llama and Mistral for prompt generation, Qwen3-VL for image scoring, and image models." src="https://b.thumbs.redditmedia.com/Xmi7dPaoOdUFPORUSazvXaBni2tyWeXMIP1keYd00ZM.jpg" title="Automating illustration for the Conan story &amp;quot;Tower of the Elephant&amp;quot;--Llama and Mistral for prompt generation, Qwen3-VL for image scoring, and image models." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;All details: &lt;a href="https://brianheming.substack.com/p/the-making-of-illustrated-conan-adventures"&gt;https://brianheming.substack.com/p/the-making-of-illustrated-conan-adventures&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I would especially be interested in people's thoughts on:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;optimizing image scoring with the vision-language model.&lt;/li&gt; &lt;li&gt;the possibilities of automating final image editing, e.g. via using a vision-language model with the image and story text to prompt an image edit model like Qwen Image Edit or Flux Klein.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RobertTetris"&gt; /u/RobertTetris &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qegs63"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qegs63/automating_illustration_for_the_conan_story_tower/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qegs63/automating_illustration_for_the_conan_story_tower/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-16T14:03:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1qe3p8d</id>
    <title>MiniMax M2.2 Coming Soon. Confirmed by Head of Engineering @MiniMax_AI</title>
    <updated>2026-01-16T02:29:49+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qe3p8d/minimax_m22_coming_soon_confirmed_by_head_of/"&gt; &lt;img alt="MiniMax M2.2 Coming Soon. Confirmed by Head of Engineering @MiniMax_AI" src="https://preview.redd.it/r2y0g60chmdg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8b4997ba8fe2fc1231baad6317e2eca9f7350374" title="MiniMax M2.2 Coming Soon. Confirmed by Head of Engineering @MiniMax_AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/r2y0g60chmdg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qe3p8d/minimax_m22_coming_soon_confirmed_by_head_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qe3p8d/minimax_m22_coming_soon_confirmed_by_head_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-16T02:29:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1qegpk4</id>
    <title>Motherboard for 4 5090s</title>
    <updated>2026-01-16T14:00:36+00:00</updated>
    <author>
      <name>/u/KigMidas0131</name>
      <uri>https://old.reddit.com/user/KigMidas0131</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;im working on a &amp;quot;Massive build&amp;quot; but coming up with engineering issues, as i cant find any 5090FEs ive went with the Zotac solid OC. I currently have 4 of these. &lt;/p&gt; &lt;p&gt;I want to put them on a board with risers obviously and my threadripper. but I cant find a good enough board for this project. &lt;/p&gt; &lt;p&gt;Im having trouble with trying to figure out my heating issue as well. Open air will be the way to go but I also need a way to mitigate dust accumulation. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KigMidas0131"&gt; /u/KigMidas0131 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qegpk4/motherboard_for_4_5090s/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qegpk4/motherboard_for_4_5090s/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qegpk4/motherboard_for_4_5090s/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-16T14:00:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1qeja1b</id>
    <title>Anyone here using a local LLM with their note taking app?</title>
    <updated>2026-01-16T15:39:34+00:00</updated>
    <author>
      <name>/u/sash20</name>
      <uri>https://old.reddit.com/user/sash20</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been trying to simplify my note taking app setup and keep more things local for privacy reasons. Most apps are fine for storing notes, but the ‚Äúthinking‚Äù part usually still happens in the cloud.&lt;/p&gt; &lt;p&gt;I use a regular note taking app just for storage, and sometimes Bluedot to capture meetings or study sessions and clean them up before saving anything long term. That works, but it‚Äôs not ideal.&lt;/p&gt; &lt;p&gt;Does anyone here is actually using a local model to help with note taking in a real, everyday workflow?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sash20"&gt; /u/sash20 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qeja1b/anyone_here_using_a_local_llm_with_their_note/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qeja1b/anyone_here_using_a_local_llm_with_their_note/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qeja1b/anyone_here_using_a_local_llm_with_their_note/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-16T15:39:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1qefz88</id>
    <title>GLM-Image trained on Huawei chips hits SOTA for text rendering</title>
    <updated>2026-01-16T13:30:02+00:00</updated>
    <author>
      <name>/u/Consistent_Damage824</name>
      <uri>https://old.reddit.com/user/Consistent_Damage824</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qefz88/glmimage_trained_on_huawei_chips_hits_sota_for/"&gt; &lt;img alt="GLM-Image trained on Huawei chips hits SOTA for text rendering" src="https://preview.redd.it/m1zahy20ppdg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4f474ae62f4add4f72c0edfe3db9bb3ee01d3801" title="GLM-Image trained on Huawei chips hits SOTA for text rendering" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;saw people talking about glm-image in a few threads but wanted to look at this from a different angle cause theres something interesting beyond the usual model release stuff&lt;/p&gt; &lt;p&gt;so the architecture is kinda a hybrid autoregressive (9B params from their GLM-4 base) plus a diffusion decoder (7B DiT). basically the AR part handles semantic understanding and what the layout should be, while the diffusion decoder does the heavy lifting on high-freq details and text rendering with a glyph encoder. its like they split &amp;quot;understand what to draw&amp;quot; from &amp;quot;actually draw it well&amp;quot; into seperate specialized components which... idk makes sense when you think about it?&lt;/p&gt; &lt;p&gt;couple things,&lt;/p&gt; &lt;p&gt;text rendering is actually SOTA for open source models. tops CVTG-2K and LongText-Bench for complex multi-region text and long text scenarios, especially strong with chinese characters. if youve ever tried generating posters or infographics with SDXL/FLUX and gotten complete garbled nonsense for text this might actually be worth testing&lt;/p&gt; &lt;p&gt;but heres the intresting part, trained entirely on Huawei Ascend chips. like soup-to-nuts on non-NVIDIA hardware (Atlas 800T A2 + MindSpore framework). whether you care about geopolitics or not its kinda cool that competitive results are achieveable outside the CUDA ecosystem. first SOTA multimodal model done this way apparently&lt;/p&gt; &lt;p&gt;its actually open too, MIT license, full weights on HF, integrates with transformers/diffusers pipelines. supports both T2I and I2I stuff (editing, style transfer, identity preservation etc)&lt;/p&gt; &lt;p&gt;tradeoffs tho: inference is expensive rn, needs 80gb single gpu or multi-gpu setup. theyre working on vllm/sglang optimization but yeah. also uses semantic-VQ tokens instead of traditional VQVAE which gives better semantic correlation but requires the two-stage architechture&lt;/p&gt; &lt;p&gt;some benchmarks: CVTG-2K hit 0.9116 word accuracy vs Qwen-Image's 0.8288. supports 1024x1024 to 2048x2048 natively without retraining. apparently few cents per image via API and they mention a faster version comming&lt;/p&gt; &lt;p&gt;curious if anyones actually tested this against FLUX.1-dev for text-heavy use cases? the semantic-VQ approach seems like a meaninful architectural choice rather then just throwing more parameters at the problem&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Consistent_Damage824"&gt; /u/Consistent_Damage824 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/m1zahy20ppdg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qefz88/glmimage_trained_on_huawei_chips_hits_sota_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qefz88/glmimage_trained_on_huawei_chips_hits_sota_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-16T13:30:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1qe5p2f</id>
    <title>Black Forest Labs releases FLUX.2 [klein]</title>
    <updated>2026-01-16T04:01:07+00:00</updated>
    <author>
      <name>/u/Old-School8916</name>
      <uri>https://old.reddit.com/user/Old-School8916</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Black Forest Labs released their new FLUX.2 [klein] model&lt;/p&gt; &lt;p&gt;&lt;a href="https://bfl.ai/blog/flux2-klein-towards-interactive-visual-intelligence"&gt;https://bfl.ai/blog/flux2-klein-towards-interactive-visual-intelligence&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;FLUX.2 [klein]: Towards Interactive Visual Intelligence&lt;/p&gt; &lt;p&gt;Today, we release the FLUX.2 [klein] model family, our fastest image models to date. FLUX.2 [klein] unifies generation and editing in a single compact architecture, delivering state-of-the-art quality with end-to-end inference as low as under a second. Built for applications that require real-time image generation without sacrificing quality, and runs on consumer hardware with as little as 13GB VRAM.&lt;/p&gt; &lt;p&gt;The klein name comes from the German word for &amp;quot;small&amp;quot;, reflecting both the compact model size and the minimal latency. But FLUX.2 [klein] is anything but limited. These models deliver exceptional performance in text-to-image generation, image editing and multi-reference generation, typically reserved for much larger models.&lt;/p&gt; &lt;/blockquote&gt; &lt;h1&gt;What's New&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Sub-second inference. Generate or edit images in under 0.5s on modern hardware.&lt;/li&gt; &lt;li&gt;Photorealistic outputs and high diversity, especially in the base variants.&lt;/li&gt; &lt;li&gt;Unified generation and editing. Text-to-image, image editing, and multi-reference support in a single model while delivering frontier performance.&lt;/li&gt; &lt;li&gt;Runs on consumer GPUs. The 4B model fits in ~13GB VRAM (RTX 3090/4070 and above).&lt;/li&gt; &lt;li&gt;Developer-friendly &amp;amp; Accessible: Apache 2.0 on 4B models, open weights for 9B models. Full open weights for customization and fine-tuning.&lt;/li&gt; &lt;li&gt;API and open weights. Production-ready API or run locally with full weights.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Resources&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Try it&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://bfl.ai/models/flux-2-klein#try-demo"&gt;Demo&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://bfl.ai/play"&gt;Playground&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/spaces/black-forest-labs/FLUX.2-klein-9B"&gt;HF Space for [klein] 9B&lt;/a&gt;, &lt;a href="https://huggingface.co/spaces/black-forest-labs/FLUX.2-klein-4B"&gt;HF Space for [klein] 4B&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Build with it&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://docs.bfl.ai/flux_2/flux2_overview#flux-2-%5Bklein%5D-models"&gt;Documentation&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/black-forest-labs/flux2"&gt;GitHub&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/collections/black-forest-labs/flux2"&gt;Model Weights&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Learn more&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://bfl.ai/models/flux-2-klein"&gt;https://bfl.ai/models/flux-2-klein&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Old-School8916"&gt; /u/Old-School8916 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qe5p2f/black_forest_labs_releases_flux2_klein/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qe5p2f/black_forest_labs_releases_flux2_klein/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qe5p2f/black_forest_labs_releases_flux2_klein/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-16T04:01:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1qe9xfi</id>
    <title>New FLUX.2 [Klein] 9B is INSANELY Fast</title>
    <updated>2026-01-16T07:48:50+00:00</updated>
    <author>
      <name>/u/Lopsided_Dot_4557</name>
      <uri>https://old.reddit.com/user/Lopsided_Dot_4557</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;BFL is has done a good job with this new Klein model, though in my testing text-to-image in distilled flavor is the best:&lt;/p&gt; &lt;p&gt;üîπ Sub-second inference on RTX 4090 hardware&lt;/p&gt; &lt;p&gt;üîπ 9B parameters matching models 5x its size&lt;/p&gt; &lt;p&gt;üîπ Step-distilled from 50 ‚Üí 4 steps, zero quality loss&lt;/p&gt; &lt;p&gt;üîπ Unified text-to-image + multi-reference editing&lt;/p&gt; &lt;p&gt;HF Model: &lt;a href="https://huggingface.co/black-forest-labs/FLUX.2-klein-base-9B"&gt;black-forest-labs/FLUX.2-klein-base-9B ¬∑ Hugging Face&lt;/a&gt;&lt;br /&gt; Detailed testing is here: &lt;a href="https://youtu.be/j3-vJuVwoWs?si=XPh7_ZClL8qoKFhl"&gt;https://youtu.be/j3-vJuVwoWs?si=XPh7_ZClL8qoKFhl&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lopsided_Dot_4557"&gt; /u/Lopsided_Dot_4557 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qe9xfi/new_flux2_klein_9b_is_insanely_fast/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qe9xfi/new_flux2_klein_9b_is_insanely_fast/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qe9xfi/new_flux2_klein_9b_is_insanely_fast/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-16T07:48:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1qek917</id>
    <title>I reproduced DeepSeek's mHC at 1.7B params (8xH100). The instability is 3x worse than reported (10k vs 3k), but the model didn't explode.</title>
    <updated>2026-01-16T16:14:54+00:00</updated>
    <author>
      <name>/u/poisson_labs</name>
      <uri>https://old.reddit.com/user/poisson_labs</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qek917/i_reproduced_deepseeks_mhc_at_17b_params_8xh100/"&gt; &lt;img alt="I reproduced DeepSeek's mHC at 1.7B params (8xH100). The instability is 3x worse than reported (10k vs 3k), but the model didn't explode." src="https://b.thumbs.redditmedia.com/zJw14CUcgynBl13gd34PUFykjliAEc5ivAIvrmPoRJA.jpg" title="I reproduced DeepSeek's mHC at 1.7B params (8xH100). The instability is 3x worse than reported (10k vs 3k), but the model didn't explode." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;Following up on my previous post about reproducing the DeepSeek-V2/V3 architecture. I decided to bite the bullet and rent an H100 cluster to scale the &amp;quot;Hyper-Connections&amp;quot; (HC) experiment from 10M to 1.7B parameter&lt;/p&gt; &lt;p&gt;The DeepSeek paper warned that standard Hyper-Connections cause signal variance to explode by ~3,000x at 27B parameters. I wanted to see if that held true or if it was a theoretical upper bound.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Results:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;It's worse than they said.&lt;/strong&gt; At just 1.7B parameters, I measured signal amplification of &lt;strong&gt;10,924x&lt;/strong&gt;. The &amp;quot;Instability Bomb&amp;quot; is real.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The &amp;quot;Twist&amp;quot;:&lt;/strong&gt; Despite signals amplifying by 10,000x, the loss &lt;strong&gt;didn't diverge&lt;/strong&gt;. The model kept learning. My theory is that modern optimizers (AdamW) and gradient clipping work overtime to mask the issue, but it's basically a ticking time bomb for longer runs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The Fix:&lt;/strong&gt; Verified that Manifold Hyper-Connections (mHC) with Sinkhorn projection completely solves this. Variance stays locked at 1.0x with zero compute overhead.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/a1gsgd87kqdg1.png?width=4160&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1d75dc5207b1401eed9fe3a8e3425e24fe560fc0"&gt;https://preview.redd.it/a1gsgd87kqdg1.png?width=4160&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1d75dc5207b1401eed9fe3a8e3425e24fe560fc0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I wrote up the full breakdown with the loss curves and Amax graphs here: &lt;a href="https://taylorkolasinski.com/notes/mhc-reproduction-part2/"&gt;https://taylorkolasinski.com/notes/mhc-reproduction-part2/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Part 1 can be found here: &lt;a href="https://taylorkolasinski.com/notes/mhc-reproduction/"&gt;https://taylorkolasinski.com/notes/mhc-reproduction/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also, there's a discussion on HN right now if you want to chat there: &lt;a href="https://news.ycombinator.com/newest?next=46647671&amp;amp;n=31"&gt;https://news.ycombinator.com/newest?next=46647671&amp;amp;n=31&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer questions about the H100 setup or the implementation!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/poisson_labs"&gt; /u/poisson_labs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qek917/i_reproduced_deepseeks_mhc_at_17b_params_8xh100/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qek917/i_reproduced_deepseeks_mhc_at_17b_params_8xh100/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qek917/i_reproduced_deepseeks_mhc_at_17b_params_8xh100/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-16T16:14:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1qe4so5</id>
    <title>Dang, M2 drives are the new DDR5 apparently.</title>
    <updated>2026-01-16T03:18:52+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qe4so5/dang_m2_drives_are_the_new_ddr5_apparently/"&gt; &lt;img alt="Dang, M2 drives are the new DDR5 apparently." src="https://preview.redd.it/c8pq1jm6qmdg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b9e429f423f72a245e2fe28f8b56d773252a3eec" title="Dang, M2 drives are the new DDR5 apparently." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/c8pq1jm6qmdg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qe4so5/dang_m2_drives_are_the_new_ddr5_apparently/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qe4so5/dang_m2_drives_are_the_new_ddr5_apparently/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-16T03:18:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1qe0cxc</id>
    <title>Latest upgrade‚Ä¶A100 40 GB</title>
    <updated>2026-01-16T00:03:21+00:00</updated>
    <author>
      <name>/u/inserterikhere</name>
      <uri>https://old.reddit.com/user/inserterikhere</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qe0cxc/latest_upgradea100_40_gb/"&gt; &lt;img alt="Latest upgrade‚Ä¶A100 40 GB" src="https://preview.redd.it/f66wnmearldg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=703e9ae4724a0bb0a84546215e98301f06d28541" title="Latest upgrade‚Ä¶A100 40 GB" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Originally this was my gaming rig but I went ITX and basically bought a new computer. So I had the case, fans, AIO, 64 GB DDR5, motherboard, PSU, and 3080 (upgraded to 5070ti RIP). I was going to sell these parts, but I started running models on my 5070ti and eventually I wanted to start running larger models. I found a 3090 on eBay for $680, and 7950x for $350. I put that together with the parts and it‚Äôs been a great AI rig for me. I really didn‚Äôt plan on upgrading this for a while, especially now with the current price surges. Welp, I saw an A100 get listed for $1000 on eBay. The catch? Listed for parts, and the description just said ‚Äúcard reports CUDA error‚Äù. So I figured it was worth the risk (for me), I could‚Äôve probably sold it for the price I paid. Well, I swapped out the 3080 and on the first boot it was recognized instantly by nvidia-smi. I was able to run and train models immediately. Nice. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/inserterikhere"&gt; /u/inserterikhere &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/f66wnmearldg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qe0cxc/latest_upgradea100_40_gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qe0cxc/latest_upgradea100_40_gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-16T00:03:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1qeimyi</id>
    <title>7 GPUs at X16 (5.0 and 4.0) on AM5 with Gen5/4 switches with the P2P driver. Some results on inference and training!</title>
    <updated>2026-01-16T15:15:34+00:00</updated>
    <author>
      <name>/u/panchovix</name>
      <uri>https://old.reddit.com/user/panchovix</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qeimyi/7_gpus_at_x16_50_and_40_on_am5_with_gen54/"&gt; &lt;img alt="7 GPUs at X16 (5.0 and 4.0) on AM5 with Gen5/4 switches with the P2P driver. Some results on inference and training!" src="https://b.thumbs.redditmedia.com/oT5YU0t-mdDb6dXVqwnx9uFeB1IvH9XzAha2HR-inYc.jpg" title="7 GPUs at X16 (5.0 and 4.0) on AM5 with Gen5/4 switches with the P2P driver. Some results on inference and training!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello guys, hoping you're fine!&lt;/p&gt; &lt;p&gt;As I mentioned in the past in this post: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1pt0av6/plxpex_pcie_40_seems_to_help_for_llms_and_p2p_ie/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1pt0av6/plxpex_pcie_40_seems_to_help_for_llms_and_p2p_ie/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;With the P2P driver (&lt;a href="https://github.com/aikitoria/open-gpu-kernel-modules/?tab=readme-ov-file"&gt;https://github.com/aikitoria/open-gpu-kernel-modules/?tab=readme-ov-file&lt;/a&gt;) you can do P2P on same gen GPUs, including consumer ones!&lt;/p&gt; &lt;p&gt;So, also, you can connect GPUs on the same PCIe switch, and with the P2P driver the info is passed directly on the switch fabric instead by going by the CPU root complex, so for example:&lt;/p&gt; &lt;p&gt;5090 &amp;lt;-&amp;gt; 5090 directly on the same switch with the P2P driver would be possible. Since PCIe it is bidirectional, you can read at 64GiB/s on one GPU and write at 64GiB/s on the other at the same time!&lt;/p&gt; &lt;p&gt;So here we go with the info. Also I will mention some products I got from Aliexpress, but without a link, else the post gets removed. I can post the links on a comment for those products if you're interested.&lt;/p&gt; &lt;p&gt;A sneakpeek:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ea7itij34qdg1.png?width=859&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=96db6103a3838accb9eea239f2fa0712b14d13d2"&gt;X16 on 7 GPUs on AM5&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Setup including switches&lt;/h1&gt; &lt;p&gt;So for my setup, I have this:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Gigabyte Aorus Master X670E&lt;/li&gt; &lt;li&gt;AMD Ryzen 9 9900X&lt;/li&gt; &lt;li&gt;192GB DDR5 6000Mhz&lt;/li&gt; &lt;li&gt;2 Asrock 1600W PSU (PG 1600G ATX 3.1)&lt;/li&gt; &lt;li&gt;1 Corsair 1500W PSU (Corsair HX1500i)&lt;/li&gt; &lt;li&gt;RTX 5090*2 (PCIe 5.0)&lt;/li&gt; &lt;li&gt;RTX 4090*2 (PCIe 4.0)&lt;/li&gt; &lt;li&gt;RTX 3090 (PCIe 4.0)&lt;/li&gt; &lt;li&gt;RTX A6000 (PCIe 4.0)&lt;/li&gt; &lt;li&gt;NVIDIA A40 (PCIe 4.0)&lt;/li&gt; &lt;li&gt;Multiple SSDs, a 40Gbps NIC, etc.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Switch 1: 100 lanes PCIe 5.0 switch, Microchip Switchtec PM50100 from c-payne, from &lt;a href="https://c-payne.com/products/pcie-gen5-mcio-switch-100-lane-microchip-switchtec-pm50100"&gt;here&lt;/a&gt;, for 2000 EUR (about 2500USD post taxes in Chile)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/srwwml1p0qdg1.png?width=1600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d032f2a2606fd6603bbe8bffa005f9a14622f52b"&gt;PCIe 5.0 100 lane switch&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This switch has one X16 5.0 upstream, to 5*X16 5.0 downstream + 1*X4 5.0 downstream, via MCIO.&lt;/p&gt; &lt;p&gt;For this, I got a MCIO Retimer from aliexpress, that looks like this:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/zc917jy21qdg1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=de574e29fbb36bf0bf833b9d8d9e3da87ba5bdac"&gt;MCIO 5.0 Retimer&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Else, with a passive MCIO adapter, some GPUs would drop randomly.&lt;/p&gt; &lt;p&gt;For the other switch, I got a PLX88096 switch one from aliexpress, for about 400USD. This is a 96 lane PCIe 4.0 switch.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/smp1c0671qdg1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=41d150605391d7b25f44a12356eb71c256285097"&gt;PLX88096 4.0 switch&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This switch has X16 upstream from the PCIe slot, and it has 10 SlimSAS downstream ports.&lt;/p&gt; &lt;p&gt;This means you can do, with the dip switch, either: 5*X16 4.0, or 10*X8 4.0, or 20*X4 4.0.&lt;/p&gt; &lt;h1&gt;Connection of the GPUs&lt;/h1&gt; &lt;p&gt;For this, I basically connected the MCIO 5.0 retimer on the main X16 5.0 slot from the motherboard, and then, on this switch, I connected 2 5090s directly on 4 MCIO ports, and on other 2 MCIO ports, I connected the PLX88096 SlimSAS switch.&lt;/p&gt; &lt;p&gt;Basically, it looks like this:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;PM50100 Switch (01:00.0) ‚îú‚îÄ‚îÄ Port 02.0 ‚Üí GPU2 (5090) direct ‚îú‚îÄ‚îÄ Port 03.0 ‚Üí PLX88096 (cascaded) ‚îÇ ‚îî‚îÄ‚îÄ Complex internal structure: ‚îÇ ‚îú‚îÄ‚îÄ GPU0 (4090) ‚îÇ ‚îú‚îÄ‚îÄ GPU1 (4090) ‚îÇ ‚îú‚îÄ‚îÄ GPU4 (A40) ‚îÇ ‚îú‚îÄ‚îÄ GPU5 (A6000) ‚îÇ ‚îî‚îÄ‚îÄ GPU6 (3090) ‚îî‚îÄ‚îÄ Port 04.0 ‚Üí GPU3 (5090) direct ‚îî‚îÄ‚îÄ Other ports unused ATM &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;What is CPU root complex? Why it is worse?&lt;/h1&gt; &lt;p&gt;When we talk about GPUs communicating via the CPU root complex, it's when the data has to move from the PCIe slot to the RAM, and viceversa on the case of no P2P. For this to happen, it HAS to pass by the CPU. If you use P2P, then it is directly via PCIe to PCIe via the CPU root complex.&lt;/p&gt; &lt;p&gt;So normally, let¬¥s say you take a motherboard that has 2*X8 5.0 slots. You connect a 5090 on each slot.&lt;/p&gt; &lt;p&gt;If you do TP (tensor parallel), or training with multiGPU, either by using P2P or not, the data has to pass between the 2 GPUs.&lt;/p&gt; &lt;p&gt;If you don't use a switch, this data has to pass by the CPU first.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;If no P2P: 5090(1) -&amp;gt; CPU -&amp;gt; RAM -&amp;gt; CPU -&amp;gt; 5090(2)&lt;/li&gt; &lt;li&gt;If P2P: 5090(1) -&amp;gt; CPU -&amp;gt; 5090(2)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This adds extra latency by doing extra hops, specially on the case of no P2P.&lt;/p&gt; &lt;h1&gt;Topology&lt;/h1&gt; &lt;p&gt;Topology looks like this (GPU 0 and 1: 5090s, 2 and 3: 4090s, 4,5 and 6: A6000, A40 and 3090):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pancho@fedora:~/cuda-samples/build/Samples/5_Domain_Specific/p2pBandwidthLatencyTest$ nvidia-smi topo -m GPU0 GPU1 GPU2 GPU3 GPU4 GPU5 GPU6 NIC0 CPU Affinity NUMA Affinity GPU NUMA ID GPU0 X PXB PXB PXB PXB PXB PIX PHB 0-23 0 N/A GPU1 PXB X PXB PXB PXB PXB PXB PHB 0-23 0 N/A GPU2 PXB PXB X PIX PXB PXB PXB PHB 0-23 0 N/A GPU3 PXB PXB PIX X PXB PXB PXB PHB 0-23 0 N/A GPU4 PXB PXB PXB PXB X PIX PXB PHB 0-23 0 N/A GPU5 PXB PXB PXB PXB PIX X PXB PHB 0-23 0 N/A GPU6 PIX PXB PXB PXB PXB PXB X PHB 0-23 0 N/A NIC0 PHB PHB PHB PHB PHB PHB PHB X Legend: X = Self SYS = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI) NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node PHB = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU) PXB = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge) PIX = Connection traversing at most a single PCIe bridge NV# = Connection traversing a bonded set of # NVLinks NIC Legend: NIC0: mlx4_0 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;As you can see, 5090 pair, or 4090 pair, or Ampere trio have PIX. That means as it says, the connection traverses at most a single PCIe bridge, without going by the CPU root complex.&lt;/p&gt; &lt;p&gt;When the GPUs have to communicate with another of other gen, then it is PXB. This is because it has to pass by the switch via hops.&lt;/p&gt; &lt;p&gt;If you don't use a switch, with or without the P2P driver, you would normally see PHB.&lt;/p&gt; &lt;h1&gt;Bandwidth&lt;/h1&gt; &lt;p&gt;For bandwidth, I did this test on cuda samples:&lt;/p&gt; &lt;p&gt;pancho@fedora:~/cuda-samples/build/Samples/5_Domain_Specific/p2pBandwidthLatencyTest$ ./p2pBandwidthLatencyTest&lt;/p&gt; &lt;p&gt;[P2P (Peer-to-Peer) GPU Bandwidth Latency Test]&lt;/p&gt; &lt;p&gt;Device: 0, NVIDIA GeForce RTX 4090, pciBusID: e, pciDeviceID: 0, pciDomainID:0&lt;/p&gt; &lt;p&gt;Device: 1, NVIDIA GeForce RTX 4090, pciBusID: 11, pciDeviceID: 0, pciDomainID:0&lt;/p&gt; &lt;p&gt;Device: 2, NVIDIA GeForce RTX 5090, pciBusID: 5, pciDeviceID: 0, pciDomainID:0&lt;/p&gt; &lt;p&gt;Device: 3, NVIDIA GeForce RTX 5090, pciBusID: 18, pciDeviceID: 0, pciDomainID:0&lt;/p&gt; &lt;p&gt;Device: 4, NVIDIA A40, pciBusID: d, pciDeviceID: 0, pciDomainID:0&lt;/p&gt; &lt;p&gt;Device: 5, NVIDIA RTX A6000, pciBusID: 12, pciDeviceID: 0, pciDomainID:0&lt;/p&gt; &lt;p&gt;Device: 6, NVIDIA GeForce RTX 3090, pciBusID: a, pciDeviceID: 0, pciDomainID:0&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pancho@fedora:~/cuda-samples/build/Samples/5_Domain_Specific/p2pBandwidthLatencyTest$ ./p2pBandwidthLatencyTest [P2P (Peer-to-Peer) GPU Bandwidth Latency Test] Device: 0, NVIDIA GeForce RTX 4090, pciBusID: e, pciDeviceID: 0, pciDomainID:0 Device: 1, NVIDIA GeForce RTX 4090, pciBusID: 11, pciDeviceID: 0, pciDomainID:0 Device: 2, NVIDIA GeForce RTX 5090, pciBusID: 5, pciDeviceID: 0, pciDomainID:0 Device: 3, NVIDIA GeForce RTX 5090, pciBusID: 18, pciDeviceID: 0, pciDomainID:0 Device: 4, NVIDIA A40, pciBusID: d, pciDeviceID: 0, pciDomainID:0 Device: 5, NVIDIA RTX A6000, pciBusID: 12, pciDeviceID: 0, pciDomainID:0 Device: 6, NVIDIA GeForce RTX 3090, pciBusID: a, pciDeviceID: 0, pciDomainID:0 ***NOTE: In case a device doesn't have P2P access to other one, it falls back to normal memcopy procedure. So you can see lesser Bandwidth (GB/s) and unstable Latency (us) in those cases. P2P Connectivity Matrix D\D 0 1 2 3 4 5 6 0 1 1 0 0 0 0 0 1 1 1 0 0 0 0 0 2 0 0 1 1 0 0 0 3 0 0 1 1 0 0 0 4 0 0 0 0 1 1 1 5 0 0 0 0 1 1 1 6 0 0 0 0 1 1 1 Unidirectional P2P=Disabled Bandwidth Matrix (GB/s) D\D 0 1 2 3 4 5 6 0 915.89 8.31 12.75 12.75 8.30 8.30 5.83 1 8.32 927.85 12.75 12.75 8.30 8.30 5.79 2 12.26 12.26 1562.55 23.21 12.21 12.21 7.99 3 12.26 12.26 23.22 1556.32 12.21 12.21 7.98 4 8.31 8.31 12.70 12.70 644.33 8.29 5.78 5 8.31 8.31 12.70 12.70 8.30 766.68 5.80 6 5.82 5.81 8.07 8.12 5.82 5.79 833.78 Unidirectional P2P=Enabled Bandwidth (P2P Writes) Matrix (GB/s) D\D 0 1 2 3 4 5 6 0 920.20 26.37 12.75 12.75 8.30 8.30 5.85 1 26.36 944.11 12.75 12.74 8.30 8.30 5.81 2 12.26 12.26 1540.97 57.23 12.21 12.21 7.99 3 12.25 12.26 57.25 1543.97 12.21 12.21 7.98 4 8.31 8.31 12.70 12.70 643.53 26.36 26.36 5 8.31 8.31 12.70 12.70 26.36 767.06 26.36 6 5.83 5.81 8.07 8.07 26.37 26.37 835.56 Bidirectional P2P=Disabled Bandwidth Matrix (GB/s) D\D 0 1 2 3 4 5 6 0 921.29 9.49 15.20 15.21 9.48 9.49 6.27 1 9.49 926.20 15.21 15.23 9.48 9.50 6.29 2 14.18 14.15 1541.62 23.43 14.12 14.17 9.71 3 14.18 14.17 23.27 1540.12 14.13 14.21 9.71 4 9.46 9.48 15.15 15.14 647.80 9.48 6.28 5 9.51 9.48 15.23 15.24 9.49 770.65 6.29 6 6.27 6.29 10.70 10.69 6.32 6.26 839.38 Bidirectional P2P=Enabled Bandwidth Matrix (GB/s) D\D 0 1 2 3 4 5 6 0 922.10 52.18 15.20 15.15 9.49 9.50 6.32 1 52.18 922.92 15.19 15.19 9.49 9.50 6.26 2 14.16 14.17 1540.86 110.82 14.13 14.20 9.72 3 14.16 14.17 110.77 1537.09 14.09 14.20 9.72 4 9.48 9.47 15.12 15.12 647.53 52.19 52.19 5 9.51 9.50 15.27 15.25 52.17 769.89 52.19 6 6.31 6.28 10.69 10.67 52.18 52.18 838.25 P2P=Disabled Latency Matrix (us) GPU 0 1 2 3 4 5 6 0 1.30 15.32 14.38 14.41 15.74 15.09 14.85 1 15.17 1.35 14.71 14.39 14.26 14.26 14.25 2 14.34 14.35 2.07 14.46 14.37 14.36 14.35 3 14.33 14.34 14.34 2.07 14.34 14.44 14.35 4 14.80 14.25 14.48 15.24 1.78 15.96 14.70 5 16.10 14.73 14.45 14.36 14.37 1.77 14.33 6 14.24 14.25 14.38 14.53 15.11 14.33 1.60 CPU 0 1 2 3 4 5 6 0 1.40 4.21 4.15 4.14 3.95 4.14 4.16 1 4.19 1.35 4.14 4.14 3.93 4.09 4.10 2 4.19 4.12 1.55 4.09 3.92 4.10 4.12 3 4.14 4.10 3.95 1.51 3.73 3.91 3.94 4 3.83 4.01 4.00 3.97 1.28 4.03 4.00 5 4.22 4.15 4.12 4.11 3.91 1.35 4.14 6 4.11 4.08 4.09 4.11 3.88 4.11 1.35 P2P=Enabled Latency (P2P Writes) Matrix (us) GPU 0 1 2 3 4 5 6 0 1.28 1.41 14.47 14.38 14.91 14.26 18.66 1 1.41 1.29 14.41 14.39 14.26 14.26 16.30 2 14.34 14.41 2.07 0.36 14.40 14.34 14.37 3 14.34 14.35 0.36 2.07 14.40 14.36 14.36 4 14.35 16.30 14.49 14.44 1.80 1.62 1.58 5 16.66 14.24 14.37 14.40 1.58 1.76 1.60 6 15.08 15.27 14.37 14.43 1.52 1.51 1.56 CPU 0 1 2 3 4 5 6 0 1.39 1.13 4.16 4.13 3.94 4.19 4.17 1 1.14 1.36 4.17 4.14 3.93 4.17 4.15 2 4.17 4.19 1.54 1.08 3.94 4.12 4.14 3 4.17 4.17 1.10 1.57 3.94 4.14 4.15 4 4.04 4.02 4.04 4.01 1.29 1.02 1.03 5 4.18 4.18 4.19 4.18 1.10 1.37 1.09 6 4.17 4.14 4.14 4.15 1.09 1.09 1.35 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Like that, we have this bidirectional bandwidth:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;5090 ‚Üî 5090: 110.82 GB/s (via PM50100 switch)&lt;/li&gt; &lt;li&gt;4090 ‚Üî 4090: 52.18 GB/s (via PLX88096 switch connected to the PM50100 switch)&lt;/li&gt; &lt;li&gt;Ampere Trio A40 ‚Üî A6000 ‚Üî 3090: 52.19 GB/s (via PLX88096 switch connected to the PM50100 switch)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Remember that when having a PCIe switch, P2P and GPUs on the same switch, they communicate directly via the switch fabric without having to pass by the CPU root complex. So you can surpass the uplink bandwidth as long you keep it inside the switch.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; P2P does not work across different GPU gens, so on that case (i.e. 5090 to 4090, or 5090 to 3090) bandwidth is reduced.&lt;/p&gt; &lt;p&gt;On that case, if using all the GPUs at the same time, bandwidth between them is about 15GB/s. About PCIe 4.0 X8 speeds (thanks to PCIe being bidirectional).&lt;/p&gt; &lt;h1&gt;Performance (on limited tests, and why I want to you to give me some ideas to test)&lt;/h1&gt; &lt;p&gt;Because I had only X4 4.0 lanes at most, I mostly only used llamacpp. But I think with the switches, for 4 GPUs at least, something like vLLM would make sense.&lt;/p&gt; &lt;p&gt;So for my tests, I only have some diffusion training, and some LLMs on llamacpp, where even with this it makes a difference.&lt;/p&gt; &lt;h1&gt;Training (diffusion)&lt;/h1&gt; &lt;p&gt;For this, I did a full finetune on a SDXL model. Not good results at all per se but it was mostly to take the time it took.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;1 5090: ~24 hours&lt;/li&gt; &lt;li&gt;2 5090s (no P2P, X8/X8): ~16 hours (mostly by increasing the effective batch size, speed was the same but steps were halved)&lt;/li&gt; &lt;li&gt;2 5090s (P2P driver, X8/X8): ~13 hours&lt;/li&gt; &lt;li&gt;2 5090s (P2P driver, X16/X16 via switch): ~8 hours&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;That is a huge uplink, mostly by using the P2P driver first. So if you have 2 5090s at X8/X8, make sure to install the P2P driver!&lt;/p&gt; &lt;h1&gt;Inference (don't kill me, just llamacpp for now)&lt;/h1&gt; &lt;p&gt;For this, I have tested 3 models, on different configurations, so it took a bit of time. I hope it helps for info!&lt;/p&gt; &lt;p&gt;First I set the device order like this:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;5090, 5090, 4090, 4090, 3090, A40, A6000 export CUDA_VISIBLE_DEVICES=2,3,0,1,6,5,4 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Also all the tests were made with the P2P driver in use (but should make no difference on llamacpp (but it does on ikllamacpp)).&lt;/p&gt; &lt;p&gt;First:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GLM 4.7 Q4_K_XL (about 196GB in size), fully loaded on GPU:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;For this one, loading with:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./llama-server \ -m '/run/media/pancho/MyDrive/models_llm_2tb/GLM-4.7-UD-Q4_K_XL.gguf' \ -c 32768 \ --no-mmap \ -ngl 999 \ -ot &amp;quot;blk.(0|1|2|3|4|5|6|7|8|9|10|11|12|13|14).ffn.=CUDA0&amp;quot; \ -ot &amp;quot;blk.(15|16|17|18|19|20|21|22|23|24|25|26).ffn.=CUDA1&amp;quot; \ -ot &amp;quot;blk.(27|28|29|30|31|32|33|34|35).ffn.=CUDA2&amp;quot; \ -ot &amp;quot;blk.(36|37|38|39|40|41|42|43|44).ffn.=CUDA3&amp;quot; \ -ot &amp;quot;blk.(45|46|47|48|49|50|51|52|53).ffn.=CUDA4&amp;quot; \ -ot &amp;quot;blk.(54|55|56|57|58|59|60|61|62|63|64|65|66|67|68|69|70|71|72|73).ffn.=CUDA5&amp;quot; \ -ot &amp;quot;blk.(74|75|76|77|78|79|80|81|82|83|84|85|86|87|88|89|90|91|92).ffn.=CUDA6&amp;quot; \ -mg 0 \ -ub 2048 -b 2048 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I have these results for different setups (PP = Prompt processing, TG = Text generation):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;5090s at X8/X8 5.0, 4090s, A6000, A40 at X4 4.0 and 3090 at X1 3.0: 665.46 t/s PP, 25.90 t/s TG&lt;/li&gt; &lt;li&gt;5090s at X8/X8 5.0, 4090s, and Ampere trio at X4 4.0: 765.51 t/s PP, 26.18 t/s TG.&lt;/li&gt; &lt;li&gt;5090(1) at X16 5.0, 5090(2) at X4 5.0, all the rest at X4 4.0: 940 t/s PP, 26.75 t/s TG.&lt;/li&gt; &lt;li&gt;5090s at X16 5.0, all the rest at X16 4.0: 1170 t/s PP, 27.64 t/s TG.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;DeepSeek V3 0324, IQ4_XS, offloading about 120GB to CPU:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Loading with:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./llama-server -m '/run/media/pancho/MyDrive2/HuggingFaceModelDownloader/Storage/GGUFs/DeepSeek-V3-0324-IQ4_XS.gguf' -c 32768 --no-mmap -ngl 999 \ -ot &amp;quot;blk.(0|1|2|3|4|5|6).ffn.=CUDA0&amp;quot; \ -ot &amp;quot;blk.(7|8|9|10|11|12).ffn.=CUDA1&amp;quot; \ -ot &amp;quot;blk.(13|14|15).ffn.=CUDA2&amp;quot; \ -ot &amp;quot;blk.(16|17|18).ffn.=CUDA3&amp;quot; \ -ot &amp;quot;blk.(19|20|21).ffn.=CUDA4&amp;quot; \ -ot &amp;quot;blk.(22|23|24).ffn.=CUDA5&amp;quot; \ -ot &amp;quot;blk.(25|26|27|28).ffn.=CUDA6&amp;quot; \ -ot &amp;quot;blk.30.ffn_(norm|gate_inp|gate_shexp|down_shexp|up_shexp).weight=CUDA2&amp;quot; \ -ot &amp;quot;blk.30.ffn_gate_exps.weight=CUDA2&amp;quot; \ -ot &amp;quot;blk.30.ffn_down_exps.weight=CUDA3&amp;quot; \ -ot &amp;quot;blk.31.ffn_(norm|gate_inp|gate_shexp|down_shexp|up_shexp).weight=CUDA0&amp;quot; \ -ot &amp;quot;blk.31.ffn_gate_exps.weight=CUDA1&amp;quot; \ -ot &amp;quot;blk.31.ffn_down_exps.weight=CUDA1&amp;quot; \ -ot &amp;quot;blk.31.ffn_up_exps.weight=CUDA6&amp;quot; \ -ot &amp;quot;blk.32.ffn_gate_exps.weight=CUDA6&amp;quot; \ -ot &amp;quot;exps=CPU&amp;quot; \ -mg 0 -ub 2048 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I have these results:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;5090s at X8/X8 5.0, 4090s, A6000, A40 at X4 4.0 and 3090 at X1 3.0: 195.66 t/s PP, 10.1 t/s TG&lt;/li&gt; &lt;li&gt;5090s at X8/X8 5.0, 4090s, and Ampere trio at X4 4.0: 244 t/s PP, 11.52 t/s TG&lt;/li&gt; &lt;li&gt;5090(1) at X16 5.0, 5090(2) at X4 5.0, all the rest at X4 4.0: 312.64 t/s PP, 11.58 t/s TG&lt;/li&gt; &lt;li&gt;5090s at X16 5.0, all the rest at X16 4.0: 360.86 t/s PP, 11.71 t/s TG&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Kimi K2 Instruct Q2_K_XL, offloading about 160GB to CPU:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Loading with:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./llama-server \ -m '/run/media/pancho/Drive954GB/models_llm_1tb/Kimi-K2-Thinking-UD-Q2_K_XL-00001-of-00008.gguf' \ -c 32768 \ --no-mmap \ -ngl 999 \ -ot &amp;quot;blk.(0|1|2|3).ffn.=CUDA0&amp;quot; \ -ot &amp;quot;blk.(4|5|6|7).ffn.=CUDA1&amp;quot; \ -ot &amp;quot;blk.(8|9|10).ffn.=CUDA2&amp;quot; \ -ot &amp;quot;blk.(11|12|13).ffn.=CUDA3&amp;quot; \ -ot &amp;quot;blk.(14|15|16).ffn.=CUDA4&amp;quot; \ -ot &amp;quot;blk.(17|18|19|20|21|22|23).ffn.=CUDA5&amp;quot; \ -ot &amp;quot;blk.(24|25|26|27|28|29|30).ffn.=CUDA6&amp;quot; \ -ot &amp;quot;blk.31.ffn_down_exps.weight=CUDA0&amp;quot; \ -ot &amp;quot;blk.32.ffn_down_exps.weight=CUDA2&amp;quot; \ -ot &amp;quot;blk.33.ffn_down_exps.weight=CUDA3&amp;quot; \ -ot &amp;quot;blk.33.ffn_gate_exps.weight=CUDA1&amp;quot; \ -ot &amp;quot;blk.(31|32|33).ffn_(norm|gate_inp|gate_shexp|down_shexp|up_shexp).weight=CUDA1&amp;quot; \ -ot &amp;quot;exps=CPU&amp;quot; \ -mg 0 \ -ub 2048 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I have these results:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;5090s at X8/X8 5.0, 4090s, A6000, A40 at X4 4.0 and 3090 at X1 3.0: 179 t/s PP, 11.34t/s TG.&lt;/li&gt; &lt;li&gt;5090s at X8/X8 5.0, 4090s, and Ampere trio at X4 4.0: 198 t/s PP y 11.6 t/s TG.&lt;/li&gt; &lt;li&gt;5090(1) at X16 5.0, 5090(2) at X4 5.0, all the rest at X4 4.0: 219.08 t/s PP, 11.91 t/s TG&lt;/li&gt; &lt;li&gt;5090s at X16 5.0, all the rest at X16 4.0: 248 t/s PP, 11.95 t/s TG&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Table for TL:DR&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Configuration&lt;/th&gt; &lt;th align="left"&gt;GLM 4.7 Q4_K_XL(196GB, GPU only)&lt;/th&gt; &lt;th align="left"&gt;DeepSeek V3 IQ4_XS(~120GB CPU offload)&lt;/th&gt; &lt;th align="left"&gt;Kimi K2 Q2_K_XL(~160GB CPU offload)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Data&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;PP / TG (t/s)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;PP / TG (t/s)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;PP / TG (t/s)&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Config 1&lt;/strong&gt;:5090s: X8/X8 Gen5, 4090s/A6000/A40: X4 Gen4, 3090: X1 Gen3&lt;/td&gt; &lt;td align="left"&gt;665.46 / 25.90&lt;/td&gt; &lt;td align="left"&gt;195.66 / 10.10&lt;/td&gt; &lt;td align="left"&gt;179.00 / 11.34&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Config 2&lt;/strong&gt;:5090s: X8/X8 Gen5, All others: X4 Gen4&lt;/td&gt; &lt;td align="left"&gt;765.51 / 26.18 &lt;em&gt;(+15% / +1%)&lt;/em&gt;&lt;/td&gt; &lt;td align="left"&gt;244.00 / 11.52 &lt;em&gt;(+25% / +14%)&lt;/em&gt;&lt;/td&gt; &lt;td align="left"&gt;198.00 / 11.60 &lt;em&gt;(+11% / +2%)&lt;/em&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Config 3&lt;/strong&gt;:5090#1: X16 Gen5, 5090#2: X4 Gen5,Others: X4 Gen4&lt;/td&gt; &lt;td align="left"&gt;940.00 / 26.75 &lt;em&gt;(+41% / +3%)&lt;/em&gt;&lt;/td&gt; &lt;td align="left"&gt;312.64 / 11.58 &lt;em&gt;(+60% / +15%)&lt;/em&gt;&lt;/td&gt; &lt;td align="left"&gt;219.08 / 11.91 &lt;em&gt;(+22% / +5%)&lt;/em&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Config 4&lt;/strong&gt;:5090s: X16 Gen5, All others: X16 Gen4&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;1170.00 / 27.64&lt;/strong&gt; (+76% / +7%)&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;360.86 / 11.71&lt;/strong&gt; (+84% / +16%)&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;248.00 / 11.95&lt;/strong&gt; (+39% / +5%)&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;As you can see here, TG is not that impacted by PCIe, but PP for sure it is, even on llamacpp!&lt;/p&gt; &lt;h1&gt;Some questions you may have&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Why?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Well, on this case it was mostly about cost. I already had the GPUs, the RAM and I was planning to get a Theadripper 9955WX plus a WRX90 motherboard.&lt;/p&gt; &lt;p&gt;But well, you know, RAM prices now are absurd.&lt;/p&gt; &lt;p&gt;On Chile, I have these prices:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Theadripper 9955WX: 2000USD&lt;/li&gt; &lt;li&gt;Cheapest WRX90 board: 1800USD (alternative is Gigabyte AI TOP for 1500USD)&lt;/li&gt; &lt;li&gt;Cheapest 128GB DDR5 RDIMM, 4800Mhz: 4000USD (yes, I'm not even joking)&lt;/li&gt; &lt;li&gt;256GB DDR5 RDIMM 4800Mhz: 6500USD&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;RAM bandwidth would have been a bit better, and also 128 5.0 lanes, I know.&lt;/p&gt; &lt;p&gt;But you're comparing a 5.0 switch (2500USD) a 4.0 switch (400USD) for a total of 2900USD, vs 7800 to 10300USD. So about 3x-4x the price.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why not a 6000 PRO?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;There was no stock of the 6000 PRO for most of the 2025. Just on December they arrived, but they go for 12000USD each. You can get 4x5090s for that price here.&lt;/p&gt; &lt;p&gt;But I understand you save: power, space and heat. I'm still thinking about it.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How do you fit so many GPUs?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;With a custom self made wood rack! I have some pics. It's not the prettiest, but it works.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/0jlsnu6s9qdg1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fbde9de64eeb52ee942786486b16fdf870a7cd6a"&gt;Multiple fans&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ddhnurlt9qdg1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=388ba71d88968adc89321ff1a80c3b84416fed71"&gt;ConnectX 3 with a fan, and MCIO retimer behind&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Final words, and please let me know what can I test!&lt;/h1&gt; &lt;p&gt;Hope you guys find informative, and if you can let me know what can I test here, let me know.&lt;/p&gt; &lt;p&gt;Have fun on the LLM side!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/panchovix"&gt; /u/panchovix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qeimyi/7_gpus_at_x16_50_and_40_on_am5_with_gen54/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qeimyi/7_gpus_at_x16_50_and_40_on_am5_with_gen54/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qeimyi/7_gpus_at_x16_50_and_40_on_am5_with_gen54/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-16T15:15:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1qehf0p</id>
    <title>Maxsun joins Sparkle in making Intel Arc B60 Pro GPUs available to regular consumers, with up to 48GB VRAM</title>
    <updated>2026-01-16T14:28:44+00:00</updated>
    <author>
      <name>/u/reps_up</name>
      <uri>https://old.reddit.com/user/reps_up</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qehf0p/maxsun_joins_sparkle_in_making_intel_arc_b60_pro/"&gt; &lt;img alt="Maxsun joins Sparkle in making Intel Arc B60 Pro GPUs available to regular consumers, with up to 48GB VRAM" src="https://external-preview.redd.it/L3K-FrP5rshi6B9P4GPKPRWImqHp_K0A7GfSUbA2aKk.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7e8fca654e67fa2c4d7aa0fa049bfeb96b0c4231" title="Maxsun joins Sparkle in making Intel Arc B60 Pro GPUs available to regular consumers, with up to 48GB VRAM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/reps_up"&gt; /u/reps_up &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.pcguide.com/news/maxsun-joins-sparkle-in-making-intel-arc-b60-pro-gpus-available-to-regular-consumers-with-up-to-48gb-vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qehf0p/maxsun_joins_sparkle_in_making_intel_arc_b60_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qehf0p/maxsun_joins_sparkle_in_making_intel_arc_b60_pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-16T14:28:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1qe2i88</id>
    <title>My story of underestimating /r/LocalLLaMA's thirst for VRAM</title>
    <updated>2026-01-16T01:36:54+00:00</updated>
    <author>
      <name>/u/EmPips</name>
      <uri>https://old.reddit.com/user/EmPips</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qe2i88/my_story_of_underestimating_rlocalllamas_thirst/"&gt; &lt;img alt="My story of underestimating /r/LocalLLaMA's thirst for VRAM" src="https://preview.redd.it/lwod7dtv7mdg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=660b9563f79c6dad7bb50c952f2b76bf9062955b" title="My story of underestimating /r/LocalLLaMA's thirst for VRAM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EmPips"&gt; /u/EmPips &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lwod7dtv7mdg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qe2i88/my_story_of_underestimating_rlocalllamas_thirst/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qe2i88/my_story_of_underestimating_rlocalllamas_thirst/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-16T01:36:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1qee2de</id>
    <title>I fucking love this community</title>
    <updated>2026-01-16T11:57:48+00:00</updated>
    <author>
      <name>/u/alhinai_03</name>
      <uri>https://old.reddit.com/user/alhinai_03</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Thank you guys, thanks to everyone who took the time to write a comment or a post explaining, teaching people how things work, the people behind llama.cpp, vllm, and all the contributors who keep the open-source community thriving.&lt;/p&gt; &lt;p&gt;I'm able to run huge models on my weak ass pc from 10 years ago relatively fast, my fastest one being nemotron-3-nano-30B-a3b-iq4_nl running @14-13.5 t/s with 65k context. While my actual GPU having only 4GB of vram, that's fucking ridiculous and it blows my mind everytime that I'm able to run these models.&lt;/p&gt; &lt;p&gt;What's been key for me is having a good amount of system memory, and as long as the model is a MoE architecture they run pretty decently.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/alhinai_03"&gt; /u/alhinai_03 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qee2de/i_fucking_love_this_community/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qee2de/i_fucking_love_this_community/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qee2de/i_fucking_love_this_community/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-16T11:57:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1qefa7q</id>
    <title>GPT-5.2 xhigh, GLM-4.7, Kimi K2 Thinking, DeepSeek v3.2 on Fresh SWE-rebench (December 2025)</title>
    <updated>2026-01-16T12:59:07+00:00</updated>
    <author>
      <name>/u/CuriousPlatypus1881</name>
      <uri>https://old.reddit.com/user/CuriousPlatypus1881</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qefa7q/gpt52_xhigh_glm47_kimi_k2_thinking_deepseek_v32/"&gt; &lt;img alt="GPT-5.2 xhigh, GLM-4.7, Kimi K2 Thinking, DeepSeek v3.2 on Fresh SWE-rebench (December 2025)" src="https://external-preview.redd.it/t4cNt5D638DSOJgsxl8f-7IwJhLpxHIh7HxK5GHcBJE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9b72b5025e78c2cc97de15c8fea348f262235ecb" title="GPT-5.2 xhigh, GLM-4.7, Kimi K2 Thinking, DeepSeek v3.2 on Fresh SWE-rebench (December 2025)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, I‚Äôm Anton from Nebius.&lt;/p&gt; &lt;p&gt;We‚Äôve updated the &lt;strong&gt;SWE-bench leaderboard&lt;/strong&gt; with our &lt;strong&gt;December runs&lt;/strong&gt; on &lt;strong&gt;48 fresh GitHub PR tasks&lt;/strong&gt; (PRs created in the previous month only). The setup is standard SWE-bench: models read real PR issues, edit code, run tests, and must make the full suite pass.&lt;/p&gt; &lt;p&gt;A few observations from this release:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Claude Opus 4.5&lt;/strong&gt; leads this snapshot at &lt;strong&gt;63.3% resolved rate&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPT-5.2 (extra high effort)&lt;/strong&gt; follows closely at &lt;strong&gt;61.5%&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Gemini 3 Flash Preview&lt;/strong&gt; slightly outperforms &lt;strong&gt;Gemini 3 Pro Preview&lt;/strong&gt; (60.0% vs 58.9%), despite being smaller and cheaper.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GLM-4.7&lt;/strong&gt; is currently the strongest open-source model on the leaderboard, ranking alongside closed models like GPT-5.1-codex.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPT-OSS-120B&lt;/strong&gt; shows a large jump in performance when run in high-effort reasoning mode, highlighting the impact of inference-time scaling.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Looking forward to your thoughts and feedback.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CuriousPlatypus1881"&gt; /u/CuriousPlatypus1881 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://swe-rebench.com/?insight=dec_2025"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qefa7q/gpt52_xhigh_glm47_kimi_k2_thinking_deepseek_v32/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qefa7q/gpt52_xhigh_glm47_kimi_k2_thinking_deepseek_v32/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-16T12:59:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
