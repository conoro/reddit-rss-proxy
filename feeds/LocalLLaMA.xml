<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-22T10:55:45+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1rbi8ht</id>
    <title>Building a tunable RAG pipeline, should I open source it? No promotion, just need ideas for roadmap</title>
    <updated>2026-02-22T10:11:48+00:00</updated>
    <author>
      <name>/u/gg223422</name>
      <uri>https://old.reddit.com/user/gg223422</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I've been working on a RAG system as a side project for the past 4-5 months, and I'm at a point where I'm not sure how to evolve it. A friend suggested I consider open-sourcing it or at least sharing it publicly to get feedback and find people working on similar problems.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Background on why I started this:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I've been following companies like Glean for years - the idea of building truly intelligent enterprise search that actually understands your organization's knowledge. That got me thinking about what it takes to build something like that, and I realized most RAG frameworks treat the whole pipeline as a black box. When you want to tune things properly or understand what's working and why, it becomes trial-and-error guesswork.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What I'm building:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I've been taking my time - spending weeks reading research papers, testing different algorithms, making sure I actually understand the theory before coding each layer. The core idea is making every component (chunking, retrieval, reranking, generation) completely modular and independently evaluable. Want to try a different vector database? Or swap embedding models? One line of code. Then run proper benchmarks with ground-truth datasets and see exactly what improved.&lt;/p&gt; &lt;p&gt;I'm not a software engineer by background (I'm DS/ML), but I do have hands-on experience with search systems in production environments. So I'm not coming at this completely blind - I understand search/retrieval fundamentals - I've just been learning the proper software architecture patterns to make everything maintainable and extensible, with comprehensive testing so components can actually be swapped without breaking things.&lt;/p&gt; &lt;p&gt;I've also spent good amount of time and built a monitoring/tuning system that can optimize the orchestration automatically based on input data - trying to avoid manual tweaking for every use case. For example, when I realized chunking strategy was significantly affecting retrieval quality, the monitoring framework started running Bayesian grid searches across different chunk sizes to find the optimal configuration for each dataset. Being able to measure and optimize these things independently is the whole point.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why I think this matters:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Honestly, I believe anything we're going to build with agentic workflows in the near future - whether that's AI assistants, automated research systems, or whatever comes next - it's all going to be garbage-in-garbage-out if the core retrieval layer isn't solid. You can't build reliable agents on top of a black-box RAG system you can't tune or debug.&lt;/p&gt; &lt;p&gt;So if I can build something that's actually tunable, scientifically testable, and adaptable to different use cases, it could be a foundation for those kinds of systems. But that's the vision - I don't have a clear roadmap on how to get there or even if I'm solving the right problems.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Where my head's at (future possibilities):&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;There are ideas I'm considering as the project evolves - graph databases for relationship-aware search, user-based ML models for personalization, focusing on specific verticals like enterprise B2B. There are tons I wrote down as possible implementations. But I'm not blindly implementing everything. Maybe focusing on a single vertical makes more sense than staying too general, but these are all just thoughts at this stage.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Where I'm at right now:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I started this solo as a learning project, but the scope keeps growing. I'm realizing to properly execute on this vision, I'd probably need help from people with skills I lack - data engineers for robust ingestion pipelines, DevOps for proper deployment, software engineers for production-grade architecture. But honestly, things are still evolving and I'm not even sure what the final product should look like yet.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My main questions:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Going open-source - Has anyone here gone from solo project ‚Üí open source? What was that transition like? Did you finish everything first or just put it out there incomplete? How do you even know when it's &amp;quot;ready&amp;quot;? I've never done this before and feeling a bit lost on whether this is worth pursuing publicly or keeping as a personal learning project. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Finding collaborators - How do you actually find people to work with on this stuff/collaborate? Posting on forums, GitHub, or just staying solo? Does it actually lead to meaningful collaboration or just noise?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;What to prioritize - Should I keep obsessing over the evaluation/tuning infrastructure or focus on missing pieces like data ingestion? Not sure where the real value is.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Any thoughts from people who've navigated this? Many thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gg223422"&gt; /u/gg223422 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbi8ht/building_a_tunable_rag_pipeline_should_i_open/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbi8ht/building_a_tunable_rag_pipeline_should_i_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbi8ht/building_a_tunable_rag_pipeline_should_i_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T10:11:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1rb062y</id>
    <title>Have you ever hesitated before typing something into ChatGPT or Claude? Are you worried about the amount of information these third party providers have about you? What are the most common use cases you worry about</title>
    <updated>2026-02-21T19:30:18+00:00</updated>
    <author>
      <name>/u/alichherawalla</name>
      <uri>https://old.reddit.com/user/alichherawalla</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What are different use cases where you'd rather not send your data to the cloud but still be able to leverage AI fully?&lt;/p&gt; &lt;p&gt;Is it legal documents, or financial documents, personal information? Please feel free to be as detailed as you'd like.&lt;/p&gt; &lt;p&gt;Thank you&lt;/p&gt; &lt;p&gt;Full disclosure I'm building something in the space. However, it's free, totally on device , and private.&lt;/p&gt; &lt;p&gt;All I want to do is make it better. Appreciate the help.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/alichherawalla"&gt; /u/alichherawalla &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rb062y/have_you_ever_hesitated_before_typing_something/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rb062y/have_you_ever_hesitated_before_typing_something/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rb062y/have_you_ever_hesitated_before_typing_something/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T19:30:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1rb8mzd</id>
    <title>This is how SLOW Local LLMs Are On My Framework 13 AMD Strix Point</title>
    <updated>2026-02-22T01:29:23+00:00</updated>
    <author>
      <name>/u/m3thos</name>
      <uri>https://old.reddit.com/user/m3thos</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I did a deep dive to understand why and how local models performed as they did in my laptop, decided to save this because I haven't seen online a good breakdown of how this performance works out.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/m3thos"&gt; /u/m3thos &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://msf.github.io/blogpost/local-llm-performance-framework13.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rb8mzd/this_is_how_slow_local_llms_are_on_my_framework/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rb8mzd/this_is_how_slow_local_llms_are_on_my_framework/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T01:29:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbgzv5</id>
    <title>The power of the sun in the palm of your hand ( Locally running Qwen 3 TTS model : LocalEcho )</title>
    <updated>2026-02-22T08:55:37+00:00</updated>
    <author>
      <name>/u/No-Cap-8145</name>
      <uri>https://old.reddit.com/user/No-Cap-8145</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbgzv5/the_power_of_the_sun_in_the_palm_of_your_hand/"&gt; &lt;img alt="The power of the sun in the palm of your hand ( Locally running Qwen 3 TTS model : LocalEcho )" src="https://external-preview.redd.it/eWljZGM0enZmMGxnMYJIVtXgOl3k8mbr55raX9pU_Bwp14RUXuSkRBoaV_CZ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2916e5c55cb1925d9d417d562cee796cdcdde588" title="The power of the sun in the palm of your hand ( Locally running Qwen 3 TTS model : LocalEcho )" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;‚ÄúI am not running on the clouds‚Ä¶ I am running locally on your computer.‚Äù&lt;/p&gt; &lt;p&gt;This project actually started while I was building a &lt;strong&gt;streaming agent audio call service&lt;/strong&gt;. I needed low-latency TTS that I could fully control ‚Äî no API limits, no external calls, no sending voice data to someone else‚Äôs servers.&lt;/p&gt; &lt;p&gt;That‚Äôs how &lt;strong&gt;LocalEcho&lt;/strong&gt; was born.&lt;/p&gt; &lt;p&gt;It‚Äôs a small open-source project that lets you run Qwen 3 TTS completely locally. Everything happens on your machine ‚Äî no API keys, no cloud dependency, no hidden usage costs.&lt;/p&gt; &lt;p&gt;It currently supports:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;üéô Custom Voice ‚Äî choose from 9 high-quality preset speakers for quick narration&lt;/li&gt; &lt;li&gt;‚úèÔ∏è Voice Design ‚Äî describe a voice in plain English and generate it&lt;/li&gt; &lt;li&gt;üî¨ Voice Cloning ‚Äî record 5‚Äì10 seconds of a voice and clone it locally&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Minimum requirement right now is &lt;strong&gt;8GB RAM&lt;/strong&gt; for a smooth experience with Qwen 3 TTS.&lt;/p&gt; &lt;p&gt;It‚Äôs not tied to one specific environment. The idea was to make it practical across common dev setups:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;üçé &lt;strong&gt;Mac (Apple Silicon)&lt;/strong&gt; ‚Äî runs using MLX with Metal GPU acceleration. Fully supported.&lt;/li&gt; &lt;li&gt;üêß &lt;strong&gt;Linux (NVIDIA GPU)&lt;/strong&gt; ‚Äî PyTorch + CUDA. Fully supported.&lt;/li&gt; &lt;li&gt;ü™ü &lt;strong&gt;Windows (NVIDIA GPU)&lt;/strong&gt; ‚Äî PyTorch + CUDA. Fully supported.&lt;/li&gt; &lt;li&gt;üíª &lt;strong&gt;CPU-only&lt;/strong&gt; ‚Äî Works with PyTorch, but it‚Äôs slow. Good for testing, not ideal for real usage.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If that doesn‚Äôt fit your setup, the project is structured so you can swap in other open-source TTS models with lighter requirements. The goal is flexibility, not locking into one model.&lt;/p&gt; &lt;p&gt;This isn‚Äôt a SaaS or wrapper around someone else‚Äôs API. It‚Äôs more of a practical local GenAI toolkit ‚Äî useful if you're:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;building voice-enabled agents&lt;/li&gt; &lt;li&gt;experimenting with offline AI systems&lt;/li&gt; &lt;li&gt;working on indie games or narration tools&lt;/li&gt; &lt;li&gt;or just curious how modern TTS pipelines work under the hood&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Would genuinely appreciate feedback ‚Äî especially around performance on different machines, latency for streaming use cases, and model alternatives people are experimenting with.&lt;/p&gt; &lt;p&gt;My first post ever on reddit. Let me know if any feedbacks.&lt;/p&gt; &lt;p&gt;Repo:&lt;br /&gt; &lt;a href="https://github.com/AnuragGupta93/LocalEcho"&gt;https://github.com/AnuragGupta93/LocalEcho&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Cap-8145"&gt; /u/No-Cap-8145 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/dcwp22zvf0lg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbgzv5/the_power_of_the_sun_in_the_palm_of_your_hand/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbgzv5/the_power_of_the_sun_in_the_palm_of_your_hand/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T08:55:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbhksy</id>
    <title>Google Open-Sources NPU IP, Synaptics Implements It</title>
    <updated>2026-02-22T09:31:16+00:00</updated>
    <author>
      <name>/u/Dontdoitagain69</name>
      <uri>https://old.reddit.com/user/Dontdoitagain69</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.eetimes.com/google-open-sources-npu-ip-synaptics-implements-it/"&gt;Google Open-Sources NPU IP, Synaptics Implements It - EE Times&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dontdoitagain69"&gt; /u/Dontdoitagain69 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbhksy/google_opensources_npu_ip_synaptics_implements_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbhksy/google_opensources_npu_ip_synaptics_implements_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbhksy/google_opensources_npu_ip_synaptics_implements_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T09:31:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1raucof</id>
    <title>Wave Field LLM ‚Äî O(n log n) attention via wave equation dynamics</title>
    <updated>2026-02-21T15:46:07+00:00</updated>
    <author>
      <name>/u/Murky-Sign37</name>
      <uri>https://old.reddit.com/user/Murky-Sign37</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been working on an alternative attention mechanism that treats language as a physical field system instead of using standard O(n¬≤) self-attention.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How it works:&lt;/strong&gt; - Tokens are mapped onto a continuous 1D field - Information propagates via damped wave equations: k(t) = exp(-Œ±¬∑t)¬∑cos(œâ¬∑t + œÜ) - Each attention head has just 3 learnable physics parameters (frequency, damping, phase) - Convolution computed via FFT in O(n log n) - Heads self-organize into different roles (local grammar, medium context, long-range)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Results (WikiText-2, 6M params, character tokenizer):&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;PPL&lt;/th&gt; &lt;th&gt;Accuracy&lt;/th&gt; &lt;th&gt;Complexity&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Standard Transformer&lt;/td&gt; &lt;td&gt;5.9&lt;/td&gt; &lt;td&gt;51.0%&lt;/td&gt; &lt;td&gt;O(n¬≤)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Wave Field V3.5&lt;/td&gt; &lt;td&gt;6.2&lt;/td&gt; &lt;td&gt;50.5%&lt;/td&gt; &lt;td&gt;O(n log n)&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;At longer sequences the savings grow: 31x at 2K tokens, 107x at 8K, 367x at 32K.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Known limitations:&lt;/strong&gt; - With BPE tokenizer (8K vocab), there's a significant capacity gap vs standard transformer - This is a model capacity issue at small scale, not an architecture flaw - Currently scaling to 100M params to see if the gap closes&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What's unique:&lt;/strong&gt; - Every bug during development was found through physics-based diagnostics (energy flow, conservation, causality tests) ‚Äî not guessing - Cross-head field coupling and wave interference for information routing - Not a Mamba/Hyena variant ‚Äî different approach entirely&lt;/p&gt; &lt;p&gt;Code: &lt;a href="https://github.com/badaramoni/wave-field-llm"&gt;https://github.com/badaramoni/wave-field-llm&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer questions about the physics, architecture decisions, or results.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Murky-Sign37"&gt; /u/Murky-Sign37 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1raucof/wave_field_llm_on_log_n_attention_via_wave/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1raucof/wave_field_llm_on_log_n_attention_via_wave/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1raucof/wave_field_llm_on_log_n_attention_via_wave/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T15:46:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbiezw</id>
    <title>Microsoft announces powerful new chip for AI inference</title>
    <updated>2026-02-22T10:22:32+00:00</updated>
    <author>
      <name>/u/Dontdoitagain69</name>
      <uri>https://old.reddit.com/user/Dontdoitagain69</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://techcrunch.com/2026/01/26/microsoft-announces-powerful-new-chip-for-ai-inference/"&gt;https://techcrunch.com/2026/01/26/microsoft-announces-powerful-new-chip-for-ai-inference/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dontdoitagain69"&gt; /u/Dontdoitagain69 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbiezw/microsoft_announces_powerful_new_chip_for_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbiezw/microsoft_announces_powerful_new_chip_for_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbiezw/microsoft_announces_powerful_new_chip_for_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T10:22:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1rawge5</id>
    <title>40,000+ AI Agents Exposed to the Internet with Full System Access</title>
    <updated>2026-02-21T17:07:50+00:00</updated>
    <author>
      <name>/u/Monterey-Jack</name>
      <uri>https://old.reddit.com/user/Monterey-Jack</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rawge5/40000_ai_agents_exposed_to_the_internet_with_full/"&gt; &lt;img alt="40,000+ AI Agents Exposed to the Internet with Full System Access" src="https://external-preview.redd.it/QJge18zM6lp5gsWJUdMOifSYjcNp_r7jcsM3Yu8BUUo.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=03267a48c2b49bc6f4cb6f80e3fcb85dc7645091" title="40,000+ AI Agents Exposed to the Internet with Full System Access" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Monterey-Jack"&gt; /u/Monterey-Jack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://threatroad.substack.com/p/40000-ai-agents-exposed-to-the-internet"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rawge5/40000_ai_agents_exposed_to_the_internet_with_full/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rawge5/40000_ai_agents_exposed_to_the_internet_with_full/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T17:07:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbhgcv</id>
    <title>smolcluster: Educational library to cluster your everyday devices to train/inference LLMs</title>
    <updated>2026-02-22T09:23:38+00:00</updated>
    <author>
      <name>/u/East-Muffin-6472</name>
      <uri>https://old.reddit.com/user/East-Muffin-6472</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For the past month, I've been working on something educational for the community on concepts related to distributed systems, particularly for training LLMs!&lt;/p&gt; &lt;p&gt;I was amazed by the work done by people at @/exolabs where they provide amazing software for connecting Mac minis/studios together to run inference on huge models!&lt;/p&gt; &lt;p&gt;I thought of doing the same, but to learn the concepts from the ground up‚Äînetworking, OS, and distributed systems‚ÄîI decided to reimplement popular algorithms like Data/Model Parallelism, FSDP, and EDP, all from scratch using only Python's socket library.&lt;/p&gt; &lt;p&gt;So, I made &lt;a href="https://www.smolcluster.com"&gt;smolcluster&lt;/a&gt;&lt;/p&gt; &lt;p&gt;An educational, distributed learning library for training and inference of neural nets on heterogeneous hardware!&lt;/p&gt; &lt;p&gt;This is primarily meant for those who want to understand various distributed training algorithms in a simple manner, as single-page Python files.&lt;/p&gt; &lt;p&gt;Current implementations:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Elastic Distributed Parallelism (EDP)&lt;/li&gt; &lt;li&gt;Synchronous Parameter Server (SyncPS)&lt;/li&gt; &lt;li&gt;Fully Sharded Data Parallelism (FSDP)&lt;/li&gt; &lt;li&gt;Standard Data Parallelism (DP)&lt;/li&gt; &lt;li&gt;Model Parallelism (MP)&lt;/li&gt; &lt;li&gt;Pipeline Parallelism (PP)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Currently under development and cleaning up the codebase is being done. &lt;/p&gt; &lt;p&gt;Tested on the a cluster of Mac minis, raspberry 4/5, 4050 GPU and Jetson Orin Nano!&lt;/p&gt; &lt;p&gt;Check it out: &lt;a href="https://github.com/YuvrajSingh-mist/smolcluster/tree/master"&gt;Code&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Perfect for students, researchers, or anyone curious about how distributed training actually works under the hood!&lt;/p&gt; &lt;p&gt;Would love to get your feedback!&lt;/p&gt; &lt;p&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/East-Muffin-6472"&gt; /u/East-Muffin-6472 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbhgcv/smolcluster_educational_library_to_cluster_your/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbhgcv/smolcluster_educational_library_to_cluster_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbhgcv/smolcluster_educational_library_to_cluster_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T09:23:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbfasf</id>
    <title>Fine-Tuning Qwen 4B for Niche Code Generation: Need Tips on Configs, Overfitting &amp; Small Datasets?</title>
    <updated>2026-02-22T07:13:28+00:00</updated>
    <author>
      <name>/u/dyeusyt</name>
      <uri>https://old.reddit.com/user/dyeusyt</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So am working on my thesis project which involves fine-tuning a small language model for a specific code generation task in a niche domain (Typescript)&lt;/p&gt; &lt;p&gt;I'm leaning toward the Qwen family of models. I started by fine-tuning the 8B version, but it didn't feel like a true SLM in terms of consumer-hardware-efficiency and size, so I'm downgrading to the 4B variant for better adherence to SLM part.&lt;/p&gt; &lt;p&gt;My main concern is my dataset: It's high-quality but small, with only 700-800 &lt;code&gt;{prompt,completion}&lt;/code&gt; pairs. Some pairs are distilled from larger LLMs, while others come from real code snippets paired with synthetically generated prompts. The data is straightforward (no chain-of-thought reasoning) but it includes potential noise: like non-code elements in code files (placeholders, plain text, or image paths). I want to train the model effectively so it performs well on my use case without picking up this noise or overfitting to the limited examples&lt;/p&gt; &lt;p&gt;For context I'm currently training on Google Colab with an A100 GPU. Here's the configuration I'm using, based on recommendations from Reddit threads and Unsloth docs:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;model = FastLanguageModel.get_peft_model( model, r=64, lora_alpha=128, lora_dropout=0.05, target_modules=[ &amp;quot;q_proj&amp;quot;, &amp;quot;k_proj&amp;quot;, &amp;quot;v_proj&amp;quot;, &amp;quot;o_proj&amp;quot;, # Self-attention &amp;quot;gate_proj&amp;quot;, # MLP gate for code generation patterns ], bias=&amp;quot;none&amp;quot;, use_gradient_checkpointing=&amp;quot;unsloth&amp;quot;, random_state=3407, use_rslora=False, loftq_config=None, ) training_args = SFTConfig( output_dir=&amp;quot;./qwen-8b-a100&amp;quot;, per_device_train_batch_size=16, gradient_accumulation_steps=2, per_device_eval_batch_size=16, num_train_epochs=3, max_steps=-1, # Use epochs (not max_steps) learning_rate=2e-4, lr_scheduler_type=&amp;quot;cosine&amp;quot;, warmup_ratio=0.05, # 5% warmup optim=&amp;quot;adamw_8bit&amp;quot;, # Memory efficient, works well with LoRA weight_decay=0.01, # Light regularization fp16=False, # Don't use FP16 on A100 bf16=True, # A100 has native BF16 support - MUCH better! tf32=True, # Enable TensorFloat-32 for even faster matmuls dataloader_num_workers=4, # Parallel data loading dataloader_pin_memory=True, # Faster GPU transfers logging_steps=5, eval_strategy=&amp;quot;steps&amp;quot;, eval_steps=10, save_strategy=&amp;quot;steps&amp;quot;, save_steps=10, # Match eval_steps save_total_limit=3, # Keep 3 best load_best_model_at_end=True, metric_for_best_model=&amp;quot;eval_loss&amp;quot;, greater_is_better=False, packing=True, max_seq_length=4096, seed=3407, report_to=&amp;quot;none&amp;quot;, dataset_text_field=&amp;quot;text&amp;quot;, ) trainer = SFTTrainer( model=model, args=training_args, processing_class=tokenizer, train_dataset=train_dataset_formatted, eval_dataset=val_dataset_formatted, ) # Using Unsloth's gradient accumulation fix from unsloth import unsloth_train trainer_stats = unsloth_train(trainer) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I'm fairly new to fine-tuning (about 60% VibeCoding; 40% reading docs) and the results so far aren't great. The model underperforms on my tasks - The 8B one.&lt;/p&gt; &lt;p&gt;So I'm reaching out to folks who've worked with Qwen models: What configs have worked well for you, especially for small datasets and code generation? Any tips on preventing overfitting? Are there must-read docs or guides to get started properly?&lt;/p&gt; &lt;p&gt;Thanks in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dyeusyt"&gt; /u/dyeusyt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbfasf/finetuning_qwen_4b_for_niche_code_generation_need/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbfasf/finetuning_qwen_4b_for_niche_code_generation_need/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbfasf/finetuning_qwen_4b_for_niche_code_generation_need/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T07:13:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1rb61og</id>
    <title>Nanbeige 4.1 is the best small LLM, it crush qwen 4b</title>
    <updated>2026-02-21T23:32:59+00:00</updated>
    <author>
      <name>/u/Individual-Source618</name>
      <uri>https://old.reddit.com/user/Individual-Source618</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Self-explenatory, try it its insane if you give him enough room to think. Its my go to local llm now. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Individual-Source618"&gt; /u/Individual-Source618 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rb61og/nanbeige_41_is_the_best_small_llm_it_crush_qwen_4b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rb61og/nanbeige_41_is_the_best_small_llm_it_crush_qwen_4b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rb61og/nanbeige_41_is_the_best_small_llm_it_crush_qwen_4b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T23:32:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbdsds</id>
    <title>Best Model for single 3090 in 2026?</title>
    <updated>2026-02-22T05:47:26+00:00</updated>
    <author>
      <name>/u/myusuf3</name>
      <uri>https://old.reddit.com/user/myusuf3</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Running a single RTX 3090 (24GB VRAM) and looking for the best overall model in 2026 for coding + reasoning.&lt;/p&gt; &lt;p&gt;Main priorities:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Strong code generation (Go/TypeScript)&lt;/li&gt; &lt;li&gt;Good reasoning depth&lt;/li&gt; &lt;li&gt;Runs comfortably in 24GB (quantized is fine)&lt;/li&gt; &lt;li&gt;Decent latency on local inference&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What are you all running on a single 3090 right now? Qwen? DeepSeek? Something else? Would love specific model names + quant setups.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/myusuf3"&gt; /u/myusuf3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbdsds/best_model_for_single_3090_in_2026/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbdsds/best_model_for_single_3090_in_2026/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbdsds/best_model_for_single_3090_in_2026/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T05:47:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1ray0vz</id>
    <title>CXMT has been offering DDR4 chips at about half the prevailing market rate</title>
    <updated>2026-02-21T18:07:33+00:00</updated>
    <author>
      <name>/u/johnnyApplePRNG</name>
      <uri>https://old.reddit.com/user/johnnyApplePRNG</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ray0vz/cxmt_has_been_offering_ddr4_chips_at_about_half/"&gt; &lt;img alt="CXMT has been offering DDR4 chips at about half the prevailing market rate" src="https://external-preview.redd.it/0K-nyzO4raoSh4Q6Gk6oShuWqJIJ5QWuThVMJGt1MKU.png?width=216&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ad4ecef7d00fc6d2fefa3cec8972e26294886527" title="CXMT has been offering DDR4 chips at about half the prevailing market rate" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/johnnyApplePRNG"&gt; /u/johnnyApplePRNG &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.koreaherald.com/article/10679206"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ray0vz/cxmt_has_been_offering_ddr4_chips_at_about_half/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ray0vz/cxmt_has_been_offering_ddr4_chips_at_about_half/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T18:07:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbbmcl</id>
    <title>Ouro 2.6B GGUFs are up ‚Äî Q8_0 and Q4_K_M | Release notes + known limitations inside</title>
    <updated>2026-02-22T03:53:12+00:00</updated>
    <author>
      <name>/u/PruneLanky3551</name>
      <uri>https://old.reddit.com/user/PruneLanky3551</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;pre&gt;&lt;code&gt;GGUFs are live on HuggingFace: https://huggingface.co/scpalmetto/Ouro-2.6B-Thinking-Fixed Q8_0 (2.7GB) and Q4_K_M (1.6GB) ‚Äî works in LM Studio, Ollama, llama.cpp. --- ## What Ouro actually is (quick recap) Ouro is a looped inference model ‚Äî instead of running the transformer once per token, it passes the output back into itself for multiple reasoning iterations before committing. The &amp;quot;thinking&amp;quot; you see in the output is real: it's the model working through loops before settling on an answer. Full writeup in the original post. --- ## ‚ö†Ô∏è Release Notes ‚Äî What the GGUF does and doesn't include **GGUF format is standard Llama architecture.** Ouro has three custom architectural features that llama.cpp doesn't support. Here's exactly what happens to each: ### 1. Early Exit Gate (skipped) Ouro has an `early_exit_gate` (weight + bias) ‚Äî a learned mechanism that lets the model decide mid-sequence whether it has &amp;quot;thought enough&amp;quot; and can exit the loop early. **In the GGUF:** This tensor is skipped entirely. The model runs all layers every pass ‚Äî no early exit. This means the GGUF is slightly *more* compute than the original per loop, but also means it won't short-circuit on hard problems. ### 2. TL2 ‚Äî Second Layer Norms (skipped) Each transformer block in Ouro has two layer norms instead of one: - `input_layernorm` (TL1) ‚Äî standard, kept ‚úÖ - `input_layernorm_2` (TL2) ‚Äî Ouro's second norm pass, skipped ‚ùå - `post_attention_layernorm` (TL1) ‚Äî standard, kept ‚úÖ - `post_attention_layernorm_2` (TL2) ‚Äî skipped ‚ùå These are present across all 48 layers. The TL2 norms appear to act as a &amp;quot;re-centering&amp;quot; step between loop iterations. Skipping them means the GGUF doesn't re-normalize between passes the way the full model does. **Practical effect:** The GGUF reasoning is still good ‚Äî the base weights carry the learned behavior. But if you notice the thinking chains being slightly less structured than the HuggingFace original, this is why. ### 3. Python Looping / Inference Wrapper (not in any GGUF) The looping itself ‚Äî passing output back as input for N iterations ‚Äî is implemented in Python at the inference layer, not baked into the weights. **No GGUF can include this** because it's control flow, not a tensor. The GGUF runs one pass per token like any standard model. What you get is essentially the *distilled reasoning capability* that Ouro developed through loop training ‚Äî the model learned to think in its weights, even if the runtime loop isn't there. For the full looped experience, use the original safetensors on HuggingFace with the inference script. --- ## What still works great - The thinking style and extended reasoning ‚Äî very much present - The chattiness and self-correction behavior - Chat template (ChatML / `&amp;lt;|im_start|&amp;gt;` `&amp;lt;|im_end|&amp;gt;`) works out of the box - Q8_0 has minimal quality loss over F16; Q4_K_M is solid for RAM-constrained setups --- ## Files | File | Size | Use case | |------|------|----------| | `ouro-2.6b-q8_0.gguf` | 2.7GB | Best quality, ~3GB VRAM | | `ouro-2.6b-q4_k_m.gguf` | 1.6GB | Fastest, ~2GB VRAM | --- Happy to answer questions about the architecture, the conversion process, or what the looping actually does. &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PruneLanky3551"&gt; /u/PruneLanky3551 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbbmcl/ouro_26b_ggufs_are_up_q8_0_and_q4_k_m_release/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbbmcl/ouro_26b_ggufs_are_up_q8_0_and_q4_k_m_release/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbbmcl/ouro_26b_ggufs_are_up_q8_0_and_q4_k_m_release/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T03:53:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1rb270r</id>
    <title>PSA on public agentic tools and the speed they are shipping updates: recent Cline release had a package injected</title>
    <updated>2026-02-21T20:52:57+00:00</updated>
    <author>
      <name>/u/bakawolf123</name>
      <uri>https://old.reddit.com/user/bakawolf123</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Some of you may remember a post about sloppy OpenCode commit a week ago or so, unsurprisingly others are embracing vibe coding speed and sloppiness as well.&lt;/p&gt; &lt;p&gt;I've randomly stumbled upon&lt;br /&gt; &lt;a href="https://www.reddit.com/r/CLine/comments/1r9p3ww/supply_chain_attack_on_cline_installs_openclaw/"&gt;https://www.reddit.com/r/CLine/comments/1r9p3ww/supply_chain_attack_on_cline_installs_openclaw/&lt;/a&gt; apparently a recent Cline release had OpenClaw installer injected Their plugin in VSCode has some 3M installs, god knows how many standalone CLI. Then you see posts about 40k OpenClaw agents exposed globally. &lt;/p&gt; &lt;p&gt;Really wish there was more scrutiny involved by the teams developing new tools but everyone is just shipping first, then thinking about it. So at the very least make sure your VSCode extensions for are not on auto-update.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bakawolf123"&gt; /u/bakawolf123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rb270r/psa_on_public_agentic_tools_and_the_speed_they/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rb270r/psa_on_public_agentic_tools_and_the_speed_they/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rb270r/psa_on_public_agentic_tools_and_the_speed_they/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T20:52:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1rb4luf</id>
    <title>O-TITANS: Orthogonal LoRAs for Gemma 3 using Google's TITANS memory architecture</title>
    <updated>2026-02-21T22:32:47+00:00</updated>
    <author>
      <name>/u/Polymorphic-X</name>
      <uri>https://old.reddit.com/user/Polymorphic-X</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I've been working on a project I call &lt;strong&gt;O-TITANS&lt;/strong&gt; (Orthogonal Tensors for Independent Task Alignment). It's an Orthogonal LoRA approach specifically for Gemma 3 that incorporates the Google TITANS memory architecture.&lt;br /&gt; It was inspired by a project by ffurfaro on HF called &amp;quot;TPTT&amp;quot; that I just couldn't get to work.&lt;/p&gt; &lt;p&gt;I'm building this to wrap into my next project: &lt;strong&gt;MoOLE-T (Mixture of Orthogonal LoRA Experts - Titans)&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;The goal of MoOLE-T is to use a smaller 8B router to select one or more O-LoRAs to pass inference through simultaneously. The output will then get translated and de-conflicted at an &amp;quot;exit node&amp;quot; (a larger 20B-80B model). Theoretically, this creates a beefed-up MoE with specific skills like a tool belt. This approach should punch way above its weight class while needing only a fraction of the VRAM footprint. The best part? It's scalable to a stupid degree, since O-Loras don't interfere directly and can be multi-slotted. You could train 100+ O-LoRAs on individual skills and have a toolbelt of capabilities without bloating a base model to hundreds of billions of parameters.&lt;/p&gt; &lt;p&gt;Still working on the MoOLE-T polyswarm idea, but I'll do another post whenever that gets finished.&lt;/p&gt; &lt;p&gt;I just finished training an example &lt;code&gt;.pt&lt;/code&gt; file on Open-Platypus using mlabonne's Gemma3-12b-it-abliterated model as a base. It's on my hugginface if you want to test the non-interference claims yourselves.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Hugging Face (O-TITANS Gemma 3 Adapters):&lt;/strong&gt; &lt;a href="https://huggingface.co/paperscarecrow/O-TITANS-Gemma3/"&gt;https://huggingface.co/paperscarecrow/O-TITANS-Gemma3/&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Open to feedback and additional ideas. This is all an attempt to try and approach human-esque parallel skill processing and selection without absurd compute.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Polymorphic-X"&gt; /u/Polymorphic-X &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rb4luf/otitans_orthogonal_loras_for_gemma_3_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rb4luf/otitans_orthogonal_loras_for_gemma_3_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rb4luf/otitans_orthogonal_loras_for_gemma_3_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T22:32:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbi0ij</id>
    <title>Are AI coding agents (GPT/Codex, Claude Sonnet/Opus) actually helping you ship real products?</title>
    <updated>2026-02-22T09:58:38+00:00</updated>
    <author>
      <name>/u/darshan_aqua</name>
      <uri>https://old.reddit.com/user/darshan_aqua</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been testing AI coding agents a lot lately and I‚Äôm curious about real-world impact beyond demos.&lt;/p&gt; &lt;p&gt;A few things I keep noticing:&lt;/p&gt; &lt;p&gt;‚Ä¢ They seem great with Python + JavaScript frameworks, but weaker with Java, C++, or more structured systems ‚Äî is that true for others too?&lt;/p&gt; &lt;p&gt;‚Ä¢ Do they genuinely speed up startup/MVP development, or do you still spend a lot of time fixing hallucinations and messy code?&lt;/p&gt; &lt;p&gt;As someone with ~15 years in software, I‚Äôm also wondering how experienced devs are adapting:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;‚Ä¢ leaning more into architecture/design? ‚Ä¢ using AI mostly for boilerplate? ‚Ä¢ building faster solo? &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Some pain points I hit often:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;‚Ä¢ confident but wrong code ‚Ä¢ fake APIs ‚Ä¢ good at small tasks, shaky at big systems &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And with local/private AI tools:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;‚Ä¢ search quality can be rough ‚Ä¢ answers don‚Äôt always stick to your actual files ‚Ä¢ weak or missing citations ‚Ä¢ hard to trust memory &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Would love to hear what‚Äôs actually working for you in production ‚Äî and what still feels like hype.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/darshan_aqua"&gt; /u/darshan_aqua &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbi0ij/are_ai_coding_agents_gptcodex_claude_sonnetopus/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbi0ij/are_ai_coding_agents_gptcodex_claude_sonnetopus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbi0ij/are_ai_coding_agents_gptcodex_claude_sonnetopus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T09:58:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbculq</id>
    <title>Lawyer says Google shut down his Gmail, Voice and Photos after NotebookLM upload - Discrepancy Report (or how I learned to love Local LLMs)</title>
    <updated>2026-02-22T04:56:59+00:00</updated>
    <author>
      <name>/u/Thrumpwart</name>
      <uri>https://old.reddit.com/user/Thrumpwart</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbculq/lawyer_says_google_shut_down_his_gmail_voice_and/"&gt; &lt;img alt="Lawyer says Google shut down his Gmail, Voice and Photos after NotebookLM upload - Discrepancy Report (or how I learned to love Local LLMs)" src="https://external-preview.redd.it/6QqGCIHe3v1WQBe6_gTslJhJpyRq4mX4jqVDYTY6xG0.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=504511a0ed53dd20492f3b96504f6d5f1655bc7b" title="Lawyer says Google shut down his Gmail, Voice and Photos after NotebookLM upload - Discrepancy Report (or how I learned to love Local LLMs)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thrumpwart"&gt; /u/Thrumpwart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://discrepancyreport.com/lawyer-says-google-shut-down-his-gmail-voice-and-photos-after-notebooklm-upload/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbculq/lawyer_says_google_shut_down_his_gmail_voice_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbculq/lawyer_says_google_shut_down_his_gmail_voice_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T04:56:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbfh1y</id>
    <title>dyslexia and ADHD in the coding community</title>
    <updated>2026-02-22T07:23:34+00:00</updated>
    <author>
      <name>/u/PruneLanky3551</name>
      <uri>https://old.reddit.com/user/PruneLanky3551</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is my third post on my first Reddit account. Here's why that took so long.&lt;/p&gt; &lt;p&gt;I have dyslexia and ADHD. I've been lurking in communities like this one for years -- reading everything, learning everything -- but never posting. Not because I had nothing to contribute. Because I was scared of what would happen when people saw how I write.&lt;/p&gt; &lt;p&gt;People with dyslexia and ADHD don't write the way the internet expects. The spelling is off. The punctuation is wrong. The sentences don't flow right. And the internet has never been kind about that. We get called stupid. We get told our ideas don't matter because the package they came in looked messy. So we lurk. We learn. We do real work quietly and never share it because the cost of being mocked is too high.&lt;/p&gt; &lt;p&gt;I use AI to help me write. Not to generate ideas -- the ideas are mine. Not to do the work -- I did the work. To help me communicate in a way that doesn't get me dismissed before anyone reads what I actually built.&lt;/p&gt; &lt;p&gt;Yesterday I shipped the first working GGUF quantization of Ouro -- ByteDance's recurrent thinking model. I figured out the tensor mapping, the layer norm mismatch, the early exit gate skip. That was me. And the first thing someone did was question whether I was human.&lt;/p&gt; &lt;p&gt;I'm posting this because I know I'm not the only one. There are people in this community right now with real knowledge, real skills, real contributions -- who won't post because they're afraid of exactly what happened to me today.&lt;/p&gt; &lt;p&gt;You belong here. Your ideas belong here. How you write doesn't determine what you know.&lt;/p&gt; &lt;p&gt;This was my first post. It won't be my last.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PruneLanky3551"&gt; /u/PruneLanky3551 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbfh1y/dyslexia_and_adhd_in_the_coding_community/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbfh1y/dyslexia_and_adhd_in_the_coding_community/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbfh1y/dyslexia_and_adhd_in_the_coding_community/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T07:23:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbafs8</id>
    <title>I Trained a Language Model on CPU for 40 Hours - It Beat the GPU Baseline</title>
    <updated>2026-02-22T02:54:39+00:00</updated>
    <author>
      <name>/u/Own-Albatross868</name>
      <uri>https://old.reddit.com/user/Own-Albatross868</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those who have been following this project, you may recall FlashLM v3, then v4 &amp;quot;Bolt&amp;quot;, and v5.2 &amp;quot;Nova-Ignition&amp;quot;. I am pleased to announce that FlashLM v5 &amp;quot;Thunderbolt&amp;quot; is now complete.&lt;/p&gt; &lt;h1&gt;Results&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Metric&lt;/th&gt; &lt;th align="left"&gt;Value&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Final PPL&lt;/td&gt; &lt;td align="left"&gt;1.36&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Final BPC&lt;/td&gt; &lt;td align="left"&gt;0.44&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Parameters&lt;/td&gt; &lt;td align="left"&gt;29.7M (26.5M ternary)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Training Time&lt;/td&gt; &lt;td align="left"&gt;~40 hours&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Hardware&lt;/td&gt; &lt;td align="left"&gt;AMD Ryzen 7950X3D&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;FlashLM v5 achieves a validation perplexity of 1.36, which beats the TinyStories-1M baseline (PPL 1.59). This represents the first instance of a CPU-trained model beating this baseline.&lt;/p&gt; &lt;h1&gt;Architecture&lt;/h1&gt; &lt;p&gt;FlashLM v5 utilizes ParallelGatedRecurrence, a MatMul-free architecture featuring:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;BitLinear with ternary weights {-1, 0, +1}&lt;/li&gt; &lt;li&gt;Parallel gated recurrence with learned decay gates&lt;/li&gt; &lt;li&gt;No matrix multiplications in the forward pass&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&amp;#8203;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Parameters: 29,750,784 Ternary: 26,542,080 (89%) Float: 3,208,704 (11%) &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Acknowledgments&lt;/h1&gt; &lt;p&gt;I would like to thank arki05 for providing the AMD Ryzen 7950X3D used for training. Without this contribution, the project would not have been possible.&lt;/p&gt; &lt;h1&gt;Generation Comparison&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Version&lt;/th&gt; &lt;th align="left"&gt;PPL&lt;/th&gt; &lt;th align="left"&gt;BPC&lt;/th&gt; &lt;th align="left"&gt;Output Quality&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;v4 &amp;quot;Bolt&amp;quot;&lt;/td&gt; &lt;td align="left"&gt;15.05&lt;/td&gt; &lt;td align="left"&gt;0.88&lt;/td&gt; &lt;td align="left"&gt;Short, repetitive&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;v5.2 &amp;quot;Nova-Ignition&amp;quot;&lt;/td&gt; &lt;td align="left"&gt;10.56&lt;/td&gt; &lt;td align="left"&gt;0.78&lt;/td&gt; &lt;td align="left"&gt;Better coherence&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;v5 &amp;quot;Thunderbolt&amp;quot;&lt;/td&gt; &lt;td align="left"&gt;1.36&lt;/td&gt; &lt;td align="left"&gt;0.44&lt;/td&gt; &lt;td align="left"&gt;Significantly better&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Analysis:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;v5 demonstrates improved cohesive storytelling compared to v4 and v5.2&lt;/li&gt; &lt;li&gt;v5 shows better vocabulary diversity and grammar&lt;/li&gt; &lt;li&gt;BPC improved from 0.88 (v4) to 0.44 (v5), representing a 2x improvement&lt;/li&gt; &lt;li&gt;PPL improved from 15.05 (v4) to 1.36 (v5), representing an 11x improvement&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Samples&lt;/h1&gt; &lt;p&gt;Prompt: &amp;quot;Once upon a time, there was a brave girl named Lucy.&amp;quot;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Once upon a time, there was a brave girl named Lucy. her big tiny looked door, and she wanted. Lucy loved to creative things. She would find toy when, while small laughing, when she thought. She would be friends all day.One day, Lucy found her toy saw a little hole. Lucy was very happy. She wanted to see who was mean. The little hole was not alone anymore. When Lucy was done playing, she saw the little...&lt;/p&gt; &lt;/blockquote&gt; &lt;h1&gt;Links&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Live Demo: &lt;a href="https://huggingface.co/spaces/changcheng967/flashlm-v5-demo"&gt;https://huggingface.co/spaces/changcheng967/flashlm-v5-demo&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Model Card: &lt;a href="https://huggingface.co/changcheng967/flashlm-v5-thunderbolt"&gt;https://huggingface.co/changcheng967/flashlm-v5-thunderbolt&lt;/a&gt;&lt;/li&gt; &lt;li&gt;GitHub: &lt;a href="https://github.com/changcheng967/FlashLM"&gt;https://github.com/changcheng967/FlashLM&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Future Directions&lt;/h1&gt; &lt;p&gt;FlashLM v5 concludes the v5 series. Future work includes:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;FlashLM v6 - Continuing to validate the ParallelGatedRecurrence architecture&lt;/li&gt; &lt;li&gt;Nano-Coder (NC series) - Applying FlashLM techniques to code generation&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Own-Albatross868"&gt; /u/Own-Albatross868 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbafs8/i_trained_a_language_model_on_cpu_for_40_hours_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbafs8/i_trained_a_language_model_on_cpu_for_40_hours_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbafs8/i_trained_a_language_model_on_cpu_for_40_hours_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T02:54:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1rawoe4</id>
    <title>PSA: The software ‚ÄúShade‚Äù is a fraudulent, plagiarized copy of Heretic</title>
    <updated>2026-02-21T17:16:21+00:00</updated>
    <author>
      <name>/u/-p-e-w-</name>
      <uri>https://old.reddit.com/user/-p-e-w-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Three days ago, the following repository was published, which its ‚Äúcreator‚Äù has been aggressively promoting on various channels since then:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/assemsabry/shade"&gt;https://github.com/assemsabry/shade&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The entire source code in the repository is plagiarized from Heretic (&lt;a href="https://github.com/p-e-w/heretic"&gt;https://github.com/p-e-w/heretic&lt;/a&gt;), with only the project name and the copyright notice replaced, claiming ‚Äúoriginal authorship‚Äù of everything. The repository does not acknowledge Heretic as its source, and has erased the commit history and the names of all Heretic contributors.&lt;/p&gt; &lt;p&gt;I and several others have called the repository owner out, but he has deleted all issues and tried to cover up his wrongdoing by adding some bogus ‚Äúadditional features‚Äù using an AI agent. A quick look at the source files, however, reveals that they are still 95% identical to Heretic‚Äôs code. In some cases, only the copyright notice was replaced.&lt;/p&gt; &lt;p&gt;**I can only assume that the ultimate goal is to push malware of some sort, and strongly advise people to stay clear of this plagiarized repository.**&lt;/p&gt; &lt;p&gt;This is one of several incidents where malicious actors tried to profit from Heretic‚Äôs surging popularity during the past days, when it reached #1 on the GitHub trending chart and was posted in various social feeds that cater to scammers.&lt;/p&gt; &lt;p&gt;Please also see &lt;a href="https://github.com/p-e-w/heretic/issues/167"&gt;https://github.com/p-e-w/heretic/issues/167&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I‚Äôm doing everything in my power to keep Heretic clean and available to everyone. Thank you for your encouragement in the past few months, it means the world to me!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-p-e-w-"&gt; /u/-p-e-w- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rawoe4/psa_the_software_shade_is_a_fraudulent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rawoe4/psa_the_software_shade_is_a_fraudulent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rawoe4/psa_the_software_shade_is_a_fraudulent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T17:16:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1raq23i</id>
    <title>they have Karpathy, we are doomed ;)</title>
    <updated>2026-02-21T12:34:51+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1raq23i/they_have_karpathy_we_are_doomed/"&gt; &lt;img alt="they have Karpathy, we are doomed ;)" src="https://preview.redd.it/ergzi9d1eukg1.png?width=140&amp;amp;height=68&amp;amp;auto=webp&amp;amp;s=2005c28094bfd489a487151bba9f5c550c22c55b" title="they have Karpathy, we are doomed ;)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;(added second image for the context)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1raq23i"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1raq23i/they_have_karpathy_we_are_doomed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1raq23i/they_have_karpathy_we_are_doomed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T12:34:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1rb2j5c</id>
    <title>Favourite niche usecases?</title>
    <updated>2026-02-21T21:06:34+00:00</updated>
    <author>
      <name>/u/Figai</name>
      <uri>https://old.reddit.com/user/Figai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rb2j5c/favourite_niche_usecases/"&gt; &lt;img alt="Favourite niche usecases?" src="https://preview.redd.it/o4l2ankhxwkg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c7201facadd4e9d14e1aac7efef2133d85d346f7" title="Favourite niche usecases?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Figai"&gt; /u/Figai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/o4l2ankhxwkg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rb2j5c/favourite_niche_usecases/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rb2j5c/favourite_niche_usecases/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T21:06:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7bsfd</id>
    <title>Best Audio Models - Feb 2026</title>
    <updated>2026-02-17T17:01:37+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;They've been a ton of audio models released of late, the most notable perhaps being Qwen3 TTS. So its time for another &lt;strong&gt;Best Audio Models&lt;/strong&gt; megathread&lt;/p&gt; &lt;p&gt;Share what your favorite ASR, TTS, STT, Text to Music models are right now &lt;strong&gt;and why.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Given the the amount of ambiguity and subjectivity in rating/testing these models, please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks etc. Closed models like Elevenlabs v3 seem to continue to be a few levels above open models especially for production use cases with long lengths/stability requirements, so comparisons, especially empirical ones are welcome.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please use the top level comments to thread your responses.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T17:01:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8snay</id>
    <title>AMA with StepFun AI - Ask Us Anything</title>
    <updated>2026-02-19T07:15:35+00:00</updated>
    <author>
      <name>/u/StepFun_ai</name>
      <uri>https://old.reddit.com/user/StepFun_ai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt; &lt;img alt="AMA with StepFun AI - Ask Us Anything" src="https://external-preview.redd.it/bqLigXK_A-XO-amDfdyeQenKCxR18nhChWpDnQXYmr4.png?width=140&amp;amp;height=75&amp;amp;auto=webp&amp;amp;s=72c79bc533c167d8f77ecf629e3b7af53ee1e6b6" title="AMA with StepFun AI - Ask Us Anything" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/w8274fg1jekg1.png?width=1785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fadbd0ec26a56e60900f9ed667ae808217d70cf2"&gt;https://preview.redd.it/w8274fg1jekg1.png?width=1785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fadbd0ec26a56e60900f9ed667ae808217d70cf2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; !&lt;/p&gt; &lt;p&gt;We are &lt;strong&gt;StepFun&lt;/strong&gt;, the team behind the &lt;strong&gt;Step&lt;/strong&gt; family models, including &lt;a href="https://huggingface.co/collections/stepfun-ai/step-35-flash"&gt;&lt;strong&gt;Step 3.5 Flash&lt;/strong&gt;&lt;/a&gt; and &lt;a href="https://huggingface.co/collections/stepfun-ai/step3-vl-10b"&gt;&lt;strong&gt;Step-3-VL-10B&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;We are super excited to host our first AMA tomorrow in this community. Our participants include CEO, CTO, Chief Scientist, LLM Researchers.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Participants&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Ok_Reach_5122"&gt;u/Ok_Reach_5122&lt;/a&gt; (Co-founder &amp;amp; CEO of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/bobzhuyb"&gt;u/bobzhuyb&lt;/a&gt; (Co-founder &amp;amp; CTO of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/user/Lost-Nectarine1016"&gt;u/Lost-Nectarine1016&lt;/a&gt; (Co-founder &amp;amp; Chief Scientist of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Elegant-Sale-1328"&gt;u/Elegant-Sale-1328&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/SavingsConclusion298"&gt;u/SavingsConclusion298&lt;/a&gt; (Post-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Spirited_Spirit3387"&gt;u/Spirited_Spirit3387&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/These-Nothing-8564/"&gt;u/These-Nothing-8564&lt;/a&gt; (Technical Project Manager)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Either-Beyond-7395"&gt;u/Either-Beyond-7395&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Human_Ad_162"&gt;u/Human_Ad_162&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Icy_Dare_3866"&gt;u/Icy_Dare_3866&lt;/a&gt; (Post-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Big-Employee5595"&gt;u/Big-Employee5595&lt;/a&gt; (Agent Algorithms Lead&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run 8 - 11 AM PST, Feburary 19th. The StepFun team will monitor and answer questions over the 24 hours after the live session.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StepFun_ai"&gt; /u/StepFun_ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T07:15:35+00:00</published>
  </entry>
</feed>
