<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-23T18:27:16+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qk9vmv</id>
    <title>1.8-3.3x faster Embedding finetuning now in Unsloth (~3GB VRAM)</title>
    <updated>2026-01-22T23:09:04+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qk9vmv/1833x_faster_embedding_finetuning_now_in_unsloth/"&gt; &lt;img alt="1.8-3.3x faster Embedding finetuning now in Unsloth (~3GB VRAM)" src="https://preview.redd.it/wwwlbq9ffzeg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=08d147840833d98030a378036e90b68b7a8dd2ff" title="1.8-3.3x faster Embedding finetuning now in Unsloth (~3GB VRAM)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey LocalLLaMA! We added embedding fine-tuning support in Unsloth! &lt;a href="https://github.com/unslothai/unsloth"&gt;Unsloth&lt;/a&gt; trains embedding models &lt;strong&gt;1.8-3.3x faster with 20% less VRAM&lt;/strong&gt;, 2x longer context &amp;amp; no accuracy loss vs. FA2 setups. Most need only 3GB of VRAM for 4bit QLoRA. 6GB for 16bit LoRA.&lt;/p&gt; &lt;p&gt;Full finetuning, LoRA (16bit) and QLoRA (4bit) are all faster by default!&lt;/p&gt; &lt;p&gt;Fine-tuning embedding models can improve retrieval &amp;amp; RAG by aligning vectors to your domain-specific notion of similarity, improving search, clustering, and recommendations on your data.&lt;/p&gt; &lt;p&gt;Blog + Guide: &lt;a href="https://unsloth.ai/docs/new/embedding-finetuning"&gt;https://unsloth.ai/docs/new/embedding-finetuning&lt;/a&gt;&lt;/p&gt; &lt;p&gt;After finetuning, you can deploy your fine-tuned model anywhere: transformers, LangChain, Ollama, vLLM, llama.cpp&lt;/p&gt; &lt;p&gt;We'd like to thank Hugging Face and Unsloth contributor: electroglyph for making this possible!&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Try the &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/EmbeddingGemma_(300M"&gt;EmbeddingGemma notebook&lt;/a&gt;.ipynb) in a free Colab T4 instance&lt;/li&gt; &lt;li&gt;We support ModernBERT, Qwen Embedding, Embedding Gemma, MiniLM-L6-v2, mpnet, BGE and all other models are supported automatically!&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;And code for doing EmbeddingGemma:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from unsloth import FastSentenceTransformer model = FastSentenceTransformer.from_pretrained( model_name = &amp;quot;unsloth/embeddinggemma-300m&amp;quot;, max_seq_length = 1024, # Choose any for long context! full_finetuning = False, # [NEW!] We have full finetuning now! ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Update Unsloth via &lt;code&gt;pip install --upgrade unsloth unsloth_zoo&lt;/code&gt; to get the latest updates. Thanks everyone!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wwwlbq9ffzeg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qk9vmv/1833x_faster_embedding_finetuning_now_in_unsloth/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qk9vmv/1833x_faster_embedding_finetuning_now_in_unsloth/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-22T23:09:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1qk68n8</id>
    <title>vLLM raising $150M confirms it: We have moved from the "Throughput Era" to the "Latency(Cold Starts)."</title>
    <updated>2026-01-22T20:45:42+00:00</updated>
    <author>
      <name>/u/pmv143</name>
      <uri>https://old.reddit.com/user/pmv143</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The news today that the team behind vLLM (Inferact) raised a $150M Seed Round at an $800M valuation is a massive signal for everyone in this space.&lt;/p&gt; &lt;p&gt;For the last two years, all the capital flowed into &lt;strong&gt;Training&lt;/strong&gt; (Foundation Models, massive clusters). This raise signals that the bottleneck has officially shifted to &lt;strong&gt;Serving&lt;/strong&gt; (Efficiency, Latency, Throughput).&lt;/p&gt; &lt;p&gt;It validates a few things we've been seeing in the open-source community:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Software &amp;gt; Hardware:&lt;/strong&gt; buying more H100s isn't enough anymore. You need the software stack (PagedAttention, specialized kernels) to actually utilize them. The &amp;quot;Software Tax&amp;quot; on inference is real.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The &amp;quot;Standardization&amp;quot; Race:&lt;/strong&gt; vLLM is clearly aiming to be the &amp;quot;Linux of Inference&amp;quot;‚Äîthe default engine that runs on NVIDIA, AMD, and Intel. I wonder though, With this kind of war chest, do we think they go for &lt;strong&gt;Horizontal Compatibility&lt;/strong&gt; (making AMD/Intel usable) or &lt;strong&gt;Vertical Optimization&lt;/strong&gt; (squeezing more latency out of CUDA)?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Personally, I think &amp;quot;Throughput&amp;quot; (Batched tokens) is largely solved. The next massive hurdle is &lt;strong&gt;Latency&lt;/strong&gt; (Cold starts and Time-to-First-Token).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmv143"&gt; /u/pmv143 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qk68n8/vllm_raising_150m_confirms_it_we_have_moved_from/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qk68n8/vllm_raising_150m_confirms_it_we_have_moved_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qk68n8/vllm_raising_150m_confirms_it_we_have_moved_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-22T20:45:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1qjtyw8</id>
    <title>Qwen dev on Twitter!!</title>
    <updated>2026-01-22T13:03:26+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjtyw8/qwen_dev_on_twitter/"&gt; &lt;img alt="Qwen dev on Twitter!!" src="https://preview.redd.it/avu4mhyvfweg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=60312577cba6dc65c74da0313ab4d31252bd6be2" title="Qwen dev on Twitter!!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/avu4mhyvfweg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjtyw8/qwen_dev_on_twitter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qjtyw8/qwen_dev_on_twitter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-22T13:03:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1qkjjif</id>
    <title>Qwen3-TTS: Qwen Team Apache'd Their TTS Model</title>
    <updated>2026-01-23T06:34:18+00:00</updated>
    <author>
      <name>/u/Lopsided_Dot_4557</name>
      <uri>https://old.reddit.com/user/Lopsided_Dot_4557</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;üîπ Design custom voices from natural language descriptions&lt;/p&gt; &lt;p&gt;üîπ Clone any voice from just 3 seconds of audio&lt;/p&gt; &lt;p&gt;üîπ 10 languages supported&lt;/p&gt; &lt;p&gt;üîπ 97ms end-to-end latency for real-time generation&lt;/p&gt; &lt;p&gt;üîπ Instruction-based control over emotion, tone &amp;amp; prosody&lt;/p&gt; &lt;p&gt;üîπ 1.7B params, runs locally with streaming support&lt;/p&gt; &lt;p&gt;HF Model: &lt;a href="https://huggingface.co/Qwen/Qwen3-TTS-12Hz-1.7B-CustomVoice"&gt;https://huggingface.co/Qwen/Qwen3-TTS-12Hz-1.7B-CustomVoice&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Install and Test Demo: &lt;a href="https://youtu.be/gR5dyKaxpEk?si=Kjye6ubN3iwIjhTD"&gt;https://youtu.be/gR5dyKaxpEk?si=Kjye6ubN3iwIjhTD&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lopsided_Dot_4557"&gt; /u/Lopsided_Dot_4557 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkjjif/qwen3tts_qwen_team_apached_their_tts_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkjjif/qwen3tts_qwen_team_apached_their_tts_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qkjjif/qwen3tts_qwen_team_apached_their_tts_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-23T06:34:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1qjul5t</id>
    <title>Qwen have open-sourced the full family of Qwen3-TTS: VoiceDesign, CustomVoice, and Base, 5 models (0.6B &amp; 1.8B), Support for 10 languages</title>
    <updated>2026-01-22T13:31:16+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjul5t/qwen_have_opensourced_the_full_family_of_qwen3tts/"&gt; &lt;img alt="Qwen have open-sourced the full family of Qwen3-TTS: VoiceDesign, CustomVoice, and Base, 5 models (0.6B &amp;amp; 1.8B), Support for 10 languages" src="https://preview.redd.it/wo9tqflvkweg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=75bf194547e68a1bb648f530175a2ec826899fd0" title="Qwen have open-sourced the full family of Qwen3-TTS: VoiceDesign, CustomVoice, and Base, 5 models (0.6B &amp;amp; 1.8B), Support for 10 languages" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Github: &lt;a href="https://github.com/QwenLM/Qwen3-TTS"&gt;https://github.com/QwenLM/Qwen3-TTS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hugging Face: &lt;a href="https://huggingface.co/collections/Qwen/qwen3-tts"&gt;https://huggingface.co/collections/Qwen/qwen3-tts&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog: &lt;a href="https://qwen.ai/blog?id=qwen3tts-0115"&gt;https://qwen.ai/blog?id=qwen3tts-0115&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Paper: &lt;a href="https://github.com/QwenLM/Qwen3-TTS/blob/main/assets/Qwen3_TTS.pdf"&gt;https://github.com/QwenLM/Qwen3-TTS/blob/main/assets/Qwen3_TTS.pdf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hugging Face Demo: &lt;a href="https://huggingface.co/spaces/Qwen/Qwen3-TTS"&gt;https://huggingface.co/spaces/Qwen/Qwen3-TTS&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wo9tqflvkweg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjul5t/qwen_have_opensourced_the_full_family_of_qwen3tts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qjul5t/qwen_have_opensourced_the_full_family_of_qwen3tts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-22T13:31:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1qkxuv1</id>
    <title>Sweep: Open-weights 1.5B model for next-edit autocomplete</title>
    <updated>2026-01-23T17:57:04+00:00</updated>
    <author>
      <name>/u/Kevinlu1248</name>
      <uri>https://old.reddit.com/user/Kevinlu1248</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;, we just open-sourced a 1.5B parameter model that predicts your next code edits. You can grab the weights on &lt;a href="https://huggingface.co/sweepai/sweep-next-edit-1.5b"&gt;Hugging Face&lt;/a&gt; or try it out via our &lt;a href="https://plugins.jetbrains.com/plugin/26860-sweep-ai-autocomp"&gt;JetBrains plugin&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What makes this different from regular autocomplete?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Next-edit prediction uses your &lt;em&gt;recent edits&lt;/em&gt; as context, not just the code around your cursor. So if you're renaming a variable or making repetitive changes, it anticipates what you're doing next. The model is small enough to run locally and actually outperforms models 4x its size on both speed and accuracy.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Some things we learned:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Prompt format matters way more than expected.&lt;/strong&gt; We ran a genetic algorithm over 30+ diff formats and found that simple &lt;code&gt;&amp;lt;original&amp;gt;&lt;/code&gt; / &lt;code&gt;&amp;lt;updated&amp;gt;&lt;/code&gt; blocks beat unified diffs. Turns out verbose formats are just easier for smaller models to grok.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RL fixed what SFT couldn't.&lt;/strong&gt; Training was SFT on ~100k examples from permissively-licensed repos (4 hrs on 8xH100), then 2000 steps of RL with tree-sitter parse checking and size regularization. This cleaned up edge cases like unparseable code and overly verbose outputs.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Benchmarks:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;We tested against Mercury (Inception), Zeta (Zed), and Instinct (Continue) across five benchmarks: next-edit above/below cursor, tab-to-jump, standard FIM, and noisiness. Exact-match accuracy ended up correlating best with real-world usability since code is precise and the solution space is small.&lt;/p&gt; &lt;p&gt;We're releasing the weights so anyone can build fast, privacy-preserving autocomplete for whatever editor they use. If you're working on VSCode, Neovim, or anything else, we'd love to see what you build with it!&lt;/p&gt; &lt;p&gt;Happy to answer questions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kevinlu1248"&gt; /u/Kevinlu1248 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkxuv1/sweep_openweights_15b_model_for_nextedit/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkxuv1/sweep_openweights_15b_model_for_nextedit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qkxuv1/sweep_openweights_15b_model_for_nextedit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-23T17:57:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1qkvytk</id>
    <title>16x V100's worth it?</title>
    <updated>2026-01-23T16:48:18+00:00</updated>
    <author>
      <name>/u/notafakename10</name>
      <uri>https://old.reddit.com/user/notafakename10</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkvytk/16x_v100s_worth_it/"&gt; &lt;img alt="16x V100's worth it?" src="https://b.thumbs.redditmedia.com/YXize4jJdcshM1CPrP71GoILzvat56IpLar078mRHpg.jpg" title="16x V100's worth it?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Found a machine near me:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;CPU: 2*Intel Xeon Platinum 8160 48 Cores 96 Threads &lt;/li&gt; &lt;li&gt;GPU: 16x Tesla V100 32GB HBM2 SXM3 (512GB VRAM in total) &lt;/li&gt; &lt;li&gt;Ram: 128GB DDR4 Server ECC Rams Storage: &lt;/li&gt; &lt;li&gt;960GB NVME SSD&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Obviously not the latest and greatest - but 512gb of VRAM sounds like a lot of fun....&lt;/p&gt; &lt;p&gt;How much will the downsides (no recent support I believe) have too much impact?&lt;/p&gt; &lt;p&gt;~$11k USD &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/c38iqiymo4fg1.jpg?width=720&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=0ef5f9458d5082c478900c4cef413ba8951b2e3c"&gt;https://preview.redd.it/c38iqiymo4fg1.jpg?width=720&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=0ef5f9458d5082c478900c4cef413ba8951b2e3c&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/notafakename10"&gt; /u/notafakename10 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkvytk/16x_v100s_worth_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkvytk/16x_v100s_worth_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qkvytk/16x_v100s_worth_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-23T16:48:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1qkuun4</id>
    <title>Invest in hardware now or wait?</title>
    <updated>2026-01-23T16:07:10+00:00</updated>
    <author>
      <name>/u/d4nger_n00dle</name>
      <uri>https://old.reddit.com/user/d4nger_n00dle</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm currently running models on my desktop pc but I want a dedicated machine with a small footprint. Should I invest in an m4 mac mini now or wait for the m5? Or are there other solutions at a similar price point?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/d4nger_n00dle"&gt; /u/d4nger_n00dle &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkuun4/invest_in_hardware_now_or_wait/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkuun4/invest_in_hardware_now_or_wait/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qkuun4/invest_in_hardware_now_or_wait/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-23T16:07:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1qkxejk</id>
    <title>What is the absoulute best opensource programing model for C++ under 8B parameters?</title>
    <updated>2026-01-23T17:40:26+00:00</updated>
    <author>
      <name>/u/Mychma</name>
      <uri>https://old.reddit.com/user/Mychma</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Its jobs its to program singular funcions nothing else just funcions so about 10 - 250 lines of code max. It needs to run max 2-3 min per task on 16GB windows machine with 680M and need to have GGUF available. Tools calling doenst matter. It matters how many funcion does it know and how to code them right. Czech language support for additional comments. Would be welcome but not nesseary. Can be opensource hooby adaptation. I dont care. It needs to be most accrurate and fast as possible. As of 2026.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mychma"&gt; /u/Mychma &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkxejk/what_is_the_absoulute_best_opensource_programing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkxejk/what_is_the_absoulute_best_opensource_programing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qkxejk/what_is_the_absoulute_best_opensource_programing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-23T17:40:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1qkpu4x</id>
    <title>Have people stopped posting tutorial videos?</title>
    <updated>2026-01-23T12:45:05+00:00</updated>
    <author>
      <name>/u/salary_pending</name>
      <uri>https://old.reddit.com/user/salary_pending</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Every youtube video I come across about any tool is just them reading through a blog post or going through stuff already announced by the official post.&lt;/p&gt; &lt;p&gt;Like for example, I wanted to see if anyone has used function gemma and NO, everyone is simply reading and showing the same apps made by Google and showing the same use cases without actually going through the model and using it.&lt;/p&gt; &lt;p&gt;As if they are just trying to please the algorithm and not the viewers :(&lt;/p&gt; &lt;p&gt;am I the only one facing this issue?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/salary_pending"&gt; /u/salary_pending &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkpu4x/have_people_stopped_posting_tutorial_videos/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkpu4x/have_people_stopped_posting_tutorial_videos/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qkpu4x/have_people_stopped_posting_tutorial_videos/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-23T12:45:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1qkujcq</id>
    <title>Some thoughts on LongCat-Flash-Thinking-2601</title>
    <updated>2026-01-23T15:55:56+00:00</updated>
    <author>
      <name>/u/missprolqui</name>
      <uri>https://old.reddit.com/user/missprolqui</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried the new Parallel Thinking and Iterative Summarization features in the online demo, and it feels like it spins up multiple instances to answer the question, then uses a summarization model to merge everything. How is this actually different from the more &amp;quot;deep divergent thinking&amp;quot; style we already get from GPT?&lt;/p&gt; &lt;p&gt;Right now I'm training my own livestreaming AI, which needs to chain together a vision model, a speech model, and a bunch of other APIs.&lt;/p&gt; &lt;p&gt;I noticed this model supports &amp;quot;environment expansion,&amp;quot; and the docs say it can call over 60 tools, has stronger agent capabilities than Claude, and even handles noisy real-world agent scenarios. If that's all true, switching my base LLM to this might seriously cut down latency across the whole response pipeline.&lt;/p&gt; &lt;p&gt;But the model is too huge, and running it is going to be really expensive. So before I commit, I'd love to know if anyone has actually tested its real performance on complex agent workflows through the API.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/missprolqui"&gt; /u/missprolqui &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkujcq/some_thoughts_on_longcatflashthinking2601/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkujcq/some_thoughts_on_longcatflashthinking2601/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qkujcq/some_thoughts_on_longcatflashthinking2601/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-23T15:55:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1qkph45</id>
    <title>Chrome's Local AI Model in production (Gemini Nano) 41% eligibility, 6x slower and $0 cost</title>
    <updated>2026-01-23T12:27:29+00:00</updated>
    <author>
      <name>/u/mbuckbee</name>
      <uri>https://old.reddit.com/user/mbuckbee</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a hobby site that tests email subject lines for people. Users kept asking for it to make suggestions for them via AI (&amp;quot;make it work with ChatGPT&amp;quot;), but I had one concern: money, money, and money.&lt;/p&gt; &lt;p&gt;The tool is free and gets tons of abuse, so I'd been reading about Chrome's built in AI model (Gemini Nano) and tried implementing it, this is my story.&lt;/p&gt; &lt;h2&gt;The Implementation&lt;/h2&gt; &lt;p&gt;Google ships Chrome with the &lt;em&gt;capability&lt;/em&gt; to run Gemini Nano, but not the model itself.&lt;/p&gt; &lt;p&gt;A few things to know:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Multiple models, no control.&lt;/strong&gt; Which model you get depends on an undocumented benchmark. You don't get to pick.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;~1.5-2GB download.&lt;/strong&gt; Downloads to Chrome's profile directory. Multiple users on one machine each need their own copy.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;On-demand.&lt;/strong&gt; The model downloads the first time any site requests it.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Background download.&lt;/strong&gt; Happens asynchronously, independent of page load.&lt;/p&gt; &lt;p&gt;Think of the requirements like a AAA video game, not a browser feature.&lt;/p&gt; &lt;h2&gt;The Fallback&lt;/h2&gt; &lt;p&gt;For users without Nano, we fall back to Google's Gemma 3N via OpenRouter. It's actually &lt;em&gt;more&lt;/em&gt; capable (6B vs 1.8B parameters, 32K vs 6K context). It also costs nothing right now.&lt;/p&gt; &lt;p&gt;Server-based AI inference is extremely cheap if you're not using frontier models.&lt;/p&gt; &lt;h2&gt;The Numbers (12,524 generations across 836 users)&lt;/h2&gt; &lt;p&gt;&lt;strong&gt;User Funnel:&lt;/strong&gt; 100%, all users&lt;/p&gt; &lt;p&gt;&lt;strong&gt;40.7%&lt;/strong&gt; Gemini Nano eligible (Chrome 138+, Desktop, English)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;~25%&lt;/strong&gt; model already downloaded and ready&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Download Stats:&lt;/strong&gt; - ~25% of eligible users already had the model - 1.9 minute median download time for the ~1.5GB file&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Inference Performance:&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;Median&lt;/th&gt; &lt;th&gt;Generations&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Gemini Nano (on-device)&lt;/td&gt; &lt;td&gt;&lt;strong&gt;7.7s&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;4,774&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Gemma 3N (server API)&lt;/td&gt; &lt;td&gt;&lt;strong&gt;1.3s&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;7,750&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;The on-device model is &lt;strong&gt;6x slower&lt;/strong&gt; than making a network request to a server on another continent.&lt;/p&gt; &lt;p&gt;The performance spread is also much wider for Nano. At p99, Nano hits 52.9 seconds while Gemma is at 2.4 seconds. Worst case for Nano was over 9 minutes. Gemma's worst was 31 seconds.&lt;/p&gt; &lt;h2&gt;What Surprised Us&lt;/h2&gt; &lt;p&gt;&lt;strong&gt;No download prompt.&lt;/strong&gt; The 1.5GB model download is completely invisible. No confirmation, no progress bar. Great for adoption. I have mixed feelings about silently dropping multi-gigabyte files onto users' machines though.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Abandoned downloads aren't a problem.&lt;/strong&gt; Close the tab and the download continues in the background. Close Chrome entirely and it resumes on next launch (within 30 days).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Local inference isn't faster.&lt;/strong&gt; I assumed &amp;quot;no network latency&amp;quot; would win. Nope. The compute power difference between a laptop GPU and a datacenter overwhelms any latency savings.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;We didn't need fallback racing.&lt;/strong&gt; We considered running both simultaneously and using whichever returns first. Turns out it's unnecessary. The eligibility check is instant.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;You can really mess up site performance with it&lt;/strong&gt; We ended up accidentally calling it multiple times on a page due to a bug..and it was real bad for users in the same way loading a massive video file or something on a page might be.&lt;/p&gt; &lt;h2&gt;Why We're Keeping It&lt;/h2&gt; &lt;p&gt;By the numbers, there's no reason to use Gemini Nano in production:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It's slow&lt;/li&gt; &lt;li&gt;~60% of users can't use it&lt;/li&gt; &lt;li&gt;It's not cheaper than API calls (OpenRouter is free for Gemma)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;We're keeping it anyway.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I think it's the future. Other browsers will add their own AI models. We'll get consistent cross-platform APIs. I also like the privacy aspects of local inference. The more we use it, the more we'll see optimizations from OS, browser, and hardware vendors.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Full article with charts and detailed methodology:&lt;/strong&gt; &lt;a href="https://sendcheckit.com/blog/ai-powered-subject-line-alternatives"&gt;https://sendcheckit.com/blog/ai-powered-subject-line-alternatives&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mbuckbee"&gt; /u/mbuckbee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkph45/chromes_local_ai_model_in_production_gemini_nano/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkph45/chromes_local_ai_model_in_production_gemini_nano/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qkph45/chromes_local_ai_model_in_production_gemini_nano/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-23T12:27:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1qkrhec</id>
    <title>The 'Infinite Context' Trap: Why 1M tokens won't solve Agentic Amnesia (and why we need a Memory OS)</title>
    <updated>2026-01-23T13:57:35+00:00</updated>
    <author>
      <name>/u/Sweet121</name>
      <uri>https://old.reddit.com/user/Sweet121</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;tbh i‚Äôve been lurking here for a while, just watching the solid work on quants and local inference. but something that‚Äôs been bugging me is the industry's obsession with massive Context Windows.&lt;/p&gt; &lt;p&gt;AI ‚Äúmemory‚Äù right now is going through the same phase databases went through before indexes and schemas existed. Early systems just dumped everything into logs. Then we realized raw history isn‚Äôt memory, structure is.&lt;/p&gt; &lt;p&gt;Everyone seems to be betting that if we just stuff 1M+ tokens into a prompt, AI 'memory' is solved. Honestly, I think this is a dead end, or at least, incredibly inefficient for those of us running things locally.&lt;/p&gt; &lt;p&gt;Treating Context as Memory is like treating RAM as a Hard Drive. It‚Äôs volatile, expensive, and gets slower the more you fill it up. You can already see this shift happening in products like Claude‚Äôs memory features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Memories are categorized (facts vs preferences)&lt;/li&gt; &lt;li&gt;Some things persist, others decay&lt;/li&gt; &lt;li&gt;Not everything belongs in the active working set&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;That‚Äôs the key insight: memory isn‚Äôt about storing more , it‚Äôs about deciding what stays active, what gets updated, and what fades out.&lt;/p&gt; &lt;p&gt;In my view, good agents need Memory Lifecycle Management:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Consolidate&lt;/strong&gt;: Turn noisy logs/chats into actual structured facts.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Evolve&lt;/strong&gt;: Update or merge memories instead of just accumulating contradictions (e.g., &amp;quot;I like coffee&amp;quot; ‚Üí &amp;quot;I quit caffeine&amp;quot;).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Forget&lt;/strong&gt;: Aggressively prune the noise so retrieval actually stays clean.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Most devs end up rebuilding some version of this logic for every agent, so we tried to pull it out into a reusable layer and built &lt;strong&gt;MemOS (Memory Operating System)&lt;/strong&gt;. It‚Äôs not just another vector DB wrapper. It‚Äôs more of an OS layer that sits between the LLM and your storage:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;The Scheduler&lt;/strong&gt;: Instead of brute-forcing context, it uses 'Next-Scene Prediction' to pre-load only what‚Äôs likely needed.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Lifecycle States&lt;/strong&gt;: Memories move from Generated ‚Üí Activated ‚Üí Merged ‚Üí Archived.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Efficiency&lt;/strong&gt;: In our tests (LoCoMo dataset), this gave us a 26% accuracy boost over standard long-context methods, while cutting token usage by ~90%. (Huge for saving VRAM and inference time on local setups).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We open-sourced the core SDK because we think this belongs in the infra stack, just like a database. If you're tired of agents forgetting who they're talking to or burning tokens on redundant history, definitely poke around the repo.&lt;/p&gt; &lt;p&gt;I‚Äôd love to hear how you guys are thinking about this:&lt;/p&gt; &lt;p&gt;Are you just leaning on long-context models for state? Or are you building custom pipelines to handle 'forgetting' and 'updating' memory?&lt;/p&gt; &lt;p&gt;Repo / Docs:&lt;/p&gt; &lt;p&gt;- &lt;strong&gt;Github&lt;/strong&gt;: &lt;a href="https://github.com/MemTensor/MemOS"&gt;https://github.com/MemTensor/MemOS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- &lt;strong&gt;Docs&lt;/strong&gt;: &lt;a href="https://memos-docs.openmem.net/cn"&gt;https://memos-docs.openmem.net/cn&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(Disclaimer: I‚Äôm one of the creators. We have a cloud version for testing but the core logic is all open for the community to tear apart.)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sweet121"&gt; /u/Sweet121 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkrhec/the_infinite_context_trap_why_1m_tokens_wont/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkrhec/the_infinite_context_trap_why_1m_tokens_wont/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qkrhec/the_infinite_context_trap_why_1m_tokens_wont/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-23T13:57:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1qkghpk</id>
    <title>Quiet Threadripper AI Workstation - 768GB DDR5 and 160GB VRAM (RTX 5090 + 4x R9700)</title>
    <updated>2026-01-23T04:00:22+00:00</updated>
    <author>
      <name>/u/sloptimizer</name>
      <uri>https://old.reddit.com/user/sloptimizer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkghpk/quiet_threadripper_ai_workstation_768gb_ddr5_and/"&gt; &lt;img alt="Quiet Threadripper AI Workstation - 768GB DDR5 and 160GB VRAM (RTX 5090 + 4x R9700)" src="https://b.thumbs.redditmedia.com/OvcCfJivz4D4HOjUCQinn-sUra3cRaaS32dKiyRmuRM.jpg" title="Quiet Threadripper AI Workstation - 768GB DDR5 and 160GB VRAM (RTX 5090 + 4x R9700)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Seeing all the quad R9700 builds inspired me to post mine!&lt;/p&gt; &lt;p&gt;I managed to squeeze in RTX 5090 and four R9700 into a workstation build by fitting some GPUs vertically in the front section. Two power supplies: 1600W for the main system and most of the components, and a smaller 850W power supply for 3 of the Radeons (the power cable is threaded through the system popping out through a small gap left by RTX 5090).&lt;/p&gt; &lt;p&gt;DeepSeek-V3.1-Terminus with context = 37279 tokens: PP = 151.76 tps, TG = 10.85 tps&lt;/p&gt; &lt;p&gt;Some things I discovered running local LLMs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;For water-cooled CPU systems, there is not enough air circulation to cool the RAM! &lt;ul&gt; &lt;li&gt;Adding RAM fans got me a 30% performance boost with DeepSeek&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Turning off remote management on WRX90E-SAGE makes it boot much faster&lt;/li&gt; &lt;li&gt;You can combine Nvidia and AMD cards in llama.cpp by compiling with &lt;code&gt;-DGGML_BACKEND_DL=ON&lt;/code&gt;&lt;/li&gt; &lt;li&gt;No significant performance penalty running RTX 5090 at 400W, but much cooler and quieter &lt;ul&gt; &lt;li&gt;To fix, run: &lt;code&gt;sudo nvidia-smi -pl 400&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;R9700 has crazy auto-overclocking by default, draining power and making a lot of noise for little gain &lt;ul&gt; &lt;li&gt;To fix, run: &lt;code&gt;sudo amd-smi set --perf-level=HIGH&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Despite aggressive auto-overclocking, R9700's default mode is sub-optimal for MoE offloading (perf-level=HIGH fixes that as well)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Component List:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Motherboard - Pro WS WRX90E-SAGE SE&lt;/li&gt; &lt;li&gt;CPU - AMD Ryzen Threadripper PRO 7975WX&lt;/li&gt; &lt;li&gt;RAM - 8x KINGSTON 96GB DDR5 5600MHz CL46&lt;/li&gt; &lt;li&gt;GPU1 - ASUS TUF GeForce RTX 5090&lt;/li&gt; &lt;li&gt;GPU2 - 4x ASRock Creator Radeon AI Pro R9700&lt;/li&gt; &lt;li&gt;NVMe - 4x Samsung 9100 PRO 2TB&lt;/li&gt; &lt;li&gt;HDD - 2x Seagate Exos 16TB Enterprise&lt;/li&gt; &lt;li&gt;Power1 - Dark Power Pro 13 1600W 80+ Titanium&lt;/li&gt; &lt;li&gt;Power2 - Seasonic FOCUS V3 GX-850, 850W 80+ Gold&lt;/li&gt; &lt;li&gt;Case - Fractal Design Define 7 XL&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sloptimizer"&gt; /u/sloptimizer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qkghpk"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkghpk/quiet_threadripper_ai_workstation_768gb_ddr5_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qkghpk/quiet_threadripper_ai_workstation_768gb_ddr5_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-23T04:00:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1qks7ua</id>
    <title>Scaling PostgreSQL to power 800 million ChatGPT users</title>
    <updated>2026-01-23T14:27:07+00:00</updated>
    <author>
      <name>/u/buntyshah2020</name>
      <uri>https://old.reddit.com/user/buntyshah2020</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Must Read!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/buntyshah2020"&gt; /u/buntyshah2020 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://openai.com/index/scaling-postgresql/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qks7ua/scaling_postgresql_to_power_800_million_chatgpt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qks7ua/scaling_postgresql_to_power_800_million_chatgpt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-23T14:27:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1qk8zj1</id>
    <title>Am I the only one who feels that, with all the AI boom, everyone is basically doing the same thing?</title>
    <updated>2026-01-22T22:31:28+00:00</updated>
    <author>
      <name>/u/Empty_Enthusiasm_167</name>
      <uri>https://old.reddit.com/user/Empty_Enthusiasm_167</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Lately I go on Reddit and I keep seeing the same idea repeated over and over again. Another chat app, another assistant, another ‚ÄúAI tool‚Äù that, in reality, already exists ‚Äî or worse, already exists in a better and more polished form.&lt;/p&gt; &lt;p&gt;Many of these are applications that could be solved perfectly with an extension, a plugin, or a simple feature inside an app we already use. I‚Äôm not saying AI is bad ‚Äî quite the opposite, it‚Äôs incredible. But there are people pouring all their money into Anthropic subscriptions or increasing their electricity bill just to build a less polished version of things like OpenWebUI, Open Code, Cline, etc&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Empty_Enthusiasm_167"&gt; /u/Empty_Enthusiasm_167 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qk8zj1/am_i_the_only_one_who_feels_that_with_all_the_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qk8zj1/am_i_the_only_one_who_feels_that_with_all_the_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qk8zj1/am_i_the_only_one_who_feels_that_with_all_the_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-22T22:31:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1qkj9zh</id>
    <title>GLM4.7-Flash REAP @ 25% live on HF + agentic coding evals</title>
    <updated>2026-01-23T06:19:37+00:00</updated>
    <author>
      <name>/u/ilzrvch</name>
      <uri>https://old.reddit.com/user/ilzrvch</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkj9zh/glm47flash_reap_25_live_on_hf_agentic_coding_evals/"&gt; &lt;img alt="GLM4.7-Flash REAP @ 25% live on HF + agentic coding evals" src="https://b.thumbs.redditmedia.com/sU6QdQb5WuTlrLDYXYowgPhhH5wsTdVh6s0BAol-lqA.jpg" title="GLM4.7-Flash REAP @ 25% live on HF + agentic coding evals" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone!&lt;/p&gt; &lt;p&gt;We're releasing a 25% REAP'd version of GLM4.7-Flash: &lt;a href="http://hf.co/cerebras/GLM-4.7-Flash-REAP-23B-A3B"&gt;hf.co/cerebras/GLM-4.7-Flash-REAP-23B-A3B&lt;/a&gt;&lt;br /&gt; and MiniMax-M2.1 is in the works!&lt;/p&gt; &lt;p&gt;We've gotten a lot of feedback that REAP pruning affects creative writing / multi-lingual capabilities of the model - this is expected for our REAPs with calibration set curated for agentic coding.&lt;/p&gt; &lt;p&gt;We wanted to see how our REAPs are doing vs. other models of comparable size. We ran the mini-swe-agent flow on SWE-rebench leaderboard for October 2025 and found (see attached image) that GLM4.7 REAPs are a big jump over GLM4.6's and are in the Pareto frontier of agentic coding vs. model size efficiency. MiniMax-M2.1 is in between GLM4.7 REAPs @ 25% and 40%, so we think REAPs MiniMax-M2.1 will shine!&lt;/p&gt; &lt;p&gt;Additionally, based on your feedback, we're considering to drop experimental REAPs for creative writing. Do let us know which datasets and evals we should explore for this. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/pw1zn8zsk1fg1.png?width=2700&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=57bacd1248548a329fca9aecaa81b4cc1a8c3c44"&gt;https://preview.redd.it/pw1zn8zsk1fg1.png?width=2700&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=57bacd1248548a329fca9aecaa81b4cc1a8c3c44&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ilzrvch"&gt; /u/ilzrvch &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkj9zh/glm47flash_reap_25_live_on_hf_agentic_coding_evals/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkj9zh/glm47flash_reap_25_live_on_hf_agentic_coding_evals/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qkj9zh/glm47flash_reap_25_live_on_hf_agentic_coding_evals/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-23T06:19:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1qkqvkr</id>
    <title>Yesterday I used GLM 4.7 flash with my tools and I was impressed..</title>
    <updated>2026-01-23T13:31:40+00:00</updated>
    <author>
      <name>/u/Loskas2025</name>
      <uri>https://old.reddit.com/user/Loskas2025</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkqvkr/yesterday_i_used_glm_47_flash_with_my_tools_and_i/"&gt; &lt;img alt="Yesterday I used GLM 4.7 flash with my tools and I was impressed.." src="https://a.thumbs.redditmedia.com/Xv2-bH8II-ZTJilk2ecwTqL8LNGuAyeIy3Hmt2lLJ64.jpg" title="Yesterday I used GLM 4.7 flash with my tools and I was impressed.." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/g4185s4ep3fg1.png?width=836&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8c7168fc67948fb9917a2c963cb5ad9a1f1c4f6a"&gt;https://preview.redd.it/g4185s4ep3fg1.png?width=836&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8c7168fc67948fb9917a2c963cb5ad9a1f1c4f6a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;...Today I look at this benchmark and understand the results I achieved.&lt;/p&gt; &lt;p&gt;I needed to update a five-year-old document, replacing the old policies with the new ones. Web search, page fetching, and access to the local RAG were fast and seamless. Really impressed.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Loskas2025"&gt; /u/Loskas2025 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkqvkr/yesterday_i_used_glm_47_flash_with_my_tools_and_i/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkqvkr/yesterday_i_used_glm_47_flash_with_my_tools_and_i/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qkqvkr/yesterday_i_used_glm_47_flash_with_my_tools_and_i/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-23T13:31:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1qkimzg</id>
    <title>Nvidia Introduces PersonaPlex: An Open-Source, Real-Time Conversational AI Voice</title>
    <updated>2026-01-23T05:46:00+00:00</updated>
    <author>
      <name>/u/44th--Hokage</name>
      <uri>https://old.reddit.com/user/44th--Hokage</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkimzg/nvidia_introduces_personaplex_an_opensource/"&gt; &lt;img alt="Nvidia Introduces PersonaPlex: An Open-Source, Real-Time Conversational AI Voice" src="https://external-preview.redd.it/MTBpcnh0Y3RlMWZnMZIsTFZLbt9sVhZK1iJgvS1KPC08YlewNjml1NOE_YRL.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b6e21790c025d36b7ace41a33c90e43761f4b274" title="Nvidia Introduces PersonaPlex: An Open-Source, Real-Time Conversational AI Voice" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;PersonaPlex is a real-time, full-duplex speech-to-speech conversational model that enables persona control through text-based role prompts and audio-based voice conditioning. Trained on a combination of synthetic and real conversations, it produces natural, low-latency spoken interactions with a consistent persona. &lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;Link to the Project Page with Demos: &lt;a href="https://research.nvidia.com/labs/adlr/personaplex/"&gt;https://research.nvidia.com/labs/adlr/personaplex/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;####Link to the Open-Sourced Code: &lt;a href="https://github.com/NVIDIA/personaplex"&gt;https://github.com/NVIDIA/personaplex&lt;/a&gt;&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;####Link To Try Out PersonaPlex: &lt;a href="https://colab.research.google.com/#fileId=https://huggingface.co/nvidia/personaplex-7b-v1.ipynb"&gt;https://colab.research.google.com/#fileId=https://huggingface.co/nvidia/personaplex-7b-v1.ipynb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;####Link to the HuggingFace: &lt;a href="https://huggingface.co/nvidia/personaplex-7b-v1"&gt;https://huggingface.co/nvidia/personaplex-7b-v1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;####Link to the PersonaPlex Preprint: &lt;a href="https://research.nvidia.com/labs/adlr/files/personaplex/personaplex%5C_preprint.pdf"&gt;https://research.nvidia.com/labs/adlr/files/personaplex/personaplex\_preprint.pdf&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/44th--Hokage"&gt; /u/44th--Hokage &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/r8hfqlcte1fg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkimzg/nvidia_introduces_personaplex_an_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qkimzg/nvidia_introduces_personaplex_an_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-23T05:46:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1qkmn4l</id>
    <title>DeepSeek-V3.2 Matches GPT-5 at 10x Lower Cost | Introl Blog</title>
    <updated>2026-01-23T09:45:43+00:00</updated>
    <author>
      <name>/u/EchoOfOppenheimer</name>
      <uri>https://old.reddit.com/user/EchoOfOppenheimer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkmn4l/deepseekv32_matches_gpt5_at_10x_lower_cost_introl/"&gt; &lt;img alt="DeepSeek-V3.2 Matches GPT-5 at 10x Lower Cost | Introl Blog" src="https://external-preview.redd.it/egKcSDfb7_eR6zvCmQcIQCLzo1D62E9bPVJSY-qk8TU.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5e934f165af819bde727b9fabcde448d350ae932" title="DeepSeek-V3.2 Matches GPT-5 at 10x Lower Cost | Introl Blog" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;DeepSeek has released V3.2, an open-source model that reportedly matches GPT-5 on math reasoning while costing 10x less to run ($0.028/million tokens). By using a new 'Sparse Attention' architecture, the Chinese lab has achieved frontier-class performance for a total training cost of just ~$5.5 million‚Äîcompared to the $100M+ spent by US tech giants.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EchoOfOppenheimer"&gt; /u/EchoOfOppenheimer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://introl.com/blog/deepseek-v3-2-open-source-ai-cost-advantage"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkmn4l/deepseekv32_matches_gpt5_at_10x_lower_cost_introl/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qkmn4l/deepseekv32_matches_gpt5_at_10x_lower_cost_introl/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-23T09:45:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1qkqjer</id>
    <title>A full AI powered cooking game, where literally any ingredient is possible with infinite combinations.</title>
    <updated>2026-01-23T13:16:42+00:00</updated>
    <author>
      <name>/u/VirtualJamesHarrison</name>
      <uri>https://old.reddit.com/user/VirtualJamesHarrison</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkqjer/a_full_ai_powered_cooking_game_where_literally/"&gt; &lt;img alt="A full AI powered cooking game, where literally any ingredient is possible with infinite combinations." src="https://external-preview.redd.it/YTNmcmg4Z3ltM2ZnMUJwJOA_Kqm7OwiZxEbYxXgv1YYIXAs9kE9ZTKKEhyEN.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b8a4a5616e09101d8d4a75226f626871f4272100" title="A full AI powered cooking game, where literally any ingredient is possible with infinite combinations." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Built with Claude Code&lt;br /&gt; Game Logic - Gemini&lt;br /&gt; Sprites - Flux&lt;/p&gt; &lt;p&gt;Try it out at: &lt;a href="https://infinite-kitchen.com/kitchen"&gt;https://infinite-kitchen.com/kitchen&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VirtualJamesHarrison"&gt; /u/VirtualJamesHarrison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/a2wy0mdym3fg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkqjer/a_full_ai_powered_cooking_game_where_literally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qkqjer/a_full_ai_powered_cooking_game_where_literally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-23T13:16:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1qkiylw</id>
    <title>OpenAI CFO hinting at "Outcome-Based Pricing" (aka royalties on your work)? Makes the case for local even stronger.</title>
    <updated>2026-01-23T06:02:31+00:00</updated>
    <author>
      <name>/u/distalx</name>
      <uri>https://old.reddit.com/user/distalx</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;UPDATE&lt;/strong&gt;: My bad on this one, guys. I got caught by the clickbait.&lt;/p&gt; &lt;p&gt;Thanks to &lt;a href="/u/evilbarron2"&gt;u/evilbarron2&lt;/a&gt; for digging up the original Business Insider source.&lt;/p&gt; &lt;p&gt;CFO was actually talking about &lt;strong&gt;&amp;quot;Outcome-Based Pricing&amp;quot;&lt;/strong&gt; for huge enterprise deals (e.g., if AI helps a Pharma company cure a disease, OpenAI wants a cut of that specific win).&lt;/p&gt; &lt;p&gt;There is basically zero evidence this applies to us regular users, indie devs, or the API. I'm keeping the post up because the concept is still interesting to debate, but definitely take the headline with a huge grain of salt.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;&lt;strong&gt;Original Post:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Saw some screenshots floating around about OpenAI planning to &amp;quot;take a cut&amp;quot; of customer discoveries (like pharma drugs, etc).&lt;/p&gt; &lt;p&gt;I tried to dig up the primary source to see if it‚Äôs just clickbait. The closest official thing is a recent blog post from their CFO Sarah Friar talking about &amp;quot;outcome-based pricing&amp;quot; and &amp;quot;sharing in the value created&amp;quot; for high-value industries.&lt;/p&gt; &lt;p&gt;&lt;del&gt;Even if the &amp;quot;royalty&amp;quot; headlines are sensationalized by tech media, the direction is pretty clear. They are signaling a shift from &amp;quot;paying for electricity&amp;quot; (tokens) to &amp;quot;taxing the factory output&amp;quot; (value).&lt;/del&gt;&lt;/p&gt; &lt;p&gt;It kind of reminds me of the whole Grid vs. Solar debate. relying on the Grid (Cloud APIs) is cheap and powerful, but you don't control the terms. If they decide your specific use case is &amp;quot;high value&amp;quot; and want a percentage, you're locked in.&lt;/p&gt; &lt;p&gt;Building a local stack is like installing solar/batteries. Expensive upfront, pain in the ass to maintain, but at least nobody knocks on your door asking for 5% of your project revenue just because you used their weights to run the math.&lt;/p&gt; &lt;p&gt;Link to article: &lt;a href="https://www.gizmochina.com/2026/01/21/openai-wants-a-cut-of-your-profits-inside-its-new-royalty-based-plan-and-other-business-models/"&gt;https://www.gizmochina.com/2026/01/21/openai-wants-a-cut-of-your-profits-inside-its-new-royalty-based-plan-and-other-business-models/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Link to the actual source: &lt;a href="https://www.businessinsider.com/openai-cfo-sarah-friar-future-revenue-sources-2026-1"&gt;https://www.businessinsider.com/openai-cfo-sarah-friar-future-revenue-sources-2026-1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/distalx"&gt; /u/distalx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkiylw/openai_cfo_hinting_at_outcomebased_pricing_aka/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkiylw/openai_cfo_hinting_at_outcomebased_pricing_aka/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qkiylw/openai_cfo_hinting_at_outcomebased_pricing_aka/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-23T06:02:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1qkm9zb</id>
    <title>Llama.cpp merges in OpenAI Responses API Support</title>
    <updated>2026-01-23T09:22:40+00:00</updated>
    <author>
      <name>/u/SemaMod</name>
      <uri>https://old.reddit.com/user/SemaMod</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkm9zb/llamacpp_merges_in_openai_responses_api_support/"&gt; &lt;img alt="Llama.cpp merges in OpenAI Responses API Support" src="https://external-preview.redd.it/jaCRAxUnJ2FTFmnP1XEivypPWJS55V8E63eMDNFL6mg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=61f06c5517f67a03da7c9a1bf80c4717a8acae9f" title="Llama.cpp merges in OpenAI Responses API Support" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Finally! Took some fussing around to get this to work with unsloth/GLM-4.7-Flash:UD-Q4_K_XL in llama.cpp (ROCm) and Codex CLI, but once set up it works great! I'm super impressed with GLM-4.7-Flash capability in the Codex CLI harness. Haven't tried any big feature implementations yet, but for exploring (large) codebases it has been surprisingly effective&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SemaMod"&gt; /u/SemaMod &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/18486"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkm9zb/llamacpp_merges_in_openai_responses_api_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qkm9zb/llamacpp_merges_in_openai_responses_api_support/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-23T09:22:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1qktwn7</id>
    <title>What's more important for voice agents, bettter models or better constraints?</title>
    <updated>2026-01-23T15:31:59+00:00</updated>
    <author>
      <name>/u/FalseExplanation5385</name>
      <uri>https://old.reddit.com/user/FalseExplanation5385</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There‚Äôs a lot of focus right now on model quality improving, but I keep running into situations where behavior issues aren‚Äôt really about the model at all. &lt;/p&gt; &lt;p&gt;Things like scope control, decision boundaries, and when an agent should or shouldn‚Äôt act seem to matter just as much as raw intelligence. A smarter model doesn‚Äôt always behave better if it‚Äôs not constrained well. Where are the biggest gains practically upgrading models or spending more time designing tighter constraints and flows? Would like to hear what others are doing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FalseExplanation5385"&gt; /u/FalseExplanation5385 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qktwn7/whats_more_important_for_voice_agents_bettter/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qktwn7/whats_more_important_for_voice_agents_bettter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qktwn7/whats_more_important_for_voice_agents_bettter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-23T15:31:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
