<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-17T19:34:27+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1o8hacw</id>
    <title>Since DGX Spark is a disappointment... What is the best value for money hardware today?</title>
    <updated>2025-10-16T20:26:47+00:00</updated>
    <author>
      <name>/u/goto-ca</name>
      <uri>https://old.reddit.com/user/goto-ca</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My current compute box (2×1080 Ti) is failing, so I’ve been renting GPUs by the hour. I’d been waiting for DGX Spark, but early reviews look disappointing for the price/perf.&lt;/p&gt; &lt;p&gt;I’m ready to build a new PC and I’m torn between a &lt;strong&gt;single&lt;/strong&gt; high-end GPU or &lt;strong&gt;dual&lt;/strong&gt; mid/high GPUs. What’s the &lt;strong&gt;best price/performance configuration&lt;/strong&gt; I can build &lt;strong&gt;for ≤ $3,999&lt;/strong&gt; (tower, not a rack server)?&lt;/p&gt; &lt;p&gt;I don't care about RGBs and things like that - it will be kept in the basement and not looked at. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/goto-ca"&gt; /u/goto-ca &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8hacw/since_dgx_spark_is_a_disappointment_what_is_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8hacw/since_dgx_spark_is_a_disappointment_what_is_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8hacw/since_dgx_spark_is_a_disappointment_what_is_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T20:26:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8k4gc</id>
    <title>DGX Spark is here, give me your non-inference workloads</title>
    <updated>2025-10-16T22:20:50+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8k4gc/dgx_spark_is_here_give_me_your_noninference/"&gt; &lt;img alt="DGX Spark is here, give me your non-inference workloads" src="https://preview.redd.it/17hmoj42ujvf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aca471462e384551e5b0d09c075c93f99e741ab1" title="DGX Spark is here, give me your non-inference workloads" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just received my DGX Spark. We all know it's trash for inference, so give me your non-inference test ideas (e.g., RL) to see what else it's trash at. I can also compare the numbers with my 4090 and H100.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/17hmoj42ujvf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8k4gc/dgx_spark_is_here_give_me_your_noninference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8k4gc/dgx_spark_is_here_give_me_your_noninference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T22:20:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8q4xt</id>
    <title>North Dakota using Llama3.2 1B with Ollama to summarize bills</title>
    <updated>2025-10-17T02:57:56+00:00</updated>
    <author>
      <name>/u/SM8085</name>
      <uri>https://old.reddit.com/user/SM8085</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Didn't see this posted here yet.&lt;/p&gt; &lt;p&gt;Apparently North Dakota has been using &lt;a href="https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct"&gt;Llama3.2 1B&lt;/a&gt; with Ollama to summarize their bills and are seeing positive results.&lt;/p&gt; &lt;p&gt;Video: &lt;a href="https://www.youtube.com/watch?v=PYqH1aYhLY0"&gt;North Dakota Legislature innovates with AI - KX News (Youtube)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm surprised they went with Llama3.2 1B, but I think it's interesting they're using a local model.&lt;/p&gt; &lt;p&gt;Somebody in ND had a spare raspberry pi 5 to give the state an AI system?&lt;/p&gt; &lt;p&gt;When I mention summarizing things with small models 4B and under people will ask what kind of accuracy I get and I'm never sure how to quantify it. I get nervous with bots under 2B, but maybe less is more when you're asking them to simply summarize things without injecting what they may or may not know on the subject?&lt;/p&gt; &lt;p&gt;I'll have to check how many bills are over 128k tokens long. I wonder what their plan is at that point? I suppose just do it the old fashioned way.&lt;/p&gt; &lt;p&gt;What does &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; think about this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SM8085"&gt; /u/SM8085 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://markets.financialcontent.com/stocks/article/tokenring-2025-10-15-north-dakota-pioneers-ai-in-government-legislative-council-adopts-meta-ai-to-revolutionize-bill-summarization"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8q4xt/north_dakota_using_llama32_1b_with_ollama_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8q4xt/north_dakota_using_llama32_1b_with_ollama_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T02:57:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8no7i</id>
    <title>Best Open Source TTS That Sounds Most Natural Voice For Storytelling? That You Can Run With 12GB Vram?</title>
    <updated>2025-10-17T01:00:05+00:00</updated>
    <author>
      <name>/u/Head-Investigator540</name>
      <uri>https://old.reddit.com/user/Head-Investigator540</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Last I heard Higgs was great - but have heard it takes 24gb vram (and I only have 12GB on my machine). So wanted to see if anyone had suggested on the best free to use (commercial or otherwise) that I can run from my own machine.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Head-Investigator540"&gt; /u/Head-Investigator540 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8no7i/best_open_source_tts_that_sounds_most_natural/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8no7i/best_open_source_tts_that_sounds_most_natural/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8no7i/best_open_source_tts_that_sounds_most_natural/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T01:00:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1o93ad1</id>
    <title>Audio transcription with llama.cpp multimodal</title>
    <updated>2025-10-17T14:46:22+00:00</updated>
    <author>
      <name>/u/TachyonicBytes</name>
      <uri>https://old.reddit.com/user/TachyonicBytes</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anybody attempted audio transcription with the newish llama.cpp audio support?&lt;/p&gt; &lt;p&gt;I have successfully compiled and run llama and a model, but I can't quite seem to understand how exactly to make the model understand the task:&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;llama-mtmd-cli -m Voxtral-Mini-3B-2507-Q4_K_M.gguf --mmproj mmproj-Voxtral-Mini-3B-2507-Q8_0.gguf --audio test-2.mp3 -p &amp;quot;What it the speaker saying?&amp;quot;&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;I am not sure if the model is too small and doesn't follow instructions, or if it cannot understand the task because of some fundamental issue.&lt;/p&gt; &lt;p&gt;`test-2.mp3` is the test file from the llama.cpp repo.&lt;/p&gt; &lt;p&gt;I know using whisper.cpp is much simpler, and I do that already, but I'd like to build some more complex functionality using a multimodal model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TachyonicBytes"&gt; /u/TachyonicBytes &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o93ad1/audio_transcription_with_llamacpp_multimodal/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o93ad1/audio_transcription_with_llamacpp_multimodal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o93ad1/audio_transcription_with_llamacpp_multimodal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T14:46:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8x4ta</id>
    <title>just added Qwen3-VL support for MNN Chat android</title>
    <updated>2025-10-17T09:55:15+00:00</updated>
    <author>
      <name>/u/Juude89</name>
      <uri>https://old.reddit.com/user/Juude89</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8x4ta/just_added_qwen3vl_support_for_mnn_chat_android/"&gt; &lt;img alt="just added Qwen3-VL support for MNN Chat android" src="https://external-preview.redd.it/zNY5eQnZ-x8ORX22UUI4aGsd0-StGUHm-Z-wi5X4Vb4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9d5a7f8b045bac80ec011d43c579c53bf810f8f7" title="just added Qwen3-VL support for MNN Chat android" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1o8x4ta/video/juu7ycgm9nvf1/player"&gt;https://reddit.com/link/1o8x4ta/video/juu7ycgm9nvf1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also support qwen3-vl-4b and qwen3-vl-8b &lt;/p&gt; &lt;p&gt;Download the 0.7.5version to experience: &lt;a href="https://github.com/alibaba/MNN/blob/master/apps/Android/MnnLlmChat/README.md#version-075"&gt;https://github.com/alibaba/MNN/blob/master/apps/Android/MnnLlmChat/README.md#version-075&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Juude89"&gt; /u/Juude89 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8x4ta/just_added_qwen3vl_support_for_mnn_chat_android/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8x4ta/just_added_qwen3vl_support_for_mnn_chat_android/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8x4ta/just_added_qwen3vl_support_for_mnn_chat_android/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T09:55:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8sbv1</id>
    <title>🚀 HuggingFaceChat Omni: Dynamic policy-baed routing to 115+ LLMs</title>
    <updated>2025-10-17T04:52:56+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8sbv1/huggingfacechat_omni_dynamic_policybaed_routing/"&gt; &lt;img alt="🚀 HuggingFaceChat Omni: Dynamic policy-baed routing to 115+ LLMs" src="https://preview.redd.it/tmc0nl14pjvf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f6edd535c0dbc7303cd47479e890c56a0f66c45b" title="🚀 HuggingFaceChat Omni: Dynamic policy-baed routing to 115+ LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Introducing: &lt;a href="https://github.com/huggingface/chat-ui"&gt;HuggingChat Omni &lt;/a&gt; &lt;/p&gt; &lt;p&gt;Select the best model for every prompt automatically &lt;/p&gt; &lt;p&gt;- Automatic model selection for your queries&lt;br /&gt; - 115 models available across 15 providers &lt;/p&gt; &lt;p&gt;Available now all Hugging Face users. 100% open source.&lt;/p&gt; &lt;p&gt;Omni uses a policy-based approach to model selection (after experimenting with different methods). Credits to &lt;a href="https://huggingface.co/katanemo"&gt;Katanemo&lt;/a&gt; for their small routing model: &lt;a href="https://huggingface.co/katanemo/Arch-Router-1.5B"&gt;katanemo/Arch-Router-1.5B&lt;/a&gt;. The model is natively integrated in &lt;a href="https://github.com/katanemo/archgw"&gt;archgw&lt;/a&gt; for those who want to build their own chat experiences with policy-based dynamic routing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/tmc0nl14pjvf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8sbv1/huggingfacechat_omni_dynamic_policybaed_routing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8sbv1/huggingfacechat_omni_dynamic_policybaed_routing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T04:52:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1o96gtu</id>
    <title>vLLM Performance Benchmark: OpenAI GPT-OSS-20B on RTX Pro 6000 Blackwell (96GB)</title>
    <updated>2025-10-17T16:45:55+00:00</updated>
    <author>
      <name>/u/notaDestroyer</name>
      <uri>https://old.reddit.com/user/notaDestroyer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o96gtu/vllm_performance_benchmark_openai_gptoss20b_on/"&gt; &lt;img alt="vLLM Performance Benchmark: OpenAI GPT-OSS-20B on RTX Pro 6000 Blackwell (96GB)" src="https://external-preview.redd.it/oLekl_ORR7Cm_gsrJon__vT598RBB5Hxp4VkS8gKBSU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6c8926f5bd6382bf4684a66eaf41cb4337b53990" title="vLLM Performance Benchmark: OpenAI GPT-OSS-20B on RTX Pro 6000 Blackwell (96GB)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Hardware:&lt;/strong&gt; NVIDIA RTX Pro 6000 Blackwell Workstation Edition (96GB VRAM)&lt;br /&gt; &lt;strong&gt;Software:&lt;/strong&gt; vLLM 0.11.0 | CUDA 13.0 | Driver 580.82.09 | FP16/BF16&lt;br /&gt; &lt;strong&gt;Model:&lt;/strong&gt; openai/gpt-oss-20b source: &lt;a href="https://huggingface.co/openai/gpt-oss-20b"&gt;https://huggingface.co/openai/gpt-oss-20b&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Ran benchmarks across different output lengths to see how context scaling affects throughput and latency. Here are the key findings:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/seig2q6uapvf1.png?width=6933&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=33dee63986b2fc78c54d8ce2b6d9560b4465acd8"&gt;500 tokens&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7i6wap4wapvf1.png?width=6907&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9680aa1f3ac9d117d7c36811d7d74143c81c0e8d"&gt;1000-2000 tokens&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;500 Token Output Results&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Peak Throughput:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Single user: 2,218 tokens/sec at 64K context&lt;/li&gt; &lt;li&gt;Scales down to 312 tokens/sec at 128K context (20 concurrent users)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Latency:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Excellent TTFT: instant (&amp;lt;250ms) up to 64K context, even at 20 concurrent users&lt;/li&gt; &lt;li&gt;Inter-token latency stays instant across all configurations&lt;/li&gt; &lt;li&gt;Average latency ranges from 2-19 seconds depending on concurrency&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Sweet Spot:&lt;/strong&gt; 1-5 concurrent users with contexts up to 64K maintain 400-1,200+ tokens/sec with minimal latency&lt;/p&gt; &lt;h1&gt;1000-2000 Token Output Results&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Peak Throughput:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Single user: 2,141 tokens/sec at 64K context&lt;/li&gt; &lt;li&gt;Maintains 521 tokens/sec at 128K with 20 users&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Latency Trade-offs:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;TTFT increases to &amp;quot;noticeable delay&amp;quot; territory at higher concurrency (still &amp;lt;6 seconds)&lt;/li&gt; &lt;li&gt;Inter-token latency remains instant throughout&lt;/li&gt; &lt;li&gt;Average latency: 8-57 seconds at high concurrency/long contexts&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Batch Scaling:&lt;/strong&gt; Efficiency improves significantly with concurrency - hits 150%+ at 20 users for longer contexts&lt;/p&gt; &lt;h1&gt;Key Observations&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Memory headroom matters:&lt;/strong&gt; 96GB VRAM handles 128K context comfortably even with 20 concurrent users&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Longer outputs smooth the curve:&lt;/strong&gt; Throughput degradation is less severe with 1500-2000 token outputs vs 500 tokens&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Context scaling penalty:&lt;/strong&gt; ~85% throughput reduction from 1K to 128K context at high concurrency&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Power efficiency:&lt;/strong&gt; Draw stays reasonable (300-440W) across configurations&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Clock stability:&lt;/strong&gt; Minor thermal throttling only at extreme loads (128K + 1 user drops to ~2670 MHz)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The Blackwell architecture shows excellent scaling characteristics for real-world inference workloads. The 96GB VRAM is the real MVP here - no OOM issues even at maximum context length with full concurrency.&lt;/p&gt; &lt;p&gt;Used: &lt;a href="https://github.com/notaDestroyer/vllm-benchmark-suite"&gt;https://github.com/notaDestroyer/vllm-benchmark-suite&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; If you're running a 20B parameter model, this GPU crushes it. Expect 1,000+ tokens/sec for typical workloads (2-5 users, 32K context) and graceful degradation at extreme scales.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/notaDestroyer"&gt; /u/notaDestroyer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o96gtu/vllm_performance_benchmark_openai_gptoss20b_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o96gtu/vllm_performance_benchmark_openai_gptoss20b_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o96gtu/vllm_performance_benchmark_openai_gptoss20b_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T16:45:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8xd0p</id>
    <title>How do you define acceptance criteria when delivering LLM projects for companies?</title>
    <updated>2025-10-17T10:09:10+00:00</updated>
    <author>
      <name>/u/piske_usagi</name>
      <uri>https://old.reddit.com/user/piske_usagi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, I’d like to ask—when you take on large language model (LLM) projects for companies, how do you usually discuss and agree on acceptance criteria?&lt;/p&gt; &lt;p&gt;My initial idea was to collaborate with the client to build an evaluation set (perhaps in the form of multiple-choice questions), and once the model achieves a mutually agreed score, it would be considered successful.&lt;/p&gt; &lt;p&gt;However, I’ve found that most companies that commission these projects have trouble accepting this approach. First, they often struggle to translate their internal knowledge into concrete evaluation steps. Second, they tend to rely more on subjective impressions to judge whether the model performs well or not.&lt;/p&gt; &lt;p&gt;I’m wondering how others handle this situation—any experiences or frameworks you can share? Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/piske_usagi"&gt; /u/piske_usagi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8xd0p/how_do_you_define_acceptance_criteria_when/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8xd0p/how_do_you_define_acceptance_criteria_when/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8xd0p/how_do_you_define_acceptance_criteria_when/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T10:09:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1o9584a</id>
    <title>Is there any wayto change reasoning effort on the fly for GPT-OSS in llama.cpp?</title>
    <updated>2025-10-17T15:58:54+00:00</updated>
    <author>
      <name>/u/kevin_1994</name>
      <uri>https://old.reddit.com/user/kevin_1994</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I run GPT-OSS-120B on my rig. I'm using a command like &lt;code&gt;llama-server ... --chat-template-kwargs '{&amp;quot;reasoning_effort&amp;quot;:&amp;quot;high&amp;quot;}'&lt;/code&gt;&lt;/p&gt; &lt;p&gt;This works, and GPT OSS is much more capable of high reasoning effort.&lt;/p&gt; &lt;p&gt;However, in some situations (coding, summarization, etc) I would like to set the reasoning effort to low.&lt;/p&gt; &lt;p&gt;I understand llama.cpp doesn't implement the entire OpenAI spec but according to &lt;a href="https://platform.openai.com/docs/api-reference/responses/create"&gt;OpenAI completions docs&lt;/a&gt; you're supposed to pass &lt;code&gt;&amp;quot;reasoning&amp;quot;: { &amp;quot;effort&amp;quot;: &amp;quot;high&amp;quot; }&lt;/code&gt; in the request. this doesn't seem to have any effect though.&lt;/p&gt; &lt;p&gt;According to &lt;a href="https://github.com/ggml-org/llama.cpp/tree/master/tools/server"&gt;llama.cpp server docs&lt;/a&gt; you should be able to pass &lt;code&gt;&amp;quot;chat_template_kwargs&amp;quot;: { &amp;quot;reasoning_effort&amp;quot;: &amp;quot;high&amp;quot; }&lt;/code&gt; in the request but this also doesn't seem to work&lt;/p&gt; &lt;p&gt;So my question: has anyone got this working? is this possible?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kevin_1994"&gt; /u/kevin_1994 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9584a/is_there_any_wayto_change_reasoning_effort_on_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9584a/is_there_any_wayto_change_reasoning_effort_on_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o9584a/is_there_any_wayto_change_reasoning_effort_on_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T15:58:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1o98vqd</id>
    <title>LM Studio not reading document correctly. But why?</title>
    <updated>2025-10-17T18:17:10+00:00</updated>
    <author>
      <name>/u/OutboundSF</name>
      <uri>https://old.reddit.com/user/OutboundSF</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o98vqd/lm_studio_not_reading_document_correctly_but_why/"&gt; &lt;img alt="LM Studio not reading document correctly. But why?" src="https://b.thumbs.redditmedia.com/_cIGyKb6m8RskMZWE7rGPSAWsXIMY_oE8BeTPxSJIOQ.jpg" title="LM Studio not reading document correctly. But why?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm a bit new to LM Studio and using it's chat interface to test model responses. But when I uploaded a transcript of a video, I'm getting a wild response. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fkk58sgxqpvf1.png?width=762&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5ea6f717c446978ee8dd27a1f9ea2645ca8a7b00"&gt;Actual Transcript content&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is about a podcaster moving to newsletters. &lt;/p&gt; &lt;p&gt;But when uploading to LM Studio, I get this&lt;br /&gt; Gemma and Command-r&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/e1ptwr26rpvf1.png?width=1282&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=244898eeb03c719ad0e032638367d7412def4896"&gt;https://preview.redd.it/e1ptwr26rpvf1.png?width=1282&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=244898eeb03c719ad0e032638367d7412def4896&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So what am I doing wrong?&lt;br /&gt; By default, when you upload a file into LMStudio, it gives you the RAG option. I've tried it with it enabled and disabled. But no dice. &lt;/p&gt; &lt;p&gt;Can someone help? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OutboundSF"&gt; /u/OutboundSF &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o98vqd/lm_studio_not_reading_document_correctly_but_why/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o98vqd/lm_studio_not_reading_document_correctly_but_why/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o98vqd/lm_studio_not_reading_document_correctly_but_why/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T18:17:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1o94fs0</id>
    <title>Local tool to search documents (RAG only)</title>
    <updated>2025-10-17T15:29:04+00:00</updated>
    <author>
      <name>/u/SuddenWerewolf7041</name>
      <uri>https://old.reddit.com/user/SuddenWerewolf7041</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there a local, open-source tool that can be used to search documents using embedding or RAG, without any LLM needed for the processing. Usually in RAG with LLM, first the document is searched and then the results are given to the LLM and so on. I am looking just for a way to search a document, let's say a PDF (assuming it's not images but just text), and when searching for a term, then it uses embedding models to find related concepts (even if the term doesn't exactly match what's written, i.e. the purpose of embeddings).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SuddenWerewolf7041"&gt; /u/SuddenWerewolf7041 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o94fs0/local_tool_to_search_documents_rag_only/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o94fs0/local_tool_to_search_documents_rag_only/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o94fs0/local_tool_to_search_documents_rag_only/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T15:29:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1o90jvf</id>
    <title>do 2x MCIO to PCIe x16 adapters exist?</title>
    <updated>2025-10-17T12:55:48+00:00</updated>
    <author>
      <name>/u/MelodicRecognition7</name>
      <uri>https://old.reddit.com/user/MelodicRecognition7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o90jvf/do_2x_mcio_to_pcie_x16_adapters_exist/"&gt; &lt;img alt="do 2x MCIO to PCIe x16 adapters exist?" src="https://b.thumbs.redditmedia.com/TJ59uHrnaDqTSwhgsFixmhauzFcf1mPJXYCciNQy3pE.jpg" title="do 2x MCIO to PCIe x16 adapters exist?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want some kind of a &amp;quot;reverse bifurcation&amp;quot;, 2 separate x8 ports combined into one x16. Is it possible to insert a x16 GPU into these two MCIO x8 ports? I've found some cables but not sure if they will work. Where do I put that 4 pin cable on the 2nd pic? Will the adapter on the 3rd pic work if I ditch the left card and plug both cables directly into the motherboard? Any other ways of expanding PCIe x16 slots on Supermicro H13SSL or H14SSL? These motherboards have just 3 full size PCIe slots.&lt;/p&gt; &lt;p&gt;Edit: motherboard manual shows that PCIe1A and PCIe1B are connected to one PCIe x16 port, however there is no information about possibility to recombine two MCIO x8 into one PCIe x16. I can not add more pictures to the thread, here is what the manual shows: &lt;a href="https://files.catbox.moe/p8e499.png"&gt;https://files.catbox.moe/p8e499.png&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MelodicRecognition7"&gt; /u/MelodicRecognition7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1o90jvf"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o90jvf/do_2x_mcio_to_pcie_x16_adapters_exist/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o90jvf/do_2x_mcio_to_pcie_x16_adapters_exist/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T12:55:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1o98m76</id>
    <title>NVIDIA sent me a 5090 so I can demo Qwen3-VL GGUF</title>
    <updated>2025-10-17T18:06:56+00:00</updated>
    <author>
      <name>/u/AlanzhuLy</name>
      <uri>https://old.reddit.com/user/AlanzhuLy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o98m76/nvidia_sent_me_a_5090_so_i_can_demo_qwen3vl_gguf/"&gt; &lt;img alt="NVIDIA sent me a 5090 so I can demo Qwen3-VL GGUF" src="https://b.thumbs.redditmedia.com/pBsmzNoWTR_PcJ6HlkTBSSZmdKgKd285MYT5zK95iGA.jpg" title="NVIDIA sent me a 5090 so I can demo Qwen3-VL GGUF" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;3 days ago. We &lt;a href="https://x.com/Alibaba_Qwen/status/1978154384098754943"&gt;partnered with the Qwen team&lt;/a&gt; so the new Qwen3-VL 4B &amp;amp; 8B models run &lt;strong&gt;day-0&lt;/strong&gt; with GGUF, MLX inside &lt;a href="https://github.com/NexaAI/nexa-sdk"&gt;NexaSDK&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; powered by our NexaML Engine — the first and only framework that supports Qwen3-VL GGUF right now. We just received a 5090 from the NVIDIA team and I want to show you how it runs on a 5090&lt;/p&gt; &lt;p&gt;Today, we also made it run &lt;em&gt;locally&lt;/em&gt; inside our desktop UI app Hyperlink, so everyone can try Qwen3VL on their device easily&lt;/p&gt; &lt;p&gt;I tried the same demo examples from the &lt;a href="https://qwen.ai/blog?id=250aaecfcd4828d55be2b2437a76d66a099860da&amp;amp;from=research.research-list"&gt;Qwen2.5-32B blog&lt;/a&gt;, and the new &lt;strong&gt;Qwen3-VL 4B &amp;amp; 8B&lt;/strong&gt; are &lt;em&gt;insane.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Benchmarks on the 5090 (Q4):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Qwen3VL-8B → &lt;strong&gt;187 tok/s, ~8GB VRAM&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Qwen3VL-4B → &lt;strong&gt;267 tok/s, ~6GB VRAM&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Demo:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1o98m76/video/mvvtazwropvf1/player"&gt;https://reddit.com/link/1o98m76/video/mvvtazwropvf1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How to try:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Install Hyperlink with one click: &lt;a href="https://hyperlink.nexa.ai/"&gt;hyperlink.nexa.ai&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Then go to &lt;em&gt;Discover Models → download Qwen3-VL GGUF to test.&lt;/em&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;How does it do on your setup? Do you see similar performance between Qwen3VL 8B and Qwen2.5-32B?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlanzhuLy"&gt; /u/AlanzhuLy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o98m76/nvidia_sent_me_a_5090_so_i_can_demo_qwen3vl_gguf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o98m76/nvidia_sent_me_a_5090_so_i_can_demo_qwen3vl_gguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o98m76/nvidia_sent_me_a_5090_so_i_can_demo_qwen3vl_gguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T18:06:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1o9agkl</id>
    <title>Local multimodal RAG with Qwen3-VL — text + image retrieval</title>
    <updated>2025-10-17T19:17:24+00:00</updated>
    <author>
      <name>/u/AlanzhuLy</name>
      <uri>https://old.reddit.com/user/AlanzhuLy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Built a small demo showing how to run a full multimodal RAG pipeline locally using &lt;strong&gt;Qwen3-VL-GGUF&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;It loads and chunks your docs, embeds both text and images, retrieves the most relevant pieces for any question, and sends everything to Qwen3-VL for reasoning. The UI is just Gradio&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1o9agkl/video/ni6pd59g1qvf1/player"&gt;https://reddit.com/link/1o9agkl/video/ni6pd59g1qvf1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You can tweak chunk size, Top-K, or even swap in your own inference and embedding model.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/NexaAI/nexa-sdk/tree/main/demos/RAG-Qwen3VL"&gt;See GitHub for code and README instructions&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlanzhuLy"&gt; /u/AlanzhuLy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9agkl/local_multimodal_rag_with_qwen3vl_text_image/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9agkl/local_multimodal_rag_with_qwen3vl_text_image/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o9agkl/local_multimodal_rag_with_qwen3vl_text_image/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T19:17:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8x2w0</id>
    <title>What in the Black Friday hell is happening with the DDR5-5600 128GB SODIMM kits ?</title>
    <updated>2025-10-17T09:51:44+00:00</updated>
    <author>
      <name>/u/HumanDrone8721</name>
      <uri>https://old.reddit.com/user/HumanDrone8721</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In summer Amazon was selling them with something like 320€, not they are almost 500€ and increasing, I wanted to update my 64GB to 128, but this is obscene :(&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HumanDrone8721"&gt; /u/HumanDrone8721 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8x2w0/what_in_the_black_friday_hell_is_happening_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8x2w0/what_in_the_black_friday_hell_is_happening_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8x2w0/what_in_the_black_friday_hell_is_happening_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T09:51:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1o98f57</id>
    <title>New from Cerebras: REAP the Experts: Why Pruning Prevails for One-Shot MoE compression</title>
    <updated>2025-10-17T17:59:47+00:00</updated>
    <author>
      <name>/u/ilzrvch</name>
      <uri>https://old.reddit.com/user/ilzrvch</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o98f57/new_from_cerebras_reap_the_experts_why_pruning/"&gt; &lt;img alt="New from Cerebras: REAP the Experts: Why Pruning Prevails for One-Shot MoE compression" src="https://external-preview.redd.it/WYinqoDP9OerKm8ljzpFUp26G03RA6wo-9izylOPBeM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6b54dc6d0fc61b0246d5aa15915fc1c876cfd68a" title="New from Cerebras: REAP the Experts: Why Pruning Prevails for One-Shot MoE compression" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TLDR: We show that one-shot pruning of experts in large MoEs is better than expert merging when looking at realistic benchmarks, not just perplexity measures. &lt;/p&gt; &lt;p&gt;Using a saliency criterion that measures expected routed contribution of each expert (REAP), we pruned Qwen3-Coder-480B to 363B (25% pruning) and 246B (50% pruning), all in FP8. At 25%, accuracy degradation is minimal across a suite of benchmarks.&lt;/p&gt; &lt;p&gt;Checkpoints on HF:&lt;br /&gt; &lt;a href="https://huggingface.co/cerebras/Qwen3-Coder-REAP-363B-A35B-FP8"&gt;https://huggingface.co/cerebras/Qwen3-Coder-REAP-363B-A35B-FP8&lt;/a&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/cerebras/Qwen3-Coder-REAP-246B-A35B-FP8"&gt;https://huggingface.co/cerebras/Qwen3-Coder-REAP-246B-A35B-FP8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;These can be run with vanilla vLLM, no patches required. &lt;/p&gt; &lt;p&gt;More evals and pruned models on the way!&lt;/p&gt; &lt;p&gt;Link to the paper: &lt;a href="https://arxiv.org/abs/2510.13999"&gt;https://arxiv.org/abs/2510.13999&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6zdkycxjnpvf1.png?width=6884&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ef2e6f9f61b89de730fa9c01d6774998dedee9d8"&gt;https://preview.redd.it/6zdkycxjnpvf1.png?width=6884&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ef2e6f9f61b89de730fa9c01d6774998dedee9d8&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ilzrvch"&gt; /u/ilzrvch &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o98f57/new_from_cerebras_reap_the_experts_why_pruning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o98f57/new_from_cerebras_reap_the_experts_why_pruning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o98f57/new_from_cerebras_reap_the_experts_why_pruning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T17:59:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8m0ti</id>
    <title>We built 3B and 8B models that rival GPT-5 at HTML extraction while costing 40-80x less - fully open source</title>
    <updated>2025-10-16T23:43:03+00:00</updated>
    <author>
      <name>/u/TerrificMist</name>
      <uri>https://old.reddit.com/user/TerrificMist</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8m0ti/we_built_3b_and_8b_models_that_rival_gpt5_at_html/"&gt; &lt;img alt="We built 3B and 8B models that rival GPT-5 at HTML extraction while costing 40-80x less - fully open source" src="https://b.thumbs.redditmedia.com/m8yoOZ6gwR1CBn-AO5mIRDQSzZ5G3sXEIoAl_NdSUMk.jpg" title="We built 3B and 8B models that rival GPT-5 at HTML extraction while costing 40-80x less - fully open source" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;em&gt;Disclaimer: I work for&lt;/em&gt; &lt;a href="http://Inference.net"&gt;&lt;em&gt;Inference.net&lt;/em&gt;&lt;/a&gt;&lt;em&gt;, creator of the Schematron model family&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Hey everyone, wanted to share something we've been working on at Inference.net: Schematron, a family of small models for web extraction.&lt;/p&gt; &lt;p&gt;Our goal was to make a small, fast model for taking HTML from website and extracting JSON that perfectly adheres to a schema.&lt;/p&gt; &lt;p&gt;We distilled a frontier model down to 8B params and managed to keep basically all the output quality for this task. Schematron-8B scores 4.64 on LLM-as-a-judge evals vs GPT-4.1's 4.74 and Gemma 3B's 2.24. Schematron-3B scores 4.41 while being even faster. The main benefit of this model is that it costs 40-80x less than GPT-5 at comparable quality (slightly worse than GPT-5, better than Gemini 2-5 Flash).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Technical details:&lt;/strong&gt; We fine-tuned &lt;a href="https://huggingface.co/meta-llama/Llama-3.1-8B"&gt;Llama-3.1-8B&lt;/a&gt;, expanded it to a 128K context window, quantized to FP8 without quality loss, and trained until it outputted strict JSON with 100% schema compliance. We also built a smaller 3B variant that's even cheaper and faster, but still maintains most of the accuracy of the 8B variant. We recommend using the 3B for most tasks, and trying 8B if it fails or most of your documents are pushing the context limit.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How we trained it:&lt;/strong&gt; We started with 1M real web pages from Common Crawl and built a synthetic dataset by clustering websites and generating schemas that mirror real-world usage patterns. We used a frontier model as a teacher and applied curriculum learning to progressively train on longer context lengths--training with context parallelism and FSDP to scale efficiently--which is why the models stay accurate even at the 128K token limit.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why this matters:&lt;/strong&gt; Processing 1 million pages daily with GPT-5 would cost you around $20,000. With Schematron-8B, that same workload runs about $480. With Schematron-3B, it's $240.&lt;/p&gt; &lt;p&gt;The speed matters too. Schematron processes pages 10x faster than frontier models. On average, Schamatron can scrape a page in 0.54 seconds, compared to 6 seconds for GPT-5. These latency gains compound very quickly for something like a browser-use agent.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Real-world impact on LLM factuality:&lt;/strong&gt; We tested this on SimpleQA to see how much it improves accuracy when paired with web search. When GPT-5 Nano was paired with Schematron-8B to extract structured data from search results provided by Exa, it went from answering barely any questions correctly (8.54% on SimpleQA) to getting over 85% right. The structured extraction approach means this was done processing lean, clean JSON (very little additional cost) instead of dumping ~8k tokens of raw HTML into your context window per page retrieved (typically LLMs are grounded with 5-10 pages/search).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Getting started:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;If you're using our &lt;a href="https://inference.net/models/schematron-3b"&gt;serverless API&lt;/a&gt;, you only need to pass your Pydantic, Zod, or JSON Schema and the HTML. We handle all the prompting in the backend for you in the backend. You get $10 in free credits to start.&lt;/p&gt; &lt;p&gt;If you're running locally, there are a few things to watch out for. You need to follow the prompting guidelines carefully and make sure you're using structured extraction properly, otherwise the model won't perform as well. &lt;/p&gt; &lt;p&gt;The models are on &lt;a href="https://huggingface.co/inference-net/Schematron-3B"&gt;HuggingFace&lt;/a&gt; and &lt;a href="https://ollama.com/Inference/Schematron"&gt;Ollama&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Full benchmarks and code examples are in our blog post: &lt;a href="https://inference.net/blog/schematron"&gt;https://inference.net/blog/schematron&lt;/a&gt;, &lt;a href="https://docs.inference.net/workhorse-models/schematron"&gt;docs&lt;/a&gt;, and &lt;a href="https://github.com/context-labs/inference-samples/blob/main/examples/schematron-scrape-companies/schematron-scrape-companies.ipynb"&gt;samples repo&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Happy to answer any technical questions about the training process or architecture. Also interested in how this would be helpful in your current scraping workflows!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TerrificMist"&gt; /u/TerrificMist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1o8m0ti"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8m0ti/we_built_3b_and_8b_models_that_rival_gpt5_at_html/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8m0ti/we_built_3b_and_8b_models_that_rival_gpt5_at_html/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T23:43:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8m5ua</id>
    <title>Meta just dropped MobileLLM-Pro, a new 1B foundational language model on Huggingface</title>
    <updated>2025-10-16T23:49:16+00:00</updated>
    <author>
      <name>/u/Sad_Consequence5629</name>
      <uri>https://old.reddit.com/user/Sad_Consequence5629</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Meta just published MobileLLM-Pro, a new 1B parameter foundational language model (pre-trained and instruction fine-tuned) on Huggingface&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/facebook/MobileLLM-Pro"&gt;https://huggingface.co/facebook/MobileLLM-Pro&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The model seems to outperform Gemma 3-1B and Llama 3-1B by quite a large margin in pre-training and shows decent performance after instruction-tuning (Looks like it works pretty well for API calling, rewriting, coding and summarization).&lt;br /&gt; The model is already in GradIO and can be directly chatted with in the browser:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/spaces/akhaliq/MobileLLM-Pro"&gt;https://huggingface.co/spaces/akhaliq/MobileLLM-Pro&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(Tweet source: &lt;a href="https://x.com/_akhaliq/status/1978916251456925757"&gt;https://x.com/_akhaliq/status/1978916251456925757&lt;/a&gt; )&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sad_Consequence5629"&gt; /u/Sad_Consequence5629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8m5ua/meta_just_dropped_mobilellmpro_a_new_1b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8m5ua/meta_just_dropped_mobilellmpro_a_new_1b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8m5ua/meta_just_dropped_mobilellmpro_a_new_1b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T23:49:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1o9agyg</id>
    <title>New model from inclusionAI - LLaDA2.0-mini-preview</title>
    <updated>2025-10-17T19:17:48+00:00</updated>
    <author>
      <name>/u/edward-dev</name>
      <uri>https://old.reddit.com/user/edward-dev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9agyg/new_model_from_inclusionai_llada20minipreview/"&gt; &lt;img alt="New model from inclusionAI - LLaDA2.0-mini-preview" src="https://external-preview.redd.it/xv_Z1skcqtXjop4d-0l1Usyn5XM0UbKgNLHO0wCID8I.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f43fd9b3305330d175918ca63404b046858d8a03" title="New model from inclusionAI - LLaDA2.0-mini-preview" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;LLaDA2-mini-preview is a diffusion language model featuring a 16BA1B Mixture-of-Experts (MoE) architecture. As an enhanced, instruction-tuned iteration of the LLaDA series, it is optimized for practical applications.&lt;/p&gt; &lt;p&gt;From the benchmarks the preview looks 'not as good' as ling mini 2.0, but it's still a preview, not the final model, and this is a diffusion language model which makes it interesting &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/edward-dev"&gt; /u/edward-dev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/inclusionAI/LLaDA2.0-mini-preview"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9agyg/new_model_from_inclusionai_llada20minipreview/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o9agyg/new_model_from_inclusionai_llada20minipreview/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T19:17:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1o90ixr</id>
    <title>LlamaBarn — A macOS menu bar app for running local LLMs (open source)</title>
    <updated>2025-10-17T12:54:41+00:00</updated>
    <author>
      <name>/u/erusev_</name>
      <uri>https://old.reddit.com/user/erusev_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o90ixr/llamabarn_a_macos_menu_bar_app_for_running_local/"&gt; &lt;img alt="LlamaBarn — A macOS menu bar app for running local LLMs (open source)" src="https://preview.redd.it/nmcd9kwwvnvf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5c988f1b4b79b949ebee5a449831ea15aab801ef" title="LlamaBarn — A macOS menu bar app for running local LLMs (open source)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;code&gt;r/LocalLLaMA&lt;/code&gt;! We just released this in beta and would love to get your feedback.&lt;/p&gt; &lt;p&gt;Here: &lt;a href="https://github.com/ggml-org/LlamaBarn"&gt;https://github.com/ggml-org/LlamaBarn&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What it does: - Download models from a curated catalog - Run models with one click — it auto-configures them for your system - Built-in web UI and REST API (via &lt;code&gt;llama.cpp&lt;/code&gt; server)&lt;/p&gt; &lt;p&gt;It's a small native app (~12 MB, 100% Swift) that wraps &lt;code&gt;llama.cpp&lt;/code&gt; to make running local models easier.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/erusev_"&gt; /u/erusev_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nmcd9kwwvnvf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o90ixr/llamabarn_a_macos_menu_bar_app_for_running_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o90ixr/llamabarn_a_macos_menu_bar_app_for_running_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T12:54:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1o96o9o</id>
    <title>RTX Pro 6000 Blackwell vLLM Benchmark: 120B Model Performance Analysis</title>
    <updated>2025-10-17T16:53:42+00:00</updated>
    <author>
      <name>/u/notaDestroyer</name>
      <uri>https://old.reddit.com/user/notaDestroyer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o96o9o/rtx_pro_6000_blackwell_vllm_benchmark_120b_model/"&gt; &lt;img alt="RTX Pro 6000 Blackwell vLLM Benchmark: 120B Model Performance Analysis" src="https://external-preview.redd.it/12ojQ9khZuJRm7jqdMaOtnKaFtBC6Yo7dfwq4qKZ3jA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4ae7c659a21f868f6dba51b958c810a90c5bfe24" title="RTX Pro 6000 Blackwell vLLM Benchmark: 120B Model Performance Analysis" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Hardware:&lt;/strong&gt; NVIDIA RTX Pro 6000 Blackwell Workstation Edition (96GB VRAM)&lt;br /&gt; &lt;strong&gt;Software:&lt;/strong&gt; vLLM 0.11.0 | CUDA 13.0 | Driver 580.82.09 | FP16/BF16&lt;br /&gt; &lt;strong&gt;Model:&lt;/strong&gt; openai/gpt-oss-120b source: &lt;a href="https://huggingface.co/openai/gpt-oss-120b"&gt;https://huggingface.co/openai/gpt-oss-120b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Ran two test scenarios with 500-token and 1000-2000-token outputs across varying context lengths (1K-128K) and concurrency levels (1-20 users).&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5io6r8cfcpvf1.png?width=6907&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=dfa2bf6d638fcf36f75be97745f4be59c5f5cade"&gt;500 tokens&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/1i5c8lcgcpvf1.png?width=6907&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f6949af68c70e0b95f2462dab8dc6c3a5be7943a"&gt;1000-2000 tokens&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Key Findings&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Peak Performance (500-token output):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;1051 tok/s&lt;/strong&gt; at 1 user, 1K context&lt;/li&gt; &lt;li&gt;Maintains &lt;strong&gt;300-476 tok/s&lt;/strong&gt; at 20 concurrent users across context lengths&lt;/li&gt; &lt;li&gt;TTFT: 200-400ms at low concurrency, scales to 2000-3000ms at 20 users&lt;/li&gt; &lt;li&gt;Average latency: 2.6s (1 user) → 30.2s (20 users) at 128K context&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Extended Output (1000-2000 tokens):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;1016 tok/s&lt;/strong&gt; peak throughput (minimal degradation vs 500-token)&lt;/li&gt; &lt;li&gt;Slightly higher latencies due to longer decode phases&lt;/li&gt; &lt;li&gt;Power draw: 300-600W depending on load&lt;/li&gt; &lt;li&gt;Batch scaling efficiency: &lt;strong&gt;EXCELLENT&lt;/strong&gt; at 2-5 users, still good up to 10 users&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Observations&lt;/h1&gt; &lt;p&gt;The Blackwell architecture handles this 120B model impressively well:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Linear scaling up to ~5 concurrent users&lt;/li&gt; &lt;li&gt;GPU clocks remain stable at 2800+ MHz under load&lt;/li&gt; &lt;li&gt;Inter-token latency stays in the &amp;quot;INSTANT&amp;quot; zone (&amp;lt;50ms) for most configurations&lt;/li&gt; &lt;li&gt;Context length scaling is predictable—throughput halves roughly every 32K context increase&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The 96GB VRAM headroom means no swapping even at 128K context with max concurrency.&lt;/p&gt; &lt;p&gt;Used: &lt;a href="https://github.com/notaDestroyer/vllm-benchmark-suite"&gt;https://github.com/notaDestroyer/vllm-benchmark-suite&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; If you're running 100B+ models locally, the RTX Pro 6000 Blackwell delivers production-grade throughput with excellent multi-user scaling. Power efficiency is reasonable given the compute density.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/notaDestroyer"&gt; /u/notaDestroyer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o96o9o/rtx_pro_6000_blackwell_vllm_benchmark_120b_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o96o9o/rtx_pro_6000_blackwell_vllm_benchmark_120b_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o96o9o/rtx_pro_6000_blackwell_vllm_benchmark_120b_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T16:53:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8wuyj</id>
    <title>Valve Developer Contributes Major Improvement To RADV Vulkan For Llama.cpp AI</title>
    <updated>2025-10-17T09:37:37+00:00</updated>
    <author>
      <name>/u/FastDecode1</name>
      <uri>https://old.reddit.com/user/FastDecode1</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FastDecode1"&gt; /u/FastDecode1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.phoronix.com/news/RADV-Valve-Boost-Llama.cpp"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8wuyj/valve_developer_contributes_major_improvement_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8wuyj/valve_developer_contributes_major_improvement_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T09:37:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1o96mwq</id>
    <title>Using llamacpp and RCP, managed to improve promt processing by 4x times (160 t/s to 680 t/s) and text generation by 2x times (12.67 t/s to 22.52 t/s) by changing the device order including RPC. GLM 4.6 IQ4_XS multiGPU + RPC.</title>
    <updated>2025-10-17T16:52:21+00:00</updated>
    <author>
      <name>/u/panchovix</name>
      <uri>https://old.reddit.com/user/panchovix</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello guys, hoping you're having a good day.&lt;/p&gt; &lt;p&gt;As you know, llamacpp has RPC since time ago.&lt;/p&gt; &lt;p&gt;I have 2 PCs in my home:&lt;/p&gt; &lt;p&gt;My &amp;quot;Server&amp;quot;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;AM5 MSI X670E Carbon&lt;/li&gt; &lt;li&gt;AMD Ryzen 9 9900X&lt;/li&gt; &lt;li&gt;192GB DDR5 6000Mhz CL32&lt;/li&gt; &lt;li&gt;7 GPUs &lt;ul&gt; &lt;li&gt;5090x2&lt;/li&gt; &lt;li&gt;4090x2&lt;/li&gt; &lt;li&gt;A6000&lt;/li&gt; &lt;li&gt;3090x2&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;MCX314A-BCCT 40Gbps NIC (totally overkill, prob 10Gbps is fine)&lt;/li&gt; &lt;li&gt;OS: Fedora 42&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;And my &amp;quot;Gaming&amp;quot; PC:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;AM5 Gigabyte X670 Aorus Master (I wouldn't recommend this board btw)&lt;/li&gt; &lt;li&gt;AMD Ryzen 7 7800X3D&lt;/li&gt; &lt;li&gt;64GB DDR5 6000Mhz CL30&lt;/li&gt; &lt;li&gt;RTX 5090&lt;/li&gt; &lt;li&gt;MCX314A-BCCT 40Gbps NIC&lt;/li&gt; &lt;li&gt;OS: Windows 11&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;PC1 and PC2 (Server and Gaming) are connected via the MCX314A-BCCT 40Gbps NIC. As info, the max bandwidth used I have seen on llamacpp was about 10-11 Gbps when loading the model (I think here I'm either SSD bound or CPU bound) and about 3-4 Gbps on first prompt processing.&lt;/p&gt; &lt;p&gt;So for the test, I &amp;quot;disabled&amp;quot; one 3090 and replaced it layers with my 5090 via RPC.&lt;/p&gt; &lt;p&gt;I'm running GLM 4.6 IQ4_XS (~180GB) with (very complex, don't blame me):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;LLAMA_SET_ROWS=1 ./llama-server \ -m '/models/GLM-4.6-IQ4_XS.gguf' \ -c 32768 \ --no-mmap \ --rpc 192.168.50.2:50052 \ -ngl 999 \ -ot &amp;quot;blk.(0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15).ffn.=CUDA0&amp;quot; \ -ot &amp;quot;blk.(16|17|18|19|20|21|22|23|24|25).ffn.=CUDA1&amp;quot; \ -ot &amp;quot;blk.(27|28|29|30|31|32|33|34|35|36).ffn.=CUDA2&amp;quot; \ -ot &amp;quot;blk.(38|39|40|41|42|43|44|45|46|47|48|49|50).ffn.=CUDA3&amp;quot; \ -ot &amp;quot;blk.(51|52|53|54|55|56|57|58|59).ffn.=CUDA4&amp;quot; \ -ot &amp;quot;blk.(61|62|63|64|65|66|67|68|69|70).ffn.=RPC0[192.168.50.2:50052]&amp;quot; \ -ot &amp;quot;blk.(72|73|74|75|76|77|78|79|80|81|82|83|84|85|86|87|88|89|90|91).ffn.=CUDA5&amp;quot; \ -ot &amp;quot;blk.26.ffn_(norm|gate_inp|gate_shexp|down_shexp|up_shexp).weight=CUDA1&amp;quot; \ -ot &amp;quot;blk.26.ffn_gate_exps.weight=CUDA1&amp;quot; \ -ot &amp;quot;blk.26.ffn_(down_exps|up_exps).weight=CUDA0&amp;quot; \ -ot &amp;quot;blk.37.ffn_(norm|gate_inp|gate_shexp|down_shexp|up_shexp).weight=CUDA2&amp;quot; \ -ot &amp;quot;blk.37.ffn_gate_exps.weight=CUDA2&amp;quot; \ -ot &amp;quot;blk.37.ffn_(down_exps|up_exps).weight=CUDA3&amp;quot; \ -ot &amp;quot;blk.60.ffn_(norm|gate_inp|gate_shexp|down_shexp|up_shexp).weight=CUDA4&amp;quot; \ -ot &amp;quot;blk.60.ffn_gate_exps.weight=CUDA4&amp;quot; \ -ot &amp;quot;blk.60.ffn_(down_exps|up_exps).weight=CUDA5&amp;quot; \ -ot &amp;quot;blk.71.ffn_(norm|gate_inp|gate_shexp|down_shexp|up_shexp).weight=RPC0[192.168.50.2:50052]&amp;quot; \ -ot &amp;quot;blk.71.ffn_gate_exps.weight=RPC0[192.168.50.2:50052]&amp;quot; \ -ot &amp;quot;blk.71.ffn_(down_exps|up_exps).weight=CUDA5&amp;quot; \ -fa on \ -mg 0 \ -ub 1792 \ &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;By default, llamacpp assigns RPC devices as &lt;strong&gt;the first device,&lt;/strong&gt; this means that the RPC device has the bigger buffers and also has to do more processing that the server itself.&lt;/p&gt; &lt;p&gt;So it is like, by the --devices parameters in this case, use:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;--device RPC0,CUDA0,CUDA1,CUDA2,CUDA3,CUDA4,CUDA5&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;And I was getting these speeds:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;prompt eval time = 27661.35 ms / 4410 tokens ( 6.27 ms per token, 159.43 tokens per second) eval time = 140832.84 ms / 1784 tokens ( 78.94 ms per token, 12.67 tokens per second) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;So, I started a question on github here &lt;a href="https://github.com/ggml-org/llama.cpp/discussions/16625"&gt;https://github.com/ggml-org/llama.cpp/discussions/16625&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And &lt;a href="https://github.com/abc-nix"&gt;abc-nix&lt;/a&gt; did the great suggestion to move it.&lt;/p&gt; &lt;p&gt;So then, used&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;--device CUDA0,CUDA1,CUDA2,CUDA3,CUDA4,RPC0,CUDA5&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;And got&lt;/p&gt; &lt;pre&gt;&lt;code&gt;prompt eval time = 6483.46 ms / 4410 tokens ( 1.47 ms per token, 680.19 tokens per second) eval time = 78029.06 ms / 1757 tokens ( 44.41 ms per token, 22.52 tokens per second) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Which is an absolutely insane performance bump.&lt;/p&gt; &lt;p&gt;Now I want to try to dual boot the &amp;quot;Gaming&amp;quot; PC to Linux to see if there's an improvement. As multiGPU by itself is really bad on Windows, not sure if that also affects RPC.&lt;/p&gt; &lt;p&gt;EDIT: If you wonder how do I connect so much on a consumer CPU:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;X16 split into X8/X4/X4 5.0 from CPU (5090 at X8 5.0, 4090/4090 at X4 4.0)&lt;/li&gt; &lt;li&gt;X4/X4 5.0 from CPU from top 2 M2 slots, to PCIe adapters (RTX 5090 at X4 5.0 and Cx314a NIC X4 3.0)&lt;/li&gt; &lt;li&gt;X4 4.0 from Chipset from bottom PCIe slot (RTX A6000)&lt;/li&gt; &lt;li&gt;X4/X4 4.0 from Chipset from bottom M2 slots, to PCIe adapters (3090/3090)&lt;/li&gt; &lt;li&gt;X1 3.0 from NFF Wifi to PCIe adapter (for now it's open, thinking what can I put there).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;EDIT2: For those wondering, I get no money return for this. I haven't rented and I haven't sold anything related to AI either. So just expenses.&lt;/p&gt; &lt;p&gt;EDIT3: I have confirmed this also works perfectly when offloading to CPU.&lt;/p&gt; &lt;p&gt;I.e. for DeepSeek V3, I ran:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;LLAMA_SET_ROWS=1 ./llama-server -m '/models_llm_2tb/DeepSeek-V3-0324-UD-Q3_K_XL.gguf' -c 32768 --no-mmap -ngl 999 \ --rpc 192.168.50.2:50052 \ -ot &amp;quot;blk.(0|1|2|3|4|5|6|7).ffn.=CUDA0&amp;quot; \ -ot &amp;quot;blk.(8|9|10).ffn.=CUDA1&amp;quot; \ -ot &amp;quot;blk.(11|12|13).ffn.=CUDA2&amp;quot; \ -ot &amp;quot;blk.(14|15|16|17|18).ffn.=CUDA3&amp;quot; \ -ot &amp;quot;blk.(19|20|21).ffn.=CUDA4&amp;quot; \ -ot &amp;quot;blk.(22|23|24).ffn.=RPC0[192.168.50.2:50052]&amp;quot; \ -ot &amp;quot;blk.(25|26|27|28|29|30|31).ffn.=CUDA5&amp;quot; \ -ot &amp;quot;blk.32.ffn_(norm|gate_inp|gate_shexp|down_shexp|up_shexp).weight=CUDA1&amp;quot; \ -ot &amp;quot;blk.32.ffn_gate_exps.weight=CUDA1&amp;quot; \ -ot &amp;quot;blk.32.ffn_down_exps.weight=CUDA1&amp;quot; \ -ot &amp;quot;blk.32.ffn_up_exps.weight=CUDA1&amp;quot; \ -ot &amp;quot;blk.33.ffn_(norm|gate_inp|gate_shexp|down_shexp|up_shexp).weight=CUDA2&amp;quot; \ -ot &amp;quot;blk.33.ffn_gate_exps.weight=CUDA2&amp;quot; \ -ot &amp;quot;blk.33.ffn_down_exps.weight=CUDA2&amp;quot; \ -ot &amp;quot;blk.33.ffn_up_exps.weight=CUDA2&amp;quot; \ -ot &amp;quot;blk.34.ffn_(norm|gate_inp|gate_shexp|down_shexp|up_shexp).weight=CUDA5&amp;quot; \ -ot &amp;quot;blk.34.ffn_gate_exps.weight=CUDA5&amp;quot; \ -ot &amp;quot;blk.34.ffn_down_exps.weight=CUDA5&amp;quot; \ -ot &amp;quot;blk.35.ffn_gate_exps.weight=CUDA3&amp;quot; \ -ot &amp;quot;blk.35.ffn_down_exps.weight=CUDA3&amp;quot; \ -ot &amp;quot;exps=CPU&amp;quot; \ -fa on -mg 0 -ub 2560 -b 2560 --device CUDA0,CUDA1,CUDA2,CUDA3,CUDA4,RPC0,CUDA5 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And got about ~10% less perf than connecting the 5090 directly into the server PC.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/panchovix"&gt; /u/panchovix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o96mwq/using_llamacpp_and_rcp_managed_to_improve_promt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o96mwq/using_llamacpp_and_rcp_managed_to_improve_promt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o96mwq/using_llamacpp_and_rcp_managed_to_improve_promt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T16:52:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8uxh6</id>
    <title>Write three times the word potato</title>
    <updated>2025-10-17T07:32:29+00:00</updated>
    <author>
      <name>/u/TooManyPascals</name>
      <uri>https://old.reddit.com/user/TooManyPascals</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8uxh6/write_three_times_the_word_potato/"&gt; &lt;img alt="Write three times the word potato" src="https://b.thumbs.redditmedia.com/VWK4WzyVVfvV7xJuANrLzK-bH1UfvcQckXM3kS4Llno.jpg" title="Write three times the word potato" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was testing how well Qwen3-0.6B could follow simple instructions... &lt;/p&gt; &lt;p&gt;and it accidentally created a trolling masterpiece.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TooManyPascals"&gt; /u/TooManyPascals &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1o8uxh6"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8uxh6/write_three_times_the_word_potato/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8uxh6/write_three_times_the_word_potato/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T07:32:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
