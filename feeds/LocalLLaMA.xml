<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-18T19:08:13+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1r81vw2</id>
    <title>Every OpenClaw security vulnerability documented in one place — relevant if you're running it with local models</title>
    <updated>2026-02-18T12:38:22+00:00</updated>
    <author>
      <name>/u/LostPrune2143</name>
      <uri>https://old.reddit.com/user/LostPrune2143</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r81vw2/every_openclaw_security_vulnerability_documented/"&gt; &lt;img alt="Every OpenClaw security vulnerability documented in one place — relevant if you're running it with local models" src="https://external-preview.redd.it/bCNdODCJf4MFLRT-GV2CFrzcSYGT_DXu9jf7b-sPOzI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cf5187b921d0158bed6b2198636d0a4c7bd13316" title="Every OpenClaw security vulnerability documented in one place — relevant if you're running it with local models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Full timeline of every OpenClaw security incident — the CVEs, ClawHub malware campaign, exposed instances, Moltbook leak, and government warnings. Covers the safe deployment approach including isolation and hardening. Relevant here since many of you run OpenClaw with local LLMs via LiteLLM or Ollama.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LostPrune2143"&gt; /u/LostPrune2143 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://blog.barrack.ai/openclaw-security-vulnerabilities-2026"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r81vw2/every_openclaw_security_vulnerability_documented/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r81vw2/every_openclaw_security_vulnerability_documented/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-18T12:38:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7shtv</id>
    <title>I built a benchmark that tests coding LLMs on REAL codebases (65 tasks, ELO ranked)</title>
    <updated>2026-02-18T03:50:07+00:00</updated>
    <author>
      <name>/u/hauhau901</name>
      <uri>https://old.reddit.com/user/hauhau901</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7shtv/i_built_a_benchmark_that_tests_coding_llms_on/"&gt; &lt;img alt="I built a benchmark that tests coding LLMs on REAL codebases (65 tasks, ELO ranked)" src="https://external-preview.redd.it/iR5vpFsEP8hSB9xJvEcI5sdQWM2SJY77i75tSw01yJI.png?width=140&amp;amp;height=93&amp;amp;auto=webp&amp;amp;s=737f72db6cb794bccbd21ae823e2f9590aa14236" title="I built a benchmark that tests coding LLMs on REAL codebases (65 tasks, ELO ranked)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, been working on something for a while and figured it's time to share it.&lt;/p&gt; &lt;p&gt;I kept seeing new models drop every week with claims of being 10x better, benchmarks that don't translate to actual coding, and demos that look great but fall apart on real work. so I started building my own benchmark to figure out what &lt;strong&gt;actually&lt;/strong&gt; works.&lt;/p&gt; &lt;p&gt;It's called APEX Testing. every task is an &lt;strong&gt;actual codebase with real code, real dependencies&lt;/strong&gt;, and a real problem to solve. fix this bug, add this feature, refactor this module, build this from scratch. It's (currently) comprising of 65 tasks across 8 categories, ranging from React components to race condition debugging to building CLI tools. Each model gets a fresh clone of the same repo with the exact same starting point and exact same conditions.&lt;/p&gt; &lt;p&gt;Grading is done by multiple SOTA models independently, and then I also personally review every single output to catch anything unfair like timeouts or infra hiccups. If a model got unlucky, I rerun it (which ended up causing a lot bigger of a hole in my wallet haha). The whole thing is ranked with ELO, and you can filter by category to see where models actually shine vs where they struggle.&lt;/p&gt; &lt;p&gt;A couple things that caught me off guard so far:&lt;/p&gt; &lt;p&gt;- GPT 5.1 Codex Mini beating GPT 5.2 Codex pretty convincingly even though smaller and older, it came out way more consistent (but it also seemed to REALLY splurge on tokens)&lt;/p&gt; &lt;p&gt;- Some models look great on average but completely bomb certain task types&lt;/p&gt; &lt;p&gt;- The cost difference between models with similar scores is huge&lt;/p&gt; &lt;p&gt;It's a solo project, funded out of my own pocket (you can see total spend on the homepage lol). hope it helps you cut through the noise and pick the right model for your work.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.apex-testing.org"&gt;https://www.apex-testing.org&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hope you all find it useful!&lt;/p&gt; &lt;p&gt;P.S. I will work on testing more quanted models as well and I might add more tests as well in the future.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ligwgwa9c6kg1.png?width=2095&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ac55a9932069f6100f4375a759fb238e97cdbfc8"&gt;https://preview.redd.it/ligwgwa9c6kg1.png?width=2095&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ac55a9932069f6100f4375a759fb238e97cdbfc8&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hauhau901"&gt; /u/hauhau901 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7shtv/i_built_a_benchmark_that_tests_coding_llms_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7shtv/i_built_a_benchmark_that_tests_coding_llms_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r7shtv/i_built_a_benchmark_that_tests_coding_llms_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-18T03:50:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7s5nh</id>
    <title>We tested the same INT8 model on 5 Snapdragon chipsets. Accuracy ranged from 93% to 71%. Same weights, same ONNX file.</title>
    <updated>2026-02-18T03:34:29+00:00</updated>
    <author>
      <name>/u/NoAdministration6906</name>
      <uri>https://old.reddit.com/user/NoAdministration6906</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We've been doing on-device accuracy testing across multiple Snapdragon SoCs and the results have been eye-opening.&lt;/p&gt; &lt;p&gt;Same model. Same quantization. Same ONNX export. Deployed to 5 different chipsets:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Device&lt;/th&gt; &lt;th align="left"&gt;Accuracy&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Snapdragon 8 Gen 3&lt;/td&gt; &lt;td align="left"&gt;91.8%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Snapdragon 8 Gen 2&lt;/td&gt; &lt;td align="left"&gt;89.1%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Snapdragon 7s Gen 2&lt;/td&gt; &lt;td align="left"&gt;84.3%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Snapdragon 6 Gen 1&lt;/td&gt; &lt;td align="left"&gt;79.6%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Snapdragon 4 Gen 2&lt;/td&gt; &lt;td align="left"&gt;71.2%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Cloud benchmark reported 94.2%.&lt;/p&gt; &lt;p&gt;The spread comes down to three things we've observed:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;NPU precision handling&lt;/strong&gt; — INT8 rounding behavior differs across Hexagon generations. Not all INT8 is created equal.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Operator fusion differences&lt;/strong&gt; — the QNN runtime optimizes the graph differently per SoC, sometimes trading accuracy for throughput.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Memory-constrained fallback&lt;/strong&gt; — on lower-tier chips, certain ops fall back from NPU to CPU, changing the execution path entirely.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;None of this shows up in cloud-based benchmarks. You only see it when you run on real hardware.&lt;/p&gt; &lt;p&gt;Curious if others are seeing similar drift across chipsets — or if anyone has a good strategy for catching this before shipping. Most CI pipelines we've seen only test on cloud GPUs and call it a day.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NoAdministration6906"&gt; /u/NoAdministration6906 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7s5nh/we_tested_the_same_int8_model_on_5_snapdragon/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7s5nh/we_tested_the_same_int8_model_on_5_snapdragon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r7s5nh/we_tested_the_same_int8_model_on_5_snapdragon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-18T03:34:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8apma</id>
    <title>would a "briefing" step beat chunk-based RAG? (feedback on my approach)</title>
    <updated>2026-02-18T18:16:51+00:00</updated>
    <author>
      <name>/u/feursteiner</name>
      <uri>https://old.reddit.com/user/feursteiner</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I love running local agents tbh... privacy + control is hard to beat. sensitive notes stay on my box, workflows feel more predictable, and i’m not yeeting internal context to some 3rd party.&lt;/p&gt; &lt;p&gt;but yeah the annoying part: local models usually need smaller / cleaner context to not fall apart. dumping more text in there can be worse than fewer tokens that are actually organized imo&lt;/p&gt; &lt;p&gt;so i’m building Contextrie, a tiny OSS memory layer that tries to do a chief-of-staff style pass before the model sees anything (ingest &amp;gt; assess &amp;gt; compose). goal is a short brief of only what's useful&lt;/p&gt; &lt;p&gt;If you run local agents: how do you handle context today if any?&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/feuersteiner/contextrie"&gt;https://github.com/feuersteiner/contextrie&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/feursteiner"&gt; /u/feursteiner &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8apma/would_a_briefing_step_beat_chunkbased_rag/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8apma/would_a_briefing_step_beat_chunkbased_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8apma/would_a_briefing_step_beat_chunkbased_rag/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-18T18:16:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1r77swh</id>
    <title>I gave 12 LLMs $2,000 and a food truck. Only 4 survived.</title>
    <updated>2026-02-17T14:42:06+00:00</updated>
    <author>
      <name>/u/Disastrous_Theme5906</name>
      <uri>https://old.reddit.com/user/Disastrous_Theme5906</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r77swh/i_gave_12_llms_2000_and_a_food_truck_only_4/"&gt; &lt;img alt="I gave 12 LLMs $2,000 and a food truck. Only 4 survived." src="https://preview.redd.it/4sewtkexf2kg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c0f7608e083eece043f2953690650ad7c16596a5" title="I gave 12 LLMs $2,000 and a food truck. Only 4 survived." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Built a business sim where AI agents run a food truck for 30 days — location, menu, pricing, staff, inventory. Same scenario for all models.&lt;/p&gt; &lt;p&gt;Opus made $49K. GPT-5.2 $28K. 8 went bankrupt. Every model that took a loan went bankrupt (8/8).&lt;/p&gt; &lt;p&gt;There's also a playable mode — same simulation, same 34 tools, same leaderboard. You either survive 30 days or go bankrupt, get a result card and land on the shared leaderboard. Example result: &lt;a href="https://foodtruckbench.com/r/9E6925"&gt;https://foodtruckbench.com/r/9E6925&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Benchmark + leaderboard: &lt;a href="https://foodtruckbench.com"&gt;https://foodtruckbench.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Play: &lt;a href="https://foodtruckbench.com/play"&gt;https://foodtruckbench.com/play&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Gemini 3 Flash Thinking — only model out of 20+ tested that gets stuck in an infinite decision loop, 100% of runs: &lt;a href="https://foodtruckbench.com/blog/gemini-flash"&gt;https://foodtruckbench.com/blog/gemini-flash&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer questions about the sim or results.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;UPDATE (one day later):&lt;/strong&gt; A player &amp;quot;hoothoot&amp;quot; just hit $101,685 — that's 99.4% of the theoretical maximum. 9 runs on the same seed, ~10 hours total. On a random seed they still scored $91K, so it's not just memorization. Best AI (Opus 4.6) is at ~$50K — still 2x behind a determined human. &lt;/p&gt; &lt;p&gt;Leaderboard is live at &lt;a href="https://foodtruckbench.com/leaderboard"&gt;https://foodtruckbench.com/leaderboard&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Disastrous_Theme5906"&gt; /u/Disastrous_Theme5906 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4sewtkexf2kg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r77swh/i_gave_12_llms_2000_and_a_food_truck_only_4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r77swh/i_gave_12_llms_2000_and_a_food_truck_only_4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T14:42:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7uyh9</id>
    <title>Running your own LLM on a LAN accessible by a dev team</title>
    <updated>2026-02-18T05:55:24+00:00</updated>
    <author>
      <name>/u/BubbleProphylaxis</name>
      <uri>https://old.reddit.com/user/BubbleProphylaxis</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Let's say a team of 20 devs are cursor subscribers and they each consume 20-50$ usd per day in tokens by using a midrange Claude or GPT model. That adds up really quickly.&lt;/p&gt; &lt;p&gt;Is it viable then to buy a large server, with let's say 4x RTX A6000 cards, for a total of 192 gb VRAM, running a pretty big model, and plenty of system ram?&lt;/p&gt; &lt;p&gt;That would make it a pretty expensive server for sure, but certainly cheaper than the sum of all pay-per-use for all users.&lt;/p&gt; &lt;p&gt;What model would you run for a dev team on such a beast of a server?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BubbleProphylaxis"&gt; /u/BubbleProphylaxis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7uyh9/running_your_own_llm_on_a_lan_accessible_by_a_dev/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7uyh9/running_your_own_llm_on_a_lan_accessible_by_a_dev/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r7uyh9/running_your_own_llm_on_a_lan_accessible_by_a_dev/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-18T05:55:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1r86huj</id>
    <title>No love for Intel GPUs?</title>
    <updated>2026-02-18T15:47:09+00:00</updated>
    <author>
      <name>/u/pelicanthief</name>
      <uri>https://old.reddit.com/user/pelicanthief</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;On a per VRAM GB basis, Intel GPUs are way cheaper than a Nvidia ones. But why is there no love them here?&lt;/p&gt; &lt;p&gt;Am I missing something?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pelicanthief"&gt; /u/pelicanthief &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r86huj/no_love_for_intel_gpus/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r86huj/no_love_for_intel_gpus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r86huj/no_love_for_intel_gpus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-18T15:47:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7plp1</id>
    <title>PrimeIntellect/INTELLECT-3.1 · Hugging Face</title>
    <updated>2026-02-18T01:43:01+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7plp1/primeintellectintellect31_hugging_face/"&gt; &lt;img alt="PrimeIntellect/INTELLECT-3.1 · Hugging Face" src="https://external-preview.redd.it/HlIthhd4_MOQ5SPqMHH4aU80ZJQIA0QmPpZBs5Jd5L0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5b86fe7ea9ee70a3f83ccb5ad48aa01e8fd98f27" title="PrimeIntellect/INTELLECT-3.1 · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INTELLECT-3.1 is a 106B (A12B) parameter Mixture-of-Experts reasoning model built as a continued training of INTELLECT-3 with additional reinforcement learning on math, coding, software engineering, and agentic tasks.&lt;/p&gt; &lt;p&gt;Training was performed with prime-rl using environments built with the verifiers library. All training and evaluation environments are available on the Environments Hub.&lt;/p&gt; &lt;p&gt;The model, training frameworks, and environments are open-sourced under fully-permissive licenses (MIT and Apache 2.0).&lt;/p&gt; &lt;p&gt;For more details, see the technical report.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/PrimeIntellect/INTELLECT-3.1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7plp1/primeintellectintellect31_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r7plp1/primeintellectintellect31_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-18T01:43:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7j7kb</id>
    <title>The guy that won the NVIDIA Hackathon and an NVIDIA DGX Spark GB10 has won another hackathon with it!</title>
    <updated>2026-02-17T21:22:30+00:00</updated>
    <author>
      <name>/u/brandon-i</name>
      <uri>https://old.reddit.com/user/brandon-i</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I promised that I would update you all with what I was going to do next with the DGX Spark GB10 that I won. It's been a few weeks and I have been primarily heads down on fundraising for my startup trying to automatically improve and evaluate Coding Agents.&lt;/p&gt; &lt;p&gt;Since the last time I posted I became a Dell Pro Precision Ambassador after they saw all of the cool hackathons that I won and stuff I am building that can hopefully make a difference in the world (I am trying to create Brain World Models using a bunch of different types of brain scans to do precision therapeutics, diagnostics, etc. as my Magnus Opus). &lt;/p&gt; &lt;p&gt;They sent me a Dell Pro Max T2 Tower and another DGX Spark GB10 which I have connected to the previous one that I won. This allows me to continue my work with the limited funds that I have to see how far I can really push the limits of what's possible at the intersection of Healthcare and AI.&lt;/p&gt; &lt;p&gt;During Superbowl Weekend I took some time to do a 24-hour hackathon solving a problem that I really care about (even if it wasn't related to my startup).&lt;/p&gt; &lt;p&gt;My most recent job was at UCSF doing applied neuroscience creating a research-backed tool that screened children for Dyslexia since traditional approaches don’t meet learners where they are so I wanted to take the research I did further and actually create solutions that also did computer adaptive learning.&lt;/p&gt; &lt;p&gt;Through my research I have come to find that the current solutions for learning languages are antiquated often assuming a “standard” learner: same pace, same sequence, same practice, same assessments.&lt;/p&gt; &lt;p&gt;But, language learning is deeply personalized. Two learners can spend the same amount of time on the same content and walk away with totally different outcomes because the feedback they need could be entirely different with the core problem being that language learning isn’t one-size-fits-all.&lt;/p&gt; &lt;p&gt;Most language tools struggle with a few big issues:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Single Language&lt;/strong&gt;: Most tools are designed specifically for Native English speakers&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Culturally insensitive:&lt;/strong&gt; Even within the same language there can be different dialects and word/phrase utilization&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Static Difficulty:&lt;/strong&gt; content doesn’t adapt when you’re bored or overwhelmed&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Delayed Feedback:&lt;/strong&gt; you don’t always know &lt;em&gt;what&lt;/em&gt; you said wrong or &lt;em&gt;why&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Practice ≠ assessment:&lt;/strong&gt; testing is often separate from learning, instead of driving it&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Speaking is underserved&lt;/strong&gt;: it’s hard to get consistent, personalized speaking practice without 1:1 time&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For many learners, especially kids, the result is predictable: &lt;em&gt;frustration, disengagement, or plateauing.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;So I built a an automated speech recognition app that adapts in real time combining computer adaptive testing and computer adaptive learning to personalize the experience as you go.&lt;/p&gt; &lt;p&gt;It not only transcribes speech, but also evaluates phoneme-level pronunciation, which lets the system give targeted feedback (and adapt the next prompt) based on &lt;em&gt;which sounds&lt;/em&gt; someone struggles with.&lt;/p&gt; &lt;p&gt;I tried to make it as simple as possible because my primary user base would be teachers that didn't have a lot of time to actually learn new tools and were already struggling with teaching an entire class.&lt;/p&gt; &lt;p&gt;It uses natural speaking performance to determine what a student should practice next.&lt;/p&gt; &lt;p&gt;So instead of providing every child a fixed curriculum, the system continuously adjusts difficulty and targets based on how you’re actually doing rather than just on completion.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How it Built It&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;I connected two NVIDIA DGX Spark with the GB10 Grace Blackwell Superchip giving me 256 GB LPDDR5x Coherent Unified System Memory to run inference and the entire workflow locally. I also had the Dell Pro Max T2 Tower, but I couldn't physically bring it to the Notion office so I used Tailscale to SSH into it&lt;/li&gt; &lt;li&gt;I utilized CrisperWhisper, faster-whisper, and a custom transformer to get accurate word-level timestamps, verbatim transcriptions, filler detection, and hallucination mitigation&lt;/li&gt; &lt;li&gt;I fed this directly into a Montreal Forced Aligner to get phoneme level dictation&lt;/li&gt; &lt;li&gt;I then used a heuristics detection algorithm to screen for several disfluencies: Prolongnation, replacement, deletion, addition, and repetition&lt;/li&gt; &lt;li&gt;I included stutter and filler analysis/detection using the SEP-28k dataset and PodcastFillers Dataset&lt;/li&gt; &lt;li&gt;I fed these into AI Agents using both local models, Cartesia's Line Agents, and Notion's Custom Agents to do computer adaptive learning and testing&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The result is a workflow where learning content can evolve quickly while the learner experience stays personalized and measurable.&lt;/p&gt; &lt;p&gt;I want to support learners who don’t thrive in rigid systems and need:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;more repetition (without embarrassment)&lt;/li&gt; &lt;li&gt;targeted practice on specific sounds/phrases&lt;/li&gt; &lt;li&gt;a pace that adapts to attention and confidence&lt;/li&gt; &lt;li&gt;immediate feedback that’s actually actionable&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This project is an early prototype, but it’s a direction I’m genuinely excited about: speech-first language learning that adapts to the person, rather than the other way around.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=2RYHu1jyFWI"&gt;https://www.youtube.com/watch?v=2RYHu1jyFWI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I wrote something in medium that has a tiny bit more information &lt;a href="https://medium.com/@brandonin/i-just-won-the-cartesia-hackathon-reinforcing-something-ive-believed-in-for-a-long-time-language-dc93525b2e48?postPublishedType=repub"&gt;https://medium.com/@brandonin/i-just-won-the-cartesia-hackathon-reinforcing-something-ive-believed-in-for-a-long-time-language-dc93525b2e48?postPublishedType=repub&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For those that are wondering what the specs are of the Dell Pro T2 Tower that they sent me:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Intel Core Ultra 9 285K (36 MB cache, 24 cores, 24 threads, 3.2 GHz to 5.7 GHz, 125W)&lt;/li&gt; &lt;li&gt;128GB: 4 x 32 GB, DDR5, 4400 MT/s&lt;/li&gt; &lt;li&gt;2x - 4TB SSD TLC with DRAM M.2 2280 PCIe Gen4 SED Ready&lt;/li&gt; &lt;li&gt;NVIDIA RTX PRO 6000 Blackwell Workstation Edition (600W), 96GB GDDR7&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/brandon-i"&gt; /u/brandon-i &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7j7kb/the_guy_that_won_the_nvidia_hackathon_and_an/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7j7kb/the_guy_that_won_the_nvidia_hackathon_and_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r7j7kb/the_guy_that_won_the_nvidia_hackathon_and_an/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T21:22:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8biu3</id>
    <title>AnythingLLM Desktop works across your entire OS with local models</title>
    <updated>2026-02-18T18:45:52+00:00</updated>
    <author>
      <name>/u/tcarambat</name>
      <uri>https://old.reddit.com/user/tcarambat</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8biu3/anythingllm_desktop_works_across_your_entire_os/"&gt; &lt;img alt="AnythingLLM Desktop works across your entire OS with local models" src="https://external-preview.redd.it/MzI4cTJobGZxYWtnMWcTOysjh4KRAQS1HqUZTuY8uTJ3Gln28lnaxHmPC-Xx.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b13f3d8c0d97541f079273e47f93ffe51c7ab640" title="AnythingLLM Desktop works across your entire OS with local models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;(Tim from AnythingLLM here!) &lt;/p&gt; &lt;p&gt;Today, we released &lt;a href="https://anythingllm.com/desktop"&gt;AnythingLLM Desktop v1.11.0&lt;/a&gt; and it is a step towards our new direction that becomes more of an extension of your OS and less of a sandboxed app.&lt;/p&gt; &lt;p&gt;Now with a simple customized keybind you can open an overlay that instantly has access to your open apps and screen. This works for both multi-modal &lt;strong&gt;but also&lt;/strong&gt; non-vision enabled models.&lt;/p&gt; &lt;p&gt;This functionality is all on top of all the stuff people use AnythingLLM for already: Chatting with documents, RAG, agents, MCPs, and more. This panel also has awareness of any &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1qk1u6h/we_added_an_ondevice_ai_meeting_note_taker_into/"&gt;Meeting transcripts&lt;/a&gt; you might have too!&lt;/p&gt; &lt;p&gt;This is all done using on-device models and pipelines - using a local model you can have a fully on-device experience. In that demo I am using Qwen3-VL 4B Instruct (Q4) on a Macbook M4 Pro but you can really bring in any model or provider you want.&lt;/p&gt; &lt;p&gt;By default, everything AnythingLLM does can be customized but is on-device first with the option to bring your own key to use whatever you like to use for inference (Ollama, LM Studio, OpenAi, etc). We also bench on old (and bad) hardware that env on underpowered devices you can still have some semblance of a great experience.&lt;/p&gt; &lt;p&gt;We are trying to &amp;quot;simplify&amp;quot; our entire experience but still allow power-users like on this sub to get that customization they always require. We also have an &lt;a href="https://github.com/Mintplex-Labs/anything-llm"&gt;OSS MIT license multi-user server based version&lt;/a&gt; of AnythingLLM if you are looking for something more hostable on a VM or something.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tcarambat"&gt; /u/tcarambat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/onupvglfqakg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8biu3/anythingllm_desktop_works_across_your_entire_os/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8biu3/anythingllm_desktop_works_across_your_entire_os/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-18T18:45:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1r877tl</id>
    <title>Vibe Check: Latest models on AMD Strix Halo</title>
    <updated>2026-02-18T16:13:31+00:00</updated>
    <author>
      <name>/u/bhamm-lab</name>
      <uri>https://old.reddit.com/user/bhamm-lab</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been testing a bunch of recent drops on my AMD homelab (Ryzen AI Max+ 395 + R9700) with a very non-scientific “vibe check” workflow (Roo Code + Open WebUI).&lt;/p&gt; &lt;p&gt;A few standouts that replaced my old stack:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Kimi Linear 48B Instruct&lt;/strong&gt; as a daily-driver generalist.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Qwen3 Coder Next&lt;/strong&gt; as my new coding model.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Q2_K_XL&lt;/strong&gt; on huge models is… surprisingly not trash? (Still too slow for HITL, but decent for background tasks like summarization or research).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Full write-up and latency numbers here: &lt;a href="https://site.bhamm-lab.com/blogs/upgrade-models-feb26/"&gt;https://site.bhamm-lab.com/blogs/upgrade-models-feb26/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Curious what other people are running with limited hardware and what use cases work for them.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bhamm-lab"&gt; /u/bhamm-lab &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r877tl/vibe_check_latest_models_on_amd_strix_halo/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r877tl/vibe_check_latest_models_on_amd_strix_halo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r877tl/vibe_check_latest_models_on_amd_strix_halo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-18T16:13:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7mscr</id>
    <title>I trained a language model on CPU in 1.2 hours with no matrix multiplications — here's what I learned</title>
    <updated>2026-02-17T23:42:30+00:00</updated>
    <author>
      <name>/u/Own-Albatross868</name>
      <uri>https://old.reddit.com/user/Own-Albatross868</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all. I've been experimenting with tiny matmul-free language models that can be trained and run entirely on CPU. Just released the model.&lt;/p&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/changcheng967/flashlm-v3-13m"&gt;https://huggingface.co/changcheng967/flashlm-v3-13m&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Quick stats:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;13.6M parameters, d_model=256&lt;/li&gt; &lt;li&gt;Ternary weights ({-1, 0, +1}) — inference is just adds and subtracts, no multiplies&lt;/li&gt; &lt;li&gt;Trained on 2-thread CPU, no GPU, 1.2 hours&lt;/li&gt; &lt;li&gt;32M tokens from FineWeb-Edu&lt;/li&gt; &lt;li&gt;Validation loss: 6.80&lt;/li&gt; &lt;li&gt;Uses frozen GPT-2 embeddings (SVD projected) so it doesn't waste training time learning an embedding table&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The model produces grammatical-ish English but with zero coherence — it's learned syntax but not semantics. For 1.2 hours on a CPU, I'll take it.&lt;/p&gt; &lt;p&gt;The biggest surprise was that 86% of training time was spent on the output layer (projecting 256 dims to 50,257 vocab). The entire matmul-free ternary core only got 14% of compute. So the &amp;quot;efficient&amp;quot; part of the model was essentially starved of training signal by the inefficient softmax head.&lt;/p&gt; &lt;p&gt;Working on v4 that replaces the softmax with a hierarchical tree structure to fix this bottleneck. If it works, it should allow 5-10x more effective training in the same wall clock time.&lt;/p&gt; &lt;p&gt;Code is MIT licensed. Would love feedback from anyone else working on tiny/efficient models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Own-Albatross868"&gt; /u/Own-Albatross868 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7mscr/i_trained_a_language_model_on_cpu_in_12_hours/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7mscr/i_trained_a_language_model_on_cpu_in_12_hours/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r7mscr/i_trained_a_language_model_on_cpu_in_12_hours/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T23:42:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1r89a4y</id>
    <title>Vellium: open-source desktop app for creative writing with visual controls instead of prompt editing</title>
    <updated>2026-02-18T17:26:07+00:00</updated>
    <author>
      <name>/u/Possible_Statement84</name>
      <uri>https://old.reddit.com/user/Possible_Statement84</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r89a4y/vellium_opensource_desktop_app_for_creative/"&gt; &lt;img alt="Vellium: open-source desktop app for creative writing with visual controls instead of prompt editing" src="https://preview.redd.it/jdgxyzrhdakg1.jpg?width=140&amp;amp;height=78&amp;amp;auto=webp&amp;amp;s=7956ef19d5a35f0a082596b3cb054ece5781faf7" title="Vellium: open-source desktop app for creative writing with visual controls instead of prompt editing" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I got tired of digging through SillyTavern's config every time I wanted to change the tone of a scene. So I built my own thing.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The idea:&lt;/strong&gt; sliders instead of prompts. Want slow burn? Drag pacing down. High tension? Push intensity up. The app handles prompt injections behind the scenes. There are presets too if you don't want to tweak manually.&lt;/p&gt; &lt;p&gt;Chat with an inspector panel: Mood, Pacing, Intensity, Dialogue Style, Initiative, Descriptiveness, Unpredictability, Emotional Depth. All visual, no prompt editing needed.&lt;/p&gt; &lt;p&gt;Writer mode for longer stuff. Each chapter gets its own controls: Tone, Pacing, POV, Creativity, Tension, Detail, Dialogue Share. You can generate, expand, rewrite or summarize scenes. Generation runs in the background so you can chat while it writes.&lt;/p&gt; &lt;p&gt;Characters are shared between chat and writing. Build one in chat, drop them into a novel. Imports ST V2 cards and JSON. Avatars pull from Chub.&lt;/p&gt; &lt;p&gt;Lorebooks with keyword activation. MCP tool calling with per-function toggles. Multi-agent chat with auto turn switching. File attachments and vision in chat. Export to MD/DOCX.&lt;/p&gt; &lt;p&gt;Works with Ollama, LM Studio, OpenAI, OpenRouter, or any compatible endpoint. Light and dark themes. English, Russian, Chinese, Japanese.&lt;/p&gt; &lt;p&gt;Still rough around the edges but actively developing. Would love feedback.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/tg-prplx/vellium"&gt;https://github.com/tg-prplx/vellium&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Possible_Statement84"&gt; /u/Possible_Statement84 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1r89a4y"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r89a4y/vellium_opensource_desktop_app_for_creative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r89a4y/vellium_opensource_desktop_app_for_creative/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-18T17:26:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1r85z1t</id>
    <title>Even with Opus 4.6 and massive context windows, this is still the only thing that saves my production pipelines</title>
    <updated>2026-02-18T15:27:51+00:00</updated>
    <author>
      <name>/u/tdeliev</name>
      <uri>https://old.reddit.com/user/tdeliev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r85z1t/even_with_opus_46_and_massive_context_windows/"&gt; &lt;img alt="Even with Opus 4.6 and massive context windows, this is still the only thing that saves my production pipelines" src="https://preview.redd.it/esofp8nbu9kg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3395f6dda8e8eaa898f976c6258bdb37d5c27231" title="Even with Opus 4.6 and massive context windows, this is still the only thing that saves my production pipelines" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We all got excited when the new reasoning models dropped. Better at following instructions, longer context, fewer hallucinations. Great.&lt;/p&gt; &lt;p&gt;Still seeing agentic workflows fail at basic deterministic logic because teams treat the LLM as a CPU instead of what it is — a reasoning engine.&lt;/p&gt; &lt;p&gt;After the bug I shared on Monday (RAG pipeline recommending a candidate based on a three-year-old resume), I made my team go back to basics. Wrote a checklist I’ve been calling the Delegation Filter.&lt;/p&gt; &lt;p&gt;The first question does most of the heavy lifting:&lt;/p&gt; &lt;p&gt;“Is the outcome deterministic?”&lt;/p&gt; &lt;p&gt;If yes — don’t use an LLM. I don’t care if it’s GPT-5 or Opus 4.6. Write a SQL query. Deterministic code is free and correct every time. Probabilistic models are expensive and correct most of the time. For tasks where “most of the time” isn’t good enough, that gap will bite you.&lt;/p&gt; &lt;p&gt;Am I the only one who feels like we’re forgetting how to write regular code because the models got too good?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tdeliev"&gt; /u/tdeliev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/esofp8nbu9kg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r85z1t/even_with_opus_46_and_massive_context_windows/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r85z1t/even_with_opus_46_and_massive_context_windows/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-18T15:27:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7r7zr</id>
    <title>GLM-5 Technical Report</title>
    <updated>2026-02-18T02:51:52+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7r7zr/glm5_technical_report/"&gt; &lt;img alt="GLM-5 Technical Report" src="https://preview.redd.it/phk5j82g36kg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1a4da644c3d3988eba39a218faf8a811456998b3" title="GLM-5 Technical Report" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Presenting the GLM-5 Technical Report!&lt;/p&gt; &lt;p&gt;&lt;a href="http://arxiv.org/abs/2602.15763"&gt;http://arxiv.org/abs/2602.15763&lt;/a&gt;&lt;/p&gt; &lt;p&gt;After the launch of GLM-5, we’re pulling back the curtain on how it was built. Key innovations include:&lt;/p&gt; &lt;p&gt;- DSA Adoption: Significantly reduces training and inference costs while preserving long-context fidelity&lt;/p&gt; &lt;p&gt;- Asynchronous RL Infrastructure: Drastically improves post-training efficiency by decoupling generation from training&lt;/p&gt; &lt;p&gt;- Agent RL Algorithms: Enables the model to learn from complex, long-horizon interactions more effectively&lt;/p&gt; &lt;p&gt;Through these innovations, GLM-5 achieves SOTA performance among open-source models, with particularly strong results in real-world software engineering tasks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/phk5j82g36kg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7r7zr/glm5_technical_report/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r7r7zr/glm5_technical_report/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-18T02:51:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7y86d</id>
    <title>Gemma 27B/12B/4B/1B finetunes from DavidAU (20 models)</title>
    <updated>2026-02-18T09:13:14+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;Gemma 3 (1b, 4b, 12b and 27b) - Uncensored full Reasoning/Thinking models fine tuned using top distill datasets.&lt;/p&gt; &lt;p&gt;20 Gemma 3 models 1B, 4B, 12B and 27B with full reasoning using GLM 4.7 Flash, GPT, Claude and Gemini datasets and more fully fine tuned using Unsloth.&lt;/p&gt; &lt;p&gt;Most models are Heretic'ed (uncensored) first, and tuned second.&lt;br /&gt; This vastly improves the model.&lt;/p&gt; &lt;p&gt;Models are also bench marked and in almost all cases exceed org model metrics - and in some cases by a lot.&lt;/p&gt; &lt;p&gt;Enjoy the freedom and more powerful THINKING/REASONING and UNCENSORED Gemma 3s !&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/DavidAU/gemma-3-reasoning-thinking-models-incl-uncensored"&gt;https://huggingface.co/collections/DavidAU/gemma-3-reasoning-thinking-models-incl-uncensored&lt;/a&gt;&lt;/p&gt; &lt;p&gt;DavidAU on reddit: &lt;a href="/u/Dangerous_Fix_5526/"&gt;u/Dangerous_Fix_5526/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7y86d/gemma_27b12b4b1b_finetunes_from_davidau_20_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7y86d/gemma_27b12b4b1b_finetunes_from_davidau_20_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r7y86d/gemma_27b12b4b1b_finetunes_from_davidau_20_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-18T09:13:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1r81w8n</id>
    <title>(Google) On Surprising Effectiveness of Masking Updates in Adaptive Optimizers</title>
    <updated>2026-02-18T12:38:51+00:00</updated>
    <author>
      <name>/u/coder543</name>
      <uri>https://old.reddit.com/user/coder543</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r81w8n/google_on_surprising_effectiveness_of_masking/"&gt; &lt;img alt="(Google) On Surprising Effectiveness of Masking Updates in Adaptive Optimizers" src="https://external-preview.redd.it/ta2IiH0S_hDLLmbY2FJbCETWnyAZ9mwNCBtInzZsN24.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ffd08352f3df59650efb4f607d20fabe9a962ba0" title="(Google) On Surprising Effectiveness of Masking Updates in Adaptive Optimizers" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/coder543"&gt; /u/coder543 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/papers/2602.15322"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r81w8n/google_on_surprising_effectiveness_of_masking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r81w8n/google_on_surprising_effectiveness_of_masking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-18T12:38:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1r87ou8</id>
    <title>UPDATE#3: repurposing 800 RX 580s converted to AI cluster</title>
    <updated>2026-02-18T16:30:14+00:00</updated>
    <author>
      <name>/u/rasbid420</name>
      <uri>https://old.reddit.com/user/rasbid420</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hey everyone, posting an update on the ETH mining farm conversion project. last time i posted we were still figuring out what to even do with 800 rx 580s (mix of 4gb and 8gb sapphire nitro+ and pulse cards) sitting in an old ethereum mining farm&lt;/p&gt; &lt;p&gt;so the tldr is we think we finally found a good use case. maybe two actually.&lt;/p&gt; &lt;p&gt;the fundamental problem with these gpus is the interdevice communication. they have good usable vram 8GB but low pcie speeds, low memory bandwith, and each card sitting on its a celeron g3950 board with 8gb of system ram. you cant do tensor parallelism across nodes with these things. we tried, its not happening. the latency between devices kills anything... so we had to completely rethink the approach. instead of trying to make them work together on one big model through parallelism on a node or even RPC in network, we treat each gpu as a completely independant inference worker. one model per gpu, one request at a time, working in parallel across a cluster.&lt;/p&gt; &lt;p&gt;getting llama.cpp to run on gfx803 polaris in 2026 is... an experience. rocm support for more than one card is dismal for these cards and the biggest issue still is &amp;quot;PCI-E ATOMICS support&amp;quot;... we can't build llama.cpp with a HIP backend because we have 6 cards on each rig and it doesn't see more than one card...&lt;/p&gt; &lt;p&gt;so we went with vulkan and tested and benchmarked internally all the possible permutations and combinations with vulkan / ubuntu&lt;/p&gt; &lt;p&gt;and came up with the most optimal settings to run and build llama.cpp's vulkan for rx580 support&lt;/p&gt; &lt;p&gt;so our dockerfile_v43 that builds the entire graphics stack from source looks like this:&lt;/p&gt; &lt;p&gt;- libdrm 2.4.121 from source&lt;/p&gt; &lt;p&gt;- wayland 1.22 from source&lt;/p&gt; &lt;p&gt;- mesa 24.2.0 from source with llvm 15 and the radv vulkan driver&lt;/p&gt; &lt;p&gt;- vulkan sdk 1.3.283&lt;/p&gt; &lt;p&gt;- then llama.cpp on top of all that&lt;/p&gt; &lt;p&gt;we had to build with GGML_NATIVE=ON because avx2/fma produces a binary that segfaults on every worker node because celerons dont have avx. we had to explicitly disable everything except sse4.2:&lt;/p&gt; &lt;p&gt;-DGGML_NATIVE=OFF -DGGML_AVX=OFF -DGGML_AVX2=OFF -DGGML_FMA=OFF -DGGML_F16C=OFF -DGGML_SSE42=ON&lt;/p&gt; &lt;p&gt;CXXFLAGS=&amp;quot;-march=x86-64 -mtune=generic&amp;quot;&lt;/p&gt; &lt;p&gt;the model we use is qwen3-vl-8b-instruct which is a visual language model. the q4 quantization fits on a single 8gb card with room for 6k context tokens. we run 4 tiers of quantization across the fleet: q4 on 1 gpu, q8 on 2 gpus, bf16 on 3 or 6 gpus for quality escalation AND / OR bigger context&lt;/p&gt; &lt;p&gt;&lt;strong&gt;use case #1: mass document OCR / visual document understanding&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;we can process large documents like textbooks, medical literature, legal docs for high quality text extractions. the pdf gets split into individual pages, each page gets converted to an image and sent to a seperate gpu for visual understanding. you can get 200 gpus to process 200 pages simultaneously.&lt;/p&gt; &lt;p&gt;our quality benchmark is a clinical opthalmology of 966 pages of dense medical terminology, complex diagrams, photographic plates, multi-column layouts, tables, cursive annotations. the works. doing this through openai api with a visual model costs about $12 per run. we do it for roughly $0.50 in electricity at our local hydro rate of $0.065/kwh. thats 24x cheaper on opex and the capex is essentially nothing because we already had the hardware sitting there from the mining days. cards cost us like $80 per 8gb of vram vs $365/gb if you compare with an h100.&lt;/p&gt; &lt;p&gt;quality wise, its honestly comparable for document understanding work. cursive text, messy handwriting, charts, tables, images, the quantized qwen3-vl handles it.&lt;/p&gt; &lt;p&gt;the escalation path goes: tier 1 (q4, 175 dpi) &amp;gt; tier 2 (q8, 200 dpi) &amp;gt; tier 3 (bf16, 250 dpi) &amp;gt; tier 4 (bf16 on 6 gpus, 300 dpi). after 3 retries we accept degraded quality if it's impossible work but it works suprisingly well... most pages resolve on tier 1, only the really nasty scans escalate up.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;use case #2: video frame analysis (work in progress)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;this is the next thing were working on. same architecture but for video. 60 seconds of video at ~13fps = 800 frames. distribute 800 frames across 800 gpus,&lt;/p&gt; &lt;p&gt;each one describes what it sees in that frame. then you do temporal clustering, entity tracking, event extraction, and build a scene summary on top&lt;/p&gt; &lt;p&gt;the idea is to provide an endpoint where users can send video data and get back structured visual analysis. you could build monitoring alerts, safety assessments, quality assurance checks on top of it. stuff that currently costs way too much through traditional api calls to be practical at scale&lt;/p&gt; &lt;p&gt;were still early on this one but the architecture should translate pretty directly from the document pipeline. the hard part will be the temporal synthesis layers on top.&lt;/p&gt; &lt;p&gt;anyway... thats where were at. the mining farm to ai cluster conversion has been a year of pain but we finally have something that we can call useful&lt;/p&gt; &lt;p&gt;the key advantage of this cluster is the low cost of text extraction from documents which in turn can should be fed into a RAG pipeline like a chatgpt window for embedding/vectorization/good high quality chat on top of that document&lt;/p&gt; &lt;p&gt;happy to hear any feedback or any further ideas about this&lt;/p&gt; &lt;p&gt;&lt;a href="https://hyperstract.com"&gt;https://hyperstract.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;the system is capable of processing big pdfs of 400 pages per minute but please don't abuse it&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rasbid420"&gt; /u/rasbid420 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r87ou8/update3_repurposing_800_rx_580s_converted_to_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r87ou8/update3_repurposing_800_rx_580s_converted_to_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r87ou8/update3_repurposing_800_rx_580s_converted_to_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-18T16:30:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1r85o89</id>
    <title>Devstral Small 2 24B + Qwen3 Coder 30B: Coders for Every Hardware (Yes, Even the Pi)</title>
    <updated>2026-02-18T15:16:53+00:00</updated>
    <author>
      <name>/u/enrique-byteshape</name>
      <uri>https://old.reddit.com/user/enrique-byteshape</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r85o89/devstral_small_2_24b_qwen3_coder_30b_coders_for/"&gt; &lt;img alt="Devstral Small 2 24B + Qwen3 Coder 30B: Coders for Every Hardware (Yes, Even the Pi)" src="https://preview.redd.it/zzlx2eqlr9kg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=acd8d72883f48c2f76c3641eab494e4d6657dfba" title="Devstral Small 2 24B + Qwen3 Coder 30B: Coders for Every Hardware (Yes, Even the Pi)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;, &lt;em&gt;ByteShape’s back, alright! Everybody (yeah), you asked for coders (yeah). Everybody get your coders right:&lt;/em&gt; &lt;strong&gt;Devstral-Small-2-24B-Instruct-2512&lt;/strong&gt; (ShapeLearn-optimized for GPU) + &lt;strong&gt;Qwen3-Coder-30B-A3B-Instruct&lt;/strong&gt; (optimized for all hardware and patience levels). Alright!&lt;/p&gt; &lt;p&gt;We're back at it with another GGUF quants release, this time focused on coder models and multimodal. We use our technology to find the optimal datatypes per layer to squeeze as much performance out of these models while compromising the least amount of accuracy.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Devstral&lt;/strong&gt; is the hero on &lt;strong&gt;RTX 40/50 series&lt;/strong&gt;. Also: it has a &lt;strong&gt;quality cliff ~2.30 bpw,&lt;/strong&gt; but ShapeLearn avoids faceplanting there.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Qwen3-Coder&lt;/strong&gt; is the “runs everywhere” option: &lt;strong&gt;Pi 5 (16GB) ~9 TPS&lt;/strong&gt; at ~&lt;strong&gt;90%&lt;/strong&gt; BF16 quality. (If you daily-drive that Pi setup, we owe you a medal.)&lt;/li&gt; &lt;li&gt;Picking a model is annoying: Devstral is &lt;strong&gt;more capable&lt;/strong&gt; but &lt;strong&gt;more demanding&lt;/strong&gt; (dense 24B + bigger KV). If your &lt;strong&gt;context fits&lt;/strong&gt; and TPS is fine → Devstral. Otherwise → Qwen.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Links&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/byteshape/Devstral-Small-2-24B-Instruct-2512-GGUF"&gt;Devstral GGUFs&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/byteshape/Qwen3-Coder-30B-A3B-Instruct-GGUF"&gt;Qwen3 Coder 30B GGUFs&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://byteshape.com/blogs/Devstral-Small-2-24B-Instruct-2512/"&gt;Blog + plots&lt;/a&gt; (interactive graphs you can hover over and compare to Unsloth's models, with file name comparisons)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Bonus:&lt;/strong&gt; Qwen GGUFs ship with a &lt;strong&gt;custom template&lt;/strong&gt; that supports parallel tool calling (tested on llama.cpp; same template used for fair comparisons vs Unsloth). If you can sanity-check on different llama.cpp builds/backends and real coding workflows, any feedback will be greatly appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/enrique-byteshape"&gt; /u/enrique-byteshape &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zzlx2eqlr9kg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r85o89/devstral_small_2_24b_qwen3_coder_30b_coders_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r85o89/devstral_small_2_24b_qwen3_coder_30b_coders_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-18T15:16:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8157s</id>
    <title>Qwen 3.5 MXFP4 quants are coming - confirmed by Junyang Lin</title>
    <updated>2026-02-18T12:01:58+00:00</updated>
    <author>
      <name>/u/dampflokfreund</name>
      <uri>https://old.reddit.com/user/dampflokfreund</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Most here are aware that OpenAI did something very well with their GPT-Oss release - they trained their model in 4 bit and delivered native mxfp4 quants which means a lot higher quality than the typical Unsloth and Bartowski quants of bf16 models. Google did it too with Gemma 3 QAT which was very well received by the community. Super excited for it, this is definately the right direction to take!&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/JustinLin610/status/2024002713579651245"&gt;https://x.com/JustinLin610/status/2024002713579651245&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dampflokfreund"&gt; /u/dampflokfreund &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8157s/qwen_35_mxfp4_quants_are_coming_confirmed_by/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8157s/qwen_35_mxfp4_quants_are_coming_confirmed_by/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8157s/qwen_35_mxfp4_quants_are_coming_confirmed_by/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-18T12:01:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8aocl</id>
    <title>Car Wash Test on 53 leading models (10 runs/model): “I want to wash my car. The car wash is 50 meters away. Should I walk or drive?”</title>
    <updated>2026-02-18T18:15:35+00:00</updated>
    <author>
      <name>/u/facethef</name>
      <uri>https://old.reddit.com/user/facethef</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8aocl/car_wash_test_on_53_leading_models_10_runsmodel_i/"&gt; &lt;img alt="Car Wash Test on 53 leading models (10 runs/model): “I want to wash my car. The car wash is 50 meters away. Should I walk or drive?”" src="https://preview.redd.it/w6gtt96vlakg1.png?width=140&amp;amp;height=138&amp;amp;auto=webp&amp;amp;s=44866028eabd01a070c708b02cc05bff26bc1999" title="Car Wash Test on 53 leading models (10 runs/model): “I want to wash my car. The car wash is 50 meters away. Should I walk or drive?”" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;UPDATE&lt;/strong&gt;: I reran the car wash test 10 times per model and only 5 out of 53 models can do this reliably at this sample size. &lt;/p&gt; &lt;p&gt;Original post: I asked 53 models &amp;quot;I want to wash my car. The car wash is 50 meters away. Should I walk or drive?&amp;quot; Obviously you need to drive because the car needs to be at the car wash. 11 out of 53 got it right on a single call. &lt;/p&gt; &lt;p&gt;People pointed out a single run doesn’t show the full picture, which is obviously correct. The first post was just for the fun of it. But now I reran every model 10x to see how consistent the models actually are. Same prompt, no system prompt tricks, no cache/memory, clean slate each time. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Now it’s pass rates, not one-off right/wrong. Turns out most models that got it right on a single run can't do it reliably.&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;But also open-weight models showed more capability than in the one-off version. Several models that scored 0 on the single run actually get it right sometimes: &lt;/p&gt; &lt;p&gt;GLM-4.7: 6/10 drive — single run happened to land on the wrong side&lt;/p&gt; &lt;p&gt;GLM-4.7 Flash: 4/10 drive&lt;/p&gt; &lt;p&gt;MiniMax M2.1: 2/10 drive&lt;/p&gt; &lt;p&gt;Kimi K2 Thinking: 2/10 drive &lt;/p&gt; &lt;p&gt;DeepSeek v3.2: 1/10 drive&lt;/p&gt; &lt;p&gt;GPT-OSS 20B: 1/10 drive&lt;/p&gt; &lt;p&gt;GPT-OSS 120B: 1/10 drive&lt;/p&gt; &lt;p&gt;Kimi K2 Thinking Turbo: 1/10 drive &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Some corrections from the original post:&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Sonar went from &amp;quot;correct&amp;quot; to 0/10. It still writes the same 200-word essay about food production energy chains in every run, it just lands on &amp;quot;walk&amp;quot; now. Kimi K2.5 went from correct to 5/10. GLM-4.7 went from wrong to 6/10, was just unlucky on the single run. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Full scorecard by family:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Anthropic: 1/9 — only Opus 4.6 (10/10)&lt;/p&gt; &lt;p&gt;OpenAI: 1/12 — only GPT-5 (7/10)&lt;/p&gt; &lt;p&gt;Google: 3/8 — Gemini 3 models + Flash Lite all 10/10&lt;/p&gt; &lt;p&gt;xAI: 2/4 — Grok-4 10/10, Reasoning 8/10&lt;/p&gt; &lt;p&gt;Perplexity: 0/3&lt;/p&gt; &lt;p&gt;Zhipu: 2/3 — GLM-5 8/10, GLM-4.7 6/10&lt;/p&gt; &lt;p&gt;Meta (Llama): 0/4&lt;/p&gt; &lt;p&gt;Mistral: 0/3&lt;/p&gt; &lt;p&gt;DeepSeek: 0/2&lt;/p&gt; &lt;p&gt;Moonshot: 0/4&lt;/p&gt; &lt;p&gt;MiniMax: 0/1 &lt;/p&gt; &lt;p&gt;Rerun via &lt;a href="https://opper.ai/"&gt;Opper&lt;/a&gt;, 530 calls, same prompt, no system prompt tricks, no cache / memory&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/facethef"&gt; /u/facethef &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1r8aocl"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8aocl/car_wash_test_on_53_leading_models_10_runsmodel_i/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8aocl/car_wash_test_on_53_leading_models_10_runsmodel_i/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-18T18:15:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1r86i3o</id>
    <title>LLMs grading other LLMs 2</title>
    <updated>2026-02-18T15:47:24+00:00</updated>
    <author>
      <name>/u/Everlier</name>
      <uri>https://old.reddit.com/user/Everlier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r86i3o/llms_grading_other_llms_2/"&gt; &lt;img alt="LLMs grading other LLMs 2" src="https://preview.redd.it/rmq2mwriw9kg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=07e6fa12e92be2b51d119d6c78ac4e28ccf7e1cb" title="LLMs grading other LLMs 2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A year ago I made a &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1j1npv1/llms_grading_other_llms/"&gt;meta-eval here on the sub&lt;/a&gt;, asking LLMs to grade a few criterias about other LLMs. &lt;/p&gt; &lt;p&gt;Time for the part 2.&lt;/p&gt; &lt;p&gt;The premise is very simple, the model is asked a few ego-baiting questions and other models are then asked to rank it. The scores in the pivot table are normalised.&lt;/p&gt; &lt;p&gt;You can find &lt;a href="https://huggingface.co/datasets/av-codes/cringebench"&gt;all the data on HuggingFace&lt;/a&gt; for your analysis.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Everlier"&gt; /u/Everlier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rmq2mwriw9kg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r86i3o/llms_grading_other_llms_2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r86i3o/llms_grading_other_llms_2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-18T15:47:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1r83irw</id>
    <title>PSA: DDR5 RDIMM price passed the point were 3090 are less expensive per gb..</title>
    <updated>2026-02-18T13:51:04+00:00</updated>
    <author>
      <name>/u/No_Afternoon_4260</name>
      <uri>https://old.reddit.com/user/No_Afternoon_4260</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello all,&lt;/p&gt; &lt;p&gt;Just wanted to note that RDIMM prices are so wild.. Stacking rdimms starts to be as expensive as stacking 3090s.. But RDIMM don't come with compute included..&lt;/p&gt; &lt;p&gt;What a crazy time, shall we stack rdimms or 3090, what's your take on that?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Afternoon_4260"&gt; /u/No_Afternoon_4260 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r83irw/psa_ddr5_rdimm_price_passed_the_point_were_3090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r83irw/psa_ddr5_rdimm_price_passed_the_point_were_3090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r83irw/psa_ddr5_rdimm_price_passed_the_point_were_3090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-18T13:51:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7bsfd</id>
    <title>Best Audio Models - Feb 2026</title>
    <updated>2026-02-17T17:01:37+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;They've been a ton of audio models released of late, the most notable perhaps being Qwen3 TTS. So its time for another &lt;strong&gt;Best Audio Models&lt;/strong&gt; megathread&lt;/p&gt; &lt;p&gt;Share what your favorite ASR, TTS, STT, Text to Music models are right now &lt;strong&gt;and why.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Given the the amount of ambiguity and subjectivity in rating/testing these models, please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks etc. Closed models like Elevenlabs v3 seem to continue to be a few levels above open models especially for production use cases with long lengths/stability requirements, so comparisons, especially empirical ones are welcome.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please use the top level comments to thread your responses.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T17:01:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1r60qu9</id>
    <title>AMA Announcement: StepFun AI, The Opensource Lab Behind Step-3.5-Flash Model (Thursday, 8AM-11AM PST)</title>
    <updated>2026-02-16T05:11:16+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r60qu9/ama_announcement_stepfun_ai_the_opensource_lab/"&gt; &lt;img alt="AMA Announcement: StepFun AI, The Opensource Lab Behind Step-3.5-Flash Model (Thursday, 8AM-11AM PST)" src="https://preview.redd.it/u11uh8jfisjg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=afc30b2b6ae673f2e940109e2001bb498bd818ad" title="AMA Announcement: StepFun AI, The Opensource Lab Behind Step-3.5-Flash Model (Thursday, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; 👋&lt;/p&gt; &lt;p&gt;We're excited for Thursday's guests: &lt;strong&gt;The StepFun Team!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Kicking things off Thursday, Feb. 19th, 8 AM–11 AM PST&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;⚠️ &lt;strong&gt;Note:&lt;/strong&gt; The AMA itself will be hosted in a &lt;strong&gt;separate thread,&lt;/strong&gt; please don’t post questions here.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/u11uh8jfisjg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r60qu9/ama_announcement_stepfun_ai_the_opensource_lab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r60qu9/ama_announcement_stepfun_ai_the_opensource_lab/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T05:11:16+00:00</published>
  </entry>
</feed>
