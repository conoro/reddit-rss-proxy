<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-18T16:09:06+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1pplvzz</id>
    <title>[Project] I built a local "System 2" VLM pipeline to mine Autonomous Driving data on a single RTX 3090 (No Cloud APIs). Beats CLIP recall by ~50%.</title>
    <updated>2025-12-18T08:36:42+00:00</updated>
    <author>
      <name>/u/Pale_Location_373</name>
      <uri>https://old.reddit.com/user/Pale_Location_373</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I‚Äôm an independent researcher working on Autonomous Vehicles. I wanted to solve the &amp;quot;Dark Data&amp;quot; problem‚Äîwe have petabytes of driving logs, but finding the weird edge cases (e.g., a wheelchair on the road, sensor glare, passive construction zones) is incredibly hard.&lt;/p&gt; &lt;p&gt;Standard methods use metadata tags (too vague) or CLIP embeddings (spatial blindness). Sending petabytes of video to GPT-4V is impossible due to cost and privacy.&lt;/p&gt; &lt;p&gt;So, I built &lt;strong&gt;Semantic-Drive&lt;/strong&gt;: A local-first, neuro-symbolic data mining engine that runs entirely on consumer hardware (tested on an RTX 3090).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Architecture (&amp;quot;System 2&amp;quot; Inference):&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Instead of just asking a VLM to &amp;quot;describe the image,&amp;quot; I implemented a &lt;strong&gt;Judge-Scout&lt;/strong&gt; architecture inspired by recent reasoning models (o1):&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Symbolic Grounding (The Eye):&lt;/strong&gt; I use &lt;strong&gt;YOLO-E&lt;/strong&gt; to extract a high-recall text inventory of objects. This is injected into the VLM's context window as a hard constraint.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cognitive Analysis (The Scouts):&lt;/strong&gt; I run quantized VLMs (&lt;strong&gt;Qwen3-VL-30B-A3B-Thinking, Gemma-3-27B-IT,&lt;/strong&gt; and &lt;strong&gt;Kimi-VL-A3B-Thinking-2506&lt;/strong&gt;) via &lt;em&gt;llama.cpp&lt;/em&gt;. They perform a Chain-of-Thought &amp;quot;&lt;em&gt;forensic analysis&lt;/em&gt;&amp;quot; to verify if the YOLO objects are actual hazards or just artifacts (like a poster of a person).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Inference-Time Consensus (The Judge):&lt;/strong&gt; A local &lt;strong&gt;Ministral-3-14B-Instruct-2512&lt;/strong&gt; aggregates reports from multiple scouts. It uses an &lt;strong&gt;Explicit Outcome Reward Model (ORM),&lt;/strong&gt; a Python script that scores generations based on YOLO consistency, to perform a &lt;strong&gt;Best-of-N&lt;/strong&gt; search.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;The Results (Benchmarked on nuScenes):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Recall:&lt;/strong&gt; 0.966 (vs 0.475 for CLIP ViT-L/14).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hallucination:&lt;/strong&gt; Reduced Risk Assessment Error by &lt;strong&gt;51%&lt;/strong&gt; compared to a raw zero-shot VLM.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cost:&lt;/strong&gt; ~$0.85 per 1k frames (Energy) vs ~$30.00 for GPT-4o.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The Tech Stack:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Inference:&lt;/strong&gt; `llama.cpp` server (Dockerized).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Models:&lt;/strong&gt; Q4_K_M GGUFs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;UI:&lt;/strong&gt; Streamlit (for human-in-the-loop verification).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I‚Äôve open-sourced the whole thing, including the Docker setup and a &amp;quot;Gold Set&amp;quot; benchmark for long-tail mining.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Repo:&lt;/strong&gt; &lt;a href="https://github.com/AntonioAlgaida/Semantic-Drive"&gt;https://github.com/AntonioAlgaida/Semantic-Drive&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Live Demo (HF Space):&lt;/strong&gt; &lt;a href="https://huggingface.co/spaces/agnprz/Semantic-Drive-Explorer"&gt;https://huggingface.co/spaces/agnprz/Semantic-Drive-Explorer&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Paper (ArXiv):&lt;/strong&gt; &lt;a href="https://arxiv.org/abs/2512.12012"&gt;https://arxiv.org/abs/2512.12012&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Happy to answer questions about the prompt engineering or the local &amp;quot;System 2&amp;quot; implementation!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pale_Location_373"&gt; /u/Pale_Location_373 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pplvzz/project_i_built_a_local_system_2_vlm_pipeline_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pplvzz/project_i_built_a_local_system_2_vlm_pipeline_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pplvzz/project_i_built_a_local_system_2_vlm_pipeline_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T08:36:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1pprg7j</id>
    <title>How do you all evaluate "underrated" models? Benchmarks vs real-world use?</title>
    <updated>2025-12-18T13:57:49+00:00</updated>
    <author>
      <name>/u/robbigo</name>
      <uri>https://old.reddit.com/user/robbigo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been noticing that underrated LLMs come up here pretty regularly, often a list of models. But reading those threads, it struck me that people often mean very different things by &amp;quot;underrated&amp;quot;.&lt;/p&gt; &lt;p&gt;Some models look incredible on benchmarks but feel underwhelming in daily use, while others with little hype punch far above their weight. &lt;/p&gt; &lt;p&gt;I think &amp;quot;underrated&amp;quot; can mean very different things depending on what you valeu. &lt;/p&gt; &lt;p&gt;How do you personally define an &amp;quot;underrated&amp;quot; model? &lt;/p&gt; &lt;p&gt;- Pure benchmark performance vs reputation? &lt;/p&gt; &lt;p&gt;- Real-world usability and reliability? &lt;/p&gt; &lt;p&gt;- Cost/performance ratio? &lt;/p&gt; &lt;p&gt;- Something else entirely? &lt;/p&gt; &lt;p&gt;Curious what others prioritize&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/robbigo"&gt; /u/robbigo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pprg7j/how_do_you_all_evaluate_underrated_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pprg7j/how_do_you_all_evaluate_underrated_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pprg7j/how_do_you_all_evaluate_underrated_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T13:57:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppsryi</id>
    <title>AI note takers across devices vs fully local setups</title>
    <updated>2025-12-18T14:54:05+00:00</updated>
    <author>
      <name>/u/Cristiano1</name>
      <uri>https://old.reddit.com/user/Cristiano1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been going back and forth between building a fully local setup (Whisper plus a local LLM) and just using an AI note taker across devices. The local approach gives you full control, but it gets annoying when you want access to notes on both your laptop and phone without babysitting sync scripts.&lt;/p&gt; &lt;p&gt;Lately I‚Äôve tried Bluedot as a middle ground since it works across devices and doesn‚Äôt rely on bots joining meetings. It‚Äôs been convenient, but I‚Äôm still weighing that against the appeal of going fully local.&lt;/p&gt; &lt;p&gt;Is anyone running a hybrid setup they‚Äôre actually happy with?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cristiano1"&gt; /u/Cristiano1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppsryi/ai_note_takers_across_devices_vs_fully_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppsryi/ai_note_takers_across_devices_vs_fully_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppsryi/ai_note_takers_across_devices_vs_fully_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T14:54:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1porpwd</id>
    <title>Microsoft's TRELLIS 2-4B, An Open-Source Image-to-3D Model</title>
    <updated>2025-12-17T08:49:00+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1porpwd/microsofts_trellis_24b_an_opensource_imageto3d/"&gt; &lt;img alt="Microsoft's TRELLIS 2-4B, An Open-Source Image-to-3D Model" src="https://external-preview.redd.it/OXpuN3VqYnE4cTdnMbhg7mfH3BLNBAJzBcqwf-BeiskbYrfqW4XgiIx-FQh0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6475aef4e90b21644bf95a26618a75433c2e08de" title="Microsoft's TRELLIS 2-4B, An Open-Source Image-to-3D Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Model Details&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Model Type:&lt;/strong&gt; Flow-Matching Transformers with Sparse Voxel based 3D VAE&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Parameters:&lt;/strong&gt; 4 Billion&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Input:&lt;/strong&gt; Single Image&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Output:&lt;/strong&gt; 3D Asset &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Model - &lt;a href="https://huggingface.co/microsoft/TRELLIS.2-4B"&gt;https://huggingface.co/microsoft/TRELLIS.2-4B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Demo - &lt;a href="https://huggingface.co/spaces/microsoft/TRELLIS.2"&gt;https://huggingface.co/spaces/microsoft/TRELLIS.2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog post - &lt;a href="https://microsoft.github.io/TRELLIS.2/"&gt;https://microsoft.github.io/TRELLIS.2/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/g8uco5dq8q7g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1porpwd/microsofts_trellis_24b_an_opensource_imageto3d/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1porpwd/microsofts_trellis_24b_an_opensource_imageto3d/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T08:49:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppu35l</id>
    <title>Mistral released Mistral OCR 3: 74% overall win rate over Mistral OCR 2 on forms, scanned documents, complex tables, and handwriting.</title>
    <updated>2025-12-18T15:47:20+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppu35l/mistral_released_mistral_ocr_3_74_overall_win/"&gt; &lt;img alt="Mistral released Mistral OCR 3: 74% overall win rate over Mistral OCR 2 on forms, scanned documents, complex tables, and handwriting." src="https://b.thumbs.redditmedia.com/CTdnDnhEVXKhvcC_xn7Fo04JbVfjTe3Wx_yk_R9kVRw.jpg" title="Mistral released Mistral OCR 3: 74% overall win rate over Mistral OCR 2 on forms, scanned documents, complex tables, and handwriting." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Source: &lt;a href="https://mistral.ai/news/mistral-ocr-3"&gt;https://mistral.ai/news/mistral-ocr-3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Mistral OCR 3 sets new benchmarks in both accuracy and efficiency, outperforming enterprise document processing solutions as well as AI-native OCR.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ppu35l"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppu35l/mistral_released_mistral_ocr_3_74_overall_win/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppu35l/mistral_released_mistral_ocr_3_74_overall_win/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T15:47:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppn7t1</id>
    <title>TIGER: Speech/Cinematic Sound Separation Demo</title>
    <updated>2025-12-18T10:06:14+00:00</updated>
    <author>
      <name>/u/Warm-Professor-9299</name>
      <uri>https://old.reddit.com/user/Warm-Professor-9299</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppn7t1/tiger_speechcinematic_sound_separation_demo/"&gt; &lt;img alt="TIGER: Speech/Cinematic Sound Separation Demo" src="https://external-preview.redd.it/aTM2bmlqNDVzeDdnMcXLO7I6Oh1OfkJnF9rX2m0V5xuhWFP5rjp4KW7s81RB.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5becf3159a24e2e869947df70751661db13aa849" title="TIGER: Speech/Cinematic Sound Separation Demo" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I stumbled upon this project that performs really well at separating the BG music, voice and effects from single audio. See for yourself: &lt;a href="https://cslikai.cn/TIGER/"&gt;https://cslikai.cn/TIGER/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Warm-Professor-9299"&gt; /u/Warm-Professor-9299 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/amc7d745sx7g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppn7t1/tiger_speechcinematic_sound_separation_demo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppn7t1/tiger_speechcinematic_sound_separation_demo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T10:06:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1pp9w31</id>
    <title>AMA with the Meta researchers behind SAM 3 + SAM 3D + SAM Audio</title>
    <updated>2025-12-17T22:18:01+00:00</updated>
    <author>
      <name>/u/AIatMeta</name>
      <uri>https://old.reddit.com/user/AIatMeta</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLlama"&gt;r/LocalLlama&lt;/a&gt;! We‚Äôre the research team behind the newest members of the Segment Anything collection of models: SAM 3 + SAM 3D + SAM Audio.&lt;/p&gt; &lt;p&gt;We‚Äôre excited to be here to talk all things SAM (sorry, we can‚Äôt share details on other projects or future work) and have members from across our team participating:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;SAM 3 (&lt;/strong&gt;&lt;a href="https://ai.meta.com/blog/segment-anything-model-3/?utm_source=reddit&amp;amp;utm_medium=organic_social&amp;amp;utm_content=ama&amp;amp;utm_campaign=sam"&gt;&lt;strong&gt;learn more&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Nikhila Ravi&lt;/li&gt; &lt;li&gt;Pengchuan Zhang&lt;/li&gt; &lt;li&gt;Shoubhik Debnath&lt;/li&gt; &lt;li&gt;Chay Ryali&lt;/li&gt; &lt;li&gt;Yuan-Ting Hu&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;SAM 3D (&lt;/strong&gt;&lt;a href="https://ai.meta.com/blog/sam-3d/?utm_source=reddit&amp;amp;utm_medium=organic_social&amp;amp;utm_content=ama&amp;amp;utm_campaign=sam"&gt;&lt;strong&gt;learn more&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Weiyao Wang&lt;/li&gt; &lt;li&gt;Sasha Sax&lt;/li&gt; &lt;li&gt;Xitong Yang&lt;/li&gt; &lt;li&gt;Jinkun Cao&lt;/li&gt; &lt;li&gt;Michelle Guo&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;SAM Audio (&lt;/strong&gt;&lt;a href="https://ai.meta.com/blog/sam-audio/?utm_source=reddit&amp;amp;utm_medium=organic_social&amp;amp;utm_content=ama&amp;amp;utm_campaign=sam"&gt;&lt;strong&gt;learn more&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Bowen Shi&lt;/li&gt; &lt;li&gt;Andros Tjandra&lt;/li&gt; &lt;li&gt;John Hoffman&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can try SAM Audio, SAM 3D, and SAM 3 in the Segment Anything Playground: &lt;a href="https://go.meta.me/87b53b"&gt;https://go.meta.me/87b53b&lt;/a&gt; &lt;/p&gt; &lt;p&gt;PROOF: &lt;a href="https://x.com/AIatMeta/status/2001429429898407977"&gt;https://x.com/AIatMeta/status/2001429429898407977&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;We‚Äôll be answering questions live on Thursday, Dec. 18, from 2-3pm PT. Hope to see you there.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AIatMeta"&gt; /u/AIatMeta &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp9w31/ama_with_the_meta_researchers_behind_sam_3_sam_3d/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp9w31/ama_with_the_meta_researchers_behind_sam_3_sam_3d/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pp9w31/ama_with_the_meta_researchers_behind_sam_3_sam_3d/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T22:18:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppjo5b</id>
    <title>Day 10: 21 Days of Building a Small Language Model: KV Cache</title>
    <updated>2025-12-18T06:14:31+00:00</updated>
    <author>
      <name>/u/Prashant-Lakhera</name>
      <uri>https://old.reddit.com/user/Prashant-Lakhera</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppjo5b/day_10_21_days_of_building_a_small_language_model/"&gt; &lt;img alt="Day 10: 21 Days of Building a Small Language Model: KV Cache" src="https://b.thumbs.redditmedia.com/vt9eeanrTHbty0PeG_jN-4PShJ8fGv9qBsfU3Bpopco.jpg" title="Day 10: 21 Days of Building a Small Language Model: KV Cache" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Welcome to Day 10 of 21 Days of Building a Small Language Model. The topic for today is the KV cache. Yesterday, we explored multi-head attention and how it allows models to look at sequences from multiple perspectives simultaneously. Today, we'll see why generating text would be impossibly slow without a clever optimization called the Key-Value cache.&lt;/p&gt; &lt;h1&gt;Problem&lt;/h1&gt; &lt;p&gt;To understand why KV cache is necessary, we first need to understand how language models generate text. The process is simple: the model predicts one token at a time, using all previously generated tokens as context.&lt;/p&gt; &lt;p&gt;Let's walk through a simple example. Suppose you prompt the model with: The algorithm processes data&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ketg7dmymw7g1.png?width=1006&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1998bceae61cdc3a85a3c13fd7292dc0f229c280"&gt;https://preview.redd.it/ketg7dmymw7g1.png?width=1006&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1998bceae61cdc3a85a3c13fd7292dc0f229c280&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here's what happens step by step:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;First pass&lt;/strong&gt;: The model processes these four tokens through all transformer layers and predicts the next token, say efficiently&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Second pass&lt;/strong&gt;: Now the sequence is. The algorithm processes data efficiently. The model feeds this &lt;em&gt;entire&lt;/em&gt; sequence through all layers again to predict the next token, perhaps by&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Third pass&lt;/strong&gt;: The sequence becomes. The algorithm processes data efficiently by, and this &lt;em&gt;entire&lt;/em&gt; sequence is processed again to predict the next token&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;This process can continue for potentially hundreds or thousands of tokens.&lt;/p&gt; &lt;p&gt;Notice something deeply inefficient here: we're repeatedly recomputing attention for all earlier tokens, even though those computations never change.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;In the first pass, we compute Query (Q), Key (K), and Value (V) vectors for [&amp;quot;The&amp;quot;, &amp;quot;algorithm&amp;quot;, &amp;quot;processes&amp;quot;, &amp;quot;data&amp;quot;]&lt;/li&gt; &lt;li&gt;In the second pass, we recompute Q/K/V for those same four tokens &lt;em&gt;again&lt;/em&gt;, plus &amp;quot;efficiently&amp;quot;&lt;/li&gt; &lt;li&gt;In the third pass, we recompute all five previous tokens &lt;em&gt;again&lt;/em&gt;, plus the new one&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Each iteration repeats 90-99% of the same computation. We're essentially throwing away all the work we did in previous iterations and starting over from scratch.&lt;/p&gt; &lt;p&gt;The problem compounds as sequences grow longer. If you're generating a 1,000-token response:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The first token's attention is computed 1,000 times&lt;/li&gt; &lt;li&gt;The second token's attention is computed 999 times&lt;/li&gt; &lt;li&gt;And so on...&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For a 100-token sequence, you'd compute Q/K/V a total of 5,050 times (1+2+...+100) when you really only need to do it 100 times (once per token). This massive redundancy is what makes inference slow and expensive without optimization.&lt;/p&gt; &lt;p&gt;üí° &lt;strong&gt;NOTE:&lt;/strong&gt; KV caching only comes during the inference stage. It does not exist during training or pretraining. The KV cache is purely an inference-time optimization that helps accelerate text generation after the model has been trained. This distinction is critical to understand. The cache is used when the model is generating text, not when it is learning from data.&lt;/p&gt; &lt;h1&gt;Only the last token matters&lt;/h1&gt; &lt;p&gt;Here's something that might not be obvious at first, but changes everything once you see it: when predicting the next token, only the last token's output matters.&lt;/p&gt; &lt;p&gt;Think about what happens at the transformer's output. We get a logits matrix with probability distributions for &lt;em&gt;every&lt;/em&gt; token in the sequence. But for prediction, we only use the last row, the logits for the most recent token.&lt;/p&gt; &lt;p&gt;When processing The algorithm processes data efficiently, we compute logits for all five tokens, but we only care about the logits for efficiently to determine what comes next. The earlier tokens? Their logits get computed and then ignored.&lt;/p&gt; &lt;p&gt;This raises an important question: why not just keep the last token and throw away everything else?&lt;/p&gt; &lt;p&gt;While we only need the last token's logits for prediction, we still need information from all earlier tokens to compute those logits correctly. Remember from Day 9, the attention mechanism needs to look at all previous tokens to create context for the current token.&lt;/p&gt; &lt;p&gt;So we can't simply discard everything. We need a smarter approach: preserve information from earlier tokens in a form that lets us efficiently compute attention for new tokens, without recomputing everything from scratch.&lt;/p&gt; &lt;h1&gt;Solution&lt;/h1&gt; &lt;p&gt;Let's work backward from what we actually need to compute the next token.&lt;/p&gt; &lt;p&gt;To compute the context vector for the latest token (say, &amp;quot;efficiently&amp;quot;), we need:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Attention weights&lt;/strong&gt; for &amp;quot;efficiently&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Value vectors&lt;/strong&gt; for all previous tokens&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;And to compute those attention weights, we need:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Query vector&lt;/strong&gt; for &amp;quot;efficiently&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Key vectors&lt;/strong&gt; for all previous tokens&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Looking at this list reveals an important pattern: we only need all previous key vectors and all previous value vectors. We do NOT need to store previous query vectors. Here's why this distinction matters.&lt;/p&gt; &lt;h1&gt;Why Queries aren't cached&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/v15xtcmymw7g1.png?width=566&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8c629f193faa2f2823f1a17ae906dcc99292fb72"&gt;https://preview.redd.it/v15xtcmymw7g1.png?width=566&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8c629f193faa2f2823f1a17ae906dcc99292fb72&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is the first question that comes to everyone‚Äôs mind. The query vector has a very specific, one time job. It's only used to compute attention weights for the &lt;em&gt;current&lt;/em&gt; token. Once we've done that and combined the value vectors, the query has served its purpose. We never need it again.&lt;/p&gt; &lt;p&gt;Let's trace through what happens with &amp;quot;efficiently&amp;quot;: ‚Ä¢ We compute its query vector to figure out which previous tokens to attend to ‚Ä¢ We compare this query to all the previous keys (from &amp;quot;The&amp;quot;, &amp;quot;algorithm&amp;quot;, &amp;quot;processes&amp;quot;, &amp;quot;data&amp;quot;) ‚Ä¢ We get attention weights and use them to combine the previous value vectors ‚Ä¢ Done. The query is never used again.&lt;/p&gt; &lt;p&gt;When the next token &amp;quot;by&amp;quot; arrives: ‚Ä¢ We'll compute &amp;quot;by&amp;quot;'s NEW query vector for its attention ‚Ä¢ But we WON'T need &amp;quot;efficiently&amp;quot;'s query vector anymore ‚Ä¢ However, we WILL need &amp;quot;efficiently&amp;quot;'s key and value vectors, because &amp;quot;by&amp;quot; needs to attend to &amp;quot;efficiently&amp;quot; and all previous tokens&lt;/p&gt; &lt;p&gt;See the pattern? Each token's query is temporary. But each token's keys and values are permanent. They're needed by every future token.&lt;/p&gt; &lt;p&gt;This is why it's called the KV cache, not the QKV cache.&lt;/p&gt; &lt;p&gt;Here's a helpful mental model: think of the query as asking a question (&amp;quot;What should I pay attention to?&amp;quot;). Once you get your answer, you don't need to ask again. But the keys and values? They're like books in a library. Future tokens will need to look them up, so we keep them around.&lt;/p&gt; &lt;h1&gt;Memory Cost&lt;/h1&gt; &lt;p&gt;While KV cache makes inference dramatically faster, this optimization comes with a significant tradeoff: it requires substantial memory.&lt;/p&gt; &lt;p&gt;The cache must store a key vector and value vector for every layer, every head, and every token in the sequence. These requirements accumulate quickly.&lt;/p&gt; &lt;p&gt;The formula for calculating memory requirements:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;KV Cache Size = layers √ó batch_size √ó num_heads √ó head_dim √ó seq_length √ó 2 √ó 2 Where: ‚Ä¢ First 2: for Keys and Values ‚Ä¢ Second 2: bytes per parameter (FP16 uses 2 bytes) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;For example, let's examine numbers from models to understand the scale of memory requirements.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Example 1: A 30B Parameter Model&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;‚Ä¢ Layers: 48 ‚Ä¢ Batch size: 128 ‚Ä¢ Total head dimensions: 7,168 ‚Ä¢ Sequence length: 1,024 tokens KV Cache Size = 48 √ó 128 √ó 7,168 √ó 1,024 √ó 2 √ó 2 = ~180 GB &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;That's 180 GB just for the cache, not even including the model parameters themselves.&lt;/p&gt; &lt;p&gt;For models designed for long contexts, the requirements grow even larger:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Example 2: A Long Context Model&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;‚Ä¢ Layers: 61 ‚Ä¢ Batch size: 1 ‚Ä¢ Heads: 128 ‚Ä¢ Head dimension: 128 ‚Ä¢ Sequence length: 100,000 tokens KV Cache Size = 61 √ó 1 √ó 128 √ó 128 √ó 100,000 √ó 2 √ó 2 = ~400 GB &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;400 GB represents a massive memory requirement. No single GPU can accommodate this, and even multi-GPU setups face significant challenges.&lt;/p&gt; &lt;p&gt;KV cache memory scales linearly with context length. Doubling the context length doubles the memory requirements, which directly translates to higher costs and fewer requests that can be served in parallel.&lt;/p&gt; &lt;h1&gt;Addressing the Memory Challenge&lt;/h1&gt; &lt;p&gt;The memory constraints of KV cache aren't just theoretical concerns. They're real bottlenecks that have driven significant innovation in several directions:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Multi Query Attention (MQA)&lt;/strong&gt;: What if all attention heads shared one key and one value projection instead of each having its own? Instead of storing H separate key/value vectors per token per layer, you'd store just one that all heads share. Massive memory savings.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Grouped Query Attention (GQA)&lt;/strong&gt;: A middle ground. Instead of all heads sharing K/V (MQA) or each head having its own (standard multi-head attention), groups of heads share K/V. Better memory than standard attention, more flexibility than MQA.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Other Approaches&lt;/strong&gt;: ‚Ä¢ Sparse attention (only attend to relevant tokens) ‚Ä¢ Linear attention (reduce the quadratic complexity) ‚Ä¢ Compression techniques (reduce precision/dimensionality of cached K/V)&lt;/p&gt; &lt;p&gt;All of these innovations address the same fundamental issue: as context length grows, KV cache memory requirements grow proportionally, making very long contexts impractical.&lt;/p&gt; &lt;h1&gt;Summary&lt;/h1&gt; &lt;p&gt;Today we uncovered one of the most important optimizations in modern language models. The KV cache is elegant in its simplicity: cache the keys and values for reuse, but skip the queries since they're only needed once.&lt;/p&gt; &lt;p&gt;However, the optimization comes at a cost. The KV cache requires substantial memory that grows with context length. This memory requirement becomes the bottleneck as contexts get longer. The cache solved computational redundancy but created a memory scaling challenge.This tradeoff explains many design decisions in modern language models. Researchers developed MQA, GQA, and other attention variants to address the memory problem.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prashant-Lakhera"&gt; /u/Prashant-Lakhera &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppjo5b/day_10_21_days_of_building_a_small_language_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppjo5b/day_10_21_days_of_building_a_small_language_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppjo5b/day_10_21_days_of_building_a_small_language_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T06:14:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppnrq5</id>
    <title>Benchmarking AI by making it play a 2D version of Portal! We're building a leaderboard of local LLMs and would love your help</title>
    <updated>2025-12-18T10:41:21+00:00</updated>
    <author>
      <name>/u/Jaxkr</name>
      <uri>https://old.reddit.com/user/Jaxkr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppnrq5/benchmarking_ai_by_making_it_play_a_2d_version_of/"&gt; &lt;img alt="Benchmarking AI by making it play a 2D version of Portal! We're building a leaderboard of local LLMs and would love your help" src="https://external-preview.redd.it/eGllMzM4YTd4eDdnMaGojmT3bbZo8yY3-KaCvnapuLu-He8EPgF2CzzXIlwS.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0c4f279fec3d9815a7b1d416d0073d5b754815d0" title="Benchmarking AI by making it play a 2D version of Portal! We're building a leaderboard of local LLMs and would love your help" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;! We are working on an open source, multiplayer game engine for building environments to train+evaluate AI.&lt;/p&gt; &lt;p&gt;Right now we've mostly focused on testing frontier models, but we want to get the local LLM community involved and benchmark smaller models on these gameplay tasks.&lt;/p&gt; &lt;p&gt;If that sounds interesting to you, check us out at &lt;a href="https://github.com/WorldQL/worldql"&gt;https://github.com/WorldQL/worldql&lt;/a&gt; or &lt;a href="https://discord.gg/nPWVJzZFnP"&gt;join our Discord&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;We'd appreciate a star and if you are into running and finetuning models, we'd love your help!&lt;/p&gt; &lt;p&gt;We want to build open source benchmarks and RL environments that are just as good as what the big labs have üòé&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Jaxkr"&gt; /u/Jaxkr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/1n6etx97xx7g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppnrq5/benchmarking_ai_by_making_it_play_a_2d_version_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppnrq5/benchmarking_ai_by_making_it_play_a_2d_version_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T10:41:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppqp83</id>
    <title>memory systems benchmarks seem way inflated, anyone else notice this?</title>
    <updated>2025-12-18T13:23:23+00:00</updated>
    <author>
      <name>/u/FeelingWatercress871</name>
      <uri>https://old.reddit.com/user/FeelingWatercress871</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;been trying to add memory to my local llama setup and all these memory systems claim crazy good numbers but when i actually test them the results are trash.&lt;/p&gt; &lt;p&gt;started with mem0 cause everyone talks about it. their website says 80%+ accuracy but when i hooked it up to my local setup i got like 64%. thought maybe i screwed up the integration so i spent weeks debugging. turns out their marketing numbers use some special evaluation setup thats not available in their actual api.&lt;/p&gt; &lt;p&gt;tried zep next. same bs - they claim 85% but i got 72%. their github has evaluation code but it uses old api versions and some preprocessing steps that arent documented anywhere.&lt;/p&gt; &lt;p&gt;getting pretty annoyed at this point so i decided to test a bunch more to see if everyone is just making up numbers:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;System &lt;/th&gt; &lt;th align="left"&gt;Their Claims&lt;/th&gt; &lt;th align="left"&gt;What I Got&lt;/th&gt; &lt;th align="left"&gt;Gap &lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Zep &lt;/td&gt; &lt;td align="left"&gt;~85% &lt;/td&gt; &lt;td align="left"&gt;72% &lt;/td&gt; &lt;td align="left"&gt;-13%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Mem0 &lt;/td&gt; &lt;td align="left"&gt;~80% &lt;/td&gt; &lt;td align="left"&gt;64% &lt;/td&gt; &lt;td align="left"&gt;-16%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MemGPT &lt;/td&gt; &lt;td align="left"&gt;~85% &lt;/td&gt; &lt;td align="left"&gt;70% &lt;/td&gt; &lt;td align="left"&gt;-15%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;gaps are huge. either im doing something really wrong or these companies are just inflating their numbers for marketing.&lt;/p&gt; &lt;p&gt;stuff i noticed while testing:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;most use private test data so you cant verify their claims&lt;/li&gt; &lt;li&gt;when they do share evaluation code its usually broken or uses old apis&lt;/li&gt; &lt;li&gt;&amp;quot;fair comparison&amp;quot; usually means they optimized everything for their own system&lt;/li&gt; &lt;li&gt;temporal stuff (remembering things from weeks ago) is universally terrible but nobody mentions this&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;tried to keep my testing fair. used the same dataset for all systems, same local llama model (llama 3.1 8b) for generating answers, same scoring method. still got way lower numbers than what they advertise.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# basic test loop i used for question in test_questions: memories = memory_system.search(question, user_id=&amp;quot;test_user&amp;quot;) context = format_context(memories) answer = local_llm.generate(question, context) score = check_answer_quality(answer, expected_answer) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;honestly starting to think this whole memory system space is just marketing hype. like everyone just slaps &amp;quot;AI memory&amp;quot; on their rag implementation and calls it revolutionary.&lt;/p&gt; &lt;p&gt;did find one open source project (github.com/EverMind-AI/EverMemOS) that actually tests multiple systems on the same benchmarks. their setup looks way more complex than what im doing but at least they seem honest about the results. they get higher numbers for their own system but also show that other systems perform closer to what i found.&lt;/p&gt; &lt;p&gt;am i missing something obvious or are these benchmark numbers just complete bs?&lt;/p&gt; &lt;p&gt;running everything locally with:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;llama 3.1 8b q4_k_m&lt;/li&gt; &lt;li&gt;32gb ram, rtx 4090&lt;/li&gt; &lt;li&gt;ubuntu 22.04&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;really want to get memory working well but hard to know which direction to go when all the marketing claims seem fake.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FeelingWatercress871"&gt; /u/FeelingWatercress871 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppqp83/memory_systems_benchmarks_seem_way_inflated/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppqp83/memory_systems_benchmarks_seem_way_inflated/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppqp83/memory_systems_benchmarks_seem_way_inflated/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T13:23:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppnoma</id>
    <title>Qwen3-Coder-REAP mxfp4 quant with custom imatrix dataset</title>
    <updated>2025-12-18T10:36:00+00:00</updated>
    <author>
      <name>/u/spectralyst</name>
      <uri>https://old.reddit.com/user/spectralyst</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just posted my first model on huggingface.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/spectralyst/Qwen3-Coder-REAP-25B-A3B-MXFP4_MOE-GGUF"&gt;spectralyst/Qwen3-Coder-REAP-25B-A3B-MXFP4_MOE-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It's a quant of cerebra's REAP of Qwen3-Coder-30B inspired by the original mxfp4 quant by &lt;a href="https://huggingface.co/noctrex/Qwen3-Coder-REAP-25B-A3B-MXFP4_MOE-GGUF"&gt;noctrex&lt;/a&gt; adding more C/C++ queries to the imatrix dataset while reducing the overall amount of code in the set and adding a bit of math queries to aid with math-based code prompts. The idea is to provide a more balanced calibration with greater emphasis on low-level coding.&lt;/p&gt; &lt;p&gt;From my limited experience, these mxfp4 quants of Qwen3-Coder-REAP-25B are the best coding models that will fit in 16 GB VRAM, although with only 16-24K context. Inference is very fast on Blackwell. Hoping this can prove useful for agentic FIM type stuff.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/spectralyst"&gt; /u/spectralyst &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppnoma/qwen3coderreap_mxfp4_quant_with_custom_imatrix/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppnoma/qwen3coderreap_mxfp4_quant_with_custom_imatrix/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppnoma/qwen3coderreap_mxfp4_quant_with_custom_imatrix/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T10:36:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppntz9</id>
    <title>GLM-V GGUF is out!</title>
    <updated>2025-12-18T10:45:27+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppntz9/glmv_gguf_is_out/"&gt; &lt;img alt="GLM-V GGUF is out!" src="https://b.thumbs.redditmedia.com/FZv8pFFPpwEa4qKxoMpu3mE3J-5QIWLQlQCBViK_yvg.jpg" title="GLM-V GGUF is out!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/collections/ggml-org/glm-v"&gt;https://huggingface.co/collections/ggml-org/glm-v&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/klip0rudzx7g1.png?width=3840&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=50865e4b0f1c5479683b40e8dc6fe68df02f03db"&gt;https://preview.redd.it/klip0rudzx7g1.png?width=3840&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=50865e4b0f1c5479683b40e8dc6fe68df02f03db&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppntz9/glmv_gguf_is_out/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppntz9/glmv_gguf_is_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppntz9/glmv_gguf_is_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T10:45:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppu4lc</id>
    <title>Fine-tuning Qwen3 at home to respond to any prompt with a dad joke</title>
    <updated>2025-12-18T15:48:58+00:00</updated>
    <author>
      <name>/u/InvadersMustLive</name>
      <uri>https://old.reddit.com/user/InvadersMustLive</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppu4lc/finetuning_qwen3_at_home_to_respond_to_any_prompt/"&gt; &lt;img alt="Fine-tuning Qwen3 at home to respond to any prompt with a dad joke" src="https://external-preview.redd.it/aeJXUJD-EG13fwr7w155noLxr7JTSfAKwf9XG0w-u3s.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9431c18c37e750b69f2ab16532111dd97d789f41" title="Fine-tuning Qwen3 at home to respond to any prompt with a dad joke" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InvadersMustLive"&gt; /u/InvadersMustLive &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://nixiesearch.substack.com/p/fine-tuning-qwen3-at-home-to-respond"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppu4lc/finetuning_qwen3_at_home_to_respond_to_any_prompt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppu4lc/finetuning_qwen3_at_home_to_respond_to_any_prompt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T15:48:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1pp6jhq</id>
    <title>Hey, LocalLLaMa. We need to talk...</title>
    <updated>2025-12-17T20:04:07+00:00</updated>
    <author>
      <name>/u/Eisenstein</name>
      <uri>https://old.reddit.com/user/Eisenstein</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I look on the front page and I see people who have spent time and effort to make something, and they share it willingly. They are getting no upvotes.&lt;/p&gt; &lt;p&gt;We are here because we are &lt;em&gt;local&lt;/em&gt; and we are &lt;em&gt;open source&lt;/em&gt;. Those things &lt;em&gt;depend on people who give us things&lt;/em&gt;, and they don't ask for anything in return, but they &lt;em&gt;need&lt;/em&gt; something in return or they will stop.&lt;/p&gt; &lt;p&gt;Pop your head into the smaller posts where someone is showing work they have done. Give honest and constructive feedback. UPVOTE IT.&lt;/p&gt; &lt;p&gt;The project may be terrible -- encourage them to grow by telling them how they can make it better. &lt;/p&gt; &lt;p&gt;The project may be awesome. They would love to hear how awesome it is. But if you use it, then they would love 100 times more to hear how you use it and how it helps you.&lt;/p&gt; &lt;p&gt;Engage with the people who share their things, and not just with the entertainment. &lt;/p&gt; &lt;p&gt;It take so little effort but it makes so much difference.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Eisenstein"&gt; /u/Eisenstein &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp6jhq/hey_localllama_we_need_to_talk/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp6jhq/hey_localllama_we_need_to_talk/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pp6jhq/hey_localllama_we_need_to_talk/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T20:04:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppnmca</id>
    <title>AI is great at answers, but terrible at uncertainty and that‚Äôs a bigger problem than hallucinations</title>
    <updated>2025-12-18T10:32:06+00:00</updated>
    <author>
      <name>/u/Mediocre_Common_4126</name>
      <uri>https://old.reddit.com/user/Mediocre_Common_4126</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Most of the criticism around LLMs focuses on hallucinations, wrong facts, or confidence issues but I think the deeper problem is AI is optimized to sound &lt;em&gt;certain&lt;/em&gt;&lt;/p&gt; &lt;p&gt;In real work, the hardest moments are not when you need an answer. They‚Äôre when you don‚Äôt even know what the right question is yet&lt;/p&gt; &lt;p&gt;The messy parts: half-formed thoughts + contradictory signals + ‚Äúthis feels wrong but I don‚Äôt know why‚Äù backtracking changing your mind mid-way&lt;/p&gt; &lt;p&gt;Humans spend a huge amount of time operating in uncertainty, we explore, we reframe, we circle around the problem&lt;/p&gt; &lt;p&gt;Most training data skips that phase entirely, we feed models clean prompts and polished conclusions, then expect them to handle ambiguity well&lt;/p&gt; &lt;p&gt;That‚Äôs why LLMs often feel impressive but fragile, they jump to conclusions too fast, they don‚Äôt linger in confusion, they optimize for closure, not exploration.&lt;/p&gt; &lt;p&gt;What‚Äôs interesting is that the best human collaborators are the opposite. They slow you down, they ask annoying clarifying questions, they surface blind spots instead of hiding them behind confident language&lt;/p&gt; &lt;p&gt;This made me rethink how AI tools should be built, less ‚Äúgive me the answer‚Äù, more ‚Äúhelp me think without collapsing the space too early‚Äù&lt;/p&gt; &lt;p&gt;Interesting if others have noticed this too. Especially people building tools on top of LLMs or using them for real decision making&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mediocre_Common_4126"&gt; /u/Mediocre_Common_4126 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppnmca/ai_is_great_at_answers_but_terrible_at/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppnmca/ai_is_great_at_answers_but_terrible_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppnmca/ai_is_great_at_answers_but_terrible_at/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T10:32:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1pper90</id>
    <title>MiraTTS: High quality and fast TTS model</title>
    <updated>2025-12-18T01:55:55+00:00</updated>
    <author>
      <name>/u/SplitNice1982</name>
      <uri>https://old.reddit.com/user/SplitNice1982</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;MiraTTS&lt;/strong&gt; is a high quality LLM based TTS finetune that can generate audio at &lt;strong&gt;100x&lt;/strong&gt; realtime and generate realistic and clear 48khz speech! I heavily optimized it using Lmdeploy and used &lt;a href="https://github.com/ysharma3501/FlashSR"&gt;FlashSR&lt;/a&gt; to enhance the audio.&lt;/p&gt; &lt;h1&gt;Benefits of this repo&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Incredibly fast: As stated before, over &lt;strong&gt;100x&lt;/strong&gt; realtime!&lt;/li&gt; &lt;li&gt;High quality: Generates realistic and 48khz speech, &lt;strong&gt;much&lt;/strong&gt; clearer then most TTS models and it‚Äôs base model.&lt;/li&gt; &lt;li&gt;Memory efficient: Works with even 6gb vram gpus!&lt;/li&gt; &lt;li&gt;Low latency: Possible latency low as &lt;strong&gt;150ms&lt;/strong&gt;, I have not released code for streaming yet but will release soon.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Basic multilingual versions are already supported, I just need to clean up code. Multispeaker is still in progress, but should come soon. If you have any other issues, I will be happy to fix them.&lt;/p&gt; &lt;p&gt;Github link: &lt;a href="https://github.com/ysharma3501/MiraTTS"&gt;https://github.com/ysharma3501/MiraTTS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Model link: &lt;a href="https://huggingface.co/YatharthS/MiraTTS"&gt;https://huggingface.co/YatharthS/MiraTTS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog explaining llm tts models: &lt;a href="https://huggingface.co/blog/YatharthS/llm-tts-models"&gt;https://huggingface.co/blog/YatharthS/llm-tts-models&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Stars/Likes would be appreciated very much, thank you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SplitNice1982"&gt; /u/SplitNice1982 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pper90/miratts_high_quality_and_fast_tts_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pper90/miratts_high_quality_and_fast_tts_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pper90/miratts_high_quality_and_fast_tts_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T01:55:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1poy0lb</id>
    <title>Apple introduces SHARP, a model that generates a photorealistic 3D Gaussian representation from a single image in seconds.</title>
    <updated>2025-12-17T14:33:13+00:00</updated>
    <author>
      <name>/u/themixtergames</name>
      <uri>https://old.reddit.com/user/themixtergames</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1poy0lb/apple_introduces_sharp_a_model_that_generates_a/"&gt; &lt;img alt="Apple introduces SHARP, a model that generates a photorealistic 3D Gaussian representation from a single image in seconds." src="https://external-preview.redd.it/YWpkODI1NDF4cjdnMbxNGAI-puPRf-AP3cgrLxlreCeM4kV742La4OIIHHvj.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a1dded6d1cdcc956e0916d9926400982637f4d7c" title="Apple introduces SHARP, a model that generates a photorealistic 3D Gaussian representation from a single image in seconds." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GitHub: &lt;a href="https://github.com/apple/ml-sharp"&gt;https://github.com/apple/ml-sharp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Paper: &lt;a href="https://arxiv.org/abs/2512.10685"&gt;https://arxiv.org/abs/2512.10685&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/themixtergames"&gt; /u/themixtergames &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/l2mp7b31xr7g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1poy0lb/apple_introduces_sharp_a_model_that_generates_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1poy0lb/apple_introduces_sharp_a_model_that_generates_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T14:33:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppstef</id>
    <title>Thoughts on recent small (under 20B) models</title>
    <updated>2025-12-18T14:55:45+00:00</updated>
    <author>
      <name>/u/surubel</name>
      <uri>https://old.reddit.com/user/surubel</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Recently we're been graced with quite a few small (under 20B) models and I've tried most of them.&lt;/p&gt; &lt;p&gt;The initial benchmarks seemed a bit too good to be true, but I've tried them regardless. &lt;/p&gt; &lt;ul&gt; &lt;li&gt;RNJ-1: this one had probably the most &amp;quot;honest&amp;quot; benchmark results. About as good as QWEN3 8B, which seems fair from my limited usage. &lt;/li&gt; &lt;li&gt;GLM 4.6v Flash: even after the latest llama.cpp update and Unsloth quantization I still have mixed feelings. Can't get it to think in English, but produces decent results. Either there are still issues with llama.cpp / quantization or it's a bit benchmaxxed&lt;/li&gt; &lt;li&gt;Ministral 3 14B: solid vision capabilities, but tends to overthink a lot. Occasionally messes up tool calls. A bit unreliable.&lt;/li&gt; &lt;li&gt;Nemotron cascade 14B. Similar to Ministral 3 14B tends to overthink a lot. Although it has great coding benchmarks, I couldn't get good results out of it. GPT OSS 20B and QWEN3 8B VL seem to give better results. This was the most underwhelming for me.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Did anyone get different results from these models? Am I missing something?&lt;/p&gt; &lt;p&gt;Seems like GPT OSS 20B and QWEN3 8B VL are still the most reliable small models, at least for me.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/surubel"&gt; /u/surubel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppstef/thoughts_on_recent_small_under_20b_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppstef/thoughts_on_recent_small_under_20b_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppstef/thoughts_on_recent_small_under_20b_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T14:55:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1pp8vo4</id>
    <title>Nvidia plans heavy cuts to GPU supply in early 2026</title>
    <updated>2025-12-17T21:37:13+00:00</updated>
    <author>
      <name>/u/HumanDrone8721</name>
      <uri>https://old.reddit.com/user/HumanDrone8721</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HumanDrone8721"&gt; /u/HumanDrone8721 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://overclock3d.net/news/gpu-displays/nvidia-plans-heavy-cuts-to-gpu-supply-in-early-2026/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp8vo4/nvidia_plans_heavy_cuts_to_gpu_supply_in_early/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pp8vo4/nvidia_plans_heavy_cuts_to_gpu_supply_in_early/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T21:37:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppq8pi</id>
    <title>Z-Image is now the default image model on HuggingChat</title>
    <updated>2025-12-18T13:01:20+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppq8pi/zimage_is_now_the_default_image_model_on/"&gt; &lt;img alt="Z-Image is now the default image model on HuggingChat" src="https://b.thumbs.redditmedia.com/q04f8-Hq7gSnGXdIq-IW8V70b-2l8sOF10WS2JF_Kks.jpg" title="Z-Image is now the default image model on HuggingChat" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From Victor M (Hugging Face) on ùïè: &lt;a href="https://x.com/victormustar/status/2001629770329858391?s=20"&gt;https://x.com/victormustar/status/2001629770329858391&lt;/a&gt;&lt;br /&gt; HuggingChat: &lt;a href="https://huggingface.co/chat/"&gt;https://huggingface.co/chat/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ppq8pi"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppq8pi/zimage_is_now_the_default_image_model_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppq8pi/zimage_is_now_the_default_image_model_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T13:01:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppm9xm</id>
    <title>NVIDIA Publishes Complete Evaluation Recipe for Nemotron 3 Nano</title>
    <updated>2025-12-18T09:03:01+00:00</updated>
    <author>
      <name>/u/paf1138</name>
      <uri>https://old.reddit.com/user/paf1138</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppm9xm/nvidia_publishes_complete_evaluation_recipe_for/"&gt; &lt;img alt="NVIDIA Publishes Complete Evaluation Recipe for Nemotron 3 Nano" src="https://external-preview.redd.it/i9rG1D6xcH_2B9JTT5Ak5wKM4ExK483hNq6oNeOkRNo.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aa9a037e77932298ed68f09b93c42491dd8ab8e0" title="NVIDIA Publishes Complete Evaluation Recipe for Nemotron 3 Nano" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paf1138"&gt; /u/paf1138 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/blog/nvidia/nemotron-3-nano-evaluation-recipe"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppm9xm/nvidia_publishes_complete_evaluation_recipe_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppm9xm/nvidia_publishes_complete_evaluation_recipe_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T09:03:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppongx</id>
    <title>Fast on-device Speech-to-text for Home Assistant (open source)</title>
    <updated>2025-12-18T11:34:57+00:00</updated>
    <author>
      <name>/u/banafo</name>
      <uri>https://old.reddit.com/user/banafo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppongx/fast_ondevice_speechtotext_for_home_assistant/"&gt; &lt;img alt="Fast on-device Speech-to-text for Home Assistant (open source)" src="https://external-preview.redd.it/6PRNLd3TFMw1DCfYP7618_nVHzwQRPRrDRjMqQg7XGU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cff7a166c2a85ced6d24604f32dc307cf599fedf" title="Fast on-device Speech-to-text for Home Assistant (open source)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We just released &lt;a href="https://github.com/orgs/kroko-ai/repositories"&gt;kroko-onnx-home-assistant &lt;/a&gt; is a &lt;strong&gt;local&lt;/strong&gt; streaming STT pipeline for home assistant.&lt;/p&gt; &lt;p&gt;It's currently just a fork of the excellent &lt;a href="https://github.com/ptbsare/sherpa-onnx-tts-stt"&gt;https://github.com/ptbsare/sherpa-onnx-tts-stt&lt;/a&gt; with support for our models added, hopefully it will be accepted in the main project. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Highlights:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;High quality&lt;/li&gt; &lt;li&gt;Real streaming (partial results, low latency)&lt;/li&gt; &lt;li&gt;100% local &amp;amp; privacy-first&lt;/li&gt; &lt;li&gt;optimized for fast CPU inference, even in low resources raspberry pi's&lt;/li&gt; &lt;li&gt;Does not require additional VAD&lt;/li&gt; &lt;li&gt;Home Assistant integration&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Repo:&lt;br /&gt; [&lt;a href="https://github.com/kroko-ai/kroko-onnx-home-assistant%5D()"&gt;https://github.com/kroko-ai/kroko-onnx-home-assistant]()&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you want to test the model quality before installing: the huggingface models running in the browser is the easiest way: &lt;a href="https://huggingface.co/spaces/Banafo/Kroko-Streaming-ASR-Wasm"&gt;https://huggingface.co/spaces/Banafo/Kroko-Streaming-ASR-Wasm&lt;/a&gt; &lt;/p&gt; &lt;p&gt;A big thanks to:&lt;br /&gt; - NaggingDaivy on discord, for the assistance.&lt;br /&gt; - the sherpa-onnx-tts-stt team for adding support for streaming models in record time.&lt;/p&gt; &lt;p&gt;Want us to integrate with your favorite open source project ? Contact us on discord:&lt;br /&gt; &lt;a href="https://discord.gg/TEbfnC7b"&gt;https://discord.gg/TEbfnC7b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Some releases you may have missed:&lt;br /&gt; - Freewitch Module: &lt;a href="https://github.com/kroko-ai/integration-demos/tree/master/asterisk-kroko"&gt;https://github.com/kroko-ai/integration-demos/tree/master/asterisk-kroko&lt;/a&gt;&lt;br /&gt; - Asterisk Module: &lt;a href="https://github.com/kroko-ai/integration-demos/tree/master/asterisk-kroko"&gt;https://github.com/kroko-ai/integration-demos/tree/master/asterisk-kroko&lt;/a&gt;&lt;br /&gt; - Full Asterisk based voicebot running with Kroko streaming models: &lt;a href="https://github.com/hkjarral/Asterisk-AI-Voice-Agent"&gt;https://github.com/hkjarral/Asterisk-AI-Voice-Agent&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We are still working on the main models, code and documentation as well, but held up a bit with urgent paid work deadlines, more coming there soon too.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/banafo"&gt; /u/banafo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/kroko-ai/kroko-onnx-home-assistant"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppongx/fast_ondevice_speechtotext_for_home_assistant/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppongx/fast_ondevice_speechtotext_for_home_assistant/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T11:34:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppt1xb</id>
    <title>Meta released Map-anything-v1: A universal transformer model for metric 3D reconstruction</title>
    <updated>2025-12-18T15:05:22+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppt1xb/meta_released_mapanythingv1_a_universal/"&gt; &lt;img alt="Meta released Map-anything-v1: A universal transformer model for metric 3D reconstruction" src="https://preview.redd.it/go7lager9z7g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=96b4f63fa1cdd2136e6c82f35c609cc6cc1ead9c" title="Meta released Map-anything-v1: A universal transformer model for metric 3D reconstruction" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hugging face: &lt;a href="https://huggingface.co/facebook/map-anything-v1"&gt;https://huggingface.co/facebook/map-anything-v1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It supports 12+ tasks like multi-view stereo and SfM in a single feed-forward pass&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/go7lager9z7g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppt1xb/meta_released_mapanythingv1_a_universal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppt1xb/meta_released_mapanythingv1_a_universal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T15:05:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1pniwfj</id>
    <title>Ai2 Open Modeling AMA ft researchers from the Molmo and Olmo teams.</title>
    <updated>2025-12-15T21:02:55+00:00</updated>
    <author>
      <name>/u/ai2_official</name>
      <uri>https://old.reddit.com/user/ai2_official</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pniwfj/ai2_open_modeling_ama_ft_researchers_from_the/"&gt; &lt;img alt="Ai2 Open Modeling AMA ft researchers from the Molmo and Olmo teams." src="https://b.thumbs.redditmedia.com/bsv34WIHZXC49Az9mFES5lSIAtaQ2CuLZJ4dCaLxsEY.jpg" title="Ai2 Open Modeling AMA ft researchers from the Molmo and Olmo teams." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;! We‚Äôre researchers and engineers from Ai2, the nonprofit AI lab. We recently announced:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Molmo 2&lt;/strong&gt;‚Äîopen multimodal models for video + images that can return grounded answers (pixel coordinates + timestamps), trained with open datasets&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Olmo 3&lt;/strong&gt;‚Äîa family of fully open language models (7B‚Äì32B) with Base/Instruct/Thinking variants, long‚Äëcontext support, open training recipes &amp;amp; checkpoints&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Ask us anything about local inference, training mixes &amp;amp; our truly open approach, long‚Äëcontext, grounded video QA/tracking, and real‚Äëworld deployment.&lt;/p&gt; &lt;p&gt;Participating in the AMA:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Molmo 2 researchers:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Ranjay Krishna ( &lt;a href="/u/ranjaykrishna"&gt;u/ranjaykrishna&lt;/a&gt; )&lt;/li&gt; &lt;li&gt;Zixian Ma ( &lt;a href="/u/Frequent_Rooster2980"&gt;u/Frequent_Rooster2980&lt;/a&gt; )&lt;/li&gt; &lt;li&gt;Chris Clark ( &lt;a href="/u/mostly_reasonable"&gt;u/mostly_reasonable&lt;/a&gt; )&lt;/li&gt; &lt;li&gt;Jieyu Zhang ( &lt;a href="/u/Jealous_Programmer51"&gt;u/Jealous_Programmer51&lt;/a&gt; )&lt;/li&gt; &lt;li&gt;Rohun Tripathi ( &lt;a href="/u/darkerWind"&gt;u/darkerWind&lt;/a&gt; )&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Olmo 3 researchers:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Kyle Lo ( &lt;a href="/u/klstats"&gt;u/klstats&lt;/a&gt; )&lt;/li&gt; &lt;li&gt;Allyson Ettinger ( &lt;a href="/u/aeclang"&gt;u/aeclang&lt;/a&gt; )&lt;/li&gt; &lt;li&gt;Finbarr Timbers ( &lt;a href="/u/fnbr"&gt;u/fnbr&lt;/a&gt; )&lt;/li&gt; &lt;li&gt;Faeze Brahman ( &lt;a href="/u/faebrhn"&gt;u/faebrhn&lt;/a&gt; )&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We‚Äôll be live from &lt;strong&gt;1pm&lt;/strong&gt; to &lt;strong&gt;2pm PST.&lt;/strong&gt; Read up on our latest releases below, and feel welcome to jump in anytime!&lt;/p&gt; &lt;ul&gt; &lt;li&gt;‚ñ∂Ô∏è &lt;strong&gt;Try in the Playground:&lt;/strong&gt; &lt;a href="https://playground.allenai.org"&gt;https://playground.allenai.org&lt;/a&gt;&lt;/li&gt; &lt;li&gt;‚¨áÔ∏è &lt;strong&gt;Download&lt;/strong&gt;: &lt;a href="https://huggingface.co/collections/allenai/molmo2"&gt;https://huggingface.co/collections/allenai/molmo2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;üìù &lt;strong&gt;Blog&lt;/strong&gt;: &lt;a href="https://allenai.org/blog/molmo2"&gt;https://allenai.org/blog/molmo2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;üìÑReport: &lt;a href="https://allenai.org/papers/molmo2"&gt;https://allenai.org/papers/molmo2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;üíª &lt;strong&gt;API coming soon&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;ü´Ü PROOF:&lt;/strong&gt; &lt;a href="https://x.com/allen_ai/status/2000692253606514828"&gt;https://x.com/allen_ai/status/2000692253606514828&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Join us on Reddit&lt;/strong&gt; &lt;a href="/r/allenai"&gt;r/allenai&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;Join Ai2 on Discord:&lt;/strong&gt; &lt;a href="https://discord.gg/6vWDHyTCQV"&gt;https://discord.gg/6vWDHyTCQV&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fxw1g2fcmf7g1.jpg?width=1080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=009a9377edfefefc5efd52db0af81b807b9971b8"&gt;https://preview.redd.it/fxw1g2fcmf7g1.jpg?width=1080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=009a9377edfefefc5efd52db0af81b807b9971b8&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Thank you everyone for the kind words and great questions! This AMA has ended as of 2pm PST (5pm EST) on Dec. 16.&lt;/p&gt; &lt;p&gt;&lt;a href="https://discord.gg/6vWDHyTCQV"&gt;Join Ai2 on Discord&lt;/a&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ai2_official"&gt; /u/ai2_official &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pniwfj/ai2_open_modeling_ama_ft_researchers_from_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pniwfj/ai2_open_modeling_ama_ft_researchers_from_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pniwfj/ai2_open_modeling_ama_ft_researchers_from_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T21:02:55+00:00</published>
  </entry>
</feed>
