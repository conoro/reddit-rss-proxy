<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-17T15:25:04+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1m1jd2r</id>
    <title>Anyone having luck with Hunyuan 80B A13B?</title>
    <updated>2025-07-16T17:47:54+00:00</updated>
    <author>
      <name>/u/Admirable-Star7088</name>
      <uri>https://old.reddit.com/user/Admirable-Star7088</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/unsloth/Hunyuan-A13B-Instruct-GGUF"&gt;Hunyuan-80B-A13B&lt;/a&gt; looked really cool on paper, I hoped it would be the &amp;quot;large equivalent&amp;quot; of the excellent Qwen3 30B A3B. According to the official &lt;a href="https://huggingface.co/tencent/Hunyuan-A13B-Instruct"&gt;Hugging Face page&lt;/a&gt;, it's &lt;strong&gt;compact&lt;/strong&gt; yet &lt;strong&gt;powerful&lt;/strong&gt;, comparable to much larger models:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;With only 13 billion active parameters (out of a total of 80 billion), the model delivers competitive performance on a wide range of benchmark tasks, rivaling much larger models.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;I tried Unsloth's UD-Q5_K_XL quant with recommended sampler settings and in the latest version of LM Studio, and I'm getting pretty overall terrible results. I also tried UD-Q8_K_XL in case the model is very sensitive to quantization, but I'm still getting bad results.&lt;/p&gt; &lt;p&gt;For example, when I ask it about astronomy, it gets basic facts wrong, such as claiming that Mars is much larger than Earth and that Mars is closer to the sun than Earth (when in fact, it is the opposite: Earth is both larger and closer to the sun than Mars).&lt;/p&gt; &lt;p&gt;It also feels weak in creative writing, where it spouts a lot of nonsense that does not make much sense.&lt;/p&gt; &lt;p&gt;I really want this model to be good. I feel like (and hope) that the issue lies with my setup rather than the model itself. Might it still be buggy in llama.cpp? Is there a problem with the Jinja/chat template? Is the model particularly sensitive to incorrect sampler settings?&lt;/p&gt; &lt;p&gt;Is anyone else having better luck with this model?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Admirable-Star7088"&gt; /u/Admirable-Star7088 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1jd2r/anyone_having_luck_with_hunyuan_80b_a13b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1jd2r/anyone_having_luck_with_hunyuan_80b_a13b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m1jd2r/anyone_having_luck_with_hunyuan_80b_a13b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-16T17:47:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1m1s7w9</id>
    <title>Regency Bewildered is a stylistic persona imprint</title>
    <updated>2025-07-16T23:37:17+00:00</updated>
    <author>
      <name>/u/FPham</name>
      <uri>https://old.reddit.com/user/FPham</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1s7w9/regency_bewildered_is_a_stylistic_persona_imprint/"&gt; &lt;img alt="Regency Bewildered is a stylistic persona imprint" src="https://preview.redd.it/q83c3ppqnbdf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0d561de93c7787a194e41936510b46a3415edfc3" title="Regency Bewildered is a stylistic persona imprint" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;You, like most people, are probably scratching your head quizzically, asking yourself &amp;quot;Who is this doofus?&amp;quot;&lt;/p&gt; &lt;p&gt;It's me! With another &amp;quot;model&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/FPHam/Regency_Bewildered_12B_GGUF"&gt;https://huggingface.co/FPHam/Regency_Bewildered_12B_GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Regency Bewildered is a stylistic persona imprint.&lt;/p&gt; &lt;p&gt;This is not a general-purpose instruction model; it is a very specific and somewhat eccentric experiment in imprinting a historical persona onto an LLM. The entire multi-step creation process, from the dataset preparation to the final, slightly unhinged result, is documented step-by-step in my upcoming book about LoRA training (currently more than 600 pages!).&lt;/p&gt; &lt;p&gt;What it does:&lt;/p&gt; &lt;p&gt;This model attempts to adopt the voice, knowledge, and limitations of a well-educated person living in the Regency/early Victorian era. It &amp;quot;steals&amp;quot; its primary literary style from Jane Austen's Pride and Prejudice but goes further by trying to reason and respond as if it has no knowledge of modern concepts.&lt;/p&gt; &lt;p&gt;Primary Goal - Linguistic purity&lt;/p&gt; &lt;p&gt;The main and primary goal was to achieve a perfect linguistic imprint of Jane Austen‚Äôs style and wit. Unlike what ChatGPT, Claude, or any other model typically call ‚ÄúJane Austen style‚Äù, which usually amounts to a sad parody full of clich√©s, this model is specifically designed to maintain stylistic accuracy. In my humble opinion (worth a nickel), it far exceeds what you‚Äôll get from the so-called big-name models.&lt;/p&gt; &lt;p&gt;Why &amp;quot;Bewildered&amp;quot;:&lt;/p&gt; &lt;p&gt;The model was deliberately trained using &amp;quot;recency bias&amp;quot; that forces it to interpret new information through the lens of its initial, archaic conditioning. When asked about modern topics like computers or AI, it often becomes genuinely perplexed, attempting to explain the unfamiliar concept using period-appropriate analogies (gears, levers, pneumatic tubes) or dismissing it with philosophical musings.&lt;/p&gt; &lt;p&gt;This makes it a fascinating, if not always practical, conversationalist.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FPham"&gt; /u/FPham &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/q83c3ppqnbdf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1s7w9/regency_bewildered_is_a_stylistic_persona_imprint/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m1s7w9/regency_bewildered_is_a_stylistic_persona_imprint/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-16T23:37:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1m27dyr</id>
    <title>Which model for local code assistant</title>
    <updated>2025-07-17T13:21:06+00:00</updated>
    <author>
      <name>/u/Wintlink-</name>
      <uri>https://old.reddit.com/user/Wintlink-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to build little coding assistant tool, but I was wondering what is the best models in your opinion for coding that I can run locally ?&lt;br /&gt; Thank you !&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wintlink-"&gt; /u/Wintlink- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m27dyr/which_model_for_local_code_assistant/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m27dyr/which_model_for_local_code_assistant/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m27dyr/which_model_for_local_code_assistant/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-17T13:21:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1m1myiq</id>
    <title>[Open-Source] self-hostable AI productivity agent using Qwen 3 (4B) - reads your apps, extracts tasks, runs them on autopilot</title>
    <updated>2025-07-16T20:03:07+00:00</updated>
    <author>
      <name>/u/therealkabeer</name>
      <uri>https://old.reddit.com/user/therealkabeer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hey everyone!&lt;/p&gt; &lt;p&gt;we're currently building an open-source autopilot for maximising productivity. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; the idea is that users can connect their apps, AI will periodically read these apps for new context (like new emails, new calendar events, etc), extract action items from them, ask the user clarifying questions (if any), create plans for tackling tasks and after I approve these plans, the AI will go ahead and complete them.&lt;/p&gt; &lt;p&gt;basically, all users need to do is answer clarifying questions and approve plans, rather than having to open a chatbot, type a long prompt explaining what they want to get done, what the AI should read for context and so on.&lt;/p&gt; &lt;p&gt;If you want to know more about the project or self-host it, check out the repo here: &lt;a href="https://github.com/existence-master/Sentient"&gt;https://github.com/existence-master/Sentient&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Here are some of the features we've implemented:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;we were tired of chat interfaces and so we've made the entire app revolve around an &amp;quot;organizer&amp;quot; page where you can dump tasks, entries, or even general thoughts and the AI will manage it for you. the AI also writes to the organizer, allowing you to keep a track of everything its done, what info it needs or what tasks need to be approved&lt;/li&gt; &lt;li&gt;the AI can run on autopilot. it can periodically read my emails + calendar and extract action items and memories about me from there. action items get added to the organizer and become plans which eventually become tasks. memories are indexed in the memory pipeline. we want to add more context sources (apart from email and calendar) that the AI can read proactively&lt;/li&gt; &lt;li&gt;the memory pipeline allows the AI to learn about the user as time progresses. preferences, personal details and more are stored in the memory pipeline. &lt;/li&gt; &lt;li&gt;it works across a bunch of apps (such as Gmail, GCalendar, GDocs, GSheets, GSlides, GDrive, Notion, Slack, GitHub, etc.) It can also search the web, get up-to-date weather info, search for shopping items, prepare charts and graphs and more.&lt;/li&gt; &lt;li&gt;You can also schedule your tasks to run at a specific time or run as recurring workflows at defined intervals. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Some other nice-to-haves we've added are WhatsApp notifications (the AI can notify users of what its doing on WhatsApp), privacy filters (block certain keywords, email addresses, etc so that the AI will never process emails or calendar events you don't want it to)&lt;/p&gt; &lt;p&gt;the project is fully open-source and self-hostable using Docker&lt;/p&gt; &lt;p&gt;Some tech stuff:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Frontend: NextJS&lt;/li&gt; &lt;li&gt;Backend: Python&lt;/li&gt; &lt;li&gt;Agentic Framework: Qwen Agent &lt;/li&gt; &lt;li&gt;Model: Qwen 3 (4B) - this is a VERY impressive small model for tool calling&lt;/li&gt; &lt;li&gt;Integrations: Custom MCP servers built with FastMCP that wrap the APIs of a bunch of services into tools that the agents can use.&lt;/li&gt; &lt;li&gt;Others: Celery for task queue management with Redis, MongoDB as the database, Docker for containerization, etc.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'd greatly appreciate any feedback or ideas for improvements we can make.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/therealkabeer"&gt; /u/therealkabeer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1myiq/opensource_selfhostable_ai_productivity_agent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1myiq/opensource_selfhostable_ai_productivity_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m1myiq/opensource_selfhostable_ai_productivity_agent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-16T20:03:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1m25rnu</id>
    <title>How does Devstral Medium 2507 compare?</title>
    <updated>2025-07-17T12:05:50+00:00</updated>
    <author>
      <name>/u/z_3454_pfk</name>
      <uri>https://old.reddit.com/user/z_3454_pfk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone used this model? I‚Äôve heard it‚Äôs very good for tool calling but can‚Äôt any specifics on performance. Can anyone share their experiences?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/z_3454_pfk"&gt; /u/z_3454_pfk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m25rnu/how_does_devstral_medium_2507_compare/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m25rnu/how_does_devstral_medium_2507_compare/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m25rnu/how_does_devstral_medium_2507_compare/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-17T12:05:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1m246sn</id>
    <title>Are local LLMs on mobile still a gimmick?</title>
    <updated>2025-07-17T10:40:50+00:00</updated>
    <author>
      <name>/u/Individual-Dot5488</name>
      <uri>https://old.reddit.com/user/Individual-Dot5488</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I think some of these smaller models have become quite good - but seems like the main advantage of running them on mobile is privacy, not accuracy or utility. The thing is, I think most people (non-programmers) use ChatGPT for search, but adding search to a local LLM would kind of defeat the purpose of privacy. So I'm struggling to see whether this is something people actually want/need or is just a nice to have, and whether it ever will be something people need.&lt;/p&gt; &lt;p&gt;What would be a situation where you would switch from relying on ChatGPT or otherwise, to using local mobile chatbot app? Will there ever be a utility?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Individual-Dot5488"&gt; /u/Individual-Dot5488 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m246sn/are_local_llms_on_mobile_still_a_gimmick/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m246sn/are_local_llms_on_mobile_still_a_gimmick/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m246sn/are_local_llms_on_mobile_still_a_gimmick/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-17T10:40:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1m2981a</id>
    <title>UTCP Golang prototype</title>
    <updated>2025-07-17T14:36:33+00:00</updated>
    <author>
      <name>/u/Revolutionary_Sir140</name>
      <uri>https://old.reddit.com/user/Revolutionary_Sir140</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone, I've started to port utcp-python to golang&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Raezil/UTCP"&gt;https://github.com/Raezil/UTCP&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I've created working prototype right now. I need to implement grpc and mcp transports for now.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Revolutionary_Sir140"&gt; /u/Revolutionary_Sir140 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2981a/utcp_golang_prototype/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2981a/utcp_golang_prototype/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m2981a/utcp_golang_prototype/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-17T14:36:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1m1hixa</id>
    <title>Playing around with the design of my pet project - does this look decent or nah?</title>
    <updated>2025-07-16T16:39:57+00:00</updated>
    <author>
      <name>/u/RIPT1D3_Z</name>
      <uri>https://old.reddit.com/user/RIPT1D3_Z</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1hixa/playing_around_with_the_design_of_my_pet_project/"&gt; &lt;img alt="Playing around with the design of my pet project - does this look decent or nah?" src="https://a.thumbs.redditmedia.com/oijm0DyxoxFre_g3otVxcpGYCKM5xFhT79TCi5btsZ8.jpg" title="Playing around with the design of my pet project - does this look decent or nah?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I posted a showcase of my project recently, would be glad to hear opinions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RIPT1D3_Z"&gt; /u/RIPT1D3_Z &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1m1hixa"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1hixa/playing_around_with_the_design_of_my_pet_project/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m1hixa/playing_around_with_the_design_of_my_pet_project/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-16T16:39:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1m1w7vp</id>
    <title>How Different Are Closed Source Models' Architectures?</title>
    <updated>2025-07-17T02:46:00+00:00</updated>
    <author>
      <name>/u/simulated-souls</name>
      <uri>https://old.reddit.com/user/simulated-souls</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How do the architectures of closed models like GPT-4o, Gemini, and Claude compare to open-source ones? Do they have any secret sauce that open models don't?&lt;/p&gt; &lt;p&gt;Most of the best open-source models right now (Qwen, Gemma, DeepSeek, Kimi) use nearly the exact same architecture. In fact, the recent Kimi K2 uses the same model code as DeepSeek V3 and R1, with only a slightly different config. The only big outlier seems to be MiniMax with its linear attention. There are also state-space models like Jamba, but those haven't seen as much adoption.&lt;/p&gt; &lt;p&gt;I would think that Gemini has something special to enable its 1M token context (maybe something to do with Google's &lt;a href="https://arxiv.org/abs/2501.00663"&gt;Titans paper&lt;/a&gt;?). However, I haven't heard of 4o or Claude being any different from standard Mixture-of-Expert transformers.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/simulated-souls"&gt; /u/simulated-souls &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1w7vp/how_different_are_closed_source_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1w7vp/how_different_are_closed_source_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m1w7vp/how_different_are_closed_source_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-17T02:46:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1m26t9w</id>
    <title>QWEN3 Output &lt;think&gt;\n\n&lt;/think&gt;\n\n</title>
    <updated>2025-07-17T12:55:59+00:00</updated>
    <author>
      <name>/u/uber-linny</name>
      <uri>https://old.reddit.com/user/uber-linny</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When doing TTS using qwen , how do i stop the output &amp;lt;think&amp;gt;\n\n&amp;lt;/think&amp;gt;\n\n ? &lt;/p&gt; &lt;p&gt;even turning off think /no_think still has it. &lt;/p&gt; &lt;p&gt;currently in n8n , but i also saw it in anything LLM&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/uber-linny"&gt; /u/uber-linny &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m26t9w/qwen3_output_thinknnthinknn/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m26t9w/qwen3_output_thinknnthinknn/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m26t9w/qwen3_output_thinknnthinknn/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-17T12:55:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1m1foz1</id>
    <title>CUDA is coming to MLX</title>
    <updated>2025-07-16T15:31:43+00:00</updated>
    <author>
      <name>/u/mrfakename0</name>
      <uri>https://old.reddit.com/user/mrfakename0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1foz1/cuda_is_coming_to_mlx/"&gt; &lt;img alt="CUDA is coming to MLX" src="https://external-preview.redd.it/w8edStcv8JcRcgUOJ4-eZrp8x-ns7z_4bZz-mt8i8eE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9049533811e0bc40173ac835b90eb4f9943876f0" title="CUDA is coming to MLX" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looks like we will soon get CUDA support in MLX - this means that we‚Äôll be able to run MLX programs on both Apple Silicon and CUDA GPUs.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mrfakename0"&gt; /u/mrfakename0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ml-explore/mlx/pull/1983"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1foz1/cuda_is_coming_to_mlx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m1foz1/cuda_is_coming_to_mlx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-16T15:31:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1m285sn</id>
    <title>Help Deciding Between NVIDIA H200 (2x GPUs) vs NVIDIA L40S (8x GPUs) for Serving 24b-30b LLM to 50 Concurrent Users</title>
    <updated>2025-07-17T13:54:04+00:00</updated>
    <author>
      <name>/u/beratcmn</name>
      <uri>https://old.reddit.com/user/beratcmn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I'm looking to upgrade my hardware for serving a 24b to 30b language model (LLM) to around 50 concurrent users, and I'm trying to decide between two NVIDIA GPU configurations:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;NVIDIA H200 (2x GPUs)&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Dual GPU setup&lt;/li&gt; &lt;li&gt;141 VRAM per GPU (for a total of 282GB VRAM)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;NVIDIA L40S (8x GPUs)&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;8 GPUs in total&lt;/li&gt; &lt;li&gt;24GB VRAM per GPU (for a total of 192GB VRAM)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I‚Äôm leaning towards a setup that offers the best performance in terms of both memory bandwidth and raw computational power, as I‚Äôll be handling complex queries and large models. My primary concern is whether the 2x GPUs with more memory (H200) will be able to handle the 24b-30b LLM load better, or if I should opt for the L40S with more GPUs but less memory per GPU.&lt;/p&gt; &lt;p&gt;Has anyone had experience with serving large models on either of these setups, and which would you recommend for optimal performance with 50 concurrent users?&lt;/p&gt; &lt;p&gt;Appreciate any insights!&lt;/p&gt; &lt;p&gt;Edit: H200 VRAM&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/beratcmn"&gt; /u/beratcmn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m285sn/help_deciding_between_nvidia_h200_2x_gpus_vs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m285sn/help_deciding_between_nvidia_h200_2x_gpus_vs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m285sn/help_deciding_between_nvidia_h200_2x_gpus_vs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-17T13:54:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1m1h0fy</id>
    <title>Support for diffusion models (Dream 7B) has been merged into llama.cpp</title>
    <updated>2025-07-16T16:20:32+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1h0fy/support_for_diffusion_models_dream_7b_has_been/"&gt; &lt;img alt="Support for diffusion models (Dream 7B) has been merged into llama.cpp" src="https://external-preview.redd.it/OqAAbOs6fFLPZaNF0M6vIqHJqNLZwtArB7hBcX1IZ7M.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8971ce6047ae48ffddcf53ec22de6523ddaa226e" title="Support for diffusion models (Dream 7B) has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Diffusion models are a new kind of language model that generate text by denoising random noise step-by-step, instead of predicting tokens left to right like traditional LLMs.&lt;/p&gt; &lt;p&gt;This PR adds basic support for diffusion models, using Dream 7B instruct as base. DiffuCoder-7B is built on the same arch so it should be trivial to add after this.&lt;br /&gt; [...]&lt;br /&gt; &lt;strong&gt;Another cool/gimmicky thing is you can see the diffusion unfold&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;In a joint effort with Huawei Noah‚Äôs Ark Lab, we release &lt;strong&gt;Dream 7B&lt;/strong&gt; (Diffusion reasoning model), the most powerful open diffusion large language model to date.&lt;/p&gt; &lt;p&gt;In short, Dream 7B:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;consistently outperforms existing diffusion language models by a large margin;&lt;/li&gt; &lt;li&gt;matches or exceeds top-tier Autoregressive (AR) language models of similar size on the general, math, and coding abilities;&lt;/li&gt; &lt;li&gt;demonstrates strong planning ability and inference flexibility that naturally benefits from the diffusion modeling.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/14644"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1h0fy/support_for_diffusion_models_dream_7b_has_been/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m1h0fy/support_for_diffusion_models_dream_7b_has_been/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-16T16:20:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1m23efn</id>
    <title>ARGO - A Local-First, Offline AI Agent That Puts You in Control</title>
    <updated>2025-07-17T09:52:41+00:00</updated>
    <author>
      <name>/u/yushiqi</name>
      <uri>https://old.reddit.com/user/yushiqi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;We're building ARGO, an open-source AI Agent client focused on privacy, power, and ease of use. Our goal is to let everyone have their own exclusive super AI agent, without giving up control of their data.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; ARGO is a desktop client that lets you easily build and use AI agents that can think for themselves, plan, and execute complex tasks. It runs on Windows, Mac, and Linux, works completely offline, and keeps 100% of your data stored locally. It integrates with local models via Ollama and major API providers, has a powerful RAG for your own documents, and a built-in &amp;quot;Agent Factory&amp;quot; to create specialized assistants for any scenario.&lt;/p&gt; &lt;p&gt;You can check out the repo here: &lt;a href="https://github.com/xark-argo/argo"&gt;&lt;code&gt;https://github.com/xark-argo/argo&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We built ARGO because we believe you shouldn't have to choose between powerful AI and your privacy. Instead of being locked into a single cloud provider or worrying about where your data is going, ARGO gives you a single, secure, and controllable hub for all your AI agent needs. No registration, no configuration hell, just plug-and-play.&lt;/p&gt; &lt;p&gt;Here are some of the features we've implemented:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;üîí &lt;strong&gt;Local First, Privacy Above All:&lt;/strong&gt; ARGO supports full offline operation and stores 100% of your data on your local machine. It‚Äôs a native app for Windows, macOS, and Linux that you can use right away without any complex setup. Perfect for anyone who is privacy-conscious.&lt;/li&gt; &lt;li&gt;üöÄ &lt;strong&gt;A Task Engine That Actually Gets Things Done:&lt;/strong&gt; This isn't just a chatbot. ARGO uses a Multi-Agent engine that can autonomously understand your intent, break down complex tasks into steps, use tools, and generate a final report. You can even review and edit its plan in natural language before it starts.&lt;/li&gt; &lt;li&gt;‚öôÔ∏è &lt;strong&gt;Agent Factory:&lt;/strong&gt; You can visually build and customize your own dedicated agents. Need a travel planner, a research analyst, or a coding assistant? Just describe what you need, bind a model, add tools, and you‚Äôre good to go.&lt;/li&gt; &lt;li&gt;üì¶ &lt;strong&gt;Integrates Ollama and All Major Providers:&lt;/strong&gt; We made using local models dead simple. ARGO has one-click Ollama integration to download and manage local models without touching the command line. It also supports APIs from OpenAI, Claude, DeepSeek, and more, letting you seamlessly switch between local and API models to balance cost and performance.&lt;/li&gt; &lt;li&gt;üß© &lt;strong&gt;Your Own Local Knowledge Base (Agentic RAG):&lt;/strong&gt; Feed ARGO your local files, folders, or even websites to create a secure, private knowledge base. It can dynamically sync with a folder, so your agent's knowledge is always up-to-date. The Agentic mode intelligently breaks down complex questions to give more complete and reliable answers based on your documents.&lt;/li&gt; &lt;li&gt;üõ†Ô∏è &lt;strong&gt;Powerful, Extensible Toolset:&lt;/strong&gt; It comes with built-in tools like a web crawler, browser control, and local file management. It also supports custom tools via the MCP protocol, so you can easily integrate your own.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The project is fully open-source and self-hostable using Docker.&lt;/p&gt; &lt;p&gt;Getting started is easy:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Desktop App:&lt;/strong&gt; Just download the installer for your OS and you're done.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Docker:&lt;/strong&gt; We have one-line Docker commands to get you up and run.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;ARGO is still in the early stages of active development, so we'd greatly appreciate any feedback, ideas, or contributions you might have. Let us know what you think!&lt;/p&gt; &lt;p&gt;If you are interested in ARGO, give us a star üåü on GitHub to follow our progress!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yushiqi"&gt; /u/yushiqi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m23efn/argo_a_localfirst_offline_ai_agent_that_puts_you/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m23efn/argo_a_localfirst_offline_ai_agent_that_puts_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m23efn/argo_a_localfirst_offline_ai_agent_that_puts_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-17T09:52:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1m28r3c</id>
    <title>AI devs in NYC ‚Äî heads up about the RAISE Act</title>
    <updated>2025-07-17T14:17:41+00:00</updated>
    <author>
      <name>/u/AI_Alliance</name>
      <uri>https://old.reddit.com/user/AI_Alliance</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyone in the NYC AI dev space paying attention to the &lt;strong&gt;RAISE Act&lt;/strong&gt;? It‚Äôs a new bill that could shape how AI systems get built and deployed‚Äîespecially open-source stuff.&lt;/p&gt; &lt;p&gt;I‚Äôm attending a virtual meetup today (July 17 @ 12PM ET) to learn more. If you‚Äôre working on agents, LLM stacks, or tool-use pipelines, this might be a good convo to drop in on.&lt;/p&gt; &lt;p&gt;Details + free registration: &lt;a href="https://events.thealliance.ai/how-the-raise-act-affects-you"&gt;https://events.thealliance.ai/how-the-raise-act-affects-you&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hoping it‚Äôll clarify what counts as ‚Äúhigh-risk‚Äù and what role open devs can play in shaping the policy. Might be useful if you're worried about future liability or compliance headache&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AI_Alliance"&gt; /u/AI_Alliance &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m28r3c/ai_devs_in_nyc_heads_up_about_the_raise_act/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m28r3c/ai_devs_in_nyc_heads_up_about_the_raise_act/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m28r3c/ai_devs_in_nyc_heads_up_about_the_raise_act/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-17T14:17:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1m1np9n</id>
    <title>Sometime‚Ä¶ in the next 3 to 5 decades‚Ä¶.</title>
    <updated>2025-07-16T20:32:09+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1np9n/sometime_in_the_next_3_to_5_decades/"&gt; &lt;img alt="Sometime‚Ä¶ in the next 3 to 5 decades‚Ä¶." src="https://preview.redd.it/obmnyjusqadf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=44af76a5484872eea14e9f64b8e559cdd32a58c0" title="Sometime‚Ä¶ in the next 3 to 5 decades‚Ä¶." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/obmnyjusqadf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1np9n/sometime_in_the_next_3_to_5_decades/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m1np9n/sometime_in_the_next_3_to_5_decades/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-16T20:32:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1m1ylw0</id>
    <title>My simple test: Qwen3-32b &gt; Qwen3-14B ‚âà DS Qwen3-8 ‚â≥ Qwen3-4B &gt; Mistral 3.2 24B &gt; Gemma3-27b-it,</title>
    <updated>2025-07-17T04:52:18+00:00</updated>
    <author>
      <name>/u/BestLeonNA</name>
      <uri>https://old.reddit.com/user/BestLeonNA</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have an article to instruct those models to rewrite in a different style without missing information, Qwen3-32B did an excellent job, it keeps the meaning but almost rewrite everything.&lt;/p&gt; &lt;p&gt;Qwen3-14B,8B tend to miss some information but acceptable&lt;/p&gt; &lt;p&gt;Qwen3-4B miss 50% of information&lt;/p&gt; &lt;p&gt;Mistral 3.2, on the other hand does not miss anything but almost copied the original with minor changes.&lt;/p&gt; &lt;p&gt;Gemma3-27: almost a true copy, just stupid&lt;/p&gt; &lt;p&gt;Structured data generation: Another test is to extract Json from raw html, Qweb3-4b fakes data and all others performs well.&lt;/p&gt; &lt;p&gt;Article classification: long messy reddit posts with simple prompt to classify if the post is looking for help, Qwen3-8,14,32 all made it 100% correct, Qwen3-4b mostly correct, Mistral and Gemma always make some mistakes to classify.&lt;/p&gt; &lt;p&gt;Overall, I should say 8b is the best one to do such tasks especially for long articles, the model consumes less vRam allows more vRam allocated to KV Cache&lt;/p&gt; &lt;p&gt;Just my small and simple test today, hope it helps if someone is looking for this use case.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BestLeonNA"&gt; /u/BestLeonNA &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1ylw0/my_simple_test_qwen332b_qwen314b_ds_qwen38/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1ylw0/my_simple_test_qwen332b_qwen314b_ds_qwen38/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m1ylw0/my_simple_test_qwen332b_qwen314b_ds_qwen38/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-17T04:52:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1m28oqc</id>
    <title>Anyone here experimenting with LLMs for translation QA ‚Äî not rewriting, just evaluating?</title>
    <updated>2025-07-17T14:15:06+00:00</updated>
    <author>
      <name>/u/NataliaShu</name>
      <uri>https://old.reddit.com/user/NataliaShu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m28oqc/anyone_here_experimenting_with_llms_for/"&gt; &lt;img alt="Anyone here experimenting with LLMs for translation QA ‚Äî not rewriting, just evaluating?" src="https://b.thumbs.redditmedia.com/DOzsE3kDEUuQywlws06uZepPnsVOAqjt3dXV5HNeD7E.jpg" title="Anyone here experimenting with LLMs for translation QA ‚Äî not rewriting, just evaluating?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi folks, has anyone used LLMs specifically to evaluate translation quality rather than generate translations? I mean using them to catch issues like dropped meaning, inconsistent terminology, awkward phrasing, and so on.&lt;/p&gt; &lt;p&gt;I‚Äôm on a team experimenting with LLMs (GPT-4, Claude, etc.) for automated translation QA. Not to create translations, but to score, flag problems, and suggest batch corrections. The tool we‚Äôre working on is called Alconost.MT/Evaluate, here's what it looks like: &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/kgu312b80gdf1.jpg?width=1200&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=ef7e69dd35bdac400f972e9b58fb94487e39b0ef"&gt;https://preview.redd.it/kgu312b80gdf1.jpg?width=1200&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=ef7e69dd35bdac400f972e9b58fb94487e39b0ef&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I‚Äôm curious: what kinds of metrics or output formats would actually be useful for you guys when comparing translation providers or assessing quality, especially when you can‚Äôt get a full human review? (I‚Äôm old-school enough to believe nothing beats a real linguist‚Äôs eyeballs, but hey, sometimes you gotta trust the bots‚Ä¶ or at least let them do the heavy lifting before the humans jump in.)&lt;/p&gt; &lt;p&gt;Cheers!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NataliaShu"&gt; /u/NataliaShu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m28oqc/anyone_here_experimenting_with_llms_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m28oqc/anyone_here_experimenting_with_llms_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m28oqc/anyone_here_experimenting_with_llms_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-17T14:15:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1m26nyu</id>
    <title>Grok4 and Kimi K2 are making waves, but here's what my dive into 439 models revealed: Wild price gaps and value wins you might be missing</title>
    <updated>2025-07-17T12:49:24+00:00</updated>
    <author>
      <name>/u/medi6</name>
      <uri>https://old.reddit.com/user/medi6</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;!&lt;/p&gt; &lt;p&gt;With all the buzz around Grok4's capabilities and Kimi K2's open-source power (1T params for cheap inference), I figured it's a good time to share some eye-opening findings from analyzing 439 models across 63 providers. After my LLM Selector post got traction here, folks asked about providers, so I crunched the data. &lt;/p&gt; &lt;p&gt;Turns out, the pricing game's full of arbitrages that could save us all serious cash.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Top 5 eye-openers from the data:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. Infinite markup on the same model&lt;/strong&gt;&lt;br /&gt; DeepSeek R1 0528 (quality 68, solid for many tasks): Free on Google Vertex (121 tok/s) or CentML (87 tok/s), but $0.91 on Deepinfra, $4.25 on Fireworks Fast, and $5.50 on SambaNova. Exact same output, why pay up when free works for testing?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. Low-latency Steals&lt;/strong&gt;&lt;br /&gt; Nebius Base delivers sub-ms magic: DeepSeek R1 at 0.61ms ($1/1M, 103 tok/s) and Qwen3 235B at 0.56ms ($0.30/1M, 50 tok/s). Groq pushes Qwen3 32B to 0.14ms ($0.36/1M, 627 tok/s). Beats pricey &amp;quot;enterprise&amp;quot; tiers for real-time stuff.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3. Throughput monsters&lt;/strong&gt;&lt;br /&gt; Cerebras cranks Qwen3 32B to 2,496 tok/s ($0.50/1M) and Llama 4 Scout to 2,808 tok/s ($0.70/1M)‚Äî50x+ faster than averages elsewhere at similar costs. Game-changer for batch processing.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;4. Quality vs price mismatches&lt;/strong&gt;&lt;br /&gt; Qwen3 235B (quality 62) at $0.10/1M on Fireworks (79 tok/s) outperforms Claude 4 Opus (quality 58) at $30/1M (19-65 tok/s). Grok 3 mini (quality 67) at $0.35/1M (210 tok/s) is another win over closed-source premiums.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;5. Big provider variances&lt;/strong&gt;&lt;br /&gt; GPT-4.1 mini ($0.70/1M): Azure at 217 tok/s vs OpenAI's 73 tok/s. o3 ($3.50/1M): OpenAI at 199 tok/s vs Azure's 99 tok/s (higher latency). Same price, huge perf flips.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why it matters&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Open-source is dominating value (top 20 bangs-for-buck are all open), while providers create chaos. Big names overcharge; smaller ones like Nebius/Cerebras optimize hard.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Quick tips:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Ditch &amp;quot;expensive = better&amp;quot; mindset.&lt;/li&gt; &lt;li&gt;Chase latency/speed arbitrages.&lt;/li&gt; &lt;li&gt;Try Cerebras for speed.&lt;/li&gt; &lt;li&gt;Test cheap open-source like Qwen3.&lt;/li&gt; &lt;li&gt;Shop providers ruthlessly.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Data from public benchmarks and artificial analysis, happy to share more details. (I used a comparison aggregator for this, but the raw insights are what count.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Seen these arbitrages in your setups?&lt;/li&gt; &lt;li&gt;Craziest markup you've spotted?&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/medi6"&gt; /u/medi6 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m26nyu/grok4_and_kimi_k2_are_making_waves_but_heres_what/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m26nyu/grok4_and_kimi_k2_are_making_waves_but_heres_what/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m26nyu/grok4_and_kimi_k2_are_making_waves_but_heres_what/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-17T12:49:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1m1vf6g</id>
    <title>Kimi K2 on Aider Polyglot Coding Leaderboard</title>
    <updated>2025-07-17T02:07:06+00:00</updated>
    <author>
      <name>/u/aratahikaru5</name>
      <uri>https://old.reddit.com/user/aratahikaru5</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1vf6g/kimi_k2_on_aider_polyglot_coding_leaderboard/"&gt; &lt;img alt="Kimi K2 on Aider Polyglot Coding Leaderboard" src="https://preview.redd.it/wvr0xh2jecdf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bb91bffcec670286acac6811feb1de48da1d6a7d" title="Kimi K2 on Aider Polyglot Coding Leaderboard" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aratahikaru5"&gt; /u/aratahikaru5 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wvr0xh2jecdf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1vf6g/kimi_k2_on_aider_polyglot_coding_leaderboard/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m1vf6g/kimi_k2_on_aider_polyglot_coding_leaderboard/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-17T02:07:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1m22w76</id>
    <title>Securing AI Agents with Honeypots, catch prompt injections before they bite</title>
    <updated>2025-07-17T09:20:00+00:00</updated>
    <author>
      <name>/u/mario_candela</name>
      <uri>https://old.reddit.com/user/mario_candela</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks üëã &lt;/p&gt; &lt;p&gt;Imagine your AI agent getting hijacked by a prompt-injection attack without you knowing. I'm the founder and maintainer of Beelzebub, an open-source project that hides &amp;quot;honeypot&amp;quot; functions inside your agent using MCP. If the model calls them... üö® BEEP! üö® You get an instant compromise alert, with detailed logs for quick investigations.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Zero false positives: Only real calls trigger the alarm.&lt;/li&gt; &lt;li&gt;Plug-and-play telemetry for tools like Grafana or ELK Stack.&lt;/li&gt; &lt;li&gt;Guard-rails fine-tuning: Every real attack strengthens the guard-rails with human input.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Read the full write-up ‚Üí &lt;a href="https://beelzebub-honeypot.com/blog/securing-ai-agents-with-honeypots/"&gt;https://beelzebub-honeypot.com/blog/securing-ai-agents-with-honeypots/&lt;/a&gt; &lt;/p&gt; &lt;p&gt;What do you think? Is it a smart defense against AI attacks, or just flashy theater? Share feedback, improvement ideas, or memes. &lt;/p&gt; &lt;p&gt;I'm all ears! üòÑ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mario_candela"&gt; /u/mario_candela &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m22w76/securing_ai_agents_with_honeypots_catch_prompt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m22w76/securing_ai_agents_with_honeypots_catch_prompt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m22w76/securing_ai_agents_with_honeypots_catch_prompt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-17T09:20:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1m1sjsn</id>
    <title>MCPS are awesome!</title>
    <updated>2025-07-16T23:52:02+00:00</updated>
    <author>
      <name>/u/iChrist</name>
      <uri>https://old.reddit.com/user/iChrist</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1sjsn/mcps_are_awesome/"&gt; &lt;img alt="MCPS are awesome!" src="https://preview.redd.it/p3766l11qbdf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8a379b48132b0610b66d0e97e1fa3f988c317315" title="MCPS are awesome!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have set up like 17 MCP servers to use with open-webui and local models, and its been amazing!&lt;br /&gt; The ai can decide if it needs to use tools like web search, windows-cli, reddit posts, wikipedia articles.&lt;br /&gt; The usefulness of LLMS became that much bigger!&lt;/p&gt; &lt;p&gt;In the picture above I asked Qwen14B to execute this command in powershell:&lt;/p&gt; &lt;p&gt;python -c &amp;quot;import psutil,GPUtil,json;print(json.dumps({'cpu':psutil.cpu_percent(interval=1),'ram':psutil.virtual_memory().percent,'gpu':[{'name':g.name,'load':g.load*100,'mem_used':g.memoryUsed,'mem_total':g.memoryTotal,'temp':g.temperature} for g in GPUtil.getGPUs()]}))&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iChrist"&gt; /u/iChrist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/p3766l11qbdf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1sjsn/mcps_are_awesome/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m1sjsn/mcps_are_awesome/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-16T23:52:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1m1i922</id>
    <title>He‚Äôs out of line but he‚Äôs right</title>
    <updated>2025-07-16T17:06:31+00:00</updated>
    <author>
      <name>/u/EstablishmentFun3205</name>
      <uri>https://old.reddit.com/user/EstablishmentFun3205</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1i922/hes_out_of_line_but_hes_right/"&gt; &lt;img alt="He‚Äôs out of line but he‚Äôs right" src="https://preview.redd.it/dqx9wlf3q9df1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d71f6c8f3707ff6aae1011b47babeb593bd890e1" title="He‚Äôs out of line but he‚Äôs right" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EstablishmentFun3205"&gt; /u/EstablishmentFun3205 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/dqx9wlf3q9df1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1i922/hes_out_of_line_but_hes_right/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m1i922/hes_out_of_line_but_hes_right/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-16T17:06:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1m253n6</id>
    <title>expectation: "We'll fire thousands of junior programmers and replace them with ten seniors and AI"</title>
    <updated>2025-07-17T11:31:00+00:00</updated>
    <author>
      <name>/u/MelodicRecognition7</name>
      <uri>https://old.reddit.com/user/MelodicRecognition7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;reality: HR's use AI to parse resum√©s and companies hire vibecoders with fake senior resum√©s written by the AI&lt;/p&gt; &lt;p&gt;stage of acceptance: &amp;quot;we'll hire information security specialists to fix all that crap made by the vibecoders&amp;quot;&lt;/p&gt; &lt;p&gt;harsh reality: HR's using AI hire vibeDevSecOpses with fake resum√©s written by the AI and vibeDevSecOpses use AI to &amp;quot;fix&amp;quot; the crap made by the vibecoders using AI&lt;/p&gt; &lt;p&gt;clown world: you are here&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MelodicRecognition7"&gt; /u/MelodicRecognition7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m253n6/expectation_well_fire_thousands_of_junior/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m253n6/expectation_well_fire_thousands_of_junior/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m253n6/expectation_well_fire_thousands_of_junior/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-17T11:31:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1m1xqv1</id>
    <title>We have hit 500,000 members! We have come a long way from the days of the leaked LLaMA 1 models</title>
    <updated>2025-07-17T04:04:21+00:00</updated>
    <author>
      <name>/u/NixTheFolf</name>
      <uri>https://old.reddit.com/user/NixTheFolf</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1xqv1/we_have_hit_500000_members_we_have_come_a_long/"&gt; &lt;img alt="We have hit 500,000 members! We have come a long way from the days of the leaked LLaMA 1 models" src="https://preview.redd.it/zfvdqak3zcdf1.png?width=216&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7a4c2e85087da70112018731aafb9b5d409cf823" title="We have hit 500,000 members! We have come a long way from the days of the leaked LLaMA 1 models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NixTheFolf"&gt; /u/NixTheFolf &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zfvdqak3zcdf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1xqv1/we_have_hit_500000_members_we_have_come_a_long/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m1xqv1/we_have_hit_500000_members_we_have_come_a_long/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-17T04:04:21+00:00</published>
  </entry>
</feed>
