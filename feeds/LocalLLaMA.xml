<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-30T08:51:55+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1oj5fgb</id>
    <title>4B model that looks like GPT-5 and focuses on accessibility, a11y, axe, and lighthouse</title>
    <updated>2025-10-29T14:04:48+00:00</updated>
    <author>
      <name>/u/smirkishere</name>
      <uri>https://old.reddit.com/user/smirkishere</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oj5fgb/4b_model_that_looks_like_gpt5_and_focuses_on/"&gt; &lt;img alt="4B model that looks like GPT-5 and focuses on accessibility, a11y, axe, and lighthouse" src="https://b.thumbs.redditmedia.com/lndb_myUhzz9ZdG20Ey6L9mbcxd3Jvmbtnf5LJgv4rM.jpg" title="4B model that looks like GPT-5 and focuses on accessibility, a11y, axe, and lighthouse" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! I set out to make the UIGEN-FX 4B model repeat less because I was disappointed with it and make it better using GRPO and ended up with some pretty good results. The original model was not that great (hence 'preview') because it kept repeating on us. So I went ahead and did the RL postraining to remove the repeats and focus on a11y, axe, and lighthouse performance scores to improve the quality and accessibility of the webpages. Its mainly focused on html but react should work. I did a similar thing while training Tesslate/Synthia-S1 so hopefully we can come out with a Synthia-S2 soon!&lt;/p&gt; &lt;p&gt;You can try the model here:&lt;br /&gt; &lt;a href="https://huggingface.co/Tesslate/UIGEN-FX-4B-RL-Preview"&gt;https://huggingface.co/Tesslate/UIGEN-FX-4B-RL-Preview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here is the dataset:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/datasets/Tesslate/UIGEN-T2"&gt;https://huggingface.co/datasets/Tesslate/UIGEN-T2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I do apologize I messed up the chat template while training so you'll see 3 'assistant' words and no markdown html escapes. (hence 'preview' again). The next step in this evolution is RL training for the roo code, cline formats. I love receiving feedback and iterating on models!&lt;/p&gt; &lt;p&gt;We have a very interesting drop tomorrow related to local, open source, vibecoding, but if you want a sneak peak just check our announcements channel: &lt;a href="https://discord.gg/TRex2Pku"&gt;https://discord.gg/TRex2Pku&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Everything is Apache 2.0!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/smirkishere"&gt; /u/smirkishere &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1oj5fgb"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oj5fgb/4b_model_that_looks_like_gpt5_and_focuses_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oj5fgb/4b_model_that_looks_like_gpt5_and_focuses_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-29T14:04:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1ojnmr2</id>
    <title>Small LLM speed tests benchmarked on terrible hardware</title>
    <updated>2025-10-30T02:06:03+00:00</updated>
    <author>
      <name>/u/Kahvana</name>
      <uri>https://old.reddit.com/user/Kahvana</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojnmr2/small_llm_speed_tests_benchmarked_on_terrible/"&gt; &lt;img alt="Small LLM speed tests benchmarked on terrible hardware" src="https://external-preview.redd.it/3fNX6Z_5ZMfJ5ddAMJlH1FDIQz9lbcXlvdfD9SR4_nU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a380f4154eec9b5c989ec1b2ccc1ff18e358c963" title="Small LLM speed tests benchmarked on terrible hardware" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a laptop without dGPU nor AVX support, and was curious how terribly it would run various general purpose models. Here are some of the results. I included most, if not all relevant information.&lt;/p&gt; &lt;p&gt;So far I must say I'm impressed with IBM's Granite 4.0 H Nano speeds. I did not expect a model to hit 3/s+ during generation. MobileLLM R1's speed is also very good.&lt;/p&gt; &lt;p&gt;Models suggestions are welcome! Just make sure they're not on the list already. Might benchmark the models with deepeval on my desktop PC later.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kahvana"&gt; /u/Kahvana &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/MerijnHendriks/smallm-bench"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojnmr2/small_llm_speed_tests_benchmarked_on_terrible/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ojnmr2/small_llm_speed_tests_benchmarked_on_terrible/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-30T02:06:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1ojh7yv</id>
    <title>Large language models show signs of introspection</title>
    <updated>2025-10-29T21:29:36+00:00</updated>
    <author>
      <name>/u/bigzyg33k</name>
      <uri>https://old.reddit.com/user/bigzyg33k</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bigzyg33k"&gt; /u/bigzyg33k &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://transformer-circuits.pub/2025/introspection/index.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojh7yv/large_language_models_show_signs_of_introspection/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ojh7yv/large_language_models_show_signs_of_introspection/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-29T21:29:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1ojt934</id>
    <title>Anything better than GLM Air Q8 for dual 6000 Pro?</title>
    <updated>2025-10-30T07:26:02+00:00</updated>
    <author>
      <name>/u/MidnightProgrammer</name>
      <uri>https://old.reddit.com/user/MidnightProgrammer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anything better than GLM Air Q8 for dual 6000 Pro? Limited to what will fit in the 192G of vram only w/ context and full kv.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MidnightProgrammer"&gt; /u/MidnightProgrammer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojt934/anything_better_than_glm_air_q8_for_dual_6000_pro/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojt934/anything_better_than_glm_air_q8_for_dual_6000_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ojt934/anything_better_than_glm_air_q8_for_dual_6000_pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-30T07:26:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1ojt9qt</id>
    <title>Deepseek-OCR Great, but not for long</title>
    <updated>2025-10-30T07:27:20+00:00</updated>
    <author>
      <name>/u/weirdkoe</name>
      <uri>https://old.reddit.com/user/weirdkoe</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So i have been testing Deepseek-OCR for the last couple of days using vLLM as the engine, and it has outperform all my other open-source options (docling, tika, marker, etc..). Yes it do need much better hardware, but the results worth it&lt;/p&gt; &lt;p&gt;Until, when I plugged a 80 pages pdf to be OCR (Arabic language content), it started repeating words.&lt;/p&gt; &lt;p&gt;Each page take around 1 sec, but the pages with the repeating tokes took 30+ seconds to process 💀&lt;/p&gt; &lt;p&gt;I have tried many solutions, but nothing worked&lt;/p&gt; &lt;p&gt;Does anyone know why does this happen?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/weirdkoe"&gt; /u/weirdkoe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojt9qt/deepseekocr_great_but_not_for_long/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojt9qt/deepseekocr_great_but_not_for_long/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ojt9qt/deepseekocr_great_but_not_for_long/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-30T07:27:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1oju6kj</id>
    <title>[ANN] Pocket Agents — A Practical Guide to On-Device AI (Kindle)</title>
    <updated>2025-10-30T08:30:37+00:00</updated>
    <author>
      <name>/u/frayala87</name>
      <uri>https://old.reddit.com/user/frayala87</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oju6kj/ann_pocket_agents_a_practical_guide_to_ondevice/"&gt; &lt;img alt="[ANN] Pocket Agents — A Practical Guide to On-Device AI (Kindle)" src="https://preview.redd.it/2rs9e82bm7yf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c52c61661da0750829c19ee9ceaf43b7f30b70fb" title="[ANN] Pocket Agents — A Practical Guide to On-Device AI (Kindle)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks — I just published a book I’ve been working on for a while: &lt;strong&gt;&lt;em&gt;Pocket Agents: A Practical Guide to On-Device Artificial Intelligence&lt;/em&gt;&lt;/strong&gt; (Kindle Edition)&lt;/p&gt; &lt;p&gt;This is a hands-on, full-stack guide to building autonomous, local AI agents using SLMs like &lt;strong&gt;Gemma&lt;/strong&gt;, &lt;strong&gt;Phi-3&lt;/strong&gt;, and &lt;strong&gt;Qwen&lt;/strong&gt; — all running directly on your own hardware.&lt;/p&gt; &lt;p&gt;It’s based on my experience building &lt;strong&gt;BastionChat (&lt;a href="https://apps.apple.com/fr/app/bastionchat/id6747981691"&gt;https://apps.apple.com/fr/app/bastionchat/id6747981691&lt;/a&gt;)&lt;/strong&gt;, a fully local assistant that proves you don’t need the cloud to get real intelligence. This book distills everything I learned: from QLoRA fine-tuning tollama.cpp deployment to building persistent, multi-step agentic workflows.&lt;/p&gt; &lt;h1&gt;What’s inside:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;🧠 &lt;strong&gt;Sovereign AI principles&lt;/strong&gt;: local-first, private-by-default, fully autonomous&lt;/li&gt; &lt;li&gt;🔧 &lt;strong&gt;Practical stack&lt;/strong&gt;: QLoRA, llama.cpp, agentic patterns, memory, tool use&lt;/li&gt; &lt;li&gt;💻 &lt;strong&gt;Device-level deployment&lt;/strong&gt;: how to reclaim the full compute of your laptop or phone&lt;/li&gt; &lt;li&gt;🔒 &lt;strong&gt;Data sovereignty&lt;/strong&gt;: your data stays local, period&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This is for anyone who’s serious about building &lt;strong&gt;independent AI systems&lt;/strong&gt; — not just running models, but designing agents that serve you and only you.&lt;/p&gt; &lt;p&gt;If that resonates, here’s the link: &lt;a href="https://www.amazon.fr/dp/B0FXXKPPRZ"&gt;https://www.amazon.fr/dp/B0FXXKPPRZ&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love feedback from this community — especially if you’re building similar systems or want to push the boundaries of what local agents can do.&lt;/p&gt; &lt;p&gt;#SovereignAI #SLM #OnDeviceAI #LocalLLaMA #BastionChat&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/frayala87"&gt; /u/frayala87 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2rs9e82bm7yf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oju6kj/ann_pocket_agents_a_practical_guide_to_ondevice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oju6kj/ann_pocket_agents_a_practical_guide_to_ondevice/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-30T08:30:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1ojsefc</id>
    <title>What is the best German TTS Model for on prem deployment</title>
    <updated>2025-10-30T06:29:19+00:00</updated>
    <author>
      <name>/u/ZeroKelvinMood</name>
      <uri>https://old.reddit.com/user/ZeroKelvinMood</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Moin&lt;/p&gt; &lt;p&gt;I’m looking for recommendations for the best German TTS model that can be deployed on premise. Ideally, it should be production ready, i’m not looking for a prototype or beginner stage project.&lt;/p&gt; &lt;p&gt;Price doesn’t really matter, but I’d like to know how different price points compare.&lt;/p&gt; &lt;p&gt;My use case is an AI phone assistant for doctors in the European Union, so low latency and GDPR compliance for sensitive medical data are absolutely critical.&lt;/p&gt; &lt;p&gt;Would really appreciate your insights, thanks in advance&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ZeroKelvinMood"&gt; /u/ZeroKelvinMood &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojsefc/what_is_the_best_german_tts_model_for_on_prem/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojsefc/what_is_the_best_german_tts_model_for_on_prem/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ojsefc/what_is_the_best_german_tts_model_for_on_prem/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-30T06:29:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ojsul0</id>
    <title>any 12b model that is smart for logic and realistic roleplay like claude? Any Hope left for roleplay?</title>
    <updated>2025-10-30T06:59:11+00:00</updated>
    <author>
      <name>/u/BeastMad</name>
      <uri>https://old.reddit.com/user/BeastMad</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was experimenting with an AI roleplay scenario just for fun — it was about a blacksmith and his wife, and I played the role of a customer buying something. The AI was roleplaying as the blacksmith. To test how realistic the AI’s reactions were, I tried flirting with the blacksmith’s wife. But instead of getting angry or acting protective, the blacksmith just laughed and said, “Feeling romantic?”&lt;/p&gt; &lt;p&gt;That kind of response really broke the immersion for me. I wish the AI would act more realistically in situations like that — for example, showing anger or hostility instead of reacting casually.&lt;/p&gt; &lt;p&gt;So any hope left for 12b the model that is smart similar to claude? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BeastMad"&gt; /u/BeastMad &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojsul0/any_12b_model_that_is_smart_for_logic_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojsul0/any_12b_model_that_is_smart_for_logic_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ojsul0/any_12b_model_that_is_smart_for_logic_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-30T06:59:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1oj5z8p</id>
    <title>JanusCoder by internlm (7B/8B/14B)</title>
    <updated>2025-10-29T14:26:23+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;models description:&lt;/p&gt; &lt;p&gt;&amp;quot;We introduce JanusCoder and JanusCoderV, a suite of open-source foundational models designed to establish a unified visual-programmatic interface for code intelligence. This model suite is built upon open-source language models (such as Qwen3-8B and 14B) and multimodal models (such as Qwen2.5-VL and InternVL3.5-8B). The JanusCoder series is trained on JANUSCODE-800K—the largest multimodal code corpus to date, generated by an innovative synthesis toolkit, covering everything from standard charts to complex interactive Web UIs and code-driven animations. This enables the models to uniformly handle diverse visual-programmatic tasks, such as generating code from textual instructions, visual inputs, or a combination of both, rather than building specialized models for isolated tasks. JanusCoder excels at flexible content generation (like data visualizations and interactive front-ends) as well as precise, program-driven editing of visual effects and complex animation construction.&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/internlm/JanusCoder-8B"&gt;https://huggingface.co/internlm/JanusCoder-8B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/internlm/JanusCoder-14B"&gt;https://huggingface.co/internlm/JanusCoder-14B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/internlm/JanusCoderV-8B"&gt;https://huggingface.co/internlm/JanusCoderV-8B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/internlm/JanusCoderV-7B"&gt;https://huggingface.co/internlm/JanusCoderV-7B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oj5z8p/januscoder_by_internlm_7b8b14b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oj5z8p/januscoder_by_internlm_7b8b14b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oj5z8p/januscoder_by_internlm_7b8b14b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-29T14:26:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1oja0o8</id>
    <title>2 x DGX Spark! Give me your non-inference workloads</title>
    <updated>2025-10-29T16:59:07+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oja0o8/2_x_dgx_spark_give_me_your_noninference_workloads/"&gt; &lt;img alt="2 x DGX Spark! Give me your non-inference workloads" src="https://preview.redd.it/vjd12ghi03yf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=44d93dacbc0dd0a138c61a15816d1e4f2c6102a7" title="2 x DGX Spark! Give me your non-inference workloads" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;2 x DGX Spark with a &lt;a href="https://network.nvidia.com/files/related-docs/prod_cables/PB_MCP1650-H0xxEyy_200Gbps_QSFP56_DAC.pdf"&gt;200Gbps interconnect&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;I posted here &lt;a href="https://www.reddit.com/r/LocalLLaMA/s/h5q2KfvTae"&gt;when my first Spark came in&lt;/a&gt; and everyone responded with inference workloads. I still tested them, but inference monkeys please BTFO this time.&lt;/p&gt; &lt;p&gt;Give me your big model non-inference workloads to test, something to push the 256GB unified memory. I have a few LORA training ones from the last post to try. I already have nanochat pretraining running. GRPO without PEFT planned.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vjd12ghi03yf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oja0o8/2_x_dgx_spark_give_me_your_noninference_workloads/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oja0o8/2_x_dgx_spark_give_me_your_noninference_workloads/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-29T16:59:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1ojs49s</id>
    <title>What's one tool or script that massively improved your local LLM workflow?</title>
    <updated>2025-10-30T06:10:49+00:00</updated>
    <author>
      <name>/u/Street-Lie-2584</name>
      <uri>https://old.reddit.com/user/Street-Lie-2584</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Beyond the popular UIs like Oobabooga and Faraday, I'm looking for those smaller utilities that save time or add a killer feature. For example, a script for batch testing prompts across multiple models, a tool for better logprobs analysis, or a clever use of llama.cpp's server features. What's your secret weapon?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Street-Lie-2584"&gt; /u/Street-Lie-2584 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojs49s/whats_one_tool_or_script_that_massively_improved/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojs49s/whats_one_tool_or_script_that_massively_improved/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ojs49s/whats_one_tool_or_script_that_massively_improved/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-30T06:10:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ojstm4</id>
    <title>How automated is your data flywheel, really?</title>
    <updated>2025-10-30T06:57:20+00:00</updated>
    <author>
      <name>/u/Individual-Library-1</name>
      <uri>https://old.reddit.com/user/Individual-Library-1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Working on my 3rd production AI deployment. Everyone talks about &amp;quot;systems that learn from user feedback&amp;quot; but in practice I'm seeing:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Users correct errors&lt;/li&gt; &lt;li&gt;Errors get logged&lt;/li&gt; &lt;li&gt;Engineers review logs weekly&lt;/li&gt; &lt;li&gt;Engineers manually update model/prompts -&lt;/li&gt; &lt;li&gt;Repeat This is just &amp;quot;manual updates with extra steps,&amp;quot; not a real flywheel.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Question: Has anyone actually built a fully automated learning loop where corrections → automatic improvements without engineering?&lt;/p&gt; &lt;p&gt;Or is &amp;quot;self-improving AI&amp;quot; still mostly marketing?&lt;/p&gt; &lt;p&gt;Open to 20-min calls to compare approaches. DM me.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Individual-Library-1"&gt; /u/Individual-Library-1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojstm4/how_automated_is_your_data_flywheel_really/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojstm4/how_automated_is_your_data_flywheel_really/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ojstm4/how_automated_is_your_data_flywheel_really/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-30T06:57:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1ojhdgi</id>
    <title>Automated metadata tagging for image collections that runs completely locally. A way to search image collections without software lock-in, databases, or cloud services.</title>
    <updated>2025-10-29T21:35:36+00:00</updated>
    <author>
      <name>/u/Eisenstein</name>
      <uri>https://old.reddit.com/user/Eisenstein</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojhdgi/automated_metadata_tagging_for_image_collections/"&gt; &lt;img alt="Automated metadata tagging for image collections that runs completely locally. A way to search image collections without software lock-in, databases, or cloud services." src="https://external-preview.redd.it/SdNcK7BdgRqMa8-G5nagIMUt2TjZgJXQeatIWMOCjqU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f967920e17edca37152de084ff4e38f8b6b72271" title="Automated metadata tagging for image collections that runs completely locally. A way to search image collections without software lock-in, databases, or cloud services." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Eisenstein"&gt; /u/Eisenstein &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/jabberjabberjabber/ImageIndexer"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojhdgi/automated_metadata_tagging_for_image_collections/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ojhdgi/automated_metadata_tagging_for_image_collections/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-29T21:35:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1ojt6sc</id>
    <title>best local llm for simple every day reasoning and some coding perhaps?</title>
    <updated>2025-10-30T07:21:43+00:00</updated>
    <author>
      <name>/u/dkatsikis</name>
      <uri>https://old.reddit.com/user/dkatsikis</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;what you guys think I should download ? will use it either on ollama or lmstudio, I can go up to 8b parameter I think cause of my Macs 16gb ram, what would you suggest? I&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dkatsikis"&gt; /u/dkatsikis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojt6sc/best_local_llm_for_simple_every_day_reasoning_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojt6sc/best_local_llm_for_simple_every_day_reasoning_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ojt6sc/best_local_llm_for_simple_every_day_reasoning_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-30T07:21:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1oivxji</id>
    <title>Qwen3 Max Thinking this week</title>
    <updated>2025-10-29T05:04:42+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oivxji/qwen3_max_thinking_this_week/"&gt; &lt;img alt="Qwen3 Max Thinking this week" src="https://preview.redd.it/pbd1ylu1hzxf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a0a191c61d159739c3e9b620cdefab0a9534288f" title="Qwen3 Max Thinking this week" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/pbd1ylu1hzxf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oivxji/qwen3_max_thinking_this_week/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oivxji/qwen3_max_thinking_this_week/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-29T05:04:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ojpfwl</id>
    <title>MLX added support for MXFP8 and NVFP4</title>
    <updated>2025-10-30T03:35:29+00:00</updated>
    <author>
      <name>/u/Direct-Stranger-4140</name>
      <uri>https://old.reddit.com/user/Direct-Stranger-4140</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;Supports mxfp8 and nvfp4 in quantize/dequantize and adds kernels for mx and nv quants.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Ops based fallback for CPU&lt;/li&gt; &lt;li&gt;Fast CUDA kernels&lt;/li&gt; &lt;li&gt;Fast Metal kernels&lt;/li&gt; &lt;li&gt;Defaults for bits and group size based on mode&amp;quot;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://github.com/ml-explore/mlx/pull/2688"&gt;https://github.com/ml-explore/mlx/pull/2688&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Direct-Stranger-4140"&gt; /u/Direct-Stranger-4140 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojpfwl/mlx_added_support_for_mxfp8_and_nvfp4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojpfwl/mlx_added_support_for_mxfp8_and_nvfp4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ojpfwl/mlx_added_support_for_mxfp8_and_nvfp4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-30T03:35:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1oj44s9</id>
    <title>If You Want to Understand Why Llama Models Flopped, Zuck is the Cause!</title>
    <updated>2025-10-29T13:11:25+00:00</updated>
    <author>
      <name>/u/Iory1998</name>
      <uri>https://old.reddit.com/user/Iory1998</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Below is a short video that attempts to explain why most Meta products fails... Spoiler alert, it's Zuck's fault.&lt;br /&gt; &lt;a href="https://www.youtube.com/watch?v=hb5cYB7Eoj8"&gt;https://www.youtube.com/watch?v=hb5cYB7Eoj8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I strongly believe Llama 5 will not come out any time soon. I don't think there will be any Llama5, to be honest. And, I don't think we will see any good competitive OS model from Meta ever again. Why do I believe that, you ask? Well, any investment requires long-term commitment and perseverance, even if you encounter a few setbacks along the way. But, as long as Meta AI is controlled by Zuck, it will never invest long enough to achieve anything meaningful simply because Zuck isn't someone who commits to an idea long enough. Flipflopping seems to be in his DNA as a CEO.&lt;/p&gt; &lt;p&gt;What do you think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Iory1998"&gt; /u/Iory1998 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oj44s9/if_you_want_to_understand_why_llama_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oj44s9/if_you_want_to_understand_why_llama_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oj44s9/if_you_want_to_understand_why_llama_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-29T13:11:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1ojswm4</id>
    <title>new Nemotrons based on Qwen3 32B</title>
    <updated>2025-10-30T07:02:40+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojswm4/new_nemotrons_based_on_qwen3_32b/"&gt; &lt;img alt="new Nemotrons based on Qwen3 32B" src="https://b.thumbs.redditmedia.com/qOhsjjl6hDVmoBy_BtTdl9Y4EEOOeDpCvErh6KfawCo.jpg" title="new Nemotrons based on Qwen3 32B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Qwen3-Nemotron-32B-RLBFF&lt;/strong&gt; is a large language model that leverages Qwen/Qwen3-32B as the foundation and is fine-tuned to improve the quality of LLM-generated responses in the default thinking mode.&lt;/p&gt; &lt;p&gt;Given a conversation with multiple turns between user and assistant and a user-specified principle, it generates a response the final user turn.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;This is a research model&lt;/strong&gt; described in and is released to support the following research paper: &lt;a href="https://arxiv.org/abs/2509.21319"&gt;https://arxiv.org/abs/2509.21319&lt;/a&gt;&lt;/p&gt; &lt;p&gt;As of 24 Sep 2025, this model achieves &lt;a href="https://github.com/lmarena/arena-hard-auto"&gt;Arena Hard V2&lt;/a&gt; of 55.6% and &lt;a href="https://huggingface.co/spaces/allenai/WildBench"&gt;WildBench&lt;/a&gt; Score of 70.33% and &lt;a href="https://arxiv.org/abs/2306.05685"&gt;MT Bench&lt;/a&gt; of 9.50. This means that our model is substantially improved over the initial Qwen3-32B model and has similar performance compared to DeepSeek R1 and O3-mini at less than 5% of the inference cost (as indicated on openrouter).&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/cap7eioa77yf1.png?width=1282&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fd3ed08aa03d73b589fcf39e3ebb20360df97ef3"&gt;https://preview.redd.it/cap7eioa77yf1.png?width=1282&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fd3ed08aa03d73b589fcf39e3ebb20360df97ef3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nvidia/Qwen3-Nemotron-32B-RLBFF"&gt;https://huggingface.co/nvidia/Qwen3-Nemotron-32B-RLBFF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojswm4/new_nemotrons_based_on_qwen3_32b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojswm4/new_nemotrons_based_on_qwen3_32b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ojswm4/new_nemotrons_based_on_qwen3_32b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-30T07:02:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1ojneas</id>
    <title>How are teams dealing with "AI fatigue"</title>
    <updated>2025-10-30T01:55:24+00:00</updated>
    <author>
      <name>/u/Temporary_Papaya_199</name>
      <uri>https://old.reddit.com/user/Temporary_Papaya_199</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I rolled out AI coding assistants for my developers, and while individual developer &amp;quot;productivity&amp;quot; went up - team alignment and developer &amp;quot;velocity&amp;quot; did not.&lt;/p&gt; &lt;p&gt;They worked more - but not shipping new features. They were now spending more time reviewing and fixing AI slob. My current theory - AI helps the individual not the team.&lt;/p&gt; &lt;p&gt;Are any of you seeing similar issues? If yes, where, translating requirements into developer tasks, figuring out how one introduction or change impacts everything else or with keeping JIRA and github synced.&lt;/p&gt; &lt;p&gt;Want to know how you guys are solving this problem.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Temporary_Papaya_199"&gt; /u/Temporary_Papaya_199 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojneas/how_are_teams_dealing_with_ai_fatigue/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojneas/how_are_teams_dealing_with_ai_fatigue/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ojneas/how_are_teams_dealing_with_ai_fatigue/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-30T01:55:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1ojrv67</id>
    <title>Tried Nvidia’s new open-source VLM, Here's My Experience</title>
    <updated>2025-10-30T05:54:42+00:00</updated>
    <author>
      <name>/u/Arindam_200</name>
      <uri>https://old.reddit.com/user/Arindam_200</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been playing around with NVIDIA’s new Nemotron Nano 12B V2 VL, and it’s easily one of the most impressive open-source vision-language models I’ve tested so far.&lt;/p&gt; &lt;p&gt;I started simple: built a small Streamlit OCR app to see how well it could parse real documents.&lt;br /&gt; Dropped in an invoice, it picked out totals, vendor details, and line items flawlessly.&lt;br /&gt; Then I gave it a &lt;em&gt;handwritten note&lt;/em&gt;, and somehow, it summarized the content correctly, no OCR hacks, no preprocessing pipelines. Just raw understanding.&lt;/p&gt; &lt;p&gt;Then I got curious.&lt;br /&gt; What if I showed it something completely different?&lt;/p&gt; &lt;p&gt;So I uploaded a frame from &lt;em&gt;Star Wars: The Force Awakens,&lt;/em&gt; Kylo Ren, lightsaber drawn, and the model instantly recognized the scene and character. ( This impressed me the Most)&lt;/p&gt; &lt;p&gt;You can run visual Q&amp;amp;A, summarization, or reasoning across up to 4 document images (1k×2k each), all with long text prompts.&lt;/p&gt; &lt;p&gt;This feels like the start of something big for open-source document and vision AI. Here's the &lt;a href="https://x.com/Arindam_1729/status/1983536576157372886"&gt;short clips&lt;/a&gt; of my tests.&lt;/p&gt; &lt;p&gt;Would love to know your experience with it!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Arindam_200"&gt; /u/Arindam_200 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojrv67/tried_nvidias_new_opensource_vlm_heres_my/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojrv67/tried_nvidias_new_opensource_vlm_heres_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ojrv67/tried_nvidias_new_opensource_vlm_heres_my/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-30T05:54:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ojd1oq</id>
    <title>Here's the best prompt you will ever need to test the new LLMs</title>
    <updated>2025-10-29T18:49:59+00:00</updated>
    <author>
      <name>/u/Cool-Chemical-5629</name>
      <uri>https://old.reddit.com/user/Cool-Chemical-5629</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojd1oq/heres_the_best_prompt_you_will_ever_need_to_test/"&gt; &lt;img alt="Here's the best prompt you will ever need to test the new LLMs" src="https://preview.redd.it/n2yilqu2k3yf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=575a481bdcf458b2fd06c48788fa5f9271cd30ad" title="Here's the best prompt you will ever need to test the new LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Prompt:&lt;/p&gt; &lt;p&gt;The numbers Mason, what do they mean?!! 10 23 68 111 8 7 7 47 53 23 63 92 15&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cool-Chemical-5629"&gt; /u/Cool-Chemical-5629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/n2yilqu2k3yf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojd1oq/heres_the_best_prompt_you_will_ever_need_to_test/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ojd1oq/heres_the_best_prompt_you_will_ever_need_to_test/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-29T18:49:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1oje2cc</id>
    <title>DeepSeek may have found a new way to improve AI’s ability to remember</title>
    <updated>2025-10-29T19:27:50+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oje2cc/deepseek_may_have_found_a_new_way_to_improve_ais/"&gt; &lt;img alt="DeepSeek may have found a new way to improve AI’s ability to remember" src="https://external-preview.redd.it/aEXxC3nzMud-eeB3QaX4fT3GiVuldP60VVX6Yf4IvC8.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0104fa768b4fc264cfeccc0fd459a4eaa256f643" title="DeepSeek may have found a new way to improve AI’s ability to remember" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.technologyreview.com/2025/10/29/1126932/deepseek-ocr-visual-compression"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oje2cc/deepseek_may_have_found_a_new_way_to_improve_ais/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oje2cc/deepseek_may_have_found_a_new_way_to_improve_ais/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-29T19:27:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1ojo8le</id>
    <title>Minimax pre-training lead explains why no linear attention</title>
    <updated>2025-10-30T02:35:13+00:00</updated>
    <author>
      <name>/u/Charuru</name>
      <uri>https://old.reddit.com/user/Charuru</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;MiniMax M2 Tech Blog 3: Why Did M2 End Up as a Full Attention Model?&lt;/p&gt; &lt;p&gt;On behave of pre-training lead Haohai Sun. (&lt;a href="https://zhihu.com/question/1965302088260104295/answer/1966810157473335067"&gt;https://zhihu.com/question/1965302088260104295/answer/1966810157473335067&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;I. Introduction&lt;/p&gt; &lt;p&gt;As the lead of MiniMax-M2 pretrain, I've been getting many queries from the community on &amp;quot;Why did you turn back the clock and go with full attention with MiniMax M2?&amp;quot; After explaining the backstory in one chat after another, I figured it's time to write down our journey in a blog.&lt;/p&gt; &lt;p&gt;Honestly, I could give you the textbook debate. I could talk all afternoon about why you should build linear/sparse attention. Then, I could turn around and talk all afternoon about why you shouldn't. But what's the point of all that hand-waving? The real question is whether you should actually do it.&lt;/p&gt; &lt;p&gt;So, let's start with the conclusion: We are always working on it. But in a real-world, industrial-grade system, the truth is that efficient attention still has some way to go before it can definitively beat full attention. As LLMs have evolved, the entire stack has become monstrously complex. We serve more scenarios, and the architecture design trade-offs are exploding: &amp;quot;How does it perform on code and math? What about agent scenarios? How does it handle multimodality? Does long-chain CoT still hold up? Can RL scale on top of it? Are there hidden traps with low-precision compute? How do you implement interleaved thinking, caching, or speculative decoding? ... &amp;quot;&lt;/p&gt; &lt;p&gt;In short, there's a vast difference between the promise on paper and its payoff in production. You only get to claim that payoff after satisfying Condition 1...n and solving Problem 1...n.&lt;/p&gt; &lt;p&gt;II. Why Efficient Attention?&lt;/p&gt; &lt;p&gt;Let's do a thought experiment. If you had infinite compute, would you even bother with linear or sparse attention? Some might bring up theoretical arguments about softmax attention &amp;quot;oversmoothing&amp;quot; in an infinite context... but who knows? Under the current compute bound, no model has truly pushed softmax attention to its absolute limit. So, for all practical purposes, the race for efficient attention is a race to save compute.&lt;/p&gt; &lt;p&gt;For our M2 design, could we aim to save tokens — achieving the same quality with fewer tokens? Well if you believe in scaling laws, to achieve this goal, you'd probably bet on other paths to get there, not efficient attention.&lt;/p&gt; &lt;p&gt;So, the simple truth is this: Compute is finite. We need an architecture that makes better use of it — models that achieve higher performance under the same budget (training &amp;amp; inference).&lt;/p&gt; &lt;p&gt;III. The Real Bottlenecks&lt;/p&gt; &lt;p&gt;To build a model that can practically be deployed and used by the community, we have to start with what users care: Quality, Speed (TPS), and Price. Quality is non-negotiable. A useless model is useless even if it's free. So how do we make a Linear/Sparse/Hybrid Attention model that performs well enough? The biggest challenge here isn’t the architecture design — the real bottleneck is the limitations of evaluation. (As for speed and price, those are heavily influenced by the inference stack—and great models tend to attract great engineers to optimize them.)&lt;/p&gt; &lt;p&gt;The Evaluation Trap: Goodhart's Law in Action&lt;/p&gt; &lt;p&gt;“As long as you build the benchmark, I’ll find a way to beat it.” Over the past few years of LLM development, the pace of leaderboard progress is staggering. No matter how hard a benchmark is — even if the SOTA score starts in single digits — once it catches the industry’s attention, it’s usually crushed within a few iterations. But how do you build an evaluation system that is comprehensive and actually reflects a model's true capabilities? That’s one of the hardest — and most critical — problems in LLM development, and it becomes even more acute when you start messing with a component as fundamental as attention.&lt;/p&gt; &lt;p&gt;Benchmarks are a Leaky Abstraction&lt;/p&gt; &lt;p&gt;There’s no free lunch. When you reduce the complexity of attention, you pay a price. The question is, where?&lt;/p&gt; &lt;p&gt;When we were developing MiniMax-Text-01, everyone was still evaluating MMLU, BBH, MATH, and LongBench (all of which are now saturated). From the perspective of a year ago, a hybrid of Lightning Attention and Full Attention looked just as good as pure full attention. Our own small-scale hybrid models confirmed this on the leaderboards. (Did we find a free lunch?)&lt;/p&gt; &lt;p&gt;Not quite. The price paid became obvious at a larger scale: the model had clear deficits in complex, multi-hop reasoning tasks.&lt;/p&gt; &lt;p&gt;Okay, once a problem is exposed, you can fix it. We developed proxy metrics for this specific weakness and iterated until the hybrid model seemed to match MHA. But does that proxy metric still correlate with real-world downstream performance at an even larger scale? Are there other hidden weaknesses? Who knows. We haven't run those experiments yet.&lt;/p&gt; &lt;p&gt;The better the models get, the harder they are to evaluate. But that’s a must part of the journey — keep it up, eval teams!&lt;/p&gt; &lt;p&gt;The High Cost of Knowing Things&lt;/p&gt; &lt;p&gt;For complex reasoning tasks, we can sometimes find early proxy metrics that correlate well with final performance — but not for all tasks (at least, not yet). As tasks get harder, the amount of experiment compute required just to get a statistically significant signal on your metric grows astronomically — which is ironic, since we study efficient attention because compute is limited.&lt;/p&gt; &lt;p&gt;And beyond the academic benchmarks, optimization issues often only surface at scale. You never really know what’s going to happen until you scale up. Anyone who read our M1 paper will recall the serious precision issues we hit during RL training — problems that would’ve been spotted earlier. Going back and analyzing Lightning Attention's numerical convergence with that experience in hand was incredibly clarifying.&lt;/p&gt; &lt;p&gt;Discovering the real problems is often far harder than solving them.&lt;/p&gt; &lt;p&gt;A Symphony of Variables&lt;/p&gt; &lt;p&gt;There are just too many variables in model training. Different architectures behave very differently on different data distributions and with different optimizers. In a world where our data is constantly being updated, an experiment run on last month's data mix might yield the opposite conclusion today. We can’t observe everything perfectly — but we’re working on finding more reliable experimental strategies.&lt;/p&gt; &lt;p&gt;Infrastructure: Where Theory Meets Metal&lt;/p&gt; &lt;p&gt;Compared to full attention, the infrastructure for linear and sparse attention is much less mature. To actually get the promised results, there’s still a lot of groundwork to fill in. Take linear attention for example: If you analyze the compute intensity of existing linear architectures, many of them are memory-bound — even during training. Without extreme IO optimization, you’re basically leaving a huge amount of GPU FLOPs on the table. And inference brings even more challenges than training: How do you deliver a service that is genuinely faster and cheaper? Linear attention has linear compute complexity and constant memory usage. That means there’s a crossover point where it becomes more efficient than full attention in compute and memory. In theory, that point lies at a few thousand tokens — which isn’t particularly long for today’s large models.&lt;/p&gt; &lt;p&gt;But that’s just theory. We need to solve a few key problems to actually approach it:&lt;/p&gt; &lt;p&gt;Low-Precision State Storage: Linear attention is currently far more sensitive to numerical precision than full attention.&lt;/p&gt; &lt;p&gt;Prefix Caching: In real-world applications, the cache-hit rate for conversations is very high. A new architecture must handle this gracefully.&lt;/p&gt; &lt;p&gt;Speculative Decoding: How do you optimize speculative decoding with linear attention backbone? Well fortunately, all of these seem solvable.&lt;/p&gt; &lt;p&gt;IV. What’s Next&lt;/p&gt; &lt;p&gt;Scaling remains the name of the game, and context scaling is one of the key problems. Longer and longer context length is key in both pre-training and post-training. As GPU compute growth slows while data length keeps increasing, the benefits of linear and sparse attention will gradually emerge. We should start preparing now:&lt;/p&gt; &lt;p&gt;Better Data: More multimodal, information-rich long-context data.&lt;/p&gt; &lt;p&gt;Better Evaluation: More informative evaluation system and experimental paradigms to speed up iteration.&lt;/p&gt; &lt;p&gt;Better Infrastructure: Mature training and inference infrastructure to fully squeeze out GPU potential.&lt;/p&gt; &lt;p&gt;V. Addendum: the SWA code...&lt;/p&gt; &lt;p&gt;We accidentally left the SWA inference code in the open-source release, and some people asked why it wasn’t used in the final model. Simple answer: the performance wasn't good enough.&lt;/p&gt; &lt;p&gt;That experiment was from quite early on, before GPT-OSS was open-sourced (we were pretty surprised to see its structure, by the way). But I can share a brief summary of our failed attempt. We tried adapting CPT into a Hybrid SWA, testing both inter &amp;amp; intra-layer mixing. The motivation for intra-layer mixing was to balance the compute intensity across all layers, which is friendly to both PP in training and PP or AFD during inference. Unfortunately, neither worked. Performance degraded noticeably as context length grew — which is unacceptable in agentic scenarios.&lt;/p&gt; &lt;p&gt;Our analysis showed that many global attention patterns (like retrieval head and induction head) were already established early during pre-training. CPT can hardly adjust those patterns afterwards. You surely can mitigate the issue by using data probes to identify and keep those heads as full attention — but unfortunately, it’s nearly impossible to discover them all from human priors.&lt;/p&gt; &lt;p&gt;(And no, this issue isn’t related to attention sinks.)&lt;/p&gt; &lt;p&gt;If you're interested in this line of research, I recommend taking a closer look at GPT-OSS, CWM, and Gemma, especially their long-context performance.&lt;/p&gt; &lt;p&gt;Finally, we’re hiring! If you want to join us, send your resume to &lt;a href="mailto:guixianren@minimaxi.com"&gt;guixianren@minimaxi.com&lt;/a&gt;.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;References&lt;/li&gt; &lt;li&gt;MiniMax-01: Scaling Foundation Models with Lightning Attention&lt;/li&gt; &lt;li&gt;MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention&lt;/li&gt; &lt;li&gt;CWM: An Open-Weights LLM for Research on Code Generation with World Models&lt;/li&gt; &lt;li&gt;Qwen3-Next&lt;/li&gt; &lt;li&gt;Gemma 3 Technical Report&lt;/li&gt; &lt;li&gt;gpt-oss-120b &amp;amp; gpt-oss-20b Model Card&lt;/li&gt; &lt;li&gt;Retrieval Head Mechanistically Explains Long-Context Factuality&lt;/li&gt; &lt;li&gt;&lt;a href="https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html"&gt;https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://x.com/zpysky1125/status/1983383094607347992"&gt;https://x.com/zpysky1125/status/1983383094607347992&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also I called it last month: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nfyjv5/cmv_qwen3next_is_an_architectural_deadend_much/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1nfyjv5/cmv_qwen3next_is_an_architectural_deadend_much/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charuru"&gt; /u/Charuru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojo8le/minimax_pretraining_lead_explains_why_no_linear/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojo8le/minimax_pretraining_lead_explains_why_no_linear/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ojo8le/minimax_pretraining_lead_explains_why_no_linear/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-30T02:35:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ojf2r1</id>
    <title>Qwen3-VL now available in Ollama locally for all sizes.</title>
    <updated>2025-10-29T20:06:18+00:00</updated>
    <author>
      <name>/u/swagonflyyyy</name>
      <uri>https://old.reddit.com/user/swagonflyyyy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojf2r1/qwen3vl_now_available_in_ollama_locally_for_all/"&gt; &lt;img alt="Qwen3-VL now available in Ollama locally for all sizes." src="https://preview.redd.it/ycizvauvx3yf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a4ef60e1f14a4f71fc6659de298dd0d9565d8acb" title="Qwen3-VL now available in Ollama locally for all sizes." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/swagonflyyyy"&gt; /u/swagonflyyyy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ycizvauvx3yf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojf2r1/qwen3vl_now_available_in_ollama_locally_for_all/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ojf2r1/qwen3vl_now_available_in_ollama_locally_for_all/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-29T20:06:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1ojqvwe</id>
    <title>Udio just robbed and betrayed its paying subscribers... Another reason why we need more Open Source</title>
    <updated>2025-10-30T04:54:59+00:00</updated>
    <author>
      <name>/u/Shockbum</name>
      <uri>https://old.reddit.com/user/Shockbum</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojqvwe/udio_just_robbed_and_betrayed_its_paying/"&gt; &lt;img alt="Udio just robbed and betrayed its paying subscribers... Another reason why we need more Open Source" src="https://external-preview.redd.it/YW51aGVlN3ZpNnlmMbXr-Angp87qmunczlw3KeeJxKKBK56n_5S01mafzhX6.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ac34d32c78cef1f79f10ce908455ea3f27985eef" title="Udio just robbed and betrayed its paying subscribers... Another reason why we need more Open Source" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I spent 12 hours working on a song, and without any prior notice, I can no longer download it as a .wav file. I’ll have to find other ways to recover the song. I’ve been a South American subscriber for months, and I trust North American companies less and less because of these anti-consumer practices. If I could give $10 a month to an open-source developer working on AI music generation, I’d gladly do it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Shockbum"&gt; /u/Shockbum &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/r40nze7vi6yf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojqvwe/udio_just_robbed_and_betrayed_its_paying/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ojqvwe/udio_just_robbed_and_betrayed_its_paying/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-30T04:54:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohdzxs</id>
    <title>AMA Announcement: Liquid AI, the team behind Liquid Foundational Models, LEAP and Apollo (Thu, Oct 30 • 10 AM – 1 PM PDT)</title>
    <updated>2025-10-27T13:10:46+00:00</updated>
    <author>
      <name>/u/LiquidAI_Team</name>
      <uri>https://old.reddit.com/user/LiquidAI_Team</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohdzxs/ama_announcement_liquid_ai_the_team_behind_liquid/"&gt; &lt;img alt="AMA Announcement: Liquid AI, the team behind Liquid Foundational Models, LEAP and Apollo (Thu, Oct 30 • 10 AM – 1 PM PDT)" src="https://preview.redd.it/47wfyylmlnxf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c359ceba4921b523ecd2e493f9cc84bd8b3e7881" title="AMA Announcement: Liquid AI, the team behind Liquid Foundational Models, LEAP and Apollo (Thu, Oct 30 • 10 AM – 1 PM PDT)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;When: Thursday 10/30, 10 AM – 1 PM PST&lt;/h1&gt; &lt;p&gt;The Liquid AI team will also continue answering questions for the following 24 hours, so jump in anytime!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Who will be there:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Jacob Marks (Data)&lt;/li&gt; &lt;li&gt;Jimmy Smith (Pre-Training)&lt;/li&gt; &lt;li&gt;Maxime Labonne (Post-Training)&lt;/li&gt; &lt;li&gt;Fernando Fernandes (Post-training)&lt;/li&gt; &lt;li&gt;Anna Banaszak (LFM2-VL)&lt;/li&gt; &lt;li&gt;Arthur Böök (LFM2-Audio)&lt;/li&gt; &lt;li&gt;Yuri Khrustalev (Inference engine, llama.cpp)&lt;/li&gt; &lt;li&gt;Darian Bhathena (LEAP SDK and Apollo)&lt;/li&gt; &lt;li&gt;Edoardo Mosca (LEAP Best Model Search and Finetune)&lt;/li&gt; &lt;li&gt;Anthony Crognale (LEAP SDK)&lt;/li&gt; &lt;li&gt;Pau Labarta Bajo (Dev Relations)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Want to get started?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;→ &lt;a href="https://leap.liquid.ai/models?utm_source=reddit&amp;amp;utm_medium=devrel"&gt;Deploy your first model on-device today&lt;/a&gt;&lt;br /&gt; → &lt;a href="https://huggingface.co/LiquidAI?utm_source=reddit&amp;amp;utm_medium=devrel"&gt;Check out our models on Hugging Face&lt;/a&gt;&lt;br /&gt; → &lt;a href="https://www.liquid.ai/apollo?utm_source=reddit&amp;amp;utm_medium=devrel"&gt;Play with models on Apollo&lt;/a&gt;&lt;br /&gt; → &lt;a href="https://www.liquid.ai/company/news?utm_source=reddit&amp;amp;utm_medium=devrel"&gt;Learn more about our recent releases&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LiquidAI_Team"&gt; /u/LiquidAI_Team &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/47wfyylmlnxf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohdzxs/ama_announcement_liquid_ai_the_team_behind_liquid/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohdzxs/ama_announcement_liquid_ai_the_team_behind_liquid/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T13:10:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohqev8</id>
    <title>Best Local TTS/STT Models - October 2025</title>
    <updated>2025-10-27T21:00:42+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Share what your favorite TTS / STT models are right now &lt;strong&gt;and why.&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Given the the amount of ambiguity and subjectivity in rating/testing these models, please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc. Closed models like Elevenlabs v3 seem to continue to be a few levels above open models, so comparisons, especially empirical ones are welcome.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please use the top level TTS/STT comments to thread your responses. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohqev8/best_local_ttsstt_models_october_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohqev8/best_local_ttsstt_models_october_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohqev8/best_local_ttsstt_models_october_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T21:00:42+00:00</published>
  </entry>
</feed>
