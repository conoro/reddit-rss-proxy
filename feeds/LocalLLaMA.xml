<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-16T16:58:07+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1pnusq8</id>
    <title>Open Source Alternative to Perplexity</title>
    <updated>2025-12-16T06:16:11+00:00</updated>
    <author>
      <name>/u/Uiqueblhats</name>
      <uri>https://old.reddit.com/user/Uiqueblhats</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those of you who aren't familiar with SurfSense, it aims to be the &lt;strong&gt;open-source alternative to NotebookLM, Perplexity, or Glean.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;In short, it's a Highly Customizable AI Research Agent that connects to your personal external sources and Search Engines (SearxNG, Tavily, LinkUp), Slack, Linear, Jira, ClickUp, Confluence, Gmail, Notion, YouTube, GitHub, Discord, Airtable, Google Calendar and more to come.&lt;/p&gt; &lt;p&gt;I'm looking for contributors. If you're interested in AI agents, RAG, browser extensions, or building open-source research tools, this is a great place to jump in.&lt;/p&gt; &lt;p&gt;Here‚Äôs a quick look at what SurfSense offers right now:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;RBAC (Role Based Access for Teams)&lt;/li&gt; &lt;li&gt;Supports 100+ LLMs&lt;/li&gt; &lt;li&gt;Supports local Ollama or vLLM setups&lt;/li&gt; &lt;li&gt;6000+ Embedding Models&lt;/li&gt; &lt;li&gt;50+ File extensions supported (Added Docling recently)&lt;/li&gt; &lt;li&gt;Podcasts support with local TTS providers (Kokoro TTS)&lt;/li&gt; &lt;li&gt;Connects with 15+ external sources such as Search Engines, Slack, Notion, Gmail, Notion, Confluence etc&lt;/li&gt; &lt;li&gt;Cross-Browser Extension to let you save any dynamic webpage you want, including authenticated content.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Upcoming Planned Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Agentic chat&lt;/li&gt; &lt;li&gt;Note Management (Like Notion)&lt;/li&gt; &lt;li&gt;Multi Collaborative Chats.&lt;/li&gt; &lt;li&gt;Multi Collaborative Documents.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Installation (Self-Host)&lt;/strong&gt;&lt;/p&gt; &lt;h1&gt;Linux/macOS:&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;docker run -d -p 3000:3000 -p 8000:8000 \ -v surfsense-data:/data \ --name surfsense \ --restart unless-stopped \ ghcr.io/modsetter/surfsense:latest &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Windows (PowerShell):&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;docker run -d -p 3000:3000 -p 8000:8000 ` -v surfsense-data:/data ` --name surfsense ` --restart unless-stopped ` ghcr.io/modsetter/surfsense:latest &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/MODSetter/SurfSense"&gt;https://github.com/MODSetter/SurfSense&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Uiqueblhats"&gt; /u/Uiqueblhats &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnusq8/open_source_alternative_to_perplexity/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnusq8/open_source_alternative_to_perplexity/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pnusq8/open_source_alternative_to_perplexity/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T06:16:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnd5uf</id>
    <title>They're finally here (Radeon 9700)</title>
    <updated>2025-12-15T17:20:23+00:00</updated>
    <author>
      <name>/u/Zeikos</name>
      <uri>https://old.reddit.com/user/Zeikos</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnd5uf/theyre_finally_here_radeon_9700/"&gt; &lt;img alt="They're finally here (Radeon 9700)" src="https://b.thumbs.redditmedia.com/LlhzLUprDuJWJk6b4cZsmRPc06FSQCX3yS5XKj_YEOk.jpg" title="They're finally here (Radeon 9700)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zeikos"&gt; /u/Zeikos &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pnd5uf"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnd5uf/theyre_finally_here_radeon_9700/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pnd5uf/theyre_finally_here_radeon_9700/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T17:20:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnzt9y</id>
    <title>2025 Open Models Year in Review</title>
    <updated>2025-12-16T11:39:20+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnzt9y/2025_open_models_year_in_review/"&gt; &lt;img alt="2025 Open Models Year in Review" src="https://a.thumbs.redditmedia.com/bAksHsJN0-iuMgHdsfyM9GRrMDkxdq7w432ACMBAY84.jpg" title="2025 Open Models Year in Review" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/x9r0l9rcyj7g1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=04829370c50c43b71249d0b687d517beaa024d53"&gt;https://preview.redd.it/x9r0l9rcyj7g1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=04829370c50c43b71249d0b687d517beaa024d53&lt;/a&gt;&lt;/p&gt; &lt;p&gt;AI research organization &lt;a href="https://www.interconnects.ai/p/2025-open-models-year-in-review"&gt;Interconnects&lt;/a&gt; released the 2025 Annual Review Report on Open-Source Models, stating that 2025 is a milestone year for the development of open-source models. The report shows that open-source models have achieved performance comparable to closed-source models in most key benchmarks, with DeepSeek R1 and Qwen 3 being recognized as the most influential models of the year.&lt;/p&gt; &lt;h1&gt;Mapping the open ecosystem&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/obgfnkd2zj7g1.png?width=1946&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9e1c376a28402c3d1b0912f2b66c2d0ee2ebbc28"&gt;https://preview.redd.it/obgfnkd2zj7g1.png?width=1946&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9e1c376a28402c3d1b0912f2b66c2d0ee2ebbc28&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The organizations are as follows.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Frontier&lt;/strong&gt;: DeepSeek, Qwen, Moonshot AI (Kimi)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Close competitors&lt;/strong&gt;: Zhipu (Z.Ai), Minimax&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Noteworthy&lt;/strong&gt;: StepFun, InclusionAI / Ant Ling, Meituan Longcat, Tencent, IBM, NVIDIA, Google, Mistral&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Specialists&lt;/strong&gt;: OpenAI, Ai2, Moondream, Arcee, RedNote, HuggingFace, LiquidAI, Microsoft, Xiaomi, Mohamed bin Zayed University of Artificial Intelligence&lt;/p&gt; &lt;p&gt;&lt;strong&gt;On the rise&lt;/strong&gt;: ByteDance Seed, Apertus, OpenBMB, Motif, Baidu, Marin Community, InternLM, OpenGVLab, ServiceNow, Skywork&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Honorable mentions&lt;/strong&gt;: TNG Group, Meta, Cohere, Beijing Academy of Artificial Intelligence, Multimodal Art Projection, Huawei&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnzt9y/2025_open_models_year_in_review/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnzt9y/2025_open_models_year_in_review/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pnzt9y/2025_open_models_year_in_review/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T11:39:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnllux</id>
    <title>New budget local AI rig</title>
    <updated>2025-12-15T22:51:32+00:00</updated>
    <author>
      <name>/u/vucamille</name>
      <uri>https://old.reddit.com/user/vucamille</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnllux/new_budget_local_ai_rig/"&gt; &lt;img alt="New budget local AI rig" src="https://preview.redd.it/6aavy1486g7g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=505d5c7891c288215bdfa28b4ea82e8ed8df45bc" title="New budget local AI rig" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to buy 32GB Mi50s but decided against it because of their recent inflated prices. However, the 16GB versions are still affordable! I might buy another one in the future, or wait until the 32GB gets cheaper again.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Qiyida X99 mobo with 32GB RAM and Xeon E5 2680 V4: 90 USD (AliExpress)&lt;/li&gt; &lt;li&gt;2x MI50 16GB with dual fan mod: 108 USD each plus 32 USD shipping (Alibaba)&lt;/li&gt; &lt;li&gt;1200W PSU bought in my country: 160 USD - lol the most expensive component in the PC&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In total, I spent about 650 USD. ROCm 7.0.2 works, and I have done some basic inference tests with llama.cpp and the two MI50, everything works well. Initially I tried with the latest ROCm release but multi GPU was not working for me.&lt;/p&gt; &lt;p&gt;I still need to buy brackets to prevent the bottom MI50 from sagging and maybe some decorations and LEDs, but so far super happy! And as a bonus, this thing can game!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vucamille"&gt; /u/vucamille &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6aavy1486g7g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnllux/new_budget_local_ai_rig/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pnllux/new_budget_local_ai_rig/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T22:51:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnul23</id>
    <title>Sometimes it‚Äôs stupid even if it works</title>
    <updated>2025-12-16T06:04:18+00:00</updated>
    <author>
      <name>/u/Stunning_Mast2001</name>
      <uri>https://old.reddit.com/user/Stunning_Mast2001</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnul23/sometimes_its_stupid_even_if_it_works/"&gt; &lt;img alt="Sometimes it‚Äôs stupid even if it works" src="https://preview.redd.it/xvgt5nx9bi7g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=52c4bfd3eb7be97e9faed9c2c4c560c81a0efa06" title="Sometimes it‚Äôs stupid even if it works" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Someone gave me a quadro but I have a 1080ti already so no internal space‚Ä¶ just strapped it to the outside with the riser cables looping out the back‚Ä¶ works fine&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Stunning_Mast2001"&gt; /u/Stunning_Mast2001 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xvgt5nx9bi7g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnul23/sometimes_its_stupid_even_if_it_works/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pnul23/sometimes_its_stupid_even_if_it_works/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T06:04:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1po3v2l</id>
    <title>XiaomiMiMo/MiMo-V2-Flash ¬∑ Hugging Face</title>
    <updated>2025-12-16T14:51:15+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1po3v2l/xiaomimimomimov2flash_hugging_face/"&gt; &lt;img alt="XiaomiMiMo/MiMo-V2-Flash ¬∑ Hugging Face" src="https://external-preview.redd.it/pEssBYcofSIqxenRV_1O2yb3vr7ekZdMtNbDln2iEbQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=413fe05449bcb79ceb4c3c13d870125113113e50" title="XiaomiMiMo/MiMo-V2-Flash ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;MiMo-V2-Flash&lt;/strong&gt; is a Mixture-of-Experts (MoE) language model with &lt;strong&gt;309B total parameters&lt;/strong&gt; and &lt;strong&gt;15B active parameters&lt;/strong&gt;. Designed for high-speed reasoning and agentic workflows, it utilizes a novel hybrid attention architecture and Multi-Token Prediction (MTP) to achieve state-of-the-art performance while significantly reducing inference costs.&lt;/p&gt; &lt;p&gt;MiMo-V2-Flash creates a new balance between long-context modeling capability and inference efficiency. Key features include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Hybrid Attention Architecture&lt;/strong&gt;: Interleaves Sliding Window Attention (SWA) and Global Attention (GA) with a 5:1 ratio and an aggressive 128-token window. This reduces KV-cache storage by nearly 6x while maintaining long-context performance via learnable &lt;strong&gt;attention sink bias&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multi-Token Prediction (MTP)&lt;/strong&gt;: Equipped with a lightweight MTP module (0.33B params/block) using dense FFNs. This triples output speed during inference and will be good to accelerates rollout in RL training.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Efficient Pre-Training&lt;/strong&gt;: Trained on 27T tokens using FP8 mixed precision and native 32k seq length. The context window supports up to 256k length.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agentic Capabilities&lt;/strong&gt;: Post-training utilizes Multi-Teacher On-Policy Distillation (MOPD) and large-scale agentic RL, achieving superior performance on &lt;strong&gt;SWE-Bench&lt;/strong&gt; and complex reasoning tasks.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/XiaomiMiMo/MiMo-V2-Flash"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1po3v2l/xiaomimimomimov2flash_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1po3v2l/xiaomimimomimov2flash_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T14:51:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnxxnq</id>
    <title>Nemotron-Cascade 8B/14B from NVIDIA (Qwen3 finetunes)</title>
    <updated>2025-12-16T09:42:03+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnxxnq/nemotroncascade_8b14b_from_nvidia_qwen3_finetunes/"&gt; &lt;img alt="Nemotron-Cascade 8B/14B from NVIDIA (Qwen3 finetunes)" src="https://external-preview.redd.it/2QEDEkegLrJTJtx6HLSiu0oL0Rwu2mfV0busJyR6xa4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=931815c93c1175064ff4b44e489163b0700b3b19" title="Nemotron-Cascade 8B/14B from NVIDIA (Qwen3 finetunes)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;powerful general-purpose model trained through sequential and domain-wise reinforcement learning&amp;quot;&lt;/p&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/nvidia/Nemotron-Cascade-8B#results"&gt;&lt;/a&gt;Results&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;We evaluate our model against competitive reasoning models on a diverse set of benchmarks, covering general-knowledge reasoning, alignment and instruction following, mathematical reasoning, competitive programming, software engineering, and tool-use proficiency.&lt;/li&gt; &lt;li&gt;For Nemotron-Cascade models, we use a maximum generation length of 64K tokens and set the temperature to 0.6 and top-p to 0.95 for reasoning tasks.&lt;/li&gt; &lt;li&gt;Our Nemotron-Cascade models achieve best-in-class performance across almost all benchmarks. Remarkably, Nemotron-Cascade-8B and Nemotron-Cascade-8B-Thinking achieve comparable LiveCodeBench (LCB) and LCB Pro scores to DeepSeek-R1-0528 (671B).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nvidia/Nemotron-Cascade-14B-Thinking"&gt;https://huggingface.co/nvidia/Nemotron-Cascade-14B-Thinking&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/gj477a45gj7g1.png?width=3686&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=44c7b8f4bdfa6c79c9fa4c4b27e7d5cb59a5d845"&gt;https://preview.redd.it/gj477a45gj7g1.png?width=3686&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=44c7b8f4bdfa6c79c9fa4c4b27e7d5cb59a5d845&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nvidia/Nemotron-Cascade-8B-Thinking"&gt;https://huggingface.co/nvidia/Nemotron-Cascade-8B-Thinking&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/4bhwmvzbgj7g1.png?width=3654&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bd06d124d8130f3e608bd39d3392fe9ec908b4eb"&gt;https://preview.redd.it/4bhwmvzbgj7g1.png?width=3654&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bd06d124d8130f3e608bd39d3392fe9ec908b4eb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nvidia/Nemotron-Cascade-8B"&gt;https://huggingface.co/nvidia/Nemotron-Cascade-8B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/pto8szxfgj7g1.png?width=3664&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d2525653a81ac3fd712ed5f92052fbbdf3674880"&gt;https://preview.redd.it/pto8szxfgj7g1.png?width=3664&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d2525653a81ac3fd712ed5f92052fbbdf3674880&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnxxnq/nemotroncascade_8b14b_from_nvidia_qwen3_finetunes/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnxxnq/nemotroncascade_8b14b_from_nvidia_qwen3_finetunes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pnxxnq/nemotroncascade_8b14b_from_nvidia_qwen3_finetunes/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T09:42:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1po49p3</id>
    <title>Full AI Voice Agent (Whisper + 700M LLM + NeuTTS) running entirely on an Nvidia Jetson Orin Nano ($250 hardware) with no internet access</title>
    <updated>2025-12-16T15:07:21+00:00</updated>
    <author>
      <name>/u/TeamNeuphonic</name>
      <uri>https://old.reddit.com/user/TeamNeuphonic</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1po49p3/full_ai_voice_agent_whisper_700m_llm_neutts/"&gt; &lt;img alt="Full AI Voice Agent (Whisper + 700M LLM + NeuTTS) running entirely on an Nvidia Jetson Orin Nano ($250 hardware) with no internet access" src="https://external-preview.redd.it/9UWsVs2hpMeho4on4Wi5M0sA3g8yJ-Upoj2dNWPAG_M.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7e9c82e009084fe605bea0791ebccc366c5869a5" title="Full AI Voice Agent (Whisper + 700M LLM + NeuTTS) running entirely on an Nvidia Jetson Orin Nano ($250 hardware) with no internet access" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We‚Äôve been playing with what's truly possible for low-latency, privacy-first voice agents, and just released a demo: Agent Santa.&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1po49p3/video/s8sca29xzk7g1/player"&gt;https://reddit.com/link/1po49p3/video/s8sca29xzk7g1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The entire voice-to-text-to-speech loop runs &lt;em&gt;locally&lt;/em&gt; on a sub-$250 Nvidia Jetson Orin Nano.&lt;/p&gt; &lt;p&gt;The ML Stack:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;STT: OpenAI Whisper EN tiny&lt;/li&gt; &lt;li&gt;LLM: LiquidAI‚Äôs 700M-parameter LFM2&lt;/li&gt; &lt;li&gt;TTS: Our NeuTTS (zero-cost cloning, high quality)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The whole thing consumes under 4GB RAM and 2GB VRAM. This showcases that complex, multi-model AI can be fully deployed on edge devices today.&lt;/p&gt; &lt;p&gt;We'd love to hear your feedback on the latency and potential applications for this level of extreme on-device efficiency.&lt;/p&gt; &lt;p&gt;Git Repo: &lt;a href="https://github.com/neuphonic/neutts-air"&gt;https://github.com/neuphonic/neutts-air&lt;/a&gt;&lt;/p&gt; &lt;p&gt;HF: &lt;a href="https://huggingface.co/neuphonic/neutts-air"&gt;https://huggingface.co/neuphonic/neutts-air&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TeamNeuphonic"&gt; /u/TeamNeuphonic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1po49p3/full_ai_voice_agent_whisper_700m_llm_neutts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1po49p3/full_ai_voice_agent_whisper_700m_llm_neutts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1po49p3/full_ai_voice_agent_whisper_700m_llm_neutts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T15:07:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1pn8upp</id>
    <title>NVIDIA releases Nemotron 3 Nano, a new 30B hybrid reasoning model!</title>
    <updated>2025-12-15T14:34:28+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn8upp/nvidia_releases_nemotron_3_nano_a_new_30b_hybrid/"&gt; &lt;img alt="NVIDIA releases Nemotron 3 Nano, a new 30B hybrid reasoning model!" src="https://preview.redd.it/sic85bvhpd7g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d01067e3a3899e680b913799c37c8ef9b609ff4c" title="NVIDIA releases Nemotron 3 Nano, a new 30B hybrid reasoning model!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Unsloth GGUF: &lt;a href="https://huggingface.co/unsloth/Nemotron-3-Nano-30B-A3B-GGUF"&gt;https://huggingface.co/unsloth/Nemotron-3-Nano-30B-A3B-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Nemotron 3 has a 1M context window and the best in class performance for SWE-Bench, reasoning and chat.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sic85bvhpd7g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn8upp/nvidia_releases_nemotron_3_nano_a_new_30b_hybrid/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pn8upp/nvidia_releases_nemotron_3_nano_a_new_30b_hybrid/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T14:34:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnslcb</id>
    <title>My Local coding agent worked 2 hours unsupervised and here is my setup</title>
    <updated>2025-12-16T04:14:38+00:00</updated>
    <author>
      <name>/u/Express_Quail_1493</name>
      <uri>https://old.reddit.com/user/Express_Quail_1493</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Setup&lt;/p&gt; &lt;p&gt;--- Model&lt;br /&gt; devstral-small-2 from bartowski IQ3_xxs version.&lt;br /&gt; Run with lm studio &amp;amp; intentionally limit the context at 40960 which should't take more than (14gb ram even when context is full)&lt;/p&gt; &lt;p&gt;---Tool&lt;br /&gt; kilo code (set file limit to 500 lines) it will read in chunks&lt;br /&gt; 40960 ctx limit is actually a strength not weakness (more ctx = easier confusion)&lt;br /&gt; Paired with qdrant in the kilo code UI.&lt;br /&gt; Setup the indexing with qdrant (the little database icon) use model &lt;a href="https://ollama.com/toshk0/nomic-embed-text-v2-moe"&gt;https://ollama.com/toshk0/nomic-embed-text-v2-moe&lt;/a&gt; in ollama (i choose ollama to keep indexing and seperate from Lm studio to allow lm studio to focus on the heavy lifting)&lt;/p&gt; &lt;p&gt;--Result&lt;br /&gt; minimal drift on tasks&lt;br /&gt; slight errors on tool call but the model quickly realign itself. A oneshot prompt implimentation of a new feature in my codebase in architect mode resulted in 2 hours of coding unsupervised kilo code auto switches to code mode to impliment after planning in architect mode which is amazing. Thats been my lived experience&lt;/p&gt; &lt;p&gt;EDIT: ministral 3 3b also works okayISH if you are desprate on hardware resources (3.5gb laptop GPU) but it will want to frequently pause and ask you some questions at the slightest hint of anythings it might be unclear on&lt;/p&gt; &lt;p&gt;Feel free to also share your fully localhost setup that also solved long running tasks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Express_Quail_1493"&gt; /u/Express_Quail_1493 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnslcb/my_local_coding_agent_worked_2_hours_unsupervised/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnslcb/my_local_coding_agent_worked_2_hours_unsupervised/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pnslcb/my_local_coding_agent_worked_2_hours_unsupervised/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T04:14:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnb824</id>
    <title>Chatterbox Turbo, new open-source voice AI model, just released on Hugging Face</title>
    <updated>2025-12-15T16:08:45+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnb824/chatterbox_turbo_new_opensource_voice_ai_model/"&gt; &lt;img alt="Chatterbox Turbo, new open-source voice AI model, just released on Hugging Face" src="https://external-preview.redd.it/Mno1ZHBiNTg0ZTdnMetROQBwb-dMzbNK88p-4KlSnzkAfcO7Jy5xOmtEL7Fy.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ea250dde0c1f1556ba5b404754e09373d2c88623" title="Chatterbox Turbo, new open-source voice AI model, just released on Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Links:&lt;br /&gt; - Model (PyTorch): &lt;a href="https://huggingface.co/ResembleAI/chatterbox-turbo"&gt;https://huggingface.co/ResembleAI/chatterbox-turbo&lt;/a&gt;&lt;br /&gt; - Model (ONNX): &lt;a href="https://huggingface.co/ResembleAI/chatterbox-turbo-ONNX"&gt;https://huggingface.co/ResembleAI/chatterbox-turbo-ONNX&lt;/a&gt;&lt;br /&gt; - GitHub: &lt;a href="https://github.com/resemble-ai/chatterbox"&gt;https://github.com/resemble-ai/chatterbox&lt;/a&gt;&lt;br /&gt; - Demo: &lt;a href="https://huggingface.co/spaces/ResembleAI/chatterbox-turbo-demo"&gt;https://huggingface.co/spaces/ResembleAI/chatterbox-turbo-demo&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/6v5yql484e7g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnb824/chatterbox_turbo_new_opensource_voice_ai_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pnb824/chatterbox_turbo_new_opensource_voice_ai_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T16:08:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnz1je</id>
    <title>support for GLM4V vision encoder has been merged into llama.cpp</title>
    <updated>2025-12-16T10:53:34+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnz1je/support_for_glm4v_vision_encoder_has_been_merged/"&gt; &lt;img alt="support for GLM4V vision encoder has been merged into llama.cpp" src="https://external-preview.redd.it/i0ktGuORgovwZVXClbj98qHky3ndOw6pJOFp0qnTifE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4d399641787a216379ff3d6b42189093d66157b6" title="support for GLM4V vision encoder has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/18042"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnz1je/support_for_glm4v_vision_encoder_has_been_merged/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pnz1je/support_for_glm4v_vision_encoder_has_been_merged/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T10:53:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1po3ln2</id>
    <title>Key Highlights of NVIDIA‚Äôs New Model: Nemotron-Cascade-8B</title>
    <updated>2025-12-16T14:40:35+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1po3ln2/key_highlights_of_nvidias_new_model/"&gt; &lt;img alt="Key Highlights of NVIDIA‚Äôs New Model: Nemotron-Cascade-8B" src="https://external-preview.redd.it/2QEDEkegLrJTJtx6HLSiu0oL0Rwu2mfV0busJyR6xa4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=931815c93c1175064ff4b44e489163b0700b3b19" title="Key Highlights of NVIDIA‚Äôs New Model: Nemotron-Cascade-8B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;[1] General-Purpose Reinforcement-Learned Model&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Trained through a sequential and domain-wise reinforcement learning pipeline built on top of a base Qwen3-8B model, enhancing performance across diverse task domains&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;[2] Dual Reasoning &amp;amp; Instruction Modes&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Supports both &lt;em&gt;thinking&lt;/em&gt; (reasoning) and &lt;em&gt;instruct&lt;/em&gt; (non-reasoning) modes, allowing flexible use cases within the same model architecture.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;[3] Strong Benchmark Performance&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Achieves competitive results on knowledge, reasoning, alignment, math, and code benchmarks, with metrics comparable to much larger models in several evaluations.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;[4] Open Model Release &amp;amp; License&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Released with the NVIDIA Open Model License and openly available for community use, research, and customization.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/nvidia/Nemotron-Cascade-8B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1po3ln2/key_highlights_of_nvidias_new_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1po3ln2/key_highlights_of_nvidias_new_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T14:40:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1pny30h</id>
    <title>The Attention Hybrid MoE Architecture is the Future. Now, AI Labs Should Dedicate Resources to Improve Long Context Recall Capabilities.</title>
    <updated>2025-12-16T09:52:11+00:00</updated>
    <author>
      <name>/u/Iory1998</name>
      <uri>https://old.reddit.com/user/Iory1998</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been using Qwen3-Next-80B-A30 since it was fully supported in Llama.cpp, and I found it to be the best open-weight model I've ever ran locally ((Unsloth)_Qwen3-Next-80B-A3B-Instruct-GGUF-Q6_K_XL). It's also the first model I could run at full context size (256K) on a single RTX3090 (forcing model expert weights onto CPU, obviously) at around 12t/s.&lt;/p&gt; &lt;p&gt;Before, you say &amp;quot;oh, that's so slow&amp;quot;, let me clarify that a 12t/s speed is twice as fast as I can ever read. Also, just last year, people were happy to run llama3-70B at an average speed of 5t/s, and 2 years ago, people were happy to run llama2-7B (8K context size ü§¶‚Äç‚ôÄÔ∏è) at 12t/s.&lt;/p&gt; &lt;p&gt;Today, I tried (Unsloth)_Nemotron-3-Nano-30B-A3B-GGUF-Q8_K_XL at full context size (1M ü§Ø), and the speed is around 12.5t/s (again, forcing model expert weights onto CPU, obviously). The full context uses 12.6GB of VRAM, leaving me with about 11GB of free VRAM üåãü§Ø. I tested it's recall capability up to 80K, and the model is solid, with almost no context degradation that I can tell.&lt;/p&gt; &lt;p&gt;So, if it's not obvious to some already, this Mamba2-Transformer Hybrid MoE architecture is here so stay. AI Labs must now improve models recall capabilities to truly benefit from in-context learning. I am no expert in the field, and please feel free to interject and correct me if I am wrong, but I think if a smaller model is well trained to fully utilize long context to draw conclusions or discover knowledge it was not trained on, if will allow for the shipping of smaller yet capable models.&lt;/p&gt; &lt;p&gt;My point is, we don't need a model that holds all the human knowledge in its weights, but one that is trained to derive or rediscover unseen knowledge and build upon that to solve novel problems. In other words, I think if a model can reason about novel data, it would reuse the same parameters for many domains, dramatically reducing the size of the training corpus needed to reach a given capability ceiling.&lt;/p&gt; &lt;p&gt;I think if this is achieved, we can expect a decrease in training costs and an increase in model intelligence. We might even see a better model generalization very soon.&lt;/p&gt; &lt;p&gt;What do you think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Iory1998"&gt; /u/Iory1998 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pny30h/the_attention_hybrid_moe_architecture_is_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pny30h/the_attention_hybrid_moe_architecture_is_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pny30h/the_attention_hybrid_moe_architecture_is_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T09:52:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnxkvw</id>
    <title>llama.cpp support for Nemotron 3 Nano merged!</title>
    <updated>2025-12-16T09:17:48+00:00</updated>
    <author>
      <name>/u/QuackerEnte</name>
      <uri>https://old.reddit.com/user/QuackerEnte</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/releases/tag/b7418"&gt;https://github.com/ggml-org/llama.cpp/releases/tag/b7418&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Details&lt;/p&gt; &lt;p&gt;llama : add support for NVIDIA Nemotron 3 Nano (#18058)&lt;/p&gt; &lt;p&gt;llama : add support for NVIDIA Nemotron Nano 3 This commit adds support for the NVIDIA Nemotron Nano 3 model, enabling the conversion and running of this model.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/QuackerEnte"&gt; /u/QuackerEnte &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnxkvw/llamacpp_support_for_nemotron_3_nano_merged/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnxkvw/llamacpp_support_for_nemotron_3_nano_merged/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pnxkvw/llamacpp_support_for_nemotron_3_nano_merged/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T09:17:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1po2slg</id>
    <title>My professor lent me an A6000, so I tried to build a coding model. Here is Anni! (Qwen3-14B Fine-tune)</title>
    <updated>2025-12-16T14:06:42+00:00</updated>
    <author>
      <name>/u/Outrageous-Yak8298</name>
      <uri>https://old.reddit.com/user/Outrageous-Yak8298</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1po2slg/my_professor_lent_me_an_a6000_so_i_tried_to_build/"&gt; &lt;img alt="My professor lent me an A6000, so I tried to build a coding model. Here is Anni! (Qwen3-14B Fine-tune)" src="https://b.thumbs.redditmedia.com/DaBXg_p6QRIuS4sCb73zScs5SsGoLmqoDUfn34hBixE.jpg" title="My professor lent me an A6000, so I tried to build a coding model. Here is Anni! (Qwen3-14B Fine-tune)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Feedback and suggestions are welcomed! &lt;a href="https://hanstan.link/how-i-trained-a-sota-coding-model-on-a-single-gpu/"&gt;Full Technical Write-up&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I‚Äôm a 2nd year undergrad AI student and just finished training my very first LLM. Like many of you, I wanted to train a capable coding model but didn't have a cluster of H100s‚Äîjust a single &lt;strong&gt;Nvidia A6000 (48GB) thanks to my professor :)&lt;/strong&gt; and a dream!&lt;/p&gt; &lt;p&gt;I spent the last few months building &lt;strong&gt;Anni&lt;/strong&gt; &lt;a href="https://github.com/CoderUni/Anni"&gt;&lt;strong&gt;https://github.com/CoderUni/Anni&lt;/strong&gt;&lt;/a&gt;, a 14B Qwen3-based model fine-tuned on the &lt;strong&gt;Nvidia OpenCodeReasoning-2&lt;/strong&gt; dataset.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Stats:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Base Model:&lt;/strong&gt; Qwen3-14B&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hardware:&lt;/strong&gt; Single A6000 (48GB VRAM)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Training Time:&lt;/strong&gt; Reduced from ~1.6 months (projected) to &lt;strong&gt;~2 weeks&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Score:&lt;/strong&gt; &lt;strong&gt;41.7% Pass@1&lt;/strong&gt; on LiveCodeBench (v6), theoretically matching &lt;strong&gt;Claude 3.5 Sonnet (Thinking)&lt;/strong&gt; and beating &lt;strong&gt;GPT-4o&lt;/strong&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;The &amp;quot;SOTA&amp;quot; Benchmark Reality Check (Please Read)&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/qwbv16c4pk7g1.jpg?width=1740&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=1975e04ca21c0dfa9d746a3ab479a4e2c8d93a2c"&gt;https://preview.redd.it/qwbv16c4pk7g1.jpg?width=1740&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=1975e04ca21c0dfa9d746a3ab479a4e2c8d93a2c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Before anyone calls it out, I want to be 100% transparent: &lt;strong&gt;This benchmark score is likely contaminated.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;After seeing the crazy numbers, I couldn't believe I beat last year's SOTA models and investigated. I then found out that the LiveCodeBench (v6) questions are from &lt;strong&gt;April‚ÄìMay 2025&lt;/strong&gt;. My training dataset (OpenCodeReasoning-2) was curated between &lt;strong&gt;March‚ÄìMay 2025&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;I would love to test it on problems released &lt;strong&gt;after June 2025&lt;/strong&gt; once LCB v7 comes out!&lt;/p&gt; &lt;p&gt;Despite my best efforts to deduplicate the data using content-based hashing, there is a high probability the model &amp;quot;saw&amp;quot; the test questions during training.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Did I beat Nvidia's Nemotron 1.1 model?&lt;/strong&gt; Unlikely.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Does it demonstrate that a student can realistically train a model that comes close to SOTA models?&lt;/strong&gt; Absolutely.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;How I decreased training times and fit this in one GPU&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;I initially thought I could simply blindly follow tutorials without understanding the fundamentals.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;DO NOT DO IT! Take your time to learn and understand the fundamentals! It's the best decision you will ever make! It helped me in the long run.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;After going through many research reports and &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; posts, I learned how to optimize everything to get this done in 2 weeks instead of 2 months. Here is what worked:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Progressive Training:&lt;/strong&gt; I didn't train on 32k context immediately. I split training into 4 stages, starting with &amp;quot;easy&amp;quot; short samples (0-4k tokens) and progressively scaling to &amp;quot;hard&amp;quot; long contexts (up to 32k). This stabilized loss and sped up convergence.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Early Stopping:&lt;/strong&gt; I realized convergence happened way faster than expected on high-quality synthetic data, saving weeks of compute.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;&amp;quot;Hacky&amp;quot; Deployment:&lt;/strong&gt; Since I can't afford a permanent GPU instance, I served the model using &lt;strong&gt;vLLM&lt;/strong&gt; inside a Colab instance, tunneled out via &lt;strong&gt;Ngrok&lt;/strong&gt; to a custom Next.js frontend. It‚Äôs janky, but it works for free.&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Blog post&lt;/h1&gt; &lt;p&gt;&lt;a href="https://hanstan.link/how-i-trained-a-high-performance-coding-model-on-a-single-gpu/"&gt;https://hanstan.link/how-i-trained-a-high-performance-coding-model-on-a-single-gpu/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I took a long time writing a deep dive into how I built Anni and the challenges I faced (Unsloth bugs, GGUF export issues, and the exact curriculum schedule). I hope that someone would be able to find it useful!&lt;/p&gt; &lt;h1&gt;Links&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Hugging Face:&lt;/strong&gt; &lt;a href="https://huggingface.co/BigJuicyData/Anni"&gt;https://huggingface.co/BigJuicyData/Anni&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GGUF:&lt;/strong&gt; &lt;a href="https://huggingface.co/BigJuicyData/Anni-Q4_K_M-GGUF"&gt;https://huggingface.co/BigJuicyData/Anni-Q4_K_M-GGUF&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Feel free to roast the model or training process! I would greatly appreciate it since I would really like to learn!&lt;/p&gt; &lt;p&gt;Cheers!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Outrageous-Yak8298"&gt; /u/Outrageous-Yak8298 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1po2slg/my_professor_lent_me_an_a6000_so_i_tried_to_build/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1po2slg/my_professor_lent_me_an_a6000_so_i_tried_to_build/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1po2slg/my_professor_lent_me_an_a6000_so_i_tried_to_build/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T14:06:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnusp9</id>
    <title>Alibaba Open-Sources CosyVoice 3, a New TTS Model</title>
    <updated>2025-12-16T06:16:09+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Key Features&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Language Coverage&lt;/strong&gt;: Covers 9 common languages (Chinese, English, Japanese, Korean, German, Spanish, French, Italian, Russian), 18+ Chinese dialects/accents and meanwhile supports both multi-lingual/cross-lingual zero-shot voice cloning.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Content Consistency &amp;amp; Naturalness&lt;/strong&gt;: Achieves state-of-the-art performance in content consistency, speaker similarity, and prosody naturalness.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Pronunciation Inpainting&lt;/strong&gt;: Supports pronunciation inpainting of Chinese Pinyin and English CMU phonemes, providing more controllability and thus suitable for production use.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Text Normalization&lt;/strong&gt;: Supports reading of numbers, special symbols and various text formats without a traditional frontend module.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Bi-Streaming&lt;/strong&gt;: Support both text-in streaming and audio-out streaming, and achieves latency as low as 150ms while maintaining high-quality audio output.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Instruct Support&lt;/strong&gt;: Supports various instructions such as languages, dialects, emotions, speed, volume, etc.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Weight: &lt;a href="https://huggingface.co/FunAudioLLM/Fun-CosyVoice3-0.5B-2512"&gt;https://huggingface.co/FunAudioLLM/Fun-CosyVoice3-0.5B-2512&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Paper: &lt;a href="https://arxiv.org/abs/2505.17589"&gt;https://arxiv.org/abs/2505.17589&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnusp9/alibaba_opensources_cosyvoice_3_a_new_tts_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnusp9/alibaba_opensources_cosyvoice_3_a_new_tts_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pnusp9/alibaba_opensources_cosyvoice_3_a_new_tts_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T06:16:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnfaqo</id>
    <title>I'm strong enough to admit that this bugs the hell out of me</title>
    <updated>2025-12-15T18:40:57+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnfaqo/im_strong_enough_to_admit_that_this_bugs_the_hell/"&gt; &lt;img alt="I'm strong enough to admit that this bugs the hell out of me" src="https://preview.redd.it/9xkz6sfcxe7g1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=55149229a153c6c87d61ae1aa53e61a1b3a65df8" title="I'm strong enough to admit that this bugs the hell out of me" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9xkz6sfcxe7g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnfaqo/im_strong_enough_to_admit_that_this_bugs_the_hell/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pnfaqo/im_strong_enough_to_admit_that_this_bugs_the_hell/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T18:40:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnz80z</id>
    <title>I may have over-quantized this little guy.</title>
    <updated>2025-12-16T11:04:26+00:00</updated>
    <author>
      <name>/u/AllergicToTeeth</name>
      <uri>https://old.reddit.com/user/AllergicToTeeth</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnz80z/i_may_have_overquantized_this_little_guy/"&gt; &lt;img alt="I may have over-quantized this little guy." src="https://preview.redd.it/35p9o4zosj7g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5f20379a29c30a291fbfb4ccd8cb2c67757d7a55" title="I may have over-quantized this little guy." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AllergicToTeeth"&gt; /u/AllergicToTeeth &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/35p9o4zosj7g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnz80z/i_may_have_overquantized_this_little_guy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pnz80z/i_may_have_overquantized_this_little_guy/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T11:04:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1po18y9</id>
    <title>GLM-4.5V, GLM-4.6V and GLM_4.6V-Flash are now supported by llama.cpp (GGUFs)</title>
    <updated>2025-12-16T12:56:27+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1po18y9/glm45v_glm46v_and_glm_46vflash_are_now_supported/"&gt; &lt;img alt="GLM-4.5V, GLM-4.6V and GLM_4.6V-Flash are now supported by llama.cpp (GGUFs)" src="https://external-preview.redd.it/bLrsVXDvN3_NMKaZkcGBPVdeuTpEZp7rVIyw-KAF9KY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cc4cc8f4e345c1545572514e6454ad7fa760089d" title="GLM-4.5V, GLM-4.6V and GLM_4.6V-Flash are now supported by llama.cpp (GGUFs)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;you need this&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1pnz1je/support_for_glm4v_vision_encoder_has_been_merged/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1pnz1je/support_for_glm4v_vision_encoder_has_been_merged/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/ggml-org/glm-4v"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1po18y9/glm45v_glm46v_and_glm_46vflash_are_now_supported/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1po18y9/glm45v_glm46v_and_glm_46vflash_are_now_supported/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T12:56:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1po3bn4</id>
    <title>XiaomiMiMo/MiMo-V2-Flash ¬∑ Hugging Face</title>
    <updated>2025-12-16T14:29:04+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1po3bn4/xiaomimimomimov2flash_hugging_face/"&gt; &lt;img alt="XiaomiMiMo/MiMo-V2-Flash ¬∑ Hugging Face" src="https://external-preview.redd.it/pEssBYcofSIqxenRV_1O2yb3vr7ekZdMtNbDln2iEbQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=413fe05449bcb79ceb4c3c13d870125113113e50" title="XiaomiMiMo/MiMo-V2-Flash ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/XiaomiMiMo/MiMo-V2-Flash"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1po3bn4/xiaomimimomimov2flash_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1po3bn4/xiaomimimomimov2flash_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T14:29:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnz9xu</id>
    <title>Qwen3 Next speed optimization has been merged into llama.cpp</title>
    <updated>2025-12-16T11:07:37+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnz9xu/qwen3_next_speed_optimization_has_been_merged/"&gt; &lt;img alt="Qwen3 Next speed optimization has been merged into llama.cpp" src="https://external-preview.redd.it/DvlPrtOQd3Cfpjgulr94g-6gX7cbuY0-dqBY_cGanOg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=51f4d927a593a1d76b03526eda2d2fe2ba251bc9" title="Qwen3 Next speed optimization has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/17996"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnz9xu/qwen3_next_speed_optimization_has_been_merged/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pnz9xu/qwen3_next_speed_optimization_has_been_merged/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T11:07:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnxekt</id>
    <title>It was Ilya who "closed" OpenAI</title>
    <updated>2025-12-16T09:05:33+00:00</updated>
    <author>
      <name>/u/licuphand</name>
      <uri>https://old.reddit.com/user/licuphand</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnxekt/it_was_ilya_who_closed_openai/"&gt; &lt;img alt="It was Ilya who &amp;quot;closed&amp;quot; OpenAI" src="https://preview.redd.it/rn6rsl7p7j7g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9fd882c08aa9fff702ae363b643c6636cc846267" title="It was Ilya who &amp;quot;closed&amp;quot; OpenAI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/licuphand"&gt; /u/licuphand &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rn6rsl7p7j7g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnxekt/it_was_ilya_who_closed_openai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pnxekt/it_was_ilya_who_closed_openai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T09:05:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1pniwfj</id>
    <title>Ai2 Open Modeling AMA ft researchers from the Molmo and Olmo teams.</title>
    <updated>2025-12-15T21:02:55+00:00</updated>
    <author>
      <name>/u/ai2_official</name>
      <uri>https://old.reddit.com/user/ai2_official</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pniwfj/ai2_open_modeling_ama_ft_researchers_from_the/"&gt; &lt;img alt="Ai2 Open Modeling AMA ft researchers from the Molmo and Olmo teams." src="https://b.thumbs.redditmedia.com/bsv34WIHZXC49Az9mFES5lSIAtaQ2CuLZJ4dCaLxsEY.jpg" title="Ai2 Open Modeling AMA ft researchers from the Molmo and Olmo teams." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;! We‚Äôre researchers and engineers from Ai2, the nonprofit AI lab. We recently announced:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Molmo 2&lt;/strong&gt;‚Äîopen multimodal models for video + images that can return grounded answers (pixel coordinates + timestamps), trained with open datasets&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Olmo 3&lt;/strong&gt;‚Äîa family of fully open language models (7B‚Äì32B) with Base/Instruct/Thinking variants, long‚Äëcontext support, open training recipes &amp;amp; checkpoints&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Ask us anything about local inference, training mixes &amp;amp; our truly open approach, long‚Äëcontext, grounded video QA/tracking, and real‚Äëworld deployment.&lt;/p&gt; &lt;p&gt;Participating in the AMA:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Molmo 2 researchers:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Ranjay Krishna &lt;/li&gt; &lt;li&gt;Zixian Ma&lt;/li&gt; &lt;li&gt;Chris Clark ( &lt;a href="/u/mostly_reasonable"&gt;u/mostly_reasonable&lt;/a&gt; )&lt;/li&gt; &lt;li&gt;Jieyu Zhang&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Olmo 3 researchers:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Kyle Lo ( &lt;a href="/u/klstats"&gt;u/klstats&lt;/a&gt; )&lt;/li&gt; &lt;li&gt;Allyson Ettinger ( &lt;a href="/u/aeclang"&gt;u/aeclang&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;Finbarr Timbers ( &lt;a href="/u/fnbr"&gt;u/fnbr&lt;/a&gt; )&lt;/li&gt; &lt;li&gt;Faeze Brahman ( &lt;a href="/u/faebrhn"&gt;u/faebrhn&lt;/a&gt; )&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We‚Äôll be live from &lt;strong&gt;1pm&lt;/strong&gt; to &lt;strong&gt;2pm PST.&lt;/strong&gt; Read up on our latest releases below, and feel welcome to jump in anytime!&lt;/p&gt; &lt;ul&gt; &lt;li&gt;‚ñ∂Ô∏è &lt;strong&gt;Try in the Playground:&lt;/strong&gt; &lt;a href="https://playground.allenai.org"&gt;https://playground.allenai.org&lt;/a&gt;&lt;/li&gt; &lt;li&gt;‚¨áÔ∏è &lt;strong&gt;Download&lt;/strong&gt;: &lt;a href="https://huggingface.co/collections/allenai/molmo2"&gt;https://huggingface.co/collections/allenai/molmo2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;üìù &lt;strong&gt;Blog&lt;/strong&gt;: &lt;a href="https://allenai.org/blog/molmo2"&gt;https://allenai.org/blog/molmo2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;üìÑReport: &lt;a href="https://allenai.org/papers/molmo2"&gt;https://allenai.org/papers/molmo2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;üíª &lt;strong&gt;API coming soon&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;ü´Ü PROOF:&lt;/strong&gt; &lt;a href="https://x.com/allen_ai/status/2000692253606514828"&gt;https://x.com/allen_ai/status/2000692253606514828&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Join us on Reddit&lt;/strong&gt; &lt;a href="/r/allenai"&gt;r/allenai&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;Join Ai2 on Discord:&lt;/strong&gt; &lt;a href="https://discord.gg/kxC5GQF74U"&gt;https://discord.gg/kxC5GQF74U&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fxw1g2fcmf7g1.jpg?width=1080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=009a9377edfefefc5efd52db0af81b807b9971b8"&gt;https://preview.redd.it/fxw1g2fcmf7g1.jpg?width=1080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=009a9377edfefefc5efd52db0af81b807b9971b8&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ai2_official"&gt; /u/ai2_official &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pniwfj/ai2_open_modeling_ama_ft_researchers_from_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pniwfj/ai2_open_modeling_ama_ft_researchers_from_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pniwfj/ai2_open_modeling_ama_ft_researchers_from_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T21:02:55+00:00</published>
  </entry>
</feed>
