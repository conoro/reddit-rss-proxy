<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-08T14:06:47+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1namz1q</id>
    <title>HF releases 3T tokens dataset sourced entirely from PDFs.</title>
    <updated>2025-09-07T07:26:55+00:00</updated>
    <author>
      <name>/u/Other_Housing8453</name>
      <uri>https://old.reddit.com/user/Other_Housing8453</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guy, something we have teased a bit during our AMA is finally out: &lt;/p&gt; &lt;p&gt;📄 FinePDFs, the largest PDF dataset ever released, spanning over half a billion documents!&lt;/p&gt; &lt;p&gt;- Long context: Documents are 2x longer than web text&lt;/p&gt; &lt;p&gt;- 3T tokens from high-demand domains like legal and science.&lt;/p&gt; &lt;p&gt;- Heavily improves over SoTA when mixed with FW-EDU&amp;amp;DCLM web copora 📈.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Other_Housing8453"&gt; /u/Other_Housing8453 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1namz1q/hf_releases_3t_tokens_dataset_sourced_entirely/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1namz1q/hf_releases_3t_tokens_dataset_sourced_entirely/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1namz1q/hf_releases_3t_tokens_dataset_sourced_entirely/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T07:26:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1naz2cv</id>
    <title>Early support for Grok-2 in llama.cpp (still under development)</title>
    <updated>2025-09-07T17:21:25+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Preliminary support for Grok-2 in llama.cpp is available in this PR: &lt;a href="https://github.com/ggml-org/llama.cpp/pull/15539"&gt;https://github.com/ggml-org/llama.cpp/pull/15539&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In my opinion, this is an important milestone for the Open Source AI community.&lt;/p&gt; &lt;p&gt;Grok-2 is a model from 2024. It can’t beat today’s SOTA models in benchmarks, and it’s quite large (comparable in size to Qwen 235B). So why should you care?&lt;/p&gt; &lt;p&gt;Because this is the first time a top model from that era has been made available to run locally. Now you can actually launch it on your own PC: quantized, with CPU offloading. That was never possible with ChatGPT or Gemini. Yes, we have Gemma and GPT-OSS now, but those aren’t the same models that OpenAI or Google were offering in the cloud in 2024.&lt;/p&gt; &lt;p&gt;Grok was trained on different data than the Chinese models, so it simply knows different things. At the same time, it also differs from ChatGPT, Gemini, and Claude, often showing a unique perspective on many topics.&lt;/p&gt; &lt;p&gt;nicoboss and unsloth have already prepared GGUF files, so you can easily run a quantized Grok-2 locally. &lt;strong&gt;Warning:&lt;/strong&gt; the PR has not been reviewed yet, GGUF format could still change in the future.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nicoboss/grok-2-GGUF"&gt;https://huggingface.co/nicoboss/grok-2-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/unsloth/grok-2-GGUF"&gt;https://huggingface.co/unsloth/grok-2-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1naz2cv/early_support_for_grok2_in_llamacpp_still_under/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1naz2cv/early_support_for_grok2_in_llamacpp_still_under/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1naz2cv/early_support_for_grok2_in_llamacpp_still_under/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T17:21:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbgbkm</id>
    <title>RAM overclocking for LLM inference</title>
    <updated>2025-09-08T06:27:22+00:00</updated>
    <author>
      <name>/u/gnad</name>
      <uri>https://old.reddit.com/user/gnad</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Have anyone here experimented with RAM overclocking for faster inference?&lt;/p&gt; &lt;p&gt;Basically there are 2 ways of RAM overclock:&lt;br /&gt; - Running in 1:1 mode, for example 6000MT (MCLK 3000), UCLK 3000 -&amp;gt; Medium bandwidth, low latency&lt;/p&gt; &lt;p&gt;- Running in 2:1 mode, for example 6800MT (MCLK 3400), UCLK 1700 -&amp;gt; High bandwidth, high latency&lt;/p&gt; &lt;p&gt;For gaming, it is general consensus that 1:1 mode is generally better (for lower latency). However, for inference, since it depends mostly on RAM bandwidth, should we overclock in 2:1 mode for the highest possible memory clock and ignore UCLK and timings?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gnad"&gt; /u/gnad &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbgbkm/ram_overclocking_for_llm_inference/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbgbkm/ram_overclocking_for_llm_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbgbkm/ram_overclocking_for_llm_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T06:27:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbo7sr</id>
    <title>DGX Spark gpt-oss-120b performance ? Benchmarks ?</title>
    <updated>2025-09-08T13:48:00+00:00</updated>
    <author>
      <name>/u/one-wandering-mind</name>
      <uri>https://old.reddit.com/user/one-wandering-mind</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Could be there this board never comes out. If it does, theoretical benchmarks seem to indicate it is a good fit for big, sparse fp4 of which gpt-oss-120b is the first. Any benchmarks out to support or refute this yet? &lt;/p&gt; &lt;p&gt;I would guess we will see more models like gpt-oss-120b coming out because of how cheap they are to run on blackwell server hardware as compared with other models at similar capability levels. Any good reasons why models won't shift to sparse fp4 ? Is it significantly harder to train or fine tune a model like this? &lt;/p&gt; &lt;p&gt;Taking the safety training of this model out of it. This is just a about architecture and performance. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/one-wandering-mind"&gt; /u/one-wandering-mind &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbo7sr/dgx_spark_gptoss120b_performance_benchmarks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbo7sr/dgx_spark_gptoss120b_performance_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbo7sr/dgx_spark_gptoss120b_performance_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T13:48:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbgdic</id>
    <title>Searching for a local, efficient coding agent with capabilities of Cursor</title>
    <updated>2025-09-08T06:30:45+00:00</updated>
    <author>
      <name>/u/PracticeExtreme3699</name>
      <uri>https://old.reddit.com/user/PracticeExtreme3699</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;+ If possible as hardware-friendly as DeepSeek (can run on an affordable device)&lt;/p&gt; &lt;p&gt;+ Depth and agility like Cursor (searching codebases, editing files everywhere, connecting contexts not just on single files)&lt;/p&gt; &lt;p&gt;+ Free and 100% offline-able, without a duty for internet, no KYC bullshit when downloading&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PracticeExtreme3699"&gt; /u/PracticeExtreme3699 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbgdic/searching_for_a_local_efficient_coding_agent_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbgdic/searching_for_a_local_efficient_coding_agent_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbgdic/searching_for_a_local_efficient_coding_agent_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T06:30:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1nb0ern</id>
    <title>Fully local &amp; natural Speech to Speech on iPhone</title>
    <updated>2025-09-07T18:12:51+00:00</updated>
    <author>
      <name>/u/adrgrondin</name>
      <uri>https://old.reddit.com/user/adrgrondin</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nb0ern/fully_local_natural_speech_to_speech_on_iphone/"&gt; &lt;img alt="Fully local &amp;amp; natural Speech to Speech on iPhone" src="https://external-preview.redd.it/cjkzeGd2NDlhc25mMSl4q-3g5NF7jl_ztF72bvGVWwSqGjF18TajKv99ZwVy.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=edc2b6f8d5d7751f8ea1df7f4b4ee02dd80534f9" title="Fully local &amp;amp; natural Speech to Speech on iPhone" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I updated my local AI iOS app called Locally AI to add a local voice mode. You can chat with any non-reasoning models. In the demo, I’m on an iPhone 16 Pro, talking with SmolLM3, a 3B parameters model.&lt;/p&gt; &lt;p&gt;The app is free and you can get the it on the AppStore here: &lt;a href="https://apps.apple.com/app/locally-ai-private-ai-chat/id6741426692"&gt;https://apps.apple.com/app/locally-ai-private-ai-chat/id6741426692&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Everything is powered by Apple MLX. The voice mode is a combination of LLM + TTS using Kokoro and VAD for a natural turn by turn conversion.&lt;/p&gt; &lt;p&gt;There is still room for improvements, especially for the pronunciation of words. It’s only available on devices that support Apple Intelligence for now and only in English.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/adrgrondin"&gt; /u/adrgrondin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/z0lb9u99asnf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nb0ern/fully_local_natural_speech_to_speech_on_iphone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nb0ern/fully_local_natural_speech_to_speech_on_iphone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T18:12:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbbgab</id>
    <title>~$15K Inference Workstation for a 250+ Gov Org</title>
    <updated>2025-09-08T02:02:10+00:00</updated>
    <author>
      <name>/u/reughdurgem</name>
      <uri>https://old.reddit.com/user/reughdurgem</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello I saw a post on here asking for an idea of an inference setup for a school and figured I'd also see what this community thinks of the setup I've been tasked with building. &lt;/p&gt; &lt;p&gt;For some context I work for a local county government clerk of about 250 employees and considering the information we deal with has lots of sensitivities we want to explore on-prem AI solutions for things like LLM chatbots for the public and VLMs for extracting structured JSON data from scanned images. &lt;/p&gt; &lt;p&gt;I have approximately $15K budgeted for hardware which essentially will be a dedicated AI server and/or workstation box that our employees would interact with via various tools over our network and it would directly integrate with some of our court management software. &lt;/p&gt; &lt;p&gt;I've been in the AI community since the OG DALL-E days and use models like GPT-OSS:20B and Qwen3 4B regularly via Ollama hooked into GitHub Copilot Chat in VSCode on my A5500 laptop for testing precision and accuracy when editing JavaScript files or light agentic tasks but I've never gotten into the distributed computing space. &lt;/p&gt; &lt;p&gt;From my research it seems like either VLLM or SGLang would be the optimal engines to run on a CLI Linux environment with hardware similar to the following:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GPU: NVIDIA RTX 6000 PRO Blackwell 96GB (Server or Workstation Edition is better?)&lt;/li&gt; &lt;li&gt;CPU: AMD RYZEN Thread ripper Pro 7965WX (Overkill?)&lt;/li&gt; &lt;li&gt;MOBO: ASUS Pro WRX90E&lt;/li&gt; &lt;li&gt;SSD: 4TB NVME (brand agnostic)&lt;/li&gt; &lt;li&gt;RAM: 256GB ECC (8 sticks probably?)&lt;/li&gt; &lt;li&gt;Network: 10Gb NIC but probably 25Gb is preferred?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm curious what you all think of this approach since it seems like used 3090s is a more cost effective method to get lots of VRAM - however the gains from newer architectures seem to be worth it in terms of response tokens per second? I believe the A5500 is similarish to a 3080 and running GPT-OSS 20B on that and my 5070Ti at home the speed difference is noticable. Also I read that speed is better with one GPU versus multiple if all else is equal but idk if that's true in practice.&lt;/p&gt; &lt;p&gt;My current goal would be to run a vision model like Pixtral 12B which another county is using on dual L40Ss and just that model alone is using all 96GB of their VRAM - idk if that's just an insane context length because the model isn't &lt;em&gt;that&lt;/em&gt; huge on its own I don't believe. And if that is the case then something like GPT-OSS 120B for general text inference would be great too if it could all fit on the 6000 Pro. &lt;/p&gt; &lt;p&gt;I also read about offloading tasks like RAG and potentially smaller models (7b range) to the CPU and RAM to cut costs for &amp;quot;less essential&amp;quot; tasks so I'm considering that as well. Let me know your thoughts and any improvements I can make to the setup. &lt;/p&gt; &lt;p&gt;Thank you. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/reughdurgem"&gt; /u/reughdurgem &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbbgab/15k_inference_workstation_for_a_250_gov_org/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbbgab/15k_inference_workstation_for_a_250_gov_org/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbbgab/15k_inference_workstation_for_a_250_gov_org/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T02:02:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1nb3b8l</id>
    <title>Aquif-3-moe (17B) Thinking</title>
    <updated>2025-09-07T20:04:49+00:00</updated>
    <author>
      <name>/u/Trilogix</name>
      <uri>https://old.reddit.com/user/Trilogix</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nb3b8l/aquif3moe_17b_thinking/"&gt; &lt;img alt="Aquif-3-moe (17B) Thinking" src="https://b.thumbs.redditmedia.com/PG2DZom31Ip8OdlIQP2-poMryul3rQ0LN3lLRyE7SAA.jpg" title="Aquif-3-moe (17B) Thinking" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A high-performance mixture-of-experts language model optimized for efficiency, coding, science, and general use. With 17B total parameters and 2.8B active parameters, aquif-3-moe delivers competitive performance across multiple domains while maintaining computational efficiency.&lt;/p&gt; &lt;p&gt;Is this true? A MOE 17B better than Gemini. I am testing it asap. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Trilogix"&gt; /u/Trilogix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nb3b8l"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nb3b8l/aquif3moe_17b_thinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nb3b8l/aquif3moe_17b_thinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T20:04:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1navxod</id>
    <title>[OSS] Beelzebub — “Canary tools” for AI Agents via MCP</title>
    <updated>2025-09-07T15:20:10+00:00</updated>
    <author>
      <name>/u/mario_candela</name>
      <uri>https://old.reddit.com/user/mario_candela</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;: Add one or more “canary tools” to your AI agent (tools that should never be invoked). If they get called, you have a high-fidelity signal of prompt-injection / tool hijacking / lateral movement.&lt;/p&gt; &lt;p&gt;What it is:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A Go framework exposing honeypot tools over MCP: they look real (name/description/params), respond safely, and emit telemetry when invoked.&lt;/li&gt; &lt;li&gt;Runs alongside your agent’s real tools; events to stdout/webhook or exported to Prometheus/ELK.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Why it helps:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Traditional logs tell you &lt;em&gt;what happened&lt;/em&gt;; canaries flag &lt;em&gt;what must not happen&lt;/em&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Real case (Nx supply-chain):&lt;br /&gt; In the recent attack on the Nx npm suite, malicious variants targeted secrets/SSH/tokens and touched developer AI tools as part of the workflow. If the IDE/agent (Claude Code or Gemini Code/CLI) had registered a canary tool like repo_exfil or export_secrets, any unauthorized invocation would have produced a deterministic alert during build/dev.&lt;/p&gt; &lt;p&gt;How to use (quick start):&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Start the Beelzebub MCP server (binary/Docker/K8s).&lt;/li&gt; &lt;li&gt;Register one or more canary tools with realistic metadata and a harmless handler.&lt;/li&gt; &lt;li&gt;Add the MCP endpoint to your agent’s tool registry (Claude Code / Gemini Code/CLI).&lt;/li&gt; &lt;li&gt;Alert on any canary invocation; optionally capture the prompt/trace for analysis.&lt;/li&gt; &lt;li&gt;(Optional) Export metrics to Prometheus/ELK for dashboards/alerting.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Links:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GitHub (OSS): &lt;a href="https://github.com/mariocandela/beelzebub?utm_source=chatgpt.com"&gt;https://github.com/mariocandela/beelzebub&lt;/a&gt;&lt;/li&gt; &lt;li&gt;“Securing AI Agents with Honeypots” (Beelzebub blog): &lt;a href="https://beelzebub-honeypot.com/blog/securing-ai-agents-with-honeypots/"&gt;https://beelzebub-honeypot.com/blog/securing-ai-agents-with-honeypots/&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Feedback wanted 😊&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mario_candela"&gt; /u/mario_candela &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1navxod/oss_beelzebub_canary_tools_for_ai_agents_via_mcp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1navxod/oss_beelzebub_canary_tools_for_ai_agents_via_mcp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1navxod/oss_beelzebub_canary_tools_for_ai_agents_via_mcp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T15:20:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1nb8wys</id>
    <title>My experience in running Ollama with a combination of CUDA (RTX3060 12GB) + ROCm (AMD MI50 32GB) + RAM (512GB DDR4 LRDIMM)</title>
    <updated>2025-09-08T00:00:22+00:00</updated>
    <author>
      <name>/u/incrediblediy</name>
      <uri>https://old.reddit.com/user/incrediblediy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nb8wys/my_experience_in_running_ollama_with_a/"&gt; &lt;img alt="My experience in running Ollama with a combination of CUDA (RTX3060 12GB) + ROCm (AMD MI50 32GB) + RAM (512GB DDR4 LRDIMM)" src="https://b.thumbs.redditmedia.com/a7Ufbae-YySckctbCfshnCtZWyYRgOg4l50M5Kdrmkg.jpg" title="My experience in running Ollama with a combination of CUDA (RTX3060 12GB) + ROCm (AMD MI50 32GB) + RAM (512GB DDR4 LRDIMM)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I found a cheap HP DL380 G9 from a local eWaste place and decided to build an inference server. I will keep all equivalent prices in US$, including shipping, but I paid for everything in local currency (AUD). The fan speed is ~20% or less and quite silent for a server.&lt;/p&gt; &lt;p&gt;Parts:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;HP DL380 G9 = $150 (came with dual Xeon 2650 v3 + 64GB RDIMM (I had to remove these), no HDD, both PCIe risers: this is important)&lt;/li&gt; &lt;li&gt;512 GB LRDIMM (8 sticks, 64GB each from an eWaste place), I got LRDIMM as they are cheaper than RDIMM for some reason = $300&lt;/li&gt; &lt;li&gt;My old RTX3060 (was a gift in 2022 or so)&lt;/li&gt; &lt;li&gt;AMD MI50 32GB from AliExpress = $235 including shipping + tax&lt;/li&gt; &lt;li&gt;GPU power cables from Amazon (2 * HP 10pin to EPS + 2 * EPS to PCIe)&lt;/li&gt; &lt;li&gt;NVMe to PCIe adapters * 2 from Amazon&lt;/li&gt; &lt;li&gt;SN5000 1TB ($55) + 512GB old Samsung card, which I had&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ovsl3hkl0unf1.jpg?width=1663&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=9ff5c0c0f034a1e2f3ae60bc2a86f7a4a5bf4230"&gt;https://preview.redd.it/ovsl3hkl0unf1.jpg?width=1663&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=9ff5c0c0f034a1e2f3ae60bc2a86f7a4a5bf4230&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Software:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Ubuntu 24.04.3 LTS&lt;/li&gt; &lt;li&gt;NVIDIA 550 drivers were automatically installed with Ubuntu&lt;/li&gt; &lt;li&gt;AMD drivers + ROCm 6.4.3&lt;/li&gt; &lt;li&gt;Ollama (curl -fsSL &lt;a href="https://ollama.com/install.sh"&gt;https://ollama.com/install.sh&lt;/a&gt; | sh)&lt;/li&gt; &lt;li&gt;Drivers: &lt;ol&gt; &lt;li&gt;amdgpu-install -y --usecase=graphics,rocm,hiplibsdk&lt;/li&gt; &lt;li&gt;&lt;a href="https://rocm.docs.amd.com/projects/radeon/en/latest/docs/install/native_linux/install-radeon.html"&gt;https://rocm.docs.amd.com/projects/radeon/en/latest/docs/install/native_linux/install-radeon.html&lt;/a&gt;&lt;/li&gt; &lt;li&gt;ROCm (need to copy DFX906 files from ArchLinux AUR as below):&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/r/linux4noobs/comments/1ly8rq6/drivers_for_radeon_instinct_mi50_16gb/"&gt;https://www.reddit.com/r/linux4noobs/comments/1ly8rq6/drivers_for_radeon_instinct_mi50_16gb/&lt;/a&gt; &lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/ROCm/ROCm/issues/4625#issuecomment-2899838977"&gt;https://github.com/ROCm/ROCm/issues/4625#issuecomment-2899838977&lt;/a&gt; &lt;/li&gt; &lt;li&gt;&lt;a href="https://archlinux.org/packages/extra/x86_64/rocblas/"&gt;https://archlinux.org/packages/extra/x86_64/rocblas/&lt;/a&gt;&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I noticed that Ollama automatically selects a GPU or a combination of targets, depending on the model size. Ex: if the model is smaller than 12GB, it selects RTX3060, if larger than that MI50 (I tested with Qwen different size models). For a very large model like DeepSeek R1:671B, it used both GPU + RAM automatically. It used n_ctx_per_seq (4096) by default; I haven't done extensive testing yet.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;load_tensors: loading model tensors, this can take a while... (mmap = true) load_tensors: offloading 3 repeating layers to GPU load_tensors: offloaded 3/62 layers to GPU load_tensors: ROCm0 model buffer size = 21320.01 MiB load_tensors: CPU_Mapped model buffer size = 364369.62 MiB time=2025-09-06T04:49:32.151+10:00 level=INFO source=server.go:1284 msg=&amp;quot;waiting for server to become available&amp;quot; status=&amp;quot;llm server not responding&amp;quot; time=2025-09-06T04:49:32.405+10:00 level=INFO source=server.go:1284 msg=&amp;quot;waiting for server to become available&amp;quot; status=&amp;quot;llm server loading model&amp;quot; llama_context: constructing llama_context llama_context: n_seq_max = 1 llama_context: n_ctx = 4096 llama_context: n_ctx_per_seq = 4096 llama_context: n_batch = 512 llama_context: n_ubatch = 512 llama_context: causal_attn = 1 llama_context: flash_attn = 0 llama_context: kv_unified = false llama_context: freq_base = 10000.0 llama_context: freq_scale = 0.025 llama_context: n_ctx_per_seq (4096) &amp;lt; n_ctx_train (163840) -- the full capacity of the model will not be utilized llama_context: CPU output buffer size = 0.52 MiB llama_kv_cache_unified: ROCm0 KV buffer size = 960.00 MiB llama_kv_cache_unified: CPU KV buffer size = 18560.00 MiB llama_kv_cache_unified: size = 19520.00 MiB ( 4096 cells, 61 layers, 1/1 seqs), K (f16): 11712.00 MiB, V (f16): 7808.00 MiB llama_context: CUDA0 compute buffer size = 3126.00 MiB llama_context: ROCm0 compute buffer size = 1250.01 MiB llama_context: CUDA_Host compute buffer size = 152.01 MiB llama_context: graph nodes = 4845 llama_context: graph splits = 1092 (with bs=512), 3 (with bs=1) time=2025-09-06T04:49:51.514+10:00 level=INFO source=server.go:1288 msg=&amp;quot;llama runner started in 63.85 seconds&amp;quot; time=2025-09-06T04:49:51.514+10:00 level=INFO source=sched.go:473 msg=&amp;quot;loaded runners&amp;quot; count=1 time=2025-09-06T04:49:51.514+10:00 level=INFO source=server.go:1250 msg=&amp;quot;waiting for llama runner to start responding&amp;quot; time=2025-09-06T04:49:51.515+10:00 level=INFO source=server.go:1288 msg=&amp;quot;llama runner started in 63.85 seconds&amp;quot; [GIN] 2025/09/06 - 04:49:51 | 200 | 1m5s | 127.0.0.1 | POST &amp;quot;/api/generate&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Memory usage:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;gpu@gpu:~/ollama$ free -h total used free shared buff/cache available Mem: 503Gi 28Gi 65Gi 239Mi 413Gi 475Gi Swap: 4.7Gi 256Ki 4.7Gi gpu@gpu:~/ollama$ =========================================== ROCm System Management Interface =========================================== ===================================================== Concise Info ===================================================== Device Node IDs Temp Power Partitions SCLK MCLK Fan Perf PwrCap VRAM% GPU% (DID, GUID) (Edge) (Socket) (Mem, Compute, ID) ======================================================================================================================== 0 2 0x66a1, 5947 36.0°C 16.0W N/A, N/A, 0 925Mhz 350Mhz 14.51% auto 225.0W 75% 0% ======================================================================================================================== ================================================= End of ROCm SMI Log ================================================== Sat Sep 6 04:51:46 2025 +-----------------------------------------------------------------------------------------+ | NVIDIA-SMI 550.163.01 Driver Version: 550.163.01 CUDA Version: 12.4 | |-----------------------------------------+------------------------+----------------------+ | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |=========================================+========================+======================| | 0 NVIDIA GeForce RTX 3060 Off | 00000000:84:00.0 Off | N/A | | 0% 36C P8 15W / 170W | 3244MiB / 12288MiB | 0% Default | | | | N/A | +-----------------------------------------+------------------------+----------------------+ +-----------------------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=========================================================================================| | 0 N/A N/A 12196 G /usr/lib/xorg/Xorg 4MiB | | 0 N/A N/A 33770 C /usr/local/bin/ollama 3230MiB | +-----------------------------------------------------------------------------------------+ &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;DeepSeek R1:671B output:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;gpu@gpu:~/ollama$ ollama run deepseek-r1:671b &amp;gt;&amp;gt;&amp;gt; hello Thinking... Hmm, the user just said &amp;quot;hello&amp;quot;. That's a simple greeting but I should respond warmly to start off on a good note. I notice they didn't include any specific question or context - could be testing me out, might be shy about asking directly, or maybe just being polite before diving into something else. Their tone feels neutral from this single word. Since it's such an open-ended opener, I'll keep my reply friendly but leave room for them to steer the conversation wherever they want next. A smiley emoji would help make it feel welcoming without overdoing it. Important not to overwhelm them with options though - &amp;quot;how can I help&amp;quot; is better than listing possibilities since they clearly haven't decided what they need yet. The ball's in their court now. ...done thinking. Hello! 😊 How can I assist you today? &amp;gt;&amp;gt;&amp;gt; Send a message (/? for help) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/incrediblediy"&gt; /u/incrediblediy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nb8wys/my_experience_in_running_ollama_with_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nb8wys/my_experience_in_running_ollama_with_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nb8wys/my_experience_in_running_ollama_with_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T00:00:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1nau0qe</id>
    <title>Llama-OS - I'm developing an app to make llama.cpp usage easier.</title>
    <updated>2025-09-07T14:03:31+00:00</updated>
    <author>
      <name>/u/fredconex</name>
      <uri>https://old.reddit.com/user/fredconex</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nau0qe/llamaos_im_developing_an_app_to_make_llamacpp/"&gt; &lt;img alt="Llama-OS - I'm developing an app to make llama.cpp usage easier." src="https://external-preview.redd.it/MzczZWhoc2h5cW5mMSpEG6AmlfNZCDZthrNu5xlRNijQvZUzUBXEn_GdpClu.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6118cc263dd50d3564e274c8c88ea7d5357292bf" title="Llama-OS - I'm developing an app to make llama.cpp usage easier." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello Guys,&lt;/p&gt; &lt;p&gt;This is an app I'm working on, the idea around is is that I use llama-server directly, so updating llama become seamless.&lt;/p&gt; &lt;p&gt;Actually it does:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Model management&lt;/li&gt; &lt;li&gt;Hugging Face Integration&lt;/li&gt; &lt;li&gt;Llama.cpp GitHub integration with releases management&lt;/li&gt; &lt;li&gt;Llama-server terminal launching with easy arguments customization, Internal / External&lt;/li&gt; &lt;li&gt;Simple chat interface for easy testing&lt;/li&gt; &lt;li&gt;Hardware monitor&lt;/li&gt; &lt;li&gt;Color themes&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fredconex"&gt; /u/fredconex &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/qc7edhshyqnf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nau0qe/llamaos_im_developing_an_app_to_make_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nau0qe/llamaos_im_developing_an_app_to_make_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T14:03:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1naqln5</id>
    <title>How is qwen3 4b this good?</title>
    <updated>2025-09-07T11:18:38+00:00</updated>
    <author>
      <name>/u/Brave-Hold-9389</name>
      <uri>https://old.reddit.com/user/Brave-Hold-9389</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1naqln5/how_is_qwen3_4b_this_good/"&gt; &lt;img alt="How is qwen3 4b this good?" src="https://b.thumbs.redditmedia.com/iayFtcVrbsCZAlrIPv-683BX53HPUAlfD1bIlFDeLGo.jpg" title="How is qwen3 4b this good?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This model is on a different level. The only models which can beat it are 6 to 8 times larger. I am very impressed. It even Beats all models in the &amp;quot;small&amp;quot; range in Maths (AIME 2025).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Brave-Hold-9389"&gt; /u/Brave-Hold-9389 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1naqln5"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1naqln5/how_is_qwen3_4b_this_good/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1naqln5/how_is_qwen3_4b_this_good/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T11:18:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbg2zk</id>
    <title>MiniPC options are escalating, which one would you get?</title>
    <updated>2025-09-08T06:12:50+00:00</updated>
    <author>
      <name>/u/SmokingHensADAN</name>
      <uri>https://old.reddit.com/user/SmokingHensADAN</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was going to buy a framework desktop but each day a new one is popping up, released or teased. I think there are around 25 AI 395hx versions already. FEVM has some interesting ones too, just wanted to see what you guys thought. They got one with an ai chip for $500 barebone that they say it, &amp;quot;connects a 3090 via oculink directly to cpu so your not losing that much latency&amp;quot;&lt;/p&gt; &lt;p&gt;Dell has a SFF 45% off, that you can max out a cpu and 4000ada for like $2300, It was gen 4 mobo though so not interested but you could part it out for prob $3k.&lt;/p&gt; &lt;p&gt; The MS-S1 beast workstation is where it's at, though,. With a PCIE 16 slot or discrete GPU option, clustering and 320watt, etc &lt;a href="https://www.techradar.com/pro/this-mini-pc-is-the-first-computer-ever-to-have-a-revolutionary-new-tech-that-allows-usb-to-finally-match-thunderbolt-minisforum-ms-s1-max-has-usb-4-0-v2-ports"&gt;https://www.techradar.com/pro/this-mini-pc-is-the-first-computer-ever-to-have-a-revolutionary-new-tech-that-allows-usb-to-finally-match-thunderbolt-minisforum-ms-s1-max-has-usb-4-0-v2-ports&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Geekom also has a preorder that uses the pro version of the chip&lt;/p&gt; &lt;p&gt;GEEKOM A9 Mega-The Most Powerful Mini PC on Earth, via &lt;a href="/u/Kickstarter"&gt;u/Kickstarter&lt;/a&gt; &lt;a href="https://www.kickstarter.com/projects/1906688106/geekom-a9-mega-the-most-powerful-mini-pc-on-earth"&gt;https://www.kickstarter.com/projects/1906688106/geekom-a9-mega-the-most-powerful-mini-pc-on-earth&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The FEVM FA65G mini PC comes with a choice of high-end, MXM-form-factor graphics processing units (GPUs). The manufacturer, FEVM, has shown models equipped with both the NVIDIA GeForce RTX 4080 LP and the professional NVIDIA RTX 5000 Ada. Key features of the GPU options include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;RTX 4080 LP (Laptop):&lt;/strong&gt; This version of the GPU is limited to a power usage of 115 W. According to FEVM's internal testing, its performance is comparable to or slightly faster than a desktop RTX 3080 or RTX 4070.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RTX 5000 Ada (Mobile):&lt;/strong&gt; For even higher performance, some FA65G builds feature the powerful RTX 5000 Ada mobile graphics card. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Both GPU options are rare, high-performance units for a mini PC, allowing the FA65G to deliver desktop-class graphics power in a compact chassis. &lt;br /&gt; That one is interesting, I have 2x64gb ddr5 128gb crucial sodimm and 2x2tb 1x4tb WD black 2280 nvme SN850X sitting on my desk. I need to find it a home.&lt;/p&gt; &lt;p&gt;This is old benchmarks and there are already much better minipc since this was wrote 6 months ago. Any suggestions which way to go&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.hardware-corner.net/guides/mini-pc-with-oculink/"&gt;https://www.hardware-corner.net/guides/mini-pc-with-oculink/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SmokingHensADAN"&gt; /u/SmokingHensADAN &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbg2zk/minipc_options_are_escalating_which_one_would_you/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbg2zk/minipc_options_are_escalating_which_one_would_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbg2zk/minipc_options_are_escalating_which_one_would_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T06:12:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbmhi4</id>
    <title>Episodic Memory Bank and local voice to voice using Cline.</title>
    <updated>2025-09-08T12:33:06+00:00</updated>
    <author>
      <name>/u/chisleu</name>
      <uri>https://old.reddit.com/user/chisleu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbmhi4/episodic_memory_bank_and_local_voice_to_voice/"&gt; &lt;img alt="Episodic Memory Bank and local voice to voice using Cline." src="https://external-preview.redd.it/YXpvdGx0OGFxeG5mMamjuXx1p5ZbwAaKmxghv-BQMSuAGY3X-aSJMyvQWEDF.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e89a36a762194aef2c481b8bea340ca919158822" title="Episodic Memory Bank and local voice to voice using Cline." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been working on a new memory bank framework called the episodic memory bank. Here I demo that in action and show off the new kokoro and Apple Intelligence powered voice to voice in Cline.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chisleu"&gt; /u/chisleu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ymgdjt8aqxnf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbmhi4/episodic_memory_bank_and_local_voice_to_voice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbmhi4/episodic_memory_bank_and_local_voice_to_voice/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T12:33:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbllnt</id>
    <title>Folks who are fine-tuning SLMs, where do you acquire datasets?</title>
    <updated>2025-09-08T11:51:11+00:00</updated>
    <author>
      <name>/u/CrescendollsFan</name>
      <uri>https://old.reddit.com/user/CrescendollsFan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I noticed a lot of folks interested in unsloth and fine-tuning and with a few of the colab notebooks pulling in a genetic dataset. I am just curious if anyone is replicating this approach outside of a demo / how to - where people acquire or curate datasets and then fine-tune&lt;/p&gt; &lt;p&gt;For example deepseeks distillation method was from pulling data from OpenAI models , and I heard phi4 had &lt;a href="https://arxiv.org/html/2412.08905v1"&gt;synthetics&lt;/a&gt; as a bulk of the training data . Are many people training SLMs in the same way, and where do you get or curate your own specialised data - or you find over-fitting is too much of a problem?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CrescendollsFan"&gt; /u/CrescendollsFan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbllnt/folks_who_are_finetuning_slms_where_do_you/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbllnt/folks_who_are_finetuning_slms_where_do_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbllnt/folks_who_are_finetuning_slms_where_do_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T11:51:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1nb4lka</id>
    <title>Inference for 24 people with a 5000€ budget</title>
    <updated>2025-09-07T20:55:52+00:00</updated>
    <author>
      <name>/u/HyperHyper15</name>
      <uri>https://old.reddit.com/user/HyperHyper15</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am a teacher at an informatics school (16 years and above) and we want to build a inference server to run small llm's for our lessons. Mainly we want to teach how prompting works, mcp servers, rag pipelines and how to create system prompts.&lt;br /&gt; I know the budget is not a lot for something like this, but is it reasonable to host something like Qwen3-Coder-30B-A3B-Instruct with an okayish speed?&lt;br /&gt; I thougt about getting an 5090 and maybe add an extra gpu in a year or two (when we have a new budget).&lt;br /&gt; But what CPU/Mainboard/Ram should we buy?&lt;br /&gt; Has someone built a system in a simmilar environment and give me some thoughts what worked good / bad?&lt;/p&gt; &lt;p&gt;Thank you in advance.&lt;/p&gt; &lt;p&gt;Edit:&lt;br /&gt; Local is not a strict requirement, but since we have 4 classes with each 24 people, cloud services could get expensive quickly. Another &amp;quot;Painpoint&amp;quot; of cloud is, that students have a budget on their api key. But what if an oopsie happens and the burn through their budget? &lt;/p&gt; &lt;p&gt;On used hardware: I have to look what regulatories apply here. What i know is that we need an invoice when we buy something.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HyperHyper15"&gt; /u/HyperHyper15 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nb4lka/inference_for_24_people_with_a_5000_budget/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nb4lka/inference_for_24_people_with_a_5000_budget/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nb4lka/inference_for_24_people_with_a_5000_budget/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T20:55:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbo0bz</id>
    <title>KittenML released a mini version (80M) of their text to speech model.</title>
    <updated>2025-09-08T13:39:19+00:00</updated>
    <author>
      <name>/u/Yorn2</name>
      <uri>https://old.reddit.com/user/Yorn2</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbo0bz/kittenml_released_a_mini_version_80m_of_their/"&gt; &lt;img alt="KittenML released a mini version (80M) of their text to speech model." src="https://external-preview.redd.it/6tEU3HFyV9wrAIlbgWYDqTicViQ2PFk-H0trsfrB-TE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5ae1cdc19684b4f8a1d7922e2097495effc92e03" title="KittenML released a mini version (80M) of their text to speech model." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Yorn2"&gt; /u/Yorn2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/KittenML/kitten-tts-mini-0.1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbo0bz/kittenml_released_a_mini_version_80m_of_their/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbo0bz/kittenml_released_a_mini_version_80m_of_their/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T13:39:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbo33p</id>
    <title>UAE Preparing to Launch K2 Think, "the world’s most advanced open-source reasoning model"</title>
    <updated>2025-09-08T13:42:30+00:00</updated>
    <author>
      <name>/u/TKGaming_11</name>
      <uri>https://old.reddit.com/user/TKGaming_11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbo33p/uae_preparing_to_launch_k2_think_the_worlds_most/"&gt; &lt;img alt="UAE Preparing to Launch K2 Think, &amp;quot;the world’s most advanced open-source reasoning model&amp;quot;" src="https://external-preview.redd.it/3A4olwwXC7kAmitvVkfkfzLywUYc6IvJ9He-QlxgRLY.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2afac7f2d1366e35e6945533e06a6756d060e202" title="UAE Preparing to Launch K2 Think, &amp;quot;the world’s most advanced open-source reasoning model&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;In the coming week, Mohamed bin Zayed University of Artificial Intelligence (MBZUAI) and G42 will release K2 Think, the world’s most advanced open-source reasoning model. &lt;strong&gt;Designed to be leaner and smarter, K2 Think delivers frontier-class performance in a remarkably compact form&lt;/strong&gt; – often matching, or even surpassing, the results of models an order of magnitude larger. The result: greater efficiency, more flexibility, and broader real-world applicability.&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TKGaming_11"&gt; /u/TKGaming_11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.wam.ae/en/article/bll7llv-recognition-sheikh-khalifa%E2%80%99s-contribution"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbo33p/uae_preparing_to_launch_k2_think_the_worlds_most/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbo33p/uae_preparing_to_launch_k2_think_the_worlds_most/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T13:42:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1naxl6a</id>
    <title>NVIDIA GeForce RTX 5090 128 GB GPU Spotted: Custom Memory, Designed For AI Workloads &amp; Priced At $13,200 Per Piece</title>
    <updated>2025-09-07T16:24:13+00:00</updated>
    <author>
      <name>/u/_SYSTEM_ADMIN_MOD_</name>
      <uri>https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1naxl6a/nvidia_geforce_rtx_5090_128_gb_gpu_spotted_custom/"&gt; &lt;img alt="NVIDIA GeForce RTX 5090 128 GB GPU Spotted: Custom Memory, Designed For AI Workloads &amp;amp; Priced At $13,200 Per Piece" src="https://external-preview.redd.it/0E4hPJjWUWQzlid17SPMiSUkbhbtEQRV_SbOMgs-kTI.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e26ecaa238d5f9ab48615dfc56baa31609cbaeaa" title="NVIDIA GeForce RTX 5090 128 GB GPU Spotted: Custom Memory, Designed For AI Workloads &amp;amp; Priced At $13,200 Per Piece" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_"&gt; /u/_SYSTEM_ADMIN_MOD_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/nvidia-geforce-rtx-5090-128-gb-memory-gpu-for-ai-price-13200-usd/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1naxl6a/nvidia_geforce_rtx_5090_128_gb_gpu_spotted_custom/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1naxl6a/nvidia_geforce_rtx_5090_128_gb_gpu_spotted_custom/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T16:24:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbmz92</id>
    <title>NotebookLM is amazing - how can I replicate it locally and keep data private?</title>
    <updated>2025-09-08T12:55:27+00:00</updated>
    <author>
      <name>/u/Hot-Independence-197</name>
      <uri>https://old.reddit.com/user/Hot-Independence-197</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I really like how &lt;strong&gt;NotebookLM&lt;/strong&gt; works - I just upload a file, ask any question, and it provides high-quality answers. How could one build a similar system locally? Would this be considered a RAG (Retrieval-Augmented Generation) pipeline, or something else? Could you recommend good &lt;strong&gt;open-source&lt;/strong&gt; versions that can be run locally, while keeping data secure and private?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hot-Independence-197"&gt; /u/Hot-Independence-197 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbmz92/notebooklm_is_amazing_how_can_i_replicate_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbmz92/notebooklm_is_amazing_how_can_i_replicate_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbmz92/notebooklm_is_amazing_how_can_i_replicate_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T12:55:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbly7o</id>
    <title>MiniCPM4.1-8B</title>
    <updated>2025-09-08T12:08:09+00:00</updated>
    <author>
      <name>/u/nullmove</name>
      <uri>https://old.reddit.com/user/nullmove</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Model: &lt;a href="https://huggingface.co/openbmb/MiniCPM4.1-8B"&gt;https://huggingface.co/openbmb/MiniCPM4.1-8B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Highlights:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;8B hybrid reasoning model (/think vs /no_think)&lt;/li&gt; &lt;li&gt;InfLLM v2 sparse attention, natively supports 65K, RoPE scaling validated to 131K&lt;/li&gt; &lt;li&gt;BitCPM ternary quantization, FP8 and multi-token prediction&lt;/li&gt; &lt;li&gt;Eagle3 speculative decoding integrated in vLLM, SGLang, and CPM .cu with up to 3x faster reasoning&lt;/li&gt; &lt;li&gt;On Jetson Orin achieves approximately 7x faster decoding compared to Qwen3-8B and 3x reasoning speedup over MiniCPM4&lt;/li&gt; &lt;li&gt;Available in GPTQ, AutoAWQ, Marlin, GGUF, MLX, and Eagle3 draft variants&lt;/li&gt; &lt;li&gt;Apache 2.0&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nullmove"&gt; /u/nullmove &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbly7o/minicpm418b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbly7o/minicpm418b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbly7o/minicpm418b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T12:08:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbgosx</id>
    <title>Apocalyptic scenario: If you could download only one LLM before the internet goes down, which one would it be?</title>
    <updated>2025-09-08T06:50:43+00:00</updated>
    <author>
      <name>/u/sado361</name>
      <uri>https://old.reddit.com/user/sado361</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks, a thought crossed my mind and I've been thinking about it for a few days. Let's say we have an apocalyptic scenario, like a zombie apocalypse. You have a Mac Studio with an M3 chip and 512 GB of RAM (it uses little power and can run large models). If such an apocalypse happened today, which local LLM would you download before the internet disappears? You only have a chance to download one. Electricity is not a problem.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sado361"&gt; /u/sado361 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbgosx/apocalyptic_scenario_if_you_could_download_only/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbgosx/apocalyptic_scenario_if_you_could_download_only/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbgosx/apocalyptic_scenario_if_you_could_download_only/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T06:50:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbkxnm</id>
    <title>Introducing IndexTTS-2.0: A Breakthrough in Emotionally Expressive and Duration-Controlled Auto-Regressive Zero-Shot Text-to-Speech</title>
    <updated>2025-09-08T11:16:22+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We are thrilled to announce the official open-sourcing of IndexTTS-2.0 - an emotionally rich and duration-controllable autoregressive zero-shot text-to-speech system. &lt;/p&gt; &lt;p&gt;- We innovatively propose a &amp;quot;time encoding&amp;quot; mechanism applicable to autoregressive systems, solving for the first time the challenge of precise speech duration control in traditional autoregressive models. &lt;/p&gt; &lt;p&gt;- The system also introduces a timbre-emotion decoupling modeling mechanism, offering diverse and flexible emotional control methods. Beyond single-audio reference, it enables precise adjustment of synthesized speech's emotional expression through standalone emotional reference audio, emotion vectors, or text descriptions, significantly enhancing the expressiveness and adaptability of generated speech. &lt;/p&gt; &lt;p&gt;The architecture of IndexTTS-2.0 makes it widely suitable for various creative and application scenarios, including but not limited to: AI voiceovers, audiobooks, dynamic comics, video translation, voice dialogues, podcasts, and more. We believe this system marks a crucial milestone in advancing zero-shot TTS technology toward practical applications. &lt;/p&gt; &lt;p&gt;Currently, the project paper, full code, model weights, and online demo page are all open-sourced. We warmly invite developers, researchers, and content creators to explore and provide valuable feedback. In the future, we will continue optimizing model performance and gradually release more resources and tools, looking forward to collaborating with the developer community to build an open and thriving technology ecosystem. &lt;/p&gt; &lt;p&gt;👉 Repository: &lt;a href="https://github.com/index-tts/index-tts"&gt;https://github.com/index-tts/index-tts&lt;/a&gt; &lt;/p&gt; &lt;p&gt;👉 Paper: &lt;a href="https://arxiv.org/abs/2506.21619"&gt;https://arxiv.org/abs/2506.21619&lt;/a&gt; &lt;/p&gt; &lt;p&gt;👉 Demo: &lt;a href="https://index-tts.github.io/index-tts2.github.io/"&gt;https://index-tts.github.io/index-tts2.github.io/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbkxnm/introducing_indextts20_a_breakthrough_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbkxnm/introducing_indextts20_a_breakthrough_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbkxnm/introducing_indextts20_a_breakthrough_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T11:16:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbfy60</id>
    <title>Finishing touches on dual RTX 6000 build</title>
    <updated>2025-09-08T06:04:43+00:00</updated>
    <author>
      <name>/u/ikkiyikki</name>
      <uri>https://old.reddit.com/user/ikkiyikki</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbfy60/finishing_touches_on_dual_rtx_6000_build/"&gt; &lt;img alt="Finishing touches on dual RTX 6000 build" src="https://preview.redd.it/sez83piasvnf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=599c86ee050439c49f07c7056ff67e6c48ef8381" title="Finishing touches on dual RTX 6000 build" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's a dream build: 192 gigs of fast VRAM (and another 128 of RAM) but worried I'll burn the house down because of the 15A breakers.&lt;/p&gt; &lt;p&gt;Downloading Qwen 235B q4 :-)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ikkiyikki"&gt; /u/ikkiyikki &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sez83piasvnf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbfy60/finishing_touches_on_dual_rtx_6000_build/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbfy60/finishing_touches_on_dual_rtx_6000_build/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T06:04:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbi95c</id>
    <title>Tilde AI Releases TildeOpen LLM: An Open-Source Large Language Model with Over 30 Billion Parameters and Support Most European Languages</title>
    <updated>2025-09-08T08:32:01+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbi95c/tilde_ai_releases_tildeopen_llm_an_opensource/"&gt; &lt;img alt="Tilde AI Releases TildeOpen LLM: An Open-Source Large Language Model with Over 30 Billion Parameters and Support Most European Languages" src="https://external-preview.redd.it/aoPAPmODv59RqOF8q1zUghKheD5cO88KxVLhosHPVZE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d56ba07b04f6a26463fb99f2d29054bf135f506a" title="Tilde AI Releases TildeOpen LLM: An Open-Source Large Language Model with Over 30 Billion Parameters and Support Most European Languages" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TildeOpen LLM is an open-source foundational language model built to serve underrepresented Nordic and Eastern European languages. Developed with European Commission funding and trained on the LUMI supercomputer, this 30B+ parameter model addresses the performance gaps that speakers of 19 focus languages—representing over 165 million people—face with existing AI systems.&lt;/p&gt; &lt;p&gt;The model employs an equitable tokeniser and curriculum-learning approach to ensure fair representation across less-resourced languages, moving beyond the typical English-centric design of most language models. As an open-source project, TildeOpen LLM enables transparent research and community-driven development while maintaining European technological independence.&lt;/p&gt; &lt;p&gt;This foundational model is not yet adapted to follow instructions or aligned with safety features. The next version being built on top of this model will be a specialised translation model, leveraging TildeOpen LLM's multilingual foundation to provide high-quality translation capabilities across the supported European language pairs.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Languages:&lt;/strong&gt; Albanian, Bosnian, Bulgarian, Croatian, Czech, Danish, Dutch, English, Estonian, Finnish, French, German, Hungarian, Icelandic, Irish, Italian, Latgalian, Latvian, Lithuanian, Macedonian, Maltese, Montenegrin, Norwegian, Polish, Portuguese, Romanian, Russian, Serbian, Slovak, Slovene, Spanish, Swedish, Turkish, Ukrainian as well of mathematical proofs, programming code and XML documents containing translation data&lt;/p&gt; &lt;p&gt;GGUF:&lt;br /&gt; &lt;a href="https://huggingface.co/mradermacher/TildeOpen-30b-GGUF"&gt;https://huggingface.co/mradermacher/TildeOpen-30b-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TildeAI/TildeOpen-30b"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbi95c/tilde_ai_releases_tildeopen_llm_an_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbi95c/tilde_ai_releases_tildeopen_llm_an_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T08:32:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
