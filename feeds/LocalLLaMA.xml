<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-10T20:37:58+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1q96x42</id>
    <title>Your favorite Claude replacement and MCPs</title>
    <updated>2026-01-10T15:29:24+00:00</updated>
    <author>
      <name>/u/val_in_tech</name>
      <uri>https://old.reddit.com/user/val_in_tech</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Opencode with searchNG/context7 seems like a solid combo. The closest I've seen to Claude Code so far. What are you favorites?&lt;/p&gt; &lt;p&gt;I also tried to run CC with own model served via Anthropic compatible endpoint on VLLM. It works, but haven't been using long enough. Its nice that the web searches go thru their servers.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/val_in_tech"&gt; /u/val_in_tech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q96x42/your_favorite_claude_replacement_and_mcps/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q96x42/your_favorite_claude_replacement_and_mcps/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q96x42/your_favorite_claude_replacement_and_mcps/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-10T15:29:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1q92olo</id>
    <title>Qwen3-VL for OCR: PDF pre-processing + prompt approach?</title>
    <updated>2026-01-10T12:18:41+00:00</updated>
    <author>
      <name>/u/Intelligent-Form6624</name>
      <uri>https://old.reddit.com/user/Intelligent-Form6624</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been testing VLMs for OCR of PDF documents. Mainly contracts with a simple layout. Conversion to markdown or JSON is preferred. &lt;/p&gt; &lt;p&gt;So far, I’ve mainly used specialised OCR models such as Deepseek-OCR and olmOCR 2.&lt;/p&gt; &lt;p&gt;However, I’ve noticed many commenters in this forum praising Qwen3-VL. So I plan on trying Qwen3-VL-30B-A3B-Instruct.&lt;/p&gt; &lt;p&gt;It seems most specialised OCR models have accompanying Python packages that take care of pre-processing and prompting.&lt;/p&gt; &lt;p&gt;What about Qwen3? Is there a preferred package or approach for processing the PDF and presenting it to the model?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Intelligent-Form6624"&gt; /u/Intelligent-Form6624 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q92olo/qwen3vl_for_ocr_pdf_preprocessing_prompt_approach/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q92olo/qwen3vl_for_ocr_pdf_preprocessing_prompt_approach/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q92olo/qwen3vl_for_ocr_pdf_preprocessing_prompt_approach/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-10T12:18:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1q97dky</id>
    <title>Entropy-Adaptive Finetuning</title>
    <updated>2026-01-10T15:47:29+00:00</updated>
    <author>
      <name>/u/netikas</name>
      <uri>https://old.reddit.com/user/netikas</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys! I did a review on a recent paper for my peers and decided it would be cool to post it here too. This is a translation from Russian via opus 4.5, I’ve checked everything, but some mistakes might have slipped. Sorry for that!&lt;/p&gt; &lt;p&gt;___&lt;/p&gt; &lt;p&gt;Fine-tuning models is hard. My master’s thesis advisor once said it’s more alchemy than science — I don’t fully agree, but there’s something to it. Wrong hyperparameters — model diverged. Dataset too small — model diverged. Too many epochs — model diverged. Used a dataset with a distribution too different from pretraining — model forgot everything it learned during previous stages, then diverged.&lt;/p&gt; &lt;p&gt;Naturally, this state of affairs doesn’t sit well with us, so people started devising methods to work around this problem. In GOLD guys from HF used distillation from the model before finetuning to restore the quality of finetuned model on a general domain — but that adds extra complexity to the training recipe, which we’d rather avoid. Today’s paper attempts to solve the problem of catastrophic forgetting during SFT without additional steps — just through a small modification to the loss.&lt;/p&gt; &lt;p&gt;Consider the standard SFT loss — cross-entropy. We train the model to approximate logprobs for the entire target sequence equally for each token, regardless of whether the tokens are “beneficial” or “harmful” for the model. So if a token’s signal happens to be “harmful,” the model will learn from it just like from all others, leading to forgetting.&lt;/p&gt; &lt;p&gt;The authors define token “harmfulness” as follows: low entropy and confidence within top-K means the model is confident about which token it wants to pick (low entropy), but this token doesn’t match the label (low label probability at that position). This creates a confident conflict — the model learned some bias during pretraining, and now during SFT this bias isn’t confirmed, essentially making it OOD. Consequently, training produces large gradients, weights change significantly, and we risk forgetting part of the pretraining knowledge.&lt;/p&gt; &lt;p&gt;As a preliminary experiment, the authors tried training the model while masking 15% of tokens with the lowest confidence and probability — and got significantly less catastrophic forgetting compared to base SFT. However, the model also learned less, so a more precise approach is needed.&lt;/p&gt; &lt;p&gt;As an improvement, the authors decided to modify standard cross-entropy with an adaptive gating mechanism — they simply multiplied the logarithm in the loss by H_t / ln(K), where H_t is the entropy over top-K, and ln(K) is the maximum entropy over top-K. So when entropy is low, the coefficient approaches zero, the loss scales down, and the model changes its weights less. Meanwhile, when entropy is high, the coefficient approaches one, and the model learns as usual. Since this is done per-token, gradients change not in scale (as they would with lower lr in SGD, for example) but in direction (since different tokens have different scales), and the model forgets less. Very elegant.&lt;/p&gt; &lt;p&gt;For experiments, they trained Qwen3-4b-Instruct, Qwen-2.5-32b-Instruct, and GLM4-9b-0414 on math, medical, and function calling, measuring the quality on these domains and some general benchmarks (MMLU, IFEval, etc) to see how much the model learns and forgets. Baselines included vanilla SFT, SFT with KL-divergence (KL was calculated in relevance to the original model), FLOW (per-sequence downweighting of dangerous samples, as I understand it), DFT (scaling loss by token probability instead of entropy), and TALR (scaling per-token loss based on gradient norm). The proposed method turned out to be the best in regards to forgetting-learning ratio among all tested approaches.&lt;/p&gt; &lt;p&gt;Additionally, the authors checked what happens if you use f(H_t) instead of H_t as the coefficient—maybe the scaling is actually nonlinear. They tried H_t^p, Sigmoid(H_t), and the aforementioned Masked SFT, but the vanilla approach proved best.&lt;/p&gt; &lt;p&gt;My thoughts:&lt;/p&gt; &lt;p&gt;- It’s rare that such a simple and elegant idea works. Huge respect to the authors.&lt;/p&gt; &lt;p&gt;- I think there will be problems when using a very different domain — for example, when adapting a model to another language, the model will not train as well since it’ll be OOD for it.&lt;/p&gt; &lt;p&gt;- An even bigger problem will emerge when switching to text that tokenizes worse. For instance, in Russian, English-centric models have many more tokens per word—so the word “выкобениваться” (a longer slang word, which is rarely used so is not really prevalent in the pretraining corpus) will have low entropy with low label probability on all tokens except the first — again, it’s a rare word, and continuing a word is easier than starting it. This means the whole sequence loss will shift, and something nasty might emerge. Word boundaries will also be problematic — as the model expects a different language and different tokens, it won’t learn to start words in the new language.&lt;/p&gt; &lt;p&gt;- Despite all this, it looks like a decent and relatively cheap way to improve robustness for small domain-specific tunes. Something like Gemma really needs this, because that model is fragile and easy to break.&lt;/p&gt; &lt;p&gt;Here’s the link to the paper, if you’re interested: &lt;a href="https://www.arxiv.org/abs/2601.02151"&gt;https://www.arxiv.org/abs/2601.02151&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/netikas"&gt; /u/netikas &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q97dky/entropyadaptive_finetuning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q97dky/entropyadaptive_finetuning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q97dky/entropyadaptive_finetuning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-10T15:47:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1q9abdh</id>
    <title>Vibe Voice 1.5 B setup help!</title>
    <updated>2026-01-10T17:41:32+00:00</updated>
    <author>
      <name>/u/Mysterious-Comment94</name>
      <uri>https://old.reddit.com/user/Mysterious-Comment94</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9abdh/vibe_voice_15_b_setup_help/"&gt; &lt;img alt="Vibe Voice 1.5 B setup help!" src="https://b.thumbs.redditmedia.com/UgyV54y0Q3QPwckfXBGkpcuxS09mhigLAVzrHsySaeo.jpg" title="Vibe Voice 1.5 B setup help!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I was trying to setup the vibe voice 1.5 B model which is no longer available officially so I used this repo:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/rsxdalv/VibeVoice"&gt;https://github.com/rsxdalv/VibeVoice&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I set it up in google colab. I ran the gradio file in the demo folder to run my interface and this is what I got.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/bzs8d0w86kcg1.png?width=1817&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=66eb7f4f42691b688e25792f198c526a73390f72"&gt;https://preview.redd.it/bzs8d0w86kcg1.png?width=1817&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=66eb7f4f42691b688e25792f198c526a73390f72&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I feel like I am doing something wrong here. Wasn't there supposed to voice cloning and all other good things? Obviously something went wrong here. Can anyone please give me a bit of guidance on how can I get the real thing?&lt;/p&gt; &lt;p&gt;Edit: I finally found something on this repo from an old youtube video. &lt;a href="https://github.com/harry2141985"&gt;https://github.com/harry2141985&lt;/a&gt;&lt;br /&gt; This person got some google collab notebooks and a clone of vibevoice and surprisingly his version had the upload voice section I was looking for. However the quality of the generation was horrendous. So... I still might be doing something wrong here.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/caji6c0ofkcg1.png?width=1112&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e622db692687f2f81d6893c8e71ee8bd162946e2"&gt;https://preview.redd.it/caji6c0ofkcg1.png?width=1112&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e622db692687f2f81d6893c8e71ee8bd162946e2&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mysterious-Comment94"&gt; /u/Mysterious-Comment94 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9abdh/vibe_voice_15_b_setup_help/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9abdh/vibe_voice_15_b_setup_help/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q9abdh/vibe_voice_15_b_setup_help/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-10T17:41:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1q9csk3</id>
    <title>Offloom Update, private web searching RAG added. My personal, locally powered, privacy first chatbot that uses small language models yet still somehow returns quality answers. Apparently SLMs paired with agentic behavior can compete with chatGPT</title>
    <updated>2026-01-10T19:15:51+00:00</updated>
    <author>
      <name>/u/Little-Put6364</name>
      <uri>https://old.reddit.com/user/Little-Put6364</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9csk3/offloom_update_private_web_searching_rag_added_my/"&gt; &lt;img alt="Offloom Update, private web searching RAG added. My personal, locally powered, privacy first chatbot that uses small language models yet still somehow returns quality answers. Apparently SLMs paired with agentic behavior can compete with chatGPT" src="https://external-preview.redd.it/c2o2am53Y2hra2NnMT4Y6BQ8jdJqfQ-1VABkhNdVh0RAKUk7WqR4J_3U91_y.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=adb66c65441112a69210f0c68866a5494ac09a9e" title="Offloom Update, private web searching RAG added. My personal, locally powered, privacy first chatbot that uses small language models yet still somehow returns quality answers. Apparently SLMs paired with agentic behavior can compete with chatGPT" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been working on my own private chatbot for awhile now. I wanted a private, locally hosted chatbot that I could use in place of chatGPT. I already have document RAG working very well, and figured the next logical step was to bundle a private web searching framework alongside it. &lt;/p&gt; &lt;p&gt;I'm a windows user, so searXNG isn't necessarily embeddable into this application while still allowing a one click download for an end user. So I choose Whoogle instead.&lt;/p&gt; &lt;p&gt;This is fully runnable on my 4090 (I think it would work on 12GB VRAM as well I just don't have a machine for testing that). It uses an agentic approach juggling between multiple models to ensure quality answers. The powerhouse model is Qwen 8B thinking model. Which gives surprisingly good results when context is engineered properly. &lt;/p&gt; &lt;p&gt;Offloom is now capable of document and web search RAG as well as image generation using comfyUI as a sidecar process. I've evolved the idea away from just simply a chatbot and want to create a local 'entertainment' center. So future plans include the ability to agentically generate coherent short stories, comics, music, text adventures, and who knows what else lol. &lt;/p&gt; &lt;p&gt;&lt;em&gt;This isn't a public project. It's simply a learning platform for me to mess around with while still being pleasant to use. I wasn't convinced I'd be able to replace chatGPT up until thinking models came into being. Now quality answers happen the vast majority of the time meaning this project went from learning to something I can actually use.&lt;/em&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Little-Put6364"&gt; /u/Little-Put6364 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/2084smchkkcg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9csk3/offloom_update_private_web_searching_rag_added_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q9csk3/offloom_update_private_web_searching_rag_added_my/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-10T19:15:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1q9dh54</id>
    <title>Built a personal knowledge system with nomic-embed-text + LanceDB - 106K vectors, 256ms queries</title>
    <updated>2026-01-10T19:41:52+00:00</updated>
    <author>
      <name>/u/Signal_Usual8630</name>
      <uri>https://old.reddit.com/user/Signal_Usual8630</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Embedded 3 years of my AI conversations (353K messages) to make them searchable by concept, not just keywords.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Stack:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;nomic-embed-text-v1.5 (768 dims, runs on Apple Silicon MPS)&lt;/li&gt; &lt;li&gt;LanceDB for vector storage&lt;/li&gt; &lt;li&gt;DuckDB for analytics&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Performance:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;106K vectors in 440MB&lt;/li&gt; &lt;li&gt;256ms semantic search&lt;/li&gt; &lt;li&gt;13-15 msg/sec embedding throughput on M4 Mac&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Key learning:&lt;/strong&gt; Started with DuckDB VSS extension. Accidentally created duplicate HNSW indexes - ended up with 14GB for 300MB of actual data. Migrated to LanceDB, same vectors in 440MB. 32x smaller.&lt;/p&gt; &lt;p&gt;Open source: &lt;a href="https://github.com/mordechaipotash/intellectual-dna"&gt;https://github.com/mordechaipotash/intellectual-dna&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Signal_Usual8630"&gt; /u/Signal_Usual8630 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9dh54/built_a_personal_knowledge_system_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9dh54/built_a_personal_knowledge_system_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q9dh54/built_a_personal_knowledge_system_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-10T19:41:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1q911fj</id>
    <title>Choosing a GGUF Model: K-Quants, I-Quants, and Legacy Formats</title>
    <updated>2026-01-10T10:42:09+00:00</updated>
    <author>
      <name>/u/tarruda</name>
      <uri>https://old.reddit.com/user/tarruda</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q911fj/choosing_a_gguf_model_kquants_iquants_and_legacy/"&gt; &lt;img alt="Choosing a GGUF Model: K-Quants, I-Quants, and Legacy Formats" src="https://external-preview.redd.it/ZMZ25MOMBEhnH9XTLPWbE9gFhyIqUzt4o8mr7UwQxDI.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=53ebe8dd0a29609cff4356a2a17066fd5e68c6a2" title="Choosing a GGUF Model: K-Quants, I-Quants, and Legacy Formats" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tarruda"&gt; /u/tarruda &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://kaitchup.substack.com/p/choosing-a-gguf-model-k-quants-i"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q911fj/choosing_a_gguf_model_kquants_iquants_and_legacy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q911fj/choosing_a_gguf_model_kquants_iquants_and_legacy/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-10T10:42:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1q9dq8b</id>
    <title>Tecent's WeDLM theoretically allows 3-10x TG for Memory-Constrained Devices (E.g. RAM, CPU/GPU Hybrid Inference)</title>
    <updated>2026-01-10T19:51:43+00:00</updated>
    <author>
      <name>/u/ImJustHereToShare25</name>
      <uri>https://old.reddit.com/user/ImJustHereToShare25</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I was thinking about &lt;a href="https://wedlm.github.io/"&gt;Tecent's WeDLM&lt;/a&gt; architecture. Long story short: they post train a normal auto-regressive llm into a diffusion model that predicts the next ~2-14 tokens (depending on complexity of the task, typical for code is like 3) at a threshold confidence per forward pass.&lt;/p&gt; &lt;p&gt;In a memory constrained environment, say DDR5/DDR4 and CPU + GPU hybrid setups, the thing we're all waiting on is weights to load in and out of our compute. Unless you are doing very sophisticated work with agentic tasks in parallel, you (we) are all likely not using that compute fully. This WeDLM arch essentially does multi-token prediction in a forward pass with a KV cache just like auto-regressive MLA, and has similar quality output (i.e. almost identical to single token auto-regressive results). &lt;/p&gt; &lt;p&gt;The reason DLM's can be faster, is they can load say 1/2 of the weights into VRAM, and do that part of the pass for say 5 tokens, and then load the next 1/2 of the weights and do that part of the pass on those 5 tokens. So: in one memory load of all the weights, we have calculated 5 tokens worth of information, instead of just 1. The reason it's variable (2-14) is that confidence is task specific. They offer counting from 1-100 as an example of a dead simple task and that's where that 14 tokens per forward pass max is achieved.&lt;/p&gt; &lt;p&gt;WeDLM seems to be a post-training solution, and seems like it would work best for Dense models since the same weights are used for all passes - say a Qwen3-32B running at 3x normal RAM fallback inference speeds. &lt;/p&gt; &lt;p&gt;Has anyone else noticed this as a bottleneck solution for Memory Constrained (i.e. 90% of local llama users) compute, and is there a reason I'm wrong on this assumption, and has LLama.cpp started work yet on supporting WeDLM or DLM's in general? &lt;/p&gt; &lt;p&gt;I would expect this to allow Dense models to get a bit closer to their MOE counterparts in speed, while keeping their quality higher. Finally, DLM's work by requiring the predicted tokens reach a certain confidence interval before accepting the token - I suspect in some situations, you could get away with tuning down that dial and effectively running a &amp;quot;flash&amp;quot; version of the same model, with identical weights, and do so even within the same inference pass (technically). Sounds like a great improvement for local inference - 2-5x token generation speeds for dense models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ImJustHereToShare25"&gt; /u/ImJustHereToShare25 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9dq8b/tecents_wedlm_theoretically_allows_310x_tg_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9dq8b/tecents_wedlm_theoretically_allows_310x_tg_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q9dq8b/tecents_wedlm_theoretically_allows_310x_tg_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-10T19:51:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1q9dvkv</id>
    <title>Workflow: Bypassing 2FA/Captchas for local web agents (Llama 3/Browser Use) by syncing Chrome cookies</title>
    <updated>2026-01-10T19:57:34+00:00</updated>
    <author>
      <name>/u/Bubbly_Gap6378</name>
      <uri>https://old.reddit.com/user/Bubbly_Gap6378</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been building local agents using Llama 3 and &lt;code&gt;browser-use&lt;/code&gt; to automate some tasks on LinkedIn and Gmail.&lt;/p&gt; &lt;p&gt;The biggest headache I hit was that the agents kept getting blocked by login screens or 2FA prompts. I didn't want to use paid APIs, and hardcoding cookies into my &lt;code&gt;.env&lt;/code&gt; file kept breaking because the sessions would expire every few days.&lt;/p&gt; &lt;p&gt;I realized the easiest fix was to just &amp;quot;borrow&amp;quot; the active session from my local Chrome browser.&lt;/p&gt; &lt;p&gt;I wrote a quick Python SDK that:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Grabs the encrypted cookies from your local Chrome profile.&lt;/li&gt; &lt;li&gt;Decrypts them locally.&lt;/li&gt; &lt;li&gt;Injects them into Playwright/Selenium so the agent starts &amp;quot;logged in.&amp;quot;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;It’s working well for my Llama 3 + Playwright setup. It’s open source if anyone else is hitting the same wall with their local agents.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Repo:&lt;/strong&gt; &lt;a href="https://github.com/jacobgadek/agent-auth"&gt;https://github.com/jacobgadek/agent-auth&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Has anyone found a better way to handle session persistence for long-running local agents?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bubbly_Gap6378"&gt; /u/Bubbly_Gap6378 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9dvkv/workflow_bypassing_2facaptchas_for_local_web/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9dvkv/workflow_bypassing_2facaptchas_for_local_web/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q9dvkv/workflow_bypassing_2facaptchas_for_local_web/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-10T19:57:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1q8x9yp</id>
    <title>Minisforum BD395i MAX motherboard at CES 2026: built-in AMD Strix Halo APU, use your own GPU</title>
    <updated>2026-01-10T06:52:19+00:00</updated>
    <author>
      <name>/u/noiserr</name>
      <uri>https://old.reddit.com/user/noiserr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8x9yp/minisforum_bd395i_max_motherboard_at_ces_2026/"&gt; &lt;img alt="Minisforum BD395i MAX motherboard at CES 2026: built-in AMD Strix Halo APU, use your own GPU" src="https://external-preview.redd.it/KG0Myw_6qN7QIlQJb4mBoOF1kj92CgMaeoC8dUJj7QQ.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0ad0e2ebc40d196f43927457a486ec1a5070565a" title="Minisforum BD395i MAX motherboard at CES 2026: built-in AMD Strix Halo APU, use your own GPU" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/noiserr"&gt; /u/noiserr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.tweaktown.com/news/109642/minisforum-bd395i-max-motherboard-at-ces-2026-built-in-amd-strix-halo-apu-use-your-own-gpu/index.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8x9yp/minisforum_bd395i_max_motherboard_at_ces_2026/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q8x9yp/minisforum_bd395i_max_motherboard_at_ces_2026/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-10T06:52:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1q8tdcz</id>
    <title>Introducing "UITPSDT" a novel approach to runtime efficiency in organic agents</title>
    <updated>2026-01-10T03:30:19+00:00</updated>
    <author>
      <name>/u/reto-wyss</name>
      <uri>https://old.reddit.com/user/reto-wyss</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8tdcz/introducing_uitpsdt_a_novel_approach_to_runtime/"&gt; &lt;img alt="Introducing &amp;quot;UITPSDT&amp;quot; a novel approach to runtime efficiency in organic agents" src="https://preview.redd.it/73wv4f66yfcg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=87519585cbcaf77c9e5c007fc061d1e497362a78" title="Introducing &amp;quot;UITPSDT&amp;quot; a novel approach to runtime efficiency in organic agents" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It is a proof of concept and application outside of the proposed domain may yield unexpected results, we hope the community can contribute to the token efficiency.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/reto-wyss"&gt; /u/reto-wyss &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/73wv4f66yfcg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8tdcz/introducing_uitpsdt_a_novel_approach_to_runtime/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q8tdcz/introducing_uitpsdt_a_novel_approach_to_runtime/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-10T03:30:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1q9e5q9</id>
    <title>GLM 4.7 is... crazy</title>
    <updated>2026-01-10T20:08:34+00:00</updated>
    <author>
      <name>/u/No-Selection2972</name>
      <uri>https://old.reddit.com/user/No-Selection2972</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9e5q9/glm_47_is_crazy/"&gt; &lt;img alt="GLM 4.7 is... crazy" src="https://b.thumbs.redditmedia.com/SrQfuqnZ5tXkMKmAli-ewMkHbEblEZlgDclRS8iplDY.jpg" title="GLM 4.7 is... crazy" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i was talking with glm in claude code and he outputted me this, it's crazy how models are evolving to make this texts, while this is half true, it will be true in the future, just look at a year before, deepseek R1 was the SOTA open-source model&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/0w6n0n3zwkcg1.png?width=1738&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=831022f78e70c5657192cb1e43426330148a36ea"&gt;https://preview.redd.it/0w6n0n3zwkcg1.png?width=1738&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=831022f78e70c5657192cb1e43426330148a36ea&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Selection2972"&gt; /u/No-Selection2972 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9e5q9/glm_47_is_crazy/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9e5q9/glm_47_is_crazy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q9e5q9/glm_47_is_crazy/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-10T20:08:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1q97081</id>
    <title>Quantized KV Cache</title>
    <updated>2026-01-10T15:32:47+00:00</updated>
    <author>
      <name>/u/val_in_tech</name>
      <uri>https://old.reddit.com/user/val_in_tech</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Have you tried to compare different quantized KV options for your local models? What's considered a sweet spot? Is performance degradation consistent across different models or is it very model specific?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/val_in_tech"&gt; /u/val_in_tech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q97081/quantized_kv_cache/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q97081/quantized_kv_cache/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q97081/quantized_kv_cache/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-10T15:32:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1q9by7w</id>
    <title>Preview logprobs in Open WebUI</title>
    <updated>2026-01-10T18:44:05+00:00</updated>
    <author>
      <name>/u/Everlier</name>
      <uri>https://old.reddit.com/user/Everlier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9by7w/preview_logprobs_in_open_webui/"&gt; &lt;img alt="Preview logprobs in Open WebUI" src="https://external-preview.redd.it/dWwycnFtMGJla2NnMaEwWMFVZEhNLMuQhxpftmmu_kTUOseThBPm8C0qkT1T.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=492a7ba88c5b57f4f44c2ee4148af4eadd5bada1" title="Preview logprobs in Open WebUI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;What is this?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;A specially crafted HTML artifact that connects back to the custom OpenAI-compatible proxy and listens to the same chunks as displayed in the UI itself, but with the logprobs data. Tokens outside of top 25% bucket are highlighted when chosen.&lt;/p&gt; &lt;p&gt;You can find the source here: &lt;a href="https://github.com/av/harbor/blob/main/boost/src/modules/logprobs.py"&gt;https://github.com/av/harbor/blob/main/boost/src/modules/logprobs.py&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Everlier"&gt; /u/Everlier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/pecxh50bekcg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9by7w/preview_logprobs_in_open_webui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q9by7w/preview_logprobs_in_open_webui/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-10T18:44:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1q8hqgd</id>
    <title>I clustered 3 DGX Sparks that NVIDIA said couldn't be clustered yet...took 1500 lines of C to make it work</title>
    <updated>2026-01-09T19:27:29+00:00</updated>
    <author>
      <name>/u/Ok-Pomegranate1314</name>
      <uri>https://old.reddit.com/user/Ok-Pomegranate1314</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8hqgd/i_clustered_3_dgx_sparks_that_nvidia_said_couldnt/"&gt; &lt;img alt="I clustered 3 DGX Sparks that NVIDIA said couldn't be clustered yet...took 1500 lines of C to make it work" src="https://preview.redd.it/dban4j25kdcg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b4a2061b8c9510f486ad4475d7a5f9b8d3a666f7" title="I clustered 3 DGX Sparks that NVIDIA said couldn't be clustered yet...took 1500 lines of C to make it work" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;NVIDIA officially supports clustering &lt;em&gt;two&lt;/em&gt; DGX Sparks together. I wanted three.&lt;/p&gt; &lt;p&gt;The problem: each Spark has two 100Gbps ConnectX-7 ports. In a 3-node triangle mesh, each link ends up on a different subnet. NCCL's built-in networking assumes all peers are reachable from a single NIC. It just... doesn't work.&lt;/p&gt; &lt;p&gt;So I wrote a custom NCCL network plugin from scratch.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it does:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Subnet-aware NIC selection (picks the right NIC for each peer)&lt;/li&gt; &lt;li&gt;Raw RDMA verbs implementation (QP state machines, memory registration, completion queues)&lt;/li&gt; &lt;li&gt;Custom TCP handshake protocol to avoid deadlocks&lt;/li&gt; &lt;li&gt;~1500 lines of C&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The result:&lt;/strong&gt; Distributed inference across all 3 nodes at 8+ GB/s over RDMA. &lt;strong&gt;The NVIDIA support tier I'm currently on:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;├── Supported configs ✓ ├── &amp;quot;Should work&amp;quot; configs ├── &amp;quot;You're on your own&amp;quot; configs ├── &amp;quot;Please don't call us&amp;quot; configs ├── &amp;quot;How did you even...&amp;quot; configs └── You are here → &amp;quot;Writing custom NCCL plugins to cluster standalone workstations over a hand-wired RDMA mesh&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;GitHub link: &lt;a href="https://github.com/autoscriptlabs/nccl-mesh-plugin"&gt;https://github.com/autoscriptlabs/nccl-mesh-plugin&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer questions about the implementation. This was a mass of low-level debugging (segfaults, RDMA state machine issues, GID table problems) but it works.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Pomegranate1314"&gt; /u/Ok-Pomegranate1314 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/dban4j25kdcg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8hqgd/i_clustered_3_dgx_sparks_that_nvidia_said_couldnt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q8hqgd/i_clustered_3_dgx_sparks_that_nvidia_said_couldnt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-09T19:27:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1q94cbp</id>
    <title>MiniMax 2.1 - Very impressed with performance</title>
    <updated>2026-01-10T13:41:13+00:00</updated>
    <author>
      <name>/u/JustinPooDough</name>
      <uri>https://old.reddit.com/user/JustinPooDough</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been developing my own agent from scratch as a hobby or over a year now - constantly changing things and tinkering with new ideas. &lt;/p&gt; &lt;p&gt;For a lot of time, open source models sucked at what I was doing. They would output intelligible text with logical fallacies or just make bad decisions. For example, for the code writing tool my agent used, I had to always switch to Claude sonnet or better - which would &lt;em&gt;mostly&lt;/em&gt; get it right. Even with the agentic stuff, sometimes the open source models would miss stuff, etc.&lt;/p&gt; &lt;p&gt;I recently tried swapping in MiniMax2.1, and holy shit - it's the first open model that actually keeps up with Claude. And when I say that, I mean I cannot actually tell the difference between them during execution of my agent.&lt;/p&gt; &lt;p&gt;Minimax 2.1 consistently get's code right within the same number of attempts as Claude. The only time I see a difference is when the code is more complicated and requires a lot more edge case exploration.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;tl;dr: Long been a skeptic of open source models in actual practise -&lt;/strong&gt; &lt;strong&gt;Minimax 2.1 blew me away.&lt;/strong&gt; I have completely switched to Minimax 2.1 due to cost savings and nearly identical performance.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;PS.&lt;/strong&gt; GLM 4.7 might be equally good, but the Claude Code plan I subscribed to with &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt; would not let me use my API key for regular client requests - only their work plan. Does anyone know of a way around this limitation?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JustinPooDough"&gt; /u/JustinPooDough &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q94cbp/minimax_21_very_impressed_with_performance/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q94cbp/minimax_21_very_impressed_with_performance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q94cbp/minimax_21_very_impressed_with_performance/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-10T13:41:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1q9bnqc</id>
    <title>RTX 50 Super GPUs may be delayed indefinitely, as Nvidia prioritizes AI during memory shortage (rumor, nothing official)</title>
    <updated>2026-01-10T18:33:05+00:00</updated>
    <author>
      <name>/u/3090orBust</name>
      <uri>https://old.reddit.com/user/3090orBust</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9bnqc/rtx_50_super_gpus_may_be_delayed_indefinitely_as/"&gt; &lt;img alt="RTX 50 Super GPUs may be delayed indefinitely, as Nvidia prioritizes AI during memory shortage (rumor, nothing official)" src="https://external-preview.redd.it/o9dwYZagSuAh6lGApyCDxDEoIhgXfz9GzoXhVGYG6FI.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cbb419ac1006074d3e7d893a55075c0de8698965" title="RTX 50 Super GPUs may be delayed indefinitely, as Nvidia prioritizes AI during memory shortage (rumor, nothing official)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/3090orBust"&gt; /u/3090orBust &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.notebookcheck.net/RTX-50-Super-GPUs-may-be-delayed-indefinitely-as-Nvidia-prioritizes-AI-during-memory-shortage.1199980.0.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9bnqc/rtx_50_super_gpus_may_be_delayed_indefinitely_as/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q9bnqc/rtx_50_super_gpus_may_be_delayed_indefinitely_as/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-10T18:33:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1q8wv24</id>
    <title>GLM 5 Is Being Trained!</title>
    <updated>2026-01-10T06:28:58+00:00</updated>
    <author>
      <name>/u/Few_Painter_5588</name>
      <uri>https://old.reddit.com/user/Few_Painter_5588</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8wv24/glm_5_is_being_trained/"&gt; &lt;img alt="GLM 5 Is Being Trained!" src="https://b.thumbs.redditmedia.com/moDsJ3Fi93wAh8NCr_tIBITFa49S-yRZWrZUjXB4v-A.jpg" title="GLM 5 Is Being Trained!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/lc29bfu0ugcg1.png?width=696&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=881458479675304548c6a39e72ed0a8b90f5b54a"&gt;https://preview.redd.it/lc29bfu0ugcg1.png?width=696&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=881458479675304548c6a39e72ed0a8b90f5b54a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Announced after their IPO&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Few_Painter_5588"&gt; /u/Few_Painter_5588 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8wv24/glm_5_is_being_trained/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8wv24/glm_5_is_being_trained/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q8wv24/glm_5_is_being_trained/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-10T06:28:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1q9bj5j</id>
    <title>I made a website to turn any confusing UI into a step-by-step guide via screen sharing (open source)</title>
    <updated>2026-01-10T18:28:17+00:00</updated>
    <author>
      <name>/u/bullmeza</name>
      <uri>https://old.reddit.com/user/bullmeza</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9bj5j/i_made_a_website_to_turn_any_confusing_ui_into_a/"&gt; &lt;img alt="I made a website to turn any confusing UI into a step-by-step guide via screen sharing (open source)" src="https://preview.redd.it/yrb4rq69ekcg1.gif?width=640&amp;amp;crop=smart&amp;amp;s=22c6cdbfa015e9f0b501163a5ab174d6aa588f3f" title="I made a website to turn any confusing UI into a step-by-step guide via screen sharing (open source)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built Screen Vision, an &lt;strong&gt;open source website&lt;/strong&gt; that guides you through any task by screen sharing with AI.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Privacy Focused:&lt;/strong&gt; Your screen data is &lt;strong&gt;never&lt;/strong&gt; stored or used to train models. &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Local LLM Support:&lt;/strong&gt; If you don't trust cloud APIs, the app has a &amp;quot;Local Mode&amp;quot; that connects to local AI models running on your own machine. Your data never leaves your computer.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Web-Native:&lt;/strong&gt; No desktop app or extension required. Works directly on your browser.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;How it works:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Instruction &amp;amp; Grounding:&lt;/strong&gt; The system uses GPT-5.2 to determine the next logical step based on your goal and current screen state. These instructions are then passed to Qwen 3VL (30B), which identifies the exact screen coordinates for the action.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Visual Verification:&lt;/strong&gt; The app monitors your screen for changes every 200ms using a pixel-comparison loop. Once a change is detected, it compares before and after snapshots using Gemini 3 Flash to confirm the step was completed successfully before automatically moving to the next task.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Source Code:&lt;/strong&gt; &lt;a href="https://github.com/bullmeza/screen.vision"&gt;https://github.com/bullmeza/screen.vision&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;Demo:&lt;/strong&gt; &lt;a href="https://screen.vision/"&gt;https://screen.vision&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I’m looking for feedback, please let me know what you think!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bullmeza"&gt; /u/bullmeza &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/yrb4rq69ekcg1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9bj5j/i_made_a_website_to_turn_any_confusing_ui_into_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q9bj5j/i_made_a_website_to_turn_any_confusing_ui_into_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-10T18:28:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1q922fv</id>
    <title>GPT OSS + Qwen VL</title>
    <updated>2026-01-10T11:43:44+00:00</updated>
    <author>
      <name>/u/Serious_Molasses313</name>
      <uri>https://old.reddit.com/user/Serious_Molasses313</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q922fv/gpt_oss_qwen_vl/"&gt; &lt;img alt="GPT OSS + Qwen VL" src="https://external-preview.redd.it/ZHdvYWFqaG5laWNnMW9SjiD2lW6nDnuEDR_4iEKM_w8YRwqIaLhq7MVR_4-G.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=61c12a19b6a10c13d4077e69539e6e203457e09c" title="GPT OSS + Qwen VL" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Figured out how to squeeze these two model on my system without crashing. Now GPT OSS reaches out to qwen for visual confirmation. &lt;/p&gt; &lt;p&gt;Before you ask what MCP server this is (I made it)&lt;/p&gt; &lt;p&gt;My specs are 6GBVRAM 32GBDDR5 &lt;/p&gt; &lt;h1&gt;PrivacyOverConvenience&lt;/h1&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Serious_Molasses313"&gt; /u/Serious_Molasses313 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/k60z16hneicg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q922fv/gpt_oss_qwen_vl/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q922fv/gpt_oss_qwen_vl/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-10T11:43:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1q959am</id>
    <title>Strix Halo (Bosgame M5) + 7900 XTX eGPU: Local LLM Benchmarks (Llama.cpp vs vLLM). A loose follow-up</title>
    <updated>2026-01-10T14:20:31+00:00</updated>
    <author>
      <name>/u/reujea0</name>
      <uri>https://old.reddit.com/user/reujea0</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is a loose follow-up to my &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1q189os/7900_xtx_rocm_a_year_later_llamacpp_vs_vllm/"&gt;previous article regarding the 7900 XTX&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;I recently got my hands on a Strix Halo system, specifically the &lt;strong&gt;Bosgame M5&lt;/strong&gt;. My goal was to benchmark the Strix Halo standalone (which is a beast), and then see what effects adding a 7900 XTX via eGPU (TB3/USB4) would have on performance.&lt;/p&gt; &lt;h1&gt;The Setup&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Host:&lt;/strong&gt; Bosgame M5 (Strix Halo)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;OS:&lt;/strong&gt; Fedora Server 43&lt;/li&gt; &lt;li&gt;&lt;strong&gt;eGPU:&lt;/strong&gt; 7900 XTX (Connected via USB4/TB3)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Toolboxes:&lt;/strong&gt; Huge thanks to &lt;a href="https://github.com/kyuz0"&gt;kyuz0 on GitHub&lt;/a&gt; for the &lt;a href="https://github.com/kyuz0/amd-strix-halo-toolboxes"&gt;llama.cpp toolboxes&lt;/a&gt; and &lt;a href="https://github.com/kyuz0/amd-strix-halo-vllm-toolboxes"&gt;vLLM toolboxes&lt;/a&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Critical Tip for eGPU users:&lt;/strong&gt; To prevent the whole system from becoming unresponsive when activating the Thunderbolt enclosure, I had to add the following kernel parameter: &lt;code&gt;pcie_port_pm=off&lt;/code&gt; (Found this solution online, it's a lifesaver for stability).&lt;/p&gt; &lt;h1&gt;Part 1: Strix Halo Standalone (Llama.cpp)&lt;/h1&gt; &lt;p&gt;I first ran the same models used in my previous 7900 XTX post, plus some larger ones that didn't fit on the 7900 XTX alone. &lt;em&gt;Backend: ROCm&lt;/em&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Size&lt;/th&gt; &lt;th align="left"&gt;Params&lt;/th&gt; &lt;th align="left"&gt;PP (512)&lt;/th&gt; &lt;th align="left"&gt;Gen (tg512)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Llama-3.1-8B-Instruct&lt;/strong&gt; (BF16)&lt;/td&gt; &lt;td align="left"&gt;14.96 GB&lt;/td&gt; &lt;td align="left"&gt;8B&lt;/td&gt; &lt;td align="left"&gt;950 t/s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;112.27 t/s&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Mistral-Small-3.2-24B&lt;/strong&gt; (Q5_K_XL)&lt;/td&gt; &lt;td align="left"&gt;15.63 GB&lt;/td&gt; &lt;td align="left"&gt;24B&lt;/td&gt; &lt;td align="left"&gt;405 t/s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;42.10 t/s&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;DeepSeek-R1-Distill-Qwen-32B&lt;/strong&gt; (Q3_K_M)&lt;/td&gt; &lt;td align="left"&gt;14.84 GB&lt;/td&gt; &lt;td align="left"&gt;32B&lt;/td&gt; &lt;td align="left"&gt;311 t/s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;42.26 t/s&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;gpt-oss-20b&lt;/strong&gt; (F16)&lt;/td&gt; &lt;td align="left"&gt;12.83 GB&lt;/td&gt; &lt;td align="left"&gt;20B&lt;/td&gt; &lt;td align="left"&gt;797 t/s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;49.62 t/s&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;gpt-oss-20b&lt;/strong&gt; (MXFP4)&lt;/td&gt; &lt;td align="left"&gt;11.27 GB&lt;/td&gt; &lt;td align="left"&gt;20B&lt;/td&gt; &lt;td align="left"&gt;766 t/s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;69.69 t/s&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Qwen3-VL-30B-Thinking&lt;/strong&gt; (Q4_K_XL)&lt;/td&gt; &lt;td align="left"&gt;16.49 GB&lt;/td&gt; &lt;td align="left"&gt;30B&lt;/td&gt; &lt;td align="left"&gt;1118 t/s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;65.45 t/s&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;gpt-oss-120b&lt;/strong&gt; (MXFP4)&lt;/td&gt; &lt;td align="left"&gt;59.02 GB&lt;/td&gt; &lt;td align="left"&gt;116B&lt;/td&gt; &lt;td align="left"&gt;612 t/s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;49.07 t/s&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;GLM-4.6V&lt;/strong&gt; (Q4_K_M)&lt;/td&gt; &lt;td align="left"&gt;65.60 GB&lt;/td&gt; &lt;td align="left"&gt;106B&lt;/td&gt; &lt;td align="left"&gt;294 t/s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;19.85 t/s&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;MiniMax-M2.1&lt;/strong&gt; (Q3_K_M)&lt;/td&gt; &lt;td align="left"&gt;101.76 GB&lt;/td&gt; &lt;td align="left"&gt;228B&lt;/td&gt; &lt;td align="left"&gt;210 t/s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;26.24 t/s&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Part 2: Strix Halo (iGPU) + 7900 XTX (eGPU) Split&lt;/h1&gt; &lt;p&gt;I wanted to see if offloading to the eGPU helped. I used &lt;code&gt;llama-serve&lt;/code&gt; with a custom Python script to measure throughput. These were all done with a context of 4K.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Strategy:&lt;/strong&gt; 1:1 split for small models; maximized 7900 XTX load for large models.&lt;/li&gt; &lt;/ul&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Split Config&lt;/th&gt; &lt;th align="left"&gt;iGPU Only&lt;/th&gt; &lt;th align="left"&gt;Split (iGPU+dGPU)&lt;/th&gt; &lt;th align="left"&gt;Improvement&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Llama-3.1-8B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;1:1&lt;/td&gt; &lt;td align="left"&gt;112.61 t/s&lt;/td&gt; &lt;td align="left"&gt;~167.7 t/s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;+49%&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Mistral-Small-24B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;1:1&lt;/td&gt; &lt;td align="left"&gt;42.10 t/s&lt;/td&gt; &lt;td align="left"&gt;~58.9 t/s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;+40%&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;DeepSeek-R1-Distill-32B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;1:1&lt;/td&gt; &lt;td align="left"&gt;42.26 t/s&lt;/td&gt; &lt;td align="left"&gt;~53.2 t/s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;+26%&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;gpt-oss-20b&lt;/strong&gt; (F16)&lt;/td&gt; &lt;td align="left"&gt;1:1&lt;/td&gt; &lt;td align="left"&gt;50.09 t/s&lt;/td&gt; &lt;td align="left"&gt;61.17 t/s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;+22%&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;gpt-oss-20b&lt;/strong&gt; (MXFP4)&lt;/td&gt; &lt;td align="left"&gt;1:1&lt;/td&gt; &lt;td align="left"&gt;70.27 t/s&lt;/td&gt; &lt;td align="left"&gt;78.01 t/s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;+11%&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Qwen3-VL-30B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;1:1&lt;/td&gt; &lt;td align="left"&gt;65.23 t/s&lt;/td&gt; &lt;td align="left"&gt;57.50 t/s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;-12%&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;gpt-oss-120b&lt;/strong&gt; (MXFP4)&lt;/td&gt; &lt;td align="left"&gt;24:3&lt;/td&gt; &lt;td align="left"&gt;49.35 t/s&lt;/td&gt; &lt;td align="left"&gt;54.56 t/s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;+11%&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;GLM-4.6V&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;2:1&lt;/td&gt; &lt;td align="left"&gt;20.54 t/s&lt;/td&gt; &lt;td align="left"&gt;23.46 t/s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;+14%&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;MiniMax-M2.1&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;17:5&lt;/td&gt; &lt;td align="left"&gt;26.22 t/s&lt;/td&gt; &lt;td align="left"&gt;27.19 t/s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;+4%&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Observations:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Adding the eGPU is beneficial for smaller, dense models where we get a &lt;strong&gt;~50% boost&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;However, for larger models or MoEs, the &lt;strong&gt;USB4/TB3 bandwidth&lt;/strong&gt; likely becomes a bottleneck. The latency introduced by splitting the model across the interconnect kills the gains, leading to diminishing returns (+4% to +14%) or even regression (-12% on Qwen3-VL).&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Part 3: vLLM on Strix Halo&lt;/h1&gt; &lt;p&gt;The situation with vLLM is a bit rougher. I wasn't willing to wrestle with multi-GPU configuration here, so these results are &lt;strong&gt;Strix Halo Single GPU only&lt;/strong&gt;.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Output Speed (tok/s)&lt;/th&gt; &lt;th align="left"&gt;TTFT (Mean)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;gpt-oss-20b&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;25.87 t/s&lt;/td&gt; &lt;td align="left"&gt;1164 ms&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Llama-3.1-8B-Instruct&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;17.34 t/s&lt;/td&gt; &lt;td align="left"&gt;633 ms&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Mistral-Small-24B&lt;/strong&gt; (bnb-4bit)&lt;/td&gt; &lt;td align="left"&gt;4.23 t/s&lt;/td&gt; &lt;td align="left"&gt;3751 ms&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;gpt-oss-20b&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;25.37 t/s&lt;/td&gt; &lt;td align="left"&gt;3625 ms&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;gpt-oss-120b&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;15.5 t/s&lt;/td&gt; &lt;td align="left"&gt;4458&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;vLLM support on ROCm (specifically for Strix Halo/consumer cards) seems to be lagging behind llama.cpp significantly. The generation speeds are much lower, and the Time To First Token (TTFT) is quite high. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/reujea0"&gt; /u/reujea0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q959am/strix_halo_bosgame_m5_7900_xtx_egpu_local_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q959am/strix_halo_bosgame_m5_7900_xtx_egpu_local_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q959am/strix_halo_bosgame_m5_7900_xtx_egpu_local_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-10T14:20:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1q90ye2</id>
    <title>Jensen Huang at CES on how open models have really revolutionized AI last year. “When AI is open, it proliferates everywhere.”</title>
    <updated>2026-01-10T10:36:58+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q90ye2/jensen_huang_at_ces_on_how_open_models_have/"&gt; &lt;img alt="Jensen Huang at CES on how open models have really revolutionized AI last year. “When AI is open, it proliferates everywhere.”" src="https://external-preview.redd.it/bGVlcWZ0bzcxaWNnMW_K1BNM1KBv7FYngB2itMlTyoA2GP6X-h0KJFWgL9Yw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=35c5c46d790cb187582f60967603f3cda5bc1020" title="Jensen Huang at CES on how open models have really revolutionized AI last year. “When AI is open, it proliferates everywhere.”" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From NVIDIA AI on 𝕏: &lt;a href="https://x.com/NVIDIAAI/status/2009731908888895516"&gt;https://x.com/NVIDIAAI/status/2009731908888895516&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/73l3tyn71icg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q90ye2/jensen_huang_at_ces_on_how_open_models_have/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q90ye2/jensen_huang_at_ces_on_how_open_models_have/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-10T10:36:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1q8ckz0</id>
    <title>The reason why RAM has become so expensive</title>
    <updated>2026-01-09T16:18:22+00:00</updated>
    <author>
      <name>/u/InvadersMustLive</name>
      <uri>https://old.reddit.com/user/InvadersMustLive</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8ckz0/the_reason_why_ram_has_become_so_expensive/"&gt; &lt;img alt="The reason why RAM has become so expensive" src="https://preview.redd.it/sgbhubsomccg1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=57d847c1b9a2d5786b0a888b5d0d25fe5ede9e12" title="The reason why RAM has become so expensive" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InvadersMustLive"&gt; /u/InvadersMustLive &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sgbhubsomccg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8ckz0/the_reason_why_ram_has_become_so_expensive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q8ckz0/the_reason_why_ram_has_become_so_expensive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-09T16:18:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1q998is</id>
    <title>Visualizing RAG, PART 2- visualizing retrieval</title>
    <updated>2026-01-10T16:59:58+00:00</updated>
    <author>
      <name>/u/Fear_ltself</name>
      <uri>https://old.reddit.com/user/Fear_ltself</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q998is/visualizing_rag_part_2_visualizing_retrieval/"&gt; &lt;img alt="Visualizing RAG, PART 2- visualizing retrieval" src="https://external-preview.redd.it/d2kxeWY2ZzV6amNnMV4lBvCWgLP7SJvsjxaLS786SW2_ibHBrw3ehpZ9iEQq.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=950c6e4a869e18d9d6e90193c9adaaa7fb875e40" title="Visualizing RAG, PART 2- visualizing retrieval" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Edit: code is live at &lt;a href="https://github.com/CyberMagician/Project_Golem"&gt;https://github.com/CyberMagician/Project_Golem&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Still editing the repository but basically just download the requirements (from requirements txt), run the python ingest to build out the brain you see here in LanceDB real quick, then launch the backend server and front end visualizer.&lt;/p&gt; &lt;p&gt;Using UMAP and some additional code to visualizing the 768D vector space of EmbeddingGemma:300m down to 3D and how the RAG “thinks” when retrieving relevant context chunks. How many nodes get activated with each query. It is a follow up from my previous post that has a lot more detail in the comments there about how it’s done. Feel free to ask questions I’ll answer when I’m free&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fear_ltself"&gt; /u/Fear_ltself &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/mrkuplj5zjcg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q998is/visualizing_rag_part_2_visualizing_retrieval/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q998is/visualizing_rag_part_2_visualizing_retrieval/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-10T16:59:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
