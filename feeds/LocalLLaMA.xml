<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-12T09:38:16+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1mn98w0</id>
    <title>Vllm documentation is garbage</title>
    <updated>2025-08-11T10:23:37+00:00</updated>
    <author>
      <name>/u/dennisitnet</name>
      <uri>https://old.reddit.com/user/dennisitnet</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Wtf is this documentation, vllm? Incomplete and so cluttered. You need someone to help with your shtty documentation&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dennisitnet"&gt; /u/dennisitnet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn98w0/vllm_documentation_is_garbage/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn98w0/vllm_documentation_is_garbage/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mn98w0/vllm_documentation_is_garbage/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T10:23:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1mn5fe6</id>
    <title>Apple patents matmul technique in GPU</title>
    <updated>2025-08-11T06:17:07+00:00</updated>
    <author>
      <name>/u/auradragon1</name>
      <uri>https://old.reddit.com/user/auradragon1</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/auradragon1"&gt; /u/auradragon1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://patentscope.wipo.int/search/en/detail.jsf?docId=US452614511&amp;amp;_cid=P12-M8WPOS-61919-1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn5fe6/apple_patents_matmul_technique_in_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mn5fe6/apple_patents_matmul_technique_in_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T06:17:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1mnfomq</id>
    <title>Searching actually viable alternative to Ollama</title>
    <updated>2025-08-11T15:13:53+00:00</updated>
    <author>
      <name>/u/mags0ft</name>
      <uri>https://old.reddit.com/user/mags0ft</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey there,&lt;/p&gt; &lt;p&gt;as we've all figured out by now, Ollama is certainly not the best way to go. Yes, it's simple, but there are so many alternatives out there which either outperform Ollama or just work with broader compatibility. So I said to myself, &amp;quot;screw it&amp;quot;, I'm gonna try that out, too.&lt;/p&gt; &lt;p&gt;Unfortunately, it turned out to be everything but simple. I need an alternative that...&lt;/p&gt; &lt;ul&gt; &lt;li&gt;implements model swapping (loading/unloading on the fly, dynamically) just like Ollama does&lt;/li&gt; &lt;li&gt;exposes an OpenAI API endpoint&lt;/li&gt; &lt;li&gt;is open-source&lt;/li&gt; &lt;li&gt;can take pretty much any GGUF I throw at it&lt;/li&gt; &lt;li&gt;is easy to set up and spins up quickly&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I looked at a few alternatives already. vLLM seems nice, but is quite the hassle to set up. It threw a lot of errors I simply did not have the time to look for, and I want a solution that &lt;em&gt;just works&lt;/em&gt;. LM Studio is closed and their open-source CLI still mandates usage of the closed LM Studio application...&lt;/p&gt; &lt;p&gt;Any go-to recommendations?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mags0ft"&gt; /u/mags0ft &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnfomq/searching_actually_viable_alternative_to_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnfomq/searching_actually_viable_alternative_to_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mnfomq/searching_actually_viable_alternative_to_ollama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T15:13:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1mo40p5</id>
    <title>"Seedability"</title>
    <updated>2025-08-12T09:27:36+00:00</updated>
    <author>
      <name>/u/ihatebeinganonymous</name>
      <uri>https://old.reddit.com/user/ihatebeinganonymous</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi. A few days ago someone mentioned &amp;quot;seedability&amp;quot; here, i.e. the ability of the model to produce deterministic output given a seed number. &lt;/p&gt; &lt;p&gt;I don't remember any seed parameter in the REST API calls or even Llama.cpp CLI. Is it something that the model has to support? Does anyone have an example?&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ihatebeinganonymous"&gt; /u/ihatebeinganonymous &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mo40p5/seedability/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mo40p5/seedability/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mo40p5/seedability/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-12T09:27:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1mnzh4v</id>
    <title>Interactive Reasoning Benchmarks | ARC-AGI-3 Preview</title>
    <updated>2025-08-12T04:45:14+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnzh4v/interactive_reasoning_benchmarks_arcagi3_preview/"&gt; &lt;img alt="Interactive Reasoning Benchmarks | ARC-AGI-3 Preview" src="https://external-preview.redd.it/kYuRuYq7QVkuqRGYxLg7ZJUh0IUdDQO6gTNOZAtie7A.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2e651d6180cd6d1e786fc7304eb22a82593128c5" title="Interactive Reasoning Benchmarks | ARC-AGI-3 Preview" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=3T4OwBp6d90"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnzh4v/interactive_reasoning_benchmarks_arcagi3_preview/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mnzh4v/interactive_reasoning_benchmarks_arcagi3_preview/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-12T04:45:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1mnh0s5</id>
    <title>Llama.cpp Vulkan is awesome, It gave new life to my old RX580</title>
    <updated>2025-08-11T16:03:21+00:00</updated>
    <author>
      <name>/u/Ssjultrainstnict</name>
      <uri>https://old.reddit.com/user/Ssjultrainstnict</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built a new computer and instead of buying a GPU I decided to give my old RX580 8GB a try for running inference. I had it laying around unused.&lt;/p&gt; &lt;p&gt;My PC specs are not crazy, its b850 motherboard, Ryzen 7700X and 32gb ram. My total cost was about 700 dollars.&lt;/p&gt; &lt;p&gt;Tried running Qwen3 30 b with about 20 layers offloaded to the GPU and got 24 tokes a second. Here is my command&lt;/p&gt; &lt;p&gt;./llama-server --n_gpu_layers 20 --ctx-size 16000 --model ../../../models/Qwen3-30B-A3B-Instruct-2507-UD-Q4_K_XL.gguf&lt;/p&gt; &lt;p&gt;Adding top_p and top_k and temp slows down the inference by about 10 tokens a second, not sure why. &lt;/p&gt; &lt;p&gt;&lt;code&gt;slot print_timing: id 0 | task 0 | prompt eval time = 559.81 ms / 13 tokens ( 43.06 ms per token, 23.22 tokens per second) eval time = 30875.68 ms / 743 tokens ( 41.56 ms per token, 24.06 tokens per second) total time = 31435.49 ms / 756 tokens &lt;/code&gt;&lt;/p&gt; &lt;p&gt;My RX580 is actually useful to me now, and it worked out of the box with Linux Mint!&lt;/p&gt; &lt;p&gt;With Vulkan being this good now, you can actually build a decent localllama build for about 7-800 dollars. Very excited for the future of local llms!&lt;/p&gt; &lt;p&gt;Edit: fixed the command i used for llama.cpp&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ssjultrainstnict"&gt; /u/Ssjultrainstnict &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnh0s5/llamacpp_vulkan_is_awesome_it_gave_new_life_to_my/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnh0s5/llamacpp_vulkan_is_awesome_it_gave_new_life_to_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mnh0s5/llamacpp_vulkan_is_awesome_it_gave_new_life_to_my/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T16:03:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1mn8ij6</id>
    <title>gpt-oss-120b ranks 16th place on lmarena.ai (20b model is ranked 38th)</title>
    <updated>2025-08-11T09:38:57+00:00</updated>
    <author>
      <name>/u/chikengunya</name>
      <uri>https://old.reddit.com/user/chikengunya</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn8ij6/gptoss120b_ranks_16th_place_on_lmarenaai_20b/"&gt; &lt;img alt="gpt-oss-120b ranks 16th place on lmarena.ai (20b model is ranked 38th)" src="https://preview.redd.it/0lv50zsy1dif1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2ea6791432a529ab3ea6d7e9ca517b8c29a23b19" title="gpt-oss-120b ranks 16th place on lmarena.ai (20b model is ranked 38th)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chikengunya"&gt; /u/chikengunya &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0lv50zsy1dif1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn8ij6/gptoss120b_ranks_16th_place_on_lmarenaai_20b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mn8ij6/gptoss120b_ranks_16th_place_on_lmarenaai_20b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T09:38:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1mnvleb</id>
    <title>KittenTTS inference speed on ARM</title>
    <updated>2025-08-12T01:35:04+00:00</updated>
    <author>
      <name>/u/Aaaaaaaaaeeeee</name>
      <uri>https://old.reddit.com/user/Aaaaaaaaaeeeee</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Source: &lt;a href="https://github.com/KittenML/KittenTTS/issues/40#issuecomment-3168324368"&gt;https://github.com/KittenML/KittenTTS/issues/40#issuecomment-3168324368&lt;/a&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Environment&lt;/th&gt; &lt;th&gt;Kokoro (kokoro-v1.0.fp16.onnx)&lt;/th&gt; &lt;th&gt;Piper (en_US-lessac-low.onnx)&lt;/th&gt; &lt;th&gt;KittenTTS (kitten_tts_nano_v0_1.onnx)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;MacBook Pro M2&lt;/td&gt; &lt;td&gt;0.75s&lt;/td&gt; &lt;td&gt;0.085s&lt;/td&gt; &lt;td&gt;0.68s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Raspberry Pi 5&lt;/td&gt; &lt;td&gt;4.83s&lt;/td&gt; &lt;td&gt;0.54s&lt;/td&gt; &lt;td&gt;4.13s&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;TLDR: KittenTTS might be lighter in size than kokoro, but speed is still similar. Both might not be fast enough for streaming compared with older TTS models when using the cpu. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aaaaaaaaaeeeee"&gt; /u/Aaaaaaaaaeeeee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnvleb/kittentts_inference_speed_on_arm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnvleb/kittentts_inference_speed_on_arm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mnvleb/kittentts_inference_speed_on_arm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-12T01:35:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1mo4056</id>
    <title>I created a persistent strategy game where you rule by giving commands to an AI council.</title>
    <updated>2025-08-12T09:26:30+00:00</updated>
    <author>
      <name>/u/rscp1147re</name>
      <uri>https://old.reddit.com/user/rscp1147re</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey Reddit,&lt;/p&gt; &lt;p&gt;For the past few months, I've been working on a passion project called &lt;strong&gt;AI Kingdom&lt;/strong&gt;, and I'm excited to share it with you all.&lt;/p&gt; &lt;p&gt;I've always loved deep strategy and kingdom-building games, but I felt that the interaction often boiled down to clicking through menus. My goal was to create a game where you feel like you're actually &lt;em&gt;ruling&lt;/em&gt;, where your words have weight, and your story is truly your own. AI Kingdom is a free, browser-based, persistent world game where you do just that.&lt;/p&gt; &lt;p&gt;Hereâ€™s what makes it different:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;## Speak to a Living Council&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Instead of a toolbar with buttons, your primary interface is a council of six AI-powered ministers, each with their own personality and expertise.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You don't click &amp;quot;Recruit Army.&amp;quot; You select your blunt Minister of War and type, &lt;strong&gt;&amp;quot;We need to bolster our northern garrisons. Recruit 1000 soldiers immediately.&amp;quot;&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;You don't drag a tax slider. You tell your meticulous Minister of Finance, &lt;strong&gt;&amp;quot;The treasury is running low. Set the national tax rate to 30%.&amp;quot;&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Each minister understands your commands, offers advice, and carries out your orders, all while evolving the narrative of your kingdom.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;## Forge Your Own Narrative&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The game world is driven by an AI storyteller. You'll face unique problems called &amp;quot;Royal Memorials&amp;quot; that are generated based on your kingdom's specific situation. The best part? &lt;strong&gt;There are no multiple-choice answers.&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;If a plague breaks out in a region, you don't choose between Option A, B, or C. You write your own decree: &lt;strong&gt;&amp;quot;Enforce a strict quarantine on the afflicted region, but ensure our royal physicians distribute food and medicine to the innocent civilians within.&amp;quot;&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;The AI evaluates the creativity and effectiveness of your written solution, which then permanently shapes the history of your kingdom and determines your reward. Your decisions truly matter and are recorded in your kingdom's unique story.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;## A Persistent World of Diplomacy &amp;amp; Betrayal&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;AI Kingdom is a multiplayer world. You can see other player-run kingdoms on the world map and interact with them.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Deep Diplomacy:&lt;/strong&gt; Form Non-Aggression Pacts, share intelligence, and even create a high-risk, high-reward &lt;strong&gt;Alliance Economy&lt;/strong&gt; where you and your ally can prosperâ€”or collapseâ€”together.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Strategic Warfare:&lt;/strong&gt; Conquest isn't about who has the biggest army number. Your attacking force is determined by the total soldiers garrisoned on your tiles &lt;em&gt;adjacent&lt;/em&gt; to the target. This makes strategic positioning, terrain, and well-fortified borders paramount to any campaign. Capturing an enemy's Capital means total victory.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The game is &lt;strong&gt;free to play&lt;/strong&gt; and runs directly in your browser, so there's nothing to install.&lt;/p&gt; &lt;p&gt;I'm actively developing it and would love to get your feedback.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Play the Game:&lt;/strong&gt; &lt;a href="https://www.playaikingdom.com/register.php"&gt;&lt;code&gt;https://www.playaikingdom.com&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Join our Discord Community:&lt;/strong&gt; &lt;a href="https://discord.gg/GbZteZe7cn"&gt;&lt;code&gt;https://discord.gg/GbZteZe7cn&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Read the Guide:&lt;/strong&gt; &lt;a href="https://www.playaikingdom.com/guide.php"&gt;&lt;code&gt;https://www.playaikingdom.com/guide.php&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thanks for reading, and I hope to see your kingdom rise (or fall!) in the world of AI Kingdom!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rscp1147re"&gt; /u/rscp1147re &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mo4056/i_created_a_persistent_strategy_game_where_you/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mo4056/i_created_a_persistent_strategy_game_where_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mo4056/i_created_a_persistent_strategy_game_where_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-12T09:26:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1mnedxo</id>
    <title>SOTA on 41 benchmarks! GLM-4.5V -- A new open-source VLM from China</title>
    <updated>2025-08-11T14:24:03+00:00</updated>
    <author>
      <name>/u/jiawei243</name>
      <uri>https://old.reddit.com/user/jiawei243</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnedxo/sota_on_41_benchmarks_glm45v_a_new_opensource_vlm/"&gt; &lt;img alt="SOTA on 41 benchmarks! GLM-4.5V -- A new open-source VLM from China" src="https://b.thumbs.redditmedia.com/euatxyG4VjgoW0nwCU8kLUP0naLHxC18HHdMXCH_NiU.jpg" title="SOTA on 41 benchmarks! GLM-4.5V -- A new open-source VLM from China" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Two weeks ago, China's &lt;a href="http://Z.ai"&gt;Z.ai&lt;/a&gt; open-sourced the &lt;a href="https://huggingface.co/collections/zai-org/glm-45-687c621d34bda8c9e4bf503b"&gt;GLM-4.5 model&lt;/a&gt;. Now, building on GLM-4.5â€™s language architecture, theyâ€™ve trained a new VLMâ€”&lt;a href="https://huggingface.co/collections/zai-org/glm-45v-68999032ddf8ecf7dcdbc102"&gt;GLM-4.5V&lt;/a&gt; â€” which achieved SOTA in â€‹&lt;strong&gt;41 out of 42 benchmarks&lt;/strong&gt;. &lt;/p&gt; &lt;p&gt;Absolutely insane! &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/zvok5ul5geif1.jpg?width=5188&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=4a692340ca8cf3126cffd7d9c091d296dc02acbc"&gt;https://preview.redd.it/zvok5ul5geif1.jpg?width=5188&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=4a692340ca8cf3126cffd7d9c091d296dc02acbc&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jiawei243"&gt; /u/jiawei243 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnedxo/sota_on_41_benchmarks_glm45v_a_new_opensource_vlm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnedxo/sota_on_41_benchmarks_glm45v_a_new_opensource_vlm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mnedxo/sota_on_41_benchmarks_glm45v_a_new_opensource_vlm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T14:24:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1mnlfyt</id>
    <title>Geocities style site by glm 4.5</title>
    <updated>2025-08-11T18:44:42+00:00</updated>
    <author>
      <name>/u/ChazychazZz</name>
      <uri>https://old.reddit.com/user/ChazychazZz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnlfyt/geocities_style_site_by_glm_45/"&gt; &lt;img alt="Geocities style site by glm 4.5" src="https://preview.redd.it/fvi60un3qfif1.gif?width=640&amp;amp;crop=smart&amp;amp;s=ac08d73fcf7b3cd7d2da2f4614ab0b91d7bf5250" title="Geocities style site by glm 4.5" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Completed in just 1 super simple prompt. GLM 4.5 is terrifyingly good at web dev now, especially as we can run it local. For me it was obvious it can generate modern and modern-ish sites but this stuff is kinda cooler to see (at least for me). The only unfortunate thing that it used emojis but that can be tweaked i guess and just included in the prompt&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ChazychazZz"&gt; /u/ChazychazZz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fvi60un3qfif1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnlfyt/geocities_style_site_by_glm_45/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mnlfyt/geocities_style_site_by_glm_45/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T18:44:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1mnd144</id>
    <title>Am I the only one who never really liked Ollama?</title>
    <updated>2025-08-11T13:30:22+00:00</updated>
    <author>
      <name>/u/a_normal_user1</name>
      <uri>https://old.reddit.com/user/a_normal_user1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With all that happens with it now and them wanting people to make accounts to use certain features(which kinda defeats the purpose of it) am I the only one who thought that it's really not the best? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/a_normal_user1"&gt; /u/a_normal_user1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnd144/am_i_the_only_one_who_never_really_liked_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnd144/am_i_the_only_one_who_never_really_liked_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mnd144/am_i_the_only_one_who_never_really_liked_ollama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T13:30:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1mnhgt0</id>
    <title>GPT-OSS Benchmarks: How GPT-OSS-120B Performs in Real Tasks</title>
    <updated>2025-08-11T16:19:43+00:00</updated>
    <author>
      <name>/u/facethef</name>
      <uri>https://old.reddit.com/user/facethef</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnhgt0/gptoss_benchmarks_how_gptoss120b_performs_in_real/"&gt; &lt;img alt="GPT-OSS Benchmarks: How GPT-OSS-120B Performs in Real Tasks" src="https://preview.redd.it/jw671veezeif1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3ef1b882c2760541b723f2922a88f046fea21c80" title="GPT-OSS Benchmarks: How GPT-OSS-120B Performs in Real Tasks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;OpenAI released their first open models since GPT-2, and GPT-OSS-120B is now the best open-weight model on our real-world TaskBench.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Some details:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Better completion performance overall compared to other open-weight models like Kimi-K2 and DeepSeek-R1, while being roughly 1/10th the size. Cheaper, better, faster.&lt;/li&gt; &lt;li&gt;Relative to closed-source models, it performs like smaller frontier models such as o4-mini or previous-generation top tier models like Claude-3.7.&lt;/li&gt; &lt;li&gt;Clearly optimized for agentic use cases, itâ€™s close to Sonnet-4 on our agentic benchmarks and could be a strong main agent model.&lt;/li&gt; &lt;li&gt;Works more like an action model than a chat or knowledge model. Multi-lingual performance is limited, and it hallucinates more on world knowledge, so it benefits from retrieval grounding and pairing with another model for multi-lingual scenarios.&lt;/li&gt; &lt;li&gt;Context recall is decent but weaker than top frontier models, so itâ€™s better suited for shorter or carefully managed context windows.&lt;/li&gt; &lt;li&gt;Excels when paired with strong context engineering and agentic engineering, where each task completion reliably feeds into the next.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Overall, this model looks to be a real gem and will likely inject more energy into open-source models.&lt;/p&gt; &lt;p&gt;Weâ€™ve published the full benchmark results, including GPT-5, mini, and nano, and our task categories and eval methods here: &lt;a href="https://opper.ai/models"&gt;https://opper.ai/models&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For those building with it, anyone else seeing similar strengths/weaknesses?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/facethef"&gt; /u/facethef &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jw671veezeif1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnhgt0/gptoss_benchmarks_how_gptoss120b_performs_in_real/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mnhgt0/gptoss_benchmarks_how_gptoss120b_performs_in_real/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T16:19:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1mo1vre</id>
    <title>Why stop at 'Strawberry'? Lets up the game with 'How many c's are there in 'pneumonoultramicroscopicsilicovolcanoconiosis'.</title>
    <updated>2025-08-12T07:08:27+00:00</updated>
    <author>
      <name>/u/riwritingreddit</name>
      <uri>https://old.reddit.com/user/riwritingreddit</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mo1vre/why_stop_at_strawberry_lets_up_the_game_with_how/"&gt; &lt;img alt="Why stop at 'Strawberry'? Lets up the game with 'How many c's are there in 'pneumonoultramicroscopicsilicovolcanoconiosis'." src="https://preview.redd.it/2e65cn38fjif1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=da4ffea4883b21f3e637daf2a89cb44028cbdb31" title="Why stop at 'Strawberry'? Lets up the game with 'How many c's are there in 'pneumonoultramicroscopicsilicovolcanoconiosis'." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen 4B got it right after thinking 30 Sec.ZLM thought for almost 2 min .GPT-5 took 5 sec.Gemini took less than 2 sec,and told me use count() function in Python which it used. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/riwritingreddit"&gt; /u/riwritingreddit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2e65cn38fjif1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mo1vre/why_stop_at_strawberry_lets_up_the_game_with_how/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mo1vre/why_stop_at_strawberry_lets_up_the_game_with_how/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-12T07:08:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1mncfif</id>
    <title>GLM-4.5V (based on GLM-4.5 Air)</title>
    <updated>2025-08-11T13:04:47+00:00</updated>
    <author>
      <name>/u/rerri</name>
      <uri>https://old.reddit.com/user/rerri</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A vision-language model (VLM) in the GLM-4.5 family. Features listed in model card:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Image reasoning&lt;/strong&gt; (scene understanding, complex multi-image analysis, spatial recognition)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Video understanding&lt;/strong&gt; (long video segmentation and event recognition)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GUI tasks&lt;/strong&gt; (screen reading, icon recognition, desktop operation assistance)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Complex chart &amp;amp; long document parsing&lt;/strong&gt; (research report analysis, information extraction)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Grounding&lt;/strong&gt; (precise visual element localization)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://huggingface.co/zai-org/GLM-4.5V"&gt;https://huggingface.co/zai-org/GLM-4.5V&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rerri"&gt; /u/rerri &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mncfif/glm45v_based_on_glm45_air/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mncfif/glm45v_based_on_glm45_air/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mncfif/glm45v_based_on_glm45_air/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T13:04:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1mo1pv4</id>
    <title>Uncensored gpt-oss-20b released</title>
    <updated>2025-08-12T06:58:18+00:00</updated>
    <author>
      <name>/u/No-Solution-8341</name>
      <uri>https://old.reddit.com/user/No-Solution-8341</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Jinx is a &amp;quot;helpful-only&amp;quot; variant of popular open-weight language models that responds to all queries without safety refusals. &lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Jinx-org/Jinx-gpt-oss-20b"&gt;https://huggingface.co/Jinx-org/Jinx-gpt-oss-20b&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Solution-8341"&gt; /u/No-Solution-8341 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mo1pv4/uncensored_gptoss20b_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mo1pv4/uncensored_gptoss20b_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mo1pv4/uncensored_gptoss20b_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-12T06:58:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1mno45o</id>
    <title>FULL LEAKED v0 by Vercel System Prompts and Internal Tools</title>
    <updated>2025-08-11T20:25:04+00:00</updated>
    <author>
      <name>/u/Independent-Box-898</name>
      <uri>https://old.reddit.com/user/Independent-Box-898</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;(Latest update: 11/08/2025)&lt;/p&gt; &lt;p&gt;I managed to get FULL official v0 system prompt and internal tools. Over 13.5K tokens and 1.3K lines.&lt;/p&gt; &lt;p&gt;Check it out at: &lt;a href="https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools"&gt;https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Box-898"&gt; /u/Independent-Box-898 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mno45o/full_leaked_v0_by_vercel_system_prompts_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mno45o/full_leaked_v0_by_vercel_system_prompts_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mno45o/full_leaked_v0_by_vercel_system_prompts_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T20:25:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1mnc8lx</id>
    <title>I built Excel Add-in for Ollama</title>
    <updated>2025-08-11T12:56:39+00:00</updated>
    <author>
      <name>/u/dbhalla4</name>
      <uri>https://old.reddit.com/user/dbhalla4</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnc8lx/i_built_excel_addin_for_ollama/"&gt; &lt;img alt="I built Excel Add-in for Ollama" src="https://preview.redd.it/mvjwf2f81eif1.gif?width=640&amp;amp;crop=smart&amp;amp;s=17b456d91ceed7000d3f08cd2f8917aec6e4254a" title="I built Excel Add-in for Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built an excel add-in that connects Ollama with Microsoft Excel. Data to remain inside excel only. You can simply write function =ollama(A1), assuming prompt in cell A1. You can simply drag to run on multiple cells. It has arguments to specify system instructions, temperature and model. You can set at both global level and specific to your prompts. &lt;a href="https://www.listendata.com/2025/08/ollama-in-excel.html"&gt;https://www.listendata.com/2025/08/ollama-in-excel.html&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dbhalla4"&gt; /u/dbhalla4 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/mvjwf2f81eif1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnc8lx/i_built_excel_addin_for_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mnc8lx/i_built_excel_addin_for_ollama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T12:56:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1mnxwmw</id>
    <title>Unsloth fixes chat_template (again). gpt-oss-120-high now scores 68.4 on Aider polyglot</title>
    <updated>2025-08-12T03:24:03+00:00</updated>
    <author>
      <name>/u/Sorry_Ad191</name>
      <uri>https://old.reddit.com/user/Sorry_Ad191</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnxwmw/unsloth_fixes_chat_template_again_gptoss120high/"&gt; &lt;img alt="Unsloth fixes chat_template (again). gpt-oss-120-high now scores 68.4 on Aider polyglot" src="https://b.thumbs.redditmedia.com/81joqRjngFFUavEApRYDiznp-6LcG-wUqoKaM4BcLls.jpg" title="Unsloth fixes chat_template (again). gpt-oss-120-high now scores 68.4 on Aider polyglot" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/tx92p3mpbiif1.png?width=688&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=de64253fcf2dba31b81554a76ac87534d3d29d2a"&gt;https://preview.redd.it/tx92p3mpbiif1.png?width=688&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=de64253fcf2dba31b81554a76ac87534d3d29d2a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Link to gguf: &lt;a href="https://huggingface.co/unsloth/gpt-oss-120b-GGUF/resolve/main/gpt-oss-120b-F16.gguf"&gt;https://huggingface.co/unsloth/gpt-oss-120b-GGUF/resolve/main/gpt-oss-120b-F16.gguf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;sha256: c6f818151fa2c6fbca5de1a0ceb4625b329c58595a144dc4a07365920dd32c51&lt;/p&gt; &lt;p&gt;edit: test was done with above Unsloth gguf downloaded Aug 5,&lt;/p&gt; &lt;p&gt;and with the new chat_template here: &lt;a href="https://huggingface.co/openai/gpt-oss-120b/resolve/main/chat_template.jinja"&gt;https://huggingface.co/openai/gpt-oss-120b/resolve/main/chat_template.jinja&lt;/a&gt;&lt;/p&gt; &lt;p&gt;newest Unsloth gguf has same link and;&lt;/p&gt; &lt;p&gt;sha256: 2d1f0298ae4b6c874d5a468598c5ce17c1763b3fea99de10b1a07df93cef014f&lt;/p&gt; &lt;p&gt;and also has an improved chat template built-in&lt;/p&gt; &lt;p&gt;currently rerunning low and medium reasoning tests with the newest gguf&lt;/p&gt; &lt;p&gt;and with the chat template built into the gguf&lt;/p&gt; &lt;p&gt;high reasoning took 2 days to run load balanced over 6 llama.cpp nodes so we will only rerun if there is a noticeable improvement with low and medium&lt;/p&gt; &lt;p&gt;high reasoning used 10x completion tokens over low, medium used 2x over low. high used 5x over medium etc. so both low and medium are much faster than high.&lt;/p&gt; &lt;p&gt;Finally here are instructions how to run locally: &lt;a href="https://docs.unsloth.ai/basics/gpt-oss-how-to-run-and-fine-tune"&gt;https://docs.unsloth.ai/basics/gpt-oss-how-to-run-and-fine-tune&lt;/a&gt;&lt;/p&gt; &lt;p&gt;and: &lt;a href="https://aider.chat/"&gt;https://aider.chat/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sorry_Ad191"&gt; /u/Sorry_Ad191 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnxwmw/unsloth_fixes_chat_template_again_gptoss120high/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnxwmw/unsloth_fixes_chat_template_again_gptoss120high/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mnxwmw/unsloth_fixes_chat_template_again_gptoss120high/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-12T03:24:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1mo1mb1</id>
    <title>GLM 4.5 AIR IS SO FKING GOODDD</title>
    <updated>2025-08-12T06:52:07+00:00</updated>
    <author>
      <name>/u/boneMechBoy69420</name>
      <uri>https://old.reddit.com/user/boneMechBoy69420</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just got to try it with our agentic system , it's so fast and perfect with its tool calls , but mostly it's freakishly fast too , thanks z.ai i love you ðŸ˜˜ðŸ’‹&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/boneMechBoy69420"&gt; /u/boneMechBoy69420 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mo1mb1/glm_45_air_is_so_fking_gooddd/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mo1mb1/glm_45_air_is_so_fking_gooddd/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mo1mb1/glm_45_air_is_so_fking_gooddd/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-12T06:52:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1mnp5nc</id>
    <title>Training an LLM only on books from the 1800's - Another update</title>
    <updated>2025-08-11T21:04:34+00:00</updated>
    <author>
      <name>/u/Remarkable-Trick-177</name>
      <uri>https://old.reddit.com/user/Remarkable-Trick-177</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm training LLM's from scratch using only texts from a specific region and time period and want to share another update. Right now it's 1800-1875 London. When I first started, my dataset was only 50 texts and I was using a 4060 for training. The latest version is trained on almost 7,000 texts using Phi 1.5 (700M parameters) on an A100 GPU. My long term goal is to see if a model trained this way can actually reason. The newest model I've trained has some promising output, it's starting to reference real historical events instead of just hallucinating everything. Also many people have told me that fine tuning will be more efficient and I agree, but I want to see how far this approach can go. And Internet Archive has around 175,000 London texts within my chosen time period, so scaling the dataset won't be an issue. &lt;a href="https://github.com/haykgrigo3/TimeCapsuleLLM"&gt;https://github.com/haykgrigo3/TimeCapsuleLLM&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Remarkable-Trick-177"&gt; /u/Remarkable-Trick-177 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnp5nc/training_an_llm_only_on_books_from_the_1800s/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnp5nc/training_an_llm_only_on_books_from_the_1800s/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mnp5nc/training_an_llm_only_on_books_from_the_1800s/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T21:04:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mncrqp</id>
    <title>ollama</title>
    <updated>2025-08-11T13:19:02+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mncrqp/ollama/"&gt; &lt;img alt="ollama" src="https://preview.redd.it/2whabjm55eif1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dea8efc9d0fe6d86f047a62709601f55061db889" title="ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2whabjm55eif1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mncrqp/ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mncrqp/ollama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T13:19:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1mo1o3d</id>
    <title>Google is cooking something...</title>
    <updated>2025-08-12T06:55:15+00:00</updated>
    <author>
      <name>/u/Ok_Ninja7526</name>
      <uri>https://old.reddit.com/user/Ok_Ninja7526</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mo1o3d/google_is_cooking_something/"&gt; &lt;img alt="Google is cooking something..." src="https://preview.redd.it/8zf0or9odjif1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=02efc5f521ba2e517fbf43631afcf28f7eab9422" title="Google is cooking something..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Ninja7526"&gt; /u/Ok_Ninja7526 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8zf0or9odjif1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mo1o3d/google_is_cooking_something/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mo1o3d/google_is_cooking_something/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-12T06:55:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1mo2gg7</id>
    <title>Jan v1: 4B model for web search with 91% SimpleQA, slightly outperforms Perplexity Pro</title>
    <updated>2025-08-12T07:45:23+00:00</updated>
    <author>
      <name>/u/Delicious_Focus3465</name>
      <uri>https://old.reddit.com/user/Delicious_Focus3465</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mo2gg7/jan_v1_4b_model_for_web_search_with_91_simpleqa/"&gt; &lt;img alt="Jan v1: 4B model for web search with 91% SimpleQA, slightly outperforms Perplexity Pro" src="https://preview.redd.it/niaetccbljif1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=83cb98af8850f5f113bf6d7f37db6c95989b888f" title="Jan v1: 4B model for web search with 91% SimpleQA, slightly outperforms Perplexity Pro" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, this is Bach from Jan. We're releasing Jan v1 today. In our evals, Jan v1 delivers 91% SimpleQA accuracy, slightly outperforming Perplexity Pro while running fully locally.&lt;/p&gt; &lt;p&gt;It's built on the new version of Qwen's &lt;a href="https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507"&gt;Qwen3-4B-Thinking&lt;/a&gt; (up to 256k context length), fine-tuned for reasoning and tool use in Jan.&lt;/p&gt; &lt;h1&gt;How to run it:&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Jan&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Download Jan v1 via Jan Hub&lt;/li&gt; &lt;li&gt;Enable search in Jan: &lt;ul&gt; &lt;li&gt;Settings â†’ Experimental Features â†’ On&lt;/li&gt; &lt;li&gt;Settings â†’ MCP Servers â†’ enable Search-related MCP (e.g. Serper)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Plus you can run the model in llama.cpp and vLLM.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Model links:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Jan-v1-4B: &lt;a href="https://huggingface.co/janhq/Jan-v1-4B"&gt;https://huggingface.co/janhq/Jan-v1-4B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Jan-v1-4B-GGUF: &lt;a href="https://huggingface.co/janhq/Jan-v1-4B-GGUF"&gt;https://huggingface.co/janhq/Jan-v1-4B-GGUF&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Recommended parameters:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;temperature: 0.6&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;top_p: 0.95&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;top_k: 20&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;min_p: 0.0&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;max_tokens: 2048&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We'd love for you to try Jan v1 and share your feedback, including what works well and where it falls short.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Delicious_Focus3465"&gt; /u/Delicious_Focus3465 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/niaetccbljif1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mo2gg7/jan_v1_4b_model_for_web_search_with_91_simpleqa/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mo2gg7/jan_v1_4b_model_for_web_search_with_91_simpleqa/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-12T07:45:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1mnxodk</id>
    <title>LocalLLaMA is the last sane place to discuss LLMs on this site, I swear</title>
    <updated>2025-08-12T03:12:34+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnxodk/localllama_is_the_last_sane_place_to_discuss_llms/"&gt; &lt;img alt="LocalLLaMA is the last sane place to discuss LLMs on this site, I swear" src="https://preview.redd.it/iu3pniar9iif1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4bac76c25e583f3690f2e1e9cdc20c74739fa84c" title="LocalLLaMA is the last sane place to discuss LLMs on this site, I swear" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/iu3pniar9iif1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnxodk/localllama_is_the_last_sane_place_to_discuss_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mnxodk/localllama_is_the_last_sane_place_to_discuss_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-12T03:12:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjf5ol</id>
    <title>r/LocalLlama is looking for moderators</title>
    <updated>2025-08-06T20:06:34+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/application/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T20:06:34+00:00</published>
  </entry>
</feed>
