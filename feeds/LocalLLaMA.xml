<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-27T20:07:24+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1pwplsz</id>
    <title>Strix Halo llama-bench Results (GLM-4.5-Air)</title>
    <updated>2025-12-27T05:16:08+00:00</updated>
    <author>
      <name>/u/b0tbuilder</name>
      <uri>https://old.reddit.com/user/b0tbuilder</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking for anyone who has some benchmarks they would like to share. I am trying to optimize my EVO-X2 (Strix Halo) 128GB box using GLM-4.5-Air for use with Cline. Trying to find out if I am in the ballpark optimization wise.&lt;/p&gt; &lt;p&gt;Model Quantization: Q4_K_XL (Unsloth)&lt;/p&gt; &lt;p&gt;KV Cache Quantization: Q8_0&lt;/p&gt; &lt;p&gt;ROCM 7.10&lt;/p&gt; &lt;p&gt;| model | size | params | backend | ngl | threads | n_ubatch | type_k | type_v | fa | mmap | test | t/s |&lt;/p&gt; &lt;p&gt;| ------------------------------ | ---------: | ---------: | ---------- | --: | ------: | -------: | -----: | -----: | -: | ---: | --------------: | -------------------: |&lt;/p&gt; &lt;p&gt;| glm4moe 106B.A12B Q4_K - Medium | 68.01 GiB | 110.47 B | ROCm | 99 | 8 | 2048 | q8_0 | q8_0 | 1 | 0 | pp256 | 166.89 ± 0.84 |&lt;/p&gt; &lt;p&gt;| glm4moe 106B.A12B Q4_K - Medium | 68.01 GiB | 110.47 B | ROCm | 99 | 8 | 2048 | q8_0 | q8_0 | 1 | 0 | pp512 | 261.15 ± 0.63 |&lt;/p&gt; &lt;p&gt;| glm4moe 106B.A12B Q4_K - Medium | 68.01 GiB | 110.47 B | ROCm | 99 | 8 | 2048 | q8_0 | q8_0 | 1 | 0 | pp2048 | 435.73 ± 0.86 |&lt;/p&gt; &lt;p&gt;| glm4moe 106B.A12B Q4_K - Medium | 68.01 GiB | 110.47 B | ROCm | 99 | 8 | 2048 | q8_0 | q8_0 | 1 | 0 | tg128 | 21.93 ± 0.03 |&lt;/p&gt; &lt;p&gt;| glm4moe 106B.A12B Q4_K - Medium | 68.01 GiB | 110.47 B | ROCm | 99 | 8 | 2048 | q8_0 | q8_0 | 1 | 0 | tg256 | 21.94 ± 0.04 |&lt;/p&gt; &lt;p&gt;| glm4moe 106B.A12B Q4_K - Medium | 68.01 GiB | 110.47 B | ROCm | 99 | 8 | 2048 | q8_0 | q8_0 | 1 | 0 | tg512 | 21.84 ± 0.01 |&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/b0tbuilder"&gt; /u/b0tbuilder &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwplsz/strix_halo_llamabench_results_glm45air/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwplsz/strix_halo_llamabench_results_glm45air/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwplsz/strix_halo_llamabench_results_glm45air/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-27T05:16:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1px6rsg</id>
    <title>Prediction: Will theartificialanalysis.ai scores hit 90+ by late 2026 if the scoring logic stays the same?</title>
    <updated>2025-12-27T19:48:03+00:00</updated>
    <author>
      <name>/u/ZeusZCC</name>
      <uri>https://old.reddit.com/user/ZeusZCC</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.reddit.com/poll/1px6rsg"&gt;View Poll&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ZeusZCC"&gt; /u/ZeusZCC &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1px6rsg/prediction_will_theartificialanalysisai_scores/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1px6rsg/prediction_will_theartificialanalysisai_scores/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1px6rsg/prediction_will_theartificialanalysisai_scores/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-27T19:48:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwwtk1</id>
    <title>2x Mac mini m4 pro 64 gb each with RDMA for local llms?</title>
    <updated>2025-12-27T12:35:11+00:00</updated>
    <author>
      <name>/u/Forward_Act4138</name>
      <uri>https://old.reddit.com/user/Forward_Act4138</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m planning a local LLM setup using &lt;strong&gt;two Mac mini M4 Pro units&lt;/strong&gt; (each with &lt;strong&gt;64 GB RAM&lt;/strong&gt;) and &lt;strong&gt;RDMA&lt;/strong&gt; between them. I’m trying to figure out what kind of performance I should realistically expect.&lt;/p&gt; &lt;p&gt;Anyone tested something like &lt;strong&gt;GPT-OSS 120B&lt;/strong&gt; (or similarly sized models) on this hardware? What were your real measurements (tokens/sec, memory usage, context scaling behavior)?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Forward_Act4138"&gt; /u/Forward_Act4138 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwwtk1/2x_mac_mini_m4_pro_64_gb_each_with_rdma_for_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwwtk1/2x_mac_mini_m4_pro_64_gb_each_with_rdma_for_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwwtk1/2x_mac_mini_m4_pro_64_gb_each_with_rdma_for_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-27T12:35:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwsb7i</id>
    <title>How is the Speculative Decoding Algorithm Constructed?</title>
    <updated>2025-12-27T07:54:04+00:00</updated>
    <author>
      <name>/u/song-sc</name>
      <uri>https://old.reddit.com/user/song-sc</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/song-sc"&gt; /u/song-sc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ki-seki.github.io/posts/251226-spec-decoding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwsb7i/how_is_the_speculative_decoding_algorithm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwsb7i/how_is_the_speculative_decoding_algorithm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-27T07:54:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1pw8nfk</id>
    <title>Nvidia acquired Groq, but why not Cerebras? Cerebras is 3x times faster than Groq, while maximum 1.5x the price. Anyone can explain?</title>
    <updated>2025-12-26T16:42:23+00:00</updated>
    <author>
      <name>/u/Conscious_Warrior</name>
      <uri>https://old.reddit.com/user/Conscious_Warrior</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyone with technical knowledge can explain why they chose Groq over Cerebras? Really interested in this. Because Cerebras is even waaay faster than Groq. Cerebras seems like a bigger threat to Nvidia than Groq...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Conscious_Warrior"&gt; /u/Conscious_Warrior &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pw8nfk/nvidia_acquired_groq_but_why_not_cerebras/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pw8nfk/nvidia_acquired_groq_but_why_not_cerebras/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pw8nfk/nvidia_acquired_groq_but_why_not_cerebras/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T16:42:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1px1z6i</id>
    <title>AI MAX 395 using NPU on linux</title>
    <updated>2025-12-27T16:32:56+00:00</updated>
    <author>
      <name>/u/UnbeliebteMeinung</name>
      <uri>https://old.reddit.com/user/UnbeliebteMeinung</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am trying to find a my way into the topic of local llms. I got this mini pc with the ai max 395 chip.&lt;/p&gt; &lt;p&gt;After hours i now am able to run some LLMs on the GPU (with rocm) instead of only CPU work on a ubuntu linux installed on the system.&lt;/p&gt; &lt;p&gt;But how do i even use the NPU? Any directions for me?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/UnbeliebteMeinung"&gt; /u/UnbeliebteMeinung &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1px1z6i/ai_max_395_using_npu_on_linux/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1px1z6i/ai_max_395_using_npu_on_linux/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1px1z6i/ai_max_395_using_npu_on_linux/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-27T16:32:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwsa27</id>
    <title>LLM Running Locally in the Browser for Infinite Dropdowns</title>
    <updated>2025-12-27T07:52:07+00:00</updated>
    <author>
      <name>/u/ilikehikingalot</name>
      <uri>https://old.reddit.com/user/ilikehikingalot</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwsa27/llm_running_locally_in_the_browser_for_infinite/"&gt; &lt;img alt="LLM Running Locally in the Browser for Infinite Dropdowns" src="https://external-preview.redd.it/Mzl0Yjc0bTdicDlnMajkkX4LG6wI_pxxo4qv3bbzlDaKVDsKMRLcrxEFlbW0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0a9a07cfb8aec90d458fed8eb8e2c0089e474bbf" title="LLM Running Locally in the Browser for Infinite Dropdowns" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I made a site playing around with what an LLM running locally in the browser can do, feel free to check it out!&lt;/p&gt; &lt;p&gt;The static site is: &lt;a href="https://rohanadwankar.github.io/unravel/"&gt;https://rohanadwankar.github.io/unravel/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The Github repo is: &lt;a href="https://github.com/RohanAdwankar/unravel"&gt;https://github.com/RohanAdwankar/unravel&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you are interested in how it works with &lt;a href="https://github.com/mlc-ai/mlc-llm"&gt;MLC&lt;/a&gt; check out the HTML file in &lt;a href="https://github.com/RohanAdwankar/unravel/commit/91690c0e7f8d190cbcd87276fbf2674552952079"&gt;this commit&lt;/a&gt; which enables running an LLM locally in under 50 lines!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ilikehikingalot"&gt; /u/ilikehikingalot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/0m5y5al7bp9g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwsa27/llm_running_locally_in_the_browser_for_infinite/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwsa27/llm_running_locally_in_the_browser_for_infinite/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-27T07:52:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1px052k</id>
    <title>Worth the 5090?</title>
    <updated>2025-12-27T15:15:43+00:00</updated>
    <author>
      <name>/u/fgoricha</name>
      <uri>https://old.reddit.com/user/fgoricha</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am debating on getting a 5090. &lt;/p&gt; &lt;p&gt;I currently have a 3090 that is actively being used for projects. It is an old 64gb ddr4 system with an i5 cpu and a 750 w psu. So nothing fancy and got it cheap. I have half a set up for a dual 3090 build. The plan was to create a dual or triple 3090 build. I have been collecting parts slowly over time without without over paying for them. I have 3 x 3090, a mobo that has spacing for 2 x 3090 (would need risers for the third 3090), 64 gb of ram ddr4, and a large psu. &lt;/p&gt; &lt;p&gt;I have used my single 3090 set up for fine tuning small models. Using lm studio, whisper, rag, and knowledge graphs. I then got into Yolo image training. I have not gotten into gaming but maybe some day. I plan to get back into traing LLMs and playing with them after my Yolo model.&lt;/p&gt; &lt;p&gt;I really like my single 3090 set up as it does not take up a lot of table space. All of my use cases have fit nicely on a single 3090 card. I think a draw back I noticed was needing to unload and load new models as part of the pipeline. I thought this could be fixed with dual 3090s so one model can live on each card. Also concerned about electrical and needing to put more thought into a build than just plug and play. One thing I wish I had was a faster card, and see that the 5090 is a significant upgrade over 3090. However the 48 vs 32 gb of vram is always nice, but the 16gb difference might not be a game changer.&lt;/p&gt; &lt;p&gt;I plan on renting a cloud 5090 to compare my current projects and see some hard numbers for speed up for training LLM models, training my Yolo models and LLM inference speeds.&lt;/p&gt; &lt;p&gt;But wht do you all think? If you were me, would finish building the dual 3090 build? Or sell two of the 3090s and put it towards a 5090?&lt;/p&gt; &lt;p&gt;I don't see myself really using more than 24 gb of vram, but I also tend to plan projects around the hardware that I have.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fgoricha"&gt; /u/fgoricha &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1px052k/worth_the_5090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1px052k/worth_the_5090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1px052k/worth_the_5090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-27T15:15:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1px5qze</id>
    <title>Is there anyone running a dual gpu setup 5090 + pro 6000 max Q?</title>
    <updated>2025-12-27T19:05:37+00:00</updated>
    <author>
      <name>/u/Dry_Mortgage_4646</name>
      <uri>https://old.reddit.com/user/Dry_Mortgage_4646</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Would this be viable for a consumer motherboard that can do 8x/8x to maximize LLMs and image/video generation? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dry_Mortgage_4646"&gt; /u/Dry_Mortgage_4646 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1px5qze/is_there_anyone_running_a_dual_gpu_setup_5090_pro/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1px5qze/is_there_anyone_running_a_dual_gpu_setup_5090_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1px5qze/is_there_anyone_running_a_dual_gpu_setup_5090_pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-27T19:05:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwf8p7</id>
    <title>What's the point of potato-tier LLMs?</title>
    <updated>2025-12-26T21:15:23+00:00</updated>
    <author>
      <name>/u/Fast_Thing_7949</name>
      <uri>https://old.reddit.com/user/Fast_Thing_7949</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwf8p7/whats_the_point_of_potatotier_llms/"&gt; &lt;img alt="What's the point of potato-tier LLMs?" src="https://b.thumbs.redditmedia.com/F0uF4Io7WMY9lYOzcakie1qYAc-lSDqqyibCA7Pa_qs.jpg" title="What's the point of potato-tier LLMs?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/64wjim607m9g1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fb5666c56138804f6be65ef56b519345f992b4cd"&gt;https://preview.redd.it/64wjim607m9g1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fb5666c56138804f6be65ef56b519345f992b4cd&lt;/a&gt;&lt;/p&gt; &lt;p&gt;After getting brought back down to earth in my last thread about replacing Claude with local models on an RTX 3090, I've got another question that's genuinely bothering me: What are 7b, 20b, 30B parameter models actually FOR? I see them released everywhere, but are they just benchmark toys so AI labs can compete on leaderboards, or is there some practical use case I'm too dense to understand? Because right now, I can't figure out what you're supposed to do with a potato-tier 7B model that can't code worth a damn and is slower than API calls anyway. &lt;/p&gt; &lt;p&gt;Seriously, what's the real-world application besides &amp;quot;I have a GPU and want to feel like I'm doing AI&amp;quot;?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fast_Thing_7949"&gt; /u/Fast_Thing_7949 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwf8p7/whats_the_point_of_potatotier_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwf8p7/whats_the_point_of_potatotier_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwf8p7/whats_the_point_of_potatotier_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T21:15:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwwd99</id>
    <title>A Farmer Doesn’t Know Coding, But Tries to Build an Executing Engine with LLMs and a Code Interpreter</title>
    <updated>2025-12-27T12:09:37+00:00</updated>
    <author>
      <name>/u/amadale</name>
      <uri>https://old.reddit.com/user/amadale</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Translated from Korean. I wrote the original in Korean and translated it myself. Final meaning and responsibility are mine.&lt;/p&gt; &lt;p&gt;I’m a garlic farmer in Korea. When I’m not working in the field, I use AI chat interfaces as my personal lab. I experiment with AIs that have sandboxed code interpreters, and I slowly build scripts that become a kind of “engine.” I don’t start from code. I start by talking to the AI, giving my thoughts and structural ideas first. Using the web tools inside the AI chat, I collect and structure information. Then I run that structured information again inside the code interpreter, and I only take the actual execution results and move forward with explainable analysis (XAI). Through this process, the concepts slowly grow, and step by step I give the AI more concrete direction. You can think of it like this: the LLM and the engine inside the sandboxed code interpreter form an indirect pipeline. User input → web tool search → LLM structures → that result is executed by the code interpreter. This is important for me: this is not an environment where I directly build pipelines with APIs. Everything happens inside the AI chat UI that I use every day. By the way, what I call a “sandboxed code interpreter” has different names depending on company or product (Code Interpreter, Data Analysis / Advanced Data Analysis, Code Execution, etc). But the core meaning is the same: An isolated execution environment where code actually runs inside the chat window (a sandboxed execution environment). And the “engine” I talk about is nothing fancy. It is just Python scripts running inside that sandbox (analysis scripts / verification scripts), and the execution-backed verification loop that repeats again and again.&lt;/p&gt; &lt;p&gt;The Question of Real Execution The biggest problem in this whole process is very simple: Is the code interpreter in the chat really executing, or not? If it is not actually executing, what comes out is close to hallucination — just simulated text with no real meaning. I have seen this many times: the output looks like execution results, but in reality nothing was executed at all. So the key question becomes only one thing for me: “Is this execution real right now?” In my case, I use reproducible code, like random-number-based checks, and make multiple AIs cross-check each other. Below is the overall flow I use, drawn in a very simple way:&lt;/p&gt; &lt;p&gt;[Me (input / thoughts)] | v [Web tool search (optional)] | v [LLM conversation / structuring] | v [Engine: sandboxed Python execution] | v [Execution output] | v [XAI / next direction] | v [Loop repeats]&lt;/p&gt; &lt;p&gt;The point I care about most is here: [Sandboxed Python execution] | +-- (execution is real) | -&amp;gt; reproducible output remains | -&amp;gt; becomes verifiable evidence | +-- (execution is fake) -&amp;gt; hallucinated / simulated text looks like “real output” -&amp;gt; high risk of wrong judgment&lt;/p&gt; &lt;p&gt;That is why I try to confirm, as much as possible, whether execution was real, by using reproducible code and cross-checking across multiple AIs. In the end, I feel that the ability to notice hallucination is also a kind of personal know-how. After many experiences, my conclusion is clear: With only one AI, it is almost impossible to get the result I want. You must cross-check between multiple AIs.&lt;/p&gt; &lt;p&gt;I want to say this clearly again: I am just a farmer. I only spend small pieces of time, working together with AIs like this. I don’t know if this method is “correct”, but sometimes meaningful results come out, so I keep using it. As time goes on, it feels more and more refined. Each AI has a different mechanism, so sometimes it is confusing. But I feel that the overall frame does work, especially when multiple AIs respond together in a consistent way. Because this is just a personal experiment, sometimes it makes my head hurt. But at the same time, I get many insights because of the AIs. Especially, I often feel that the diversity of AIs itself creates real effectiveness. What do you think about this way of working? I don’t really know how to code, but I learn by talking with AIs. Since multiple AIs give different opinions, I focus only on direction and intent, and let the AIs handle the experiments. Many times, this gives better results than I expected. When I watch code flowing down on my phone screen, it sometimes feels like watching the code scenes from the movie Matrix. And honestly, that part is fun by itself.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/amadale"&gt; /u/amadale &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwwd99/a_farmer_doesnt_know_coding_but_tries_to_build_an/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwwd99/a_farmer_doesnt_know_coding_but_tries_to_build_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwwd99/a_farmer_doesnt_know_coding_but_tries_to_build_an/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-27T12:09:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwx3n5</id>
    <title>Small ai model for a school project.</title>
    <updated>2025-12-27T12:50:33+00:00</updated>
    <author>
      <name>/u/Substantial_Cod_6019</name>
      <uri>https://old.reddit.com/user/Substantial_Cod_6019</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys I need help with my school project. It's for my finals in high school. I set out to create small ai model that will predict wheter the price will go up or down based on the news that come out about the company. &lt;/p&gt; &lt;p&gt;The stock it will be trying to predict is $APPL. I downloaded already some datasets that have a lot of data about how certain news affected the stock in the past.&lt;/p&gt; &lt;p&gt;It will be predicting if the price will increase or decrease, not by how many points. &lt;/p&gt; &lt;p&gt;Can you please help me with this, maybe give me some reccommendations for tools, programming languages and sources where I can learn how to do something like this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Substantial_Cod_6019"&gt; /u/Substantial_Cod_6019 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwx3n5/small_ai_model_for_a_school_project/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwx3n5/small_ai_model_for_a_school_project/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwx3n5/small_ai_model_for_a_school_project/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-27T12:50:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1px234n</id>
    <title>Seeking "Abliterated" Gemma 3 or Llama 3.3 that retains logic and multilingual (Slovak/Czech) capabilities</title>
    <updated>2025-12-27T16:37:26+00:00</updated>
    <author>
      <name>/u/FollowingFresh6411</name>
      <uri>https://old.reddit.com/user/FollowingFresh6411</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m looking for a specific type of model recommendation. I’ve been testing &lt;strong&gt;Gemma 3 12B/27B&lt;/strong&gt; and I’m impressed by its reasoning and excellent support for Central European languages (especially Slovak and Czech). However, the base/instruct versions are far too restrictive and prone to preachy refusals.&lt;/p&gt; &lt;p&gt;I’m looking for an &lt;strong&gt;uncensored or abliterated&lt;/strong&gt; version, but with a few crucial requirements:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;No &amp;quot;Brain Damage&amp;quot;:&lt;/strong&gt; I’ve tried some uncensored tunes that completely lost their reasoning capabilities or became repetitive. I need the model to stay as smart as the original.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multilingual Retention:&lt;/strong&gt; Many abliteration techniques or fine-tunes are heavily slanted toward English. I need a model that hasn't &amp;quot;forgotten&amp;quot; how to speak Slovak/Czech fluently while being uncensored.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Logical Consistency:&lt;/strong&gt; It shouldn't just be &amp;quot;edgy&amp;quot;; it should follow complex instructions without breaking character or losing the thread of the conversation.&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FollowingFresh6411"&gt; /u/FollowingFresh6411 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1px234n/seeking_abliterated_gemma_3_or_llama_33_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1px234n/seeking_abliterated_gemma_3_or_llama_33_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1px234n/seeking_abliterated_gemma_3_or_llama_33_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-27T16:37:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1px6t93</id>
    <title>Llama 3.2 3B fMRI update (early findings)</title>
    <updated>2025-12-27T19:49:46+00:00</updated>
    <author>
      <name>/u/Due_Hunter_4891</name>
      <uri>https://old.reddit.com/user/Due_Hunter_4891</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello all! I was exploring some logs, when I noticed something interesting. across multiple layers and steps, one dim kept popping out as active: 3039.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/nwq1woi7vs9g1.png?width=1858&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9dafbdf4058a87814294f56c1ba2795dab9d0ebc"&gt;step 7, basic greeting prompt. that dim that's constantly engaged is 3039.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/gukr15afvs9g1.png?width=1858&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=455716c8ec94bd5166727f2bbe162e345732747b"&gt;Here is the same prompt, several steps later. that dim stays consistent on steps in between &lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm not quite sure what to do with this information yet, but wanted to share because I found it pretty interesting!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Due_Hunter_4891"&gt; /u/Due_Hunter_4891 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1px6t93/llama_32_3b_fmri_update_early_findings/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1px6t93/llama_32_3b_fmri_update_early_findings/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1px6t93/llama_32_3b_fmri_update_early_findings/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-27T19:49:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1pweljh</id>
    <title>NVIDIA has 72GB VRAM version now</title>
    <updated>2025-12-26T20:48:17+00:00</updated>
    <author>
      <name>/u/decentralize999</name>
      <uri>https://old.reddit.com/user/decentralize999</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pweljh/nvidia_has_72gb_vram_version_now/"&gt; &lt;img alt="NVIDIA has 72GB VRAM version now" src="https://external-preview.redd.it/sC0_RV1rBP5Nka4zzrlrlknHQcvT_QUrChxq3hP_lVg.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e745729c3f7132892c715292c6b31f385f223e8f" title="NVIDIA has 72GB VRAM version now" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is 96GB too expensive? And AI community has no interest for 48GB?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/decentralize999"&gt; /u/decentralize999 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.nvidia.com/en-us/products/workstations/professional-desktop-gpus/rtx-pro-5000/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pweljh/nvidia_has_72gb_vram_version_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pweljh/nvidia_has_72gb_vram_version_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T20:48:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwybpe</id>
    <title>XiaomiMiMo.MiMo-V2-Flash: is there a reason why i see so few ggufs?</title>
    <updated>2025-12-27T13:52:37+00:00</updated>
    <author>
      <name>/u/LegacyRemaster</name>
      <uri>https://old.reddit.com/user/LegacyRemaster</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwybpe/xiaomimimomimov2flash_is_there_a_reason_why_i_see/"&gt; &lt;img alt="XiaomiMiMo.MiMo-V2-Flash: is there a reason why i see so few ggufs?" src="https://b.thumbs.redditmedia.com/tLv4egdIALish6T1JvUuABgC-fkC5rwBTFo_swD0v1U.jpg" title="XiaomiMiMo.MiMo-V2-Flash: is there a reason why i see so few ggufs?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/n58umc1l4r9g1.png?width=1334&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=538ec36b5f10702f983a6d812e260e470663342e"&gt;xiaomimimo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I've been testing the model for two days. It's incredibly fast at generating tokens compared to other models (certainly faster than both GLM and Minimax).&lt;/p&gt; &lt;p&gt;But I see few people talking about it and few posts. Is there a specific reason? Even Unsloth hasn't released anything yet.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LegacyRemaster"&gt; /u/LegacyRemaster &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwybpe/xiaomimimomimov2flash_is_there_a_reason_why_i_see/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwybpe/xiaomimimomimov2flash_is_there_a_reason_why_i_see/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwybpe/xiaomimimomimov2flash_is_there_a_reason_why_i_see/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-27T13:52:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1px3x5s</id>
    <title>More than 20% of videos shown to new YouTube users are ‘AI slop’, study finds</title>
    <updated>2025-12-27T17:51:45+00:00</updated>
    <author>
      <name>/u/EnigmaticEmir</name>
      <uri>https://old.reddit.com/user/EnigmaticEmir</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1px3x5s/more_than_20_of_videos_shown_to_new_youtube_users/"&gt; &lt;img alt="More than 20% of videos shown to new YouTube users are ‘AI slop’, study finds" src="https://external-preview.redd.it/ooS_D03GlqwVPelbZ5QA6BTQCahuN4HTtwsZt66twbs.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=056af1543405bd553af9d22df91d4e8918c27ecd" title="More than 20% of videos shown to new YouTube users are ‘AI slop’, study finds" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EnigmaticEmir"&gt; /u/EnigmaticEmir &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.theguardian.com/technology/2025/dec/27/more-than-20-of-videos-shown-to-new-youtube-users-are-ai-slop-study-finds"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1px3x5s/more_than_20_of_videos_shown_to_new_youtube_users/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1px3x5s/more_than_20_of_videos_shown_to_new_youtube_users/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-27T17:51:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwt6ir</id>
    <title>Asus isn't going into memory manufacturing — Taiwanese tech giant issues statement smashing rumor</title>
    <updated>2025-12-27T08:49:41+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwt6ir/asus_isnt_going_into_memory_manufacturing/"&gt; &lt;img alt="Asus isn't going into memory manufacturing — Taiwanese tech giant issues statement smashing rumor" src="https://external-preview.redd.it/iOAde8UE4DyQsY7bL3QZWs7PMlmK1Bl2f85BU0xU79M.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0c2acdf39b02a0864d08303117e213729c9c540f" title="Asus isn't going into memory manufacturing — Taiwanese tech giant issues statement smashing rumor" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.tomshardware.com/pc-components/dram/no-asus-isnt-going-into-memory-manufacturing-taiwanese-tech-giant-issues-statement-smashing-rumor"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwt6ir/asus_isnt_going_into_memory_manufacturing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwt6ir/asus_isnt_going_into_memory_manufacturing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-27T08:49:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwybg6</id>
    <title>llama.cpp, experimental native mxfp4 support for blackwell (25% preprocessing speedup!)</title>
    <updated>2025-12-27T13:52:16+00:00</updated>
    <author>
      <name>/u/bfroemel</name>
      <uri>https://old.reddit.com/user/bfroemel</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/17906"&gt;https://github.com/ggml-org/llama.cpp/pull/17906&lt;/a&gt;&lt;/p&gt; &lt;p&gt;love that kind of evolution:&lt;/p&gt; &lt;p&gt;&amp;gt; at the moment this PR is &lt;del&gt;10%&lt;/del&gt; &lt;em&gt;&lt;del&gt;slower&lt;/del&gt;&lt;/em&gt; &lt;del&gt;than master&lt;/del&gt; 25% faster than master on PP.&lt;/p&gt; &lt;p&gt;&amp;gt; To compile &lt;code&gt;-DCMAKE_CUDA_ARCHITECTURES=&amp;quot;120f&amp;quot;&lt;/code&gt; is required.&lt;/p&gt; &lt;p&gt;probably/currently most useful for gpt-oss models! (also while reading the PR it seems that we might see more native nvfp4 support soon!)&lt;/p&gt; &lt;p&gt;Thanks to &lt;a href="/u/am17an"&gt;u/am17an&lt;/a&gt; (PR author) &amp;amp; llama.cpp devs!!&lt;/p&gt; &lt;p&gt;/edit: better point that also out (although, so far I am not noticing any quality degradation with gpt-oss-120b!):&lt;br /&gt; &amp;gt; [..] we quantize activation to mxfp4 instead of q8, which lead to failures in &lt;code&gt;test-backend-ops&lt;/code&gt;, however PPL tests are okay with this change (though not ruling out correctness issues)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bfroemel"&gt; /u/bfroemel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwybg6/llamacpp_experimental_native_mxfp4_support_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwybg6/llamacpp_experimental_native_mxfp4_support_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwybg6/llamacpp_experimental_native_mxfp4_support_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-27T13:52:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwwsag</id>
    <title>The Infinite Software Crisis: We're generating complex, unmaintainable code faster than we can understand it. Is 'vibe-coding' the ultimate trap?</title>
    <updated>2025-12-27T12:33:15+00:00</updated>
    <author>
      <name>/u/madSaiyanUltra_9789</name>
      <uri>https://old.reddit.com/user/madSaiyanUltra_9789</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwwsag/the_infinite_software_crisis_were_generating/"&gt; &lt;img alt="The Infinite Software Crisis: We're generating complex, unmaintainable code faster than we can understand it. Is 'vibe-coding' the ultimate trap?" src="https://b.thumbs.redditmedia.com/CWbH9aocZDksMFttENvwXtHE-6zGD_eJPUb93Q-jv0M.jpg" title="The Infinite Software Crisis: We're generating complex, unmaintainable code faster than we can understand it. Is 'vibe-coding' the ultimate trap?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, &lt;/p&gt; &lt;p&gt;I just watched &lt;a href="https://www.youtube.com/watch?v=eIoohUmYpGI"&gt;The Infinite Software Crisis – Jake Nations&lt;/a&gt; on YouTube and it got me thinking... the limitations of software development has never been typing speed, but rather our ability to comprehend and design the system correctly in the first place. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Highlights from the talk:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Every developer has shipped code they didn't completely understand. it passed the tests and that was enough validation to deploy it.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The hard part is timeless:&lt;/strong&gt; The hard part isn't the mechanics of coding; it's the conceptual difficulty of designing a solution. Every tool, including AI, just makes implementation easier.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;AI amplifies the problem:&lt;/strong&gt; We can now generate code as fast as we can describe it. The scale is infinite, but our comprehension isn't. The core challenge of understanding &lt;em&gt;what&lt;/em&gt; to build remains.&lt;/li&gt; &lt;li&gt;The real trap we fall into is confusing easy with simple. &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Easy&lt;/strong&gt; is what's within reach. What can you access without effort? Generate it with AI, copy-paste, or install a framework. It's about speed.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Simple&lt;/strong&gt; is about structure. It means one fold, one braid, no entanglement. It requires thought and design.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;LLMs do not understand logic, they merely relate language and substitute those relations as &amp;quot;code&amp;quot;, so the importance of &lt;em&gt;patterns and architectural decisions&lt;/em&gt; in your codebase are lost. &lt;/li&gt; &lt;li&gt;when &amp;quot;vibe-coding&amp;quot; technical debt doesn't register as debt; it's just more code to preserve. &lt;/li&gt; &lt;li&gt;The result? Complex, highly-coupled, and error-prone code generated in minutes that could take you weeks to understand (if ever).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The real danger here is that we're accumulating complexity faster than we can comprehend it because we're not doing the hard work of understanding our systems.&lt;/p&gt; &lt;p&gt;The proposed solution: SLOW DOWN, DO EVERYTHING MANUALLY; architectural design + scaffolding, etc and only let the LLM in at the last step of filling in the scaffolding. &lt;/p&gt; &lt;p&gt;What's your take, Is 'vibe-coding' a trap, or is there a way to use these tools without losing the ability to understand our systems? &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/c4mknoudlq9g1.png?width=553&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=28a6f37623fb0e0725f5b603f4b3a8ce51653ac9"&gt;https://preview.redd.it/c4mknoudlq9g1.png?width=553&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=28a6f37623fb0e0725f5b603f4b3a8ce51653ac9&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/madSaiyanUltra_9789"&gt; /u/madSaiyanUltra_9789 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwwsag/the_infinite_software_crisis_were_generating/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwwsag/the_infinite_software_crisis_were_generating/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwwsag/the_infinite_software_crisis_were_generating/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-27T12:33:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwyw36</id>
    <title>MiniMaxAI/MiniMax-M2.1 seems to be the strongest model per param</title>
    <updated>2025-12-27T14:19:07+00:00</updated>
    <author>
      <name>/u/SlowFail2433</name>
      <uri>https://old.reddit.com/user/SlowFail2433</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Going by the Artifical Analysis benchaes, MiniMaxAI/MiniMax-M2.1 can compete with Kimi K2 Thinking, Deepseek 3.2 and GLM 4.7 in performance.&lt;/p&gt; &lt;p&gt;But what feels especially notable is that MiniMaxAI/MiniMax-M2.1 is only 229B param which is around half of GLM 4.7, around a third of Deepseek 3.2 and around a fifth of Kimi K2 Thinking&lt;/p&gt; &lt;p&gt;What this means is that MiniMaxAI/MiniMax-M2.1 seems to be the best value model now&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SlowFail2433"&gt; /u/SlowFail2433 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwyw36/minimaxaiminimaxm21_seems_to_be_the_strongest/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwyw36/minimaxaiminimaxm21_seems_to_be_the_strongest/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwyw36/minimaxaiminimaxm21_seems_to_be_the_strongest/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-27T14:19:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1px55wg</id>
    <title>GLM 4.7 IS NOW THE #1 OPEN SOURCE MODEL IN ARTIFICIAL ANALYSIS</title>
    <updated>2025-12-27T18:42:04+00:00</updated>
    <author>
      <name>/u/ZeeleSama</name>
      <uri>https://old.reddit.com/user/ZeeleSama</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1px55wg/glm_47_is_now_the_1_open_source_model_in/"&gt; &lt;img alt="GLM 4.7 IS NOW THE #1 OPEN SOURCE MODEL IN ARTIFICIAL ANALYSIS" src="https://preview.redd.it/9wzn809jks9g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a4de55096d77fed72c8e49191b815e4f735ff388" title="GLM 4.7 IS NOW THE #1 OPEN SOURCE MODEL IN ARTIFICIAL ANALYSIS" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ZeeleSama"&gt; /u/ZeeleSama &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9wzn809jks9g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1px55wg/glm_47_is_now_the_1_open_source_model_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1px55wg/glm_47_is_now_the_1_open_source_model_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-27T18:42:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1px1c41</id>
    <title>Head of Engineering @MiniMax__AI on MiniMax M2 int4 QAT</title>
    <updated>2025-12-27T16:06:19+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1px1c41/head_of_engineering_minimax_ai_on_minimax_m2_int4/"&gt; &lt;img alt="Head of Engineering @MiniMax__AI on MiniMax M2 int4 QAT" src="https://preview.redd.it/1e9anmnmsr9g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7456cd2a6f5b63217ca62ea494cdbf87700184fa" title="Head of Engineering @MiniMax__AI on MiniMax M2 int4 QAT" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1e9anmnmsr9g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1px1c41/head_of_engineering_minimax_ai_on_minimax_m2_int4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1px1c41/head_of_engineering_minimax_ai_on_minimax_m2_int4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-27T16:06:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptxm3x</id>
    <title>AMA With Z.AI, The Lab Behind GLM-4.7</title>
    <updated>2025-12-23T16:04:23+00:00</updated>
    <author>
      <name>/u/zixuanlimit</name>
      <uri>https://old.reddit.com/user/zixuanlimit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi r/LocalLLaMA&lt;/p&gt; &lt;p&gt;Today we are having &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt;, the research lab behind the GLM 4.7. We’re excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Yuxuan Zhang, &lt;a href="/u/YuxuanZhangzR"&gt;u/YuxuanZhangzR&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Qinkai Zheng, &lt;a href="/u/QinkaiZheng"&gt;u/QinkaiZheng&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Aohan Zeng, &lt;a href="/u/Sengxian"&gt;u/Sengxian&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Zhenyu Hou, &lt;a href="/u/ZhenyuHou"&gt;u/ZhenyuHou&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Xin Lv, &lt;a href="/u/davidlvxin"&gt;u/davidlvxin&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The AMA will run from 8 AM – 11 AM PST, with the &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt; team continuing to follow up on questions over the next 48 hours.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zixuanlimit"&gt; /u/zixuanlimit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T16:04:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwh0q9</id>
    <title>Best Local LLMs - 2025</title>
    <updated>2025-12-26T22:31:28+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Year end thread for the best LLMs of 2025!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;2025 is almost done! Its been &lt;strong&gt;a wonderful year&lt;/strong&gt; for us Open/Local AI enthusiasts. And its looking like Xmas time brought some great gifts in the shape of Minimax M2.1 and GLM4.7 that are touting frontier model performance. Are we there already? are we at parity with proprietary models?!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The standard spiel:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Share what your favorite models are right now &lt;strong&gt;and why.&lt;/strong&gt; Given the nature of the beast in evaluating LLMs (untrustworthiness of benchmarks, immature tooling, intrinsic stochasticity), please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Only open weights models&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;em&gt;Please thread your responses in the top level comments for each Application below to enable readability&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Applications&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;General&lt;/strong&gt;: Includes practical guidance, how to, encyclopedic QnA, search engine replacement/augmentation&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agentic/Agentic Coding/Tool Use/Coding&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Creative Writing/RP&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Speciality&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;If a category is missing, please create a top level comment under the Speciality comment&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Notes&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Useful breakdown of how folk are using LLMs: &lt;a href="https://preview.redd.it/i8td7u8vcewf1.png?width=1090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d"&gt;https://preview.redd.it/i8td7u8vcewf1.png?width=1090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d&lt;/a&gt; &lt;/p&gt; &lt;p&gt;A good suggestion for last time, breakdown/classify your recommendation by model memory footprint: (you can and should be using multiple models in each size range for different tasks)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Unlimited: &amp;gt;128GB VRAM &lt;/li&gt; &lt;li&gt;Medium: 8 to 128GB VRAM&lt;/li&gt; &lt;li&gt;Small: &amp;lt;8GB VRAM&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T22:31:28+00:00</published>
  </entry>
</feed>
