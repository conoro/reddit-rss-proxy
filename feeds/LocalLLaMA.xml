<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-06-30T22:49:53+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1lnzj5e</id>
    <title>So whatever happened to d(iffuser)LLMs?</title>
    <updated>2025-06-30T05:29:09+00:00</updated>
    <author>
      <name>/u/IngwiePhoenix</name>
      <uri>https://old.reddit.com/user/IngwiePhoenix</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This morning, I got an E-Mail from the team behind the Mercury Coder LLM, Inception (&lt;a href="https://www.inceptionlabs.ai/"&gt;https://www.inceptionlabs.ai/&lt;/a&gt;) that basically announced a chat-focused model. Pretty neat, sent along an API example with cURL also. Simple and nice.&lt;/p&gt; &lt;p&gt;But this reminded me of dLLMs in general - they haven't really been talked a lot about lately. So I wanted to ask into the broad space: What's up? I like the idea of dLLMs being a different approach and perhaps easier to run compared to transformers. But I also understand the tech is relatively new - that is, diffusers for text rather than images.&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IngwiePhoenix"&gt; /u/IngwiePhoenix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnzj5e/so_whatever_happened_to_diffuserllms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnzj5e/so_whatever_happened_to_diffuserllms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lnzj5e/so_whatever_happened_to_diffuserllms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T05:29:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1lo3y10</id>
    <title>From the trenches, running TinyLlama-1.1B-Chat-v0.1 on iPhone</title>
    <updated>2025-06-30T10:21:45+00:00</updated>
    <author>
      <name>/u/rvnllm</name>
      <uri>https://old.reddit.com/user/rvnllm</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lo3y10/from_the_trenches_running_tinyllama11bchatv01_on/"&gt; &lt;img alt="From the trenches, running TinyLlama-1.1B-Chat-v0.1 on iPhone" src="https://external-preview.redd.it/dt9-WMKabCLh-xNeXHwgVKpGxeuZavoRA4i4gCf5uKw.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=79dbb93cb56753787d457bd94d66062c9785a7f7" title="From the trenches, running TinyLlama-1.1B-Chat-v0.1 on iPhone" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just sharing my efforts, really, and thank you for reading in advance.&lt;/p&gt; &lt;p&gt;I am working on an LLM engine nicknamed Nyra in rust and c++20.&lt;/p&gt; &lt;p&gt;So managed to do local LLM Inference on iPhone in 70ms and 15 TPS (could be massively improved once metal is in motion)&lt;/p&gt; &lt;p&gt;One of the images shows that previously I optimized safetensors loading on-device for my custom runtime. That was step one.&lt;br /&gt; Since then, after some really hard push over the last 48 hours, I've integrated inference, built tokenizer support. So today Nyra generated her first poem.&lt;br /&gt; That was step two.&lt;/p&gt; &lt;p&gt;It is fully offline. Started to work after I nearly gave up multiple times, fully loaded with coffee and being lost between calculations, kernels and the like. Also occasionally my face took the shape of the keyboard falling asleep on it.&lt;/p&gt; &lt;p&gt;So what is it that I am showing?&lt;br /&gt; -&amp;gt; iphone in flight mode, check.&lt;br /&gt; -&amp;gt; No cloud. No API. No fluff. Just pure, local inference, check.&lt;br /&gt; -&amp;gt; Loaded 1.1B model in &amp;lt;2s, check. \-&amp;gt; Ran inference at 15 tokens/sec, well could be better as there is no Metal just yet, but check.&lt;br /&gt; -&amp;gt; CLI-based stream loop, well for devs thats cool, swiftui coming up, check.&lt;br /&gt; -&amp;gt; All native Rust + C++20 + SwiftUI pipeline, possibility to improve speed, check.&lt;br /&gt; -&amp;gt; Zero cloud, full privacy and full locality, yes thats my core philosophy, check.&lt;/p&gt; &lt;p&gt;Cloud? no. All local privacy driven. So yes, lets be sovereign.If one doesn't have the proper hardware AI is slow. I try to change that by running AI (LLMs) with acceptable speed on any hardware and anywhere.&lt;br /&gt; Nyra is different: she's modular, fast, local - and soon pluggable.&lt;/p&gt; &lt;p&gt;here is a demo video&lt;br /&gt; &lt;a href="https://www.youtube.com/watch?v=6ZMplYIsTyw"&gt;https://www.youtube.com/watch?v=6ZMplYIsTyw&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Thanks for reading&lt;br /&gt; Ervin&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/s14m8e4bj1af1.jpg?width=800&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=6e1b0c1f95a26d2b284a9e18b5936dfc7e0340f9"&gt;https://preview.redd.it/s14m8e4bj1af1.jpg?width=800&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=6e1b0c1f95a26d2b284a9e18b5936dfc7e0340f9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/amav0e4bj1af1.jpg?width=800&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=2a878999d57ab1d386ff367d6ca84792da082804"&gt;https://preview.redd.it/amav0e4bj1af1.jpg?width=800&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=2a878999d57ab1d386ff367d6ca84792da082804&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rvnllm"&gt; /u/rvnllm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lo3y10/from_the_trenches_running_tinyllama11bchatv01_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lo3y10/from_the_trenches_running_tinyllama11bchatv01_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lo3y10/from_the_trenches_running_tinyllama11bchatv01_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T10:21:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1lo0rk8</id>
    <title>Accelerated LLM Inference on AMD Instinct‚Ñ¢ GPUs with vLLM 0.9.x and ROCm</title>
    <updated>2025-06-30T06:48:55+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://rocm.blogs.amd.com/software-tools-optimization/vllm-0.9.x-rocm/README.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lo0rk8/accelerated_llm_inference_on_amd_instinct_gpus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lo0rk8/accelerated_llm_inference_on_amd_instinct_gpus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T06:48:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1lo5uz6</id>
    <title>Has anyone tried running 2 AMD Ryzen‚Ñ¢ AI Max+ 395 in parallel?</title>
    <updated>2025-06-30T12:09:31+00:00</updated>
    <author>
      <name>/u/orkutmuratyilmaz</name>
      <uri>https://old.reddit.com/user/orkutmuratyilmaz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;Some models require more VRAM to run. I was thinking of getting 2 AMD Ryzen‚Ñ¢ AI Max+ 395 and trying to run them in parallel. I wonder if anyone has tried this? Does anyone have any information?&lt;/p&gt; &lt;p&gt;Have a nice one:)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/orkutmuratyilmaz"&gt; /u/orkutmuratyilmaz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lo5uz6/has_anyone_tried_running_2_amd_ryzen_ai_max_395/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lo5uz6/has_anyone_tried_running_2_amd_ryzen_ai_max_395/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lo5uz6/has_anyone_tried_running_2_amd_ryzen_ai_max_395/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T12:09:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1lojd3e</id>
    <title>Gemma-3n VRAM usage</title>
    <updated>2025-06-30T21:10:17+00:00</updated>
    <author>
      <name>/u/el_pr3sid3nt3</name>
      <uri>https://old.reddit.com/user/el_pr3sid3nt3</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello fellow redditors,&lt;/p&gt; &lt;p&gt;I am trying to run Gemma-3n-E2B and E4B advertised as 2gb-3gb VRAM models. However, I couldn't run E4B due to torch outOfMemory, but when I ran E2B it took 10gbs and after few requests I went out of memory.&lt;/p&gt; &lt;p&gt;I am trying to understand, is there a way to run these models really on 2gb-3gb VRAM, and if yes how so, and what I missed?&lt;/p&gt; &lt;p&gt;Thank you all&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/el_pr3sid3nt3"&gt; /u/el_pr3sid3nt3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lojd3e/gemma3n_vram_usage/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lojd3e/gemma3n_vram_usage/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lojd3e/gemma3n_vram_usage/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T21:10:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1lol3na</id>
    <title>[Dataset] 4,000 hours of full-body, in-person, human face-to-face interaction videos</title>
    <updated>2025-06-30T22:20:44+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Dataset on Huggingface: &lt;a href="https://huggingface.co/datasets/facebook/seamless-interaction"&gt;https://huggingface.co/datasets/facebook/seamless-interaction&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.aidemos.meta.com/seamless_interaction_dataset"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lol3na/dataset_4000_hours_of_fullbody_inperson_human/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lol3na/dataset_4000_hours_of_fullbody_inperson_human/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T22:20:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1loal9v</id>
    <title>Drafted Llama as an enhanced parser for interactive fiction puzzles/games</title>
    <updated>2025-06-30T15:32:42+00:00</updated>
    <author>
      <name>/u/Fit-Lengthiness-4747</name>
      <uri>https://old.reddit.com/user/Fit-Lengthiness-4747</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1loal9v/drafted_llama_as_an_enhanced_parser_for/"&gt; &lt;img alt="Drafted Llama as an enhanced parser for interactive fiction puzzles/games" src="https://preview.redd.it/wg741dwa23af1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=91fd5c66c0a9e6f241558b7edd95831e8c284bc3" title="Drafted Llama as an enhanced parser for interactive fiction puzzles/games" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Using Llama as a way to expand the types of games that can be played within interactive fiction, such as creating non-deterministic rubrics to grade puzzle solutions, allowing building/crafting with a wide range of objects.combinatorial possibilities, and enabling sentiment and emotion-based responses with NPCs as a way of getting game information. try is here: &lt;a href="https://thoughtauction.itch.io/last-audit-of-the-damned"&gt;https://thoughtauction.itch.io/last-audit-of-the-damned&lt;/a&gt; And if you like, please vote for us in the ParserComp 2025 contest, as well as play some of the other entries. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fit-Lengthiness-4747"&gt; /u/Fit-Lengthiness-4747 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wg741dwa23af1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1loal9v/drafted_llama_as_an_enhanced_parser_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1loal9v/drafted_llama_as_an_enhanced_parser_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T15:32:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1lojgxl</id>
    <title>OpenSource CLI Agent with Local models.</title>
    <updated>2025-06-30T21:14:27+00:00</updated>
    <author>
      <name>/u/x8ko_dev</name>
      <uri>https://old.reddit.com/user/x8ko_dev</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I'm building this CLI coding agent right now. My big goal is to turn it into a fully autonomous bot that runs on a server, handles error reports, crash logs, and random issues, then tracks them down and fixes everything on its own.&lt;/p&gt; &lt;p&gt;For the moment, it's just a basic CLI tool packed with features for dealing with files, GitHub, general docs, and a bunch more.If you could test it out on your projects and hit me with some feedback or suggestions for improvements, that'd be super helpful.&lt;/p&gt; &lt;p&gt;Im struggling to find any edge cases that arent UI/Command related in my personal usage currently so i think its time to get a little real world responses.&lt;/p&gt; &lt;p&gt;I currently support LMStudio, Requesty and OpenRouter.&lt;br /&gt; So far our testing of local models (devstral, qwen and alike) are working really well. I'd love to hear your feedback, the worse the better. i want to know every issue, minor details and alike, im not here to get my ass kissed like ive seen from others.&lt;/p&gt; &lt;p&gt;Check it out here: &lt;a href="https://github.com/xyOz-dev/LogiQCLI/"&gt;https://github.com/xyOz-dev/LogiQCLI/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/x8ko_dev"&gt; /u/x8ko_dev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lojgxl/opensource_cli_agent_with_local_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lojgxl/opensource_cli_agent_with_local_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lojgxl/opensource_cli_agent_with_local_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T21:14:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1lobzkr</id>
    <title>n8n ,proxmox ,docker and Google API.</title>
    <updated>2025-06-30T16:26:28+00:00</updated>
    <author>
      <name>/u/Able-Consequence8872</name>
      <uri>https://old.reddit.com/user/Able-Consequence8872</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lobzkr/n8n_proxmox_docker_and_google_api/"&gt; &lt;img alt="n8n ,proxmox ,docker and Google API." src="https://preview.redd.it/02pteeydc3af1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fc69450af42416d14cf7491d093e484565716a06" title="n8n ,proxmox ,docker and Google API." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hi, trying to use Google API in 8n8 (in a PROXMOX container ) and LMstudio (another machine in the same LAN) but it won't take my LAN ip adresse.n8n gives the localhost value by default. I know there is a trick with docker, like &lt;a href="https://local.docker/v1"&gt;https://local.docker/v1&lt;/a&gt;, but it works only if both n8n and LMstudio work on the same machine. n8n is on a different machine on the LAN.&lt;/p&gt; &lt;p&gt;how can I fix this? I want to run everything locally, with 2 different machines on the LAN, using Google workspace with my assistant in 8n8, and Mistral as a local AI in LMstudio.&lt;/p&gt; &lt;p&gt;thx..&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Able-Consequence8872"&gt; /u/Able-Consequence8872 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/02pteeydc3af1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lobzkr/n8n_proxmox_docker_and_google_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lobzkr/n8n_proxmox_docker_and_google_api/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T16:26:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1lo84yj</id>
    <title>[2506.21734] Hierarchical Reasoning Model</title>
    <updated>2025-06-30T13:54:40+00:00</updated>
    <author>
      <name>/u/absolooot1</name>
      <uri>https://old.reddit.com/user/absolooot1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Abstract:&lt;/p&gt; &lt;p&gt;Reasoning, the process of devising and executing complex goal-oriented action sequences, remains a critical challenge in AI. Current large language models (LLMs) primarily employ Chain-of-Thought (CoT) techniques, which suffer from brittle task decomposition, extensive data requirements, and high latency. Inspired by the hierarchical and multi-timescale processing in the human brain, we propose the Hierarchical Reasoning Model (HRM), a novel recurrent architecture that attains significant computational depth while maintaining both training stability and efficiency. HRM executes sequential reasoning tasks in a single forward pass without explicit supervision of the intermediate process, through two interdependent recurrent modules: a high-level module responsible for slow, abstract planning, and a low-level module handling rapid, detailed computations. With only 27 million parameters, HRM achieves exceptional performance on complex reasoning tasks using only 1000 training samples. The model operates without pre-training or CoT data, yet achieves nearly perfect performance on challenging tasks including complex Sudoku puzzles and optimal path finding in large mazes. Furthermore, HRM outperforms much larger models with significantly longer context windows on the Abstraction and Reasoning Corpus (ARC), a key benchmark for measuring artificial general intelligence capabilities. These results underscore HRM's potential as a transformative advancement toward universal computation and general-purpose reasoning systems.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/absolooot1"&gt; /u/absolooot1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2506.21734"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lo84yj/250621734_hierarchical_reasoning_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lo84yj/250621734_hierarchical_reasoning_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T13:54:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1lokcrw</id>
    <title>How do "AI detectors" work</title>
    <updated>2025-06-30T21:50:20+00:00</updated>
    <author>
      <name>/u/BlueeWaater</name>
      <uri>https://old.reddit.com/user/BlueeWaater</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey there, I'm doing research on how &amp;quot;AI detectors&amp;quot; work or if they are even real? they sound like snake oil to me... but do people actually pay for that? any insights on this would be highly appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BlueeWaater"&gt; /u/BlueeWaater &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lokcrw/how_do_ai_detectors_work/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lokcrw/how_do_ai_detectors_work/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lokcrw/how_do_ai_detectors_work/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T21:50:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1lohzzj</id>
    <title>How to run Hunyuan-A13B on a RTX 5090 / Blackwell ?</title>
    <updated>2025-06-30T20:16:23+00:00</updated>
    <author>
      <name>/u/celsowm</name>
      <uri>https://old.reddit.com/user/celsowm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi folks!&lt;/p&gt; &lt;p&gt;Since the launch of Hunyuan-A13B, I‚Äôve been struggling to get it running on an RTX 5090 with 32 GB of RAM. The official Docker images from Tencent don‚Äôt seem to be compatible with the Blackwell architecture. I even tried building vLLM from source via &lt;code&gt;git clone&lt;/code&gt;, but no luck either.&lt;/p&gt; &lt;p&gt;Any hints?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/celsowm"&gt; /u/celsowm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lohzzj/how_to_run_hunyuana13b_on_a_rtx_5090_blackwell/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lohzzj/how_to_run_hunyuana13b_on_a_rtx_5090_blackwell/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lohzzj/how_to_run_hunyuana13b_on_a_rtx_5090_blackwell/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T20:16:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1lo5vnf</id>
    <title>What is the current best local coding model with &lt;= 4B parameters?</title>
    <updated>2025-06-30T12:10:32+00:00</updated>
    <author>
      <name>/u/Wooden-Key751</name>
      <uri>https://old.reddit.com/user/Wooden-Key751</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I am looking for &amp;lt;= 4B coding models. I realize that none of these will be practical for now just looking for some to do experiments.&lt;/p&gt; &lt;p&gt;Here is what i found so far:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Menlo / Jan-nano ‚Äî 4.02 B (Not really coding but I expect it to be better than others)&lt;/li&gt; &lt;li&gt;Gemma ‚Äî 4 B / 2 B&lt;/li&gt; &lt;li&gt;Qwen 3 ‚Äî 4 B / 0.6 B&lt;/li&gt; &lt;li&gt;Phi-4 Mini ‚Äî 3.8 B&lt;/li&gt; &lt;li&gt;Phi-3.5 Mini ‚Äî 3.5 B&lt;/li&gt; &lt;li&gt;Llama-3.2 ‚Äî 3.2 B&lt;/li&gt; &lt;li&gt;Starcoder ‚Äî 3 B / 1 B&lt;/li&gt; &lt;li&gt;Starcoder 2 ‚Äî 3 B&lt;/li&gt; &lt;li&gt;Stable-Code ‚Äî 3 B&lt;/li&gt; &lt;li&gt;Granite ‚Äî 3 B / 2.53 B&lt;/li&gt; &lt;li&gt;Cogito ‚Äî 3 B&lt;/li&gt; &lt;li&gt;DeepSeek Coder ‚Äî 2.6 B / 1.3 B&lt;/li&gt; &lt;li&gt;DeepSeek R1 Distill (Qwen-tuned) ‚Äî 1.78 B&lt;/li&gt; &lt;li&gt;Qwen 2.5 ‚Äî 1.5 B / 0.5 B&lt;/li&gt; &lt;li&gt;Yi-Coder ‚Äî 1.5 B&lt;/li&gt; &lt;li&gt;Deepscaler ‚Äî 1.5 B&lt;/li&gt; &lt;li&gt;Deepcoder ‚Äî 1.5 B&lt;/li&gt; &lt;li&gt;CodeGen2 ‚Äî 1 B&lt;/li&gt; &lt;li&gt;BitNet-B1.58 ‚Äî 0.85 B&lt;/li&gt; &lt;li&gt;ERNIE-4.5 ‚Äî 0.36 B&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Has anyone tried any of these or compared &amp;lt;= 4B models on coding tasks? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wooden-Key751"&gt; /u/Wooden-Key751 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lo5vnf/what_is_the_current_best_local_coding_model_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lo5vnf/what_is_the_current_best_local_coding_model_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lo5vnf/what_is_the_current_best_local_coding_model_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T12:10:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1lnrd1t</id>
    <title>You can just RL a model to beat any "AI detectors"</title>
    <updated>2025-06-29T22:22:55+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnrd1t/you_can_just_rl_a_model_to_beat_any_ai_detectors/"&gt; &lt;img alt="You can just RL a model to beat any &amp;quot;AI detectors&amp;quot;" src="https://external-preview.redd.it/nkhh65ujo5BznFJFojoMPaKjGuLSpPj6KGhRov-ykOg.png?width=216&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0e2f90964c81a1de52938be6bcb08665605293f2" title="You can just RL a model to beat any &amp;quot;AI detectors&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/p4binxqqvx9f1.png?width=783&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5af26533b3e667d6f0382d11163331aedf6bc42d"&gt;https://preview.redd.it/p4binxqqvx9f1.png?width=783&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5af26533b3e667d6f0382d11163331aedf6bc42d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/k4tcfdmsvx9f1.png?width=2574&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=934ff9d043c7021764743c443feff0f0767c25cd"&gt;https://preview.redd.it/k4tcfdmsvx9f1.png?width=2574&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=934ff9d043c7021764743c443feff0f0767c25cd&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Baseline&lt;br /&gt; ‚Ä¢ Model: Llama-3.1 8B-Instruct&lt;br /&gt; ‚Ä¢ Prompt: plain &amp;quot;Write an essay about X&amp;quot;&lt;br /&gt; ‚Ä¢ Detector: ZeroGPT&lt;br /&gt; Result: 100 % AI-written&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/09nmithvvx9f1.png?width=1204&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=82d0071d8579effb1f1b75eaa5c037a56385ef9d"&gt;https://preview.redd.it/09nmithvvx9f1.png?width=1204&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=82d0071d8579effb1f1b75eaa5c037a56385ef9d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Data&lt;br /&gt; ‚Ä¢ Synthetic dataset of 150 school-style prompts (history, literature, tech). Nothing fancy, just json lines + system prompt &amp;quot;You are a human essay writer&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/d189whuxvx9f1.png?width=3456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5fdd406d1df4a40f3f4c1623b6b049026559f29e"&gt;https://preview.redd.it/d189whuxvx9f1.png?width=3456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5fdd406d1df4a40f3f4c1623b6b049026559f29e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;First training run&lt;br /&gt; After ~30 GRPO steps on a single A100:&lt;br /&gt; ‚Ä¢ ZeroGPT score drops from 100 ‚Üí 42 %&lt;br /&gt; The model learned:&lt;br /&gt; Write a coherent intro&lt;br /&gt; Stuff one line of high-entropy junk&lt;br /&gt; Finish normally&lt;br /&gt; Average &amp;quot;human-ness&amp;quot; skyrockets because detector averages per-sentence scores&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/c4bkar70wx9f1.png?width=941&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4e3a86287c2d0cc273fd9f3854634cbd7c8ecf75"&gt;https://preview.redd.it/c4bkar70wx9f1.png?width=941&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4e3a86287c2d0cc273fd9f3854634cbd7c8ecf75&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Patch #1&lt;br /&gt; Added a gibberish classifier (tiny DistilRoBERTa) and multiplied reward by its minimum &amp;quot;clean&amp;quot; score. Junk lines now tank reward ‚Üí behaviour disappears. GRPO‚Äôs beta ‚âà how harshly to penalize incoherence. Set Œ≤ = 0.4 and reward curve stabilized; no more oscillation between genius &amp;amp; garbage. Removed reasoning (memory constraints).&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/prmgkja2wx9f1.png?width=652&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=79f46c100445337e257dc3b7666ffdf2ba826252"&gt;https://preview.redd.it/prmgkja2wx9f1.png?width=652&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=79f46c100445337e257dc3b7666ffdf2ba826252&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Tiny models crush it&lt;br /&gt; Swapped in Qwen 0.5B LoRA rank 8, upped num_generations ‚Üí 64.&lt;br /&gt; Result after 7 steps: best sample already at 28 % &amp;quot;human&amp;quot;. Smaller vocab seems to help leak less LM &amp;quot;signature&amp;quot; (the model learned to use lots of proper nouns to trick the detector).&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/2e6g1pm7wx9f1.png?width=800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cfbcaa7fd8c6baa2a05d063a3989ba282c8d31a2"&gt;https://preview.redd.it/2e6g1pm7wx9f1.png?width=800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cfbcaa7fd8c6baa2a05d063a3989ba282c8d31a2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Colab: &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B"&gt;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb&lt;/a&gt;-GRPO.ipynb)&lt;/p&gt; &lt;p&gt;Detector bug?&lt;br /&gt; ZeroGPT sometimes marks the first half AI, second half human for the same paragraph. The RL agent locks onto that gradient and exploits it. Classifier clearly over-fits surface patterns rather than semantics&lt;/p&gt; &lt;p&gt;Single scalar feedback is enough for LMs to reverse-engineer public detectors &lt;/p&gt; &lt;p&gt;Add even a tiny auxiliary reward (gibberish, length) to stop obvious failure modes &lt;/p&gt; &lt;p&gt;Public &amp;quot;AI/Not-AI&amp;quot; classifiers are security-through-obscurity&lt;/p&gt; &lt;p&gt;Reward function: &lt;a href="https://codefile.io/f/R4O9IdGEhg"&gt;https://codefile.io/f/R4O9IdGEhg&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnrd1t/you_can_just_rl_a_model_to_beat_any_ai_detectors/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnrd1t/you_can_just_rl_a_model_to_beat_any_ai_detectors/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lnrd1t/you_can_just_rl_a_model_to_beat_any_ai_detectors/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-29T22:22:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1loj134</id>
    <title>arXiv2Docker: Computational Reproducibility with the ExperimentOps Agent</title>
    <updated>2025-06-30T20:57:06+00:00</updated>
    <author>
      <name>/u/remyxai</name>
      <uri>https://old.reddit.com/user/remyxai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1loj134/arxiv2docker_computational_reproducibility_with/"&gt; &lt;img alt="arXiv2Docker: Computational Reproducibility with the ExperimentOps Agent" src="https://preview.redd.it/rak71t31n4af1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fcce9df76f1ce564209c9bfa33c8883ba2b4cbc5" title="arXiv2Docker: Computational Reproducibility with the ExperimentOps Agent" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We've all been there, spend a morning setting up to find out it's not gonna work for your application.&lt;/p&gt; &lt;p&gt;From &lt;a href="https://arxiv.org/pdf/2409.07440"&gt;SUPER&lt;/a&gt;:&lt;/p&gt; &lt;p&gt;&lt;em&gt;As a recent study shows (Storks et al., 2023), both novice and advanced researchers find the challenge of &amp;quot;setting up the code base&amp;quot; to be the most difficult part of reproducing experiments.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;I'm sharing auto-generated Docker images for papers my agent recommends based on what I'm building.&lt;/p&gt; &lt;p&gt;Today's recommendation: &lt;strong&gt;LLaVA-Scissor&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;docker pull remyxai/2506.21862v1:latest docker run --gpus all -it remyxai/2506.21862v1 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;More on &lt;a href="https://remyxai.substack.com/p/the-experimentops-agent"&gt;ExperimentOps&lt;/a&gt; and computational reproducibility. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/remyxai"&gt; /u/remyxai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rak71t31n4af1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1loj134/arxiv2docker_computational_reproducibility_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1loj134/arxiv2docker_computational_reproducibility_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T20:57:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1lnlxp1</id>
    <title>4x 4090 48GB inference box (I may have overdone it)</title>
    <updated>2025-06-29T18:33:40+00:00</updated>
    <author>
      <name>/u/101m4n</name>
      <uri>https://old.reddit.com/user/101m4n</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnlxp1/4x_4090_48gb_inference_box_i_may_have_overdone_it/"&gt; &lt;img alt="4x 4090 48GB inference box (I may have overdone it)" src="https://external-preview.redd.it/o67J1SHcLKrQAlXicnfT20w0glJr7s4wb4-c1GOwiA8.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=936572d5a67f4298cbb8ecc135d737e991ade403" title="4x 4090 48GB inference box (I may have overdone it)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A few months ago I discovered that 48GB 4090s were starting to show up on the western market in large numbers. I didn't think much of it at the time, but then I got my payout from the mt.gox bankruptcy filing (which has been ongoing for over 10 years now), and decided to blow a chunk of it on an inference box for local machine learning experiments.&lt;/p&gt; &lt;p&gt;After a delay receiving some of the parts (and admittedly some procrastination on my end), I've finally found the time to put the whole machine together!&lt;/p&gt; &lt;p&gt;Specs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Asrock romed8-2t motherboard (SP3)&lt;/li&gt; &lt;li&gt;32 core epyc&lt;/li&gt; &lt;li&gt;256GB 2666V memory&lt;/li&gt; &lt;li&gt;4x &amp;quot;tronizm&amp;quot; rtx 4090D 48GB modded GPUs from china&lt;/li&gt; &lt;li&gt;2x 1tb nvme (striped) for OS and local model storage&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The cards are very well built. I have no doubts as to their quality whatsoever. They were heavy, the heatsinks made contact with all the board level components and the shrouds were all-metal and very solid. It was almost a shame to take them apart! They were however incredibly loud. At idle, the fan sits at 30%, and at that level they are already as loud as the loudest blower cards for gaming. At full load, they are truly deafening and definitely not something you want to share space with. Hence the water-cooling.&lt;/p&gt; &lt;p&gt;There are however no full-cover waterblocks for these GPUs (they use a custom PCB), so to cool them I had to get a little creative. Corsair makes a (kinda) &lt;a href="https://www.corsair.com/uk/en/p/custom-liquid-cooling/cx-9025001-ww/icue-link-xg3-rgb-hybrid-gpu-water-block-4090-4080-cx-9025001-ww?srsltid=AfmBOopBdweqKN5Wpj6wHKLSR9SEYZmNpOpOyaFZTLLdld7hLBrg1iCg"&gt;generic block&lt;/a&gt; called the xg3. The product itself is a bit rubbish, requiring corsairs proprietary i-cue system to run the fan which is supposed to cool the components not covered by the coldplate. It's also overpriced. However these are more or less the only option here. As a side note, these &amp;quot;generic&amp;quot; blocks only work work because the mounting hole and memory layout around the core is actually standardized to some extent, something I learned during my research.&lt;/p&gt; &lt;p&gt;The cold-plate on these blocks turned out to foul one of the components near the core, so I had to modify them a bit. I also couldn't run the aforementioned fan without corsairs i-cue link nonsense and the fan and shroud were too thick anyway and would have blocked the next GPU anyway. So I removed the plastic shroud and fabricated a frame + heatsink arrangement to add some support and cooling for the VRMs and other non-core components.&lt;/p&gt; &lt;p&gt;As another side note, the marketing material for the xg3 claims that the block contains a built-in temperature sensor. However I saw no indication of a sensor anywhere when disassembling the thing. Go figure.&lt;/p&gt; &lt;p&gt;Lastly there's the case. I couldn't find a case that I liked the look of that would support three 480mm radiators, so I built something out of pine furniture board. Not the easiest or most time efficient approach, but it was fun and it does the job (fire hazard notwithstanding).&lt;/p&gt; &lt;p&gt;As for what I'll be using it for, I'll be hosting an LLM for local day-to-day usage, but I also have some more unique project ideas, some of which may show up here in time. Now that such projects won't take up resources on my regular desktop, I can afford to do a lot of things I previously couldn't!&lt;/p&gt; &lt;p&gt;P.S. If anyone has any questions or wants to replicate any of what I did here, feel free to DM me with any questions, I'm glad to help any way I can!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/101m4n"&gt; /u/101m4n &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lnlxp1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnlxp1/4x_4090_48gb_inference_box_i_may_have_overdone_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lnlxp1/4x_4090_48gb_inference_box_i_may_have_overdone_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-29T18:33:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1lok3r2</id>
    <title>[News] Datacenter GPUs May Have an Astonishingly Short Lifespan of Only 1 to 3 Years | TrendForce News</title>
    <updated>2025-06-30T21:40:05+00:00</updated>
    <author>
      <name>/u/EasternBeyond</name>
      <uri>https://old.reddit.com/user/EasternBeyond</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lok3r2/news_datacenter_gpus_may_have_an_astonishingly/"&gt; &lt;img alt="[News] Datacenter GPUs May Have an Astonishingly Short Lifespan of Only 1 to 3 Years | TrendForce News" src="https://external-preview.redd.it/7cRnC2dFTB8VTd7qs9tim3BVul_HOXlhVu97BYC8mXw.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cfea0e06944005f53398ccc99f53814a8c4923f4" title="[News] Datacenter GPUs May Have an Astonishingly Short Lifespan of Only 1 to 3 Years | TrendForce News" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EasternBeyond"&gt; /u/EasternBeyond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.trendforce.com/news/2024/10/31/news-datacenter-gpus-may-have-an-astonishingly-short-lifespan-of-only-1-to-3-years/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lok3r2/news_datacenter_gpus_may_have_an_astonishingly/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lok3r2/news_datacenter_gpus_may_have_an_astonishingly/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T21:40:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1lobyx5</id>
    <title>Upcoming Coding Models?</title>
    <updated>2025-06-30T16:25:45+00:00</updated>
    <author>
      <name>/u/pmttyji</name>
      <uri>https://old.reddit.com/user/pmttyji</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Based on past threads from this sub, I see that below coding models are coming.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Qwen3 Coder - &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1lm92se/qwen3_coder_soon/"&gt;Recent thread&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Deep Cogito - Preview models there&lt;/li&gt; &lt;li&gt;Polaris - Preview models there&lt;/li&gt; &lt;li&gt;Granite releasing any new coding models? Preview (General) models there for upcoming Version 4. How good is their existing coding models.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;What other coding models coming apart from above ones?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmttyji"&gt; /u/pmttyji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lobyx5/upcoming_coding_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lobyx5/upcoming_coding_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lobyx5/upcoming_coding_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T16:25:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1lnxo8y</id>
    <title>Major AI platforms will eventually have ads</title>
    <updated>2025-06-30T03:40:35+00:00</updated>
    <author>
      <name>/u/MattDTO</name>
      <uri>https://old.reddit.com/user/MattDTO</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I see this as a huge reason to continue advancement of local LLMs. OpenAI, Google, Microsoft, Anthropic, etc. all the big players have investors to answer to, and will eventually need to stop burning money. They will get pressured into a sustainable business model. I think Google has already lost a lot of traffic to AI search that they will try to win back. Right now, they are giving LLM access in exchange for data to train on. Eventually they will have enough that it won‚Äôt be worth it anymore. &lt;/p&gt; &lt;p&gt;Anyone else see this coming?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MattDTO"&gt; /u/MattDTO &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnxo8y/major_ai_platforms_will_eventually_have_ads/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnxo8y/major_ai_platforms_will_eventually_have_ads/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lnxo8y/major_ai_platforms_will_eventually_have_ads/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T03:40:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1load8a</id>
    <title>[Day 6/50] Building a Small Language Model from Scratch - What Is Positional Embedding and Why Does It Matter?</title>
    <updated>2025-06-30T15:23:52+00:00</updated>
    <author>
      <name>/u/Prashant-Lakhera</name>
      <uri>https://old.reddit.com/user/Prashant-Lakhera</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1load8a/day_650_building_a_small_language_model_from/"&gt; &lt;img alt="[Day 6/50] Building a Small Language Model from Scratch - What Is Positional Embedding and Why Does It Matter?" src="https://b.thumbs.redditmedia.com/QrXwS0MMtdu4-LCvZnP-VTv25rOcYvXpPucHJrkYiSQ.jpg" title="[Day 6/50] Building a Small Language Model from Scratch - What Is Positional Embedding and Why Does It Matter?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/dbs9gal713af1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cea1a073a4106381b16f3f732c8c137a894c4dc7"&gt;https://preview.redd.it/dbs9gal713af1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cea1a073a4106381b16f3f732c8c137a894c4dc7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you‚Äôve ever peeked inside models like GPT or BERT and wondered &lt;em&gt;how&lt;/em&gt; they understand the &lt;em&gt;order&lt;/em&gt; of words, the secret sauce is something called positional embedding.&lt;/p&gt; &lt;p&gt;Without it, a language model can‚Äôt tell the difference between:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;‚ÄúThe cat sat on the mat‚Äù&lt;/li&gt; &lt;li&gt;‚ÄúThe mat sat on the cat‚Äù&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;The Problem: Transformers Don‚Äôt Understand Word Order&lt;/h1&gt; &lt;p&gt;Transformers process all tokens at once, which is great for speed, but unlike RNNs, they don‚Äôt read text sequentially. That means they don‚Äôt naturally know the order of words.&lt;/p&gt; &lt;p&gt;To a plain Transformer, ‚ÄúI love AI‚Äù could mean the same as ‚ÄúAI love I.‚Äù&lt;/p&gt; &lt;h1&gt;The Solution: Positional Embeddings&lt;/h1&gt; &lt;p&gt;To fix this, we add a second layer of information: positional embeddings. These vectors tell the model &lt;em&gt;where&lt;/em&gt; each word appears in the input sequence.&lt;/p&gt; &lt;p&gt;So instead of just using word embeddings, we do:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Final Input = Word Embedding + Positional Embedding &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now the model knows both the meaning of each word and its position in the sentence.&lt;/p&gt; &lt;h1&gt;Why Not Let the Model Learn Position on Its Own?&lt;/h1&gt; &lt;p&gt;In theory, a large model &lt;em&gt;could&lt;/em&gt; infer word order from patterns. But in practice, that‚Äôs inefficient and unreliable. Positional embeddings provide the model with a strong starting point, akin to adding page numbers to a shuffled book.&lt;/p&gt; &lt;h1&gt;Two Common Types of Positional Embeddings&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Sinusoidal Positional Embeddings&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Used in the original Transformer paper&lt;/li&gt; &lt;li&gt;Not learned, uses sine and cosine functions&lt;/li&gt; &lt;li&gt;Good for generalizing to longer sequences&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Learned Positional Embeddings&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Used in models like BERT&lt;/li&gt; &lt;li&gt;Learned during training, like word embeddings&lt;/li&gt; &lt;li&gt;Flexible, but may not generalize well to unseen sequence lengths&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Real Example: Why It Matters&lt;/h1&gt; &lt;p&gt;Compare:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;‚ÄúThe dog chased the cat.‚Äù&lt;/li&gt; &lt;li&gt;‚ÄúThe cat chased the dog‚Äù&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Same words, totally different meaning. Without positional embeddings, the model can‚Äôt tell which animal is doing the chasing.&lt;/p&gt; &lt;h1&gt;What‚Äôs New: Rotary Positional Embeddings (RoPE)&lt;/h1&gt; &lt;p&gt;Modern models, such as DeepSeek and LLaMA, utilize RoPE to integrate position into the attention mechanism itself. It‚Äôs more efficient for long sequences and performs better in certain settings.&lt;/p&gt; &lt;h1&gt;TL;DR&lt;/h1&gt; &lt;p&gt;Positional embeddings help Transformers make sense of word order. Without them, a model is just guessing how words relate to each other, like trying to read a book with the pages shuffled.&lt;/p&gt; &lt;p&gt;üëâ Tomorrow, we‚Äôre going to code positional embeddings from scratch‚Äîso stay tuned!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prashant-Lakhera"&gt; /u/Prashant-Lakhera &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1load8a/day_650_building_a_small_language_model_from/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1load8a/day_650_building_a_small_language_model_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1load8a/day_650_building_a_small_language_model_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T15:23:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1lnu4zl</id>
    <title>Baidu releases ERNIE 4.5 models on huggingface</title>
    <updated>2025-06-30T00:34:16+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnu4zl/baidu_releases_ernie_45_models_on_huggingface/"&gt; &lt;img alt="Baidu releases ERNIE 4.5 models on huggingface" src="https://external-preview.redd.it/Wyzo5BvQjbbXvCrrpLypEcj3XicuXWigLyl_Acs2b5k.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=33f879948e7c84df63582ea3398eb078c0298c8e" title="Baidu releases ERNIE 4.5 models on huggingface" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;llama.cpp support for ERNIE 4.5 0.3B&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/14408"&gt;https://github.com/ggml-org/llama.cpp/pull/14408&lt;/a&gt;&lt;/p&gt; &lt;p&gt;vllm Ernie4.5 and Ernie4.5MoE Model Support&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/vllm-project/vllm/pull/20220"&gt;https://github.com/vllm-project/vllm/pull/20220&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/baidu/ernie-45-6861cd4c9be84540645f35c9"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnu4zl/baidu_releases_ernie_45_models_on_huggingface/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lnu4zl/baidu_releases_ernie_45_models_on_huggingface/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T00:34:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1lokp88</id>
    <title>Intel Arc Pro B60 Dual 48G Turbo Maxsun GPU Pricing Revealed</title>
    <updated>2025-06-30T22:04:32+00:00</updated>
    <author>
      <name>/u/Airwalker19</name>
      <uri>https://old.reddit.com/user/Airwalker19</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Like many others, I was hyped for the dual GPU Intel Arc Pro B60, so I emailed Maxsun for a quote. Their US distributor hit me back with $5k per unit for 3 GPUs, or $4.5k each for 5+.&lt;/p&gt; &lt;p&gt;Sure, dual GPUs should cost more, but this is &lt;em&gt;10x&lt;/em&gt; the rumored MSRP of the 24GB card. Space savings are nice, but not &lt;em&gt;that&lt;/em&gt; nice.&lt;/p&gt; &lt;p&gt;RIP my hopes for an (affordable) AI desktop win.&lt;/p&gt; &lt;p&gt;Anyone else think this pricing is delusional, or just me?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Airwalker19"&gt; /u/Airwalker19 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lokp88/intel_arc_pro_b60_dual_48g_turbo_maxsun_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lokp88/intel_arc_pro_b60_dual_48g_turbo_maxsun_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lokp88/intel_arc_pro_b60_dual_48g_turbo_maxsun_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T22:04:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1lojlrw</id>
    <title>[WIRED] Here Is Everyone Mark Zuckerberg Has Hired So Far for Meta‚Äôs ‚ÄòSuperintelligence‚Äô Team</title>
    <updated>2025-06-30T21:19:51+00:00</updated>
    <author>
      <name>/u/bllshrfv</name>
      <uri>https://old.reddit.com/user/bllshrfv</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lojlrw/wired_here_is_everyone_mark_zuckerberg_has_hired/"&gt; &lt;img alt="[WIRED] Here Is Everyone Mark Zuckerberg Has Hired So Far for Meta‚Äôs ‚ÄòSuperintelligence‚Äô Team" src="https://external-preview.redd.it/hHtdtWWuX05qlxJIIZFgrRzaMxdrmlIQ8OiqTPog1_w.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e97f33d6160ce6f067a79278cab0942d295e3325" title="[WIRED] Here Is Everyone Mark Zuckerberg Has Hired So Far for Meta‚Äôs ‚ÄòSuperintelligence‚Äô Team" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bllshrfv"&gt; /u/bllshrfv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.wired.com/story/mark-zuckerberg-welcomes-superintelligence-team/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lojlrw/wired_here_is_everyone_mark_zuckerberg_has_hired/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lojlrw/wired_here_is_everyone_mark_zuckerberg_has_hired/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T21:19:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1lodmc6</id>
    <title>ERNIE 4.5 Collection from Baidu</title>
    <updated>2025-06-30T17:27:55+00:00</updated>
    <author>
      <name>/u/AppearanceHeavy6724</name>
      <uri>https://old.reddit.com/user/AppearanceHeavy6724</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AppearanceHeavy6724"&gt; /u/AppearanceHeavy6724 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ernie.baidu.com/blog/posts/ernie4.5/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lodmc6/ernie_45_collection_from_baidu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lodmc6/ernie_45_collection_from_baidu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T17:27:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1lococc</id>
    <title>Open Source AI Editor: First Milestone</title>
    <updated>2025-06-30T16:52:52+00:00</updated>
    <author>
      <name>/u/isidor_n</name>
      <uri>https://old.reddit.com/user/isidor_n</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lococc/open_source_ai_editor_first_milestone/"&gt; &lt;img alt="Open Source AI Editor: First Milestone" src="https://external-preview.redd.it/7W6FU5na7gC1vKxg3pWb3QkD0a8T5GyzeaLh8U3roNc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d188c22d72aa036de764ff96aa9d951cba5ae6b3" title="Open Source AI Editor: First Milestone" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Let me know if you have any questions about open sourcing. Happy to answer. &lt;/p&gt; &lt;p&gt;vscode pm here&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/isidor_n"&gt; /u/isidor_n &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://code.visualstudio.com/blogs/2025/06/30/openSourceAIEditorFirstMilestone"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lococc/open_source_ai_editor_first_milestone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lococc/open_source_ai_editor_first_milestone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T16:52:52+00:00</published>
  </entry>
</feed>
