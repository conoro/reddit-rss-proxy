<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-26T11:48:39+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1pw23mi</id>
    <title>Learning Java by Building an AI Agent Library. Would Love Your Feedback</title>
    <updated>2025-12-26T11:23:43+00:00</updated>
    <author>
      <name>/u/Time-Plum-7893</name>
      <uri>https://old.reddit.com/user/Time-Plum-7893</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been diving into Java as a learning project, and coming from a Python background (where I've used libraries like PydanticAI and LangGraph), I wanted to challenge myself with something ambitious: building an agentic AI library in Java.&lt;/p&gt; &lt;p&gt;After playing with **LangChain4J** and **Spring AI**, I found the learning curve steeper than expected‚Äîespecially around patterns like agent instantiation, structured outputs, and streaming. The concepts felt quite different from their Python counterparts, and I kept thinking: &amp;quot;I wish there was something more intuitive for someone like me.&amp;quot;&lt;/p&gt; &lt;p&gt;So instead of just complaining, I built something. It was as much a learning exercise as anything else.&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;## The Pain Points I Tried to Solve&lt;/p&gt; &lt;p&gt;Here's what tripped me up when learning Java AI frameworks:&lt;/p&gt; &lt;p&gt;**1. Agent orchestration felt complex**&lt;/p&gt; &lt;p&gt;I wanted something closer to what I'd seen in Python‚Äîeasy handoffs between agents, routing, running things in parallel.&lt;/p&gt; &lt;p&gt;**2. Human-in-the-loop per tool (not globally)**&lt;/p&gt; &lt;p&gt;Most examples I found were &amp;quot;approve everything or nothing.&amp;quot; I wanted granular control: `delete_records` should pause for approval, but `get_weather` should just run.&lt;/p&gt; &lt;p&gt;**3. Streaming + structured output together**&lt;/p&gt; &lt;p&gt;I wanted to update a UI in real-time while JSON was being generated, not wait for the complete response.&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;## What I Ended Up Building&lt;/p&gt; &lt;p&gt;I'm calling it **Agentle4j** (yes, the name needs work üòÖ). Here's what it looks like:&lt;/p&gt; &lt;p&gt;### Multi-Agent Routing&lt;/p&gt; &lt;p&gt;```java&lt;/p&gt; &lt;p&gt;RouterAgent router = RouterAgent.builder()&lt;/p&gt; &lt;p&gt;.addRoute(billingAgent, &amp;quot;invoices, payments&amp;quot;)&lt;/p&gt; &lt;p&gt;.addRoute(techSupport, &amp;quot;bugs, errors&amp;quot;)&lt;/p&gt; &lt;p&gt;.fallback(generalAgent)&lt;/p&gt; &lt;p&gt;.build();&lt;/p&gt; &lt;p&gt;// Or run agents in parallel&lt;/p&gt; &lt;p&gt;ParallelAgents team = ParallelAgents.of(researcher, analyst);&lt;/p&gt; &lt;p&gt;AgentResult combined = team.runAndSynthesize(&amp;quot;Market trends?&amp;quot;, writer);&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;### Guardrails &amp;amp; Handoffs&lt;/p&gt; &lt;p&gt;```java&lt;/p&gt; &lt;p&gt;Agent agent = Agent.builder()&lt;/p&gt; &lt;p&gt;.addInputGuardrail((input, ctx) -&amp;gt; &lt;/p&gt; &lt;p&gt;input.contains(&amp;quot;password&amp;quot;) ? GuardrailResult.reject(&amp;quot;Blocked&amp;quot;) : GuardrailResult.pass())&lt;/p&gt; &lt;p&gt;.addHandoff(Handoff.to(billingAgent, &amp;quot;billing issues&amp;quot;))&lt;/p&gt; &lt;p&gt;.build();&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;### Per-Tool Human Approval&lt;/p&gt; &lt;p&gt;```java&lt;/p&gt; &lt;p&gt;&lt;a href="/u/FunctionMetadata"&gt;u/FunctionMetadata&lt;/a&gt;(name = &amp;quot;delete_records&amp;quot;, requiresConfirmation = true)&lt;/p&gt; &lt;p&gt;public class DeleteTool extends FunctionTool&amp;lt;...&amp;gt; { }&lt;/p&gt; &lt;p&gt;// Only dangerous tools pause for approval&lt;/p&gt; &lt;p&gt;agent.interactStream(&amp;quot;Delete old users and check weather&amp;quot;)&lt;/p&gt; &lt;p&gt;.onToolCallPending((tool, approve) -&amp;gt; approve.accept(askUser(&amp;quot;Execute &amp;quot; + tool.name() + &amp;quot;?&amp;quot;)))&lt;/p&gt; &lt;p&gt;.start();&lt;/p&gt; &lt;p&gt;// delete_records ‚Üí waits | get_weather ‚Üí auto-executes&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;### Structured Outputs (Type-Safe)&lt;/p&gt; &lt;p&gt;```java&lt;/p&gt; &lt;p&gt;record Person(String name, int age, List&amp;lt;String&amp;gt; skills) {}&lt;/p&gt; &lt;p&gt;Person person = responder.respond(payload)&lt;/p&gt; &lt;p&gt;.withStructuredOutput(Person.class)&lt;/p&gt; &lt;p&gt;.join()&lt;/p&gt; &lt;p&gt;.parsed();&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;### Streaming with Partial JSON Parsing&lt;/p&gt; &lt;p&gt;```java&lt;/p&gt; &lt;p&gt;responder.respond(payload)&lt;/p&gt; &lt;p&gt;.onTextDelta(System.out::print)&lt;/p&gt; &lt;p&gt;.onToolCall((name, args) -&amp;gt; System.out.println(&amp;quot;üîß &amp;quot; + name))&lt;/p&gt; &lt;p&gt;.onPartialJson(fields -&amp;gt; updateUI(fields.get(&amp;quot;title&amp;quot;))) // Update UI before response completes!&lt;/p&gt; &lt;p&gt;.start();&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;### Cross-Conversation Memory&lt;/p&gt; &lt;p&gt;```java&lt;/p&gt; &lt;p&gt;Agent agent = Agent.builder()&lt;/p&gt; &lt;p&gt;.addMemoryTools(InMemoryMemory.create()) // or Redis, JDBC&lt;/p&gt; &lt;p&gt;.build();&lt;/p&gt; &lt;p&gt;agent.interact(&amp;quot;My favorite color is blue&amp;quot;, context);&lt;/p&gt; &lt;p&gt;// Later: &amp;quot;What's my favorite color?&amp;quot; ‚Üí &amp;quot;blue&amp;quot;&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;## Other Things It Includes&lt;/p&gt; &lt;p&gt;- **300+ models** via OpenRouter (GPT-4o, Claude, Gemini, Llama, etc.)&lt;/p&gt; &lt;p&gt;- **Vision support** for image analysis&lt;/p&gt; &lt;p&gt;- **Built-in OpenTelemetry** for observability&lt;/p&gt; &lt;p&gt;- **Async-first** with `CompletableFuture` and virtual threads&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;## Honest Limitations&lt;/p&gt; &lt;p&gt;- **No built-in RAG/vector stores** ‚Äî you'd need to use tools to integrate your own&lt;/p&gt; &lt;p&gt;- **Still new** ‚Äî definitely not as battle-tested as LangChain4J or Spring AI&lt;/p&gt; &lt;p&gt;- **No MCP support yet** ‚Äî actively working on this&lt;/p&gt; &lt;p&gt;- **Requires Java 21+**&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;## Questions for the Community&lt;/p&gt; &lt;p&gt;For folks using **LangChain4J** or **Spring AI**, I'm genuinely curious:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;What keeps you on your current framework?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;What pain points do you still encounter?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;What's the one feature you wish existed?&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I'm not trying to replace anything here‚ÄîI built this to learn and to scratch my own itch. But if anyone's interested in poking around or has feedback, I'd really appreciate it.&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;üîó **Docs**: &lt;a href="https://paragon-intelligence.github.io/agentle4j/"&gt;https://paragon-intelligence.github.io/agentle4j/&lt;/a&gt; &lt;/p&gt; &lt;p&gt;üîó **GitHub**: &lt;a href="https://github.com/paragon-intelligence/agentle4j"&gt;https://github.com/paragon-intelligence/agentle4j&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks for reading! üôè&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Time-Plum-7893"&gt; /u/Time-Plum-7893 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pw23mi/learning_java_by_building_an_ai_agent_library/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pw23mi/learning_java_by_building_an_ai_agent_library/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pw23mi/learning_java_by_building_an_ai_agent_library/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T11:23:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1pveluj</id>
    <title>Honestly, has anyone actually tried GLM 4.7 yet? (Not just benchmarks)</title>
    <updated>2025-12-25T14:35:15+00:00</updated>
    <author>
      <name>/u/Empty_Break_8792</name>
      <uri>https://old.reddit.com/user/Empty_Break_8792</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm seeing all these charts claiming GLM 4.7 is officially the ‚ÄúSonnet 4.5 and GPT-5.2 killer‚Äù for coding and math. The benchmarks look insane, but we all know how easy it is to game those for a release day hype cycle.&lt;/p&gt; &lt;p&gt;I‚Äôm specifically curious about using it as a daily driver for complex web development. Most of my work involves managing complex TypeScript code and refactoring legacy React code.&lt;/p&gt; &lt;p&gt;For those of you who have actually hooked the API into an agent like &lt;strong&gt;Kilo Code&lt;/strong&gt; or &lt;strong&gt;OpenCode&lt;/strong&gt; (or even just &lt;strong&gt;Cline&lt;/strong&gt; / &lt;strong&gt;Roo Code&lt;/strong&gt;), how is your experience with it? Please be honest i don't just believe the benchmarks. Tell me if you really use it, and with which agent?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Empty_Break_8792"&gt; /u/Empty_Break_8792 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pveluj/honestly_has_anyone_actually_tried_glm_47_yet_not/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pveluj/honestly_has_anyone_actually_tried_glm_47_yet_not/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pveluj/honestly_has_anyone_actually_tried_glm_47_yet_not/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-25T14:35:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1pvw9jo</id>
    <title>Day 18: 21 Days of Building a Small Language Model: Quantization</title>
    <updated>2025-12-26T05:12:24+00:00</updated>
    <author>
      <name>/u/Prashant-Lakhera</name>
      <uri>https://old.reddit.com/user/Prashant-Lakhera</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Merry Christmas to all of you üéÑ&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Today, I want to talk about one of my favorite topics, quantization, and why it‚Äôs so important for running large language models on consumer-grade GPUs.&lt;/p&gt; &lt;p&gt;Welcome to Day 18 of 21 Days of Building a Small Language Model. The topic for today is quantization, one of the most practical techniques for deploying large language models. Yesterday we explored Mixture of Experts and how it enables massive scale. Today, we'll discover how quantization makes models 4x to 8x smaller while preserving most of their performance, and why it's essential for real-world deployment&lt;/p&gt; &lt;h1&gt;Deployment Problem&lt;/h1&gt; &lt;p&gt;Before we dive into quantization, let's understand the problem it solves. Modern language models are enormous. A 7 billion parameter model stored in full precision (FP32) requires approximately 28 GB of memory just for the weights. A 70 billion parameter model? That's 280 GB. Before considering activations, KV cache, optimizer states, or any runtime memory, we're already talking about memory requirements that exceed what most systems can handle.&lt;/p&gt; &lt;p&gt;This creates a fundamental barrier to deployment. Even high-end consumer GPUs like the A100/H100 with 80+ GB of VRAM cannot load many state-of-the-art models in full precision. The compute requirements make inference prohibitively slow or expensive, especially for real-time applications. The energy consumption makes them impractical for battery-powered devices or environmentally conscious deployments.&lt;/p&gt; &lt;p&gt;This is where quantization becomes essential. Quantization is the process of reducing the precision of model weights and activations from high precision formats (like 32-bit or 16-bit floating point) to lower precision formats (like 8-bit integers or even 4-bit integers). By representing weights with fewer bits, we dramatically reduce memory requirements and can often accelerate inference on hardware optimized for integer operations.&lt;/p&gt; &lt;h1&gt;Memory Problem&lt;/h1&gt; &lt;p&gt;To appreciate why quantization is so impactful, we need to understand how weights are stored. In a transformer model, weights exist in every layer: in attention mechanisms (query, key, and value projection matrices), in feed-forward networks, in embedding layers, and in normalization layers. Each weight is a single floating point value that determines how strongly different parts of the input influence the output.&lt;/p&gt; &lt;p&gt;Let's break down the numbers for a typical 7 billion parameter model:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Per Attention Head:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Q matrix: 4096 √ó 4096 = 16,777,216 parameters&lt;/li&gt; &lt;li&gt;K matrix: 4096 √ó 4096 = 16,777,216 parameters&lt;/li&gt; &lt;li&gt;V matrix: 4096 √ó 4096 = 16,777,216 parameters&lt;/li&gt; &lt;li&gt;Output projection: 4096 √ó 4096 = 16,777,216 parameters&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Per head: 67,108,864 parameters&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Per Transformer Layer (32 attention heads):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Attention: 32 √ó 67,108,864 = 2,147,483,648 parameters&lt;/li&gt; &lt;li&gt;Feed-forward layers: ~90,000,000 parameters&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Per layer: ~2.2 billion parameters&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Total Model (32 layers):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Transformer layers: 32 √ó 2.2 billion = ~71 billion parameters&lt;/li&gt; &lt;li&gt;Embeddings and output head: ~100 million parameters&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Total: ~7 billion parameters&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Memory Requirements:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;FP32 storage: 7 billion √ó 4 bytes = &lt;strong&gt;28 GB&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;FP16 storage: 7 billion √ó 2 bytes = &lt;strong&gt;14 GB&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;INT8 storage: 7 billion √ó 1 byte = &lt;strong&gt;7 GB&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;INT4 storage: 7 billion √ó 0.5 bytes = &lt;strong&gt;3.5 GB&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This is just for storing weights. Additional memory is needed for activations during inference, KV cache for efficient generation, optimizer states during training, and intermediate computations. For a 70 billion parameter model, the 280 GB requirement is far beyond what most systems can handle.&lt;/p&gt; &lt;h1&gt;How Quantization Works&lt;/h1&gt; &lt;p&gt;Quantization is the process of mapping a large, continuous range of floating point values into a smaller set of discrete integer values. Think of it like dividing a continuous number line into &amp;quot;buckets&amp;quot; or &amp;quot;bins.&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Example: Quantizing weights from FP32 to 8-bit integers&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Let's say we have weights that range from -2.5 to +2.5:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Define the range&lt;/strong&gt;: Min = -2.5, Max = +2.5, Range = 5.0&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Create discrete buckets&lt;/strong&gt;: 8-bit gives us 256 possible integer values (0 to 255). We map the continuous range [-2.5, +2.5] to integers [0, 255].&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Calculate scale factor&lt;/strong&gt;: (255 - 0) / (2.5 - (-2.5)) = 255 / 5.0 = 51.0&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Quantize each weight&lt;/strong&gt;:&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Dequantize (convert back for computation)&lt;/strong&gt;:&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The key insight is that quantization trades precision for storage efficiency. Instead of storing each weight as a 32-bit float (4 bytes), we store it as an 8-bit integer (1 byte), reducing storage by 4x. The trade-off is that we can only represent 256 distinct values instead of billions, but for neural networks, this often works remarkably well because:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Neural networks are robust to small weight changes&lt;/li&gt; &lt;li&gt;The most important information is often preserved in the quantization buckets&lt;/li&gt; &lt;li&gt;Modern quantization techniques can minimize the information loss through careful calibration&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Does Quantization hurt model quality?&lt;/h1&gt; &lt;p&gt;This is the million-dollar question, and the answer is both yes and no. Quantization does introduce errors, but modern techniques minimize quality loss to the point where it's often negligible.&lt;/p&gt; &lt;h1&gt;Understanding Quantization Error&lt;/h1&gt; &lt;p&gt;Quantization error arises from two fundamental operations: rounding and clipping.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Rounding Error:&lt;/strong&gt; When we quantize a weight, we're mapping a continuous floating point value to the nearest discrete integer value. For example, if we have a weight value of &lt;code&gt;0.1234&lt;/code&gt; and our quantization scale maps it to integer &lt;code&gt;25.67&lt;/code&gt;, we round to &lt;code&gt;26&lt;/code&gt;. The difference between &lt;code&gt;25.67&lt;/code&gt; and &lt;code&gt;26&lt;/code&gt; is the rounding error.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Clipping Error:&lt;/strong&gt; Clipping occurs when a weight value falls outside the representable range. For 8-bit signed integers, the range is -128 to 127. If a weight would quantize to -150, it gets clipped to -128, losing information.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;These errors propagate through the network, but neural networks are remarkably robust to these changes, which is why quantization works so well in practice.&lt;/p&gt; &lt;h1&gt;Why some layers are more sensitive&lt;/h1&gt; &lt;p&gt;Not all layers are equally sensitive to quantization:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Attention Layers are more sensitive:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Attention weights determine how much the model focuses on each token. Small errors can shift attention from one token to another.&lt;/li&gt; &lt;li&gt;The softmax operation in attention is sensitive to small differences in scores.&lt;/li&gt; &lt;li&gt;Attention involves multiple matrix multiplications, so errors compound.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Feed-Forward Layers are less sensitive:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Many feed-forward layers use ReLU, which zeros out negative values, making them less sensitive to small errors in negative weights.&lt;/li&gt; &lt;li&gt;Feed-forward operations are more additive, so errors don't compound as dramatically.&lt;/li&gt; &lt;li&gt;Feed-forward layers often learn redundant features, so small weight changes don't drastically affect outputs.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Embedding and Output Layers:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;These are typically kept in full precision (FP16 or FP32) rather than quantized.&lt;/li&gt; &lt;li&gt;Embeddings encode semantic meaning, and small errors here directly affect the model's understanding.&lt;/li&gt; &lt;li&gt;The output layer produces logits that determine final predictions, and small errors can significantly change probabilities.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Keeping these layers in full precision typically adds only 1-2% to total model size while preserving critical model quality.&lt;/p&gt; &lt;h1&gt;Small vs Large Models&lt;/h1&gt; &lt;p&gt;Research and practical experience reveal interesting patterns:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Small Models (under 1B parameters):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Show slight but noticeable quality degradation when quantized&lt;/li&gt; &lt;li&gt;More sensitive to precision loss because each weight carries more information&lt;/li&gt; &lt;li&gt;Typical impact: 2-5% perplexity increase for 8-bit, 10-30% for 4-bit&lt;/li&gt; &lt;li&gt;Example: A 0.6B model might show perplexity increase from 5.12 to 5.35 (4.5% increase) with 8-bit quantization&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Large Models (7B+ parameters):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Show negligible quality loss from quantization&lt;/li&gt; &lt;li&gt;High redundancy means quantization errors are absorbed without significant impact&lt;/li&gt; &lt;li&gt;Typical impact: Less than 1% perplexity increase for 8-bit, 2-5% for 4-bit&lt;/li&gt; &lt;li&gt;Example: A 7B model might show perplexity increase from 3.45 to 3.47 (0.6% increase) with 8-bit quantization&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The larger the model, the less quality is lost. This is because large models are overparameterized, meaning they have more capacity than strictly necessary. This excess capacity provides robustness to quantization errors.&lt;/p&gt; &lt;h1&gt;When to use Quantization&lt;/h1&gt; &lt;p&gt;Quantization is one of the most practical techniques for deploying large language models. Here's when it makes sense:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Use Quantization when:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You need to reduce memory requirements (running larger models on limited hardware)&lt;/li&gt; &lt;li&gt;You want faster inference (integer operations are often faster than floating point)&lt;/li&gt; &lt;li&gt;You're deploying to edge devices or resource-constrained environments&lt;/li&gt; &lt;li&gt;You need to reduce infrastructure costs (smaller models = lower costs)&lt;/li&gt; &lt;li&gt;You want to enable local models (privacy, offline functionality)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Choose 8-bit:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Quality is critical and you can afford the memory&lt;/li&gt; &lt;li&gt;You want minimal quality loss (less than 1% on large models)&lt;/li&gt; &lt;li&gt;Production deployments where quality matters most&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Choose 4-bit:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Memory is the primary constraint&lt;/li&gt; &lt;li&gt;You can accept slight quality trade-offs (2-5% on large models)&lt;/li&gt; &lt;li&gt;Resource-constrained environments where maximum compression is needed&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Don't Quantize:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You have abundant memory and compute resources&lt;/li&gt; &lt;li&gt;Quality degradation is unacceptable for your use case&lt;/li&gt; &lt;li&gt;You're still in the research/development phase (quantize later for deployment)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;My Experience&lt;/h1&gt; &lt;p&gt;From working with quantized models in practice, here's what I've learned:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Good:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Memory savings are real and significant. I've been able to run 7B models on hardware that couldn't handle them in full precision.&lt;/li&gt; &lt;li&gt;Quality preservation is remarkable. For most use cases, the difference between full precision and 8-bit quantized is imperceptible.&lt;/li&gt; &lt;li&gt;Inference speed improvements are noticeable, especially on hardware optimized for integer operations.&lt;/li&gt; &lt;li&gt;The tooling (BitsAndBytes, GGUF) makes quantization straightforward to apply.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Challenges:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Small models show more quality degradation. If you're working with models under 1B parameters, expect more noticeable quality loss.&lt;/li&gt; &lt;li&gt;Some tasks are more sensitive. Mathematical reasoning, long context windows, and low-resource languages may show more degradation.&lt;/li&gt; &lt;li&gt;Calibration matters. Using representative calibration data improves results significantly.&lt;/li&gt; &lt;li&gt;Not all layers should be quantized. Keeping embeddings and output layers in full precision is standard practice and worth the small memory cost.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Surprising:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;How well it works. I was skeptical at first, but the results speak for themselves. Modern quantization techniques are genuinely impressive.&lt;/li&gt; &lt;li&gt;How large models quantize better. The larger the model, the less quality is lost. This makes quantization especially valuable for the largest models.&lt;/li&gt; &lt;li&gt;How practical it is. The tooling has matured to the point where quantization is now a standard part of the deployment pipeline.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Summary&lt;/h1&gt; &lt;p&gt;Today we explored quantization, one of the most practical techniques for deploying large language models. We learned how reducing precision from 32-bit floating point to 8-bit or 4-bit integers can achieve dramatic memory savings (4x to 8x compression) while preserving most model performance.&lt;/p&gt; &lt;p&gt;Understanding quantization is essential for anyone deploying language models in production. It's the technique that makes running large models on consumer hardware possible, enables edge deployment, and reduces infrastructure costs. Without quantization, many of the most exciting applications of LLMs would simply be impossible.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prashant-Lakhera"&gt; /u/Prashant-Lakhera &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvw9jo/day_18_21_days_of_building_a_small_language_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvw9jo/day_18_21_days_of_building_a_small_language_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pvw9jo/day_18_21_days_of_building_a_small_language_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T05:12:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1pw1dt0</id>
    <title>Training/tuning on textbook data</title>
    <updated>2025-12-26T10:37:50+00:00</updated>
    <author>
      <name>/u/Infamous_Patience129</name>
      <uri>https://old.reddit.com/user/Infamous_Patience129</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Excuse my ignorance, I'm very new to the topic. &lt;/p&gt; &lt;p&gt;I want to train up / tune one of these new open source models, to help troubleshoot diesel / heavy duty diagnostics.&lt;/p&gt; &lt;p&gt;I've played around with chatgpt, claude, deepseek and with good prompting i can get some pretty good answers, (looking for like spoonfed next steps on diagnosing issues), but the ai is often missing specific information and makes alot of guesses, or maybe doesnt seem to know how a system works or what the best next step should be.&lt;/p&gt; &lt;p&gt;Wondering how i could improve a self hosted/cloud hosted model, I have some pdf textbooks and also some service/troubleshooting manuals; these textbooks &amp;quot;fundamentals of diesel engines&amp;quot; etc are very comprehenive and if you knew them cover to cover, you would pretty much always know the next troubleshooting step; I am wondering what impact training or tuning using these as data would have.&lt;/p&gt; &lt;p&gt;Also wondering what the best current model might be suggested?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Infamous_Patience129"&gt; /u/Infamous_Patience129 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pw1dt0/trainingtuning_on_textbook_data/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pw1dt0/trainingtuning_on_textbook_data/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pw1dt0/trainingtuning_on_textbook_data/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T10:37:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1pw1k0q</id>
    <title>sSanityLayer Demo with screenshots</title>
    <updated>2025-12-26T10:48:48+00:00</updated>
    <author>
      <name>/u/ValuableLucky8566</name>
      <uri>https://old.reddit.com/user/ValuableLucky8566</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is a follow up to my previous post on sSanityLayer. Check&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1put96m/a_sanity_layer_that_can_make_slms_useful/"&gt;A sanity layer that can make SLMs useful (sSanityLayer) : r/LocalLLaMA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;These are demo screenshots for the two architectures.&lt;/p&gt; &lt;p&gt;Screenshot 1-3: The vector bias and vector intrusion logic with the GPT2 model.&lt;/p&gt; &lt;p&gt;Screenshot 4-6: The 77KB potato model.&lt;/p&gt; &lt;p&gt;Why look for vector intrusion below 60%? Because the boost variable approximately turns to a factor of 2 for altering logits.&lt;/p&gt; &lt;p&gt;Try it yourself:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/kavyamali/sSanityLayer"&gt;https://github.com/kavyamali/sSanityLayer&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ValuableLucky8566"&gt; /u/ValuableLucky8566 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pw1k0q"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pw1k0q/ssanitylayer_demo_with_screenshots/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pw1k0q/ssanitylayer_demo_with_screenshots/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T10:48:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1pvta3k</id>
    <title>An unnoficial and easy implementation of Nested Learning paradigm(Ali Behrouz et al, and other Google Researchers)</title>
    <updated>2025-12-26T02:31:37+00:00</updated>
    <author>
      <name>/u/Big-Welcome-3169</name>
      <uri>https://old.reddit.com/user/Big-Welcome-3169</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i know this isn't a Local LLM Topic, but i need help with scaling it to a bigger model and train on a bigger dataset and language modeling, here is the link: &lt;a href="https://github.com/WindOfNature/Nested-Learning"&gt;https://github.com/WindOfNature/Nested-Learning&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The proof of concept there is just on scikit learn(digit) and the accuracy is bad, i think this is because of the CMS bottlenecking the vision(because CMS mutating i think?), or because no CNN and small dim(128) and small max samples(200) So i need help with trying to scale it to larger model and task such as: * Language Modeling(Generative/Autoregressive Chatbots,etc) * Larger Vision task(ImageNet)&lt;/p&gt; &lt;p&gt;and etc, Hope you guys enjoyed it(if anyone reading this), Feel free to Issues and PR to help improve this framework.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Big-Welcome-3169"&gt; /u/Big-Welcome-3169 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvta3k/an_unnoficial_and_easy_implementation_of_nested/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvta3k/an_unnoficial_and_easy_implementation_of_nested/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pvta3k/an_unnoficial_and_easy_implementation_of_nested/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T02:31:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1pw0mtz</id>
    <title>GLM 4.7 for Agentic</title>
    <updated>2025-12-26T09:49:32+00:00</updated>
    <author>
      <name>/u/SlowFail2433</name>
      <uri>https://old.reddit.com/user/SlowFail2433</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GLM 4.7 is the new hot potato&lt;/p&gt; &lt;p&gt;Has anyone tested it for agentic use yet? Even just tool calling and MCP use?&lt;/p&gt; &lt;p&gt;I noticed it beat Deepseek 3.2 and Kimi K2 Thinking on the agentic benches&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SlowFail2433"&gt; /u/SlowFail2433 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pw0mtz/glm_47_for_agentic/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pw0mtz/glm_47_for_agentic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pw0mtz/glm_47_for_agentic/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T09:49:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1pvpifv</id>
    <title>Steering LLM Behavior Without Fine-Tuning</title>
    <updated>2025-12-25T23:18:18+00:00</updated>
    <author>
      <name>/u/Bakkario</name>
      <uri>https://old.reddit.com/user/Bakkario</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvpifv/steering_llm_behavior_without_finetuning/"&gt; &lt;img alt="Steering LLM Behavior Without Fine-Tuning" src="https://external-preview.redd.it/Du7zeDzqswWAolCphXzRi_zj33jOeXB6IU0TL-7DQwc.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ba4471d708a8d7ead91e841688311fb6555ded1b" title="Steering LLM Behavior Without Fine-Tuning" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This video from HuggingFave is a masterpiece!! I thought it should not go unnoticed - despite the good views it has - and share it with you guys. &lt;/p&gt; &lt;p&gt;It shows how you can modify the behavior or the personality of a model at inference time, without fine-tuning or prompt engineering. It‚Äôs inspired by the Golden Gate experiment done by Anthropic. Anthropic‚Äôs researchers changed the behavior of the large language model Claude Sonnet, making it answer as if it were the Golden Gate, no fine tuning whatsoever üòÖ&lt;/p&gt; &lt;p&gt;Enjoy!! And thank you HF and Sabid who made the video üôèüèæ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bakkario"&gt; /u/Bakkario &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://m.youtube.com/watch?v=F2jd5WuT-zg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvpifv/steering_llm_behavior_without_finetuning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pvpifv/steering_llm_behavior_without_finetuning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-25T23:18:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1pvke55</id>
    <title>llama.cpp's recent updates - --fit flag</title>
    <updated>2025-12-25T19:09:25+00:00</updated>
    <author>
      <name>/u/pmttyji</name>
      <uri>https://old.reddit.com/user/pmttyji</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Haven't updated llama.cpp for last 2 weeks. Liked the new CLI after last time update.&lt;/p&gt; &lt;p&gt;Wanted to mention these PRs.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/16653"&gt;llama: automatically set parameters not set by the user in such a way that maximizes GPU utilization #16653&lt;/a&gt; - I was waiting for this one. Looks like this one got merged already &amp;amp; also few more related PRs too done with fixes. How many of you used &lt;code&gt;--fit&lt;/code&gt; flag on your llama.cpp commands? Please share your stats on this(Would be nice to see before &amp;amp; after results).&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/18343"&gt;ggml : optimize cuda cumsum fallback (~2.5x speedup vs CUB) #18343&lt;/a&gt; - This one is from latest update. (As a non-techie) I have no idea what this is &amp;amp; how it works. But the number in title ~2.5x looks nice. PR don't have t/s results with before &amp;amp; after. Somebody please share details on this. I have 4060 Laptop GPU(8GB VRAM).&lt;/p&gt; &lt;p&gt;EDIT:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1pn2e1c/llamacpp_automation_for_gpu_layers_tensor_split/"&gt;Previous thread&lt;/a&gt; from this sub on 1st PR topic. Sorry I had very less context/memory on this one.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmttyji"&gt; /u/pmttyji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvke55/llamacpps_recent_updates_fit_flag/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvke55/llamacpps_recent_updates_fit_flag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pvke55/llamacpps_recent_updates_fit_flag/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-25T19:09:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1pvr0w0</id>
    <title>I tested GLM 4.7 and minimax-m2.1 and compared it to CC and Codex</title>
    <updated>2025-12-26T00:34:22+00:00</updated>
    <author>
      <name>/u/jstanaway</name>
      <uri>https://old.reddit.com/user/jstanaway</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TL;DR&lt;/p&gt; &lt;p&gt;Claude=best, mimimax-m2.1=excellent (surprised), Codex 5.2-med=very good, GLM-4.7=bad&lt;/p&gt; &lt;p&gt;Ok, so I tested codex5.2-med today and minimax-m2.1 today. I ran these same tests on GLM 4.7 and Claude code (sonnet 4.5 and Haiku 4.5) yesterday. &lt;/p&gt; &lt;p&gt;Lets me add some background to my job I had for it. I tested it on a Vue JS frontend project. I have a parent component with 28 child components which contain different fields in each one. The job was to create one generic component that can be used in place of all 28 components. Heres what needed to happen for this to work out. &lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Extract the required fields from an existing JSON object I supplied to the model. It needed to extract a specific property and put it into another existing JSON object that stores some hardcoded frontend configuration. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Extract some custom text from all 28 of the files for another property that will be added to the existing JSON object in #1. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Pass numerous props into the new generic component including all the fields that will be displayed. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Create the generic component that will display the fields that are passed in. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Updated the type related to this data in types file. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Remove the unneeded 28 files. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Make sure the parent component can still submit successfully without modifying any of the existing logic. &lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Heres the results in the order that they performed from best to worst. Claude was in Claude code, Codex in the Codex CLI. Minimax and GLM-4.7 were in Opencode. &lt;/p&gt; &lt;ol&gt; &lt;li&gt;Claude (Sonnet 4.5 planning, Haiku 4.5 implementation). &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;No surprise here, Claude is a beast. Felt like it had the best most comprehensive plan to implement this. Thought of things I left out of the prompt like also extracting and creating a property for footer text that was different in each of the child components. Planned in Sonnet 4.5 and executed in Haiku 4.5. Worked perfectly on first try. Gave a really nice summary at the end outlining how many lines we eliminated etc. &lt;/p&gt; &lt;ol&gt; &lt;li&gt;minimax-m2.1&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Kind of a surprise here. I did NOT expect this model to do this on the first try, especially because I had tested GLM-4.7 first and was let down. Plan had to be refined upon presentation, nothing major. Once I gave it the go ahead it took ~8mins. Worked on first try, no issues. Overall I was impressed. ~50% of context used, total cost $0.13&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Codex 5.2 medium&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Codex asked more refinement questions about the implementation than all the others. Guess this could be good or bad depending on how you look at it. It worked on the first try but changing the value of the dropdown which selects the content for the child component did not work properly after the initial selection. I had to prompt it and it fixed it on the second try in a couple seconds. Overall, pretty much on the first try but I figured it would be cheating if I didn't give credit to the models who actually DID get it on the first try 100%. Total time of implementation once plan approved was like ~10mins. &lt;/p&gt; &lt;ol&gt; &lt;li&gt;GLM-4.7&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Not impressed at all. Did not successfully complete. It messed up my submission code while it got the child component functionality right. I must have prompted it maybe an additional 6-7 times and it never did get it working. It really seemed to get wrapped up in it's own thinking. Based on my experience at least with my small test job I would not use it.&lt;/p&gt; &lt;p&gt;Conclusion&lt;/p&gt; &lt;p&gt;Claude was the best, no surprise there I think. But, for a budget model like minimax I was really surprised. Did it faster than Codex and on the first try. I have ChatGPT Plus and Claude Pro so i probably won't sub to minimax but if I needed a budget model I would definitely start using it, overall impressive. Especially if you consider it should be open source. &lt;/p&gt; &lt;p&gt;I primarily use Haiku 4.5 on my Claude plan, I find it's enough for 80% of my stuff. Ive used sonnet the rest and Opus 4.5 twice since it was released. So, I get quite a bit of usage out of my CC Pro plan. I won't leave ChatGPT, I use it for everything else so Codex is a give in and an excellent option as well. I will add that I do really like the UI of Opencode. I wish CC would adopt the way the thinking is displayed in Opencode. They've improved the way the diffs are highlighted but I feel like they can still improve it more. Anyway, I hope you guys enjoy the read!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jstanaway"&gt; /u/jstanaway &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvr0w0/i_tested_glm_47_and_minimaxm21_and_compared_it_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvr0w0/i_tested_glm_47_and_minimaxm21_and_compared_it_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pvr0w0/i_tested_glm_47_and_minimaxm21_and_compared_it_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T00:34:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1pvonzn</id>
    <title>Admins, can we create GPU memory tiers</title>
    <updated>2025-12-25T22:35:25+00:00</updated>
    <author>
      <name>/u/ScoreUnique</name>
      <uri>https://old.reddit.com/user/ScoreUnique</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As the title says, it happens often that there's people with RTX 6000 PRO commenting on RTX 3050 and the other way around without sometimes realizing what tier performance is expected, can we create a new set of tags that mark different GPU tiers based on VRAM &amp;amp; RAM richness (I suppose most of us use unified memory) &lt;/p&gt; &lt;p&gt;Looking for ideas on how to better organise the sub. Thanks in advance. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ScoreUnique"&gt; /u/ScoreUnique &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvonzn/admins_can_we_create_gpu_memory_tiers/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvonzn/admins_can_we_create_gpu_memory_tiers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pvonzn/admins_can_we_create_gpu_memory_tiers/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-25T22:35:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1pvgell</id>
    <title>Train a 4B model to beat Claude Sonnet 4.5 and Gemini Pro 2.5 at tool calling - for free (Colab included)</title>
    <updated>2025-12-25T16:05:14+00:00</updated>
    <author>
      <name>/u/DecodeBytes</name>
      <uri>https://old.reddit.com/user/DecodeBytes</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvgell/train_a_4b_model_to_beat_claude_sonnet_45_and/"&gt; &lt;img alt="Train a 4B model to beat Claude Sonnet 4.5 and Gemini Pro 2.5 at tool calling - for free (Colab included)" src="https://b.thumbs.redditmedia.com/Af4I55UhMYXW5gk6wlYIuAjMVZjoUq-PUY2tOWlSN3E.jpg" title="Train a 4B model to beat Claude Sonnet 4.5 and Gemini Pro 2.5 at tool calling - for free (Colab included)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Using Open Source DeepFabric, a tool that lets you:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Pick any MCP server or any given set of Tools&lt;/li&gt; &lt;li&gt;A specific root topic (DevOps, Customer Care, Coding Agent)&lt;/li&gt; &lt;li&gt;Auto-generate a tool calling / reasoning topic specific dataset, with real tool traces executed within isolated webassembly components.&lt;/li&gt; &lt;li&gt;Fine-tune an SLM to become an expert at that specific MCP server using Unsloth's awesome training framework&lt;/li&gt; &lt;li&gt;Evaluate against a training-blind subset of the dataset.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;We trained Qwen3-4B to outperform Claude Sonnet 4.5 and Gemini Pro 2.5 against the more challenging to use Blender MCP server.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Score&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;DeepFabric Fine Tuned&lt;/td&gt; &lt;td align="left"&gt;93.50%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Claude Sonnet 4.5&lt;/td&gt; &lt;td align="left"&gt;80.50%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Google Gemini Pro 2.5&lt;/td&gt; &lt;td align="left"&gt;47.00%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;The idea is simple:&lt;/strong&gt; frontier models are generalists, but a small model fine-tuned on domain-specific tool calling data can become a specialist that beats them at that specific task.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/x6svlmqird9g1.png?width=2816&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e44c8203ce3d7383951397b5ae5b33870ceab7e0"&gt;https://preview.redd.it/x6svlmqird9g1.png?width=2816&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e44c8203ce3d7383951397b5ae5b33870ceab7e0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Try it yourself on Google Colab using a Free T4:&lt;/strong&gt; &lt;a href="https://colab.research.google.com/drive/1EG1V40v5xkJKLf6Ra6W4378vYqlZNVWq"&gt;https://colab.research.google.com/drive/1EG1V40v5xkJKLf6Ra6W4378vYqlZNVWq&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/always-further/deepfabric"&gt;https://github.com/always-further/deepfabric&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love feedback from the community, especially if you decide to generate your own agent.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DecodeBytes"&gt; /u/DecodeBytes &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvgell/train_a_4b_model_to_beat_claude_sonnet_45_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvgell/train_a_4b_model_to_beat_claude_sonnet_45_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pvgell/train_a_4b_model_to_beat_claude_sonnet_45_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-25T16:05:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1pvx6a0</id>
    <title>TurboDiffusion ‚Äî 100‚Äì200√ó faster video diffusion on a single GPU</title>
    <updated>2025-12-26T06:04:06+00:00</updated>
    <author>
      <name>/u/freesysck</name>
      <uri>https://old.reddit.com/user/freesysck</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvx6a0/turbodiffusion_100200_faster_video_diffusion_on_a/"&gt; &lt;img alt="TurboDiffusion ‚Äî 100‚Äì200√ó faster video diffusion on a single GPU" src="https://preview.redd.it/b5vrplmioh9g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=82a5c94a6996fffe772bd0c3adc81b86a0ee00d3" title="TurboDiffusion ‚Äî 100‚Äì200√ó faster video diffusion on a single GPU" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Open framework that speeds up end-to-end video generation by 100‚Äì200√ó while keeping quality, shown on a single RTX 5090. Ôøº ‚Ä¢ How: low-bit SageAttention + trainable Sparse-Linear Attention, rCM step distillation, and W8A8 quantization. Ôøº ‚Ä¢ Repo: &lt;a href="https://github.com/thu-ml/TurboDiffusion"&gt;https://github.com/thu-ml/TurboDiffusion&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/freesysck"&gt; /u/freesysck &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/b5vrplmioh9g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvx6a0/turbodiffusion_100200_faster_video_diffusion_on_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pvx6a0/turbodiffusion_100200_faster_video_diffusion_on_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T06:04:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1pvxmqt</id>
    <title>Finally a Kimi-Linear-48B-A3B GGUF! [Experimental PR]</title>
    <updated>2025-12-26T06:32:16+00:00</updated>
    <author>
      <name>/u/KvAk_AKPlaysYT</name>
      <uri>https://old.reddit.com/user/KvAk_AKPlaysYT</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvxmqt/finally_a_kimilinear48ba3b_gguf_experimental_pr/"&gt; &lt;img alt="Finally a Kimi-Linear-48B-A3B GGUF! [Experimental PR]" src="https://b.thumbs.redditmedia.com/3L7s9Y4g4SXcNJbIUac3xq5NcNnmOFl2SLzQbPE94bI.jpg" title="Finally a Kimi-Linear-48B-A3B GGUF! [Experimental PR]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;Yes, it's finally happening! I recently pushed some changes and have gotten Kimi-Linear to work (fully; fingers crossed) PR (#18381). &lt;/p&gt; &lt;p&gt;I've tested it heavily on Q2_K (mind BLOWING coherence :), and it‚Äôs now passing logic puzzles, long-context essay generation, and basic math - all of which were previously broken.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/mjychgkcth9g1.png?width=555&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f02c3fda1ea59629b4aac6664cc7c4a071f7ebd1"&gt;q2_k&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Resources:&lt;/p&gt; &lt;p&gt;PR Branch: &lt;a href="http://github.com/ggml-org/llama.cpp/pull/18381"&gt;github.com/ggml-org/llama.cpp/pull/18381&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GGUFs (Use above PR): &lt;a href="https://huggingface.co/AaryanK/Kimi-Linear-48B-A3B-Instruct-GGUF"&gt;huggingface.co/AaryanK/Kimi-Linear-48B-A3B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Use this free Colab notebook or copy the code from it for a quick start :) &lt;a href="https://colab.research.google.com/drive/1NMHMmmht-jxyfZqJr5xMlOE3O2O4-WDq?usp=sharing"&gt;https://colab.research.google.com/drive/1NMHMmmht-jxyfZqJr5xMlOE3O2O4-WDq?usp=sharing&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Please give it a spin and let me know if you run into any divergent logits or loops!&lt;/p&gt; &lt;p&gt;I am currently looking for open positions! ü§ó&lt;/p&gt; &lt;p&gt;If you find this model useful or are looking for a talented AI/LLM Engineer, please reach out to me on LinkedIn: &lt;a href="https://www.linkedin.com/in/theaaryankapoor/"&gt;Aaryan Kapoor&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KvAk_AKPlaysYT"&gt; /u/KvAk_AKPlaysYT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvxmqt/finally_a_kimilinear48ba3b_gguf_experimental_pr/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvxmqt/finally_a_kimilinear48ba3b_gguf_experimental_pr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pvxmqt/finally_a_kimilinear48ba3b_gguf_experimental_pr/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T06:32:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1pvvv8m</id>
    <title>Kimi-Linear Support in progress (you can download gguf and run it)</title>
    <updated>2025-12-26T04:50:52+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvvv8m/kimilinear_support_in_progress_you_can_download/"&gt; &lt;img alt="Kimi-Linear Support in progress (you can download gguf and run it)" src="https://external-preview.redd.it/Ez8JR9W3z41Aa9GoVbfLGF_GmJCt-mt-65CyiCbmgv4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c85131734185b5feabb39b027b4f431dac21c4a1" title="Kimi-Linear Support in progress (you can download gguf and run it)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's not reviewed, so don't get too excited yet &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/18381"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvvv8m/kimilinear_support_in_progress_you_can_download/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pvvv8m/kimilinear_support_in_progress_you_can_download/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T04:50:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1pvr64e</id>
    <title>A Christmas Miracle: Managed to grab 3x RTX 5090 FE at MSRP for my home inference cluster.</title>
    <updated>2025-12-26T00:41:51+00:00</updated>
    <author>
      <name>/u/Sudden_Rip7717</name>
      <uri>https://old.reddit.com/user/Sudden_Rip7717</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvr64e/a_christmas_miracle_managed_to_grab_3x_rtx_5090/"&gt; &lt;img alt="A Christmas Miracle: Managed to grab 3x RTX 5090 FE at MSRP for my home inference cluster." src="https://preview.redd.it/go1uf72v2g9g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a49f1c4f3df4fa34d397b34c3fcf1212cd609c5d" title="A Christmas Miracle: Managed to grab 3x RTX 5090 FE at MSRP for my home inference cluster." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;It has been a challenging year, but it has brought its own blessings too. I am truly grateful to God for so much more than just hardware, but I am also specifically thankful for this opportunity to upgrade my local AI research lab.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;I just want to wish everyone here a Merry Christmas! Don't give up on your dreams, be ready to work hard, look boldly into the future, and try to enjoy every single day you live.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Merry Christmas and God bless!&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sudden_Rip7717"&gt; /u/Sudden_Rip7717 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/go1uf72v2g9g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvr64e/a_christmas_miracle_managed_to_grab_3x_rtx_5090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pvr64e/a_christmas_miracle_managed_to_grab_3x_rtx_5090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T00:41:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1pvs8l3</id>
    <title>ASUS Rumored To Enter DRAM Market Next Year</title>
    <updated>2025-12-26T01:36:47+00:00</updated>
    <author>
      <name>/u/Highwaytothebeach</name>
      <uri>https://old.reddit.com/user/Highwaytothebeach</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Well instead of learning about AI and having a pretty small chince finding a real job with that knoweledge actually seems that right now and in near future the most proffitable is investing in AI and tech stocks. And some people make money when stocks go sharp down.&lt;/p&gt; &lt;p&gt;Because of PC CPUs are locked at max 256 RAM support for too long and also DDR market looks weird lacking higher capacity widelly affordable modules in AI times, I was thinking tons of motherboards , barebones, PSUs and alot of other hardware is just going to hit recycling facilities, despite being reasonably priced.. And found this &lt;a href="https://wccftech.com/asus-enter-dram-market-next-year-to-tackle-memory-shortages-rumor"&gt;https://wccftech.com/asus-enter-dram-market-next-year-to-tackle-memory-shortages-rumor&lt;/a&gt; Any chance it may be true?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Highwaytothebeach"&gt; /u/Highwaytothebeach &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvs8l3/asus_rumored_to_enter_dram_market_next_year/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvs8l3/asus_rumored_to_enter_dram_market_next_year/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pvs8l3/asus_rumored_to_enter_dram_market_next_year/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T01:36:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1pvjpmb</id>
    <title>Why I quit using Ollama</title>
    <updated>2025-12-25T18:38:36+00:00</updated>
    <author>
      <name>/u/SoLoFaRaDi</name>
      <uri>https://old.reddit.com/user/SoLoFaRaDi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For about a year, I've used Ollama like... 24/7. It was always my go-to, as it was frequently updated and had support for every model I needed.&lt;/p&gt; &lt;p&gt;Over the past few months, there's been a serious decline in the updates &amp;amp; update content that releases with Ollama. I understand that, and just went about my day, as the maintainers obviously have a life. Cool! Then the **Cloud** update dropped. I saw Ollama as a great model runner, you just download a model and boom. Nope! They decided to combine proprietary models with the models uploaded on their Library. At first, it seemed cool. We can now run AI models that were otherwise impossible to run on consumer hardware, but then I started getting confused. Why did they add in Cloud, what's the point? What were the privacy implications? It just felt like they were adding more and more bloatware into their already massive binaries, so about a month ago, I made the decision, and quit Ollama for good.&lt;/p&gt; &lt;p&gt;I feel like with every update they are seriously straying away from the main purpose of their application; to provide a secure inference platform for LOCAL AI models. I understand they're simply trying to fund their platform with the Cloud option, but it feels like a terrible move from the Ollama maintainers. &lt;/p&gt; &lt;p&gt;What do you guys think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SoLoFaRaDi"&gt; /u/SoLoFaRaDi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvjpmb/why_i_quit_using_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvjpmb/why_i_quit_using_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pvjpmb/why_i_quit_using_ollama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-25T18:38:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1pvwlfh</id>
    <title>systemctl disable ollama</title>
    <updated>2025-12-26T05:30:55+00:00</updated>
    <author>
      <name>/u/copenhagen_bram</name>
      <uri>https://old.reddit.com/user/copenhagen_bram</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvwlfh/systemctl_disable_ollama/"&gt; &lt;img alt="systemctl disable ollama" src="https://preview.redd.it/8qvw6jdjih9g1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=67cb047d78cc712448a65395f1aff5b8269410ca" title="systemctl disable ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;151GB timeshift snapshot composed of mainly Flatpak repo data (Alpaca?) and /usr/share/ollama&lt;/p&gt; &lt;p&gt;From now on I'm storing models in my home directory&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/copenhagen_bram"&gt; /u/copenhagen_bram &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8qvw6jdjih9g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvwlfh/systemctl_disable_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pvwlfh/systemctl_disable_ollama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T05:30:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1pvxq2t</id>
    <title>Hard lesson learned after a year of running large models locally</title>
    <updated>2025-12-26T06:38:00+00:00</updated>
    <author>
      <name>/u/inboundmage</name>
      <uri>https://old.reddit.com/user/inboundmage</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, go easy with me I'm new at running large models.&lt;/p&gt; &lt;p&gt;After spending about 12 months tinkering with locally hosted LLMs, I thought I had my setup dialed in. I‚Äôm running everything off a workstation with a single RTX 3090, Ubuntu 22.04, llama.cpp for smaller models and vLLM for anything above 30 B parameters. &lt;/p&gt; &lt;p&gt;My goal has always been to avoid cloud dependencies and keep as much computation offline as possible, so I‚Äôve tried every quantization trick and caching tweak I could find.&lt;/p&gt; &lt;p&gt;The biggest friction point has been scaling beyond 13 B models. &lt;/p&gt; &lt;p&gt;Even with 24 GB of VRAM, running a 70 B model in int4 still exhausts memory when the context window grows and attention weights balloon. &lt;/p&gt; &lt;p&gt;Offloading to system RAM works, but inference latency spikes into seconds, and batching requests becomes impossible. &lt;/p&gt; &lt;p&gt;I‚Äôve also noticed that GPU VRAM fragmentation accumulates over time when swapping between models, after a few hours, vLLM refuses to load a model that would normally fit because of leftover allocations.&lt;/p&gt; &lt;p&gt;My takeaway so far is that local first inference is viable for small to medium models, but there‚Äôs a hard ceiling unless you invest in server grade hardware or cluster multiple GPUs. &lt;/p&gt; &lt;p&gt;Quantization helps, but you trade some quality and run into new bugs. &lt;/p&gt; &lt;p&gt;For privacy sensitive tasks, the trade‚Äëoff is worth it; for fast iteration, it‚Äôs been painful compared to cloud based runners. &lt;/p&gt; &lt;p&gt;I‚Äôm curious if anyone has found a reliable way to manage VRAM fragmentation or offload attention blocks more efficiently on consumer cards, or whether the answer is simply ‚Äúbuy more VRAM.‚Äù &lt;/p&gt; &lt;p&gt;How are others solving this without compromising on running fully offline?&lt;/p&gt; &lt;p&gt;Thx&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/inboundmage"&gt; /u/inboundmage &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvxq2t/hard_lesson_learned_after_a_year_of_running_large/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvxq2t/hard_lesson_learned_after_a_year_of_running_large/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pvxq2t/hard_lesson_learned_after_a_year_of_running_large/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T06:38:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1pvz7bf</id>
    <title>MiniMax-M2.1 uploaded on HF</title>
    <updated>2025-12-26T08:12:23+00:00</updated>
    <author>
      <name>/u/ciprianveg</name>
      <uri>https://old.reddit.com/user/ciprianveg</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/MiniMaxAI/MiniMax-M2.1/tree/main"&gt;https://huggingface.co/MiniMaxAI/MiniMax-M2.1/tree/main&lt;/a&gt; Hurray!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ciprianveg"&gt; /u/ciprianveg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvz7bf/minimaxm21_uploaded_on_hf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvz7bf/minimaxm21_uploaded_on_hf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pvz7bf/minimaxm21_uploaded_on_hf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T08:12:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1pvz7v2</id>
    <title>Minimax M2.1 released</title>
    <updated>2025-12-26T08:13:29+00:00</updated>
    <author>
      <name>/u/__Maximum__</name>
      <uri>https://old.reddit.com/user/__Maximum__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Link to xcancel: &lt;a href="https://xcancel.com/ModelScope2022/status/2004462984698253701#m"&gt;https://xcancel.com/ModelScope2022/status/2004462984698253701#m&lt;/a&gt;&lt;/p&gt; &lt;p&gt;New on ModelScope: MiniMax M2.1 is open-source!&lt;/p&gt; &lt;p&gt;‚úÖ SOTA in 8+ languages (Rust, Go, Java, C++, TS, Kotlin, Obj-C, JS) ‚úÖ Full-stack Web &amp;amp; mobile dev: Android/iOS, 3D visuals, vibe coding that actually ships ‚úÖ Smarter, faster, 30% fewer tokens ‚Äî with lightning mode (M2.1-lightning) for high-TPS workflows ‚úÖ Top-tier on SWE-bench, VIBE, and custom coding/review benchmarks ‚úÖ Works flawlessly in Cursor, Cline, Droid, BlackBox, and more&lt;/p&gt; &lt;p&gt;It‚Äôs not just ‚Äúbetter code‚Äù ‚Äî it‚Äôs AI-native development, end to end.&lt;/p&gt; &lt;p&gt;&lt;a href="https://modelscope.cn/models/MiniMax/MiniMax-M2.1/summary"&gt;https://modelscope.cn/models/MiniMax/MiniMax-M2.1/summary&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/__Maximum__"&gt; /u/__Maximum__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvz7v2/minimax_m21_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvz7v2/minimax_m21_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pvz7v2/minimax_m21_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T08:13:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1pvpkqo</id>
    <title>I wish this GPU VRAM upgrade modification became mainstream and ubiquitous to shred monopoly abuse of NVIDIA</title>
    <updated>2025-12-25T23:21:39+00:00</updated>
    <author>
      <name>/u/CeFurkan</name>
      <uri>https://old.reddit.com/user/CeFurkan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvpkqo/i_wish_this_gpu_vram_upgrade_modification_became/"&gt; &lt;img alt="I wish this GPU VRAM upgrade modification became mainstream and ubiquitous to shred monopoly abuse of NVIDIA" src="https://external-preview.redd.it/eHAyeXBnM2xvZjlnMcbYDDf5MmPAc5-kZmkvzc1kUbOViw5SF6SuJ_dOojri.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9b4e5d7a038251df406b6345161c5136f2011960" title="I wish this GPU VRAM upgrade modification became mainstream and ubiquitous to shred monopoly abuse of NVIDIA" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CeFurkan"&gt; /u/CeFurkan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/u1mxlc3lof9g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvpkqo/i_wish_this_gpu_vram_upgrade_modification_became/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pvpkqo/i_wish_this_gpu_vram_upgrade_modification_became/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-25T23:21:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1pt50mt</id>
    <title>AMA Announcement: Z.ai, The Opensource Lab Behind GLM-4.7 (Tuesday, 8AM-11AM PST)</title>
    <updated>2025-12-22T17:12:21+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pt50mt/ama_announcement_zai_the_opensource_lab_behind/"&gt; &lt;img alt="AMA Announcement: Z.ai, The Opensource Lab Behind GLM-4.7 (Tuesday, 8AM-11AM PST)" src="https://preview.redd.it/r06ch4zyfs8g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e0a10789f393f350618520fcb81174f3a3dae1c7" title="AMA Announcement: Z.ai, The Opensource Lab Behind GLM-4.7 (Tuesday, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/r06ch4zyfs8g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pt50mt/ama_announcement_zai_the_opensource_lab_behind/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pt50mt/ama_announcement_zai_the_opensource_lab_behind/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T17:12:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptxm3x</id>
    <title>AMA With Z.AI, The Lab Behind GLM-4.7</title>
    <updated>2025-12-23T16:04:23+00:00</updated>
    <author>
      <name>/u/zixuanlimit</name>
      <uri>https://old.reddit.com/user/zixuanlimit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi r/LocalLLaMA&lt;/p&gt; &lt;p&gt;Today we are having &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt;, the research lab behind the GLM 4.7. We‚Äôre excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Yuxuan Zhang, &lt;a href="/u/YuxuanZhangzR"&gt;u/YuxuanZhangzR&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Qinkai Zheng, &lt;a href="/u/QinkaiZheng"&gt;u/QinkaiZheng&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Aohan Zeng, &lt;a href="/u/Sengxian"&gt;u/Sengxian&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Zhenyu Hou, &lt;a href="/u/ZhenyuHou"&gt;u/ZhenyuHou&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Xin Lv, &lt;a href="/u/davidlvxin"&gt;u/davidlvxin&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The AMA will run from 8 AM ‚Äì 11 AM PST, with the &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt; team continuing to follow up on questions over the next 48 hours.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zixuanlimit"&gt; /u/zixuanlimit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T16:04:23+00:00</published>
  </entry>
</feed>
