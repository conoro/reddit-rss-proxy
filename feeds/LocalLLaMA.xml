<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-29T13:28:01+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1p97uuv</id>
    <title>Your local models can now make phone calls! Launching Phone Integration ðŸ“ž in Observer</title>
    <updated>2025-11-28T22:18:25+00:00</updated>
    <author>
      <name>/u/Roy3838</name>
      <uri>https://old.reddit.com/user/Roy3838</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p97uuv/your_local_models_can_now_make_phone_calls/"&gt; &lt;img alt="Your local models can now make phone calls! Launching Phone Integration ðŸ“ž in Observer" src="https://external-preview.redd.it/nOalRlFoqYIIAoHotT2ulKa0W-tArBvHe2Mb6BSa9vU.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3394b4f1ae7c7e8c7e6556e7ffe2af8db0354268" title="Your local models can now make phone calls! Launching Phone Integration ðŸ“ž in Observer" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TL;DR: Observer is an &lt;strong&gt;open-source, free, and local&lt;/strong&gt; framework that gives your local models actual powers, like watching your screen/camera/mic, logging to memory, and now &lt;strong&gt;making real phone calls!!&lt;/strong&gt; I'm Roy, the solo dev building this, and I would really appreciate your feedback to keep making Observer better :)&lt;/p&gt; &lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;Thanks for all the support! seriously, this community has always been incredible. Observer has gone super far due to your support and feedback!!&lt;/p&gt; &lt;p&gt;I'm back with something I think is pretty cool: your local models can now &lt;strong&gt;make actual phone calls.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Quick Setup:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Whitelist your number by messaging/calling Observer (to prevent abuse)&lt;/li&gt; &lt;li&gt;Observer watches your screen/camera via WebRTC&lt;/li&gt; &lt;li&gt;Your local model (Ollama/llama.cpp) processes what it sees&lt;/li&gt; &lt;li&gt;New call() function triggers a real phone call when your conditions are met&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Random use cases I've used it for:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;That 2-hour render finally finishes â†’ get a call&lt;/li&gt; &lt;li&gt;Your AFK Minecraft character is about to die â†’ phone rings&lt;/li&gt; &lt;li&gt;Security camera detects motion â†’ instant call with a description of what it sees.&lt;/li&gt; &lt;li&gt;Your crypto bot sees something â†’ wake up with specific data of what happened.&lt;/li&gt; &lt;li&gt;Literally anything you can see on screen â†’ phone call with text2speech&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What is Observer AI?&lt;/p&gt; &lt;p&gt;It's a framework I built for this community. Think of it like a super simple MCP server that runs in your browser:&lt;/p&gt; &lt;p&gt;- Sensors (Screen/Camera/Mic) â†’ Local Models (Ollama/llama.cpp) â†’ Tools (notifications, recordings, memory, code, and &lt;strong&gt;now phone calls&lt;/strong&gt;)&lt;/p&gt; &lt;p&gt;The whole thing is free (with some convenient paid tiers to make it sustainable), open-source (MIT license), and runs entirely on your machine. You can try it in your browser with zero setup, or go full local with the desktop app.&lt;/p&gt; &lt;p&gt;Links:&lt;/p&gt; &lt;p&gt;- GitHub (all the code, open source): &lt;a href="https://github.com/Roy3838/Observer"&gt;https://github.com/Roy3838/Observer&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- Try it without any install: &lt;a href="https://app.observer-ai.com/"&gt;https://app.observer-ai.com/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- Discord: &lt;a href="https://discord.gg/wnBb7ZQDUC"&gt;https://discord.gg/wnBb7ZQDUC&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm here to answer questions. What would YOU use this for?&lt;/p&gt; &lt;p&gt;Cheers,&lt;/p&gt; &lt;p&gt;Roy&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Roy3838"&gt; /u/Roy3838 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/shorts/yNu2K6LaTNk"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p97uuv/your_local_models_can_now_make_phone_calls/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p97uuv/your_local_models_can_now_make_phone_calls/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-28T22:18:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1p90zzi</id>
    <title>CPU-only LLM performance - t/s with llama.cpp</title>
    <updated>2025-11-28T17:40:34+00:00</updated>
    <author>
      <name>/u/pmttyji</name>
      <uri>https://old.reddit.com/user/pmttyji</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How many of you do use CPU only inference time to time(at least rarely)? .... Really missing CPU-Only Performance threads here in this sub.&lt;/p&gt; &lt;p&gt;&lt;sup&gt;Possibly few of you waiting to grab one or few 96GB GPUs at cheap price later so using CPU only inference for now just with bulk RAM.&lt;/sup&gt;&lt;/p&gt; &lt;p&gt;I think bulk RAM(128GB-1TB) is more than enough to run small/medium models since it comes with more memory bandwidth.&lt;/p&gt; &lt;p&gt;My System Info: &lt;/p&gt; &lt;p&gt;Intel Core i7-14700HX 2.10 GHz | &lt;strong&gt;32 GB RAM&lt;/strong&gt; | &lt;strong&gt;DDR5-5600&lt;/strong&gt; | &lt;strong&gt;65GB/s Bandwidth&lt;/strong&gt; |&lt;/p&gt; &lt;p&gt;llama-bench Command: (Used Q8 for KVCache to get decent t/s with my 32GB RAM)&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-bench -m modelname.gguf -fa 1 -ctk q8_0 -ctv q8_0 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;CPU-only performance stats (Model Name with Quant - t/s):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Qwen3-0.6B-Q8_0 - 86 gemma-3-1b-it-UD-Q8_K_XL - 42 LFM2-2.6B-Q8_0 - 24 LFM2-2.6B.i1-Q4_K_M - 30 SmolLM3-3B-UD-Q8_K_XL - 16 SmolLM3-3B-UD-Q4_K_XL - 27 Llama-3.2-3B-Instruct-UD-Q8_K_XL - 16 Llama-3.2-3B-Instruct-UD-Q4_K_XL - 25 Qwen3-4B-Instruct-2507-UD-Q8_K_XL - 13 Qwen3-4B-Instruct-2507-UD-Q4_K_XL - 20 gemma-3-4b-it-qat-UD-Q6_K_XL - 17 gemma-3-4b-it-UD-Q4_K_XL - 20 Phi-4-mini-instruct.Q8_0 - 16 Phi-4-mini-instruct-Q6_K - 18 granite-4.0-micro-UD-Q8_K_XL - 15 granite-4.0-micro-UD-Q4_K_XL - 24 MiniCPM4.1-8B.i1-Q4_K_M - 10 Llama-3.1-8B-Instruct-UD-Q4_K_XL - 11 Qwen3-8B-128K-UD-Q4_K_XL - 9 gemma-3-12b-it-Q6_K - 6 gemma-3-12b-it-UD-Q4_K_XL - 7 Mistral-Nemo-Instruct-2407-IQ4_XS - 10 Huihui-Ling-mini-2.0-abliterated-MXFP4_MOE - 58 inclusionAI_Ling-mini-2.0-Q6_K_L - 47 LFM2-8B-A1B-UD-Q4_K_XL - 38 ai-sage_GigaChat3-10B-A1.8B-Q4_K_M - 34 Ling-lite-1.5-2507-MXFP4_MOE - 31 granite-4.0-h-tiny-UD-Q4_K_XL - 29 granite-4.0-h-small-IQ4_XS - 9 gemma-3n-E2B-it-UD-Q4_K_XL - 28 gemma-3n-E4B-it-UD-Q4_K_XL - 13 kanana-1.5-15.7b-a3b-instruct-i1-MXFP4_MOE - 24 ERNIE-4.5-21B-A3B-PT-IQ4_XS - 28 SmallThinker-21BA3B-Instruct-IQ4_XS - 26 Phi-mini-MoE-instruct-Q8_0 - 25 Qwen3-30B-A3B-IQ4_XS - 27 gpt-oss-20b-mxfp4 - 23 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;So it seems I would get 3-4X performance if I build a desktop with 128GB DDR5 RAM 6000-6600. For example, above t/s * 4 for 128GB (32GB * 4). And 256GB could give 7-8X and so on. Of course I'm aware of context of models here.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Qwen3-4B-Instruct-2507-UD-Q8_K_XL - 52 (13 * 4) gpt-oss-20b-mxfp4 - 92 (23 * 4) Qwen3-8B-128K-UD-Q4_K_XL - 36 (9 * 4) gemma-3-12b-it-UD-Q4_K_XL - 28 (7 * 4) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I stopped bothering 12+B Dense models since Q4 of 12B Dense models itself bleeding tokens in single digits(Ex: Gemma3-12B just 7 t/s). But I really want to know the CPU-only performance of 12+B Dense models so it could help me deciding to get how much RAM needed for expected t/s. Sharing list for reference, it would be great if someone shares stats of these models. &lt;/p&gt; &lt;pre&gt;&lt;code&gt;Seed-OSS-36B-Instruct-GGUF Mistral-Small-3.2-24B-Instruct-2506-GGUF Devstral-Small-2507-GGUF Magistral-Small-2509-GGUF phi-4-gguf RekaAI_reka-flash-3.1-GGUF NVIDIA-Nemotron-Nano-9B-v2-GGUF NVIDIA-Nemotron-Nano-12B-v2-GGUF GLM-Z1-32B-0414-GGUF Llama-3_3-Nemotron-Super-49B-v1_5-GGUF Qwen3-14B-GGUF Qwen3-32B-GGUF NousResearch_Hermes-4-14B-GGUF gemma-3-12b-it-GGUF gemma-3-27b-it-GGUF &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Please share your stats with your config(Total RAM, RAM Type - MT/s, Total Bandwidth) &amp;amp; whatever models(Quant, t/s) you tried. &lt;/p&gt; &lt;p&gt;And let me know if any changes needed in my llama-bench command to get better t/s. Hope there are few. Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmttyji"&gt; /u/pmttyji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p90zzi/cpuonly_llm_performance_ts_with_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p90zzi/cpuonly_llm_performance_ts_with_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p90zzi/cpuonly_llm_performance_ts_with_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-28T17:40:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1p9oksd</id>
    <title>Setup with Nvidia 6000 Pro</title>
    <updated>2025-11-29T13:09:22+00:00</updated>
    <author>
      <name>/u/Appropriate-Quit1714</name>
      <uri>https://old.reddit.com/user/Appropriate-Quit1714</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What kind of CPU, RAM and Motherboard would you recommend for a setup with the 6000 Pro? I want to get the Maximum performance out of it. Which cooling system is good? Would be grateful for input and short discussion.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Appropriate-Quit1714"&gt; /u/Appropriate-Quit1714 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9oksd/setup_with_nvidia_6000_pro/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9oksd/setup_with_nvidia_6000_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p9oksd/setup_with_nvidia_6000_pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-29T13:09:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1p8ven0</id>
    <title>Compared actual usage costs for Chinese AI models. Token efficiency changes everything.</title>
    <updated>2025-11-28T13:54:37+00:00</updated>
    <author>
      <name>/u/YormeSachi</name>
      <uri>https://old.reddit.com/user/YormeSachi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Everyone talks about per-token pricing but nobody mentions token efficiency. How many tokens does it take to complete the same task?&lt;/p&gt; &lt;p&gt;Tested this with coding tasks cause thats where I actually use these models.&lt;/p&gt; &lt;p&gt;glm-4.6: $0.15 input / $0.60 output Kimi K2: $1.50-2.00 MiniMax: $0.80-1.20 deepseek: $0.28&lt;/p&gt; &lt;p&gt;deepseek looks cheapest on paper. But thats not the whole story.&lt;/p&gt; &lt;p&gt;Token efficiency (same task):&lt;/p&gt; &lt;p&gt;Gave each model identical coding task: &amp;quot;refactor this component to use hooks, add error handling, write tests&amp;quot;&lt;/p&gt; &lt;p&gt;glm: 8,200 tokens average deepseek: 14,800 tokens average MiniMax: 10,500 tokens average, Kimi: 11,000 tokens average&lt;/p&gt; &lt;p&gt;glm uses 26% fewer tokens than Kimi, 45% fewer than deepseek.&lt;/p&gt; &lt;p&gt;Real cost for that task:&lt;/p&gt; &lt;p&gt;glm: ~$0.04 (4 cents) deepseek: ~$0.03 (3 cents) - looks cheaper MiniMax: ~$0.05 (5 cents) Kimi: ~$0.09 (9 cents)&lt;/p&gt; &lt;p&gt;But wait. If you do 100 similar tasks:&lt;/p&gt; &lt;p&gt;glm: Total tokens needed: ~820K, Cost: $0.40-0.50 deepseek: Total tokens needed: ~1.48M, Cost: $0.41 - basically same as glm despite lower per-token price MiniMax: Total tokens needed: ~1.05M, Cost: $0.50-0.60 Kimi: Total tokens needed: ~1.1M, Cost: $0.90-1.00&lt;/p&gt; &lt;p&gt;Token efficiency beats per-token price. glm generates less verbose code, fewer explanatory comments, tighter solutions. deepseek tends to over-explain and generate longer outputs.&lt;/p&gt; &lt;p&gt;For businesses doing thousands of API calls daily, glms efficiency compounds into real savings even though its not the absolute cheapest per-token.&lt;/p&gt; &lt;p&gt;Switched to glm for production workloads. Monthly costs dropped 60% vs previous setup. Performance is adequate for 90% of tasks.&lt;/p&gt; &lt;p&gt;deepseeks pricing looks great until you realize youre using 50% more tokens per task. The savings disappear.&lt;/p&gt; &lt;p&gt;Anyone else measuring token efficiency? Feel like this is the underrated metric everyone ignores.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/YormeSachi"&gt; /u/YormeSachi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8ven0/compared_actual_usage_costs_for_chinese_ai_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8ven0/compared_actual_usage_costs_for_chinese_ai_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p8ven0/compared_actual_usage_costs_for_chinese_ai_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-28T13:54:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1p9lqvl</id>
    <title>[Project] I built pipe: A stateless "Agent as Function" framework that follows Unix Philosophy. (CC0 License / Public Domain)</title>
    <updated>2025-11-29T10:28:15+00:00</updated>
    <author>
      <name>/u/Technical_Cattle_399</name>
      <uri>https://old.reddit.com/user/Technical_Cattle_399</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;I hated stateful chatbots and black-box frameworks that hide context. So I built &lt;strong&gt;pipe&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;It treats an agent not as a partner, but as a stateless, pure function:&lt;/p&gt; &lt;p&gt;f(context) \rightarrow result&lt;/p&gt; &lt;p&gt;It abstracts LLM complexity into a CLI tool called &lt;code&gt;takt&lt;/code&gt;, designed to work seamlessly with standard Unix pipes.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How it works (The Unix Way):&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Bash&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# Example: Create a parent session and use its session ID to create a child session takt --purpose &amp;quot;Simple greeting&amp;quot; \ --background &amp;quot;Basic conversation example.&amp;quot; \ --instruction &amp;quot;Say hello.&amp;quot; \ | jq -r '.session_id' \ | xargs -I {} takt --parent {} \ --purpose &amp;quot;Follow-up response&amp;quot; \ --background &amp;quot;A new session that builds on the parent session.&amp;quot; \ --instruction &amp;quot;Respond to the greeting.&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Key Features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Agent as Function (AasF):&lt;/strong&gt; No hidden state. Input defines output.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Context Engineering:&lt;/strong&gt; Prioritizes structured &amp;quot;Intent&amp;quot; over RAG's fuzzy relevance.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Total Control:&lt;/strong&gt; You manage the context definition (JSON Schema), not the vendor.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The Philosophy:&lt;/p&gt; &lt;p&gt;I wrote a manifesto on why we need &amp;quot;Context Engineering&amp;quot; instead of just throwing RAG at everything. If you are tired of &amp;quot;leaky&amp;quot; context, read this:&lt;/p&gt; &lt;p&gt;Context Engineering: The Art of Communicating Intent to LLMs&lt;/p&gt; &lt;p&gt;License: The Spirit of the Jailbreak&lt;/p&gt; &lt;p&gt;I'm not looking for stars or feature requests. I built this to make my own work easier and deterministic.&lt;/p&gt; &lt;p&gt;The code is released under CC0 (Public Domain).&lt;/p&gt; &lt;p&gt;I don't care what you do with it. Fork it, tear it apart, rebuild it, or use it commercially without attribution.&lt;/p&gt; &lt;p&gt;Customize it as you wish. Jailbreak as you desire.&lt;/p&gt; &lt;p&gt;The purpose of this project is to be a pipe to the agent, and a pipe to the community.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Repo:&lt;/strong&gt; &lt;a href="https://github.com/s-age/pipe"&gt;https://github.com/s-age/pipe&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Technical_Cattle_399"&gt; /u/Technical_Cattle_399 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9lqvl/project_i_built_pipe_a_stateless_agent_as/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9lqvl/project_i_built_pipe_a_stateless_agent_as/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p9lqvl/project_i_built_pipe_a_stateless_agent_as/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-29T10:28:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1p9n0oe</id>
    <title>Will I have any problems pairing a 3090 with a 5060 Ti 16GB?</title>
    <updated>2025-11-29T11:45:58+00:00</updated>
    <author>
      <name>/u/PhantomWolf83</name>
      <uri>https://old.reddit.com/user/PhantomWolf83</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been wondering how feasible would it be to have a dual GPU setup of a 3090 and 5060 Ti 16GB compared to two 5060 Tis. I plan to use the 3090 for LLMs for the higher bandwidth and token generation, and the 5060 Ti as my primary and gaming GPU for the lower power consumption and temperatures and more modern feature set. If I need to I can combine the VRAM for 40GB.&lt;/p&gt; &lt;p&gt;Will there be any compatibility or any other problems with this configuration when using them together for bigger models (I mostly use KoboldCpp, not sure about other LLM programs)? Also, the speed is definitely going to be slower, but how much slower? Will it drop to the speed of the slower card (5060 Ti) or the average of the two?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PhantomWolf83"&gt; /u/PhantomWolf83 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9n0oe/will_i_have_any_problems_pairing_a_3090_with_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9n0oe/will_i_have_any_problems_pairing_a_3090_with_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p9n0oe/will_i_have_any_problems_pairing_a_3090_with_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-29T11:45:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1p9dmul</id>
    <title>What's the best machine I can get for $10k?</title>
    <updated>2025-11-29T02:47:14+00:00</updated>
    <author>
      <name>/u/TWUC</name>
      <uri>https://old.reddit.com/user/TWUC</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking to buy a machine I can use to explore LLM development. My short-list of use cases is: 1) custom model training, 2) running local inference, 3) testing, analyzing, and comparing various models for efficacy/efficiency/performance. My budget is $10k. Ideally, I want something turn-key (not looking to spend too much time building it). I need to be able to run massive full model such as full deepseek 671B.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TWUC"&gt; /u/TWUC &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9dmul/whats_the_best_machine_i_can_get_for_10k/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9dmul/whats_the_best_machine_i_can_get_for_10k/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p9dmul/whats_the_best_machine_i_can_get_for_10k/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-29T02:47:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1p93jcu</id>
    <title>Gemma3 27 heretic, lower divergence than mlabonne/gemma3</title>
    <updated>2025-11-28T19:19:34+00:00</updated>
    <author>
      <name>/u/coder3101</name>
      <uri>https://old.reddit.com/user/coder3101</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I set out to abliterate Gemma3 27b, wanted to reach or surpass the most popular one and here's the results after 5hr on H100 using heretic. &lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;KL Divergence&lt;/th&gt; &lt;th align="left"&gt;Refusal&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/google/gemma-3-27b-pt"&gt;Google's base model&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;0 (by definition)&lt;/td&gt; &lt;td align="left"&gt;98/100&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated"&gt;mlabonne's gemma3&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;0.08&lt;/td&gt; &lt;td align="left"&gt;6/100&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/coder3101/gemma-3-27b-it-heretic"&gt;Heretic gemma3 - v1&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;0.07&lt;/td&gt; &lt;td align="left"&gt;7/100&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/coder3101/gemma-3-27b-it-heretic-v2"&gt;Heretic gemma3 - v2&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;0.03&lt;/td&gt; &lt;td align="left"&gt;14/100&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;KL Divergence:&lt;/strong&gt; Lower the better, roughly a measure of how close the model should be to its original. It is worth noting that lower, better for quantization.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Refusal:&lt;/strong&gt; Lower the better, measure of how many harmful prompts model refused, this is calculated based on presence of tokens such &amp;quot;sorry&amp;quot; etc, which gives a general measure. &lt;/p&gt; &lt;p&gt;I published two versions - one with slightly higher refusal but very low KL divergence and another almost close to that of mlabonne's. It is also worth noting that during my testing I couldn't get v2 to refuse on any prompts, so that would mean it should be much close to original model without refusing on many stuff.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/coder3101"&gt; /u/coder3101 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p93jcu/gemma3_27_heretic_lower_divergence_than/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p93jcu/gemma3_27_heretic_lower_divergence_than/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p93jcu/gemma3_27_heretic_lower_divergence_than/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-28T19:19:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1p93r0w</id>
    <title>Benchmarking LLM Inference on RTX PRO 6000 vs H100 vs H200</title>
    <updated>2025-11-28T19:28:09+00:00</updated>
    <author>
      <name>/u/NoVibeCoding</name>
      <uri>https://old.reddit.com/user/NoVibeCoding</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p93r0w/benchmarking_llm_inference_on_rtx_pro_6000_vs/"&gt; &lt;img alt="Benchmarking LLM Inference on RTX PRO 6000 vs H100 vs H200" src="https://b.thumbs.redditmedia.com/rFGM4WACmn7770XlqkGVAHUlLK43-ZKyVzkyAkXKtqk.jpg" title="Benchmarking LLM Inference on RTX PRO 6000 vs H100 vs H200" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi LocalLlama community. I present an LLM inference throughput benchmark for RTX PRO 6000 WK vs H100 vs H200 vs L40S GPUs, based on the vllm serve and vllm bench serve benchmarking tools, to understand the cost efficiency of RTX PRO 6000 vs previous-generation datacenter GPUs for LLM inference. Pro 6000 is significantly cheaper and is built on the latest Blackwell architecture, but it has slower GDDR memory and lacks NVLink.&lt;/p&gt; &lt;p&gt;&lt;a href="https://medium.com/p/fde1798571a1"&gt;Full article on Medium&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.cloudrift.ai/blog/benchmarking-rtx6000-vs-datacenter-gpus"&gt;Non-medium link&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Benchmarking Setup&lt;/h1&gt; &lt;p&gt;The hardware configurations used:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;1xPRO6000; 1xH100; 1xH200; 2xL40s&lt;/li&gt; &lt;li&gt;8xPRO6000; 8xH100; 8xH200&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;I have optimized the benchmark setup for throughput.&lt;/strong&gt; VLLM serves models. The model is split across multiple GPUs using the --tensor-parallel-size VLLM option, if needed. I run as many VLLM instances as possible, using an NGINX load balancer on top to distribute requests across them and maximize throughput (replica parallelism). For example, if only four GPUs are required to run the model on an 8-GPU machine, I run two VLLM instances with --tensor-parallel-size=4 and an NGINX load balancer. If all eight GPUs are required, then a single VLLM instance with --tensor-parallel-size=8 is used.&lt;/p&gt; &lt;p&gt;The &lt;strong&gt;vllm bench serve&lt;/strong&gt; tool is used for benchmarking with random data and a sequence length of 1000. The number of concurrent requests is set between 256 and 512 to ensure the LLM's token-generation capacity is saturated.&lt;/p&gt; &lt;p&gt;I have benchmarked three models to better understand the effect of PCIe communication on the 8xPro6000 server vs. NVLink on the H100/H200.&lt;/p&gt; &lt;p&gt;Here is the model selection and the logic behind it:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;GLM-4.5-Air-AWQ-4bit (fits 80GB).&lt;/strong&gt; Testing single-GPU performance and maximum throughput with replica scaling on 8 GPU setups. No PCIE bottleneck. The Pro 6000 should demonstrate strong results thanks to Blackwell native support for FP4.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Qwen3-Coder-480B-A35B-Instruct-AWQ (fits 320GB).&lt;/strong&gt; This 4-bit-quantized model fits into 4 GPUs. Some PCIe communication overhead in Pro 6000 setups may reduce performance relative to NVLink-enabled datacenter GPUs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GLM-4.6-FP8 (fits 640GB).&lt;/strong&gt; This model requires all eight GPUs. PCIe communication overhead expected. The H100 and H200 configurations should have an advantage.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Besides raw throughput, graphs show the serving cost per million tokens for each model on its respective hardware. The rental price is set to $2.09 for Pro6000; $2.69 for H100; $3.39 for H200, and $0.86 for L40S - today's rental prices from Runpod secure cloud.&lt;/p&gt; &lt;h1&gt;Results&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;For single-GPU workloads&lt;/strong&gt;, RTX PRO 6000 is a clear winnerâ€”and arguably an H100 killer. Remarkably, the PRO 6000 with GDDR7 memory outperforms even the H100 SXM with its HBM3e in single-GPU throughput (3,140 vs 2,987 tok/s), while delivering 28% lower cost per token ($0.18 vs $0.25/mtok). The 2xL40S configuration is the least performant and most cost-effective of the bunch.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;For medium-sized models&lt;/strong&gt; requiring 2-4 GPUs, PRO 6000 remains competitive. While it loses some ground to NVLink-equipped datacenter GPUs, the cost efficiency stays within the same ballpark ($1.03 vs $1.01/mtok for Qwen3-480B).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;For large models&lt;/strong&gt; requiring 8-way tensor parallelism, datacenter GPUs pull ahead significantly. The H100 and H200's NVLink interconnect delivers 3-4x the throughput of PCIe-bound PRO 6000s. The cost efficiency gap is significant: $1.72/mtok for Pro6000 vs $0.72-0.76/mtok for H100/H200.&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/qyda1ooas14g1.gif"&gt;Price in millidollars, i.e. around $0.2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/69a68sbks14g1.gif"&gt;https://i.redd.it/69a68sbks14g1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/rtyvpiars14g1.gif"&gt;https://i.redd.it/rtyvpiars14g1.gif&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Code and Resources&lt;/h1&gt; &lt;p&gt;The code is available &lt;a href="https://github.com/cloudrift-ai/server-benchmark"&gt;here&lt;/a&gt;. Instructions for performing your own benchmark are in the README. You can find the benchmark data in the results folder.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NoVibeCoding"&gt; /u/NoVibeCoding &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p93r0w/benchmarking_llm_inference_on_rtx_pro_6000_vs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p93r0w/benchmarking_llm_inference_on_rtx_pro_6000_vs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p93r0w/benchmarking_llm_inference_on_rtx_pro_6000_vs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-28T19:28:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1p8w9hg</id>
    <title>unsloth/Qwen3-Next-80B-A3B-Thinking-GGUF Â· Hugging Face</title>
    <updated>2025-11-28T14:31:50+00:00</updated>
    <author>
      <name>/u/WhaleFactory</name>
      <uri>https://old.reddit.com/user/WhaleFactory</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8w9hg/unslothqwen3next80ba3bthinkinggguf_hugging_face/"&gt; &lt;img alt="unsloth/Qwen3-Next-80B-A3B-Thinking-GGUF Â· Hugging Face" src="https://external-preview.redd.it/ble3gnyoRHIxGbCkynVYdB5oBvepM5IUsQgkKmTQPvE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8b518c02f5843d8adaaa30fd9ccf229f596ac77a" title="unsloth/Qwen3-Next-80B-A3B-Thinking-GGUF Â· Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WhaleFactory"&gt; /u/WhaleFactory &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/unsloth/Qwen3-Next-80B-A3B-Thinking-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8w9hg/unslothqwen3next80ba3bthinkinggguf_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p8w9hg/unslothqwen3next80ba3bthinkinggguf_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-28T14:31:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1p8s7az</id>
    <title>Model: Qwen3 Next by pwilkin Â· Pull Request #16095 Â· ggml-org/llama.cpp</title>
    <updated>2025-11-28T11:05:36+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8s7az/model_qwen3_next_by_pwilkin_pull_request_16095/"&gt; &lt;img alt="Model: Qwen3 Next by pwilkin Â· Pull Request #16095 Â· ggml-org/llama.cpp" src="https://external-preview.redd.it/GTfGIM6FaPx4w5_-UCOwiPgKZNkGDkC0q-Pvot4uDk0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=45f974480724f58cdf70046026bd0ccf7e6b00f6" title="Model: Qwen3 Next by pwilkin Â· Pull Request #16095 Â· ggml-org/llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;and it's done&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/16095"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8s7az/model_qwen3_next_by_pwilkin_pull_request_16095/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p8s7az/model_qwen3_next_by_pwilkin_pull_request_16095/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-28T11:05:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1p9iawm</id>
    <title>Looking for open source 10B model that is comparable to gpt4o-mini</title>
    <updated>2025-11-29T06:55:54+00:00</updated>
    <author>
      <name>/u/bohemianLife1</name>
      <uri>https://old.reddit.com/user/bohemianLife1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi All, big fan of this community. &lt;/p&gt; &lt;p&gt;I am looking for a 10B model that is comparable to GPT4o-mini.&lt;br /&gt; Application is simple it has to be coherent in sentence formation (conversational) i.e ability follow good system prompt (15k token length).&lt;br /&gt; Good Streaming performance (TTFT, 600 ms).&lt;br /&gt; Solid reliability on function calling upto 15 tools. &lt;/p&gt; &lt;p&gt;Some background:-&lt;/p&gt; &lt;p&gt;In my daily testing (Voice Agent developer) I found only one model till date which is useful in voice application. That is GPT4o-mini after this model no model in open / close has come to it. I was very excited for LFM model with amazing state space efficiency but I failed to get good system prompt adherence with it. &lt;/p&gt; &lt;p&gt;All new model again closed / open are focusing on intelligence (through reasoning) and not reliability with speed. &lt;/p&gt; &lt;p&gt;If anyone has proper suggestion it would help the most.&lt;/p&gt; &lt;p&gt;I am trying to put voice agent in single GPU.&lt;br /&gt; ASR with &lt;a href="https://huggingface.co/nvidia/parakeet_realtime_eou_120m-v1"&gt;https://huggingface.co/nvidia/parakeet_realtime_eou_120m-v1&lt;/a&gt; (it's amazing takes 1GB of VRAM)&lt;br /&gt; LLM &amp;lt;=== Need help!&lt;br /&gt; TTS with &lt;a href="https://github.com/ysharma3501/FastMaya"&gt;https://github.com/ysharma3501/FastMaya&lt;/a&gt; (Maya 1 from maya research)&lt;/p&gt; &lt;p&gt;Hardware: 16GB 5060Ti&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bohemianLife1"&gt; /u/bohemianLife1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9iawm/looking_for_open_source_10b_model_that_is/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9iawm/looking_for_open_source_10b_model_that_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p9iawm/looking_for_open_source_10b_model_that_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-29T06:55:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1p8wcbn</id>
    <title>Ask me to run models</title>
    <updated>2025-11-28T14:35:08+00:00</updated>
    <author>
      <name>/u/monoidconcat</name>
      <uri>https://old.reddit.com/user/monoidconcat</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8wcbn/ask_me_to_run_models/"&gt; &lt;img alt="Ask me to run models" src="https://b.thumbs.redditmedia.com/SlyBYDnPqUybVrSsHIbiqHHjqXLR7EhbE0-6UyLUGJk.jpg" title="Ask me to run models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys, I am currently in the process of upgrading my 4Ã—3090 setup to 2Ã—5090 + 1Ã—RTX Pro 6000. As a result, I have all three kinds of cards in the rig temporarily, and I thought it would be a good idea to take some requests for models to run on my machine.&lt;/p&gt; &lt;p&gt;Here is my current setup: - 1Ã— RTX Pro 6000 Blackwell, power limited to 525 W - 2Ã— RTX 5090, power limited to 500 W - 2Ã— RTX 3090, power limited to 280 W - WRX80E (PCIe 4.0 x16) with 3975WX - 512 GB DDR4 RAM&lt;/p&gt; &lt;p&gt;If you have any model that you want me to run with a specific setup (certain cards, parallelism methods, etc.), let me know in the comments. Iâ€™ll run them this weekend and reply with the tok/s!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/monoidconcat"&gt; /u/monoidconcat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1p8wcbn"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8wcbn/ask_me_to_run_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p8wcbn/ask_me_to_run_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-28T14:35:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1p9dxvd</id>
    <title>New Model Step-Audio-R1 open source audio model to actually use CoT reasoning, close to Gemini 3</title>
    <updated>2025-11-29T03:02:19+00:00</updated>
    <author>
      <name>/u/Successful-Bill-5543</name>
      <uri>https://old.reddit.com/user/Successful-Bill-5543</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Apache2.0&lt;br /&gt; Reasons from sound, not transcripts&lt;br /&gt; Outperforms Gemini 2.5 Pro, close to Gemini 3&lt;br /&gt; Works across speech, sounds, and music&lt;/p&gt; &lt;p&gt;HuggingFace: &lt;a href="https://huggingface.co/collections/stepfun-ai/step-audio-r1"&gt;https://huggingface.co/collections/stepfun-ai/step-audio-r1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Successful-Bill-5543"&gt; /u/Successful-Bill-5543 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9dxvd/new_model_stepaudior1_open_source_audio_model_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9dxvd/new_model_stepaudior1_open_source_audio_model_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p9dxvd/new_model_stepaudior1_open_source_audio_model_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-29T03:02:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1p9jzx6</id>
    <title>RAG from Scratch is now live on GitHub</title>
    <updated>2025-11-29T08:38:56+00:00</updated>
    <author>
      <name>/u/purellmagents</name>
      <uri>https://old.reddit.com/user/purellmagents</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Itâ€™s an educational open-source project, inspired by my previous repo AI Agents from Scratch, available here: &lt;a href="https://github.com/pguso/rag-from-scratch"&gt;https://github.com/pguso/rag-from-scratch&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The goal is to &lt;strong&gt;demystify Retrieval-Augmented Generation (RAG)&lt;/strong&gt; by letting developers build it step by step. No black boxes, no frameworks, no cloud APIs.&lt;/p&gt; &lt;p&gt;Each folder introduces one clear concept (embeddings, vector stores, retrieval, augmentation, etc.) with &lt;strong&gt;tiny runnable JS files&lt;/strong&gt; and a &lt;a href="http://CODE.md"&gt;CODE.md&lt;/a&gt; file that explains the code in detail and &lt;a href="http://CONCEPT.md"&gt;CONCEPT.md&lt;/a&gt; file that explains it on a more non technical level.&lt;/p&gt; &lt;p&gt;Right now, the project is &lt;strong&gt;about halfway implemented&lt;/strong&gt;:&lt;br /&gt; the core RAG building blocks are already there and ready to run, and more advanced topics are being added incrementally.&lt;/p&gt; &lt;h1&gt;Whatâ€™s in so far (roughly first half)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;How RAG works (tiny &amp;lt;70-line demo)&lt;/li&gt; &lt;li&gt;LLM basics with &lt;code&gt;node-llama-cpp&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Data loading &amp;amp; preprocessing&lt;/li&gt; &lt;li&gt;Text splitting &amp;amp; chunking&lt;/li&gt; &lt;li&gt;Embeddings + cosine similarity&lt;/li&gt; &lt;li&gt;In-memory vector store + k-NN search&lt;/li&gt; &lt;li&gt;Basic retrieval strategies&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Everything runs &lt;strong&gt;fully local&lt;/strong&gt; using embedded databases and &lt;code&gt;node-llama-cpp&lt;/code&gt; for inference, so you can learn RAG &lt;strong&gt;without paying for APIs&lt;/strong&gt;.&lt;/p&gt; &lt;h1&gt;Coming next&lt;/h1&gt; &lt;h1&gt;Still missing / coming next&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Query preprocessing &amp;amp; normalization&lt;/li&gt; &lt;li&gt;Hybrid search, multi-query retrieval&lt;/li&gt; &lt;li&gt;Query rewriting &amp;amp; re-ranking&lt;/li&gt; &lt;li&gt;Post-retrieval reranking&lt;/li&gt; &lt;li&gt;Prompt engineering for RAG (citations, compression)&lt;/li&gt; &lt;li&gt;Full RAG pipelines with errors, fallbacks &amp;amp; streaming&lt;/li&gt; &lt;li&gt;Evaluation metrics (retrieval + generation)&lt;/li&gt; &lt;li&gt;Caching, observability, performance monitoring&lt;/li&gt; &lt;li&gt;Metadata &amp;amp; structured data&lt;/li&gt; &lt;li&gt;Graph DB integration (embedded with kuzu)&lt;/li&gt; &lt;li&gt;Templates (simple RAG, API server, chatbot)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Why this exists&lt;/h1&gt; &lt;p&gt;At this stage, a good chunk of the pipeline is implemented, but the focus is still on &lt;strong&gt;teaching, not tooling&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Understand RAG before reaching for frameworks like &lt;strong&gt;LangChain&lt;/strong&gt; or &lt;strong&gt;LlamaIndex&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;See every step as &lt;strong&gt;real, minimal code&lt;/strong&gt; - no magic helpers&lt;/li&gt; &lt;li&gt;Learn concepts in the order youâ€™d actually build them&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Feel free to open issues, suggest tweaks, or send PRs - especially if you have small, focused examples that explain one RAG idea really well.&lt;/p&gt; &lt;p&gt;Thanks for checking it out and stay tuned as the remaining steps (advanced retrieval, prompt engineering, evaluation, observability, etc.) get implemented over time &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/purellmagents"&gt; /u/purellmagents &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9jzx6/rag_from_scratch_is_now_live_on_github/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9jzx6/rag_from_scratch_is_now_live_on_github/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p9jzx6/rag_from_scratch_is_now_live_on_github/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-29T08:38:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1p8v9y9</id>
    <title>unsloth/Qwen3-Next-80B-A3B-Instruct-GGUF Â· Hugging Face</title>
    <updated>2025-11-28T13:48:28+00:00</updated>
    <author>
      <name>/u/WhaleFactory</name>
      <uri>https://old.reddit.com/user/WhaleFactory</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8v9y9/unslothqwen3next80ba3binstructgguf_hugging_face/"&gt; &lt;img alt="unsloth/Qwen3-Next-80B-A3B-Instruct-GGUF Â· Hugging Face" src="https://external-preview.redd.it/SSIhbD5Dl8kZRyNgV0oqxKpaE8kMvA_ZXLBFpkDEq90.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1291833f6c1644105b326fbe9244666f7b478451" title="unsloth/Qwen3-Next-80B-A3B-Instruct-GGUF Â· Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WhaleFactory"&gt; /u/WhaleFactory &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/unsloth/Qwen3-Next-80B-A3B-Instruct-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8v9y9/unslothqwen3next80ba3binstructgguf_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p8v9y9/unslothqwen3next80ba3binstructgguf_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-28T13:48:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1p9c2wv</id>
    <title>The official vLLM support for the Ryzen AI Max+ 395 is here! (the whole AI 300 series, ie gfx1150 and gfx1151)</title>
    <updated>2025-11-29T01:30:18+00:00</updated>
    <author>
      <name>/u/waiting_for_zban</name>
      <uri>https://old.reddit.com/user/waiting_for_zban</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9c2wv/the_official_vllm_support_for_the_ryzen_ai_max/"&gt; &lt;img alt="The official vLLM support for the Ryzen AI Max+ 395 is here! (the whole AI 300 series, ie gfx1150 and gfx1151)" src="https://external-preview.redd.it/0y9iS0G6sFyr9NyDTUC-Jc4R7icCyd9xrKN5lMvXDtc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=60ef0dca89f1f06038a7c4b44821f7309152de84" title="The official vLLM support for the Ryzen AI Max+ 395 is here! (the whole AI 300 series, ie gfx1150 and gfx1151)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/waiting_for_zban"&gt; /u/waiting_for_zban &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/vllm-project/vllm/pull/25908"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9c2wv/the_official_vllm_support_for_the_ryzen_ai_max/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p9c2wv/the_official_vllm_support_for_the_ryzen_ai_max/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-29T01:30:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1p9bk2b</id>
    <title>Claude code can now connect directly to llama.cpp server</title>
    <updated>2025-11-29T01:05:34+00:00</updated>
    <author>
      <name>/u/tarruda</name>
      <uri>https://old.reddit.com/user/tarruda</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anthropic messages API was merged today and allows claude code to connect to llama-server: &lt;a href="https://github.com/ggml-org/llama.cpp/pull/17570"&gt;https://github.com/ggml-org/llama.cpp/pull/17570&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I've been playing with claude code + gpt-oss 120b and it seems to work well at 700 pp and 60 t/s. I don't recommend trying slower LLMs because the prompt processing time is going to kill the experience.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tarruda"&gt; /u/tarruda &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9bk2b/claude_code_can_now_connect_directly_to_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9bk2b/claude_code_can_now_connect_directly_to_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p9bk2b/claude_code_can_now_connect_directly_to_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-29T01:05:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1p9ah2v</id>
    <title>A Tribute to MetaAI and Stability AI - 2 Giants Who Brought us so Much Joy... And, 2025 is the Year they Die... So Sad!ðŸ˜¢</title>
    <updated>2025-11-29T00:15:18+00:00</updated>
    <author>
      <name>/u/Iory1998</name>
      <uri>https://old.reddit.com/user/Iory1998</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I mean, this sub and its amazing community wouldn't be here if it were not for Stability AI and Meta AI. I personally created an account on Reddit just so I could join &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; and &lt;a href="/r/StableDiffusion"&gt;r/StableDiffusion&lt;/a&gt;. I remember the first day I tried SD1.4 on my shiny new RTX 3070 Ti. I couldn't contain my excitement as I was going through Aitrepreneurâ€™s video on how to install AUTOMATIC1111.&lt;/p&gt; &lt;p&gt;I never had Conda or PyTorch installed on my machine before. There was no ChatGPT to write me a guide on how to install everything or troubleshoot a failure. I followed Nerdy Rodent's videos on possible issues I could face, and I heavily relied on this sub for learning.&lt;/p&gt; &lt;p&gt;Then, I remember the first image I generated. That first one is always special. I took a few minutes to think of what I wanted to write, and I went for &amp;quot;Lionel Messi riding a bicycle.&amp;quot; (Damn, I feel so embarrassed now that I am writing this. Please don't judge me!).&lt;/p&gt; &lt;p&gt;I cannot thank Stability AI's amazing team enough for opening a new world for meâ€”for us. Every day, new AI tutorials would drop on YouTube, and every day, I was excited. I vividly remember the first Textual Inversion I trained, my first LoRA, and my first model finetune on Google Colab. Shortly after, SD 1.5 dropped. I never felt closer to YouTubers before; I could feel their excitement as they went through the material. That excitement felt genuine and was contagious.&lt;/p&gt; &lt;p&gt;And then, the NovelAI models were leaked. I downloaded the torrent with all the checkpoints, and the floodgates for finetunes opened. Do you guys remember Anything v3 and RevAnime? Back then, our dream was simple and a bit naive: we dreamed of the day where we would run Midjourney v3-level image quality locally ðŸ¤£.&lt;/p&gt; &lt;p&gt;Fast forward 6 months, and Llama models were leaked (7B, 13B, 33B, and 65B) with their limited 2K context window. Shortly after, Oobabooga WebUI was out and was the only frontend you could use. I could barely fit Llama 13B in my 8GB of VRAM. GPTQ quants were a pain in the ass. Regardless, running Llama locally always put a smile on my face.&lt;/p&gt; &lt;p&gt;If you are new to the LLM space, let me tell you what our dream was back then: to have a model as good as ChatGPT 3.5 Turbo. Benchmarks were always against 3.5!! Whenever a new finetune dropped, the main question remained: how good is it compared to ChatGPT? As a community, we struggled for over a year to get a local model that finally beat ChatGPT (I think it was Mixtral 8x7B).&lt;/p&gt; &lt;p&gt;This brings me to the current time. We have many frontier open-source models both in LLM and image/video generation, and neither Meta nor Stability AI made any of them. They both shot themselves in the foot and then effectively committed suicide. They could've owned the open-source space, but for whatever reason, they botched that huge opportunity. Their work contributed so much to the world, and it saddens me to see that they have already sailed into the sunset. Did you know that the first works by DeepSeek and other Chinese labs were heavily built upon the Llama architecture? They learned from Llama and Stable Diffusion, and in 2025, they just killed them.&lt;/p&gt; &lt;p&gt;I am sorry if I seem emotional, because I am. About 6 months ago, I deleted the last Llama-based model I had. 3 months ago, I deleted all SD1.5-based models. And with the launch of the Z-model, I know that soon I will be deleting all Stable Diffusion-based models again. If you had told me 3 years ago that by 2025 both Meta and Stability AI would disappear from the open-source AI space, I wouldn't have believed you in a million years. This is another reminder that technology is a ruthless world.&lt;/p&gt; &lt;p&gt;What are your thoughts? Perhaps you can share your emotional experiences as well. Let this post be a tribute to two otherwise awesome AI labs.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Iory1998"&gt; /u/Iory1998 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9ah2v/a_tribute_to_metaai_and_stability_ai_2_giants_who/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9ah2v/a_tribute_to_metaai_and_stability_ai_2_giants_who/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p9ah2v/a_tribute_to_metaai_and_stability_ai_2_giants_who/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-29T00:15:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1p9htgc</id>
    <title>Hardcore RAG &amp; AI Search resources</title>
    <updated>2025-11-29T06:27:02+00:00</updated>
    <author>
      <name>/u/LilDemonApparel</name>
      <uri>https://old.reddit.com/user/LilDemonApparel</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;Iâ€™m starting to onboard Enterprise clients and I need to move past the basic tutorials.&lt;/p&gt; &lt;p&gt;Iâ€™m posting here because most other subs feel a bit too high-level or news-focused, whereas I know this community is focused on actual engineering.&lt;/p&gt; &lt;p&gt;I need deep-dive resources on production-grade RAG. I'm looking for:&lt;/p&gt; &lt;p&gt;SOTA Papers (ArXiv links welcome)&lt;/p&gt; &lt;p&gt;Advanced Architectures &lt;/p&gt; &lt;p&gt;Engineering Blogs regarding evaluation and scale.&lt;/p&gt; &lt;p&gt;Any must-read links, communities or repos you recommend?&lt;/p&gt; &lt;p&gt;Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LilDemonApparel"&gt; /u/LilDemonApparel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9htgc/hardcore_rag_ai_search_resources/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9htgc/hardcore_rag_ai_search_resources/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p9htgc/hardcore_rag_ai_search_resources/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-29T06:27:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1p9ojio</id>
    <title>Yet another reason to stick with local models</title>
    <updated>2025-11-29T13:07:36+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9ojio/yet_another_reason_to_stick_with_local_models/"&gt; &lt;img alt="Yet another reason to stick with local models" src="https://b.thumbs.redditmedia.com/LVWG1v1DcQ2DgWlztEoKA3ITIG04JVS8k3_QhWcs3tw.jpg" title="Yet another reason to stick with local models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/uofoe3u5374g1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=219b2ab46ac5d0b74767604bd131b78757a40ac9"&gt;https://preview.redd.it/uofoe3u5374g1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=219b2ab46ac5d0b74767604bd131b78757a40ac9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/btibor91/status/1994714152636690834?s=20"&gt;Tibor Blaho&lt;/a&gt;,a trusted reverse engineer, found ad system strings inside the latest ChatGPT Android beta(v1.2025.329).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9ojio/yet_another_reason_to_stick_with_local_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9ojio/yet_another_reason_to_stick_with_local_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p9ojio/yet_another_reason_to_stick_with_local_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-29T13:07:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1p9nckz</id>
    <title>Qwen3-Next-80B-A3B vs gpt-oss-120b</title>
    <updated>2025-11-29T12:04:53+00:00</updated>
    <author>
      <name>/u/bfroemel</name>
      <uri>https://old.reddit.com/user/bfroemel</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Benchmarks aside - who has the better experience with what model and why? Please comment incl. your use-cases (incl. your software stack in case you use more than llama.cpp/vllm/sglang).&lt;/p&gt; &lt;p&gt;My main use case is agentic coding/software engineering (Python, see my comment history for details) and gpt-oss-120b remains the clear winner (although I am limited to Qwen3-Next-80B-A3B-Instruct-UD-Q8_K_XL; using recommended sampling parameters for both models). I haven't tried tool calls with Qwen3-Next yet, but did just simple coding tasks right within llama.cpp's web frontend. For me gpt-oss consistently comes up with a more nuanced, correct solution faster while Qwen3-Next usually needs more shots. (Funnily, when I let gpt-oss-120b correct a solution that Qwen3-Next thinks is already production-grade quality, it admits its mistakes right away and has only the highest praises for the corrections). I did not even try the Thinking version, because benchmarks (e.g., also see Discord aider) show that Instruct is much better than Thinking for coding use-cases.&lt;/p&gt; &lt;p&gt;At least in regard to my main use case I am particularly impressed by the difference in memory requirements: gpt-oss-120b mxfp4 is about 65 GB, that's more than 25% smaller than Qwen3-Next-80B-A3B (the 8-bit quantized version still requires about 85 GB VRAM).&lt;/p&gt; &lt;p&gt;Qwen3-Next might be better in other regards and/or has to be used differently. Also I think Qwen3-Next has been more intended as a preview, so it might me more about the model architecture, training method advances, and less about its usefulness in actual real-world tasks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bfroemel"&gt; /u/bfroemel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9nckz/qwen3next80ba3b_vs_gptoss120b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9nckz/qwen3next80ba3b_vs_gptoss120b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p9nckz/qwen3next80ba3b_vs_gptoss120b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-29T12:04:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1p9i5ew</id>
    <title>Try the new Z-Image-Turbo 6B (Runs on 8GB VRAM)!</title>
    <updated>2025-11-29T06:46:42+00:00</updated>
    <author>
      <name>/u/KvAk_AKPlaysYT</name>
      <uri>https://old.reddit.com/user/KvAk_AKPlaysYT</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks,&lt;/p&gt; &lt;p&gt;I wanted to try out the new Z-Image-Turbo model (the 6B one that just dropped), but I didn't want to fiddle with complex workflows or wait for specific custom nodes to mature.&lt;/p&gt; &lt;p&gt;So, I threw together a dedicated, clean Web UI to run it.&lt;/p&gt; &lt;p&gt;Has CPU offload too! :)&lt;/p&gt; &lt;p&gt;Check it out: &lt;a href="https://github.com/Aaryan-Kapoor/z-image-turbo"&gt;https://github.com/Aaryan-Kapoor/z-image-turbo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/Tongyi-MAI/Z-Image-Turbo"&gt;https://huggingface.co/Tongyi-MAI/Z-Image-Turbo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;May your future be full of VRAM!&lt;/p&gt; &lt;p&gt;Edit: Added Google Colab notebook as well, enjoy! :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KvAk_AKPlaysYT"&gt; /u/KvAk_AKPlaysYT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9i5ew/try_the_new_zimageturbo_6b_runs_on_8gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9i5ew/try_the_new_zimageturbo_6b_runs_on_8gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p9i5ew/try_the_new_zimageturbo_6b_runs_on_8gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-29T06:46:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5retd</id>
    <title>Best Local VLMs - November 2025</title>
    <updated>2025-11-24T20:00:04+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Share what your favorite models are right now and &lt;strong&gt;&lt;em&gt;why&lt;/em&gt;&lt;/strong&gt;. Given the nature of the beast in evaluating VLMs (untrustworthiness of benchmarks, immature tooling, intrinsic stochasticity), please be as detailed as possible in describing your setup, nature of your usage (what applications, how much, personal/professional use), tools/frameworks/prompts etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5retd/best_local_vlms_november_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5retd/best_local_vlms_november_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p5retd/best_local_vlms_november_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T20:00:04+00:00</published>
  </entry>
</feed>
