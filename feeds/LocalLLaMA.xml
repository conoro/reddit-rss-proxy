<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-22T20:07:33+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1odcib3</id>
    <title>Qwen3-VL-2B GGUF is here</title>
    <updated>2025-10-22T16:08:01+00:00</updated>
    <author>
      <name>/u/AlanzhuLy</name>
      <uri>https://old.reddit.com/user/AlanzhuLy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odcib3/qwen3vl2b_gguf_is_here/"&gt; &lt;img alt="Qwen3-VL-2B GGUF is here" src="https://external-preview.redd.it/W7AGVvstE9pre0GHVfitXRaqvaJcdswOTBT90OSCLds.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0b8a9fdf290f4fb4c4b047a45249827638e70e25" title="Qwen3-VL-2B GGUF is here" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GGUFs are available (Note currently only NexaSDK supports Qwen3-VL-2B GGUF model)&lt;br /&gt; &lt;a href="https://huggingface.co/NexaAI/Qwen3-VL-2B-Thinking-GGUF"&gt;https://huggingface.co/NexaAI/Qwen3-VL-2B-Thinking-GGUF&lt;/a&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/NexaAI/Qwen3-VL-2B-Instruct-GGUF"&gt;https://huggingface.co/NexaAI/Qwen3-VL-2B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here's a quick demo of it counting circles: 155 t/s on M4 Max&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1odcib3/video/y3bwkg6psowf1/player"&gt;https://reddit.com/link/1odcib3/video/y3bwkg6psowf1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Quickstart in 2 steps&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Step 1: Download &lt;a href="https://github.com/NexaAI/nexa-sdk"&gt;NexaSDK&lt;/a&gt; with one click&lt;/li&gt; &lt;li&gt;Step 2: one line of code to run in your terminal: &lt;ul&gt; &lt;li&gt;&lt;code&gt;nexa infer NexaAI/Qwen3-VL-2B-Instruct-GGUF&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;nexa infer NexaAI/Qwen3-VL-2B-Thinking-GGUF&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What would you use this model for?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlanzhuLy"&gt; /u/AlanzhuLy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odcib3/qwen3vl2b_gguf_is_here/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odcib3/qwen3vl2b_gguf_is_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1odcib3/qwen3vl2b_gguf_is_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-22T16:08:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1odilom</id>
    <title>RamaLama: Running LLMs as containers adding MLX support</title>
    <updated>2025-10-22T19:52:16+00:00</updated>
    <author>
      <name>/u/ProfessionalHorse707</name>
      <uri>https://old.reddit.com/user/ProfessionalHorse707</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odilom/ramalama_running_llms_as_containers_adding_mlx/"&gt; &lt;img alt="RamaLama: Running LLMs as containers adding MLX support" src="https://external-preview.redd.it/cTYxeHE0emh3cHdmMXy1caZuZgS62CJImk8F9ntJO8-7vfSdfeZ_sZ1BZhe-.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9cbd4b2a2706487e2adcb2c8d660e5515a13f61a" title="RamaLama: Running LLMs as containers adding MLX support" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m not sure if anyone has played around with it yet but RamaLama is CLI for running and building LLMs as container images. &lt;/p&gt; &lt;p&gt;We recently added support for MLX in addition to llama.cpp and vLLM (shoutout to kush-gupt)! We are aiming to be totally runtime and hardware agnostic but it’s been an uphill battle with vLLM support still a little shaky. Still, we’ve got support for Apple Silicon GPUs, Nvidia GPUs (cuda), AMD GPUs (rocm, vulkan), Intel GPUs, Moore Threads GPUs, and Ascend NPUs. With so much variation we could really use help finding people with atypical hardware configurations to test against. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Github&lt;/strong&gt;: &lt;a href="https://github.com/containers/ramalama"&gt;https://github.com/containers/ramalama&lt;/a&gt; &lt;/p&gt; &lt;p&gt;As an aside, there’s going to be a developer forum in a few weeks for new users: &lt;a href="http://ramalama.com/events/dev-forum-1"&gt;http://ramalama.com/events/dev-forum-1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ProfessionalHorse707"&gt; /u/ProfessionalHorse707 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/x0zcn3zhwpwf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odilom/ramalama_running_llms_as_containers_adding_mlx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1odilom/ramalama_running_llms_as_containers_adding_mlx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-22T19:52:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1odax0g</id>
    <title>Feasibility Check: Modifying DeepSeek-OCR (2510.18234) into an Instruction-Following Document VLM?</title>
    <updated>2025-10-22T15:09:45+00:00</updated>
    <author>
      <name>/u/hiiamtin</name>
      <uri>https://old.reddit.com/user/hiiamtin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone &lt;/p&gt; &lt;p&gt;I've been digging into the new DeepSeek-OCR paper (arXiv: 2510.18234), and its DeepEncoder looks like a game-changer for handling high-resolution, dense documents with its high-compression ratio.&lt;/p&gt; &lt;p&gt;As I understand it, the model in its current form is a pure OCR engine, with a workflow of:&lt;/p&gt; &lt;p&gt;Image -&amp;gt; [Encoder -&amp;gt; Decoder] -&amp;gt; Full Text (It seems it's not designed to take text instructions, only image inputs).&lt;/p&gt; &lt;p&gt;I'm wondering about the feasibility of modifying this to become an instruction-following Visual Language Model (VLM) for documents.&lt;/p&gt; &lt;p&gt;The Core Idea: To change the workflow to: Image + Text Instruction -&amp;gt; Specific Answer&lt;/p&gt; &lt;p&gt;For example: * Input: (Image of an invoice) + &amp;quot;Extract the final total.&amp;quot; * Output: &amp;quot;$450.72&amp;quot; * Input: (Image of a paper) + &amp;quot;Summarize the abstract.&amp;quot; * Output: &amp;quot;The paper introduces a novel optical compression engine...&amp;quot;&lt;/p&gt; &lt;p&gt;Proposed High-Level Approach:&lt;/p&gt; &lt;p&gt;Since the base model only accepts images, a modification would be necessary:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Keep the DeepEncoder: Leverage the pre-trained DeepEncoder as the powerful, high-resolution vision backbone.&lt;/li&gt; &lt;li&gt;Modify the Architecture: This is the key step. We would need to adapt the model (likely the DeepSeek3B-MoE decoder part) to accept two types of input simultaneously: &lt;ul&gt; &lt;li&gt;The vision_tokens (from the document via the Encoder/Projector).&lt;/li&gt; &lt;li&gt;The text_tokens (from the user's new instruction).&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Instruction Fine-Tune: Re-train (SFT) this modified model on a new dataset of (image, instruction, answer) pairs. This would teach the LLM decoder to reason based on the combined inputs, rather than just transcribe the visual input.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;My Questions: * Is this a sound approach? Does this architectural modification make sense? * Has anyone tried this? I know of models like LLaVA, Donut, etc., but the appeal here is starting with DeepSeek's SOTA document-specific encoder, rather than a general-purpose one like CLIP. * What are the biggest challenges? I assume preventing &amp;quot;catastrophic forgetting&amp;quot; (i.e., making sure it can still do basic OCR) would be one. How hard is it to get the model to properly attend to both the image and text instructions?&lt;/p&gt; &lt;p&gt;Would love to hear any thoughts or see if I'm missing a more obvious path. Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hiiamtin"&gt; /u/hiiamtin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odax0g/feasibility_check_modifying_deepseekocr_251018234/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odax0g/feasibility_check_modifying_deepseekocr_251018234/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1odax0g/feasibility_check_modifying_deepseekocr_251018234/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-22T15:09:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1odasbj</id>
    <title>GPT-OSS-20b TAKE THE HELM! Further experiments in autopilot.</title>
    <updated>2025-10-22T15:04:54+00:00</updated>
    <author>
      <name>/u/teachersecret</name>
      <uri>https://old.reddit.com/user/teachersecret</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odasbj/gptoss20b_take_the_helm_further_experiments_in/"&gt; &lt;img alt="GPT-OSS-20b TAKE THE HELM! Further experiments in autopilot." src="https://external-preview.redd.it/ZspUdJpzHTOkiEa4knHQwTZYSVuilpI-6KVKwszZkLU.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5807a68162444fb98dc8bc5d6f30eaf44af008c3" title="GPT-OSS-20b TAKE THE HELM! Further experiments in autopilot." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/Deveraux-Parker/GPT-OSS-20b-TAKE-THE-WHEEL"&gt;Github...&lt;/a&gt;&lt;/p&gt; &lt;p&gt;After fiddling around the other day I did a little more messing with gpt-oss-20b and prompting to get it to be a bit more reliable at flying/shooting/controlling the spaceship.&lt;/p&gt; &lt;p&gt;The basic idea is that the system calculates bad and good control choices and feeds the AI a list of options with pre-filled &amp;quot;thinking&amp;quot; on the choices that encourage it to make correct choices. It is still given agency and does deviate from perfect flight from time to time (and will eventually crash as you see here).&lt;/p&gt; &lt;p&gt;To allow fast-paced decision making, this whole stack is running gpt-oss-20b in VLLM on a 4090, and since each generation is only looking to output a single token (that represents a single control input), it allows the system to run in near-realtime. The look-ahead code tries to predict and mitigate the already low latency and the result is an autopilot that is actually reasonably good at flying the ship.&lt;/p&gt; &lt;p&gt;I went ahead and collapsed everything into a single HTML file if you feel like messing with it, and tossed it at the github link above. You'll need an openAI spec API to use it with gpt-oss-20b running on port 8005 (or have to edit the file appropriately to match your own system).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/teachersecret"&gt; /u/teachersecret &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=Yo7GWnGtpoc"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odasbj/gptoss20b_take_the_helm_further_experiments_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1odasbj/gptoss20b_take_the_helm_further_experiments_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-22T15:04:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1odg1wm</id>
    <title>Introducing ExecuTorch 1.0</title>
    <updated>2025-10-22T18:17:14+00:00</updated>
    <author>
      <name>/u/dayanruben</name>
      <uri>https://old.reddit.com/user/dayanruben</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dayanruben"&gt; /u/dayanruben &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://pytorch.org/blog/introducing-executorch-1-0/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odg1wm/introducing_executorch_10/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1odg1wm/introducing_executorch_10/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-22T18:17:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1odf4ei</id>
    <title>Devs, what are your experiences with Qwen3-coder-30b?</title>
    <updated>2025-10-22T17:43:11+00:00</updated>
    <author>
      <name>/u/AzRedx</name>
      <uri>https://old.reddit.com/user/AzRedx</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From code completion, method refactoring, to generating a full MVP project, how well does Qwen3-coder-30b perform?&lt;/p&gt; &lt;p&gt;I have a desktop with 32GB DDR5 RAM and I'm planning to buy an RTX 50 series with at least 16GB of VRAM. Can it handle the quantized version of this model well?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AzRedx"&gt; /u/AzRedx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odf4ei/devs_what_are_your_experiences_with_qwen3coder30b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odf4ei/devs_what_are_your_experiences_with_qwen3coder30b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1odf4ei/devs_what_are_your_experiences_with_qwen3coder30b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-22T17:43:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1od59hx</id>
    <title>Qwen3-VL-32B-Instruct GGUF with unofficial llama.cpp release to run it (Pre-release build)</title>
    <updated>2025-10-22T11:08:02+00:00</updated>
    <author>
      <name>/u/Main-Wolverine-1042</name>
      <uri>https://old.reddit.com/user/Main-Wolverine-1042</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1od59hx/qwen3vl32binstruct_gguf_with_unofficial_llamacpp/"&gt; &lt;img alt="Qwen3-VL-32B-Instruct GGUF with unofficial llama.cpp release to run it (Pre-release build)" src="https://external-preview.redd.it/PKrFTdIjOXXO8Una8BkhYkAJCuw5_mrHyizXt_acZpM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=11da2e1a8abf95953a3114c228cd807881642de2" title="Qwen3-VL-32B-Instruct GGUF with unofficial llama.cpp release to run it (Pre-release build)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/c0glmg5dzmwf1.jpg?width=2048&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=a63b57f9aded62db68cf7c75a9f0108f4131e1b4"&gt;https://preview.redd.it/c0glmg5dzmwf1.jpg?width=2048&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=a63b57f9aded62db68cf7c75a9f0108f4131e1b4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/yairpatch/llama.cpp"&gt;https://github.com/yairpatch/llama.cpp&lt;/a&gt; - Clone this repository and build it.&lt;/p&gt; &lt;p&gt;Or use this prebuilt release - &lt;a href="https://github.com/yairpatch/llama.cpp/releases"&gt;https://github.com/yairpatch/llama.cpp/releases&lt;/a&gt;&lt;/p&gt; &lt;p&gt;32B Model page - &lt;a href="https://huggingface.co/yairpatch/Qwen3-VL-32B-Instruct-GGUF"&gt;https://huggingface.co/yairpatch/Qwen3-VL-32B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;4B Model page - &lt;a href="https://huggingface.co/yairzar/Qwen3-VL-4B-Instruct-GGUF"&gt;https://huggingface.co/yairzar/Qwen3-VL-4B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Uploading in progress of more QWEN3VL variants.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Main-Wolverine-1042"&gt; /u/Main-Wolverine-1042 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1od59hx/qwen3vl32binstruct_gguf_with_unofficial_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1od59hx/qwen3vl32binstruct_gguf_with_unofficial_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1od59hx/qwen3vl32binstruct_gguf_with_unofficial_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-22T11:08:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1ocx27p</id>
    <title>A quickly put together a GUI for the DeepSeek-OCR model that makes it a bit easier to use</title>
    <updated>2025-10-22T03:01:37+00:00</updated>
    <author>
      <name>/u/SmashShock</name>
      <uri>https://old.reddit.com/user/SmashShock</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ocx27p/a_quickly_put_together_a_gui_for_the_deepseekocr/"&gt; &lt;img alt="A quickly put together a GUI for the DeepSeek-OCR model that makes it a bit easier to use" src="https://external-preview.redd.it/wAx0-554iQTzQwf91BRGvJShhLon54aeVFhskNqXubo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9b92062881d606dc5d0bb4a9e9814b152550e5f6" title="A quickly put together a GUI for the DeepSeek-OCR model that makes it a bit easier to use" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;EDIT: this should now work with newer Nvidia cards. Please try the setup instructions again (with a fresh zip) if it failed for you previously.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;I put together a GUI for DeepSeek's new OCR model. The model seems quite good at document understanding and structured text extraction so I figured it deserved the start of a proper interface.&lt;/p&gt; &lt;p&gt;The various OCR types available correspond in-order to the first 5 entries in &lt;a href="https://github.com/deepseek-ai/DeepSeek-OCR/blob/8cf003d38821fa1b19c73da3bd1b0dc262ea8136/README.md#prompts-examples"&gt;this list&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Flask backend manages the model, Electron frontend for the UI. The model downloads automatically from HuggingFace on first load, about 6.7 GB.&lt;/p&gt; &lt;p&gt;Runs on Windows, with untested support for Linux. Currently requires an Nvidia card. If you'd like to help test it out or fix issues on Linux or other platforms, or you would like to contribute in any other way, please feel free to make a PR!&lt;/p&gt; &lt;p&gt;Download and repo:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ihatecsv/deepseek-ocr-client"&gt;https://github.com/ihatecsv/deepseek-ocr-client&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SmashShock"&gt; /u/SmashShock &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/klnlh8omskwf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ocx27p/a_quickly_put_together_a_gui_for_the_deepseekocr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ocx27p/a_quickly_put_together_a_gui_for_the_deepseekocr/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-22T03:01:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1odbco8</id>
    <title>LMStudio - Now has GLM 4.6 Support (CUDA)</title>
    <updated>2025-10-22T15:26:02+00:00</updated>
    <author>
      <name>/u/YouAreRight007</name>
      <uri>https://old.reddit.com/user/YouAreRight007</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, just so you know, LMStudio seems to now have GLM 4.6 support. Yay.&lt;/p&gt; &lt;p&gt;I'm getting 2.99 tokens a second when generating 3000 tokens using 1 3090 and PC RAM.&lt;/p&gt; &lt;p&gt;Model: Unsloth GLM 4.6 UD - Q3_K_XL (147.22GB)&lt;/p&gt; &lt;p&gt;Hardware setup: single 3090 + 14700K with 192GB RAM DDR5333. (14700K limited to 250Watts)&lt;/p&gt; &lt;p&gt;NOTE: Getting a buffer related error when trying to offload layers onto 2x 3090s.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/YouAreRight007"&gt; /u/YouAreRight007 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odbco8/lmstudio_now_has_glm_46_support_cuda/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odbco8/lmstudio_now_has_glm_46_support_cuda/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1odbco8/lmstudio_now_has_glm_46_support_cuda/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-22T15:26:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1odh7ns</id>
    <title>First impressions and thoughts on the GTR9 Pro (Beelink's 395)</title>
    <updated>2025-10-22T19:00:20+00:00</updated>
    <author>
      <name>/u/kmouratidis</name>
      <uri>https://old.reddit.com/user/kmouratidis</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odh7ns/first_impressions_and_thoughts_on_the_gtr9_pro/"&gt; &lt;img alt="First impressions and thoughts on the GTR9 Pro (Beelink's 395)" src="https://external-preview.redd.it/OAXSl8SY6T3JK9MGQyKxkoYbqZ71HQRYXLeB8CV0NXg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0f53460a90493497883ab4cacbbb58e2acb464c4" title="First impressions and thoughts on the GTR9 Pro (Beelink's 395)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;tl;dr: Good and bad, some &amp;quot;benchmarks&amp;quot; and details &lt;a href="https://gist.github.com/KMouratidis/88456bea439ea8d38f452bb6df289b58"&gt;here&lt;/a&gt;. Not sure I'd recommend it. Not yet.&lt;/p&gt; &lt;p&gt;Hey y'all! Just like many others I wanted to try the 395, but since I mostly wanted it as a server first (and LLM runner third), I wanted one with 10 Gbps networking. The MS-S1 hadn't come out yet, so I went with the &lt;a href="https://www.bee-link.com/products/beelink-gtr9-pro-amd-ryzen-ai-max-395?variant=47842426224882"&gt;Beelink GTR9 Pro AMD Ryzen™ AI Max+ 395&lt;/a&gt;, and ~25 days later it's here.&lt;/p&gt; &lt;p&gt;I tried the preinstalled Windows, which functioned for a bit, quickly devolved into a mess that made me want to return it. Thankfully, I wanted it as a server, which means I'll be running Linux, but I had to test it. Plenty of crashes under load, the Intel network card not working, and other weirdness. Turns out there are plenty of known issues that may be hardware or driver related, plenty of posts and speculation in &lt;a href="/r/BeelinkOfficial"&gt;r/BeelinkOfficial&lt;/a&gt; and it has been going for a couple weeks it seems, and may also affect Linux, but oh well, time to move on.&lt;/p&gt; &lt;p&gt;People suggest you use Fedora or Debian Sid, or anything with a recent kernel, and that's probably good advice for most people, but I ain't running Fedora for my server. I used a heavily configured DietPi (so basically Debian) instead, for no other reason than consistency with the rest of my (actually mini*) servers. Surely the driver situation can't be that bad, right? Actually yes, it's perfectly fine to run Debian and I haven't had an issue yet, although it's early, let's see if it reach even 10% the uptime my TrueNAS server has. After troubleshooting a few issues, installing the (hopefully) correct drivers, and building llama.cpp (lemonade and vLLM will have to wait until the weekend), I quickly tested a bunch of models, and the results I'm getting seem to roughly align with what others are getting (&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1mqtnz7/comment/n8uhbp3/"&gt;1&lt;/a&gt;, &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1mqtnz7/comment/n8wnxzc/"&gt;2&lt;/a&gt;, &lt;a href="https://forum.level1techs.com/t/strix-halo-ryzen-ai-max-395-llm-benchmark-results/233796"&gt;3&lt;/a&gt;, &lt;a href="https://github.com/lhl/strix-halo-testing/tree/main/llm-bench/gpt-oss-120b-F16"&gt;4&lt;/a&gt;). I have documented everything in the &lt;a href="https://gist.github.com/KMouratidis/88456bea439ea8d38f452bb6df289b58"&gt;gist&lt;/a&gt; (I think!).&lt;/p&gt; &lt;p&gt;Out of the box, the Beelink runs with 96GB allocated as VRAM and can consume up to 170W without me messing with BIOS or Linux settings. In short, the results are exactly as you would expect:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GPT-OSS-120B is probably the best model to run&lt;/li&gt; &lt;li&gt;Flash Attention helps, but not always by a lot&lt;/li&gt; &lt;li&gt;Performance mode didn't do a thing and maybe was worse, graphics overclocking &lt;em&gt;seems&lt;/em&gt; to help a bit with prefill/pp/input, but not a low&lt;/li&gt; &lt;li&gt;ECO still consumes 100W during inference, &lt;em&gt;but the performance hit can be as little ~15% for ~45% less max power&lt;/em&gt;, which is kinda insane but well-known by now that max power only gives marginal improvements&lt;/li&gt; &lt;li&gt;You must be dense if you expect to run dense models&lt;/li&gt; &lt;/ul&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Size&lt;/th&gt; &lt;th align="left"&gt;Params&lt;/th&gt; &lt;th align="left"&gt;Backend&lt;/th&gt; &lt;th align="left"&gt;Test&lt;/th&gt; &lt;th align="left"&gt;Tokens/s (FA 0)&lt;/th&gt; &lt;th align="left"&gt;Tokens/s (FA 1)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;GLM-4.5-Air (Q4_K_XL)&lt;/td&gt; &lt;td align="left"&gt;68.01 GiB&lt;/td&gt; &lt;td align="left"&gt;110.47B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;142.90 ± 1.39&lt;/td&gt; &lt;td align="left"&gt;152.65 ± 1.49&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;20.31 ± 0.07&lt;/td&gt; &lt;td align="left"&gt;20.83 ± 0.12&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-30B (Q4_K_XL)&lt;/td&gt; &lt;td align="left"&gt;16.49 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;496.63 ± 11.29&lt;/td&gt; &lt;td align="left"&gt;503.25 ± 6.42&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;63.26 ± 0.28&lt;/td&gt; &lt;td align="left"&gt;64.43 ± 0.71&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPT-OSS-120B (F16)&lt;/td&gt; &lt;td align="left"&gt;60.87 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;636.25 ± 5.49&lt;/td&gt; &lt;td align="left"&gt;732.70 ± 5.99&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;34.44 ± 0.01&lt;/td&gt; &lt;td align="left"&gt;34.60 ± 0.07&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Happy to run tests / benchmarks or answer questions, but some stuff may need to wait for the weekend.&lt;/p&gt; &lt;p&gt;----------&lt;/p&gt; &lt;p&gt;* Bonus: I sent this photo of the Beelink with my old &lt;a href="https://refurbished.minisforum.com/products/minisforum-z83-f-refurbished"&gt;Minisforum Z83-F&lt;/a&gt; to someone, joking about how mini PCs looked in 2015 vs in 2025. She thought the Minisforum was the one from 2025.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6hlbhs9lmpwf1.jpg?width=2304&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=8ab4f30de9ecd33370f73213ebd0d121a055c041"&gt;Beelink GTR9 Pro (2025) dwarfs it's little bro, the Minisforum Z83-F (2015)&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kmouratidis"&gt; /u/kmouratidis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odh7ns/first_impressions_and_thoughts_on_the_gtr9_pro/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odh7ns/first_impressions_and_thoughts_on_the_gtr9_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1odh7ns/first_impressions_and_thoughts_on_the_gtr9_pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-22T19:00:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1od35w1</id>
    <title>New model from Tencent, HunyuanWorld-Mirror</title>
    <updated>2025-10-22T09:01:59+00:00</updated>
    <author>
      <name>/u/edward-dev</name>
      <uri>https://old.reddit.com/user/edward-dev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1od35w1/new_model_from_tencent_hunyuanworldmirror/"&gt; &lt;img alt="New model from Tencent, HunyuanWorld-Mirror" src="https://external-preview.redd.it/4mzrgM79cCe_QE-8XZM35Vw90_ckM3tR76mq1apuGAU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1d7d696f5cccceda23331d47643538ea7a5dce0c" title="New model from Tencent, HunyuanWorld-Mirror" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;HunyuanWorld-Mirror is a versatile feed-forward model for comprehensive 3D geometric prediction. It integrates diverse geometric priors (camera poses, calibrated intrinsics, depth maps) and simultaneously generates various 3D representations (point clouds, multi-view depths, camera parameters, surface normals, 3D Gaussians) in a single forward pass.&lt;/p&gt; &lt;p&gt;Really interesting for folks into 3D...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/edward-dev"&gt; /u/edward-dev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/tencent/HunyuanWorld-Mirror"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1od35w1/new_model_from_tencent_hunyuanworldmirror/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1od35w1/new_model_from_tencent_hunyuanworldmirror/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-22T09:01:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1odavba</id>
    <title>Running whisper-large-v3-turbo (OpenAI) Exclusively on AMD Ryzen™ AI NPU</title>
    <updated>2025-10-22T15:07:57+00:00</updated>
    <author>
      <name>/u/BandEnvironmental834</name>
      <uri>https://old.reddit.com/user/BandEnvironmental834</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odavba/running_whisperlargev3turbo_openai_exclusively_on/"&gt; &lt;img alt="Running whisper-large-v3-turbo (OpenAI) Exclusively on AMD Ryzen™ AI NPU" src="https://external-preview.redd.it/y7g9bbQ0RPFXfYX-nbtW899i8fH3DJk9OyQ8tRM7MG4.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b115fd1754530c90df1effac911d91b79c4dfb2b" title="Running whisper-large-v3-turbo (OpenAI) Exclusively on AMD Ryzen™ AI NPU" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;About the Demo&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Workflow:&lt;/strong&gt; &lt;code&gt;whisper-large-v3-turbo&lt;/code&gt; transcribes audio; &lt;code&gt;gpt-oss:20b&lt;/code&gt; generates the summary. Both models are &lt;strong&gt;pre-loaded&lt;/strong&gt; on the NPU.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Settings:&lt;/strong&gt; &lt;code&gt;gpt-oss:20b&lt;/code&gt; reasoning effort = &lt;strong&gt;High&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Test system:&lt;/strong&gt; ASRock 4X4 BOX-AI340 Mini PC (Kraken Point), 96 GB RAM.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Software:&lt;/strong&gt; FastFlowLM (CLI mode).&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;About FLM&lt;/h1&gt; &lt;p&gt;We’re a small team building &lt;strong&gt;FastFlowLM (FLM)&lt;/strong&gt; — a fast runtime for running &lt;strong&gt;Whisper (Audio)&lt;/strong&gt;, &lt;strong&gt;GPT-OSS (first MoE on NPUs), Gemma3 (vision), Medgemma,&lt;/strong&gt; &lt;strong&gt;Qwen3,&lt;/strong&gt; &lt;strong&gt;DeepSeek-R1&lt;/strong&gt;, &lt;strong&gt;LLaMA3.x,&lt;/strong&gt; and others &lt;strong&gt;entirely on the AMD Ryzen AI NPU&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Think &lt;strong&gt;Ollama (maybe llama.cpp since we have our own backend?)&lt;/strong&gt;, but deeply optimized for AMD NPUs — with both &lt;strong&gt;CLI&lt;/strong&gt; and &lt;strong&gt;Server Mode (OpenAI-compatible)&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;✨ &lt;strong&gt;From Idle Silicon to Instant Power — FastFlowLM (FLM) Makes Ryzen™ AI Shine.&lt;/strong&gt;&lt;/p&gt; &lt;h1&gt;Key Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;No GPU fallback&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Faster and over 10× more power efficient.&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Supports context lengths up to 256k tokens (qwen3:4b-2507).&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ultra-Lightweight (16 MB). Installs within 20 seconds.&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Try It Out&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/FastFlowLM/FastFlowLM"&gt;github.com/FastFlowLM/FastFlowLM&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Live Demo → Remote machine access on the repo page&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;YouTube Demos:&lt;/strong&gt; &lt;a href="https://www.youtube.com/@FastFlowLM-YT/playlists"&gt;FastFlowLM - YouTube&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We’re iterating fast and would &lt;strong&gt;love your feedback, critiques, and ideas&lt;/strong&gt;🙏&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BandEnvironmental834"&gt; /u/BandEnvironmental834 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/0t8ijUPg4A0?si=539G5mrICJNOwe6Z"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odavba/running_whisperlargev3turbo_openai_exclusively_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1odavba/running_whisperlargev3turbo_openai_exclusively_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-22T15:07:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1od7hyu</id>
    <title>M5 MacBook Pro: Up to ~45% PP Improvement. ~25% TG (Ollama Tested)</title>
    <updated>2025-10-22T12:55:20+00:00</updated>
    <author>
      <name>/u/Noble00_</name>
      <uri>https://old.reddit.com/user/Noble00_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1od7hyu/m5_macbook_pro_up_to_45_pp_improvement_25_tg/"&gt; &lt;img alt="M5 MacBook Pro: Up to ~45% PP Improvement. ~25% TG (Ollama Tested)" src="https://preview.redd.it/2r0ue3k6unwf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4b506f746bde959cb1cf422094c3babaa4c4113e" title="M5 MacBook Pro: Up to ~45% PP Improvement. ~25% TG (Ollama Tested)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;[&lt;a href="https://www.youtube.com/watch?v=BKQggt9blGo"&gt;Geekerwan&lt;/a&gt;]&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Noble00_"&gt; /u/Noble00_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2r0ue3k6unwf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1od7hyu/m5_macbook_pro_up_to_45_pp_improvement_25_tg/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1od7hyu/m5_macbook_pro_up_to_45_pp_improvement_25_tg/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-22T12:55:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1ocrocy</id>
    <title>DeepSeek-OCR - Lives up to the hype</title>
    <updated>2025-10-21T22:51:20+00:00</updated>
    <author>
      <name>/u/Bohdanowicz</name>
      <uri>https://old.reddit.com/user/Bohdanowicz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ocrocy/deepseekocr_lives_up_to_the_hype/"&gt; &lt;img alt="DeepSeek-OCR - Lives up to the hype" src="https://external-preview.redd.it/DGhh7DsESjWeCrgHI7E8he7jk8ACLXaitOluBNwi630.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=86af002276d4a89cdea0ff0abd7fac0d455b8d9a" title="DeepSeek-OCR - Lives up to the hype" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I decided to try this out. Dockerized the model with fastapi in a wsl environment. Gave it 10000 pdfs to convert to markdown.&lt;/p&gt; &lt;p&gt;Hardware - 1 x A6000 ADA on a Ryzen 1700 /w 32gb ram&lt;/p&gt; &lt;p&gt;Processed prompts: 100%|██████████| 1/1 [00:00&amp;lt;00:00, 3.29it/s, est. speed input: 3000.81 toks/s, output: 220.20 toks/s]&lt;/p&gt; &lt;p&gt;I'm averaging less than 1 second per page.&lt;/p&gt; &lt;p&gt;This is the real deal.&lt;/p&gt; &lt;p&gt;EDIT: Decided to share the docker build if anyone is interested. It wraps the model up nicely so you can try it out directly with the api. it uses the vllm-openapi 0.8.5 public docker image.&lt;/p&gt; &lt;p&gt;Also included a pdf to markdown utility which will process anything in the /data subfolder to .md just by running it since there is an issue using the batch processor directly via the api.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/se9r9dsnyjwf1.png?width=1458&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fcd8118c3e1c167cc13d159579527a802e55fd84"&gt;https://preview.redd.it/se9r9dsnyjwf1.png?width=1458&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fcd8118c3e1c167cc13d159579527a802e55fd84&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Bogdanovich77/DeekSeek-OCR---Dockerized-API"&gt;https://github.com/Bogdanovich77/DeekSeek-OCR---Dockerized-API&lt;/a&gt;&lt;/p&gt; &lt;p&gt;EDIT: Updated API to allow custom prompts. Also implemented the deepseek post processing in the pdf_to_*_enhanced.py prompts. Now properly extracts images.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bohdanowicz"&gt; /u/Bohdanowicz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ocrocy/deepseekocr_lives_up_to_the_hype/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ocrocy/deepseekocr_lives_up_to_the_hype/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ocrocy/deepseekocr_lives_up_to_the_hype/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T22:51:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1odddyg</id>
    <title>Free GPU memory during local LLM inference without KV cache hogging VRAM</title>
    <updated>2025-10-22T16:40:37+00:00</updated>
    <author>
      <name>/u/ivaniumr</name>
      <uri>https://old.reddit.com/user/ivaniumr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odddyg/free_gpu_memory_during_local_llm_inference/"&gt; &lt;img alt="Free GPU memory during local LLM inference without KV cache hogging VRAM" src="https://external-preview.redd.it/Qexkfi7FQ3mBNXUsI349OiBIZqFoa4py3iuRmtBXCE0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3a119b694af4a1e62532c3a6e7da37245e136c66" title="Free GPU memory during local LLM inference without KV cache hogging VRAM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We are building &lt;a href="https://github.com/ovg-project/kvcached"&gt;kvcached&lt;/a&gt;, a library that lets local LLM inference engines such as &lt;strong&gt;SGLang&lt;/strong&gt; and &lt;strong&gt;vLLM&lt;/strong&gt; free idle KV cache memory instead of occupying the entire GPU. This allows you to run a model locally without using all available VRAM, so other applications can still run or even share the GPU.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;✅ Works out of the box with SGLang and vLLM&lt;/li&gt; &lt;li&gt;🔧 Support for Ollama and LM Studio is in progress&lt;/li&gt; &lt;li&gt;🧩 No changes to your model or prompts required&lt;/li&gt; &lt;li&gt;🚀 Install with pip and it runs out of the box&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Our code is open source: &lt;a href="https://github.com/ovg-project/kvcached"&gt;https://github.com/ovg-project/kvcached&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Deep dive blog for those interested in the techniques behind it: &lt;a href="https://yifanqiao.notion.site/Solve-the-GPU-Cost-Crisis-with-kvcached-289da9d1f4d68034b17bf2774201b141"&gt;https://yifanqiao.notion.site/Solve-the-GPU-Cost-Crisis-with-kvcached-289da9d1f4d68034b17bf2774201b141&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We would love feedback from the local LLM community. If you want to run multiple models on one GPU, combine LLMs with other GPU applications, or simply reduce memory usage, feel free to try it out and ask questions. Happy to discuss and improve together 🙌&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/co5zu9swyowf1.jpg?width=3217&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=f6fd3f6976033f9a3c4a23fb1743fa7f5dd0a59f"&gt;https://preview.redd.it/co5zu9swyowf1.jpg?width=3217&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=f6fd3f6976033f9a3c4a23fb1743fa7f5dd0a59f&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ivaniumr"&gt; /u/ivaniumr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odddyg/free_gpu_memory_during_local_llm_inference/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odddyg/free_gpu_memory_during_local_llm_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1odddyg/free_gpu_memory_during_local_llm_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-22T16:40:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1od0jw1</id>
    <title>[R] We figured out how to predict 32B model reasoning performance with a 1B model. 100x cheaper. Paper inside.</title>
    <updated>2025-10-22T06:13:40+00:00</updated>
    <author>
      <name>/u/jshin49</name>
      <uri>https://old.reddit.com/user/jshin49</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Remember our &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nedq3i/we_just_released_the_worlds_first_70b/?sort=new"&gt;70B intermediate checkpoints release&lt;/a&gt;? We said we wanted to enable real research on training dynamics. Well, here's exactly the kind of work we hoped would happen.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;rBridge:&lt;/strong&gt; Use 1B models to predict whether your 32B model will be good at reasoning. Actually works.&lt;/p&gt; &lt;p&gt;The problem: Small models can't do reasoning (emergence happens at 7B+), so how do you know if your training recipe works without spending $200k?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Our solution:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Align evaluation with both pre-training objective AND target task&lt;/li&gt; &lt;li&gt;Use frontier model reasoning traces as gold labels&lt;/li&gt; &lt;li&gt;Weight tokens by task importance automatically&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Results:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;100x compute reduction vs baselines&lt;/li&gt; &lt;li&gt;Accurately predict which datasets are worth training on&lt;/li&gt; &lt;li&gt;R² = 0.826 predicting 32B performance from 1B proxy&lt;/li&gt; &lt;li&gt;Works zero-shot on new datasets&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Tested on: GSM8K, MATH500, ARC-C, MMLU Pro, CQA, HumanEval&lt;/p&gt; &lt;p&gt;Paper: &lt;a href="https://www.arxiv.org/abs/2509.21013"&gt;https://www.arxiv.org/abs/2509.21013&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is what open research looks like - building on each other's work to make LLM development accessible to everyone, not just companies with infinite compute.&lt;/p&gt; &lt;p&gt;Code coming soon. Apache 2.0 as always.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jshin49"&gt; /u/jshin49 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1od0jw1/r_we_figured_out_how_to_predict_32b_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1od0jw1/r_we_figured_out_how_to_predict_32b_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1od0jw1/r_we_figured_out_how_to_predict_32b_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-22T06:13:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1od5dxw</id>
    <title>I created a corporate-level chat UI with advanced features</title>
    <updated>2025-10-22T11:14:42+00:00</updated>
    <author>
      <name>/u/BlueLemonPixel</name>
      <uri>https://old.reddit.com/user/BlueLemonPixel</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1od5dxw/i_created_a_corporatelevel_chat_ui_with_advanced/"&gt; &lt;img alt="I created a corporate-level chat UI with advanced features" src="https://external-preview.redd.it/dPYWmiXWuS054Q7CG9UBLqSEshctiMSWamOBef2b2Gs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=31fcc76b887198eea07c204089b8d5f4a93bc082" title="I created a corporate-level chat UI with advanced features" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BlueLemonPixel"&gt; /u/BlueLemonPixel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/25jaz4q9cnwf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1od5dxw/i_created_a_corporatelevel_chat_ui_with_advanced/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1od5dxw/i_created_a_corporatelevel_chat_ui_with_advanced/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-22T11:14:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1od4wj4</id>
    <title>2025 Skynet is released in beta version</title>
    <updated>2025-10-22T10:48:00+00:00</updated>
    <author>
      <name>/u/Max-HWN</name>
      <uri>https://old.reddit.com/user/Max-HWN</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1od4wj4/2025_skynet_is_released_in_beta_version/"&gt; &lt;img alt="2025 Skynet is released in beta version" src="https://preview.redd.it/nstd6t1x7nwf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fe7be5c4c7050b73bdeb33c732a3875526215c72" title="2025 Skynet is released in beta version" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, if you are afraid of AI taking over, we still have a lot of time 😂&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Max-HWN"&gt; /u/Max-HWN &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nstd6t1x7nwf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1od4wj4/2025_skynet_is_released_in_beta_version/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1od4wj4/2025_skynet_is_released_in_beta_version/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-22T10:48:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1odf249</id>
    <title>Ling-1T is very impressive – why are there no independent benchmarks?</title>
    <updated>2025-10-22T17:40:56+00:00</updated>
    <author>
      <name>/u/Snail_Inference</name>
      <uri>https://old.reddit.com/user/Snail_Inference</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Today, I finally had the chance to run some tests with ubergarm’s GGUF version of Ling-1T: &lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/ubergarm/Ling-1T-GGUF"&gt;Hugging Face – Ling-1T-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I focused on mathematical and reasoning tasks, and I have to say: I’m genuinely impressed. I only used IQ2_K-quants and Ling-1T solved every problem I threw at it, while keeping costs low thanks to its minimal token usage.&lt;/p&gt; &lt;p&gt;But: I can’t find &lt;strong&gt;any&lt;/strong&gt; independent benchmarks. No results on Artificial Analysis, LiveBench, Aider’s LLM Leaderboard, EQ-Bench… nothing beyond anecdotal impressions. &lt;/p&gt; &lt;p&gt;What are your thoughts? Any ideas why this model seems to fly under the radar?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Snail_Inference"&gt; /u/Snail_Inference &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odf249/ling1t_is_very_impressive_why_are_there_no/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odf249/ling1t_is_very_impressive_why_are_there_no/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1odf249/ling1t_is_very_impressive_why_are_there_no/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-22T17:40:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1odg6pz</id>
    <title>olmoOCR 2 released, big quality improvements, fully open training data and code</title>
    <updated>2025-10-22T18:22:09+00:00</updated>
    <author>
      <name>/u/whistling_frank</name>
      <uri>https://old.reddit.com/user/whistling_frank</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odg6pz/olmoocr_2_released_big_quality_improvements_fully/"&gt; &lt;img alt="olmoOCR 2 released, big quality improvements, fully open training data and code" src="https://external-preview.redd.it/jMHnzDUDDsA1xIrP_vxD1Z6TLTLk5mgpCRd-v7PwCn4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=60397e0ca48d71f38e4f50d1bb3a4a5210618f9a" title="olmoOCR 2 released, big quality improvements, fully open training data and code" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Given the interest in OCR models recently, Ai2's release today should be on your radar. The weights, training data, and training code are all open, and you can try it for free here:&lt;br /&gt; &lt;a href="https://olmocr.allenai.org/"&gt;https://olmocr.allenai.org/&lt;/a&gt; &lt;/p&gt; &lt;p&gt;📚 Blog: &lt;a href="https://allenai.org/blog/olmocr-2"&gt;https://allenai.org/blog/olmocr-2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;💻 Model: &lt;a href="https://huggingface.co/allenai/olmOCR-2-7B-1025-FP8"&gt;https://huggingface.co/allenai/olmOCR-2-7B-1025-FP8&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/whistling_frank"&gt; /u/whistling_frank &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://allenai.org/blog/olmocr-2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odg6pz/olmoocr_2_released_big_quality_improvements_fully/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1odg6pz/olmoocr_2_released_big_quality_improvements_fully/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-22T18:22:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1odbwjj</id>
    <title>LFM2-VL 3B released today</title>
    <updated>2025-10-22T15:46:03+00:00</updated>
    <author>
      <name>/u/cruncherv</name>
      <uri>https://old.reddit.com/user/cruncherv</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;New &lt;strong&gt;LFM2-VL 3B&lt;/strong&gt; version released by LiquidAI today.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.liquid.ai/blog/lfm2-vl-3b-a-new-efficient-vision-language-for-the-edge"&gt;Blog post&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/LiquidAI/LFM2-VL-3B"&gt;HuggingFace &lt;/a&gt;page&lt;/li&gt; &lt;li&gt;Available quant: &lt;a href="https://huggingface.co/LiquidAI/LFM2-VL-3B-GGUF"&gt;GGUF&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;&lt;strong&gt;Model&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;Average&lt;/th&gt; &lt;th align="left"&gt;MMStar&lt;/th&gt; &lt;th align="left"&gt;MMMU (val)&lt;/th&gt; &lt;th align="left"&gt;MathVista&lt;/th&gt; &lt;th align="left"&gt;BLINK&lt;/th&gt; &lt;th align="left"&gt;InfoVQA (val)&lt;/th&gt; &lt;th align="left"&gt;MMBench (dev en)&lt;/th&gt; &lt;th align="left"&gt;OCRBench&lt;/th&gt; &lt;th align="left"&gt;POPE&lt;/th&gt; &lt;th align="left"&gt;RealWorldQA&lt;/th&gt; &lt;th align="left"&gt;MME&lt;/th&gt; &lt;th align="left"&gt;MM-IFEval&lt;/th&gt; &lt;th align="left"&gt;SEEDBench&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;InternVL3_5-2B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;66.63&lt;/td&gt; &lt;td align="left"&gt;57.67&lt;/td&gt; &lt;td align="left"&gt;51.78&lt;/td&gt; &lt;td align="left"&gt;61.6&lt;/td&gt; &lt;td align="left"&gt;50.97&lt;/td&gt; &lt;td align="left"&gt;69.29&lt;/td&gt; &lt;td align="left"&gt;78.18&lt;/td&gt; &lt;td align="left"&gt;834&lt;/td&gt; &lt;td align="left"&gt;87.17&lt;/td&gt; &lt;td align="left"&gt;60.78&lt;/td&gt; &lt;td align="left"&gt;2,128.83&lt;/td&gt; &lt;td align="left"&gt;47.31&lt;/td&gt; &lt;td align="left"&gt;75.41&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Qwen2.5-VL-3B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;66.61&lt;/td&gt; &lt;td align="left"&gt;56.13&lt;/td&gt; &lt;td align="left"&gt;51.67&lt;/td&gt; &lt;td align="left"&gt;62.5&lt;/td&gt; &lt;td align="left"&gt;48.97&lt;/td&gt; &lt;td align="left"&gt;76.12&lt;/td&gt; &lt;td align="left"&gt;80.41&lt;/td&gt; &lt;td align="left"&gt;824&lt;/td&gt; &lt;td align="left"&gt;86.17&lt;/td&gt; &lt;td align="left"&gt;65.23&lt;/td&gt; &lt;td align="left"&gt;2,163.29&lt;/td&gt; &lt;td align="left"&gt;38.62&lt;/td&gt; &lt;td align="left"&gt;73.88&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;InternVL3-2B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;66.46&lt;/td&gt; &lt;td align="left"&gt;61.1&lt;/td&gt; &lt;td align="left"&gt;48.7&lt;/td&gt; &lt;td align="left"&gt;57.6&lt;/td&gt; &lt;td align="left"&gt;53.1&lt;/td&gt; &lt;td align="left"&gt;66.1&lt;/td&gt; &lt;td align="left"&gt;81.1&lt;/td&gt; &lt;td align="left"&gt;831&lt;/td&gt; &lt;td align="left"&gt;90.1&lt;/td&gt; &lt;td align="left"&gt;65.1&lt;/td&gt; &lt;td align="left"&gt;2,186.40&lt;/td&gt; &lt;td align="left"&gt;38.49&lt;/td&gt; &lt;td align="left"&gt;74.95&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;SmolVLM2-2.2B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;54.85&lt;/td&gt; &lt;td align="left"&gt;46&lt;/td&gt; &lt;td align="left"&gt;41.6&lt;/td&gt; &lt;td align="left"&gt;51.5&lt;/td&gt; &lt;td align="left"&gt;42.3&lt;/td&gt; &lt;td align="left"&gt;37.75&lt;/td&gt; &lt;td align="left"&gt;69.24&lt;/td&gt; &lt;td align="left"&gt;725&lt;/td&gt; &lt;td align="left"&gt;85.1&lt;/td&gt; &lt;td align="left"&gt;57.5&lt;/td&gt; &lt;td align="left"&gt;1792.5&lt;/td&gt; &lt;td align="left"&gt;19.42&lt;/td&gt; &lt;td align="left"&gt;71.3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;LFM2-VL-3B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;67.31&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;57.73&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;45.33&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;62.2&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;51.03&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;67.37&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;79.81&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;822&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;89.01&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;71.37&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;2,050.90&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;51.83&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;76.55&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Table from: &lt;a href="http://liquid.ai/blog/lfm2-vl-3b-a-new-efficient-vision-language-for-the-edge"&gt;liquid.ai/blog/lfm2-vl-3b-a-new-efficient-vision-language-for-the-edge&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cruncherv"&gt; /u/cruncherv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odbwjj/lfm2vl_3b_released_today/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odbwjj/lfm2vl_3b_released_today/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1odbwjj/lfm2vl_3b_released_today/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-22T15:46:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1od1hw4</id>
    <title>hey Z.ai, two weeks was yesterday</title>
    <updated>2025-10-22T07:13:00+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1od1hw4/hey_zai_two_weeks_was_yesterday/"&gt; &lt;img alt="hey Z.ai, two weeks was yesterday" src="https://preview.redd.it/lg6u60lj5mwf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fb37b472c5a42bbe348ff5652a5ce811e269f95d" title="hey Z.ai, two weeks was yesterday" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lg6u60lj5mwf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1od1hw4/hey_zai_two_weeks_was_yesterday/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1od1hw4/hey_zai_two_weeks_was_yesterday/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-22T07:13:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1odi1c0</id>
    <title>Meta lays off 600 employees within AI unit</title>
    <updated>2025-10-22T19:30:58+00:00</updated>
    <author>
      <name>/u/a_slay_nub</name>
      <uri>https://old.reddit.com/user/a_slay_nub</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odi1c0/meta_lays_off_600_employees_within_ai_unit/"&gt; &lt;img alt="Meta lays off 600 employees within AI unit" src="https://external-preview.redd.it/J07EauFcN4nV9LOcRS0eXwdDIcxd3OiFJdlO3Bhl-Rc.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b63bb44a8b15efeaf11cd6f80af319b8caa01688" title="Meta lays off 600 employees within AI unit" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/a_slay_nub"&gt; /u/a_slay_nub &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.cnbc.com/2025/10/22/meta-layoffs-ai.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odi1c0/meta_lays_off_600_employees_within_ai_unit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1odi1c0/meta_lays_off_600_employees_within_ai_unit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-22T19:30:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1od8fz0</id>
    <title>YES! Super 80b for 8gb VRAM - Qwen3-Next-80B-A3B-Instruct-GGUF</title>
    <updated>2025-10-22T13:34:41+00:00</updated>
    <author>
      <name>/u/Mangleus</name>
      <uri>https://old.reddit.com/user/Mangleus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So amazing to be able to run this beast on a 8GB VRAM laptop &lt;a href="https://huggingface.co/lefromage/Qwen3-Next-80B-A3B-Instruct-GGUF"&gt;https://huggingface.co/lefromage/Qwen3-Next-80B-A3B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Note that this is not yet supported by latest llama.cpp so you need to compile the non-official version as shown in the link above. (Do not forget to add GPU support when compiling). &lt;/p&gt; &lt;p&gt;Have fun! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mangleus"&gt; /u/Mangleus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1od8fz0/yes_super_80b_for_8gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1od8fz0/yes_super_80b_for_8gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1od8fz0/yes_super_80b_for_8gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-22T13:34:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1oda8mk</id>
    <title>Qwen team is helping llama.cpp again</title>
    <updated>2025-10-22T14:44:44+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oda8mk/qwen_team_is_helping_llamacpp_again/"&gt; &lt;img alt="Qwen team is helping llama.cpp again" src="https://preview.redd.it/dh1iaky2eowf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=addcf456730d4f5ec04b561980fa9d74dfb18d96" title="Qwen team is helping llama.cpp again" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/dh1iaky2eowf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oda8mk/qwen_team_is_helping_llamacpp_again/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oda8mk/qwen_team_is_helping_llamacpp_again/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-22T14:44:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1obqkpe</id>
    <title>Best Local LLMs - October 2025</title>
    <updated>2025-10-20T19:06:06+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Welcome to the first monthly &amp;quot;Best Local LLMs&amp;quot; post!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Share what your favorite models are right now &lt;strong&gt;and why.&lt;/strong&gt; Given the nature of the beast in evaluating LLMs (untrustworthiness of benchmarks, immature tooling, intrinsic stochasticity), please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Applications&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;General&lt;/li&gt; &lt;li&gt;Agentic/Tool Use&lt;/li&gt; &lt;li&gt;Coding&lt;/li&gt; &lt;li&gt;Creative Writing/RP&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;(look for the top level comments for each Application and please thread your responses under that)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obqkpe/best_local_llms_october_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obqkpe/best_local_llms_october_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1obqkpe/best_local_llms_october_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T19:06:06+00:00</published>
  </entry>
</feed>
