<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-30T17:39:30+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1mcrx23</id>
    <title>PSA: The new Threadripper PROs (9000 WX) are still CCD-Memory Bandwidth bottlenecked</title>
    <updated>2025-07-30T00:10:03+00:00</updated>
    <author>
      <name>/u/henfiber</name>
      <uri>https://old.reddit.com/user/henfiber</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've seen people claim that the new TR PROs can achieve the full 8-channel memory bandwidth even in SKUs with 16-cores. That's not the case.&lt;/p&gt; &lt;p&gt;The issue with the limited CCD bandwidth seems to still be present, and affects the low-number CCD parts. You can only achieve the full 8-channel bandwidth with 64-core+ WX CPUs.&lt;/p&gt; &lt;p&gt;Check the &amp;quot;Latest baselines&amp;quot; section in a processor's page at &lt;a href="http://cpubenchmark.net"&gt;cpubenchmark.net&lt;/a&gt; with links to individual results where the &amp;quot;Memory Threaded&amp;quot; result is listed under &amp;quot;Memory Mark&amp;quot;:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;CPU&lt;/th&gt; &lt;th align="left"&gt;Memory BW&lt;/th&gt; &lt;th align="left"&gt;Reference&lt;/th&gt; &lt;th align="left"&gt;Notes&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.cpubenchmark.net/cpu.php?cpu=AMD+Ryzen+Threadripper+PRO+9955WX&amp;amp;id=6803"&gt;AMD Threadripper PRO 9955WX&lt;/a&gt; (16-cores)&lt;/td&gt; &lt;td align="left"&gt;~115 GB/s&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.passmark.com/baselines/V11/display.php?id=509905130667"&gt;BL5099051 - Jul 20 2025&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;2x CCD&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.cpubenchmark.net/cpu.php?cpu=AMD+Ryzen+Threadripper+PRO+9965WX&amp;amp;id=6804"&gt;AMD Threadripper PRO 9965WX&lt;/a&gt; (24-cores)&lt;/td&gt; &lt;td align="left"&gt;~272 GB/s&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.passmark.com/baselines/V11/display.php?id=279748548819"&gt;BL2797485 - Jul 29 2025&lt;/a&gt; (other baselines start from 250GB/s)&lt;/td&gt; &lt;td align="left"&gt;4x CCDs&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.cpubenchmark.net/cpu.php?cpu=AMD+Ryzen+Threadripper+PRO+9975WX&amp;amp;id=6799"&gt;AMD Threadripper PRO 9975WX&lt;/a&gt; (32-cores)&lt;/td&gt; &lt;td align="left"&gt;~272 GB/s&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.passmark.com/baselines/V11/display.php?id=279782022829"&gt;BL2797820 - Jul 29 2025&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;4x CCDs&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.cpubenchmark.net/cpu.php?cpu=AMD+Ryzen+Threadripper+PRO+9985WX&amp;amp;id=6807"&gt;AMD Threadripper PRO 9985WX&lt;/a&gt; (64-cores)&lt;/td&gt; &lt;td align="left"&gt;~367 GB/s&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.passmark.com/baselines/V11/display.php?id=509913021820"&gt;BL5099130 - Jul 21 2025&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;8x CCDs&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Therefore:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;the 16-core 9955WX has lower mem bw than even a DDR4 EPYC CPU (e.g. &lt;a href="https://www.passmark.com/baselines/V10/display.php?id=226455755507"&gt;7R43 with 191 GB/s&lt;/a&gt;).&lt;/li&gt; &lt;li&gt;the 24-core and 32-core parts have lower mem bw than DDR5 Genoa EPYCs (even some 16-core parts).&lt;/li&gt; &lt;li&gt;the 64-core and 96-core Threadrippers are not CCD-number limited, but still lose to the EPYCs since those have 12 channels (unless you use 7200 MT/s memory).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For comparison, check the excellent related threads by &lt;a href="/u/fairydreaming"&gt;u/fairydreaming&lt;/a&gt; for the previous gen Threadrippers and EPYC Genoa/Turin:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/r/threadripper/comments/1azmkvg/comparing_threadripper_7000_memory_bandwidth_for/"&gt;Comparing Threadripper 7000 memory bandwidth for all models : r/threadripper&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1fcy8x6/memory_bandwidth_values_stream_triad_benchmark/"&gt;Memory bandwidth values (STREAM TRIAD benchmark results) for most Epyc Genoa CPUs (single and dual configurations) : r/LocalLLaMA&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1h3doy8/stream_triad_memory_bandwidth_benchmark_values/"&gt;STREAM TRIAD memory bandwidth benchmark values for Epyc Turin - almost 1 TB/s for a dual CPU system : r/LocalLLaMA&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If someone insists on buying a new TR Pro for their great compute throughput, I would suggest to at least skip the 16-core part.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/henfiber"&gt; /u/henfiber &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mcrx23/psa_the_new_threadripper_pros_9000_wx_are_still/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mcrx23/psa_the_new_threadripper_pros_9000_wx_are_still/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mcrx23/psa_the_new_threadripper_pros_9000_wx_are_still/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-30T00:10:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1mcg4qt</id>
    <title>üöÄ Qwen3-30B-A3B Small Update</title>
    <updated>2025-07-29T16:29:59+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mcg4qt/qwen330ba3b_small_update/"&gt; &lt;img alt="üöÄ Qwen3-30B-A3B Small Update" src="https://preview.redd.it/nd904g7gbuff1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b713bd1bbe154007dd6c0b8474098b47bf58ba4d" title="üöÄ Qwen3-30B-A3B Small Update" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;üöÄ Qwen3-30B-A3B Small Update: Smarter, faster, and local deployment-friendly.&lt;/p&gt; &lt;p&gt;‚ú® Key Enhancements:&lt;/p&gt; &lt;p&gt;‚úÖ Enhanced reasoning, coding, and math skills&lt;/p&gt; &lt;p&gt;‚úÖ Broader multilingual knowledge&lt;/p&gt; &lt;p&gt;‚úÖ Improved long-context understanding (up to 256K tokens)&lt;/p&gt; &lt;p&gt;‚úÖ Better alignment with user intent and open-ended tasks&lt;/p&gt; &lt;p&gt;‚úÖ No more &amp;lt;think&amp;gt; blocks ‚Äî now operating exclusively in non-thinking mode&lt;/p&gt; &lt;p&gt;üîß With 3B activated parameters, it's approaching the performance of GPT-4o and Qwen3-235B-A22B Non-Thinking&lt;/p&gt; &lt;p&gt;Hugging Face: &lt;a href="https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507-FP8"&gt;https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507-FP8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Qwen Chat: &lt;a href="https://chat.qwen.ai/?model=Qwen3-30B-A3B-2507"&gt;https://chat.qwen.ai/?model=Qwen3-30B-A3B-2507&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Model scope: &lt;a href="https://modelscope.cn/models/Qwen/Qwen3-30B-A3B-Instruct-2507/summary"&gt;https://modelscope.cn/models/Qwen/Qwen3-30B-A3B-Instruct-2507/summary&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nd904g7gbuff1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mcg4qt/qwen330ba3b_small_update/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mcg4qt/qwen330ba3b_small_update/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-29T16:29:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1md9nc8</id>
    <title>GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning</title>
    <updated>2025-07-30T15:29:39+00:00</updated>
    <author>
      <name>/u/Thrumpwart</name>
      <uri>https://old.reddit.com/user/Thrumpwart</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;em&gt;Large language models (LLMs) are increasingly adapted to downstream tasks via reinforcement learning (RL) methods like Group Relative Policy Optimization (GRPO), which often require thousands of rollouts to learn new tasks. We argue that the interpretable nature of language can often provide a much richer learning medium for LLMs, compared with policy gradients derived from sparse, scalar rewards. To test this, we introduce GEPA (Genetic-Pareto), a prompt optimizer that thoroughly incorporates natural language reflection to learn high-level rules from trial and error. Given any AI system containing one or more LLM prompts, GEPA samples system-level trajectories (e.g., reasoning, tool calls, and tool outputs) and reflects on them in natural language to diagnose problems, propose and test prompt updates, and combine complementary lessons from the Pareto frontier of its own attempts. As a result of GEPA's design, it can often turn even just a few rollouts into a large quality gain. Across four tasks, GEPA outperforms GRPO by 10% on average and by up to 20%, while using up to 35x fewer rollouts. GEPA also outperforms the leading prompt optimizer, MIPROv2, by over 10% across two LLMs, and demonstrates promising results as an inference-time search strategy for code optimization.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thrumpwart"&gt; /u/Thrumpwart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2507.19457"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1md9nc8/gepa_reflective_prompt_evolution_can_outperform/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1md9nc8/gepa_reflective_prompt_evolution_can_outperform/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-30T15:29:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1mcvc46</id>
    <title>GLM-4.5 Air on 64gb Mac with MLX</title>
    <updated>2025-07-30T02:52:11+00:00</updated>
    <author>
      <name>/u/jarec707</name>
      <uri>https://old.reddit.com/user/jarec707</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Simon Willison says ‚ÄúIvan Fioravanti built this 44GB 3bit quantized version for MLX, specifically sized so people with 64GB machines could have a chance of running it. I tried it out... and it works extremely well.‚Äù&lt;/p&gt; &lt;p&gt;&lt;a href="https://open.substack.com/pub/simonw/p/my-25-year-old-laptop-can-write-space?r=bmuv&amp;amp;utm_campaign=post&amp;amp;utm_medium=email"&gt;https://open.substack.com/pub/simonw/p/my-25-year-old-laptop-can-write-space?r=bmuv&amp;amp;utm_campaign=post&amp;amp;utm_medium=email&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I‚Äôve run the model with LMStudio on a 64gb M1 Max Studio. LMStudio initially would not run the model, providing a popup to that effect. The popup also allowed me to adjust the guardrails. I had to turn them off entirely to run the model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jarec707"&gt; /u/jarec707 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mcvc46/glm45_air_on_64gb_mac_with_mlx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mcvc46/glm45_air_on_64gb_mac_with_mlx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mcvc46/glm45_air_on_64gb_mac_with_mlx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-30T02:52:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1md1fka</id>
    <title>Benchmark: 15 STT models on long-form medical dialogue</title>
    <updated>2025-07-30T08:51:19+00:00</updated>
    <author>
      <name>/u/MajesticAd2862</name>
      <uri>https://old.reddit.com/user/MajesticAd2862</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1md1fka/benchmark_15_stt_models_on_longform_medical/"&gt; &lt;img alt="Benchmark: 15 STT models on long-form medical dialogue" src="https://preview.redd.it/nxnp5xsw4zff1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e11b8504e80c4f6a6e15f6408d1b821538221d77" title="Benchmark: 15 STT models on long-form medical dialogue" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm building a fully local AI-Scribe for doctors and wanted to know which speech-to-text engines perform well with 5-10 min patient-doctor chats.&lt;br /&gt; I ran 55 mock GP consultations (PriMock57) through 15 open- and closed-source models, logged word-error rate (WER) and speed, and only chunked audio when a model crashed on &amp;gt;40 s clips.&lt;/p&gt; &lt;h1&gt;All results&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;#&lt;/th&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Avg WER&lt;/th&gt; &lt;th align="left"&gt;Avg sec/file&lt;/th&gt; &lt;th align="left"&gt;Host&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;ElevenLabs Scribe v1&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;15.0 %&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;36 s&lt;/td&gt; &lt;td align="left"&gt;API (ElevenLabs)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;MLX Whisper-L v3-turbo&lt;/td&gt; &lt;td align="left"&gt;17.6 %&lt;/td&gt; &lt;td align="left"&gt;13 s&lt;/td&gt; &lt;td align="left"&gt;Local (Apple M4)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;3&lt;/td&gt; &lt;td align="left"&gt;Parakeet-0.6 B v2&lt;/td&gt; &lt;td align="left"&gt;17.9 %&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;5 s&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Local (Apple M4)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;4&lt;/td&gt; &lt;td align="left"&gt;Canary-Qwen 2.5 B&lt;/td&gt; &lt;td align="left"&gt;18.2 %&lt;/td&gt; &lt;td align="left"&gt;105 s&lt;/td&gt; &lt;td align="left"&gt;Local (L4 GPU)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;5&lt;/td&gt; &lt;td align="left"&gt;Apple SpeechAnalyzer&lt;/td&gt; &lt;td align="left"&gt;18.2 %&lt;/td&gt; &lt;td align="left"&gt;6 s&lt;/td&gt; &lt;td align="left"&gt;Local (macOS)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;6&lt;/td&gt; &lt;td align="left"&gt;Groq Whisper-L v3&lt;/td&gt; &lt;td align="left"&gt;18.4 %&lt;/td&gt; &lt;td align="left"&gt;9 s&lt;/td&gt; &lt;td align="left"&gt;API (Groq)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;7&lt;/td&gt; &lt;td align="left"&gt;Voxtral-mini 3 B&lt;/td&gt; &lt;td align="left"&gt;18.5 %&lt;/td&gt; &lt;td align="left"&gt;74 s&lt;/td&gt; &lt;td align="left"&gt;Local (L4 GPU)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;8&lt;/td&gt; &lt;td align="left"&gt;Groq Whisper-L v3-turbo&lt;/td&gt; &lt;td align="left"&gt;18.7 %&lt;/td&gt; &lt;td align="left"&gt;8 s&lt;/td&gt; &lt;td align="left"&gt;API (Groq)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;9&lt;/td&gt; &lt;td align="left"&gt;Canary-1B-Flash&lt;/td&gt; &lt;td align="left"&gt;18.8 %&lt;/td&gt; &lt;td align="left"&gt;23 s&lt;/td&gt; &lt;td align="left"&gt;Local (L4 GPU)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;10&lt;/td&gt; &lt;td align="left"&gt;Voxtral-mini (API)&lt;/td&gt; &lt;td align="left"&gt;19.0 %&lt;/td&gt; &lt;td align="left"&gt;23 s&lt;/td&gt; &lt;td align="left"&gt;API (Mistral)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;11&lt;/td&gt; &lt;td align="left"&gt;WhisperKit-L v3-turbo&lt;/td&gt; &lt;td align="left"&gt;19.1 %&lt;/td&gt; &lt;td align="left"&gt;21 s&lt;/td&gt; &lt;td align="left"&gt;Local (macOS)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;12&lt;/td&gt; &lt;td align="left"&gt;OpenAI Whisper-1&lt;/td&gt; &lt;td align="left"&gt;19.6 %&lt;/td&gt; &lt;td align="left"&gt;104 s&lt;/td&gt; &lt;td align="left"&gt;API (OpenAI)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;13&lt;/td&gt; &lt;td align="left"&gt;OpenAI GPT-4o-mini&lt;/td&gt; &lt;td align="left"&gt;20.6 %&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;API (OpenAI)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;14&lt;/td&gt; &lt;td align="left"&gt;OpenAI GPT-4o&lt;/td&gt; &lt;td align="left"&gt;21.7 %&lt;/td&gt; &lt;td align="left"&gt;28 s&lt;/td&gt; &lt;td align="left"&gt;API (OpenAI)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;15&lt;/td&gt; &lt;td align="left"&gt;Azure Foundry Phi-4&lt;/td&gt; &lt;td align="left"&gt;36.6 %&lt;/td&gt; &lt;td align="left"&gt;213 s&lt;/td&gt; &lt;td align="left"&gt;API (Azure)&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Take-aways&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;ElevenLabs Scribe&lt;/strong&gt; leads accuracy but can hallucinate on edge cases.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Parakeet-0.6 B on an M4&lt;/strong&gt; runs ~5√ó real-time‚Äîgreat if English-only is fine.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Groq Whisper-v3 (turbo)&lt;/strong&gt; offers the best cloud price/latency combo.&lt;/li&gt; &lt;li&gt;Canary/Canary-Qwen/Phi-4 needed chunking, which bumped runtime.&lt;/li&gt; &lt;li&gt;Apple SpeechAnalyzer is a good option for Swift apps.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For details on the dataset, hardware, and full methodology, see the blog post ‚Üí &lt;a href="https://omi.health/blog/benchmarking-tts"&gt;https://omi.health/blog/benchmarking-tts&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to chat‚Äîlet me know if you‚Äôd like the evaluation notebook once it‚Äôs cleaned up!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MajesticAd2862"&gt; /u/MajesticAd2862 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nxnp5xsw4zff1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1md1fka/benchmark_15_stt_models_on_longform_medical/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1md1fka/benchmark_15_stt_models_on_longform_medical/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-30T08:51:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1mdbiei</id>
    <title>AI for normal PCs?</title>
    <updated>2025-07-30T16:40:03+00:00</updated>
    <author>
      <name>/u/ShardsOfSalt</name>
      <uri>https://old.reddit.com/user/ShardsOfSalt</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'd like to make a video game that utilizes AI to have some conversation with users. It doesn't need to win an IMO but it should be able to carry normal every day conversations. And preferably it would be able to do text to speech. But I don't think normal computers are powerful enough for this? Am I mistaken? Can a local llama of some type be run on an average PC to understand and speak?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ShardsOfSalt"&gt; /u/ShardsOfSalt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdbiei/ai_for_normal_pcs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdbiei/ai_for_normal_pcs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mdbiei/ai_for_normal_pcs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-30T16:40:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1md6cxq</id>
    <title>Is it just me or is OpenRouter an absolute roulette wheel lately?</title>
    <updated>2025-07-30T13:17:33+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;No matter which model I choose it seems like I get 1-2 absolutely off the rails responses for every 5 requests I make. Are some providers using ridiculous settings, not respecting configuration (temp, etc..) passed in, or using &lt;em&gt;heavily&lt;/em&gt; quantized models?&lt;/p&gt; &lt;p&gt;I noticed that this &lt;em&gt;never&lt;/em&gt; happens if I pick an individual provider I'm happy with and use their service directly.&lt;/p&gt; &lt;p&gt;Lately seeing it with Llama4-Maverick, Qwen3-235B (both thinking and non thinking), Deepseek (both R1 and V3), and Qwen3-Code-480B.&lt;/p&gt; &lt;p&gt;Anyone else having this experience?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1md6cxq/is_it_just_me_or_is_openrouter_an_absolute/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1md6cxq/is_it_just_me_or_is_openrouter_an_absolute/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1md6cxq/is_it_just_me_or_is_openrouter_an_absolute/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-30T13:17:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1mcfmd2</id>
    <title>Qwen/Qwen3-30B-A3B-Instruct-2507 ¬∑ Hugging Face</title>
    <updated>2025-07-29T16:11:03+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mcfmd2/qwenqwen330ba3binstruct2507_hugging_face/"&gt; &lt;img alt="Qwen/Qwen3-30B-A3B-Instruct-2507 ¬∑ Hugging Face" src="https://external-preview.redd.it/4L2FXW9Fym-Ol4pha2Ze5zHkeeMTtxPBl8ihz-UFknI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c994da656f69e4f6e8089e52864a4ba31055fa1f" title="Qwen/Qwen3-30B-A3B-Instruct-2507 ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mcfmd2/qwenqwen330ba3binstruct2507_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mcfmd2/qwenqwen330ba3binstruct2507_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-29T16:11:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1mco449</id>
    <title>Lemonade: I'm hyped about the speed of the new Qwen3-30B-A3B-Instruct-2507 on Radeon 9070 XT</title>
    <updated>2025-07-29T21:28:04+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mco449/lemonade_im_hyped_about_the_speed_of_the_new/"&gt; &lt;img alt="Lemonade: I'm hyped about the speed of the new Qwen3-30B-A3B-Instruct-2507 on Radeon 9070 XT" src="https://external-preview.redd.it/czBmdXM1aHVydmZmMQf6BkKZI7Ikr6YU2YwAQgo-ERGqCSuuIIibFbpDzG0R.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d458366fefeec47c4d65d3844419bf9e79783f21" title="Lemonade: I'm hyped about the speed of the new Qwen3-30B-A3B-Instruct-2507 on Radeon 9070 XT" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I saw &lt;a href="https://huggingface.co/unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF"&gt;unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF ¬∑ Hugging Face&lt;/a&gt; just came out so I took it for a test drive on Lemonade Server today on my Radeon 9070 XT rig (llama.cpp+vulkan backend, Q4_0, OOB performance with no tuning). The fact that it one-shots the solution with no thinking tokens makes it way faster-to-solution than the previous Qwen3 MOE. I'm excited to see what else it can do this week!&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/lemonade-sdk/lemonade"&gt;lemonade-sdk/lemonade: Local LLM Server with GPU and NPU Acceleration&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/7xpye5hurvff1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mco449/lemonade_im_hyped_about_the_speed_of_the_new/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mco449/lemonade_im_hyped_about_the_speed_of_the_new/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-29T21:28:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1mcxdiu</id>
    <title>New, faster SoftMax math makes Llama inference faster by 5%</title>
    <updated>2025-07-30T04:38:29+00:00</updated>
    <author>
      <name>/u/Odd_Employee128</name>
      <uri>https://old.reddit.com/user/Odd_Employee128</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mcxdiu/new_faster_softmax_math_makes_llama_inference/"&gt; &lt;img alt="New, faster SoftMax math makes Llama inference faster by 5%" src="https://external-preview.redd.it/uygeA57Mfstw_haQUN7kQKKgZbH8FlPoXrBVy5xM7pc.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=82e5920a65a2670b0982cf057b124c956d782352" title="New, faster SoftMax math makes Llama inference faster by 5%" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/1zbwyzlgwxff1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5478539a6ccee17607c04f611ec28225919b2586"&gt;Fast Attention algorithm speeds SoftMax function by about 30&amp;#37; on RTX4090. As a result, we have 5&amp;#37; decrease in inference time for Meta LLM on A100&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://fastattention.ai/#7cb9a932-8d17-4d96-953c-952dfa732171"&gt;https://fastattention.ai/#7cb9a932-8d17-4d96-953c-952dfa732171&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/jtw45kflwxff1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c757888d664c20e32a761fb8bdf23236062472f3"&gt;https://preview.redd.it/jtw45kflwxff1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c757888d664c20e32a761fb8bdf23236062472f3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Update:&lt;/p&gt; &lt;p&gt;Hardware: RTX4090 for SoftMax benchmarks, A100 for inference benchmarks. Ubuntu 22.04. A standard PyTorch 2.7.1. I directly modified aten/src/ATen/native/sparse/cuda/SoftMax.cu. To compare and benchmark, I need to switch between Python virtual environments (venvs). Not Fast-Attention-1 or -2 or -3.&lt;/p&gt; &lt;p&gt;The 5% increase is practically &amp;quot;invisible&amp;quot; for a regular local user but might be beneficial for big datacenters.&lt;/p&gt; &lt;p&gt;The comparison is between my build and a standard PyTorch download from &lt;a href="https://pytorch.org/get-started/locally/"&gt;Get Started&lt;/a&gt;. For the benchmark, I built PyTorch with `python3 ./setup. bdist_wheel` command.&lt;/p&gt; &lt;p&gt;L1 delta between the original and &amp;quot;mine&amp;quot; softamx output is less than 10^-6&lt;/p&gt; &lt;p&gt;The SoftMax benchmark Python code:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;import torch import torch.nn.functional as F # Set dimensions and number of repetitions rows = 1 # &amp;lt;- I manually modify this value cols = 2_000_000_000 # &amp;lt;- I manually modify this value N = 100 # &amp;lt;- I manually modify this value # Step 1: Allocate a CPU tensor with safe values cpu_tensor = torch.full((rows, cols), fill_value=1.0, dtype=torch.float32) # Step 2: Move tensor to CUDA cuda_tensor = cpu_tensor.to('cuda') # Step 3: Set up timing events start_event = torch.cuda.Event(enable_timing=True) end_event = torch.cuda.Event(enable_timing=True) # Step 4: Warm-up (to ensure fair timing) _ = F.softmax(cuda_tensor, dim=1) # Step 5: Record start start_event.record() # Step 6: Run LogSoftMax N times for _ in range(N): _ = F.softmax(cuda_tensor, dim=1) # Step 7: Record end end_event.record() # Step 8: Wait for GPU to finish torch.cuda.synchronize() # Step 9: Measure elapsed time total_time_ms = start_event.elapsed_time(end_event) average_time_ms = total_time_ms / N print(f&amp;quot;Total time for {N} LogSoftMax calls of size {cols:,} x {rows:,} elements: {total_time_ms:.2f} ms&amp;quot;) print(f&amp;quot;Average time per call of length {cols:,} x {rows:,} elements: {average_time_ms:.2f} ms&amp;quot;) # Step 10: Verify softmax normalization log_softmax_result = F.log_softmax(cuda_tensor, dim=1).cpu() sum_exp_cpu = torch.exp(log_softmax_result).sum(dim=1) print(&amp;quot;Sum of all softmax elements (should be 1.0):&amp;quot;, sum_exp_cpu.max().item()) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Odd_Employee128"&gt; /u/Odd_Employee128 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mcxdiu/new_faster_softmax_math_makes_llama_inference/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mcxdiu/new_faster_softmax_math_makes_llama_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mcxdiu/new_faster_softmax_math_makes_llama_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-30T04:38:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1mci7uu</id>
    <title>Newest Qwen made me cry. It's not perfect, but I still love it.</title>
    <updated>2025-07-29T17:45:49+00:00</updated>
    <author>
      <name>/u/Cool-Chemical-5629</name>
      <uri>https://old.reddit.com/user/Cool-Chemical-5629</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mci7uu/newest_qwen_made_me_cry_its_not_perfect_but_i/"&gt; &lt;img alt="Newest Qwen made me cry. It's not perfect, but I still love it." src="https://preview.redd.it/gnkbnxzlouff1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=431c53f32897af3a4225062d97bdc95913f53ec0" title="Newest Qwen made me cry. It's not perfect, but I still love it." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is from the latest Qwen3-30B-A3B-Instruct-2507. ‚ù§&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cool-Chemical-5629"&gt; /u/Cool-Chemical-5629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gnkbnxzlouff1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mci7uu/newest_qwen_made_me_cry_its_not_perfect_but_i/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mci7uu/newest_qwen_made_me_cry_its_not_perfect_but_i/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-29T17:45:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1mdawyz</id>
    <title>Just launched Transformer Lab Recipes: 13 pre-built templates including Llama 3.2 fine-tuning, quantization, and benchmarking.</title>
    <updated>2025-07-30T16:17:35+00:00</updated>
    <author>
      <name>/u/aliasaria</name>
      <uri>https://old.reddit.com/user/aliasaria</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdawyz/just_launched_transformer_lab_recipes_13_prebuilt/"&gt; &lt;img alt="Just launched Transformer Lab Recipes: 13 pre-built templates including Llama 3.2 fine-tuning, quantization, and benchmarking." src="https://preview.redd.it/x7gqer73e1gf1.gif?width=640&amp;amp;crop=smart&amp;amp;s=59a0719796cc2073a0df325ec3f1a9ef590559cd" title="Just launched Transformer Lab Recipes: 13 pre-built templates including Llama 3.2 fine-tuning, quantization, and benchmarking." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After getting helpful feedback from you all, our team just shipped &amp;quot;Recipes‚Äù which are pre-built, fully-runnable workflows for common LLM tasks.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Some of the most popular recipes include:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Llama 3.2 1B fine-tuning&lt;/strong&gt; (with Apple Silicon MLX optimization!)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model quantization to GGUF&lt;/strong&gt; format (CPU and GPU)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Benchmark evaluation&lt;/strong&gt; (MMLU, HellaSwag, PIQA, Winogrande)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LoRA training&lt;/strong&gt; with before/after comparisons&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Dialogue summarization&lt;/strong&gt; (perfect for chat logs)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We support local hardware (CUDA, AMD ROCm, Apple MLX, or CPU) and let you modify anything: model, data, params. Zero config to get started and we‚Äôre open source.&lt;/p&gt; &lt;p&gt;Been testing the Llama 3.2 fine-tuning recipe and the results are great. Way faster than setting everything up from scratch. &lt;/p&gt; &lt;p&gt;What local training workflows are you all using? This seems like it could replace a lot of custom scripts. Appreciate your feedback. What recipes should we add?&lt;/p&gt; &lt;p&gt;üîó Try it here ‚Üí&lt;a href="https://transformerlab.ai/docs/intro"&gt; &lt;/a&gt;&lt;a href="https://transformerlab.ai/"&gt;https://transformerlab.ai/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üîó Useful? Please star us on GitHub ‚Üí &lt;a href="https://github.com/transformerlab"&gt;https://github.com/transformerlab/transformerlab-app&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üîó Ask for help on our Discord Community ‚Üí &lt;a href="https://discord.gg/transformerlab"&gt;https://discord.gg/transformerlab&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aliasaria"&gt; /u/aliasaria &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/x7gqer73e1gf1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdawyz/just_launched_transformer_lab_recipes_13_prebuilt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mdawyz/just_launched_transformer_lab_recipes_13_prebuilt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-30T16:17:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1mcoce7</id>
    <title>AMD's Ryzen AI MAX+ Processors Now Offer a Whopping 96 GB Memory for Consumer Graphics, Allowing Gigantic 128B-Parameter LLMs to Run Locally on PCs</title>
    <updated>2025-07-29T21:37:02+00:00</updated>
    <author>
      <name>/u/_SYSTEM_ADMIN_MOD_</name>
      <uri>https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mcoce7/amds_ryzen_ai_max_processors_now_offer_a_whopping/"&gt; &lt;img alt="AMD's Ryzen AI MAX+ Processors Now Offer a Whopping 96 GB Memory for Consumer Graphics, Allowing Gigantic 128B-Parameter LLMs to Run Locally on PCs" src="https://external-preview.redd.it/9cxUs2c7UTW3WnCYfQNVG3P3u4GjtOuwQSim_dwuwEI.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d50b7793829c5aa107cf8ecaa3b004d46e3cdef0" title="AMD's Ryzen AI MAX+ Processors Now Offer a Whopping 96 GB Memory for Consumer Graphics, Allowing Gigantic 128B-Parameter LLMs to Run Locally on PCs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_"&gt; /u/_SYSTEM_ADMIN_MOD_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/amd-ryzen-ai-max-processors-offer-a-96gb-memory-for-consumer-graphics/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mcoce7/amds_ryzen_ai_max_processors_now_offer_a_whopping/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mcoce7/amds_ryzen_ai_max_processors_now_offer_a_whopping/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-29T21:37:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1md0gfh</id>
    <title>RTX 5090 form INNO3D 1 slot with Alphacool-waterkoeling look perfect for local AI machines</title>
    <updated>2025-07-30T07:46:19+00:00</updated>
    <author>
      <name>/u/jwestra</name>
      <uri>https://old.reddit.com/user/jwestra</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1md0gfh/rtx_5090_form_inno3d_1_slot_with/"&gt; &lt;img alt="RTX 5090 form INNO3D 1 slot with Alphacool-waterkoeling look perfect for local AI machines" src="https://preview.redd.it/eeopjbr7uyff1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=071af31f7998dd67f773b41419988ca83dd8fdd3" title="RTX 5090 form INNO3D 1 slot with Alphacool-waterkoeling look perfect for local AI machines" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ul&gt; &lt;li&gt;Keeping your warranty.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;1 slot&lt;br /&gt;&lt;/li&gt; &lt;li&gt;backside tube exits &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Look perfect to make a dense AI machine.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.inno3d.com/news/inno3d-geforce-rtx-5090-rtx-5080-frostbite-pro-1-slot-design"&gt;https://www.inno3d.com/news/inno3d-geforce-rtx-5090-rtx-5080-frostbite-pro-1-slot-design&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jwestra"&gt; /u/jwestra &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/eeopjbr7uyff1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1md0gfh/rtx_5090_form_inno3d_1_slot_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1md0gfh/rtx_5090_form_inno3d_1_slot_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-30T07:46:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1md7h5z</id>
    <title>Meta‚Äôs Vision for the future of Personal SuperIntelligence</title>
    <updated>2025-07-30T14:04:42+00:00</updated>
    <author>
      <name>/u/5h3r_10ck</name>
      <uri>https://old.reddit.com/user/5h3r_10ck</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1md7h5z/metas_vision_for_the_future_of_personal/"&gt; &lt;img alt="Meta‚Äôs Vision for the future of Personal SuperIntelligence" src="https://b.thumbs.redditmedia.com/wTDzxveLNV6lyThS6Dq2WJK_bRtGmie5PYzTvcrT62c.jpg" title="Meta‚Äôs Vision for the future of Personal SuperIntelligence" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Today Mark shared Meta‚Äôs vision for the future of personal superintelligence for everyone. &lt;/p&gt; &lt;p&gt;Redditors!! What's your take on this?&lt;/p&gt; &lt;p&gt;Read his full letter here: &lt;a href="https://www.meta.com/superintelligence/"&gt;https://www.meta.com/superintelligence/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/5h3r_10ck"&gt; /u/5h3r_10ck &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1md7h5z"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1md7h5z/metas_vision_for_the_future_of_personal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1md7h5z/metas_vision_for_the_future_of_personal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-30T14:04:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1mcr64f</id>
    <title>4B models are consistently overlooked. Runs Locally and Crushes It. Reasoning for UI, Mobile, Software and Frontend design.</title>
    <updated>2025-07-29T23:36:00+00:00</updated>
    <author>
      <name>/u/smirkishere</name>
      <uri>https://old.reddit.com/user/smirkishere</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mcr64f/4b_models_are_consistently_overlooked_runs/"&gt; &lt;img alt="4B models are consistently overlooked. Runs Locally and Crushes It. Reasoning for UI, Mobile, Software and Frontend design." src="https://b.thumbs.redditmedia.com/3wFSGxs0og7hUYyLF8nuoy2CBvu34JQ_m2cRe7ujEoc.jpg" title="4B models are consistently overlooked. Runs Locally and Crushes It. Reasoning for UI, Mobile, Software and Frontend design." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/Tesslate/UIGEN-X-4B-0729"&gt;https://huggingface.co/Tesslate/UIGEN-X-4B-0729&lt;/a&gt; 4B model that does reasoning for Design. We also released a 32B earlier in the week. &lt;/p&gt; &lt;p&gt;As per the last post -&amp;gt;&lt;br /&gt; Specifically trained for modern web and mobile development across frameworks like React (Next.js, Remix, Gatsby, Vite), Vue (Nuxt, Quasar), Angular (Angular CLI, Ionic), and SvelteKit, along with Solid.js, Qwik, Astro, and static site tools like 11ty and Hugo. Styling options include Tailwind CSS, CSS-in-JS (Styled Components, Emotion), and full design systems like Carbon and Material UI. We cover UI libraries for every framework React (shadcn/ui, Chakra, Ant Design), Vue (Vuetify, PrimeVue), Angular, and Svelte plus headless solutions like Radix UI. State management spans Redux, Zustand, Pinia, Vuex, NgRx, and universal tools like MobX and XState. For animation, we support Framer Motion, GSAP, and Lottie, with icons from Lucide, Heroicons, and more. Beyond web, we enable React Native, Flutter, and Ionic for mobile, and Electron, Tauri, and Flutter Desktop for desktop apps. Python integration includes Streamlit, Gradio, Flask, and FastAPI. All backed by modern build tools, testing frameworks, and support for 26+ languages and UI approaches, including JavaScript, TypeScript, Dart, HTML5, CSS3, and component-driven architectures.&lt;/p&gt; &lt;p&gt;We're looking for some beta testers for some new models and open source projects!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/smirkishere"&gt; /u/smirkishere &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mcr64f"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mcr64f/4b_models_are_consistently_overlooked_runs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mcr64f/4b_models_are_consistently_overlooked_runs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-29T23:36:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1md00oc</id>
    <title>Kudos to Qwen 3 team!</title>
    <updated>2025-07-30T07:17:24+00:00</updated>
    <author>
      <name>/u/ExcuseAccomplished97</name>
      <uri>https://old.reddit.com/user/ExcuseAccomplished97</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1md00oc/kudos_to_qwen_3_team/"&gt; &lt;img alt="Kudos to Qwen 3 team!" src="https://b.thumbs.redditmedia.com/mUsja5QiJMisNYHJhZA4P57qdlHnaGZYvKopTiB-51E.jpg" title="Kudos to Qwen 3 team!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The Qwen3-30B-A3B-Instruct-2507 is an amazing release! Congratulations!&lt;/p&gt; &lt;p&gt;However, the three-month-old 32B shows better performance across the board in the benchmark. I hope the Qwen3-32B Instruct/Thinking and Qwen3-30B-A3B-Thinking-2507 versions will be released soon!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ExcuseAccomplished97"&gt; /u/ExcuseAccomplished97 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1md00oc/kudos_to_qwen_3_team/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1md00oc/kudos_to_qwen_3_team/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1md00oc/kudos_to_qwen_3_team/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-30T07:17:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1md6xba</id>
    <title>Skywork/Skywork-UniPic-1.5B - A unified autoregressive multimodal model</title>
    <updated>2025-07-30T13:41:50+00:00</updated>
    <author>
      <name>/u/nullmove</name>
      <uri>https://old.reddit.com/user/nullmove</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1md6xba/skyworkskyworkunipic15b_a_unified_autoregressive/"&gt; &lt;img alt="Skywork/Skywork-UniPic-1.5B - A unified autoregressive multimodal model" src="https://external-preview.redd.it/NU1es84U5dKcUVq65hYCHqeHpunplTrqbG-pwQUy3MM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0deccacf66db69a58065546ea92e99fae0e3c4d6" title="Skywork/Skywork-UniPic-1.5B - A unified autoregressive multimodal model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nullmove"&gt; /u/nullmove &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Skywork/Skywork-UniPic-1.5B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1md6xba/skyworkskyworkunipic15b_a_unified_autoregressive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1md6xba/skyworkskyworkunipic15b_a_unified_autoregressive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-30T13:41:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1mdbm5t</id>
    <title>Eigent ‚Äì Open Source, Local-First Multi-Agent Workforce</title>
    <updated>2025-07-30T16:44:03+00:00</updated>
    <author>
      <name>/u/FitHeron1933</name>
      <uri>https://old.reddit.com/user/FitHeron1933</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdbm5t/eigent_open_source_localfirst_multiagent_workforce/"&gt; &lt;img alt="Eigent ‚Äì Open Source, Local-First Multi-Agent Workforce" src="https://b.thumbs.redditmedia.com/3RqGcIVLHN9WLuz3G6pO2CwDYLwlqMU3O2iTSdwHGzY.jpg" title="Eigent ‚Äì Open Source, Local-First Multi-Agent Workforce" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just launched &lt;strong&gt;Eigent,&lt;/strong&gt; a fully open-source, local-first multi-agent desktop application designed for developers and teams who want full control over their AI workflows.&lt;br /&gt; Built on top of CAMEL-AI‚Äôs modular framework, Eigent allows you to:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Run tasks in parallel with customizable agent workflows&lt;/li&gt; &lt;li&gt;Deploy locally or in the cloud with ‚ÄúBring Your Own Key‚Äù (BYOK) support&lt;/li&gt; &lt;li&gt;Maintain full data privacy ‚Äî no information leaves your machine&lt;/li&gt; &lt;li&gt;Step in anytime with Human-in-the-Loop control&lt;/li&gt; &lt;li&gt;Integrate seamlessly with your existing stack&lt;/li&gt; &lt;li&gt;Use 200+ MCP-compatible tools (or bring your own)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The goal is simple: give teams a secure, customizable, and scalable AI workforce on their own infrastructure.&lt;br /&gt; ‚Üí GitHub: &lt;a href="http://github.com/eigent-ai/eigent"&gt;github.com/eigent-ai/eigent&lt;/a&gt;&lt;br /&gt; ‚Üí Download: &lt;a href="http://www.eigent.ai/"&gt;eigent.ai &lt;/a&gt;&lt;br /&gt; Feel free to ask me anything below, whether it‚Äôs about the architecture, use cases, or how to extend it for your own needs.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FitHeron1933"&gt; /u/FitHeron1933 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mdbm5t"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdbm5t/eigent_open_source_localfirst_multiagent_workforce/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mdbm5t/eigent_open_source_localfirst_multiagent_workforce/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-30T16:44:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1md8rxu</id>
    <title>Qwen/Qwen3-30B-A3B-Thinking-2507 ¬∑ Hugging Face</title>
    <updated>2025-07-30T14:56:12+00:00</updated>
    <author>
      <name>/u/MariusNocturnum</name>
      <uri>https://old.reddit.com/user/MariusNocturnum</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1md8rxu/qwenqwen330ba3bthinking2507_hugging_face/"&gt; &lt;img alt="Qwen/Qwen3-30B-A3B-Thinking-2507 ¬∑ Hugging Face" src="https://external-preview.redd.it/-lNzejy2CT3wd1ovuVIcDeuPfMRg-vkESkjpQgo3tYU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bd872c4c3958b52ad860a6db5ba53994da65552e" title="Qwen/Qwen3-30B-A3B-Thinking-2507 ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MariusNocturnum"&gt; /u/MariusNocturnum &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-30B-A3B-Thinking-2507"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1md8rxu/qwenqwen330ba3bthinking2507_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1md8rxu/qwenqwen330ba3bthinking2507_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-30T14:56:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1md5k8f</id>
    <title>GLM4.5 EQ-Bench and Creative Write</title>
    <updated>2025-07-30T12:42:02+00:00</updated>
    <author>
      <name>/u/pcdacks</name>
      <uri>https://old.reddit.com/user/pcdacks</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1md5k8f/glm45_eqbench_and_creative_write/"&gt; &lt;img alt="GLM4.5 EQ-Bench and Creative Write" src="https://preview.redd.it/ubwsl0gdb0gf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=abdf15ff9928a1e321306852523e66da9ac4b1cf" title="GLM4.5 EQ-Bench and Creative Write" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pcdacks"&gt; /u/pcdacks &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ubwsl0gdb0gf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1md5k8f/glm45_eqbench_and_creative_write/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1md5k8f/glm45_eqbench_and_creative_write/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-30T12:42:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1md8slx</id>
    <title>Qwen3-30b-a3b-thinking-2507 This is insane performance</title>
    <updated>2025-07-30T14:56:57+00:00</updated>
    <author>
      <name>/u/3oclockam</name>
      <uri>https://old.reddit.com/user/3oclockam</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1md8slx/qwen330ba3bthinking2507_this_is_insane_performance/"&gt; &lt;img alt="Qwen3-30b-a3b-thinking-2507 This is insane performance" src="https://external-preview.redd.it/-lNzejy2CT3wd1ovuVIcDeuPfMRg-vkESkjpQgo3tYU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bd872c4c3958b52ad860a6db5ba53994da65552e" title="Qwen3-30b-a3b-thinking-2507 This is insane performance" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;On par with qwen3-235b?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/3oclockam"&gt; /u/3oclockam &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-30B-A3B-Thinking-2507"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1md8slx/qwen330ba3bthinking2507_this_is_insane_performance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1md8slx/qwen330ba3bthinking2507_this_is_insane_performance/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-30T14:56:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1md93bj</id>
    <title>Qwen3 Coder 30B-A3B tomorrow!!!</title>
    <updated>2025-07-30T15:08:26+00:00</updated>
    <author>
      <name>/u/R46H4V</name>
      <uri>https://old.reddit.com/user/R46H4V</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1md93bj/qwen3_coder_30ba3b_tomorrow/"&gt; &lt;img alt="Qwen3 Coder 30B-A3B tomorrow!!!" src="https://preview.redd.it/zv92612t11gf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=45b98263f660ff1bebd4634907371461fd4e0207" title="Qwen3 Coder 30B-A3B tomorrow!!!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/R46H4V"&gt; /u/R46H4V &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zv92612t11gf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1md93bj/qwen3_coder_30ba3b_tomorrow/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1md93bj/qwen3_coder_30ba3b_tomorrow/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-30T15:08:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1md8t1g</id>
    <title>üöÄ Qwen3-30B-A3B-Thinking-2507</title>
    <updated>2025-07-30T14:57:27+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1md8t1g/qwen330ba3bthinking2507/"&gt; &lt;img alt="üöÄ Qwen3-30B-A3B-Thinking-2507" src="https://preview.redd.it/eaag1cpuz0gf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e073b4b20cd702585ec6bbac8fc80938677c24f8" title="üöÄ Qwen3-30B-A3B-Thinking-2507" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;üöÄ Qwen3-30B-A3B-Thinking-2507, a medium-size model that can think!&lt;/p&gt; &lt;p&gt;‚Ä¢ Nice performance on reasoning tasks, including math, science, code &amp;amp; beyond ‚Ä¢ Good at tool use, competitive with larger models ‚Ä¢ Native support of 256K-token context, extendable to 1M&lt;/p&gt; &lt;p&gt;Hugging Face: &lt;a href="https://huggingface.co/Qwen/Qwen3-30B-A3B-Thinking-2507"&gt;https://huggingface.co/Qwen/Qwen3-30B-A3B-Thinking-2507&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Model scope: &lt;a href="https://modelscope.cn/models/Qwen/Qwen3-30B-A3B-Thinking-2507/summary"&gt;https://modelscope.cn/models/Qwen/Qwen3-30B-A3B-Thinking-2507/summary&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/eaag1cpuz0gf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1md8t1g/qwen330ba3bthinking2507/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1md8t1g/qwen330ba3bthinking2507/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-30T14:57:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1md6t2h</id>
    <title>Bye bye, Meta AI, it was good while it lasted.</title>
    <updated>2025-07-30T13:36:51+00:00</updated>
    <author>
      <name>/u/absolooot1</name>
      <uri>https://old.reddit.com/user/absolooot1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Zuck has posted a video and a longer letter about the superintelligence plans at Meta. In the letter he says:&lt;/p&gt; &lt;p&gt;&amp;quot;That said, superintelligence will raise novel safety concerns. We'll need to be rigorous about mitigating these risks and careful about what we choose to open source.&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.meta.com/superintelligence/"&gt;https://www.meta.com/superintelligence/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;That means that Meta will not open source the best they have. But it is inevitable that others will release their best models and agents, meaning that Meta has committed itself to oblivion, not only in open source but in proprietary too, as they are not a major player in that space. The ASI they will get to will be for use in their products only.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/absolooot1"&gt; /u/absolooot1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1md6t2h/bye_bye_meta_ai_it_was_good_while_it_lasted/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1md6t2h/bye_bye_meta_ai_it_was_good_while_it_lasted/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1md6t2h/bye_bye_meta_ai_it_was_good_while_it_lasted/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-30T13:36:51+00:00</published>
  </entry>
</feed>
