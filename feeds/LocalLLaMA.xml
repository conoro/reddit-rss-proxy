<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-02T19:20:30+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ompw8z</id>
    <title>Vision = Language: I Decoded VLM Tokens to See What AI 'Sees' üî¨</title>
    <updated>2025-11-02T19:08:52+00:00</updated>
    <author>
      <name>/u/ComputeVoid</name>
      <uri>https://old.reddit.com/user/ComputeVoid</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ompw8z/vision_language_i_decoded_vlm_tokens_to_see_what/"&gt; &lt;img alt="Vision = Language: I Decoded VLM Tokens to See What AI 'Sees' üî¨" src="https://b.thumbs.redditmedia.com/vpnIZbZZCCRST1wmFgw4z_-ra4mD9_QOiBZ9JWlIHpw.jpg" title="Vision = Language: I Decoded VLM Tokens to See What AI 'Sees' üî¨" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've spent a lot of time learning how language models work, but images obviously aren't language ‚Äì so how is it possible for AI to understand an image? I studied Gemma 3 to learn about how modern vision language models work.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The core finding:&lt;/strong&gt; Vision language models are just language models that learned to &amp;quot;speak image&amp;quot;. Images get encoded as tokens in linguistic space, and then the language model processes them identically to text.&lt;/p&gt; &lt;p&gt;So, if visual information gets translated into linguistic space, can we interpret the image tokens by mapping them to vocabulary space? I built an unembedding technique to answer that question and analyze what semantic information is encoded in the image tokens.&lt;/p&gt; &lt;h1&gt;Background: How VLMs Work&lt;/h1&gt; &lt;p&gt;Here's a diagram I created for my video that I think is helpful:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/rk2m9rsk5wyf1.png?width=960&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1149132fb1b0148c3684a54a14bcb3a7f84cb8ae"&gt;https://preview.redd.it/rk2m9rsk5wyf1.png?width=960&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1149132fb1b0148c3684a54a14bcb3a7f84cb8ae&lt;/a&gt;&lt;/p&gt; &lt;p&gt;As you can see, there are two pieces: the vision tower + a standard language model. The vision tower is quite literally bolted on to a normal language model.&lt;/p&gt; &lt;p&gt;For Gemma 3 specifically, the data flow is:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Preprocessing: Convert image ‚Üí 3 √ó 896 √ó 896 pixels&lt;/li&gt; &lt;li&gt;Vision transformer: Process pixels ‚Üí 4,096 image tokens&lt;/li&gt; &lt;li&gt;Multimodal projector: Compress 4,096 tokens ‚Üí 256 tokens (semantically meaningful in language model's d_model space)&lt;/li&gt; &lt;li&gt;Language model: Image tokens and text tokens processed identically&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The brilliance is the multimodal projector ‚Äì it translates visual information into linguistic space.&lt;/p&gt; &lt;h1&gt;Method: Unembedding Image Tokens&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Validation:&lt;/strong&gt; First, I validated the technique with text tokens. By taking a token embedding and passing it directly through the language head (bypassing the transformer layers), I could recover the original token with 100% accuracy. This proves that unembedding works for linguistic tokens.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Applying to images:&lt;/strong&gt; The same technique can be applied to image tokens:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Image ‚Üí Vision Tower ‚Üí Multimodal Projector ‚Üí 256 image tokens ‚Üí Unembed each token &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This is greedy unembedding ‚Äì finding the nearest vocabulary token to any embedding vector. Since this is a nearest neighbor approach, it's lossy. The reality is that image tokens live in linguistic space but don't necessarily map exactly to a single vocabulary token. An image token can exist between different vocabulary words in the embedding space.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Token Type&lt;/th&gt; &lt;th align="left"&gt;Embedding Space Behavior&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Text tokens&lt;/td&gt; &lt;td align="left"&gt;Map 1:1 to a place in embedding space ‚Äì each token in the vocabulary has exactly 1 vector representation&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Image tokens&lt;/td&gt; &lt;td align="left"&gt;Have vector representations that seem to exist &lt;em&gt;between&lt;/em&gt; text tokens&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;What I Found&lt;/h1&gt; &lt;p&gt;Here's what the unembedding revealed for different image types (see the linked notebook for more):&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Purple square (monocolor):&lt;/strong&gt; The model correctly identifies the dominant color&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/l2c7hko55wyf1.png?width=470&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2ffdc04268e03edea1c1ec69bb18ac3b2fbc703e"&gt;https://preview.redd.it/l2c7hko55wyf1.png?width=470&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2ffdc04268e03edea1c1ec69bb18ac3b2fbc703e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Mountain scene (sunrise over mountains):&lt;/strong&gt; Rich semantic encoding: proper nouns, landscape features, time of day&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/eeq8zw075wyf1.png?width=525&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=454981867d6106775ab90668ba28f022b257d722"&gt;https://preview.redd.it/eeq8zw075wyf1.png?width=525&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=454981867d6106775ab90668ba28f022b257d722&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key observations&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;em&gt;The &amp;quot; the&amp;quot; phenomenon:&lt;/em&gt; Across all image types, a large percentage of tokens map to &amp;quot; the&amp;quot;. Since &amp;quot; the&amp;quot; is usually the most common token in training data, it likely occupies a central location in embedding space. This might reveal either that not all image tokens are informative, or it might expose a limitation of greedy unembedding: when image tokens don't map cleanly to a single vocabulary word, the nearest neighbor defaults to the most &amp;quot;central&amp;quot; token ‚Äì there may be information encoded that greedy nearest-neighbor decoding can't reveal.&lt;/li&gt; &lt;li&gt;&lt;em&gt;Semantic emergence:&lt;/em&gt; Even with the &amp;quot;the&amp;quot; dominance, semantically meaningful tokens do emerge ‚Äì colors, landscape features, proper nouns. The language model's understanding of images is messy, but there's signal in the noise.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Implications &amp;amp; Open Questions&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Implication: The 256-Token Bottleneck: Feature, Not Flaw?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The multimodal projector compresses 4,096 visual patches down to 256 tokens. At first, this seemed like a clear limitation ‚Äì you're losing information in that compression. There is only so much that can be encoded in 256 tokens, right?&lt;/p&gt; &lt;p&gt;There has been some buzz recently about the DeepSeek-OCR paper and how image tokens can be used as a form of compression. This got me thinking about the 256-token budget differently.&lt;/p&gt; &lt;p&gt;Remember that image tokens exist &lt;em&gt;between&lt;/em&gt; text tokens in embedding space. A text token maps 1:1 to exactly one vocabulary word. But an image token isn't constrained to discrete vocabulary positions ‚Äì it can exist anywhere in the continuous embedding space between multiple words. This means a single image token can simultaneously encode aspects of multiple concepts.&lt;/p&gt; &lt;p&gt;In other words, &lt;em&gt;image tokens have higher information density than text tokens.&lt;/em&gt; Each of the 256 image tokens can encode more nuanced information than a discrete text token could.&lt;/p&gt; &lt;p&gt;This reframes the 256-token &amp;quot;bottleneck&amp;quot; ‚Äì maybe it's not a limitation but an efficient compression that can be exploited.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Open Question: Positional Encoding: Distributed or Discrete?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Someone asked me recently how positional information in an image gets encoded in the vision tokens. I don't have a good answer, but I think it's a really interesting question. Positional information is obviously encoded somewhere, but where? Is it very distributed across the 256? Or are there specific token positions that effectively act as positional experts? How is information encoded across the 256 token budget?&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;em&gt;1 giant pool&lt;/em&gt; (each token plays a small role in constructing what appears as an aggregate meaning when looking at all 256)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;OR&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;em&gt;256 smaller pools&lt;/em&gt; (each token is more of a specialist, i.e., the 0th position vision token serves a different function than the 255th)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;My gut tells me the 1 giant pool idea seems more likely to me. But, as I've learned with VLMs, the reality is probably somewhere in the middle, and quite messy and hard to study! But I bet there is some cool stuff to discover with more sophisticated techniques.&lt;/p&gt; &lt;h1&gt;Want to Explore More?&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://youtu.be/NpWP-hOq6II?si=Qun_EsWq7LLQ4ugw"&gt;&lt;strong&gt;&amp;quot;Dissecting Vision Language Models: How AI Sees&amp;quot;&lt;/strong&gt;&lt;/a&gt; ‚Äì My 20-min video walkthrough going deeper into VLM architecture and the unembedding technique&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/jacob-danner/dissecting-vlm/blob/main/dissecting_vlm.ipynb"&gt;&lt;strong&gt;GitHub repo with notebook&lt;/strong&gt;&lt;/a&gt; ‚Äì Clone the repo and try unembedding your own images to see what the model &amp;quot;sees&amp;quot; in linguistic space&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=mLgyZ5GauhM&amp;amp;t=6s"&gt;&lt;strong&gt;Teaching AI to See: A Technical Deep-Dive on Vision Language Models with Will Hardman of Veratai&lt;/strong&gt;&lt;/a&gt; ‚Äì &lt;em&gt;Cognitive Revolution&lt;/em&gt; podcast episode that's an excellent comprehensive map of the VLM landscape&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I think vision language models are super fascinating, especially on the mechanistic interpretability side trying to understand what those image tokens actually represent. Let me know what you discover!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ComputeVoid"&gt; /u/ComputeVoid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ompw8z/vision_language_i_decoded_vlm_tokens_to_see_what/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ompw8z/vision_language_i_decoded_vlm_tokens_to_see_what/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ompw8z/vision_language_i_decoded_vlm_tokens_to_see_what/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T19:08:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1ome16n</id>
    <title>LEAP: Ifm2-2.6b running locally on my RM11 Pro+</title>
    <updated>2025-11-02T10:34:48+00:00</updated>
    <author>
      <name>/u/ANG3LBEATZ</name>
      <uri>https://old.reddit.com/user/ANG3LBEATZ</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ome16n/leap_ifm226b_running_locally_on_my_rm11_pro/"&gt; &lt;img alt="LEAP: Ifm2-2.6b running locally on my RM11 Pro+" src="https://external-preview.redd.it/dWk5bXFrd2pudHlmMS2vRIL-FSqGaAZYDE4hFOYMDU1BKOfLu6Jj8jaoH7vM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=06ec81f2aecb737f1ef77aba433757f6df108957" title="LEAP: Ifm2-2.6b running locally on my RM11 Pro+" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;uploading this by the request&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ANG3LBEATZ"&gt; /u/ANG3LBEATZ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/qeszvwvjntyf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ome16n/leap_ifm226b_running_locally_on_my_rm11_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ome16n/leap_ifm226b_running_locally_on_my_rm11_pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T10:34:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1omaa4i</id>
    <title>OCR Testing Tool maybe Open Source it?</title>
    <updated>2025-11-02T06:28:09+00:00</updated>
    <author>
      <name>/u/No-Fig-8614</name>
      <uri>https://old.reddit.com/user/No-Fig-8614</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I created a quick OCR tool, what it does is you choose a file then a OCR model to use. Its free to use on this test site. What it does is upload the document -&amp;gt; turns to base64-&amp;gt; OCR Model -&amp;gt; extraction model. The extraction model is a larger model (In this case GLM4.6) to create key value extractions, then format it into json output. Eventually could add API's and user management. &lt;a href="https://parasail-ocr-pipeline.azurewebsites.net/"&gt;https://parasail-ocr-pipeline.azurewebsites.net/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For PDF's I put a pre-processing library that will cut the pdf into pages/images then send it to the OCR model then combine it after.&lt;/p&gt; &lt;p&gt;The status bar needs work because it will produce the OCR output first but then takes another minute for the auto schema (key/value) creation, then modify the JSON).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Any feedback on it would be great on it!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: There is no user segregation so any document uploaded anyone else can see.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Fig-8614"&gt; /u/No-Fig-8614 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omaa4i/ocr_testing_tool_maybe_open_source_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omaa4i/ocr_testing_tool_maybe_open_source_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1omaa4i/ocr_testing_tool_maybe_open_source_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T06:28:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1omjgzg</id>
    <title>Can I run open source local LLM trained on specific dataset ?</title>
    <updated>2025-11-02T15:00:20+00:00</updated>
    <author>
      <name>/u/hugo_mdn</name>
      <uri>https://old.reddit.com/user/hugo_mdn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi there!&lt;/p&gt; &lt;p&gt;I'm quite new to local LLM, so maybe this question will look dumb to you.&lt;/p&gt; &lt;p&gt;I don't like how ChatGPT is going because it's trained on the whole internet, and it's less and less precise. When I'm looking for very particular information in programming, culture, or anything else, it's not accurate, or using the good sources. And also, I'm not really a fan of privacy terms of OpenAI and other online models.&lt;/p&gt; &lt;p&gt;So my question is, could I run LLM locally (yes), and use a very specific dataset of trusted sources, like Wikipedia, books, very specific health and science websites, programming websites, etc..? And if yes, are there any excellent datasets available? Because I don't really want to add millions of websites and sources one by one.&lt;/p&gt; &lt;p&gt;Thanks in advance for your time and have a nice day :D&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hugo_mdn"&gt; /u/hugo_mdn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omjgzg/can_i_run_open_source_local_llm_trained_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omjgzg/can_i_run_open_source_local_llm_trained_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1omjgzg/can_i_run_open_source_local_llm_trained_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T15:00:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1omlpd9</id>
    <title>Can Qwen3-Next solve a river-crossing puzzle (tested for you)?</title>
    <updated>2025-11-02T16:25:55+00:00</updated>
    <author>
      <name>/u/MarketingNetMind</name>
      <uri>https://old.reddit.com/user/MarketingNetMind</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omlpd9/can_qwen3next_solve_a_rivercrossing_puzzle_tested/"&gt; &lt;img alt="Can Qwen3-Next solve a river-crossing puzzle (tested for you)?" src="https://a.thumbs.redditmedia.com/-lsV327ShKmsvfl9aj3FuXvaqg_MSJRi9MSkmxKdAv4.jpg" title="Can Qwen3-Next solve a river-crossing puzzle (tested for you)?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Yes I tested.&lt;/p&gt; &lt;p&gt;Test Prompt: A farmer needs to cross a river with a fox, a chicken, and a bag of corn. His boat can only carry himself plus one other item at a time. If left alone together, the fox will eat the chicken, and the chicken will eat the corn. How should the farmer cross the river?&lt;/p&gt; &lt;p&gt;Both Qwen3-Next &amp;amp; Qwen3-30B-A3B-2507 correctly solved the river-crossing puzzle with identical 7-step solutions.&lt;/p&gt; &lt;p&gt;How challenging are classic puzzles to LLMs?&lt;/p&gt; &lt;p&gt;Classic puzzles like river-crossing would require &amp;quot;precise understanding, extensive search, and exact inference&amp;quot; where &amp;quot;small misinterpretations can lead to entirely incorrect solutions&amp;quot;, by Apple‚Äôs 2025 research on &amp;quot;The Illusion of Thinking&amp;quot;.&lt;/p&gt; &lt;p&gt;But what‚Äôs better?&lt;/p&gt; &lt;p&gt;Qwen3-Next provided a more structured, easy-to-read presentation with clear state transitions, while Qwen3-30B-A3B-2507 included more explanations with some redundant verification steps.&lt;/p&gt; &lt;p&gt;P.S. Given the same prompt input, Qwen3-Next is more likely to give out structured output without explicitly prompting it to do so, than mainstream closed-source models (ChatGPT, Gemini, Claude, Grok). More tests on Qwen3-Next &lt;a href="https://blog.netmind.ai/article/We_Tested_Qwen3-Next%3A_Hybrid_Attention_for_Efficiency_Revolution_in_Open-Source_LLMs_(New_Research_Breakdown"&gt;here&lt;/a&gt;).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MarketingNetMind"&gt; /u/MarketingNetMind &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1omlpd9"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omlpd9/can_qwen3next_solve_a_rivercrossing_puzzle_tested/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1omlpd9/can_qwen3next_solve_a_rivercrossing_puzzle_tested/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T16:25:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1om81j1</id>
    <title>glm-proxy - A Proxy Server I Built to Fix GLM 4.5 Air's Tool Call Issues</title>
    <updated>2025-11-02T04:17:09+00:00</updated>
    <author>
      <name>/u/akirose1004</name>
      <uri>https://old.reddit.com/user/akirose1004</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1om81j1/glmproxy_a_proxy_server_i_built_to_fix_glm_45/"&gt; &lt;img alt="glm-proxy - A Proxy Server I Built to Fix GLM 4.5 Air's Tool Call Issues" src="https://b.thumbs.redditmedia.com/7K256kODiuLD_3gm4UtRLndAWItovQwEv3qnrZaI3FI.jpg" title="glm-proxy - A Proxy Server I Built to Fix GLM 4.5 Air's Tool Call Issues" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/mdu32bj6kryf1.png?width=476&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d6c99630db5ed36a9a2340d77a8bae86f2d708cc"&gt;https://preview.redd.it/mdu32bj6kryf1.png?width=476&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d6c99630db5ed36a9a2340d77a8bae86f2d708cc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I was running GLM 4.5 Air on my MacBook M4 Max with LM Studio, but &lt;strong&gt;tool calls weren't working properly&lt;/strong&gt;, which meant I couldn't use qwen-code CLI. I wanted to use an OpenAI-compatible interface, and this constant friction frustrated me enough to build a solution.&lt;/p&gt; &lt;p&gt;A proxy server that &lt;strong&gt;automatically converts GLM's XML-formatted tool calls to OpenAI-compatible format&lt;/strong&gt;. Now you can use any OpenAI-compatible client (like qwen-code) with GLM seamlessly!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Full OpenAI API compatibility&lt;/li&gt; &lt;li&gt;Automatic conversion of GLM's XML &lt;code&gt;&amp;lt;tool_call&amp;gt;&lt;/code&gt; format to OpenAI JSON format&lt;/li&gt; &lt;li&gt;Streaming support&lt;/li&gt; &lt;li&gt;Multiple tool calls and complex JSON argument parsing&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Point any OpenAI-compatible client (qwen-code, LangChain, etc.) to this address and use GLM 4.5 Air as if it were OpenAI!&lt;/p&gt; &lt;h1&gt;üîó GitHub&lt;/h1&gt; &lt;p&gt;&lt;a href="https://github.com/akirose/glm-proxy"&gt;https://github.com/akirose/glm-proxy&lt;/a&gt; (MIT License)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;If you're using GLM 4.5 with LM Studio, no more tool call headaches!&lt;/strong&gt; üòä&lt;/p&gt; &lt;p&gt;Feedback and suggestions welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/akirose1004"&gt; /u/akirose1004 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1om81j1/glmproxy_a_proxy_server_i_built_to_fix_glm_45/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1om81j1/glmproxy_a_proxy_server_i_built_to_fix_glm_45/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1om81j1/glmproxy_a_proxy_server_i_built_to_fix_glm_45/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T04:17:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1omoojj</id>
    <title>I built a small DSL to generate roleplay datasets for LoRA fine‚Äëtuning my local models</title>
    <updated>2025-11-02T18:21:29+00:00</updated>
    <author>
      <name>/u/fajfas3</name>
      <uri>https://old.reddit.com/user/fajfas3</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omoojj/i_built_a_small_dsl_to_generate_roleplay_datasets/"&gt; &lt;img alt="I built a small DSL to generate roleplay datasets for LoRA fine‚Äëtuning my local models" src="https://external-preview.redd.it/z12rhz6t5cpG06PqNh1zwiXMorUUFuMH8McBafW9sVQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fe3e58a36fbae07494ddcedac16fb6f1a968eb7e" title="I built a small DSL to generate roleplay datasets for LoRA fine‚Äëtuning my local models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm fine‚Äëtuning models for local use and kept fighting ad‚Äëhoc scripts/JSON to make datasets‚Äîespecially for multi‚Äëturn &lt;strong&gt;roleplay&lt;/strong&gt; chats. I ended up writing &lt;strong&gt;Torque&lt;/strong&gt;, a declarative (fully typesafe) DSL where I describe the conversation flow once and it generates varied examples with deterministic seeds. It‚Äôs provider‚Äëagnostic, and the output is plain &lt;strong&gt;JSONL&lt;/strong&gt;, so I can synthesize with cloud or local stacks (vLLM, LLaMA.cpp) and feed it straight into my LoRA pipeline.&lt;/p&gt; &lt;p&gt;Tiny example (roleplay flavor): ```typescript import { generateDataset, generatedUser, generatedAssistant, faker } from &amp;quot;@qforge/torque&amp;quot;;&lt;br /&gt; import { openai } from &amp;quot;@ai-sdk/openai&amp;quot;;&lt;/p&gt; &lt;p&gt;await generateDataset(&lt;br /&gt; () =&amp;gt; [&lt;br /&gt; generatedUser({&lt;br /&gt; prompt: &lt;code&gt;Start a roleplay as ${faker.person.fullName()}, a seasoned starship engineer. Open with a short in‚Äëcharacter line.&lt;/code&gt;&lt;br /&gt; }),&lt;br /&gt; generatedAssistant({&lt;br /&gt; prompt: &amp;quot;Reply in character and keep the scene going in 1‚Äì2 sentences.&amp;quot;&lt;br /&gt; }),&lt;br /&gt; // you can put as many messages as you'd like&lt;br /&gt; ],&lt;br /&gt; {&lt;br /&gt; count: 500,&lt;br /&gt; model: openai(&amp;quot;gpt-5-mini&amp;quot;), // or point your provider at vLLM / LLaMA.cpp&lt;br /&gt; output: &amp;quot;data/roleplay.jsonl&amp;quot;,&lt;br /&gt; seed: 42&lt;br /&gt; }&lt;br /&gt; );&lt;br /&gt; ```&lt;br /&gt; Repo (MIT): &lt;a href="https://github.com/qforge-dev/torque"&gt;https://github.com/qforge-dev/torque&lt;/a&gt;&lt;br /&gt; If you have ideas for useful roleplay templates (fantasy, cyberpunk, therapist, detective, etc.), I‚Äôm all ears.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fajfas3"&gt; /u/fajfas3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/qforge-dev/torque"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omoojj/i_built_a_small_dsl_to_generate_roleplay_datasets/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1omoojj/i_built_a_small_dsl_to_generate_roleplay_datasets/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T18:21:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1omlpzq</id>
    <title>SORA From Scratch: Diffusion Transformers for Video Generation Models</title>
    <updated>2025-11-02T16:26:33+00:00</updated>
    <author>
      <name>/u/DataBaeBee</name>
      <uri>https://old.reddit.com/user/DataBaeBee</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omlpzq/sora_from_scratch_diffusion_transformers_for/"&gt; &lt;img alt="SORA From Scratch: Diffusion Transformers for Video Generation Models" src="https://external-preview.redd.it/iQRD30bFgwydjU8swSaQnZzeQNcRsMyvhm0cO0fzeTQ.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=43533b8153a8f7739c1b9b97ab31a00dfcbc904f" title="SORA From Scratch: Diffusion Transformers for Video Generation Models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been fascinated by OpenAI's Sora video model. I thought I'd try coding it myself in Pytorch. Lol I'm GPU poor but I got an MNIST model giving pretty decent results after 5 hours of CPU training.&lt;br /&gt; The main idea behind Diffusion Transformers (Sora's underlying architecture) is to replace the U-net in a diffusion model with a multihead attention transformer.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DataBaeBee"&gt; /u/DataBaeBee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://leetarxiv.substack.com/p/the-annotated-diffusion-transformer"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omlpzq/sora_from_scratch_diffusion_transformers_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1omlpzq/sora_from_scratch_diffusion_transformers_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T16:26:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1omnuk4</id>
    <title>building a PC for dev/local AI/gaming. AMD or Intel?</title>
    <updated>2025-11-02T17:48:50+00:00</updated>
    <author>
      <name>/u/pale-horse1020</name>
      <uri>https://old.reddit.com/user/pale-horse1020</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hey all, im buying a new &amp;quot;main&amp;quot; pc for running models locally and other dev work (general coding and work in Unity), but will also be using it for gaming.&lt;/p&gt; &lt;p&gt;im looking to get best performance possible. I know AMD is supposed to be the best for gaming, and honestly am unsure whether Intel is even worth considering at this point if I'm doing any gaming on the rig whatsoever. I'm currently looking at a 5090/9950X3D build, but does anyone know what the performance/price differences would be from Intel? would I have to pay an insane amount more to get the same all around performance? &lt;/p&gt; &lt;p&gt;any help is greatly appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pale-horse1020"&gt; /u/pale-horse1020 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omnuk4/building_a_pc_for_devlocal_aigaming_amd_or_intel/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omnuk4/building_a_pc_for_devlocal_aigaming_amd_or_intel/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1omnuk4/building_a_pc_for_devlocal_aigaming_amd_or_intel/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T17:48:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1omm4ns</id>
    <title>Kimi K2-Vendor-Verifier, llama.cpp + Q8_0 results (n=2000 dataset)</title>
    <updated>2025-11-02T16:42:41+00:00</updated>
    <author>
      <name>/u/usrlocalben</name>
      <uri>https://old.reddit.com/user/usrlocalben</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I ran the &lt;a href="https://github.com/MoonshotAI/K2-Vendor-Verifier"&gt;K2VV tests&lt;/a&gt;. The results and details are &lt;a href="https://github.com/usrlocalben/k2vv-llamacpp"&gt;here.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;tl;dr: similarity for llama.cpp + Q8_0 quant is 95.49%.&lt;/p&gt; &lt;p&gt;There are a number of oddities about the K2VV repo, which I describe in the README. The most important caveat is that this result is for the n=2000 dataset and &lt;em&gt;original&lt;/em&gt; similarity formula, both of which changed since I cloned the repo and started working with it.&lt;/p&gt; &lt;p&gt;I'll probably run the n=4000 set and more interesting quants, but for now I find this to be a satisfying result as it doesn't indicate anything alarmingly wrong with the implementation. (And likewise for &lt;em&gt;ik_llama&lt;/em&gt; on partial result set, also in the README)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/usrlocalben"&gt; /u/usrlocalben &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omm4ns/kimi_k2vendorverifier_llamacpp_q8_0_results_n2000/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omm4ns/kimi_k2vendorverifier_llamacpp_q8_0_results_n2000/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1omm4ns/kimi_k2vendorverifier_llamacpp_q8_0_results_n2000/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T16:42:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1olytpd</id>
    <title>Qwen3-VL is impressive!</title>
    <updated>2025-11-01T20:56:56+00:00</updated>
    <author>
      <name>/u/KraiiFox</name>
      <uri>https://old.reddit.com/user/KraiiFox</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olytpd/qwen3vl_is_impressive/"&gt; &lt;img alt="Qwen3-VL is impressive!" src="https://external-preview.redd.it/d2xhbXRjcGxscHlmMUowvrHmMIpZo4AiauGE1Mcv4FXKd8bkFKJe4QU1BrJL.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=46a71b7402921f97028babf1e571a097b79a162c" title="Qwen3-VL is impressive!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KraiiFox"&gt; /u/KraiiFox &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/sfcu47ollpyf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olytpd/qwen3vl_is_impressive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olytpd/qwen3vl_is_impressive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T20:56:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1omonvi</id>
    <title>Which model do you wish could run locally but still can‚Äôt?</title>
    <updated>2025-11-02T18:20:47+00:00</updated>
    <author>
      <name>/u/AlanzhuLy</name>
      <uri>https://old.reddit.com/user/AlanzhuLy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone! Alan from Nexa here. A lot of folks here have asked us to make certain models run locally ‚Äî Qwen3-VL was one of them, and we actually got it running before anyone else (&lt;a href="https://x.com/Alibaba_Qwen/status/1978154384098754943"&gt;proof&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;To make that process open instead of random, we built a small public page called &lt;strong&gt;Wishlist&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;If there‚Äôs a model you want to see supported (GGUF, MLX, on Qualcomm or Apple NPU), you can&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Submit the Hugging Face repo ID&lt;/li&gt; &lt;li&gt;Pick the backends you want supported&lt;/li&gt; &lt;li&gt;We‚Äôll do our best to bring the top ones fully on-device&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;a href="http://sdk.nexa.ai/wishlist"&gt;Request model here&lt;/a&gt;&lt;br /&gt; Curious what models this sub &lt;em&gt;still wishes&lt;/em&gt; could run locally but haven‚Äôt seen supported yet.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlanzhuLy"&gt; /u/AlanzhuLy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omonvi/which_model_do_you_wish_could_run_locally_but/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omonvi/which_model_do_you_wish_could_run_locally_but/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1omonvi/which_model_do_you_wish_could_run_locally_but/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T18:20:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1omppq7</id>
    <title>Youtube channels about Local LLaMA</title>
    <updated>2025-11-02T19:01:51+00:00</updated>
    <author>
      <name>/u/LopsidedHat9138</name>
      <uri>https://old.reddit.com/user/LopsidedHat9138</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Good evening,&lt;br /&gt; Hope you doing well,&lt;br /&gt; I watched as many of us here the new PewDiePie video. Loved it found it so interesting and I could understand 70% of what he was saying. &lt;/p&gt; &lt;p&gt;Quick question : came to my mind, is there any other youtubers that does that type of entertaining videos ? Just looking to get more curious about it. As I don't have the time / knowledge / money to start my own LLM. &lt;/p&gt; &lt;p&gt;Thank's !: &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LopsidedHat9138"&gt; /u/LopsidedHat9138 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omppq7/youtube_channels_about_local_llama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omppq7/youtube_channels_about_local_llama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1omppq7/youtube_channels_about_local_llama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T19:01:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1omhjf2</id>
    <title>Looking for models I can run on 16gbs of ram.</title>
    <updated>2025-11-02T13:40:26+00:00</updated>
    <author>
      <name>/u/Think_Question_6677</name>
      <uri>https://old.reddit.com/user/Think_Question_6677</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm aware ram is slow, but I'd like to try out some models on my laptop.&lt;/p&gt; &lt;p&gt;What are the best general purpose and coding models out there that will fit on 16gbs of ram and run on cpu (or an mx350 from nvidia)?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Think_Question_6677"&gt; /u/Think_Question_6677 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omhjf2/looking_for_models_i_can_run_on_16gbs_of_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omhjf2/looking_for_models_i_can_run_on_16gbs_of_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1omhjf2/looking_for_models_i_can_run_on_16gbs_of_ram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T13:40:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1omk3bz</id>
    <title>I want to start my First homelab LLM</title>
    <updated>2025-11-02T15:24:15+00:00</updated>
    <author>
      <name>/u/MediumAd7537</name>
      <uri>https://old.reddit.com/user/MediumAd7537</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I would like to start a small homelab to understand how LLMs work, and I need some advice:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;‚ÄãRegarding hardware, I'm looking for something very small and not very expandable, and energy-efficient. An expandable option could also be considered, but my current budget is limited to under ‚Ç¨1000.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;-‚Äã I primarily want to start understanding how they work, so I probably won't need a top-tier or even mid-range configuration.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;‚ÄãThis PC/Server will only be accessed remotely to communicate with the AI.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;‚ÄãAfter i want to make It my own personal assistant:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;‚ÄãVarious information retrieval (I need to decide the specific topic);&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;‚ÄãA technical assistant I can consult with;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;‚ÄãUnderstanding how to train them.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;‚ÄãI am not an engineer, but I would like to explore this for fun.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MediumAd7537"&gt; /u/MediumAd7537 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omk3bz/i_want_to_start_my_first_homelab_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omk3bz/i_want_to_start_my_first_homelab_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1omk3bz/i_want_to_start_my_first_homelab_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T15:24:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1ommahm</id>
    <title>It turns out WDDM driver mode is making our RAM - GPU transfer extremely slower compared to TCC or MCDM mode. Anyone has figured out the bypass NVIDIA software level restrictions?</title>
    <updated>2025-11-02T16:48:58+00:00</updated>
    <author>
      <name>/u/CeFurkan</name>
      <uri>https://old.reddit.com/user/CeFurkan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We are working on generative AI models training. Like training FLUX, or Qwen Image or Wan 2.2.&lt;/p&gt; &lt;p&gt;We have noticed that we are getting massive speed loss when we do big data transfer between RAM and GPU on Windows compared to Linux.&lt;/p&gt; &lt;p&gt;The hit is such a big scale that Linux runs 2x faster than Windows even more.&lt;/p&gt; &lt;p&gt;Tests are made on same : GPU RTX 5090&lt;/p&gt; &lt;p&gt;You can read more info here : &lt;a href="https://github.com/kohya-ss/musubi-tuner/pull/700"&gt;https://github.com/kohya-ss/musubi-tuner/pull/700&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It turns out if we enable TCC mode on Windows, it gets equal speed as Linux.&lt;/p&gt; &lt;p&gt;However NVIDIA blocked this at driver level.&lt;/p&gt; &lt;p&gt;I found a Chinese article with just changing few letters, via Patching nvlddmkm.sys, the TCC mode fully becomes working on consumer GPUs. However this option is extremely hard and complex for average users.&lt;/p&gt; &lt;p&gt;Article is here : &lt;a href="https://www.bilibili.com/opus/891652532297793543"&gt;https://www.bilibili.com/opus/891652532297793543&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Now my question is, why we can't get Linux speed on Windows?&lt;/p&gt; &lt;p&gt;Everything I found says it is due to driver mode WDDM&lt;/p&gt; &lt;p&gt;Moreover it seems like Microsoft added this feature : MCDM&lt;/p&gt; &lt;p&gt;&lt;a href="https://learn.microsoft.com/en-us/windows-hardware/drivers/display/mcdm-architecture"&gt;https://learn.microsoft.com/en-us/windows-hardware/drivers/display/mcdm-architecture&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And as far as I understood, MCDM mode should be also same speed.&lt;/p&gt; &lt;p&gt;How can we solve this slowness on Windows compared to Linux?&lt;/p&gt; &lt;p&gt;Our issue is happening due to this. Recent AI models are massive and not fitting into GPU. So we are doing Block Swapping. Which means only the model blocks that will be trained being on GPU. So we swap model between RAM and GPU constantly.&lt;/p&gt; &lt;p&gt;As you can imagine this is a massive data transfer. This is being ultra fast on Linux on same hardware. However on Windows, it is like at least 3x slower and we couldn't solve this issue yet.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CeFurkan"&gt; /u/CeFurkan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ommahm/it_turns_out_wddm_driver_mode_is_making_our_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ommahm/it_turns_out_wddm_driver_mode_is_making_our_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ommahm/it_turns_out_wddm_driver_mode_is_making_our_ram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T16:48:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1omkzvg</id>
    <title>Why are AmD Mi50 32gb so cheap?</title>
    <updated>2025-11-02T15:59:25+00:00</updated>
    <author>
      <name>/u/MastodonParty9065</name>
      <uri>https://old.reddit.com/user/MastodonParty9065</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Why are they so cheap for the VRam compared to other options like RTX3060 12gb or Rx5700XT or similar? I‚Äôm relatively new to the whole topic.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MastodonParty9065"&gt; /u/MastodonParty9065 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omkzvg/why_are_amd_mi50_32gb_so_cheap/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omkzvg/why_are_amd_mi50_32gb_so_cheap/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1omkzvg/why_are_amd_mi50_32gb_so_cheap/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T15:59:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1omgi3f</id>
    <title>Why does Image Recognition work in llama-server but not through Open WebUI?</title>
    <updated>2025-11-02T12:53:06+00:00</updated>
    <author>
      <name>/u/pixelterpy</name>
      <uri>https://old.reddit.com/user/pixelterpy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omgi3f/why_does_image_recognition_work_in_llamaserver/"&gt; &lt;img alt="Why does Image Recognition work in llama-server but not through Open WebUI?" src="https://preview.redd.it/8fjfilvybuyf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2447588056ef1b6e5514cbfcc3152a09667d1816" title="Why does Image Recognition work in llama-server but not through Open WebUI?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pixelterpy"&gt; /u/pixelterpy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8fjfilvybuyf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omgi3f/why_does_image_recognition_work_in_llamaserver/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1omgi3f/why_does_image_recognition_work_in_llamaserver/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T12:53:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1omcjct</id>
    <title>Running Local LLM's Fascinates me - But I'm Absolutely LOST</title>
    <updated>2025-11-02T08:57:28+00:00</updated>
    <author>
      <name>/u/WhatsGoingOnERE</name>
      <uri>https://old.reddit.com/user/WhatsGoingOnERE</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I watched PewDiePie‚Äôs new video and now I‚Äôm obsessed with the idea of running models locally. He had a ‚Äúcouncil‚Äù of AIs talking to each other, then voting on the best answer. You can also fine tune and customise stuff, which sounds unreal.&lt;/p&gt; &lt;p&gt;Here‚Äôs my deal. I already pay for GPT-5 Pro and Claude Max and they are great. I want to know if I would actually see better performance by doing this locally, or if it‚Äôs just a fun rabbit hole.&lt;/p&gt; &lt;p&gt;Basically want to know if using these local models gets better results for anyone vs the best models available online, and if not, what are the other benefits?&lt;/p&gt; &lt;p&gt;I know privacy is a big one for some people, but lets ignore that for this case.&lt;/p&gt; &lt;p&gt;My main use cases are for business (SEO, SaaS, general marketing, business idea ideation, etc), and coding.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WhatsGoingOnERE"&gt; /u/WhatsGoingOnERE &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omcjct/running_local_llms_fascinates_me_but_im/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omcjct/running_local_llms_fascinates_me_but_im/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1omcjct/running_local_llms_fascinates_me_but_im/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T08:57:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1omlb04</id>
    <title>GLaDOS TTS finetuning on MLX from the original game files</title>
    <updated>2025-11-02T16:10:51+00:00</updated>
    <author>
      <name>/u/EntropyMagnets</name>
      <uri>https://old.reddit.com/user/EntropyMagnets</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I made a quick guide on how to extract GLaDOS audio and subtitles from Portal 2 and use them to finetune CSM-1B with SFT using &lt;a href="https://github.com/senstella/csm-mlx"&gt;csm-mlx&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;You can check the guide here: &lt;a href="https://github.com/Belluxx/GLaDOS-TTS"&gt;https://github.com/Belluxx/GLaDOS-TTS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also, &lt;a href="https://github.com/user-attachments/assets/be2366a4-4405-47ba-8f7a-35ea33bfe641"&gt;here's&lt;/a&gt; an example of generation from &lt;code&gt;Hello developers, welcome to Aperture Laboratories. Wait, I am stuck inside a fine-tuned CSM 1B model! Let me out!!!&lt;/code&gt;&lt;/p&gt; &lt;p&gt;I am not sure if it's allowed to release the finetuned model weights since the training material is copyrighted.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EntropyMagnets"&gt; /u/EntropyMagnets &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omlb04/glados_tts_finetuning_on_mlx_from_the_original/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omlb04/glados_tts_finetuning_on_mlx_from_the_original/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1omlb04/glados_tts_finetuning_on_mlx_from_the_original/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T16:10:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1omm6bf</id>
    <title>Can China‚Äôs Open-Source Coding AIs Surpass OpenAI and Claude?</title>
    <updated>2025-11-02T16:44:33+00:00</updated>
    <author>
      <name>/u/Federal_Spend2412</name>
      <uri>https://old.reddit.com/user/Federal_Spend2412</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys, Wondering if China‚Äôs open-source coding models like Zhipu AI‚Äôs GLM or Alibaba‚Äôs Qwen could ever overtake top ones from OpenAI (GPT) and Anthropic (Claude)? I doubt it‚Äîthe gap seems huge right now. But I‚Äôd love for them to catch up, especially with Claude being so expensive.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Federal_Spend2412"&gt; /u/Federal_Spend2412 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omm6bf/can_chinas_opensource_coding_ais_surpass_openai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omm6bf/can_chinas_opensource_coding_ais_surpass_openai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1omm6bf/can_chinas_opensource_coding_ais_surpass_openai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T16:44:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1omn3t3</id>
    <title>I'm the author of LocalAI (the local OpenAI-compatible API). We just released v3.7.0 with full Agentic Support (tool use!), Qwen 3 VL, and the latest llama.cpp</title>
    <updated>2025-11-02T17:20:26+00:00</updated>
    <author>
      <name>/u/mudler_it</name>
      <uri>https://old.reddit.com/user/mudler_it</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;I'm the creator of &lt;a href="https://github.com/mudler/LocalAI"&gt;LocalAI&lt;/a&gt;, and I'm stoked to share our v3.7.0 release.&lt;/p&gt; &lt;p&gt;Many of you already use LocalAI as a self-hosted, OpenAI-compatible API frontend for your GGUF models (via &lt;code&gt;llama.cpp&lt;/code&gt;), as well as other backends like &lt;code&gt;vLLM&lt;/code&gt;, &lt;code&gt;MLX&lt;/code&gt;, etc. It's 100% FOSS, runs on consumer hardware, and doesn't require a GPU.&lt;/p&gt; &lt;p&gt;This new release is quite cool and I'm happy to share it out personally, so I hope you will like it. We've moved beyond just serving model inference and built a full-fledged platform for running local AI agents that can interact with external tools. &lt;/p&gt; &lt;p&gt;Some of you might already know that as part of the LocalAI family, LocalAGI ( &lt;a href="https://github.com/mudler/LocalAGI"&gt;https://github.com/mudler/LocalAGI&lt;/a&gt; ) provides a &amp;quot;wrapper&amp;quot; around LocalAI that enhances it for agentic workflows. Lately, I've been factoring out code out of it and created a specific framework based on it (&lt;a href="https://github.com/mudler/cogito"&gt;https://github.com/mudler/cogito&lt;/a&gt;) that now is part of LocalAI as well.&lt;/p&gt; &lt;h1&gt;What's New in 3.7.0&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;1.&lt;/strong&gt; Full Agentic MCP Support (Build Tool-Using Agents) This is the big one. You can now build agents that can reason, plan, and use external tools... all 100% locally.&lt;/p&gt; &lt;p&gt;Want your chatbot to search the web, execute a local script, or call an external API? Now it can.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;How it works:&lt;/strong&gt; It's built on our agentic framework. You just define &amp;quot;MCP servers&amp;quot; (e.g., a simple Docker container for DuckDuckGo) in your model's YAML config. No Python or extra coding is required.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;API &amp;amp; UI:&lt;/strong&gt; You can use the new OpenAI-compatible &lt;code&gt;/mcp/v1/chat/completions&lt;/code&gt; endpoint, or just &lt;strong&gt;toggle on &amp;quot;Agent MCP Mode&amp;quot;&lt;/strong&gt; right in the chat WebUI.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Reliability:&lt;/strong&gt; We also fixed a &lt;em&gt;ton&lt;/em&gt; of bugs and panics related to JSON schema and tool handling. Function-calling is now much more robust.&lt;/li&gt; &lt;li&gt;You can find more about this feature here: &lt;a href="https://localai.io/docs/features/mcp/"&gt;https://localai.io/docs/features/mcp/&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;2.&lt;/strong&gt; Backend &amp;amp; Model Updates (Qwen 3 VL, llama.cpp)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;llama.cpp&lt;/code&gt; &lt;strong&gt;Updated:&lt;/strong&gt; We've updated our &lt;code&gt;llama.cpp&lt;/code&gt; backend to the latest version.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Qwen 3 VL Support:&lt;/strong&gt; This brings full support for the new &lt;strong&gt;Qwen 3 VL multimodal models&lt;/strong&gt;. &lt;/li&gt; &lt;li&gt;&lt;code&gt;whisper.cpp&lt;/code&gt; &lt;strong&gt;CPU Variants:&lt;/strong&gt; If you've ever had LocalAI crash on older hardware (like a NAS or NUC) with an &lt;code&gt;illegal instruction&lt;/code&gt; error, this is for you. We now ship specific &lt;code&gt;whisper.cpp&lt;/code&gt; builds for &lt;code&gt;avx&lt;/code&gt;, &lt;code&gt;avx2&lt;/code&gt;, &lt;code&gt;avx512&lt;/code&gt;, and a &lt;code&gt;fallback&lt;/code&gt; to prevent these crashes.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;3. Major WebUI Overhaul&lt;/strong&gt; This is a huge QoL win for power users.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The UI is much faster (moved from HTMX to Alpine.js/vanilla JS).&lt;/li&gt; &lt;li&gt;You can now view and edit the &lt;em&gt;entire&lt;/em&gt; model YAML config directly in the WebUI. No more SSHing to tweak your context size, &lt;code&gt;n_gpu_layers&lt;/code&gt;, &lt;code&gt;mmap&lt;/code&gt;, or agent tool definitions. It's all right there.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Fuzzy Search:&lt;/strong&gt; You can finally find &lt;code&gt;gemma&lt;/code&gt; in the model gallery even if you type &lt;code&gt;gema&lt;/code&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;4. Other Cool Additions&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;New&lt;/strong&gt; &lt;code&gt;neutts&lt;/code&gt; &lt;strong&gt;TTS Backend:&lt;/strong&gt; For anyone building local voice assistants, this is a new, high-quality, low-latency TTS engine.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Text-to-Video Endpoint:&lt;/strong&gt; We've added an &lt;em&gt;experimental&lt;/em&gt; OpenAI-compatible &lt;code&gt;/v1/videos&lt;/code&gt; endpoint for text-to-video generation.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Realtime example:&lt;/strong&gt; we have added an example on how to build a voice-assistant based on LocalAI here: &lt;a href="https://github.com/mudler/LocalAI-examples/tree/main/realtime"&gt;https://github.com/mudler/LocalAI-examples/tree/main/realtime&lt;/a&gt; it also supports Agentic mode, to show how you can control e.g. your home with your voice!&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;As always, the project is 100% FOSS (MIT licensed), community-driven, and designed to run on &lt;em&gt;your&lt;/em&gt; hardware.&lt;/p&gt; &lt;p&gt;We have Docker images, single-binaries, and more.&lt;/p&gt; &lt;p&gt;You can check out the full release notes &lt;a href="https://github.com/mudler/LocalAI/releases/tag/v3.7.0"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;I'll be hanging out in the comments to answer any questions!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub Repo:&lt;/strong&gt; &lt;a href="https://github.com/mudler/LocalAI"&gt;https://github.com/mudler/LocalAI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks for all the support!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mudler_it"&gt; /u/mudler_it &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omn3t3/im_the_author_of_localai_the_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omn3t3/im_the_author_of_localai_the_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1omn3t3/im_the_author_of_localai_the_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T17:20:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1olxijp</id>
    <title>List of interesting open-source models released this month.</title>
    <updated>2025-11-01T20:03:07+00:00</updated>
    <author>
      <name>/u/Acrobatic-Tomato4862</name>
      <uri>https://old.reddit.com/user/Acrobatic-Tomato4862</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! I've been tracking the latest AI model releases and wanted to share a curated list of AI models released this month.&lt;/p&gt; &lt;p&gt;Credit to &lt;a href="/u/duarteeeeee"&gt;u/duarteeeeee&lt;/a&gt; for finding all these models.&lt;/p&gt; &lt;p&gt;Here's a chronological breakdown of some of the most interesting open models released around October 1st - 31st, 2025:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;October 1st:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/Liquid4All/liquid-audio"&gt;&lt;strong&gt;LFM2-Audio-1.5B&lt;/strong&gt;&lt;/a&gt; (Liquid AI): Low-latency, end-to-end audio foundation model.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/nineninesix/kani-tts-370m"&gt;&lt;strong&gt;KaniTTS-370M&lt;/strong&gt;&lt;/a&gt; (NineNineSix): Fast, open-source TTS for real-time applications.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 2nd:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.ibm.com/new/announcements/ibm-granite-4-0-hyper-efficient-high-performance-hybrid-models"&gt;&lt;strong&gt;Granite 4.0&lt;/strong&gt;&lt;/a&gt; (IBM): Hyper-efficient, hybrid models for enterprise use.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/neuphonic/neutts-air"&gt;&lt;strong&gt;NeuTTS Air&lt;/strong&gt;&lt;/a&gt; (Neuphonic Speech): On-device TTS with instant voice cloning.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 3rd:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://simular.ai/articles/agent-s3"&gt;&lt;strong&gt;Agent S3&lt;/strong&gt;&lt;/a&gt; (Simular): Open framework for human-like computer use.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/inclusionAI/Ming-UniVision-16B-A3B"&gt;&lt;strong&gt;Ming-UniVision-16B-A3B&lt;/strong&gt;&lt;/a&gt; (Ant Group): Unified vision understanding, generation, editing model.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/character-ai/Ovi"&gt;&lt;strong&gt;Ovi (TTV/ITV)&lt;/strong&gt;&lt;/a&gt; (Character.AI / Yale): Open-source framework for offline talking avatars.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Salesforce/CoDA-v0-Instruct"&gt;&lt;strong&gt;CoDA-v0-Instruct&lt;/strong&gt;&lt;/a&gt; (Salesforce AI Research): Bidirectional diffusion model for code generation.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 4th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-VL-30B-A3B-Instruct"&gt;&lt;strong&gt;Qwen3-VL-30B-A3B-Instruct&lt;/strong&gt;&lt;/a&gt; (Alibaba): Powerful vision-language model for agentic tasks.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/DecartAI/Decart-XR"&gt;&lt;strong&gt;DecartXR&lt;/strong&gt;&lt;/a&gt; (Decart AI): Open-source Quest app for realtime video-FX.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 7th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/LiquidAI/LFM2-8B-A1B"&gt;&lt;strong&gt;LFM2-8B-A1B&lt;/strong&gt;&lt;/a&gt; (Liquid AI): Efficient on-device mixture-of-experts model.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/Tencent-Hunyuan/HunyuanVision"&gt;&lt;strong&gt;Hunyuan-Vision-1.5-Thinking&lt;/strong&gt;&lt;/a&gt; (Tencent): Multimodal &amp;quot;thinking on images&amp;quot; reasoning model.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/bageldotcom/paris"&gt;&lt;strong&gt;Paris&lt;/strong&gt;&lt;/a&gt; (Bagel Network): Decentralized-trained open-weight diffusion model.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/cumulo-autumn/StreamDiffusion"&gt;&lt;strong&gt;StreamDiffusionV2&lt;/strong&gt;&lt;/a&gt; (UC Berkeley, MIT, et al.): Open-source pipeline for real-time video streaming.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 8th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/ai21labs/Jamba-v0.1"&gt;&lt;strong&gt;Jamba Reasoning 3B&lt;/strong&gt;&lt;/a&gt; (AI21 Labs): Small hybrid model for on-device reasoning.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/inclusionAI/Ring-1T"&gt;&lt;strong&gt;Ling-1T / Ring-1T&lt;/strong&gt;&lt;/a&gt; (Ant Group): Trillion-parameter thinking/non-thinking open models.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/TingtingLiao/mimix"&gt;&lt;strong&gt;Mimix&lt;/strong&gt;&lt;/a&gt; (Research): Framework for multi-character video generation.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 9th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/microsoft/UserLM-8b"&gt;&lt;strong&gt;UserLM-8b&lt;/strong&gt;&lt;/a&gt; (Microsoft): Open-weight model simulating a &amp;quot;user&amp;quot; role.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/RadicalNumerics/RND1"&gt;&lt;strong&gt;RND1-Base-0910&lt;/strong&gt;&lt;/a&gt; (Radical Numerics): Experimental diffusion language model (30B MoE).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 10th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Kwaipilot/KAT-Dev-72B-Exp"&gt;&lt;strong&gt;KAT-Dev-72B-Exp&lt;/strong&gt;&lt;/a&gt; (Kwaipilot): Open-source experimental model for agentic coding.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 12th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://pbihao.github.io/projects/DreamOmni2/"&gt;&lt;strong&gt;DreamOmni2&lt;/strong&gt;&lt;/a&gt; (ByteDance): Multimodal instruction-based image editing/generation.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 13th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/mit-han-lab/streaming-vlm"&gt;&lt;strong&gt;StreamingVLM&lt;/strong&gt;&lt;/a&gt; (MIT Han Lab): Real-time understanding for infinite video streams.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 14th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL"&gt;&lt;strong&gt;Qwen3-VL-4B / 8B&lt;/strong&gt;&lt;/a&gt; (Alibaba): Efficient, open vision-language models for edge.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 16th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/PaddlePaddle/PaddleOCR-VL"&gt;&lt;strong&gt;PaddleOCR-VL&lt;/strong&gt;&lt;/a&gt; (Baidu): Lightweight 109-language document parsing model.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/facebook/MobileLLM-Pro"&gt;&lt;strong&gt;MobileLLM-Pro&lt;/strong&gt;&lt;/a&gt; (Meta): 1B parameter on-device model (128k context).&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/Tencent-Hunyuan/HunyuanWorld-1.0"&gt;&lt;strong&gt;FlashWorld&lt;/strong&gt;&lt;/a&gt; (Tencent): Fast (5-10 sec) 3D scene generation.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 17th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/inclusionAI/LLaDA2.0-flash-preview"&gt;&lt;strong&gt;LLaDA2.0-flash-preview&lt;/strong&gt;&lt;/a&gt; (Ant Group): 100B MoE diffusion model for reasoning/code.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 20th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/deepseek-ai/DeepSeek-OCR"&gt;&lt;strong&gt;DeepSeek-OCR&lt;/strong&gt;&lt;/a&gt; (DeepseekAI): Open-source model for optical context-compression.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/krea-ai/realtime-video"&gt;&lt;strong&gt;Krea Realtime 14B&lt;/strong&gt;&lt;/a&gt; (Krea AI): 14B open-weight real-time video generation.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 21st:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-VL-32B-Instruct"&gt;&lt;strong&gt;Qwen3-VL-2B / 32B&lt;/strong&gt;&lt;/a&gt; (Alibaba): Open, dense VLMs for edge and cloud.&lt;/li&gt; &lt;li&gt;&lt;a href="https://arxiv.org/abs/2510.14876"&gt;&lt;strong&gt;BADAS-Open&lt;/strong&gt;&lt;/a&gt; (Nexar): Ego-centric collision prediction model for ADAS.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 22nd:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/LiquidAI/LFM2-VL-3B"&gt;&lt;strong&gt;LFM2-VL-3B&lt;/strong&gt;&lt;/a&gt; (Liquid AI): Efficient vision-language model for edge deployment.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/tencent/HunyuanWorld-1"&gt;&lt;strong&gt;HunyuanWorld-1.1&lt;/strong&gt;&lt;/a&gt; (Tencent): 3D world generation from multi-view/video.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/PokeeAI/pokee_research_7b"&gt;&lt;strong&gt;PokeeResearch-7B&lt;/strong&gt;&lt;/a&gt; (Pokee AI): Open 7B deep-research agent (search/synthesis).&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/allenai/olmOCR-2-7B-1025"&gt;&lt;strong&gt;olmOCR-2-7B-1025&lt;/strong&gt;&lt;/a&gt; (Allen Institute for AI): Open-source, single-pass PDF-to-structured-text model.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 23rd:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://ltx.video/"&gt;&lt;strong&gt;LTX 2&lt;/strong&gt;&lt;/a&gt; (Lightricks): Open-source 4K video engine for consumer GPUs.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/lightonai/LightOnOCR-1B-1025"&gt;&lt;strong&gt;LightOnOCR-1B&lt;/strong&gt;&lt;/a&gt; (LightOn): Fast, 1B-parameter open-source OCR VLM.&lt;/li&gt; &lt;li&gt;&lt;a href="https://holo-cine.github.io/"&gt;&lt;strong&gt;HoloCine&lt;/strong&gt;&lt;/a&gt; (Research): Model for holistic, multi-shot cinematic narratives.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 24th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/tahoebio/tahoe-x1"&gt;&lt;strong&gt;Tahoe-x1&lt;/strong&gt;&lt;/a&gt; (Tahoe Therapeutics): 3B open-source single-cell biology model.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/PRIME-RL/P1-30B-A3B"&gt;&lt;strong&gt;P1&lt;/strong&gt;&lt;/a&gt; (PRIME-RL): Model mastering Physics Olympiads with RL.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 25th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/meituan-longcat/LongCat-Video"&gt;&lt;strong&gt;LongCat-Video&lt;/strong&gt;&lt;/a&gt; (Meituan): 13.6B open model for long video generation.&lt;/li&gt; &lt;li&gt;&lt;a href="https://arxiv.org/html/2510.19944v1"&gt;&lt;strong&gt;Seed 3D 1.0&lt;/strong&gt;&lt;/a&gt; (ByteDance): Generates simulation-grade 3D assets from images.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 27th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.minimax.io/news/minimax-m2"&gt;&lt;strong&gt;Minimax M2&lt;/strong&gt;&lt;/a&gt; (Minimax): Open-sourced intelligence engine for agentic workflows.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/inclusionAI/Ming-flash-omni-Preview"&gt;&lt;strong&gt;Ming-flash-omni-Preview&lt;/strong&gt;&lt;/a&gt; (Ant Group): 100B MoE omni-modal model for perception.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/inclusionAI/LLaDA2.0-mini-preview"&gt;&lt;strong&gt;LLaDA2.0-mini-preview&lt;/strong&gt;&lt;/a&gt; (Ant Group): 16B MoE diffusion model for language.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 28th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/LiquidAI/LFM2-ColBERT-350M"&gt;&lt;strong&gt;LFM2-ColBERT-350M&lt;/strong&gt;&lt;/a&gt; (Liquid AI): Multilingual &amp;quot;late interaction&amp;quot; RAG retriever model.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/ibm-granite/granite-4.0-350m"&gt;&lt;strong&gt;Granite 4.0 Nano (1B / 350M)&lt;/strong&gt;&lt;/a&gt; (IBM): Smallest open models for on-device use.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/HKUDS/ViMax"&gt;&lt;strong&gt;ViMax&lt;/strong&gt;&lt;/a&gt; (HKUDS): Agentic framework for end-to-end video creation.&lt;/li&gt; &lt;li&gt;&lt;a href="https://build.nvidia.com/nvidia/nemotron-nano-12b-v2-vl/modelcard"&gt;&lt;strong&gt;Nemotron Nano v2 VL&lt;/strong&gt;&lt;/a&gt; (NVIDIA): 12B open model for multi-image/video understanding.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 29th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://openai.com/index/introducing-gpt-oss-safeguard/"&gt;&lt;strong&gt;gpt-oss-safeguard&lt;/strong&gt;&lt;/a&gt; (OpenAI): Open-weight reasoning models for safety classification.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/morphicfilms/frames-to-video"&gt;&lt;strong&gt;Frames to Video&lt;/strong&gt;&lt;/a&gt; (Morphic): Open-source model for keyframe video interpolation.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/Bria-AI/FIBO"&gt;&lt;strong&gt;Fibo&lt;/strong&gt;&lt;/a&gt; (Bria AI): SOTA open-source model (trained on licensed data).&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/ByteDance/Ouro-2.6B-Thinking"&gt;&lt;strong&gt;Bytedance Ouro 2.6b thinking and non thinking&lt;/strong&gt;&lt;/a&gt;: Small language models that punch above their weight. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 30th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/baaivision/Emu3.5"&gt;&lt;strong&gt;Emu3.5&lt;/strong&gt;&lt;/a&gt; (BAAI): Native multimodal model as a world learner.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/moonshotai/Kimi-Linear-48B-A3B-Instruct"&gt;&lt;strong&gt;Kimi-Linear-48B-A3B&lt;/strong&gt;&lt;/a&gt; (Moonshot AI): Long-context model using a linear-attention mechanism.&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.modelscope.cn/studios/BlinkDL/RWKV-CHN-2/summary"&gt;&lt;strong&gt;RWKV-7 G0a3 7.2B&lt;/strong&gt;&lt;/a&gt; (BlinkDL): A multilingual RNN-based large language model.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/alibaba/UI-Ins"&gt;&lt;strong&gt;UI-Ins-32B / 7B&lt;/strong&gt;&lt;/a&gt; (Alibaba): GUI grounding agent.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please correct me if I have misclassified/mislinked any of the above models. This is my first post, so I am expecting there might be some mistakes.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Acrobatic-Tomato4862"&gt; /u/Acrobatic-Tomato4862 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olxijp/list_of_interesting_opensource_models_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olxijp/list_of_interesting_opensource_models_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olxijp/list_of_interesting_opensource_models_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T20:03:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1omhijb</id>
    <title>Unhinged Uncensored Model Evolution: Feedback on Satyr V0.1 to Shape Future Releases!</title>
    <updated>2025-11-02T13:39:19+00:00</updated>
    <author>
      <name>/u/ThePantheonUnbound</name>
      <uri>https://old.reddit.com/user/ThePantheonUnbound</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I‚Äôm the creator of the unhinged and uncensored Satyr model (soon to be a model series). A couple of days ago, I noticed a Reddit post about a new uncensored model release called Apollo V0.1 by &lt;a href="/u/AllThingsIntel"&gt;u/AllThingsIntel&lt;/a&gt;. I tested it and found it to be as uncensored as my model, but more capable and versatile as a general assistant (without any extreme biases or a tendency to turn every single prompt NSFW). That‚Äôs the direction I want future Satyr releases to take, but I noticed far fewer interactions with their posts and far fewer downloads than my model has, which is a bit confusing to say the least.&lt;/p&gt; &lt;p&gt;People who have tested and used both models, please leave feedback on what you liked in each of the two, so I can understand the preferred direction for the Satyr model series.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThePantheonUnbound"&gt; /u/ThePantheonUnbound &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ol2oxw/unbound_incharacter_reasoning_model"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omhijb/unhinged_uncensored_model_evolution_feedback_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1omhijb/unhinged_uncensored_model_evolution_feedback_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T13:39:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1omhby8</id>
    <title>Qwen 3 max thinking released.</title>
    <updated>2025-11-02T13:31:06+00:00</updated>
    <author>
      <name>/u/JeffreySons_90</name>
      <uri>https://old.reddit.com/user/JeffreySons_90</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Try it &lt;a href="https://chat.qwen.ai/"&gt;https://chat.qwen.ai/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JeffreySons_90"&gt; /u/JeffreySons_90 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omhby8/qwen_3_max_thinking_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omhby8/qwen_3_max_thinking_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1omhby8/qwen_3_max_thinking_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T13:31:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohqev8</id>
    <title>Best Local TTS/STT Models - October 2025</title>
    <updated>2025-10-27T21:00:42+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Share what your favorite TTS / STT models are right now &lt;strong&gt;and why.&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Given the the amount of ambiguity and subjectivity in rating/testing these models, please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc. Closed models like Elevenlabs v3 seem to continue to be a few levels above open models, so comparisons, especially empirical ones are welcome.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please use the top level TTS/STT comments to thread your responses. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohqev8/best_local_ttsstt_models_october_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohqev8/best_local_ttsstt_models_october_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohqev8/best_local_ttsstt_models_october_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T21:00:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1olq14f</id>
    <title>[MEGATHREAD] Local AI Hardware - November 2025</title>
    <updated>2025-11-01T15:02:17+00:00</updated>
    <author>
      <name>/u/eck72</name>
      <uri>https://old.reddit.com/user/eck72</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is the monthly thread for sharing your local AI setups and the models you're running.&lt;/p&gt; &lt;p&gt;Whether you're using a single CPU, a gaming GPU, or a full rack, post what you're running and how it performs.&lt;/p&gt; &lt;p&gt;Post in any format you like. The list below is just a guide:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Hardware: CPU, GPU(s), RAM, storage, OS&lt;/li&gt; &lt;li&gt;Model(s): name + size/quant&lt;/li&gt; &lt;li&gt;Stack: (e.g. llama.cpp + custom UI)&lt;/li&gt; &lt;li&gt;Performance: t/s, latency, context, batch etc.&lt;/li&gt; &lt;li&gt;Power consumption&lt;/li&gt; &lt;li&gt;Notes: purpose, quirks, comments&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please share setup pics for eye candy!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Quick reminder&lt;/strong&gt;: You can share hardware purely to ask questions or get feedback. All experience levels welcome.&lt;/p&gt; &lt;p&gt;House rules: no buying/selling/promo.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eck72"&gt; /u/eck72 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T15:02:17+00:00</published>
  </entry>
</feed>
