<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-15T18:09:29+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1m0muph</id>
    <title>GitHub - restyler/awesome-sandbox: Awesome Code Sandboxing for AI</title>
    <updated>2025-07-15T16:45:13+00:00</updated>
    <author>
      <name>/u/superjet1</name>
      <uri>https://old.reddit.com/user/superjet1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0muph/github_restylerawesomesandbox_awesome_code/"&gt; &lt;img alt="GitHub - restyler/awesome-sandbox: Awesome Code Sandboxing for AI" src="https://external-preview.redd.it/3orai7Y7GblddxnamUhPbryXPrepMmnkXMhTLyT4mF0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1bdeb375d1742cba6be4bec953e2d804fb502e55" title="GitHub - restyler/awesome-sandbox: Awesome Code Sandboxing for AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/superjet1"&gt; /u/superjet1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/restyler/awesome-sandbox"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0muph/github_restylerawesomesandbox_awesome_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m0muph/github_restylerawesomesandbox_awesome_code/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-15T16:45:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1lzl5zk</id>
    <title>UTCP: A safer, scalable tool-calling alternative to MCP</title>
    <updated>2025-07-14T12:33:01+00:00</updated>
    <author>
      <name>/u/juanviera23</name>
      <uri>https://old.reddit.com/user/juanviera23</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzl5zk/utcp_a_safer_scalable_toolcalling_alternative_to/"&gt; &lt;img alt="UTCP: A safer, scalable tool-calling alternative to MCP" src="https://preview.redd.it/wv84vx7h3ucf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=44e4d83d52673aeb1bf507e10f4ab32bff06db95" title="UTCP: A safer, scalable tool-calling alternative to MCP" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/juanviera23"&gt; /u/juanviera23 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wv84vx7h3ucf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzl5zk/utcp_a_safer_scalable_toolcalling_alternative_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lzl5zk/utcp_a_safer_scalable_toolcalling_alternative_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-14T12:33:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1m0gyhy</id>
    <title>We built Explainable AI with pinpointed citations &amp; reasoning ‚Äî works across PDFs, Excel, CSV, Docs &amp; more</title>
    <updated>2025-07-15T12:52:41+00:00</updated>
    <author>
      <name>/u/Effective-Ad2060</name>
      <uri>https://old.reddit.com/user/Effective-Ad2060</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We just added explainability to our RAG pipeline ‚Äî the AI now shows &lt;strong&gt;pinpointed citations&lt;/strong&gt; down to the &lt;strong&gt;exact paragraph, table row, or cell&lt;/strong&gt; it used to generate its answer.&lt;/p&gt; &lt;p&gt;It doesn‚Äôt just name the source file but also &lt;strong&gt;highlights the exact text&lt;/strong&gt; and lets you &lt;strong&gt;jump directly to that part of the document&lt;/strong&gt;. This works across formats: PDFs, Excel, CSV, Word, PowerPoint, Markdown, and more.&lt;/p&gt; &lt;p&gt;It makes AI answers easy to &lt;strong&gt;trust and verify&lt;/strong&gt;, especially in messy or lengthy enterprise files. You also get insight into the &lt;strong&gt;reasoning&lt;/strong&gt; behind the answer.&lt;/p&gt; &lt;p&gt;It‚Äôs fully open-source: &lt;a href="https://github.com/pipeshub-ai/pipeshub-ai"&gt;https://github.com/pipeshub-ai/pipeshub-ai&lt;/a&gt;&lt;br /&gt; Would love to hear your thoughts or feedback!&lt;/p&gt; &lt;p&gt;üìπ Demo: &lt;a href="https://youtu.be/1MPsp71pkVk"&gt;https://youtu.be/1MPsp71pkVk&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Effective-Ad2060"&gt; /u/Effective-Ad2060 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0gyhy/we_built_explainable_ai_with_pinpointed_citations/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0gyhy/we_built_explainable_ai_with_pinpointed_citations/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m0gyhy/we_built_explainable_ai_with_pinpointed_citations/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-15T12:52:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1m0c7am</id>
    <title>Open source LLMs leaderboard</title>
    <updated>2025-07-15T08:19:11+00:00</updated>
    <author>
      <name>/u/oh_my_right_leg</name>
      <uri>https://old.reddit.com/user/oh_my_right_leg</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;Is there a leaderboard for open source LLMs? I know &lt;a href="https://huggingface.co/spaces/opencompass/open_vlm_leaderboard"&gt;this&lt;/a&gt; one for VLMs and there used to be one from HuggingFace, but I think that &lt;a href="https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/?columns=rank%2Cmodel.type_icon%2Cid%2Cmodel.average_score%2Cevaluations.ifeval.normalized_score%2Cevaluations.bbh.normalized_score%2Cevaluations.math.normalized_score%2Cevaluations.gpqa.normalized_score%2Cevaluations.musr.normalized_score%2Cevaluations.mmlu_pro.normalized_score%2Cmetadata.params_billions&amp;amp;official=true"&gt;one&lt;/a&gt; is no longer maintained.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/oh_my_right_leg"&gt; /u/oh_my_right_leg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0c7am/open_source_llms_leaderboard/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0c7am/open_source_llms_leaderboard/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m0c7am/open_source_llms_leaderboard/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-15T08:19:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1m0o6am</id>
    <title>A personal mathematics benchmark (IOQM 2024)</title>
    <updated>2025-07-15T17:33:42+00:00</updated>
    <author>
      <name>/u/Informal_Ad_4172</name>
      <uri>https://old.reddit.com/user/Informal_Ad_4172</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello guys,&lt;/p&gt; &lt;p&gt;I conducted my own personal benchmark of several leading LLMs using problems from the Indian Olympiad Qualifier in Mathematics (IOQM 2024). I wanted to see how they would perform on these challenging math problems (similar to AIME).&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;score&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;gemini-2.5-pro&lt;/td&gt; &lt;td align="left"&gt;100%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;grok-3-mini-high&lt;/td&gt; &lt;td align="left"&gt;95%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;o3-2025-04-16&lt;/td&gt; &lt;td align="left"&gt;95%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;grok-4-0706&lt;/td&gt; &lt;td align="left"&gt;95%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;kimi-k2-0711-preview&lt;/td&gt; &lt;td align="left"&gt;90%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;o4-mini-2025-04-16&lt;/td&gt; &lt;td align="left"&gt;87%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;o3-mini&lt;/td&gt; &lt;td align="left"&gt;87%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;claude-3-7-sonnet-20250219-thinking-32k&lt;/td&gt; &lt;td align="left"&gt;81%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-4.1-2025-04-14&lt;/td&gt; &lt;td align="left"&gt;67%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;claude-opus-4-20250514&lt;/td&gt; &lt;td align="left"&gt;60%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;claude-sonnet-4-20250514&lt;/td&gt; &lt;td align="left"&gt;54%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen-235b-a22b-no-thinking&lt;/td&gt; &lt;td align="left"&gt;54%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;ernie-4.5-300b-r47b&lt;/td&gt; &lt;td align="left"&gt;36%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama-4-scout-17b-16e-instruct&lt;/td&gt; &lt;td align="left"&gt;34%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama-4-maverick-17b-128e-instruct&lt;/td&gt; &lt;td align="left"&gt;30%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;claude-3-5-haiku-20241022&lt;/td&gt; &lt;td align="left"&gt;17%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama-3.3-70b-instruct&lt;/td&gt; &lt;td align="left"&gt;10%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama-3.1-8b-instruct&lt;/td&gt; &lt;td align="left"&gt;7.5%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;What do you all think of these results? A single 5 mark problem sets apart grok-4 and o3 from gemini-2.5-pro and a perfect score. Kimi K2 performs extremely well for a non-reasoning model...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Informal_Ad_4172"&gt; /u/Informal_Ad_4172 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0o6am/a_personal_mathematics_benchmark_ioqm_2024/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0o6am/a_personal_mathematics_benchmark_ioqm_2024/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m0o6am/a_personal_mathematics_benchmark_ioqm_2024/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-15T17:33:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1m0mo2d</id>
    <title>OK, now we're at 1T parameter models, what's the 3090 equivalent way to run them locally?</title>
    <updated>2025-07-15T16:38:09+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Running in VRAM is not affordable, I'm guessing a hybrid setup with a x090 GPU on a server with lots of DRAM makes sense.&lt;/p&gt; &lt;p&gt;But what options are there for decently good RAM servers that are not too expensive?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0mo2d/ok_now_were_at_1t_parameter_models_whats_the_3090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0mo2d/ok_now_were_at_1t_parameter_models_whats_the_3090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m0mo2d/ok_now_were_at_1t_parameter_models_whats_the_3090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-15T16:38:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1m0dqgh</id>
    <title>Open source and free iOS app to chat with your LLMs when you are away from home.</title>
    <updated>2025-07-15T10:00:21+00:00</updated>
    <author>
      <name>/u/Valuable-Run2129</name>
      <uri>https://old.reddit.com/user/Valuable-Run2129</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I made a one-click solution to let anyone run local models on their mac at home and enjoy them from anywhere on their iPhones. &lt;/p&gt; &lt;p&gt;I find myself telling people to run local models instead of using ChatGPT, but the reality is that the whole thing is too complicated for 99.9% of them.&lt;br /&gt; So I made these two companion apps (one for iOS and one for Mac). You just install them and they work. &lt;/p&gt; &lt;p&gt;The Mac app has a selection of Qwen models that run directly on the Mac app with llama.cpp (advanced users can simply ignore those and turn on their Ollama or LMStudio).&lt;br /&gt; The iOS app is a chatbot app like ChatGPT with voice input, attachments with OCR, web search, thinking mode toggle‚Ä¶&lt;br /&gt; The UI is super intuitive for anyone who has ever used a chatbot. &lt;/p&gt; &lt;p&gt;They don't need setting up tailscale or any VPN/tunnel. They work by sending back and forward an iCloud record containing the conversation. Your conversations never leave your private Apple environment. &lt;/p&gt; &lt;p&gt;The only thing that is remotely technical is inserting a Serper API Key in the Mac app to allow web search.&lt;/p&gt; &lt;p&gt;The iOS app is called LLM Pigeon and this is the link:&lt;br /&gt; &lt;a href="https://apps.apple.com/it/app/llm-pigeon/id6746935952?l=en-GB"&gt;https://apps.apple.com/it/app/llm-pigeon/id6746935952?l=en-GB&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The MacOS app is called LLM Pigeon Server and this is the link:&lt;br /&gt; &lt;a href="https://apps.apple.com/it/app/llm-pigeon-server/id6746935822?l=en-GB&amp;amp;mt=12"&gt;https://apps.apple.com/it/app/llm-pigeon-server/id6746935822?l=en-GB&amp;amp;mt=12&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Valuable-Run2129"&gt; /u/Valuable-Run2129 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0dqgh/open_source_and_free_ios_app_to_chat_with_your/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0dqgh/open_source_and_free_ios_app_to_chat_with_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m0dqgh/open_source_and_free_ios_app_to_chat_with_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-15T10:00:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1m0eq11</id>
    <title>Whisper.cpp Node.js Addon with Vulkan Support</title>
    <updated>2025-07-15T10:58:46+00:00</updated>
    <author>
      <name>/u/Kutalia</name>
      <uri>https://old.reddit.com/user/Kutalia</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;üåã Introducing my first (open-source) NPM package: Whisper Node Addon.&lt;br /&gt; It allows to transcribe audio with Whisper.cpp straight in your Node.js environment after just installing it, no manual configuration or compilation needed. Not only that, it comes with scripts if you wish to build your binaries manually.‚Äç&lt;/p&gt; &lt;p&gt;üî• And the biggest part? It supports GPU acceleration through Vulkan API (or Metal on Apple systems), effectively making real-time transcriptions possible with a decent hardware. If you don't have a GPU or you mind using it (while gaming, for example, to save resources), you can always fall back to CPU usage with a single option.&lt;/p&gt; &lt;p&gt;‚öôÔ∏è To make all of this possible, I have forked previous works by others and improved upon the addon source in C++, typing (TypeScript), CI/CD (Github Actions) and many other aspects.&lt;/p&gt; &lt;p&gt;Get prebuilt binaries at:&lt;br /&gt; &lt;a href="https://www.npmjs.com/package/@kutalia/whisper-node-addon"&gt;https://www.npmjs.com/package/@kutalia/whisper-node-addon&lt;/a&gt;&lt;br /&gt; Source code:&lt;br /&gt; &lt;a href="https://github.com/Kutalia/whisper-node-addon"&gt;https://github.com/Kutalia/whisper-node-addon&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kutalia"&gt; /u/Kutalia &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0eq11/whispercpp_nodejs_addon_with_vulkan_support/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0eq11/whispercpp_nodejs_addon_with_vulkan_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m0eq11/whispercpp_nodejs_addon_with_vulkan_support/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-15T10:58:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1m0115d</id>
    <title>Meta on track to be first lab with a 1GW supercluster</title>
    <updated>2025-07-14T22:43:36+00:00</updated>
    <author>
      <name>/u/jd_3d</name>
      <uri>https://old.reddit.com/user/jd_3d</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0115d/meta_on_track_to_be_first_lab_with_a_1gw/"&gt; &lt;img alt="Meta on track to be first lab with a 1GW supercluster" src="https://preview.redd.it/584vdadc4xcf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8e603dc0a062f5e964b5a1e007efdb4a66dc293f" title="Meta on track to be first lab with a 1GW supercluster" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jd_3d"&gt; /u/jd_3d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/584vdadc4xcf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0115d/meta_on_track_to_be_first_lab_with_a_1gw/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m0115d/meta_on_track_to_be_first_lab_with_a_1gw/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-14T22:43:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1m0cgmc</id>
    <title>Cognition, maker of the AI coding agent Devin, acquires Windsurf</title>
    <updated>2025-07-15T08:37:43+00:00</updated>
    <author>
      <name>/u/FullstackSensei</name>
      <uri>https://old.reddit.com/user/FullstackSensei</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0cgmc/cognition_maker_of_the_ai_coding_agent_devin/"&gt; &lt;img alt="Cognition, maker of the AI coding agent Devin, acquires Windsurf" src="https://external-preview.redd.it/xjif0n1LUAq81GjPtgqYQLxRcOem8kNx4gYhgzVoXdw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=045e7a9473212511b88ba864670fee8b5d269a71" title="Cognition, maker of the AI coding agent Devin, acquires Windsurf" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The announcement comes just days after Google hired away Windsurf‚Äôs CEO Varun Mohan, co-founder Douglas Chen, and research leaders in a $2.4 billion reverse-acquihire that left much of the startup‚Äôs 250-person team behind. Google‚Äôs deal occurred just hours after OpenAI‚Äôs $3 billion offer to acquire Windsurf expired, clearing the way for the AI coding startup to explore other options.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullstackSensei"&gt; /u/FullstackSensei &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://techcrunch.com/2025/07/14/cognition-maker-of-the-ai-coding-agent-devin-acquires-windsurf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0cgmc/cognition_maker_of_the_ai_coding_agent_devin/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m0cgmc/cognition_maker_of_the_ai_coding_agent_devin/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-15T08:37:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1m0hgtt</id>
    <title>Why LangGraph overcomplicates AI agents (and my Go alternative)</title>
    <updated>2025-07-15T13:14:58+00:00</updated>
    <author>
      <name>/u/Historical_Wing_9573</name>
      <uri>https://old.reddit.com/user/Historical_Wing_9573</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After my &lt;a href="https://vitaliihonchar.com/insights/how-to-build-react-agent"&gt;LangGraph problem analysis&lt;/a&gt; gained significant traction, I kept digging into why AI agent development feels so unnecessarily complex.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The fundamental issue:&lt;/strong&gt; LangGraph treats programming language control flow as a problem to solve, when it's actually the solution.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What LangGraph does:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Vertices = business logic&lt;/li&gt; &lt;li&gt;Edges = control flow&lt;/li&gt; &lt;li&gt;Runtime graph compilation and validation&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What any programming language already provides:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Functions = business logic&lt;/li&gt; &lt;li&gt;if/else = control flow&lt;/li&gt; &lt;li&gt;Compile-time validation&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;My realization:&lt;/strong&gt; An AI agent is just this pattern:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;for { response := callLLM(context) if response.ToolCalls { context = executeTools(response.ToolCalls) } if response.Finished { return } } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;So I built&lt;/strong&gt; &lt;a href="https://github.com/vitalii-honchar/go-agent"&gt;&lt;strong&gt;go-agent&lt;/strong&gt;&lt;/a&gt; - no graphs, no abstractions, just native Go:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Type safety:&lt;/strong&gt; Catch errors at compile time, not runtime&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Performance:&lt;/strong&gt; True parallelism, no Python GIL&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Simplicity:&lt;/strong&gt; Standard control flow, no graph DSL to learn&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Production-ready:&lt;/strong&gt; Built for infrastructure workloads&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The developer experience focuses on what matters:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Define tools with type safety&lt;/li&gt; &lt;li&gt;Write behavior prompts&lt;/li&gt; &lt;li&gt;Let the library handle ReAct implementation&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Current status:&lt;/strong&gt; Active development, MIT licensed, API stabilizing before v1.0.0&lt;/p&gt; &lt;p&gt;Full technical analysis: &lt;a href="https://vitaliihonchar.com/insights/go-ai-agent-library"&gt;Why LangGraph Overcomplicates AI Agents&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thoughts? Especially interested in feedback from folks who've hit similar walls with Python-based agent frameworks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Historical_Wing_9573"&gt; /u/Historical_Wing_9573 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0hgtt/why_langgraph_overcomplicates_ai_agents_and_my_go/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0hgtt/why_langgraph_overcomplicates_ai_agents_and_my_go/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m0hgtt/why_langgraph_overcomplicates_ai_agents_and_my_go/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-15T13:14:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1lzywie</id>
    <title>Kimi K2 tops creative writing benchmark</title>
    <updated>2025-07-14T21:19:11+00:00</updated>
    <author>
      <name>/u/fictionlive</name>
      <uri>https://old.reddit.com/user/fictionlive</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzywie/kimi_k2_tops_creative_writing_benchmark/"&gt; &lt;img alt="Kimi K2 tops creative writing benchmark" src="https://preview.redd.it/q48f55vcpwcf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=83d8a4d11cd481b0f3d6a15556baa79acf5df855" title="Kimi K2 tops creative writing benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fictionlive"&gt; /u/fictionlive &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/q48f55vcpwcf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzywie/kimi_k2_tops_creative_writing_benchmark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lzywie/kimi_k2_tops_creative_writing_benchmark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-14T21:19:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1m021nx</id>
    <title>Thank you, Unsloth! You guys are legends!!! (Now I just need 256GB of DDR5)</title>
    <updated>2025-07-14T23:25:45+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m021nx/thank_you_unsloth_you_guys_are_legends_now_i_just/"&gt; &lt;img alt="Thank you, Unsloth! You guys are legends!!! (Now I just need 256GB of DDR5)" src="https://preview.redd.it/nl35mhaybxcf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=427166a43aad977ff4e628d5d89073bd9fd90280" title="Thank you, Unsloth! You guys are legends!!! (Now I just need 256GB of DDR5)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nl35mhaybxcf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m021nx/thank_you_unsloth_you_guys_are_legends_now_i_just/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m021nx/thank_you_unsloth_you_guys_are_legends_now_i_just/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-14T23:25:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1m0ny3k</id>
    <title>My dream project is finally live: An open-source AI voice agent framework.</title>
    <updated>2025-07-15T17:25:32+00:00</updated>
    <author>
      <name>/u/videosdk_live</name>
      <uri>https://old.reddit.com/user/videosdk_live</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey community,&lt;/p&gt; &lt;p&gt;I'm Sagar, co-founder of &lt;strong&gt;VideoSDK&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;I've been working in real-time communication for years, building the infrastructure that powers live voice and video across thousands of applications. But now, as developers push models to communicate in real-time, a new layer of complexity is emerging.&lt;/p&gt; &lt;p&gt;Today, voice is becoming the new UI. We expect agents to feel human, to understand us, respond instantly, and work seamlessly across web, mobile, and even telephony. But developers have been forced to stitch together fragile stacks: STT here, LLM there, TTS somewhere else‚Ä¶ glued with HTTP endpoints and prayer.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;So we built something to solve that.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Today, we're open-sourcing our &lt;strong&gt;AI Voice Agent framework&lt;/strong&gt;, a real-time infrastructure layer built specifically for voice agents. It's production-grade, developer-friendly, and designed to abstract away the painful parts of building real-time, AI-powered conversations.&lt;/p&gt; &lt;p&gt;We are live on Product Hunt today and would be incredibly grateful for your feedback and support.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Product Hunt Link:&lt;/strong&gt; &lt;a href="https://www.producthunt.com/products/video-sdk/launches/voice-agent-sdk"&gt;https://www.producthunt.com/products/video-sdk/launches/voice-agent-sdk&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Here's what it offers:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Build agents in just 10 lines of code&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Plug in any models you like&lt;/strong&gt; - OpenAI, ElevenLabs, Deepgram, and others&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Built-in voice activity detection and turn-taking&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Session-level observability&lt;/strong&gt; for debugging and monitoring&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Global infrastructure&lt;/strong&gt; that scales out of the box&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Works across platforms:&lt;/strong&gt; web, mobile, IoT, and even Unity&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Option to deploy on VideoSDK Cloud&lt;/strong&gt;, fully optimized for low cost and performance&lt;/li&gt; &lt;li&gt;&lt;strong&gt;And most importantly, it's 100% open source&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Most importantly, it's fully open source. We didn't want to create another black box. We wanted to give developers a transparent, extensible foundation they can rely on, and build on top of.&lt;/p&gt; &lt;p&gt;Here is the &lt;strong&gt;Github Repo&lt;/strong&gt;: &lt;a href="https://github.com/videosdk-live/agents"&gt;https://github.com/videosdk-live/agents&lt;/a&gt;&lt;br /&gt; (Please do star the repo to help it reach others as well)&lt;/p&gt; &lt;p&gt;This is the first of several launches we've lined up for the week.&lt;/p&gt; &lt;p&gt;I'll be around all day, would love to hear your feedback, questions, or what you're building next.&lt;/p&gt; &lt;p&gt;Thanks for being here,&lt;/p&gt; &lt;p&gt;Sagar&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/videosdk_live"&gt; /u/videosdk_live &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0ny3k/my_dream_project_is_finally_live_an_opensource_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0ny3k/my_dream_project_is_finally_live_an_opensource_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m0ny3k/my_dream_project_is_finally_live_an_opensource_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-15T17:25:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1m04a20</id>
    <title>EXAONE 4.0 32B</title>
    <updated>2025-07-15T01:06:15+00:00</updated>
    <author>
      <name>/u/minpeter2</name>
      <uri>https://old.reddit.com/user/minpeter2</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m04a20/exaone_40_32b/"&gt; &lt;img alt="EXAONE 4.0 32B" src="https://external-preview.redd.it/8nr2BOfjyJy107kRprOzRDlPGzQeiMZ1zJzNkF1pk6I.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=18202842c69b787ccdb604277c8c0ce21247e4d3" title="EXAONE 4.0 32B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/minpeter2"&gt; /u/minpeter2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-32B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m04a20/exaone_40_32b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m04a20/exaone_40_32b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-15T01:06:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1m0mnjk</id>
    <title>Kimi K2 at ~200 tps on Groq</title>
    <updated>2025-07-15T16:37:37+00:00</updated>
    <author>
      <name>/u/mrfakename0</name>
      <uri>https://old.reddit.com/user/mrfakename0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0mnjk/kimi_k2_at_200_tps_on_groq/"&gt; &lt;img alt="Kimi K2 at ~200 tps on Groq" src="https://external-preview.redd.it/Stw5ew6ARua3PKojcWVyE-uMkKHKq3GsO7UrzRoqJ50.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=da5aa7094d7d761b72304995d549ba6ca5c343e2" title="Kimi K2 at ~200 tps on Groq" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It also works on Groq's free plan&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mrfakename0"&gt; /u/mrfakename0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://console.groq.com/docs/model/moonshotai/kimi-k2-instruct"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0mnjk/kimi_k2_at_200_tps_on_groq/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m0mnjk/kimi_k2_at_200_tps_on_groq/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-15T16:37:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1m0j7w4</id>
    <title>Swiss Open LLM</title>
    <updated>2025-07-15T14:27:35+00:00</updated>
    <author>
      <name>/u/bleeckerj</name>
      <uri>https://old.reddit.com/user/bleeckerj</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In late summer 2025, a publicly developed large language model (LLM) will be released ‚Äî co-created by researchers at EPFL, ETH Zurich, and the Swiss National Supercomputing Centre (CSCS).&lt;/p&gt; &lt;p&gt;This LLM will be fully open: This openness is designed to support broad adoption and foster innovation across science, society, and industry. &lt;/p&gt; &lt;p&gt;A defining feature of the model is its multilingual fluency in over 1,000 languages.&lt;/p&gt; &lt;p&gt;&lt;a href="https://ethz.ch/en/news-and-events/eth-news/news/2025/07/a-language-model-built-for-the-public-good.html"&gt;https://ethz.ch/en/news-and-events/eth-news/news/2025/07/a-language-model-built-for-the-public-good.html&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bleeckerj"&gt; /u/bleeckerj &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0j7w4/swiss_open_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0j7w4/swiss_open_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m0j7w4/swiss_open_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-15T14:27:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1m0nutb</id>
    <title>Totally lightweight local inference...</title>
    <updated>2025-07-15T17:22:09+00:00</updated>
    <author>
      <name>/u/Weary-Wing-6806</name>
      <uri>https://old.reddit.com/user/Weary-Wing-6806</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0nutb/totally_lightweight_local_inference/"&gt; &lt;img alt="Totally lightweight local inference..." src="https://preview.redd.it/r05r0wfvn2df1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8c622b493252fd700bcdd538ffef56559bdbbcd5" title="Totally lightweight local inference..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Weary-Wing-6806"&gt; /u/Weary-Wing-6806 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/r05r0wfvn2df1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0nutb/totally_lightweight_local_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m0nutb/totally_lightweight_local_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-15T17:22:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1m0gvhm</id>
    <title>Study finds AI tools made open source software developers 19 percent slower</title>
    <updated>2025-07-15T12:48:24+00:00</updated>
    <author>
      <name>/u/Educational_Sun_8813</name>
      <uri>https://old.reddit.com/user/Educational_Sun_8813</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Coders spent more time prompting and reviewing AI generations than they saved on coding. &lt;a href="https://arstechnica.com/ai/2025/07/study-finds-ai-tools-made-open-source-software-developers-19-percent-slower/"&gt;https://arstechnica.com/ai/2025/07/study-finds-ai-tools-made-open-source-software-developers-19-percent-slower/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Educational_Sun_8813"&gt; /u/Educational_Sun_8813 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0gvhm/study_finds_ai_tools_made_open_source_software/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0gvhm/study_finds_ai_tools_made_open_source_software/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m0gvhm/study_finds_ai_tools_made_open_source_software/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-15T12:48:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1m0cgnl</id>
    <title>Kimi K2: cheap and fast API access for those who can't run locally</title>
    <updated>2025-07-15T08:37:47+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0cgnl/kimi_k2_cheap_and_fast_api_access_for_those_who/"&gt; &lt;img alt="Kimi K2: cheap and fast API access for those who can't run locally" src="https://external-preview.redd.it/uByFfvtd1L9z8WhbCkOvHhqLd2Est6Gau8RSyoYdbWM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8de1b4b36c00b224fb29471c6864b8730dd4f7f2" title="Kimi K2: cheap and fast API access for those who can't run locally" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you can't run &lt;a href="https://huggingface.co/moonshotai/Kimi-K2-Instruct"&gt;kimi-k2&lt;/a&gt; locally, there are now more providers offering API access. DeepInfra is now the cheapest provider, while Groq is (by far) the fastest at around ~250 tokens per second:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://deepinfra.com/moonshotai/Kimi-K2-Instruct"&gt;https://deepinfra.com/moonshotai/Kimi-K2-Instruct&lt;/a&gt; ($0.55/$2.20 in/out Mtoken)&lt;/li&gt; &lt;li&gt;&lt;a href="https://console.groq.com/docs/model/moonshotai/kimi-k2-instruct"&gt;https://console.groq.com/docs/model/moonshotai/kimi-k2-instruct&lt;/a&gt; ($1/$3 in/out Mtoken, but very fast)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;That makes it cheaper than Claude Haiku 3.5, GPT-4.1 and Gemini 2.5 Pro. Not bad for the best non-thinking model currently publicly available!&lt;/p&gt; &lt;p&gt;It also shows the power of an open weights model with an permissive license: Even if you can't run it yourself, there's a lot more options in API access.&lt;/p&gt; &lt;p&gt;See all providers on OpenRouter: &lt;a href="https://openrouter.ai/moonshotai/kimi-k2"&gt;https://openrouter.ai/moonshotai/kimi-k2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Edit:&lt;/strong&gt; There's also a free variant, but I don't know the details: &lt;a href="https://openrouter.ai/moonshotai/kimi-k2:free"&gt;https://openrouter.ai/moonshotai/kimi-k2:free&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://openrouter.ai/moonshotai/kimi-k2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0cgnl/kimi_k2_cheap_and_fast_api_access_for_those_who/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m0cgnl/kimi_k2_cheap_and_fast_api_access_for_those_who/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-15T08:37:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1m0mg5b</id>
    <title>Least sycophantic AI yet? Kimi K2</title>
    <updated>2025-07-15T16:30:01+00:00</updated>
    <author>
      <name>/u/PrimaryBalance315</name>
      <uri>https://old.reddit.com/user/PrimaryBalance315</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Holy crap this thing has sass. First time I've ever engaged with an AI that replied &amp;quot;No.&amp;quot;&lt;br /&gt; That's it. It was fantastic.&lt;/p&gt; &lt;p&gt;Actually let me grab some lines from the conversation -&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&amp;quot;Thermodynamics kills the romance&amp;quot;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&amp;quot;Everything else is commentary&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&amp;quot;If your 'faith' can be destroyed by a single fMRI paper or a bad meditation session, it's not faith, it's a hypothesis&amp;quot;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&amp;quot;Bridges that don't creak aren't being walked on&amp;quot;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;And my favorite zinger - &lt;strong&gt;&amp;quot;Beautiful scaffolding with no cargo yet&amp;quot;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Fucking Killing it Moonshot. Like this thing never once said &amp;quot;that's interesting&amp;quot; or &amp;quot;great question&amp;quot; - it just went straight for the my intelligence every single time. It's like talking to someone that genuinely doesn't give a shit if you can handle the truth or not. Just pure &amp;quot;Show me or shut up&amp;quot;. It makes me think instead of feeling good about thinking. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PrimaryBalance315"&gt; /u/PrimaryBalance315 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0mg5b/least_sycophantic_ai_yet_kimi_k2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0mg5b/least_sycophantic_ai_yet_kimi_k2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m0mg5b/least_sycophantic_ai_yet_kimi_k2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-15T16:30:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1m0d0vz</id>
    <title>Analyzed 5K+ reddit posts to see how people are actually using AI in their work (other than for coding)</title>
    <updated>2025-07-15T09:15:07+00:00</updated>
    <author>
      <name>/u/yingyn</name>
      <uri>https://old.reddit.com/user/yingyn</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0d0vz/analyzed_5k_reddit_posts_to_see_how_people_are/"&gt; &lt;img alt="Analyzed 5K+ reddit posts to see how people are actually using AI in their work (other than for coding)" src="https://b.thumbs.redditmedia.com/Qky5LMYmgq28yvhGu7XZfILzJYn7CxOqgZAo-mu3Knk.jpg" title="Analyzed 5K+ reddit posts to see how people are actually using AI in their work (other than for coding)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Was keen to figure out how AI was actually being used in the workplace by knowledge workers - have personally heard things ranging from &amp;quot;praise be machine god&amp;quot; to &amp;quot;worse than my toddler&amp;quot;. So here're the findings!&lt;/p&gt; &lt;p&gt;If there're any questions you think we should explore from a data perspective, feel free to drop them in and we'll get to it!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yingyn"&gt; /u/yingyn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1m0d0vz"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0d0vz/analyzed_5k_reddit_posts_to_see_how_people_are/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m0d0vz/analyzed_5k_reddit_posts_to_see_how_people_are/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-15T09:15:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1m0lyjn</id>
    <title>Kimi has impressive coding performance! Even deep into context usage.</title>
    <updated>2025-07-15T16:11:58+00:00</updated>
    <author>
      <name>/u/mattescala</name>
      <uri>https://old.reddit.com/user/mattescala</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! Just wanted to share some thoughts on my experience with the new Kimi K2 model.&lt;/p&gt; &lt;p&gt;Ever since Unsloth released their quantized version of Kimi K2 yesterday, I‚Äôve been giving it a real workout. I‚Äôve mostly been pairing it with Roo Code, and honestly‚Ä¶ I‚Äôm blown away.&lt;/p&gt; &lt;p&gt;Back in March, I built myself a server mainly for coding experiments and to mess around with all sorts of models and setups (definitely not to save money‚Äîlet‚Äôs be real, using the Claude API probably would have been cheaper). But this became a hobby, and I wanted to really get into it.&lt;/p&gt; &lt;p&gt;Up until now, I‚Äôve tried DeepSeek V3, R1, R1 0528‚Äîyou name it. Nothing comes close to what I‚Äôm seeing with Kimi K2 today. Usually, my server was just for quick bug fixes that didn‚Äôt need much context. For anything big or complex, I‚Äôd have to use Claude.&lt;/p&gt; &lt;p&gt;But now that‚Äôs changed. Kimi K2 is handling everything I throw at it, even big, complicated tasks. For example, it‚Äôs making changes to a C++ firmware project‚Äî&lt;em&gt;deep&lt;/em&gt; into a 90,000-token context‚Äîand it‚Äôs nailing the search and replace stuff in Roo Code without getting lost or mixing things up.&lt;/p&gt; &lt;p&gt;Just wanted to share my excitement! Huge thanks to the folks at Moonshot AI for releasing this, and big shoutout to Unsloth and Ik_llama. Seriously, none of this would be possible without you all. You‚Äôre the real MVPs.&lt;/p&gt; &lt;p&gt;If you‚Äôre curious about my setup: I‚Äôm running this on a dual EPYC 7532 server, 512GB of DDR4 RAM (overclocked a bit), and three RTX 3090s.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mattescala"&gt; /u/mattescala &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0lyjn/kimi_has_impressive_coding_performance_even_deep/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0lyjn/kimi_has_impressive_coding_performance_even_deep/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m0lyjn/kimi_has_impressive_coding_performance_even_deep/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-15T16:11:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1m0k22v</id>
    <title>mistralai/Voxtral-Mini-3B-2507 ¬∑ Hugging Face</title>
    <updated>2025-07-15T15:00:20+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0k22v/mistralaivoxtralmini3b2507_hugging_face/"&gt; &lt;img alt="mistralai/Voxtral-Mini-3B-2507 ¬∑ Hugging Face" src="https://external-preview.redd.it/Fqp3ABstOuPD3LEzj5sGjjSlveTWbvbitVuSNOGFlaY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=709cf21da4c4a0ff86a826909c9d3b8548549207" title="mistralai/Voxtral-Mini-3B-2507 ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/mistralai/Voxtral-Mini-3B-2507"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0k22v/mistralaivoxtralmini3b2507_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m0k22v/mistralaivoxtralmini3b2507_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-15T15:00:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1m0g2mk</id>
    <title>Well, if anyone was waiting for Llama 4 Behemoth, it's gone</title>
    <updated>2025-07-15T12:09:02+00:00</updated>
    <author>
      <name>/u/Ok-Elevator5091</name>
      <uri>https://old.reddit.com/user/Ok-Elevator5091</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0g2mk/well_if_anyone_was_waiting_for_llama_4_behemoth/"&gt; &lt;img alt="Well, if anyone was waiting for Llama 4 Behemoth, it's gone" src="https://external-preview.redd.it/460vGqJsGvGsLCuYSDVQ0J6aXCEA7hjiOX3-wBrRv_s.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b7417749598717bd5400069706a3c0d563e32ab4" title="Well, if anyone was waiting for Llama 4 Behemoth, it's gone" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We're likely getting a closed source model instead &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Elevator5091"&gt; /u/Ok-Elevator5091 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://analyticsindiamag.com/global-tech/meta-plans-to-abandon-llama-4-behemoth-but-why/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0g2mk/well_if_anyone_was_waiting_for_llama_4_behemoth/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m0g2mk/well_if_anyone_was_waiting_for_llama_4_behemoth/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-15T12:09:02+00:00</published>
  </entry>
</feed>
