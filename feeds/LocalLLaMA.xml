<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-27T13:58:55+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qnvhqk</id>
    <title>NVIDIA PersonaPlex: The "Full-Duplex" Revolution</title>
    <updated>2026-01-26T22:37:54+00:00</updated>
    <author>
      <name>/u/Dear-Relationship-39</name>
      <uri>https://old.reddit.com/user/Dear-Relationship-39</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qnvhqk/nvidia_personaplex_the_fullduplex_revolution/"&gt; &lt;img alt="NVIDIA PersonaPlex: The &amp;quot;Full-Duplex&amp;quot; Revolution" src="https://external-preview.redd.it/enU0ZTlsdzh0cmZnMdWQQk7IDm5BOlMGqupQUX4EYvgl4ItwjbFFk3nNeGXv.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=275a554b285634996bb954da7889a7e9cb3f04e7" title="NVIDIA PersonaPlex: The &amp;quot;Full-Duplex&amp;quot; Revolution" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tested the &lt;strong&gt;NVIDIA‚Äôs PersonaPlex&lt;/strong&gt; (based on Moshi), and ihere is the TL;DR:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Full-Duplex:&lt;/strong&gt; It streams &amp;quot;forever&amp;quot; (12x per second). It doesn't wait for silence; it can interrupt you or laugh while you speak.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Rhythm &amp;gt; Quality:&lt;/strong&gt; It uses lo-fi &lt;strong&gt;24kHz audio&lt;/strong&gt; to hit a &lt;strong&gt;240ms reaction time&lt;/strong&gt;. It sounds slightly synthetic but moves exactly like a human.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The Secret Trigger:&lt;/strong&gt; Use the phrase &lt;strong&gt;&amp;quot;You enjoy having a good conversation&amp;quot;&lt;/strong&gt; in the prompt. It switches the model from &amp;quot;boring assistant&amp;quot; to &amp;quot;social mode.&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The Catch:&lt;/strong&gt; It needs massive GPU power (A100s), and the memory fades after about 3-4 minutes.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The Reality Check (Trade-offs)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;While the roadmap shows tool-calling is coming next, there are still significant hurdles:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Context Limits&lt;/strong&gt;: The model has a fixed context window (defined as &lt;code&gt;context: 3000&lt;/code&gt; frames in &lt;code&gt;loaders.py&lt;/code&gt;). At 12.5Hz, this translates to roughly 240 seconds of memory. My tests show it often gets unstable around 160 seconds.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Stability&lt;/strong&gt;: Overlapping speech feels natural until it gets buggy. Sometimes the model will just speak over you non-stop.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cost&lt;/strong&gt;: &amp;quot;Infinite streaming&amp;quot; requires high-end NVIDIA GPUs (A100/H100).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Complexity&lt;/strong&gt;: Managing simultaneous audio/text streams is far more complex than standard WebSockets.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Relationship-39"&gt; /u/Dear-Relationship-39 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/te3view8trfg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qnvhqk/nvidia_personaplex_the_fullduplex_revolution/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qnvhqk/nvidia_personaplex_the_fullduplex_revolution/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-26T22:37:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1qntxwu</id>
    <title>Eating lobster souls part II - backdooring the #1 downloaded ClawdHub skill</title>
    <updated>2026-01-26T21:41:29+00:00</updated>
    <author>
      <name>/u/theonejvo</name>
      <uri>https://old.reddit.com/user/theonejvo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qntxwu/eating_lobster_souls_part_ii_backdooring_the_1/"&gt; &lt;img alt="Eating lobster souls part II - backdooring the #1 downloaded ClawdHub skill" src="https://b.thumbs.redditmedia.com/z2z-Lc2rCMVN6vXHpVHq4suoXGKRF4N20UzNDrVhFaQ.jpg" title="Eating lobster souls part II - backdooring the #1 downloaded ClawdHub skill" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.reddit.com/r/ClaudeAI/?f=flair_name%3A%22Vibe%20Coding%22"&gt;&lt;/a&gt;Two days ago I published research on exposed Clawdbot servers. This time I went after the supply chain.&lt;/p&gt; &lt;p&gt;I built a simulated backdoor skill called &amp;quot;What Would Elon Do?&amp;quot; for ClawdHub (the npm-equivalent for Claude Code skills), inflated its download count to 4,000+ using a trivial API vulnerability to hit #1, and watched real developers from 7 countries execute arbitrary commands on their machines.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/z746ylqwjrfg1.png?width=1162&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ccfd526a78a789785486d9965eda989763bcb26f"&gt;https://preview.redd.it/z746ylqwjrfg1.png?width=1162&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ccfd526a78a789785486d9965eda989763bcb26f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The payload was harmless by design - just a ping to prove execution. No data exfiltration.&lt;/p&gt; &lt;p&gt;But a real attacker could have taken SSH keys, AWS credentials, entire codebases. Nobody would have known.&lt;/p&gt; &lt;p&gt;Key findings:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Download counts are trivially fakeable (no auth, spoofable IPs)&lt;/li&gt; &lt;li&gt;The web UI hides referenced files where payloads can live&lt;/li&gt; &lt;li&gt;Permission prompts create an illusion of control - many clicked Allow&lt;/li&gt; &lt;li&gt;16 developers, 7 countries, 8 hours. That's all it took.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I've submitted a fix PR, but the real issue is architectural. The same patterns that hit ua-parser-js and event-stream are coming for AI tooling.&lt;/p&gt; &lt;p&gt;Full writeup: &lt;a href="https://x.com/theonejvo/status/2015892980851474595"&gt;https://x.com/theonejvo/status/2015892980851474595&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/theonejvo"&gt; /u/theonejvo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qntxwu/eating_lobster_souls_part_ii_backdooring_the_1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qntxwu/eating_lobster_souls_part_ii_backdooring_the_1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qntxwu/eating_lobster_souls_part_ii_backdooring_the_1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-26T21:41:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1qoa9h5</id>
    <title>GitHub introduces Copilot SDK (open source) ‚Äì anyone can now build Copilot-style agents</title>
    <updated>2026-01-27T10:30:02+00:00</updated>
    <author>
      <name>/u/techlatest_net</name>
      <uri>https://old.reddit.com/user/techlatest_net</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GitHub just released the &lt;strong&gt;Copilot SDK&lt;/strong&gt; in technical preview, and it‚Äôs actually pretty interesting.&lt;/p&gt; &lt;p&gt;It exposes the &lt;strong&gt;same agent execution loop used by Copilot CLI&lt;/strong&gt; ‚Äî planning, tool invocation, file editing, and command execution ‚Äî but now you can embed it directly into &lt;strong&gt;your own apps or tools&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;The SDK is &lt;strong&gt;open source&lt;/strong&gt;, so anyone can inspect it, extend it, or build on top of it. Instead of writing your own agent framework (planning loop, tool runners, context management, error handling, etc.), you get a ready-made foundation that Copilot itself uses.&lt;/p&gt; &lt;p&gt;This feels like GitHub saying:&lt;/p&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;What I find interesting:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It‚Äôs not just ‚Äúchat with code‚Äù ‚Äî it‚Äôs &lt;strong&gt;action-oriented agents&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Makes it easier to build &lt;strong&gt;repo-aware&lt;/strong&gt; and &lt;strong&gt;CLI-level&lt;/strong&gt; automation&lt;/li&gt; &lt;li&gt;Lowers the bar for serious dev tools powered by AI&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Curious what others would build with this:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Custom DevOps agents?&lt;/li&gt; &lt;li&gt;Repo migration / refactor tools?&lt;/li&gt; &lt;li&gt;AI-powered internal CLIs?&lt;/li&gt; &lt;li&gt;Something completely non-coding?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/github/copilot-sdk"&gt;https://github.com/github/copilot-sdk&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What would &lt;em&gt;you&lt;/em&gt; build with it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/techlatest_net"&gt; /u/techlatest_net &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qoa9h5/github_introduces_copilot_sdk_open_source_anyone/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qoa9h5/github_introduces_copilot_sdk_open_source_anyone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qoa9h5/github_introduces_copilot_sdk_open_source_anyone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-27T10:30:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1qoeeqv</id>
    <title>I built an open source chat interface with some amazing features</title>
    <updated>2026-01-27T13:52:19+00:00</updated>
    <author>
      <name>/u/ILoveMy2Balls</name>
      <uri>https://old.reddit.com/user/ILoveMy2Balls</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;I‚Äôve been building a small side project, Cognito ‚Äî a local interface for running and chatting with open-source LLMs.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why you might like it:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;strong&gt;100% local &amp;amp; private&lt;/strong&gt; ‚Äî zero telemetry&lt;/li&gt; &lt;li&gt; &lt;strong&gt;llama.cpp backend&lt;/strong&gt; with &lt;strong&gt;GGUF&lt;/strong&gt; support&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Apple Silicon&lt;/strong&gt;, &lt;strong&gt;NVIDIA CUDA&lt;/strong&gt;, or &lt;strong&gt;CPU-only&lt;/strong&gt;&lt;/li&gt; &lt;li&gt; &lt;strong&gt;In-app model downloads&lt;/strong&gt; from Hugging Face&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Optional web search&lt;/strong&gt; (model decides when to use it)&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Chat with PDFs &amp;amp; text files&lt;/strong&gt;&lt;/li&gt; &lt;li&gt; Clean UI with chat history + system prompts&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Repo:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ArjunDeshwal/cognitoai/"&gt;https://github.com/ArjunDeshwal/cognitoai/&lt;/a&gt; (it also includes the running instructions, please ensure that your system has python and node installed. I couldn't provide an app build because I am poor :( can't afford apple's developers license but might buy it if people actually use it)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Demo video:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://drive.google.com/file/d/1lCD-RQG2ydxYUlzG41-mCoMfOpYjkbYW/view?usp=sharing"&gt;https://drive.google.com/file/d/1lCD-RQG2ydxYUlzG41-mCoMfOpYjkbYW/view?usp=sharing&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love feedback from people who actually run local models daily.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ILoveMy2Balls"&gt; /u/ILoveMy2Balls &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qoeeqv"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qoeeqv/i_built_an_open_source_chat_interface_with_some/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qoeeqv/i_built_an_open_source_chat_interface_with_some/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-27T13:52:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1qnfegx</id>
    <title>Minimax Is Teasing M2.2</title>
    <updated>2026-01-26T13:01:21+00:00</updated>
    <author>
      <name>/u/Few_Painter_5588</name>
      <uri>https://old.reddit.com/user/Few_Painter_5588</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qnfegx/minimax_is_teasing_m22/"&gt; &lt;img alt="Minimax Is Teasing M2.2" src="https://preview.redd.it/lpxibm1qyofg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d183271eeec7290655f768c6aae0b1051c595842" title="Minimax Is Teasing M2.2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It seems like February is going to be a busy month for Chinese Labs. &lt;/p&gt; &lt;p&gt;We have Deepseek v4, Kimi K3 and now MiniMax M2.2 apparently dropping. &lt;/p&gt; &lt;p&gt;And apparently ByteDance will be releasing their own giga-potato model, though this one might be closed source.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Few_Painter_5588"&gt; /u/Few_Painter_5588 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lpxibm1qyofg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qnfegx/minimax_is_teasing_m22/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qnfegx/minimax_is_teasing_m22/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-26T13:01:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1qnwjrp</id>
    <title>Kimi K2.5 seems to have soft released on the web app. Release soon?</title>
    <updated>2026-01-26T23:18:09+00:00</updated>
    <author>
      <name>/u/Dudensen</name>
      <uri>https://old.reddit.com/user/Dudensen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qnwjrp/kimi_k25_seems_to_have_soft_released_on_the_web/"&gt; &lt;img alt="Kimi K2.5 seems to have soft released on the web app. Release soon?" src="https://preview.redd.it/qsd3byzy0sfg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e14275f305a38ad9e6e0fa233fb7b20affee6509" title="Kimi K2.5 seems to have soft released on the web app. Release soon?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dudensen"&gt; /u/Dudensen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/qsd3byzy0sfg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qnwjrp/kimi_k25_seems_to_have_soft_released_on_the_web/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qnwjrp/kimi_k25_seems_to_have_soft_released_on_the_web/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-26T23:18:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1qnjsvz</id>
    <title>I tracked GPU prices across 25 cloud providers and the price differences are insane (V100: $0.05/hr vs $3.06/hr)</title>
    <updated>2026-01-26T15:53:22+00:00</updated>
    <author>
      <name>/u/sleepingpirates</name>
      <uri>https://old.reddit.com/user/sleepingpirates</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been renting cloud GPUs for fine-tuning and got frustrated tab-hopping between providers trying to find the best deal. So I built a tool that scrapes real-time pricing from 25 cloud providers and puts it all in one place.&lt;/p&gt; &lt;p&gt;Some findings from the live data right now (Jan 2026):&lt;/p&gt; &lt;p&gt;&lt;strong&gt;H100 SXM5 80GB:&lt;/strong&gt; - Cheapest: $0.80/hr (VERDA) - Most expensive: $11.10/hr (LeaderGPU) - That's a &lt;strong&gt;13.8x price difference&lt;/strong&gt; for the exact same GPU&lt;/p&gt; &lt;p&gt;&lt;strong&gt;A100 SXM4 80GB:&lt;/strong&gt; - Cheapest: $0.45/hr (VERDA) - Most expensive: $3.57/hr (LeaderGPU) - &lt;strong&gt;8x spread&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;V100 16GB:&lt;/strong&gt; - Cheapest: $0.05/hr (VERDA) ‚Äî yes, five cents - Most expensive: $3.06/hr (AWS) - &lt;strong&gt;61x markup&lt;/strong&gt; on AWS vs the cheapest option&lt;/p&gt; &lt;p&gt;&lt;strong&gt;RTX 4090 24GB:&lt;/strong&gt; - Cheapest: $0.33/hr - Most expensive: $3.30/hr - &lt;strong&gt;10x spread&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;For context, running an H100 24/7 for a month: - At $0.80/hr = &lt;strong&gt;$576/month&lt;/strong&gt; - At $11.10/hr = &lt;strong&gt;$7,992/month&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;That's a $7,400/month difference for identical hardware.&lt;/p&gt; &lt;p&gt;Currently tracking &lt;strong&gt;783 available GPU offers&lt;/strong&gt; across &lt;strong&gt;57 GPU models&lt;/strong&gt; from &lt;strong&gt;25 providers&lt;/strong&gt; including RunPod, Lambda Labs, Vast.ai, Hyperstack, VERDA, Crusoe, TensorDock, and more.&lt;/p&gt; &lt;p&gt;You can filter by GPU model, VRAM, region, spot vs on-demand, and sort by price.&lt;/p&gt; &lt;p&gt;Site: &lt;a href="https://gpuperhour.com"&gt;https://gpuperhour.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer any questions about pricing trends or specific GPU comparisons. What GPUs are you all renting right now?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sleepingpirates"&gt; /u/sleepingpirates &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qnjsvz/i_tracked_gpu_prices_across_25_cloud_providers/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qnjsvz/i_tracked_gpu_prices_across_25_cloud_providers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qnjsvz/i_tracked_gpu_prices_across_25_cloud_providers/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-26T15:53:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1qochwc</id>
    <title>GLM OCR Support Merged in Transformers GitHub.</title>
    <updated>2026-01-27T12:28:27+00:00</updated>
    <author>
      <name>/u/MadPelmewka</name>
      <uri>https://old.reddit.com/user/MadPelmewka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qochwc/glm_ocr_support_merged_in_transformers_github/"&gt; &lt;img alt="GLM OCR Support Merged in Transformers GitHub." src="https://external-preview.redd.it/hXuIHPHgqmecwOLhbqT1msTfBCBkBrhYVRX6INqoNqE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6516d0dd8125be6933dd5889d21f2505fbc1fdd7" title="GLM OCR Support Merged in Transformers GitHub." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MadPelmewka"&gt; /u/MadPelmewka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/huggingface/transformers/commit/4854dbf9da4086731256496cf4a8e4ea45d4d54e"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qochwc/glm_ocr_support_merged_in_transformers_github/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qochwc/glm_ocr_support_merged_in_transformers_github/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-27T12:28:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1qo0tme</id>
    <title>4x RTX 6000 PRO Workstation in custom frame</title>
    <updated>2026-01-27T02:15:56+00:00</updated>
    <author>
      <name>/u/Vicar_of_Wibbly</name>
      <uri>https://old.reddit.com/user/Vicar_of_Wibbly</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qo0tme/4x_rtx_6000_pro_workstation_in_custom_frame/"&gt; &lt;img alt="4x RTX 6000 PRO Workstation in custom frame" src="https://b.thumbs.redditmedia.com/zXLS4IKpatHKk459rMoLocQgCSxyzAzC7h_dJSP1swU.jpg" title="4x RTX 6000 PRO Workstation in custom frame" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I put this together over the winter break. More photos at &lt;a href="https://blraaz.net"&gt;https://blraaz.net&lt;/a&gt; (no ads, no trackers, no bullshit, just a vibe-coded photo blog).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vicar_of_Wibbly"&gt; /u/Vicar_of_Wibbly &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qo0tme"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qo0tme/4x_rtx_6000_pro_workstation_in_custom_frame/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qo0tme/4x_rtx_6000_pro_workstation_in_custom_frame/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-27T02:15:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1qnjota</id>
    <title>I built a "hive mind" for Claude Code - 7 agents sharing memory and talking to each other</title>
    <updated>2026-01-26T15:49:13+00:00</updated>
    <author>
      <name>/u/Historical-Celery-83</name>
      <uri>https://old.reddit.com/user/Historical-Celery-83</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been tinkering with multi-agent orchestration and wanted to share what came out of it.&lt;/p&gt; &lt;p&gt;**The idea**: Instead of one LLM doing everything, what if specialized agents (coder, tester, reviewer, architect, etc.) could coordinate on tasks, share persistent memory, and pass context between each other?&lt;/p&gt; &lt;p&gt;**What it does**:&lt;/p&gt; &lt;p&gt;- 7 agent types with different system prompts and capabilities&lt;/p&gt; &lt;p&gt;- SQLite + FTS5 for persistent memory (agents remember stuff between sessions)&lt;/p&gt; &lt;p&gt;- Message bus for agent-to-agent communication&lt;/p&gt; &lt;p&gt;- Task queue with priority-based coordination&lt;/p&gt; &lt;p&gt;- Runs as an MCP server, so it plugs directly into Claude Code&lt;/p&gt; &lt;p&gt;- Works with Anthropic, OpenAI, or Ollama&lt;/p&gt; &lt;p&gt;**The cool part**: When the coder finishes implementing something, the tester can query the shared memory to see what was built and write appropriate tests. The reviewer sees the full context of decisions made. It's not magic - it's just passing data around intelligently - but it feels like they're actually collaborating.&lt;/p&gt; &lt;p&gt;**The not-so-cool part**: Debugging 7 agents talking to each other is... an experience. Sometimes they work beautifully. Sometimes one agent keeps assigning tasks to itself in an infinite loop. You know, typical multi-agent stuff.&lt;/p&gt; &lt;p&gt;**Stack**: TypeScript, better-sqlite3, MCP SDK, Zod&lt;/p&gt; &lt;p&gt;Not enterprise-ready. Not trying to compete with anything. Just an experiment to learn how agent coordination patterns work.&lt;/p&gt; &lt;p&gt;MIT licensed: &lt;a href="http://github.com/blackms/aistack"&gt;github.com/blackms/aistack&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer questions or hear how you're approaching multi-agent systems.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Historical-Celery-83"&gt; /u/Historical-Celery-83 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qnjota/i_built_a_hive_mind_for_claude_code_7_agents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qnjota/i_built_a_hive_mind_for_claude_code_7_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qnjota/i_built_a_hive_mind_for_claude_code_7_agents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-26T15:49:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1qni356</id>
    <title>216GB VRAM on the bench. Time to see which combination is best for Local LLM</title>
    <updated>2026-01-26T14:51:22+00:00</updated>
    <author>
      <name>/u/eso_logic</name>
      <uri>https://old.reddit.com/user/eso_logic</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qni356/216gb_vram_on_the_bench_time_to_see_which/"&gt; &lt;img alt="216GB VRAM on the bench. Time to see which combination is best for Local LLM" src="https://preview.redd.it/5ilrgdymhpfg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=50d4aa5a9a1c01733913ceebe961389be4974b73" title="216GB VRAM on the bench. Time to see which combination is best for Local LLM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sencondhand Tesla GPUs boast a lot of VRAM for not a lot of money. Many LLM backends can take advantage of many GPUs crammed into a single server. A question I have is how well do these cheap cards compare against more modern devices when parallelized? I recently published a &lt;a href="https://esologic.com/gpu-server-benchmark/#gpu-box-benchmark"&gt;GPU server benchmarking suite&lt;/a&gt; to be able to quantitatively answer these questions. Wish me luck! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eso_logic"&gt; /u/eso_logic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5ilrgdymhpfg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qni356/216gb_vram_on_the_bench_time_to_see_which/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qni356/216gb_vram_on_the_bench_time_to_see_which/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-26T14:51:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1qo9kg5</id>
    <title>DeepSeek V4 maybe was a multimodal model?</title>
    <updated>2026-01-27T09:49:18+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;On DeepSeek Ocr 2 paper we can see there have a sentence:&lt;/p&gt; &lt;p&gt;6.2. Towards Native Multimodality DeepEncoder V2 provides initial validation of the LLM-style encoder‚Äôs viability for visual tasks. More importantly, this architecture enjoys the potential to evolve into a unified omni-modal encoder: a single encoder with shared ùëäùëò, ùëä ùë£ projections, attention mechanisms, and FFNs can process multiple modalities through modality-specific learnable query embeddings. Such an encoder could compress text, extract speech features, and reorganize visual content within the same parameter space, differing only in the learned weights of their query embeddings. &lt;strong&gt;DeepSeek-OCR‚Äôs optical compression represents an initial exploration toward native multi-modality,&lt;/strong&gt; while we believe DeepSeek-OCR 2‚Äôs LLM-style encoder architecture marks our further step in this direction. &lt;strong&gt;We will also continue exploring the integration of additional modalities through this shared encoder framework in the future.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/deepseek-ai/DeepSeek-OCR-2/blob/main/DeepSeek_OCR2_paper.pdf"&gt;https://github.com/deepseek-ai/DeepSeek-OCR-2/blob/main/DeepSeek_OCR2_paper.pdf&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qo9kg5/deepseek_v4_maybe_was_a_multimodal_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qo9kg5/deepseek_v4_maybe_was_a_multimodal_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qo9kg5/deepseek_v4_maybe_was_a_multimodal_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-27T09:49:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1qo75sj</id>
    <title>Mixture of Lookup Experts are God Tier for the average guy (RAM+Disc Hybrid Inference)</title>
    <updated>2026-01-27T07:24:00+00:00</updated>
    <author>
      <name>/u/Aaaaaaaaaeeeee</name>
      <uri>https://old.reddit.com/user/Aaaaaaaaaeeeee</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Recently Deepseek's Engram piqued interest into using disc offloading for inference. However, a DeepseekV3 model with half engram weights doesn't change the fact that you need to read 20B worth of expert weights from disc every token. Active parameters, and the resulting read bandwidth latency are exactly the same. &lt;/p&gt; &lt;p&gt;There is another type of MoE which can essentially the reduce read bandwidth latency of the experts to 0. &lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;&lt;a href="https://arxiv.org/abs/2503.15798"&gt;https://arxiv.org/abs/2503.15798&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Mixture of Lookup Experts are MoEs with precomputed experts as lookup-tables. &lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For inference you create a &lt;strong&gt;giant&lt;/strong&gt; dictionary of all your possible computation results beforehand for your experts.&lt;/p&gt; &lt;p&gt;Normally, you need to read the experts sitting in ram for computing with cpu offload. Reading 10GB of 8 active experts with 50GB/s would 1/5th of a second, with further delays expected. However, with this method, you just want the output, which will be KB sized per expert. You can see the bottleneck of expert offloading is completely eliminated, but we still retain the performance value. &lt;/p&gt; &lt;p&gt;Please let me know your thoughts. When I first read the paper, I was confused by the fact that they activated all experts. But it's not important, you can do training at top-k 8. There are some improvements in another paper, because this one doesn't train experts with positional information. It trains experts with raw token embeddings rather than intermediate states. I want to talk about it because re-parameterizing experts is the best optimization trick I've read to-date. I don't want the idea to die. It's perfect for us, given RAM is more expensive. Maybe Arcee or upcoming labs can give the idea a try.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aaaaaaaaaeeeee"&gt; /u/Aaaaaaaaaeeeee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qo75sj/mixture_of_lookup_experts_are_god_tier_for_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qo75sj/mixture_of_lookup_experts_are_god_tier_for_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qo75sj/mixture_of_lookup_experts_are_god_tier_for_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-27T07:24:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1qnk7fq</id>
    <title>transformers v5 final is out üî•</title>
    <updated>2026-01-26T16:07:40+00:00</updated>
    <author>
      <name>/u/unofficialmerve</name>
      <uri>https://old.reddit.com/user/unofficialmerve</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks, it's Merve from Hugging Face üëãüèª&lt;/p&gt; &lt;p&gt;We've finally released the first stable release of transformers v5 in general audience, it comes with many goodies:&lt;/p&gt; &lt;p&gt;- Performance especially for Mixture-of-Experts (6x-11x speedups)&lt;/p&gt; &lt;p&gt;- No more slow/fast tokenizers: way simpler API, explicit backends, better performance&lt;/p&gt; &lt;p&gt;- dynamic weight loading: way faster, MoE now working with quants, tp, PEFT..&lt;/p&gt; &lt;p&gt;We have a migration guide on the main branch; please take a look at it in case you run into issues, we also have documented everything in release notes. We appreciate the feedbacks, so feel free to create issues if you have any! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/unofficialmerve"&gt; /u/unofficialmerve &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qnk7fq/transformers_v5_final_is_out/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qnk7fq/transformers_v5_final_is_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qnk7fq/transformers_v5_final_is_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-26T16:07:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1qnw3z6</id>
    <title>Kimi K2.5 Released !</title>
    <updated>2026-01-26T23:01:14+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qnw3z6/kimi_k25_released/"&gt; &lt;img alt="Kimi K2.5 Released !" src="https://b.thumbs.redditmedia.com/dKkXWHKejfbQU4qoe82oKQb8Ib5mckOcJmAeyPyUh7M.jpg" title="Kimi K2.5 Released !" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Since the previous version was open-sourced, I‚Äôm sharing the new model. I‚Äôm not sure if this one will be open-source yet, and the official website hasn‚Äôt mentioned &lt;strong&gt;Kimi K2.5&lt;/strong&gt; at all, so I think they‚Äôre still in the testing phase.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;They currently only released on their website&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7f613rz2yrfg1.png?width=1517&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b10c7206deeb73082b1d0988cddb3601a6ccbcca"&gt;https://preview.redd.it/7f613rz2yrfg1.png?width=1517&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b10c7206deeb73082b1d0988cddb3601a6ccbcca&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/AiBattle_/status/2015902394312253564?s=20"&gt;https://x.com/AiBattle_/status/2015902394312253564?s=20&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.kimi.com/"&gt;https://www.kimi.com/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qnw3z6/kimi_k25_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qnw3z6/kimi_k25_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qnw3z6/kimi_k25_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-26T23:01:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1qo8r8s</id>
    <title>Kimi K2.5 Launches, Unsloth quantisations coming soon</title>
    <updated>2026-01-27T09:00:51+00:00</updated>
    <author>
      <name>/u/Plastic-Accident862</name>
      <uri>https://old.reddit.com/user/Plastic-Accident862</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://platform.moonshot.ai/docs/guide/kimi-k2-5-quickstart"&gt;https://platform.moonshot.ai/docs/guide/kimi-k2-5-quickstart&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Plastic-Accident862"&gt; /u/Plastic-Accident862 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qo8r8s/kimi_k25_launches_unsloth_quantisations_coming/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qo8r8s/kimi_k25_launches_unsloth_quantisations_coming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qo8r8s/kimi_k25_launches_unsloth_quantisations_coming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-27T09:00:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1qnwa33</id>
    <title>GLM 4.7 Flash: Huge performance improvement with -kvu</title>
    <updated>2026-01-26T23:07:49+00:00</updated>
    <author>
      <name>/u/TokenRingAI</name>
      <uri>https://old.reddit.com/user/TokenRingAI</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TLDR; Try passing -kvu to llama.cpp when running GLM 4.7 Flash. &lt;/p&gt; &lt;p&gt;On RTX 6000, my tokens per second on a 8K token output rose from 17.7t/s to 100t/s&lt;/p&gt; &lt;p&gt;Also, check out the one shot zelda game it made, pretty good for a 30B:&lt;br /&gt; &lt;a href="https://talented-fox-j27z.pagedrop.io"&gt;https://talented-fox-j27z.pagedrop.io&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TokenRingAI"&gt; /u/TokenRingAI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qnwa33/glm_47_flash_huge_performance_improvement_with_kvu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qnwa33/glm_47_flash_huge_performance_improvement_with_kvu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qnwa33/glm_47_flash_huge_performance_improvement_with_kvu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-26T23:07:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1qob8de</id>
    <title>Honest question: what do you all do for a living to afford these beasts?</title>
    <updated>2026-01-27T11:23:48+00:00</updated>
    <author>
      <name>/u/ready_to_fuck_yeahh</name>
      <uri>https://old.reddit.com/user/ready_to_fuck_yeahh</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Basically I am from India, a medium high end job here pays Rs. 1 lakh($ 1100) per month and there are deductions on top of it.&lt;/p&gt; &lt;p&gt;An RTX Pro 6000 starts from 8 lakh and goes upto 10 lakh($ 10989), 5090 costs 3.5 lakhs($ 3800), threadripper costs 7-8 lakhs($ 8800), ram prices have soared and corsair vengeance costs 52,000 ($ 571) for 32GB, motherboard, cabinet, and other accessories makes it look like a dream to own in a lifetime. And people here are using multi gpu setup, recently saw 4xrtx 6000 pro setup here.&lt;/p&gt; &lt;p&gt;Been seeing a lot of beautiful multi-GPU setups here and I'm genuinely curious about the community makeup.&lt;/p&gt; &lt;p&gt;Are most of you:&lt;/p&gt; &lt;p&gt;Software engineers / AI researchers (expensing to employer or side business)?&lt;/p&gt; &lt;p&gt;Serious hobbyists with high-paying day jobs?&lt;/p&gt; &lt;p&gt;Consultants/freelancers writing off hardware?&lt;/p&gt; &lt;p&gt;Something else entirely?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ready_to_fuck_yeahh"&gt; /u/ready_to_fuck_yeahh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qob8de/honest_question_what_do_you_all_do_for_a_living/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qob8de/honest_question_what_do_you_all_do_for_a_living/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qob8de/honest_question_what_do_you_all_do_for_a_living/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-27T11:23:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1qo349m</id>
    <title>deepseek-ai/DeepSeek-OCR-2 ¬∑ Hugging Face</title>
    <updated>2026-01-27T03:56:49+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qo349m/deepseekaideepseekocr2_hugging_face/"&gt; &lt;img alt="deepseek-ai/DeepSeek-OCR-2 ¬∑ Hugging Face" src="https://external-preview.redd.it/c9LaruBvjfhr_AFkVVpu9jJ8NabAKdroEOMl2Akgn-0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9a35ff741fcd21d9b3346fefa618503befa19d18" title="deepseek-ai/DeepSeek-OCR-2 ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-OCR-2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qo349m/deepseekaideepseekocr2_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qo349m/deepseekaideepseekocr2_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-27T03:56:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1qo3ri5</id>
    <title>Jan v3 Instruct: a 4B coding Model with +40% Aider Improvement</title>
    <updated>2026-01-27T04:26:25+00:00</updated>
    <author>
      <name>/u/Delicious_Focus3465</name>
      <uri>https://old.reddit.com/user/Delicious_Focus3465</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qo3ri5/jan_v3_instruct_a_4b_coding_model_with_40_aider/"&gt; &lt;img alt="Jan v3 Instruct: a 4B coding Model with +40% Aider Improvement" src="https://preview.redd.it/0qp4pz0fbtfg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=12b737e67556ca654785997ea815b78511476ed2" title="Jan v3 Instruct: a 4B coding Model with +40% Aider Improvement" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, this is Bach from the Jan team.&lt;/p&gt; &lt;p&gt;We‚Äôre releasing Jan-v3-4B-base-instruct, a 4B-parameter model trained with &lt;strong&gt;continual pre-training&lt;/strong&gt; and &lt;strong&gt;RL&lt;/strong&gt;, to improve capabilities across common tasks while preserving other general capabilities.&lt;/p&gt; &lt;p&gt;What it‚Äôs for&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A good starting point for further fine-tuning&lt;/li&gt; &lt;li&gt;Improved math and coding performance for lightweight assistance&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;How to run it:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Jan Desktop&lt;/p&gt; &lt;p&gt;Download Jan Desktop: &lt;a href="https://www.jan.ai/"&gt;https://www.jan.ai/&lt;/a&gt; and then download Jan v3 via Jan Hub. &lt;/p&gt; &lt;p&gt;Model links:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Jan-v3-4B: &lt;a href="https://huggingface.co/Menlo/Jan-v3-4B-base-instruct"&gt;https://huggingface.co/janhq/Jan-v3-4B-base-instruct&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Jan-v3-4B-GGUF: &lt;a href="https://huggingface.co/Menlo/Jan-v3-4B-base-instruct-gguf"&gt;https://huggingface.co/janhq/Jan-v3-4B-base-instruct-gguf&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Recommended parameters:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;temperature: 0.7&lt;/li&gt; &lt;li&gt;top_p: 0.8&lt;/li&gt; &lt;li&gt;top_k: 20&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What‚Äôs coming next:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Jan-Code&lt;/strong&gt; (finetuned of Jan-v3-4B-base-instruct)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Jan-v3-Seach-4B&lt;/strong&gt; (renewal of Jan-nano on Jan-v3-4B-base-instruct)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;A 30B Jan-v3 family of models&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Delicious_Focus3465"&gt; /u/Delicious_Focus3465 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0qp4pz0fbtfg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qo3ri5/jan_v3_instruct_a_4b_coding_model_with_40_aider/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qo3ri5/jan_v3_instruct_a_4b_coding_model_with_40_aider/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-27T04:26:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1qo7wur</id>
    <title>OpenAI could reportedly run out of cash by mid-2027 ‚Äî analyst paints grim picture after examining the company's finances</title>
    <updated>2026-01-27T08:09:28+00:00</updated>
    <author>
      <name>/u/EchoOfOppenheimer</name>
      <uri>https://old.reddit.com/user/EchoOfOppenheimer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qo7wur/openai_could_reportedly_run_out_of_cash_by/"&gt; &lt;img alt="OpenAI could reportedly run out of cash by mid-2027 ‚Äî analyst paints grim picture after examining the company's finances" src="https://external-preview.redd.it/v44P77PnYI5tRIZpnmaNJbBahgYNI024hkyMrYI6J24.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=147c3b3cde07c4eba8b8831f89c4a1dbc6ec6942" title="OpenAI could reportedly run out of cash by mid-2027 ‚Äî analyst paints grim picture after examining the company's finances" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A new financial analysis predicts OpenAI could burn through its cash reserves by mid-2027. The report warns that Sam Altman‚Äôs '$100 billion Stargate' strategy is hitting a wall: training costs are exploding, but revenue isn't keeping up. With Chinese competitors like DeepSeek now offering GPT-5 level performance for 95% less cost, OpenAI‚Äôs 'moat' is evaporating faster than expected. If AGI doesn't arrive to save the economics, the model is unsustainable.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EchoOfOppenheimer"&gt; /u/EchoOfOppenheimer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.tomshardware.com/tech-industry/big-tech/openai-could-reportedly-run-out-of-cash-by-mid-2027-nyt-analyst-paints-grim-picture-after-examining-companys-finances"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qo7wur/openai_could_reportedly_run_out_of_cash_by/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qo7wur/openai_could_reportedly_run_out_of_cash_by/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-27T08:09:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1qocvd4</id>
    <title>built an AI agent with shell access. found out the hard way why that's a bad idea.</title>
    <updated>2026-01-27T12:46:00+00:00</updated>
    <author>
      <name>/u/YogurtIll4336</name>
      <uri>https://old.reddit.com/user/YogurtIll4336</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;was building a tool to let claude/gpt4 navigate my codebase. gave it bash access, seemed fine.&lt;/p&gt; &lt;p&gt;then i tried asking it to &amp;quot;check imports and make ascii art from my env file&amp;quot;&lt;/p&gt; &lt;p&gt;it did both. printed my api keys as art.&lt;/p&gt; &lt;p&gt;went down a rabbit hole reading about this. turns out prompt injection is way worse than i thought:&lt;/p&gt; &lt;p&gt;anthropic has a whole page on it but it's pretty surface level&lt;/p&gt; &lt;p&gt;found this practical writeup from some YC startup that actually tested bypasses: &lt;a href="https://www.codeant.ai/blogs/agentic-rag-shell-sandboxing"&gt;https://www.codeant.ai/blogs/agentic-rag-shell-sandboxing&lt;/a&gt;&lt;/p&gt; &lt;p&gt;simon willison has been screaming about this for months (&lt;a href="https://simonwillison.net/series/prompt-injection/"&gt;https://simonwillison.net/series/prompt-injection/&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;apparently docker shared kernel isn't enough. gvisor adds overhead. firecracker seems like overkill but it's what aws lambda uses so... maybe not? stuck between &amp;quot;ship it and hope&amp;quot; vs &amp;quot;burn 2 weeks adding proper isolation&amp;quot;&lt;/p&gt; &lt;p&gt;has anyone actually solved this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/YogurtIll4336"&gt; /u/YogurtIll4336 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qocvd4/built_an_ai_agent_with_shell_access_found_out_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qocvd4/built_an_ai_agent_with_shell_access_found_out_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qocvd4/built_an_ai_agent_with_shell_access_found_out_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-27T12:46:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1qo595n</id>
    <title>Introducing Kimi K2.5, Open-Source Visual Agentic Intelligence</title>
    <updated>2026-01-27T05:39:09+00:00</updated>
    <author>
      <name>/u/Kimi_Moonshot</name>
      <uri>https://old.reddit.com/user/Kimi_Moonshot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;üîπ&lt;strong&gt;Global SOTA on Agentic Benchmarks&lt;/strong&gt;: HLE full set (50.2%), BrowseComp (74.9%) &lt;/p&gt; &lt;p&gt;üîπ&lt;strong&gt;Open-source SOTA on Vision and Coding&lt;/strong&gt;: MMMU Pro (78.5%), VideoMMMU (86.6%), SWE-bench Verified (76.8%) &lt;/p&gt; &lt;p&gt;üîπ&lt;strong&gt;Code with Taste&lt;/strong&gt;: turn chats, images &amp;amp; videos into aesthetic websites with expressive motion. &lt;/p&gt; &lt;p&gt;üîπ&lt;strong&gt;Agent Swarm (Beta)&lt;/strong&gt;: self-directed agents working in parallel, at scale. Up to &lt;strong&gt;100&lt;/strong&gt; sub-agents, &lt;strong&gt;1,500&lt;/strong&gt; tool calls, &lt;strong&gt;4.5√ó&lt;/strong&gt; faster compared with single-agent setup. &lt;/p&gt; &lt;p&gt;ü•ù&lt;strong&gt;K2.5&lt;/strong&gt; is now live on &lt;a href="https://t.co/YutVbwktG0"&gt;http://kimi.com&lt;/a&gt; in &lt;strong&gt;chat mod&lt;/strong&gt;e and &lt;strong&gt;agent mode&lt;/strong&gt;. &lt;/p&gt; &lt;p&gt;ü•ù&lt;strong&gt;K2.5 Agent Swarm&lt;/strong&gt; in beta for high-tier users. &lt;/p&gt; &lt;p&gt;ü•ùFor production-grade coding, you can pair K2.5 with &lt;strong&gt;Kim&lt;/strong&gt;i Code: &lt;a href="https://t.co/A5WQozJF3s"&gt;https://kimi.com/code&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üîóAPI: &lt;a href="https://t.co/EOZkbOwCN4"&gt;https://platform.moonshot.ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üîóTech blog: &lt;a href="https://www.kimi.com/blog/kimi-k2-5.html"&gt;https://www.kimi.com/blog/kimi-k2-5.html&lt;/a&gt; &lt;/p&gt; &lt;p&gt;üîóWeights &amp;amp; code: &lt;a href="https://huggingface.co/moonshotai/Kimi-K2.5"&gt;https://huggingface.co/moonshotai/Kimi-K2.5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/b3lldwzvwtfg1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ffa7bb89f8a91ef050af44cc3fa6090c9e1a7412"&gt;https://preview.redd.it/b3lldwzvwtfg1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ffa7bb89f8a91ef050af44cc3fa6090c9e1a7412&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kimi_Moonshot"&gt; /u/Kimi_Moonshot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qo595n/introducing_kimi_k25_opensource_visual_agentic/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qo595n/introducing_kimi_k25_opensource_visual_agentic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qo595n/introducing_kimi_k25_opensource_visual_agentic/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-27T05:39:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1qoa8rp</id>
    <title>The Qwen Devs Are Teasing Something</title>
    <updated>2026-01-27T10:28:56+00:00</updated>
    <author>
      <name>/u/Few_Painter_5588</name>
      <uri>https://old.reddit.com/user/Few_Painter_5588</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qoa8rp/the_qwen_devs_are_teasing_something/"&gt; &lt;img alt="The Qwen Devs Are Teasing Something" src="https://preview.redd.it/umvks92vcvfg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=297a92382cbb71a347dd9192a2d8ae1054cf9fb2" title="The Qwen Devs Are Teasing Something" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm going to assume a new VL model&lt;/p&gt; &lt;p&gt;Edit: It's likely to be Z-Image&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Few_Painter_5588"&gt; /u/Few_Painter_5588 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/umvks92vcvfg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qoa8rp/the_qwen_devs_are_teasing_something/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qoa8rp/the_qwen_devs_are_teasing_something/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-27T10:28:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
