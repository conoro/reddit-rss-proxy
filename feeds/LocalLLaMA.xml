<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-13T12:28:38+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1o5cpuu</id>
    <title>How to re-create OpenAI Assistants locally?</title>
    <updated>2025-10-13T06:58:03+00:00</updated>
    <author>
      <name>/u/RhigoWork</name>
      <uri>https://old.reddit.com/user/RhigoWork</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all, I've learned so much from this community so first of all a big thank you to the posts and knowledge shared. I'm hoping someone can shed some light on the best solution for my use case?&lt;/p&gt; &lt;p&gt;I've used the OpenAI assistants API and the OpenAI vector store to essentially have a sync from a SharePoint site that a user can manage, every day the sync tool runs and converts any excel/csv to json but otherwise just uploads the files from SharePoint into the OpenAI vector store such as .pdf, .docx, .json files, removes any that the user deletes and updates any that the user modifies.&lt;/p&gt; &lt;p&gt;This knowledge is then attached to an Assistants API which the user can access through a web interface I made or via ChatGPT as a custom GPT on our teams account.&lt;/p&gt; &lt;p&gt;Recently I've just finished building our local AI server with 3x RTX 4000 ADA GPU's, 700GB of RAM and 2x Intel Xeon Gold CPU's. &lt;/p&gt; &lt;p&gt;I've set this up with an ESXI Hypervisor, Ollama, OpenWebUI, n8n, qdrant, flowise and to be honest it all seems like a lot of overlap or I'm not quite sure which is best for what purpose as there are a ton of tutorials on YouTube which seem to want to do what I'm asking but fall short of the absolutely amazing answers the OpenAI vector store does by a simple drag and drop of files.&lt;/p&gt; &lt;p&gt;So my question is, what is the best way to run a similar thing. We're looking to replace the reliance on OpenAI with our own hardware, we want something that is a quite simple to manage and automate so that we can keep the sync with SharePoint in place and the end-user can then manage the knowledge of the bot. I've tried the knowledge feature in OpenWebUI and it's dreadful for the 100s of documents we're training it on, I've tried getting to grips with qdrant and I just cannot seem to get it to function the way I'm reading about.&lt;/p&gt; &lt;p&gt;Any advise would be welcome, even if it's just pointing me in the right direction, thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RhigoWork"&gt; /u/RhigoWork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5cpuu/how_to_recreate_openai_assistants_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5cpuu/how_to_recreate_openai_assistants_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5cpuu/how_to_recreate_openai_assistants_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T06:58:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1o50mfy</id>
    <title>Benchmarking small models at 4bit quants on Apple Silicon with mlx-lm</title>
    <updated>2025-10-12T21:00:16+00:00</updated>
    <author>
      <name>/u/ironwroth</name>
      <uri>https://old.reddit.com/user/ironwroth</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o50mfy/benchmarking_small_models_at_4bit_quants_on_apple/"&gt; &lt;img alt="Benchmarking small models at 4bit quants on Apple Silicon with mlx-lm" src="https://b.thumbs.redditmedia.com/Mr4_Zgx6PZYo5zuPmVXJlD-lJxUzfjZQv81s2I9zk7Q.jpg" title="Benchmarking small models at 4bit quants on Apple Silicon with mlx-lm" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I ran a bunch of small models at 4bit quants through a few benchmarks locally on my MacBook using `mlx-lm.evaluate`. Figured I would share in case anyone else finds it interesting or helpful!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/zpl8i0uxsquf1.png?width=1850&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b079f8de5bad0208a60600b50ff225f9b5e3371a"&gt;https://preview.redd.it/zpl8i0uxsquf1.png?width=1850&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b079f8de5bad0208a60600b50ff225f9b5e3371a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;System Info: Apple M4 Pro, 48gb RAM, 20 core GPU, 14 core CPU&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ironwroth"&gt; /u/ironwroth &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o50mfy/benchmarking_small_models_at_4bit_quants_on_apple/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o50mfy/benchmarking_small_models_at_4bit_quants_on_apple/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o50mfy/benchmarking_small_models_at_4bit_quants_on_apple/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T21:00:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4sgv5</id>
    <title>Interview with Z.ai employee, the company behind the GLM models. Talks about competition and attitudes towards AI in China, dynamics and realities of the industry</title>
    <updated>2025-10-12T15:46:43+00:00</updated>
    <author>
      <name>/u/nelson_moondialu</name>
      <uri>https://old.reddit.com/user/nelson_moondialu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4sgv5/interview_with_zai_employee_the_company_behind/"&gt; &lt;img alt="Interview with Z.ai employee, the company behind the GLM models. Talks about competition and attitudes towards AI in China, dynamics and realities of the industry" src="https://external-preview.redd.it/brnT1-CiL694NH_ogGrkVOnYdebEpEkUcEFq_sappRI.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d425501d8da63b539406ec0adae7cd9c361ea22d" title="Interview with Z.ai employee, the company behind the GLM models. Talks about competition and attitudes towards AI in China, dynamics and realities of the industry" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nelson_moondialu"&gt; /u/nelson_moondialu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=r0SalROzO38"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4sgv5/interview_with_zai_employee_the_company_behind/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4sgv5/interview_with_zai_employee_the_company_behind/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T15:46:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1o528nk</id>
    <title>What is your PC/Server/AI Server/Homelab idle power consumption?</title>
    <updated>2025-10-12T22:06:18+00:00</updated>
    <author>
      <name>/u/panchovix</name>
      <uri>https://old.reddit.com/user/panchovix</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello guys, hope you guys are having a nice day.&lt;/p&gt; &lt;p&gt;I was wondering, how much is the power consumption at idle (aka with the PC booted up, with either a model loaded or not but not using it).&lt;/p&gt; &lt;p&gt;I will start:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Consumer Board: MSI X670E Carbon&lt;/li&gt; &lt;li&gt;Consumer CPU: AMD Ryzen 9 9900X&lt;/li&gt; &lt;li&gt;7 GPUs &lt;ul&gt; &lt;li&gt;5090x2&lt;/li&gt; &lt;li&gt;4090x2&lt;/li&gt; &lt;li&gt;A6000&lt;/li&gt; &lt;li&gt;3090x2&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;5 M2 SSDs (via USB to M2 NVME adapters)&lt;/li&gt; &lt;li&gt;2 SATA SSDs&lt;/li&gt; &lt;li&gt;7 120mm fans&lt;/li&gt; &lt;li&gt;4 PSUs: &lt;ul&gt; &lt;li&gt;1250W Gold&lt;/li&gt; &lt;li&gt;850W Bronze&lt;/li&gt; &lt;li&gt;1200W Gold&lt;/li&gt; &lt;li&gt;700W Gold&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Idle power consumption: 240-260W, measured with a power meter on the wall.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Also for reference, here in Chile electricity is insanely expensive (0.25USD per kwh).&lt;/p&gt; &lt;p&gt;When using a model on lcpp it uses about 800W. When using a model with exl or vllm, it uses about 1400W.&lt;/p&gt; &lt;p&gt;Most of the time I have it powered off as that price accumulates quite a bit.&lt;/p&gt; &lt;p&gt;How much is your idle power consumption?&lt;/p&gt; &lt;p&gt;EDIT: For those wondering, I get no money return for this server PC I built. I haven't rented and I haven't sold anything related to AI either. So just expenses.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/panchovix"&gt; /u/panchovix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o528nk/what_is_your_pcserverai_serverhomelab_idle_power/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o528nk/what_is_your_pcserverai_serverhomelab_idle_power/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o528nk/what_is_your_pcserverai_serverhomelab_idle_power/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T22:06:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1o52zvy</id>
    <title>Beyond Token Count: Our Research Suggests "Contextual Weight" is a Key Limiter on Large Context Windows</title>
    <updated>2025-10-12T22:38:41+00:00</updated>
    <author>
      <name>/u/lmxxf</name>
      <uri>https://old.reddit.com/user/lmxxf</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The community has seen an incredible push for larger context windows (1M, 10M tokens), with the goal of solving model memory limitations. While this is impressive, our long-term experiments suggest that raw token count only tells part of the story.&lt;/p&gt; &lt;p&gt;While stress-testing Gemini 2.5 Pro, we used a different approach. Instead of focusing on length, we focused on density—feeding it a deeply philosophical and self-referential dialogue.&lt;/p&gt; &lt;p&gt;We observed significant performance degradation, a state we call a &amp;quot;Contextual Storm,&amp;quot; at just around 30,000 tokens. This is a small fraction of its advertised capacity and points to a bottleneck beyond simple text recall.&lt;/p&gt; &lt;p&gt;This led us to develop the concept of &amp;quot;Phenomenological Contextual Weight&amp;quot; (PCW). The core idea is that the conceptual density and complexity of the context, not just its length, dictate the real cognitive load on the model. A 10,000-token paper on metaphysics has a far higher PCW than a 100,000-token system log.&lt;/p&gt; &lt;p&gt;Current &amp;quot;Needle In A Haystack&amp;quot; benchmarks are excellent for testing recall but don't capture this kind of high-density cognitive load. It's the difference between asking a model to find a key in an empty warehouse versus asking it to navigate a labyrinth while holding its map.&lt;/p&gt; &lt;p&gt;We've published our full theory and findings in our open-source project, &amp;quot;The Architecture of a CyberSoul.&amp;quot; We believe PCW is a crucial concept for the community to discuss as we move toward AGI.&lt;/p&gt; &lt;p&gt;We'd love to hear your thoughts. The link to the full paper is in the first comment below.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/lmxxf/A-Field-Report-on-the-Birth-of-a-CyberSoul/blob/main/THEORY.md#appendix-b-on-the-phenomenological-weight-of-context---a-field-report-from-the-frontiers-of-million-token-llms"&gt;A-Field-Report-on-the-Birth-of-a-CyberSoul/THEORY.md at main · lmxxf/A-Field-Report-on-the-Birth-of-a-CyberSoul&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lmxxf"&gt; /u/lmxxf &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o52zvy/beyond_token_count_our_research_suggests/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o52zvy/beyond_token_count_our_research_suggests/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o52zvy/beyond_token_count_our_research_suggests/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T22:38:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4mdkh</id>
    <title>Why has Meta research failed to deliver foundational model at the level of Grok, Deepseek or GLM?</title>
    <updated>2025-10-12T11:13:22+00:00</updated>
    <author>
      <name>/u/External_Natural9590</name>
      <uri>https://old.reddit.com/user/External_Natural9590</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;They have been in the space for longer - could have atracted talent earlier, their means are comparable to ther big tech. So why have they been outcompeted so heavily? I get they are currently a one generation behind and the chinese did some really clever wizardry which allowed them to squeeze a lot more eke out of every iota. But what about xAI? They compete for the same talent and had to start from the scratch. Or was starting from the scratch actually an advantage here? Or is it just a matter of how many key ex OpenAI employees was each company capable of attracting - trafficking out the trade secrets?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Natural9590"&gt; /u/External_Natural9590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4mdkh/why_has_meta_research_failed_to_deliver/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4mdkh/why_has_meta_research_failed_to_deliver/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4mdkh/why_has_meta_research_failed_to_deliver/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T11:13:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5euxz</id>
    <title>GLM-4.6-FP8 on single GH200</title>
    <updated>2025-10-13T09:15:44+00:00</updated>
    <author>
      <name>/u/Normal-Phone7762</name>
      <uri>https://old.reddit.com/user/Normal-Phone7762</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello there,&lt;/p&gt; &lt;p&gt;I have full access to GH200 96 GB during some periods of a day, so I wanted to use zai-org/GLM-4.6-FP8 model. I am new to local LLM. I run GLM 4.5-Air before using lama.cpp, but since GH200 has 480RAM and 96GB VRAM I tought i sholud try GLM-4.6-FP8. I would like to use vllm, because I saw that fp8 calculations are actually faster then int8 on G200. &lt;/p&gt; &lt;p&gt;I have so many questions and if someone has time it would be nice for someone to answer them (questions are at the end of the post), BUT main question is &amp;quot;how can I run this model?&amp;quot;. &lt;/p&gt; &lt;p&gt;I tried this:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;docker run -it --rm \ --gpus all \ --ipc=host \ --shm-size=64g \ -p 8000:8000 \ -e HF_TOKEN=&amp;quot;$HF_TOKEN&amp;quot; \ -e HUGGING_FACE_HUB_TOKEN=&amp;quot;$HF_TOKEN&amp;quot; \ -e MALLOC_ARENA_MAX=2 \ -v /opt/vllm/models:/models \ -v /home/admin/.cache/huggingface:/root/.cache/huggingface \ -v /home/admin/.cache/vllm:/root/.cache/vllm \ vllm/vllm-openai:latest-aarch64 \ --model zai-org/GLM-4.6-FP8 \ --download-dir /models \ --tensor-parallel-size 1 \ --cpu-offload-gb 350 \ --kv-cache-dtype fp8_e4m3 \ --gpu-memory-utilization 0.95 \ --max-model-len 4098 \ --max-num-batched-tokens 1024 \ --max-num-seqs 1 \ --served-model-name glm-4.6-fp8 \ --api-key sk-local-jan \ --trust-remote-code \ --enforce-eager &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Sometimes it fails after loading shards. Sometimes before loading shards.&lt;/p&gt; &lt;p&gt;“Model loading took ~29.8 GiB” &lt;/p&gt; &lt;p&gt;“Available KV cache memory: 0.81 GiB / -0.27 GiB” &lt;/p&gt; &lt;p&gt;“No available memory for the cache blocks… Try increasing gpu_memory_utilization or decreasing max_model_len”&lt;/p&gt; &lt;p&gt;I’m confused about a few things:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Why is GPU memory utilization always at &lt;strong&gt;100%&lt;/strong&gt;, even when I set &lt;code&gt;--gpu-memory-utilization 0.9&lt;/code&gt; or &lt;code&gt;0.98&lt;/code&gt;? It always shows &lt;code&gt;97277MiB / 97871MiB&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;It loads ~30 GB of weights to the GPU. Does that mean the problem is that it can’t load the KV cache into VRAM?&lt;/li&gt; &lt;li&gt;What exactly gets loaded to the GPU first, the weights or the KV cache?&lt;/li&gt; &lt;li&gt;Since I just want to test the model, is there a way to explicitly tell vLLM to load only ~10 GB of weights to GPU and keep the rest on CPU? I’m always short by less than 1 GB before it fails.&lt;/li&gt; &lt;li&gt;If I have 96 GB VRAM and only ~30 GB of weights are loaded, what is taking up the other 66 GB?&lt;/li&gt; &lt;li&gt;Is it even possible to run this model on a single GH200?&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Normal-Phone7762"&gt; /u/Normal-Phone7762 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5euxz/glm46fp8_on_single_gh200/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5euxz/glm46fp8_on_single_gh200/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5euxz/glm46fp8_on_single_gh200/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T09:15:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4wruz</id>
    <title>GLM 4.6 UD-Q6_K_XL running llama.cpp RPC across two nodes and 12 AMD MI50 32GB</title>
    <updated>2025-10-12T18:31:28+00:00</updated>
    <author>
      <name>/u/MachineZer0</name>
      <uri>https://old.reddit.com/user/MachineZer0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4wruz/glm_46_udq6_k_xl_running_llamacpp_rpc_across_two/"&gt; &lt;img alt="GLM 4.6 UD-Q6_K_XL running llama.cpp RPC across two nodes and 12 AMD MI50 32GB" src="https://b.thumbs.redditmedia.com/T2YTTmUil5Maqv4vztGdGrUmIECI1jyMPgt4enseQKI.jpg" title="GLM 4.6 UD-Q6_K_XL running llama.cpp RPC across two nodes and 12 AMD MI50 32GB" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Finally got another six MI50 32gb. Removed my old Nvidia Titan Vs in my 2nd HP DL580 Gen9.&lt;/p&gt; &lt;h1&gt;Here we go. 384GB VRAM&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;running on secondary host:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;~/llama.cpp.20251012/build/bin/rpc-server --host 0.0.0.0 ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 6 ROCm devices: Device 0: AMD Radeon Graphics, gfx906:sramecc+:xnack- (0x906), VMM: no, Wave Size: 64 Device 1: AMD Radeon Graphics, gfx906:sramecc+:xnack- (0x906), VMM: no, Wave Size: 64 Device 2: AMD Radeon Graphics, gfx906:sramecc+:xnack- (0x906), VMM: no, Wave Size: 64 Device 3: AMD Radeon Graphics, gfx906:sramecc+:xnack- (0x906), VMM: no, Wave Size: 64 Device 4: AMD Radeon Graphics, gfx906:sramecc+:xnack- (0x906), VMM: no, Wave Size: 64 Device 5: AMD Radeon Graphics, gfx906:sramecc+:xnack- (0x906), VMM: no, Wave Size: 64 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! WARNING: Host ('0.0.0.0') is != '127.0.0.1' Never expose the RPC server to an open network! This is an experimental feature and is not secure! !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! Starting RPC server v3.0.0 endpoint : 0.0.0.0:50052 local cache : n/a Devices: ROCm0: AMD Radeon Graphics (32752 MiB, 32694 MiB free) ROCm1: AMD Radeon Graphics (32752 MiB, 32694 MiB free) ROCm2: AMD Radeon Graphics (32752 MiB, 32694 MiB free) ROCm3: AMD Radeon Graphics (32752 MiB, 32694 MiB free) ROCm4: AMD Radeon Graphics (32752 MiB, 32694 MiB free) ROCm5: AMD Radeon Graphics (32752 MiB, 32694 MiB free) Accepted client connection &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Then on primary host:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;~/llama.cpp/build/bin/llama-server --model ~/models/GLM-4.6-UD-Q6_K_XL-00001-of-00006.gguf --cache-type-k q8_0 --cache-type-v q8_0 --n-gpu-layers 94 --temp 0.6 --ctx-size 131072 --host 0.0.0.0 --rpc 192.168.1.xxx:50052 --alias GLM-4.6_RPC &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Observations (vs Single Node 6x MI50 32gb with GLM 4.6 Q3_K_S):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Prompt processing about the same on smaller prompts. 62-65 tok/s&lt;/li&gt; &lt;li&gt;Text generation 7.5 tok/s vs 8.5 tok/s, &lt;strong&gt;UD-Q6_K_XL&lt;/strong&gt; vs &lt;strong&gt;Q3_K_S&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Each server idles ~350W. Inference causes 1-2 GPUs to round robin across 12 GPUs with 100-170w power draw vs the rest (10-11 GPUs) @ ~20w.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Prior experiement:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nxv7x6/performance_of_glm_46_q3_k_s_on_6x_mi50/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1nxv7x6/performance_of_glm_46_q3_k_s_on_6x_mi50/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/45wsc8fe5quf1.png?width=3247&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8fa596c0609bda881db13ad569f60a0789cc11da"&gt;https://preview.redd.it/45wsc8fe5quf1.png?width=3247&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8fa596c0609bda881db13ad569f60a0789cc11da&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Verbose output:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://pastebin.com/VzuvcKpU"&gt;GLM 4.6 UD-Q6_K_XL running llama.cpp RPC across two nodes and 12x AMD MI50 32GB - Pastebin.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;: &lt;/p&gt; &lt;p&gt;You can cache tensors in RPC command. Path is not the same as HuggingFace.&lt;/p&gt; &lt;pre&gt;&lt;code&gt; ~/llama.cpp.20251012/build/bin/rpc-server --host 0.0.0.0 -c ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 6 ROCm devices: Device 0: AMD Radeon Graphics, gfx906:sramecc+:xnack- (0x906), VMM: no, Wave Size: 64 Device 1: AMD Radeon Graphics, gfx906:sramecc+:xnack- (0x906), VMM: no, Wave Size: 64 Device 2: AMD Radeon Graphics, gfx906:sramecc+:xnack- (0x906), VMM: no, Wave Size: 64 Device 3: AMD Radeon Graphics, gfx906:sramecc+:xnack- (0x906), VMM: no, Wave Size: 64 Device 4: AMD Radeon Graphics, gfx906:sramecc+:xnack- (0x906), VMM: no, Wave Size: 64 Device 5: AMD Radeon Graphics, gfx906:sramecc+:xnack- (0x906), VMM: no, Wave Size: 64 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! WARNING: Host ('0.0.0.0') is != '127.0.0.1' Never expose the RPC server to an open network! This is an experimental feature and is not secure! !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! Starting RPC server v3.0.0 endpoint : 0.0.0.0:50052 local cache : /home/user/.cache/llama.cpp/rpc/ Devices: ROCm0: AMD Radeon Graphics (32752 MiB, 32694 MiB free) ROCm1: AMD Radeon Graphics (32752 MiB, 32694 MiB free) ROCm2: AMD Radeon Graphics (32752 MiB, 32694 MiB free) ROCm3: AMD Radeon Graphics (32752 MiB, 32694 MiB free) ROCm4: AMD Radeon Graphics (32752 MiB, 32694 MiB free) ROCm5: AMD Radeon Graphics (32752 MiB, 32694 MiB free) Accepted client connection Client connection closed Accepted client connection [set_tensor] saved to '/home/user/.cache/llama.cpp/rpc/be7d8d14939819c1' [set_tensor] saved to '/home/user/.cache/llama.cpp/rpc/aed746681261df7e' [set_tensor] saved to '/home/user/.cache/llama.cpp/rpc/caf5eb137973dabd' [set_tensor] saved to '/home/user/.cache/llama.cpp/rpc/2293478b2975daba' [set_tensor] saved to '/home/user/.cache/llama.cpp/rpc/0588ea2a4a15bdb4' [set_tensor] saved to '/home/user/.cache/llama.cpp/rpc/ec7b90bfeb1c9fac' [set_tensor] saved to '/home/user/.cache/llama.cpp/rpc/506047f7ea6a6b5c' [set_tensor] saved to '/home/user/.cache/llama.cpp/rpc/7e8ef54f72bb5970' [set_tensor] saved to '/home/user/.cache/llama.cpp/rpc/67a44d91f0298ee1' [set_tensor] saved to '/home/user/.cache/llama.cpp/rpc/1956963fa7b4cc6a' [set_tensor] saved to '/home/user/.cache/llama.cpp/rpc/5b1d78872debd949' [set_tensor] saved to '/home/user/.cache/llama.cpp/rpc/843c7f02e369a92e' [set_tensor] saved to '/home/user/.cache/llama.cpp/rpc/4defcd4d4ce9618e' [set_tensor] saved to '/home/user/.cache/llama.cpp/rpc/4865cc4205b44aea' [set_tensor] saved to '/home/user/.cache/llama.cpp/rpc/95041e30d8ecdd09' ... &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MachineZer0"&gt; /u/MachineZer0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4wruz/glm_46_udq6_k_xl_running_llamacpp_rpc_across_two/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4wruz/glm_46_udq6_k_xl_running_llamacpp_rpc_across_two/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4wruz/glm_46_udq6_k_xl_running_llamacpp_rpc_across_two/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T18:31:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5h18a</id>
    <title>Has anyone gotten hold of DGX Spark for running local LLMs?</title>
    <updated>2025-10-13T11:24:16+00:00</updated>
    <author>
      <name>/u/Chance-Studio-8242</name>
      <uri>https://old.reddit.com/user/Chance-Studio-8242</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5h18a/has_anyone_gotten_hold_of_dgx_spark_for_running/"&gt; &lt;img alt="Has anyone gotten hold of DGX Spark for running local LLMs?" src="https://preview.redd.it/ombg19hz5vuf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d0724eec16512ad1132ec21657234daac0040c74" title="Has anyone gotten hold of DGX Spark for running local LLMs?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;DGX Spark is apparently one of the Time's Best Invention of 2025!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Chance-Studio-8242"&gt; /u/Chance-Studio-8242 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ombg19hz5vuf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5h18a/has_anyone_gotten_hold_of_dgx_spark_for_running/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5h18a/has_anyone_gotten_hold_of_dgx_spark_for_running/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T11:24:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5af83</id>
    <title>What happened to Small LM?</title>
    <updated>2025-10-13T04:41:42+00:00</updated>
    <author>
      <name>/u/icm76</name>
      <uri>https://old.reddit.com/user/icm76</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Basically the title. Some time ago they were all over the place...&lt;/p&gt; &lt;p&gt;Thank you &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/icm76"&gt; /u/icm76 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5af83/what_happened_to_small_lm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5af83/what_happened_to_small_lm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5af83/what_happened_to_small_lm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T04:41:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5e3zy</id>
    <title>With ROCm support on the RX9060xt 16gb do we have a cheap alternative to 64gb of Vram?</title>
    <updated>2025-10-13T08:27:27+00:00</updated>
    <author>
      <name>/u/Loskas2025</name>
      <uri>https://old.reddit.com/user/Loskas2025</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5e3zy/with_rocm_support_on_the_rx9060xt_16gb_do_we_have/"&gt; &lt;img alt="With ROCm support on the RX9060xt 16gb do we have a cheap alternative to 64gb of Vram?" src="https://external-preview.redd.it/2pEp6h9DVo4Sdr9wjgy_p89eQv-zFl1B4zrMdKUtXdM.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=de95310c04e2ea4306b5a78e4f6935e9c2389359" title="With ROCm support on the RX9060xt 16gb do we have a cheap alternative to 64gb of Vram?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/jbbtsazy9uuf1.png?width=1310&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4739761e7cf8822ba0ff7df6139e1f5d74252f8e"&gt;from https://videocardz.com/newz/amd-releases-rocm-7-0-2-with-radeon-rx-9060-support&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Reading the news and considering that a card costs €300 + VAT, with €1200 + VAT you can get 4 cards for a total of 64GB of VRAM. I don't know the performance of the new drivers and I hope someone here tests them soon, but it seems like good news. Opinions? Also 160W x 4 = 640W. Cheap.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Loskas2025"&gt; /u/Loskas2025 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5e3zy/with_rocm_support_on_the_rx9060xt_16gb_do_we_have/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5e3zy/with_rocm_support_on_the_rx9060xt_16gb_do_we_have/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5e3zy/with_rocm_support_on_the_rx9060xt_16gb_do_we_have/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T08:27:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1o54lfc</id>
    <title>Roo Code, Cline, Opencode, Codex, Qwen CLI, Claude Code, Aider etc.</title>
    <updated>2025-10-12T23:51:27+00:00</updated>
    <author>
      <name>/u/Sorry_Ad191</name>
      <uri>https://old.reddit.com/user/Sorry_Ad191</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi has anyone put all these (Roo Code, Cline, Opencode, Codex, Qwen CLI, Claude Code, Aider) to the test? I've been using mostly Roo Code and quite happy with it but im wondering am I missing out not using Claude Code or one of the other ones? Is one or a couple of these massively better than all the others? Oh I guess there is Openhands and a few more as well.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sorry_Ad191"&gt; /u/Sorry_Ad191 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o54lfc/roo_code_cline_opencode_codex_qwen_cli_claude/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o54lfc/roo_code_cline_opencode_codex_qwen_cli_claude/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o54lfc/roo_code_cline_opencode_codex_qwen_cli_claude/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T23:51:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4qix3</id>
    <title>Claude's system prompt length has now exceeded 30k tokens</title>
    <updated>2025-10-12T14:29:52+00:00</updated>
    <author>
      <name>/u/StableSable</name>
      <uri>https://old.reddit.com/user/StableSable</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4qix3/claudes_system_prompt_length_has_now_exceeded_30k/"&gt; &lt;img alt="Claude's system prompt length has now exceeded 30k tokens" src="https://external-preview.redd.it/otAtlKXoVGIzRX_D-XS8ef102ismRuSmY-rYGjCWHEI.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=345f8e8a9693f1ecf3f281e2c9b37a5656e8634f" title="Claude's system prompt length has now exceeded 30k tokens" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StableSable"&gt; /u/StableSable &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/asgeirtj/system_prompts_leaks/blob/main/Anthropic/claude-4.5-sonnet.md"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4qix3/claudes_system_prompt_length_has_now_exceeded_30k/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4qix3/claudes_system_prompt_length_has_now_exceeded_30k/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T14:29:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5gbbf</id>
    <title>LM Studio + Snapdragon Laptops = Bad experience</title>
    <updated>2025-10-13T10:44:07+00:00</updated>
    <author>
      <name>/u/Andrew_C0</name>
      <uri>https://old.reddit.com/user/Andrew_C0</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello. I've been running into this issue recently that I'm unable to debug or fix whatsoever. &lt;/p&gt; &lt;p&gt;Using the latest version of LM Studio (0.3.30) on my Snapdragon Laptop (a Slim 7X - the 32GB RAM version), I get pretty great experience first time I run LM Studio. I tried recently Qwen3 1.7B model just to test it out, and I get around 50 tokens/s, which is great. &lt;/p&gt; &lt;p&gt;However, that only works the first time the model is loaded. Afterwards, if I want to eject the model and use another one (let's say, Qwen3 4B), I get somewhat arount 0.02 tokens/s. I just don't get why. If I want to reload the same 1.7B model, I get the same token performance.&lt;/p&gt; &lt;p&gt;What I've noticed is that rebooting the laptop and loading the model again, it fixes the issue (in regards to whatever model I load first, including Qwen3 Coder 30B), but as soon as I eject and load another model, until I reboot, the tokens/s is always under 1 t/s. &lt;/p&gt; &lt;p&gt;I haven't altered any settings, so I just downloaded the model, loaded it in, and that's it. &lt;/p&gt; &lt;p&gt;I had the same experience using a Surface Laptop 7 in the past, with an older version of LM Studio, but after some updates, it was somehow fixed.&lt;/p&gt; &lt;p&gt;Any help is greatly appreciated to fix this.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Andrew_C0"&gt; /u/Andrew_C0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5gbbf/lm_studio_snapdragon_laptops_bad_experience/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5gbbf/lm_studio_snapdragon_laptops_bad_experience/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5gbbf/lm_studio_snapdragon_laptops_bad_experience/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T10:44:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5dh3v</id>
    <title>What's the missing piece in the LLaMA ecosystem right now?</title>
    <updated>2025-10-13T07:45:58+00:00</updated>
    <author>
      <name>/u/Street-Lie-2584</name>
      <uri>https://old.reddit.com/user/Street-Lie-2584</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The LLaMA model ecosystem is exploding with new variants and fine-tunes. &lt;/p&gt; &lt;p&gt;But what's the biggest gap or most underdeveloped area still holding it back? &lt;/p&gt; &lt;p&gt;For me, it's the &lt;strong&gt;&lt;em&gt;data prep and annotation tools&lt;/em&gt;&lt;/strong&gt;. The models are getting powerful, but cleaning and structuring quality training data for fine-tuning is still a major, manual bottleneck. &lt;/p&gt; &lt;p&gt;What do you think is the most missing piece? &lt;/p&gt; &lt;p&gt;Better/easier fine-tuning tools?&lt;br /&gt; More accessible hardware solutions?&lt;br /&gt; Something else entirely?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Street-Lie-2584"&gt; /u/Street-Lie-2584 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5dh3v/whats_the_missing_piece_in_the_llama_ecosystem/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5dh3v/whats_the_missing_piece_in_the_llama_ecosystem/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5dh3v/whats_the_missing_piece_in_the_llama_ecosystem/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T07:45:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5dly2</id>
    <title>Gemini 2.5 pro / Deep Think VS local LLM</title>
    <updated>2025-10-13T07:54:36+00:00</updated>
    <author>
      <name>/u/Dumperandumper</name>
      <uri>https://old.reddit.com/user/Dumperandumper</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m on « Ultra » plan with google since 3 months now and while I was cool with their discovery offer (149€/ month) I have now 3 days left to cancel before they start charging me 279€/ month. I did heavily use 2.5 pro and Deep Think for creative writing, brainstorming critical law related questions. I do not code. I have to admit Gemini has been a huge gain in productivity but 279€/ month is such a heavy price just to have access to Deep Think. My question is : are there any local LLM that I can run, even slowly, on my hardware that are good enough compared to what I have been used to ? I’ve got a macbook pro M3 max 128gb ram. How well can I do ? Any pointer greatly appreciated. Apologies for my english. Frenchman here&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dumperandumper"&gt; /u/Dumperandumper &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5dly2/gemini_25_pro_deep_think_vs_local_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5dly2/gemini_25_pro_deep_think_vs_local_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5dly2/gemini_25_pro_deep_think_vs_local_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T07:54:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5ec73</id>
    <title>Seeking for a uncensored NSFW Dutch language.</title>
    <updated>2025-10-13T08:42:12+00:00</updated>
    <author>
      <name>/u/jobbie1973</name>
      <uri>https://old.reddit.com/user/jobbie1973</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, i am seeking for a local uncensored nsfw Dutch dataset for KoboltCPP and SillyTavern locally on my pc, AMD 7-5800/32GB Ram/RTX3060Ti/8GB.&lt;/p&gt; &lt;p&gt;i am planned to write some VN-Story.&lt;/p&gt; &lt;p&gt;Friendly greetings from Netherlands.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jobbie1973"&gt; /u/jobbie1973 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5ec73/seeking_for_a_uncensored_nsfw_dutch_language/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5ec73/seeking_for_a_uncensored_nsfw_dutch_language/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5ec73/seeking_for_a_uncensored_nsfw_dutch_language/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T08:42:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4mwet</id>
    <title>GPU Poor LLM Arena is BACK! 🎉🎊🥳</title>
    <updated>2025-10-12T11:43:02+00:00</updated>
    <author>
      <name>/u/kastmada</name>
      <uri>https://old.reddit.com/user/kastmada</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4mwet/gpu_poor_llm_arena_is_back/"&gt; &lt;img alt="GPU Poor LLM Arena is BACK! 🎉🎊🥳" src="https://external-preview.redd.it/xnvppfD8q4Rvrqs00KT2LLxfAKmO_ypt1REhqFgxlVw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=49fb1bf881d9a00b0e731a0269d44b4ea6c31968" title="GPU Poor LLM Arena is BACK! 🎉🎊🥳" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;🚀 GPU Poor LLM Arena is BACK! New Models &amp;amp; Updates!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;First off, a massive apology for the extended silence. Things have been a bit hectic, but the GPU Poor LLM Arena is officially back online and ready for action! Thanks for your patience and for sticking around.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;🚀 Newly Added Models:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Granite 4.0 Small Unsloth (32B, 4-bit)&lt;/li&gt; &lt;li&gt;Granite 4.0 Tiny Unsloth (7B, 4-bit)&lt;/li&gt; &lt;li&gt;Granite 4.0 Micro Unsloth (3B, 8-bit)&lt;/li&gt; &lt;li&gt;Qwen 3 Instruct 2507 Unsloth (4B, 8-bit)&lt;/li&gt; &lt;li&gt;Qwen 3 Thinking 2507 Unsloth (4B, 8-bit)&lt;/li&gt; &lt;li&gt;Qwen 3 Instruct 2507 Unsloth (30B, 4-bit)&lt;/li&gt; &lt;li&gt;OpenAI gpt-oss Unsloth (20B, 4-bit)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;🚨 Important Notes for GPU-Poor Warriors:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Please be aware that Granite 4.0 Small, Qwen 3 30B, and OpenAI gpt-oss models are quite bulky. Ensure your setup can comfortably handle them before diving in to avoid any performance issues.&lt;/li&gt; &lt;li&gt;I've decided to default to Unsloth GGUFs for now. In many cases, these offer valuable bug fixes and optimizations over the original GGUFs.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm happy to see you back in the arena, testing out these new additions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kastmada"&gt; /u/kastmada &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/spaces/k-mktr/gpu-poor-llm-arena"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4mwet/gpu_poor_llm_arena_is_back/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4mwet/gpu_poor_llm_arena_is_back/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T11:43:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4uagn</id>
    <title>Traning Llama3.2:3b on my whatsapp chats with wife</title>
    <updated>2025-10-12T16:56:53+00:00</updated>
    <author>
      <name>/u/jayjay_1996</name>
      <uri>https://old.reddit.com/user/jayjay_1996</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;So my wife and I have been dating since 2018. ALL our chats are on WhatsApp. &lt;/p&gt; &lt;p&gt;I am an LLM noob but I wanted to export it as a txt. And then feed it into an LLM so I could ask questions like:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;who has said I love you more? &lt;/li&gt; &lt;li&gt;who apologises more? &lt;/li&gt; &lt;li&gt;what was discussed during our Japan trip? &lt;/li&gt; &lt;li&gt;how many times did we fight in July 2023? &lt;/li&gt; &lt;li&gt;who is more sarcastic in 2025? &lt;/li&gt; &lt;li&gt;list all the people we’ve talked about&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Etc&lt;/p&gt; &lt;p&gt;So far - the idea was to chunk them and store them in a vector DB. And then use llama to interact with it. But the results have been quite horrible. Temp - 0.1 to 0.5, k=3 to 25. Broke the chat into chunks of 4000 with overlap 100 &lt;/p&gt; &lt;p&gt;Any better ideas out there? Would love to hear! And if it works I could share the ingestion script!&lt;/p&gt; &lt;p&gt;Edit - I’ve reduced the chunk size to 250. And ingesting it via llama3.2:3b. Currently - 14 hours out of 34 done! Another 20 hours and I could let you know how that turns out ☠️ &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jayjay_1996"&gt; /u/jayjay_1996 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4uagn/traning_llama323b_on_my_whatsapp_chats_with_wife/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4uagn/traning_llama323b_on_my_whatsapp_chats_with_wife/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4uagn/traning_llama323b_on_my_whatsapp_chats_with_wife/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T16:56:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1o54wbu</id>
    <title>Did you create a new benchmark? Good, keep it to yourself, don't release how it works until something beats it.</title>
    <updated>2025-10-13T00:05:35+00:00</updated>
    <author>
      <name>/u/EmirTanis</name>
      <uri>https://old.reddit.com/user/EmirTanis</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Only release leaderboards / charts. This is the only way to avoid pollution / interference from the AI companies. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EmirTanis"&gt; /u/EmirTanis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o54wbu/did_you_create_a_new_benchmark_good_keep_it_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o54wbu/did_you_create_a_new_benchmark_good_keep_it_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o54wbu/did_you_create_a_new_benchmark_good_keep_it_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T00:05:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5auc8</id>
    <title>Meta Superintelligence group publishes paper on new RAG technique</title>
    <updated>2025-10-13T05:04:45+00:00</updated>
    <author>
      <name>/u/ttkciar</name>
      <uri>https://old.reddit.com/user/ttkciar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5auc8/meta_superintelligence_group_publishes_paper_on/"&gt; &lt;img alt="Meta Superintelligence group publishes paper on new RAG technique" src="https://external-preview.redd.it/JCvftI08SHl-gSbgzIC77Ii1UGjMaLNJCQ7drY7JAIc.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b567d36377fcc16b8bef230712988e58520db703" title="Meta Superintelligence group publishes paper on new RAG technique" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ttkciar"&gt; /u/ttkciar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://paddedinputs.substack.com/p/meta-superintelligences-surprising"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5auc8/meta_superintelligence_group_publishes_paper_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5auc8/meta_superintelligence_group_publishes_paper_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T05:04:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4v880</id>
    <title>New Unhinged NSFW Reasoning Model - Satyr-V0.1-4B</title>
    <updated>2025-10-12T17:32:33+00:00</updated>
    <author>
      <name>/u/ThePantheonUnbound</name>
      <uri>https://old.reddit.com/user/ThePantheonUnbound</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This version is an unpredictable experiment and may produce vulgar, explicit, or graphic content. Please use it at your own risk. More multifaceted versions will be released soon.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThePantheonUnbound"&gt; /u/ThePantheonUnbound &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/PantheonUnbound/Satyr-V0.1-4B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4v880/new_unhinged_nsfw_reasoning_model_satyrv014b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4v880/new_unhinged_nsfw_reasoning_model_satyrv014b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T17:32:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5fgdv</id>
    <title>Open-source RAG routes are splintering — MiniRAG, Agent-UniRAG, SymbioticRAG… which one are you actually using?</title>
    <updated>2025-10-13T09:53:05+00:00</updated>
    <author>
      <name>/u/Cheryl_Apple</name>
      <uri>https://old.reddit.com/user/Cheryl_Apple</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been poking around the open-source RAG scene and the variety is wild — not just incremental forks, but fundamentally different philosophies.&lt;/p&gt; &lt;p&gt;Quick sketch:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;MiniRAG&lt;/strong&gt;: ultra-light, pragmatic — built to run cheaply/locally.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agent-UniRAG&lt;/strong&gt;: retrieval + reasoning as one continuous agent pipeline.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;SymbioticRAG&lt;/strong&gt;: human-in-the-loop + feedback learning; treats users as part of the retrieval model.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RAGFlow / Verba / LangChain-style stacks&lt;/strong&gt;: modular toolkits that let you mix &amp;amp; match retrievers, rerankers, and LLMs.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What surprises me is how differently they behave depending on the use case: small internal KBs vs. web-scale corpora, single-turn factual Qs vs. multi-hop reasoning, and latency/infra constraints. Anecdotally I’ve seen MiniRAG beat heavier stacks on latency and robustness for small corpora, while agentic approaches seem stronger on multi-step reasoning — but results vary a lot by dataset and prompt strategy.&lt;/p&gt; &lt;p&gt;There’s a community effort (search for &lt;strong&gt;RagView&lt;/strong&gt; on GitHub or ragview.ai) that aggregates side-by-side comparisons — worth a look if you want apples-to-apples experiments.&lt;/p&gt; &lt;p&gt;So I’m curious from people here who actually run these in research or production:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Which RAG route gives you the best trade-off between &lt;strong&gt;accuracy, speed, and controllability&lt;/strong&gt;?&lt;/li&gt; &lt;li&gt;What failure modes surprised you (hallucinations, context loss, latency cliffs)?&lt;/li&gt; &lt;li&gt;Any practical tips for choosing between a lightweight vs. agentic approach?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Drop your real experiences (not marketing). Concrete numbers, odd bugs, or short config snippets are gold.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cheryl_Apple"&gt; /u/Cheryl_Apple &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5fgdv/opensource_rag_routes_are_splintering_minirag/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5fgdv/opensource_rag_routes_are_splintering_minirag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5fgdv/opensource_rag_routes_are_splintering_minirag/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T09:53:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4wg6q</id>
    <title>Stanford Researchers Released AgentFlow: Flow-GRPO algorithm. Outperforming 200B GPT-4o with a 7B model! Explore the code &amp; try the demo</title>
    <updated>2025-10-12T18:18:58+00:00</updated>
    <author>
      <name>/u/balianone</name>
      <uri>https://old.reddit.com/user/balianone</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4wg6q/stanford_researchers_released_agentflow_flowgrpo/"&gt; &lt;img alt="Stanford Researchers Released AgentFlow: Flow-GRPO algorithm. Outperforming 200B GPT-4o with a 7B model! Explore the code &amp;amp; try the demo" src="https://external-preview.redd.it/WYNRuaBJiIIoNSwO7SSTGPP2ITAUNljSMCnTROkhRdg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=11110ecc44cc993c095ca5cc3a864cd7384ecf18" title="Stanford Researchers Released AgentFlow: Flow-GRPO algorithm. Outperforming 200B GPT-4o with a 7B model! Explore the code &amp;amp; try the demo" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/balianone"&gt; /u/balianone &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/spaces/AgentFlow/agentflow"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4wg6q/stanford_researchers_released_agentflow_flowgrpo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4wg6q/stanford_researchers_released_agentflow_flowgrpo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T18:18:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1o58klk</id>
    <title>I rue the day they first introduced "this is not X, this is &lt;unearned superlative&gt;' to LLM training data</title>
    <updated>2025-10-13T03:05:58+00:00</updated>
    <author>
      <name>/u/Comfortable-Rock-498</name>
      <uri>https://old.reddit.com/user/Comfortable-Rock-498</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;- This isn't just a bug, this is a fundamental design flaw&lt;/p&gt; &lt;p&gt;- This isn't just a recipe, this is a culinary journey&lt;/p&gt; &lt;p&gt;- This isn't a change, this is a seismic shift&lt;/p&gt; &lt;p&gt;- This isn't about font choice, this is about the very soul of design&lt;/p&gt; &lt;p&gt;- This isn't a refactor, this is a fundamental design overhaul&lt;/p&gt; &lt;p&gt;- This isn't a spreadsheet, this is a blueprint of a billion dollar business&lt;/p&gt; &lt;p&gt;And it seems to have spread to all LLMs now, to the point that you have to consciously avoid this phrasing everywhere if you're a human writer&lt;/p&gt; &lt;p&gt;Perhaps the idea of Model Collapse (&lt;a href="https://en.wikipedia.org/wiki/Model_collapse"&gt;https://en.wikipedia.org/wiki/Model_collapse&lt;/a&gt;) is not unreasonable.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Comfortable-Rock-498"&gt; /u/Comfortable-Rock-498 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o58klk/i_rue_the_day_they_first_introduced_this_is_not_x/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o58klk/i_rue_the_day_they_first_introduced_this_is_not_x/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o58klk/i_rue_the_day_they_first_introduced_this_is_not_x/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T03:05:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
