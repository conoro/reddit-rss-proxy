<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-02T18:27:13+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1pcbh2p</id>
    <title>I want to share a model with the community, but how?</title>
    <updated>2025-12-02T15:33:02+00:00</updated>
    <author>
      <name>/u/TastyWriting8360</name>
      <uri>https://old.reddit.com/user/TastyWriting8360</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello community, I wanted to ask, what would be the correct approach to release my model? hugging face? also how do I go about evaluating it?&lt;/p&gt; &lt;p&gt;Do I just upload the weights or quantize it first?&lt;/p&gt; &lt;p&gt;It's nothing big deal, but maybe some people will find it interesting, I am honestly more into making stuff than publishing them.&lt;/p&gt; &lt;p&gt;I just want to share this thing and see what the community think of it fully open source made with a humble budget.&lt;/p&gt; &lt;p&gt;Regards&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TastyWriting8360"&gt; /u/TastyWriting8360 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcbh2p/i_want_to_share_a_model_with_the_community_but_how/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcbh2p/i_want_to_share_a_model_with_the_community_but_how/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pcbh2p/i_want_to_share_a_model_with_the_community_but_how/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-02T15:33:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1pcbid1</id>
    <title>Qwen's Latest Image Generation vs. Gemini nano banana pro</title>
    <updated>2025-12-02T15:34:28+00:00</updated>
    <author>
      <name>/u/pmes9866</name>
      <uri>https://old.reddit.com/user/pmes9866</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcbid1/qwens_latest_image_generation_vs_gemini_nano/"&gt; &lt;img alt="Qwen's Latest Image Generation vs. Gemini nano banana pro" src="https://b.thumbs.redditmedia.com/CRiFRnvhsMw_WbFExbQty5IHylro0udTGgo9qA2NkOo.jpg" title="Qwen's Latest Image Generation vs. Gemini nano banana pro" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So the China-region version of Qwen rolled out an update today, including image generation. I suspect that might be the ‚Äòqwen image edit 2511‚Äô model.&lt;br /&gt; So I used it to compare against Gemini Nano Banana Pro&lt;/p&gt; &lt;p&gt;prompt:&lt;/p&gt; &lt;p&gt;A top-down view of a breakfast tray. There are exactly 3 blueberry pancakes stacked on a white plate in the center. To the left of the plate is a glass of orange juice. To the right is a silver fork. Above the plate is one red strawberry. The table is made of dark oak wood.&lt;/p&gt; &lt;p&gt;A close-up photograph of a neon sign glowing on a brick wall at night. The neon sign clearly reads &amp;quot;MODEL TEST 2024&amp;quot; in blue and pink tubes. Below it, smaller paint text on the brick says &amp;quot;AI GENERATION LAB&amp;quot;. It is raining, and the neon reflects on wet pavement.&lt;/p&gt; &lt;p&gt;‚ÄãA dreamlike landscape where a giant ancient oak tree grows inside a crumbling gothic cathedral. The tree's branches have replaced the roof, and its leaves are made of glowing open books floating in the air. A river made of flowing stars flows through the cathedral aisle. Cinematic, magical atmosphere, muted colors.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmes9866"&gt; /u/pmes9866 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pcbid1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcbid1/qwens_latest_image_generation_vs_gemini_nano/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pcbid1/qwens_latest_image_generation_vs_gemini_nano/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-02T15:34:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1pc7pgu</id>
    <title>LM Studio beta supports Qwen3 80b Next.</title>
    <updated>2025-12-02T12:55:23+00:00</updated>
    <author>
      <name>/u/sleepingsysadmin</name>
      <uri>https://old.reddit.com/user/sleepingsysadmin</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sleepingsysadmin"&gt; /u/sleepingsysadmin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/lmstudio/status/1995646603919606140"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pc7pgu/lm_studio_beta_supports_qwen3_80b_next/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pc7pgu/lm_studio_beta_supports_qwen3_80b_next/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-02T12:55:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1pbnadc</id>
    <title>My logical reasoning benchmark just got owned by DeepSeek V3.2 Speciale</title>
    <updated>2025-12-01T20:06:59+00:00</updated>
    <author>
      <name>/u/fairydreaming</name>
      <uri>https://old.reddit.com/user/fairydreaming</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbnadc/my_logical_reasoning_benchmark_just_got_owned_by/"&gt; &lt;img alt="My logical reasoning benchmark just got owned by DeepSeek V3.2 Speciale" src="https://preview.redd.it/hli4hr98bn4g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=666636cd797f91736d9b2deed97e109b078febcc" title="My logical reasoning benchmark just got owned by DeepSeek V3.2 Speciale" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;DeepSeek V3.2 Speciale made only a single mistake in my &lt;a href="https://github.com/fairydreaming/lineage-bench"&gt;lineage-bench&lt;/a&gt; benchmark.&lt;/p&gt; &lt;p&gt;Compared to my &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ij1ew9/lineagebench_benchmark_results_updated_with/"&gt;previous benchmarking attempts&lt;/a&gt; I reduced the number of quizzes in the benchmark run from 800 to 160 and increased difficulty by using lineage relationship graphs of sizes 8, 64, 128 and 192 (previously it was 8, 16, 32 and 64).&lt;/p&gt; &lt;p&gt;If anyone is interested in details see the &lt;a href="https://github.com/fairydreaming/lineage-bench#description"&gt;project description&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fairydreaming"&gt; /u/fairydreaming &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hli4hr98bn4g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbnadc/my_logical_reasoning_benchmark_just_got_owned_by/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pbnadc/my_logical_reasoning_benchmark_just_got_owned_by/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-01T20:06:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1pcb8wj</id>
    <title>How to run Qwen3-Next-80B GGUF on Ryzen AI MAX 395 (Strix Halo) with ROCm in just 3 commands (Linux or Windows)</title>
    <updated>2025-12-02T15:24:25+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcb8wj/how_to_run_qwen3next80b_gguf_on_ryzen_ai_max_395/"&gt; &lt;img alt="How to run Qwen3-Next-80B GGUF on Ryzen AI MAX 395 (Strix Halo) with ROCm in just 3 commands (Linux or Windows)" src="https://preview.redd.it/hcj385hmzs4g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bb193e685658df74937722add3bb1e17505b95db" title="How to run Qwen3-Next-80B GGUF on Ryzen AI MAX 395 (Strix Halo) with ROCm in just 3 commands (Linux or Windows)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was excited to see Qwen3-Next support merge into llama.cpp over the weekend and wanted to make sure support in Lemonade was ready ASAP. As far as I know, this is one of the easiest ways to get Qwen3-Next up and running with ROCm on the Strix Halo GPU.&lt;/p&gt; &lt;h1&gt;Quick Start Instructions&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Ubuntu&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;code&gt;wget https://github.com/lemonade-sdk/lemonade/releases/latest/download/lemonade-server-minimal_9.0.5_amd64.deb&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;sudo dpkg -i lemonade-server-minimal_9.0.5_amd64.deb&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;lemonade-server run Qwen3-Next-80B-A3B-Instruct-GGUF --llamacpp rocm&lt;/code&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Windows&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Go to &lt;a href="https://lemonade-server.ai"&gt;https://lemonade-server.ai&lt;/a&gt;, click download, and run &lt;code&gt;lemoande-server-minimal.msi&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Open a terminal and run &lt;code&gt;lemonade-server run Qwen3-Next-80B-A3B-Instruct-GGUF --llamacpp rocm&lt;/code&gt;&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;What Happens&lt;/h1&gt; &lt;p&gt;&lt;code&gt;lemonade-server run MODEL --llamacpp rocm&lt;/code&gt; automatically does the following:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Downloads a build of llamacpp + ROCm 7.10 from &lt;a href="https://github.com/lemonade-sdk/llamacpp-rocm"&gt;https://github.com/lemonade-sdk/llamacpp-rocm&lt;/a&gt; (which in turn is building llamacpp source code against a fresh nightly from TheRock)&lt;/li&gt; &lt;li&gt;Downloads the model from &lt;a href="https://huggingface.co/unsloth/Qwen3-Next-80B-A3B-Instruct-GGUF"&gt;https://huggingface.co/unsloth/Qwen3-Next-80B-A3B-Instruct-GGUF&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Launches a llama-server process with those two artifacts and makes it available via Lemonade's reverse-proxy (so other models and backends can be hot swapped from the same URL)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;What to Expect&lt;/h1&gt; &lt;p&gt;The model doesn't run super fast yet. I am seeing about 10 TPS with ROCm and 13 TPS with Vulkan in some very unofficial testing, which is less than I'd expect for a fully optimized 80B-A3B. This is definitely more &amp;quot;trying out the bleeding edge&amp;quot; than a model I'd use as a daily driver.&lt;/p&gt; &lt;h1&gt;Acknowledgement&lt;/h1&gt; &lt;p&gt;The amazing maintainers of llama.cpp, Unsloth, and TheRock did 99% of the work here (if not more).&lt;/p&gt; &lt;p&gt;My teammate Daniel and I just automated everything to make a 3-command quick start possible!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hcj385hmzs4g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcb8wj/how_to_run_qwen3next80b_gguf_on_ryzen_ai_max_395/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pcb8wj/how_to_run_qwen3next80b_gguf_on_ryzen_ai_max_395/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-02T15:24:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1pcbbff</id>
    <title>That's not bad benchmarks ig ü§î</title>
    <updated>2025-12-02T15:27:10+00:00</updated>
    <author>
      <name>/u/Independent-Wind4462</name>
      <uri>https://old.reddit.com/user/Independent-Wind4462</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcbbff/thats_not_bad_benchmarks_ig/"&gt; &lt;img alt="That's not bad benchmarks ig ü§î" src="https://preview.redd.it/wrznpfa37t4g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ffce093e013d6bb6289465170a1298d0940b8e3b" title="That's not bad benchmarks ig ü§î" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Wind4462"&gt; /u/Independent-Wind4462 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wrznpfa37t4g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcbbff/thats_not_bad_benchmarks_ig/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pcbbff/thats_not_bad_benchmarks_ig/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-02T15:27:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1pc1xeg</id>
    <title>Arcee AI debuts Trinity models - Mini (26B-A3B) and Nano (6B-A1B preview)</title>
    <updated>2025-12-02T07:03:31+00:00</updated>
    <author>
      <name>/u/AppearanceHeavy6724</name>
      <uri>https://old.reddit.com/user/AppearanceHeavy6724</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pc1xeg/arcee_ai_debuts_trinity_models_mini_26ba3b_and/"&gt; &lt;img alt="Arcee AI debuts Trinity models - Mini (26B-A3B) and Nano (6B-A1B preview)" src="https://external-preview.redd.it/qbjwf5GDjlrOZc3hH1_u8ZJsfHPJX8EhbfLKlPIDDIw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=583b519c5f4efc89c89b9e615eee90c542c70dee" title="Arcee AI debuts Trinity models - Mini (26B-A3B) and Nano (6B-A1B preview)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AppearanceHeavy6724"&gt; /u/AppearanceHeavy6724 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.arcee.ai/blog/the-trinity-manifesto"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pc1xeg/arcee_ai_debuts_trinity_models_mini_26ba3b_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pc1xeg/arcee_ai_debuts_trinity_models_mini_26ba3b_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-02T07:03:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1pcfeiq</id>
    <title>32B model stress test: Qwen 2.5/Coder/3 on dual RTX 5060 Ti (zero failures)</title>
    <updated>2025-12-02T17:57:49+00:00</updated>
    <author>
      <name>/u/Defilan</name>
      <uri>https://old.reddit.com/user/Defilan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I ran comprehensive stress testing on three different 32B parameter models to see if consumer-grade hardware can handle production workloads. Spoiler: it can.&lt;/p&gt; &lt;h1&gt;Hardware Setup&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;2x NVIDIA RTX 5060 Ti (16GB each, Blackwell architecture)&lt;/li&gt; &lt;li&gt;32GB total VRAM&lt;/li&gt; &lt;li&gt;CUDA acceleration with layer-based sharding across GPUs&lt;/li&gt; &lt;li&gt;Completely air-gapped capable setup&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Models Tested (10 iterations each)&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Qwen 2.5 32B Instruct&lt;/strong&gt; (general-purpose)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Qwen 2.5 Coder 32B Instruct&lt;/strong&gt; (code-specialized)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Qwen 3 32B&lt;/strong&gt; (latest generation)&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Results Summary&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;&lt;strong&gt;Model&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;Gen Speed&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;P50 Latency&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;P99 Latency&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;VRAM Usage&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;Load Time&lt;/strong&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen 2.5 32B&lt;/td&gt; &lt;td align="left"&gt;16.6 tok/s&lt;/td&gt; &lt;td align="left"&gt;4.4s&lt;/td&gt; &lt;td align="left"&gt;4.9s&lt;/td&gt; &lt;td align="left"&gt;18-24GB&lt;/td&gt; &lt;td align="left"&gt;18s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen 2.5 Coder 32B&lt;/td&gt; &lt;td align="left"&gt;16.5 tok/s&lt;/td&gt; &lt;td align="left"&gt;4.9s&lt;/td&gt; &lt;td align="left"&gt;5.9s&lt;/td&gt; &lt;td align="left"&gt;18-24GB&lt;/td&gt; &lt;td align="left"&gt;32s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen 3 32B&lt;/td&gt; &lt;td align="left"&gt;16.2 tok/s&lt;/td&gt; &lt;td align="left"&gt;15.8s&lt;/td&gt; &lt;td align="left"&gt;15.9s&lt;/td&gt; &lt;td align="left"&gt;18-24GB&lt;/td&gt; &lt;td align="left"&gt;28s&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Total iterations:&lt;/strong&gt; 30 (10 per model) &lt;strong&gt;Failures:&lt;/strong&gt; 0 (no OOM errors, no thermal throttling)&lt;/p&gt; &lt;h1&gt;Key Findings&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;1. Consistent Performance&lt;/strong&gt; All three models maintained steady ~16.5 tok/s generation across 10 iterations. No degradation over time.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. Production-Ready Latency&lt;/strong&gt; Two out of three models hit sub-6s P99 latency, which is acceptable for most enterprise internal tools (not trying to compete with ChatGPT's instant responses, but good enough for specialized business applications).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3. Zero Stability Issues&lt;/strong&gt; 30 total iterations plus warmup requests. Not a single OOM failure or crash. This is critical for production environments where reliability matters more than raw speed.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;4. Qwen 3 Latency Anomaly&lt;/strong&gt; Qwen 3 had significantly higher latency (15.8s vs 4.4s) due to different context window defaults and attention mechanisms. This isn't a regression, just different config. You'd tune this in production based on your use case.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;5. Model Load Times Matter&lt;/strong&gt; 18-32 seconds from cold start to first token. For environments where pods restart (Kubernetes, etc.), this is important to factor in. Persistent caching helps here.&lt;/p&gt; &lt;h1&gt;What This Proves&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Consumer GPUs work for 32B models.&lt;/strong&gt; You don't need A100s or H100s for this class of model.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multi-GPU sharding is stable.&lt;/strong&gt; Layer-based distribution across cards worked flawlessly.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;$2,500 in consumer hardware&lt;/strong&gt; can handle enterprise-scale inference that most people assume requires $30,000+ datacenter cards.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Testing Methodology&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Used LLMKube (Kubernetes operator) for orchestration&lt;/li&gt; &lt;li&gt;Each model: 2 warmup requests + 10 benchmark iterations&lt;/li&gt; &lt;li&gt;Max tokens: 256 per request&lt;/li&gt; &lt;li&gt;Automatic cleanup between models&lt;/li&gt; &lt;li&gt;Monitored GPU utilization, temps, and VRAM throughout&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Limitations &amp;amp; Next Steps&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Only tested Qwen family models (want to test Llama, Mistral next)&lt;/li&gt; &lt;li&gt;Haven't pushed to true 70B models yet (that's the next stress test)&lt;/li&gt; &lt;li&gt;GPU utilization was 45-55%, suggesting potential optimization opportunities&lt;/li&gt; &lt;li&gt;Only tested GGUF quantized models (Q4_K_M variants)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Why I Did This&lt;/h1&gt; &lt;p&gt;I'm building infrastructure for organizations that can't use cloud APIs (defense, healthcare, finance). They need to know: can they run serious models on hardware they can actually procure? Answer: yes.&lt;/p&gt; &lt;p&gt;Full writeup with detailed methodology: &lt;a href="https://llmkube.com/blog/shadowstack-32b-stress-test"&gt;https://llmkube.com/blog/shadowstack-32b-stress-test&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer questions about the setup, methodology, or results. Also interested in hearing what models you'd want to see tested next.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Defilan"&gt; /u/Defilan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcfeiq/32b_model_stress_test_qwen_25coder3_on_dual_rtx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcfeiq/32b_model_stress_test_qwen_25coder3_on_dual_rtx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pcfeiq/32b_model_stress_test_qwen_25coder3_on_dual_rtx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-02T17:57:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1pc4muy</id>
    <title>I tested qwen next 80b instruct with Claude Code on llama.cpp. I created the Tetris game and it worked 100% locally.</title>
    <updated>2025-12-02T09:59:52+00:00</updated>
    <author>
      <name>/u/LegacyRemaster</name>
      <uri>https://old.reddit.com/user/LegacyRemaster</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pc4muy/i_tested_qwen_next_80b_instruct_with_claude_code/"&gt; &lt;img alt="I tested qwen next 80b instruct with Claude Code on llama.cpp. I created the Tetris game and it worked 100% locally." src="https://b.thumbs.redditmedia.com/dxqgNeWrqYlDzGfOOVHaitPbgyshMiQU3w10smIopiE.jpg" title="I tested qwen next 80b instruct with Claude Code on llama.cpp. I created the Tetris game and it worked 100% locally." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/cez74xm8kr4g1.png?width=1410&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=182c75d9e77ae9ad6fa887b5e84862ca85e3fb88"&gt;Qwen next unsloth + Claude Code Local. &lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5rvdacbbkr4g1.png?width=1293&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7390eb3fff094830677defb858782854ed717cc6"&gt;Tetris done&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I ran this test on an RTX 5070ti + 128GB DDR 3200. About 11 tokens per second. Not the fastest, but since it's all automated, it completed the task in about 11 minutes. I'm testing different models to see which one generates the best results in the shortest time. For now, qwen next is fine.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LegacyRemaster"&gt; /u/LegacyRemaster &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pc4muy/i_tested_qwen_next_80b_instruct_with_claude_code/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pc4muy/i_tested_qwen_next_80b_instruct_with_claude_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pc4muy/i_tested_qwen_next_80b_instruct_with_claude_code/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-02T09:59:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1pbl22j</id>
    <title>transformers v5 is out!</title>
    <updated>2025-12-01T18:45:03+00:00</updated>
    <author>
      <name>/u/unofficialmerve</name>
      <uri>https://old.reddit.com/user/unofficialmerve</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbl22j/transformers_v5_is_out/"&gt; &lt;img alt="transformers v5 is out!" src="https://b.thumbs.redditmedia.com/sx5PM1Scf98WyxbVipRmJM0LcS-1L5bG1HP2F_EmhxU.jpg" title="transformers v5 is out!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks, it's Merve from Hugging Face! üëãüèª&lt;/p&gt; &lt;p&gt;I'm here with big news: today we release transformers v5! üôåüèª&lt;/p&gt; &lt;p&gt;With this, we enable interoperability with our friends in ecosystem (llama.cpp, vLLM and others) from training to inference, simplify the addition of new models and significantly improve the library ü§ó&lt;/p&gt; &lt;p&gt;We have written a blog on the changes, would love to hear your feedback!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/hl2gx5yd1n4g1.png?width=1800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3b21e4f7f786f42df4b56566e523138103ea07ab"&gt;https://preview.redd.it/hl2gx5yd1n4g1.png?width=1800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3b21e4f7f786f42df4b56566e523138103ea07ab&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/unofficialmerve"&gt; /u/unofficialmerve &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbl22j/transformers_v5_is_out/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbl22j/transformers_v5_is_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pbl22j/transformers_v5_is_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-01T18:45:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1pc702b</id>
    <title>For every closed model, there is an open source alternative</title>
    <updated>2025-12-02T12:19:22+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In the early days of LLMs, there is an opinion that proprietary LLMs are far better than open-source.&lt;/p&gt; &lt;p&gt;However, this opinion is proved wrong by many of the popular open-source models. I tried multiple open-source models and I'm sharing this list as this will be useful to many.&lt;/p&gt; &lt;p&gt;Here are my open source alternatives to popular closed models.&lt;/p&gt; &lt;p&gt;Sonnet 4.5 ‚Üí GLM 4.6 / Minimax m2&lt;/p&gt; &lt;p&gt;Gemini 3 pro ‚Üí Deepseek v3.2 Speciale&lt;/p&gt; &lt;p&gt;Nano Banana ‚Üí Qwen Image Edit&lt;/p&gt; &lt;p&gt;Grok code fast ‚Üí Qwen 3 Coder&lt;/p&gt; &lt;p&gt;GPT 5 ‚Üí Deepseek v3.2&lt;/p&gt; &lt;p&gt;Let me know your favorite open source alternatives.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pc702b/for_every_closed_model_there_is_an_open_source/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pc702b/for_every_closed_model_there_is_an_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pc702b/for_every_closed_model_there_is_an_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-02T12:19:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1pbs9u9</id>
    <title>WebGPU Finally, it is compatible with all major browsers</title>
    <updated>2025-12-01T23:21:23+00:00</updated>
    <author>
      <name>/u/Illustrious-Swim9663</name>
      <uri>https://old.reddit.com/user/Illustrious-Swim9663</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbs9u9/webgpu_finally_it_is_compatible_with_all_major/"&gt; &lt;img alt="WebGPU Finally, it is compatible with all major browsers" src="https://preview.redd.it/5red1ziseo4g1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e6ef97359d12315990eddc88d51923dc498b8a3b" title="WebGPU Finally, it is compatible with all major browsers" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Post : &lt;a href="https://web.dev/blog/webgpu-supported-major-browsers?hl=es-419#browser_and_os_availability"&gt;https://web.dev/blog/webgpu-supported-major-browsers?hl=es-419#browser_and_os_availability&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Illustrious-Swim9663"&gt; /u/Illustrious-Swim9663 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5red1ziseo4g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbs9u9/webgpu_finally_it_is_compatible_with_all_major/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pbs9u9/webgpu_finally_it_is_compatible_with_all_major/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-01T23:21:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1pc2zz6</id>
    <title>Apple releases open weights video model</title>
    <updated>2025-12-02T08:11:20+00:00</updated>
    <author>
      <name>/u/zxyzyxz</name>
      <uri>https://old.reddit.com/user/zxyzyxz</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zxyzyxz"&gt; /u/zxyzyxz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://starflow-v.github.io/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pc2zz6/apple_releases_open_weights_video_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pc2zz6/apple_releases_open_weights_video_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-02T08:11:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1pcbb8o</id>
    <title>Mistral 3 announcement</title>
    <updated>2025-12-02T15:26:58+00:00</updated>
    <author>
      <name>/u/JChataigne</name>
      <uri>https://old.reddit.com/user/JChataigne</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcbb8o/mistral_3_announcement/"&gt; &lt;img alt="Mistral 3 announcement" src="https://external-preview.redd.it/QLQU1soiMTzFAm8GzW6EPDbaX5jrcYYFqy1ql5NYoiQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4c3b97e1405ebb7916bf71d7b9a3da9a44efaea7" title="Mistral 3 announcement" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Mistral 3 includes three state-of-the-art small, dense models (14B, 8B, and 3B) and Mistral Large 3, a sparse mixture-of-experts trained with 41B active and 675B total parameters. All models are released under the Apache 2.0 license.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JChataigne"&gt; /u/JChataigne &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://mistral.ai/news/mistral-3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcbb8o/mistral_3_announcement/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pcbb8o/mistral_3_announcement/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-02T15:26:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1pbzw8f</id>
    <title>Would you rent B300 (Blackwell Ultra) GPUs in Mongolia at ~$5/hr? (market sanity check)</title>
    <updated>2025-12-02T05:08:57+00:00</updated>
    <author>
      <name>/u/CloudPattern1313</name>
      <uri>https://old.reddit.com/user/CloudPattern1313</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I work for a small-ish team that somehow ended up with a pile of B300 (Blackwell Ultra) allocations and a half-empty data center in Ulaanbaatar (yes, the capital of Mongolia, yes, the coldest one).&lt;/p&gt; &lt;p&gt;Important bit so this doesn‚Äôt sound totally random:&lt;br /&gt; ~40% of our initial build-out is already committed (local gov/enterprise workloads + two research labs). My actual job right now is to figure out what to do with the &lt;em&gt;rest&lt;/em&gt; of the capacity ‚Äî I‚Äôve started cold-reaching a few teams in KR/JP/SG/etc., and Reddit is my ‚Äútalk to actual humans‚Äù channel.&lt;/p&gt; &lt;p&gt;Boss looked at the latency numbers, yelled ‚ÄúEUREKA,‚Äù and then voluntold me to do ‚Äúmarket research on Reddit‚Äù because apparently that‚Äôs a legitimate business strategy in 2025.&lt;/p&gt; &lt;p&gt;So here‚Äôs the deal (numbers are real, measured yesterday):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;B300 bare-metal:&lt;/strong&gt; ‚âà &lt;strong&gt;$5 / GPU-hour&lt;/strong&gt; on-demand (reserved is way lower)&lt;/li&gt; &lt;li&gt;Ping from the DC right now: &lt;ul&gt; &lt;li&gt;Beijing ~35 ms&lt;/li&gt; &lt;li&gt;Seoul ~85 ms&lt;/li&gt; &lt;li&gt;Tokyo ~95 ms&lt;/li&gt; &lt;li&gt;Singapore ~110 ms&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Experience:&lt;/strong&gt; full root, no hypervisor, 3.2 Tb/s InfiniBand, PyTorch + SLURM pre-installed so you don‚Äôt hate us immediately&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Jurisdiction:&lt;/strong&gt; hosted in Mongolia ‚Üí neutral territory, no magical backdoors or surprise subpoenas from the usual suspects&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Questions I was literally told to ask (lightly edited from my boss‚Äôs Slack message):&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Would any team in South Korea / Japan / Singapore / Taiwan / HK / Vietnam / Indonesia actually use this instead of CoreWeave, Lambda, or the usual suspects for training/fine-tuning/inference?&lt;/li&gt; &lt;li&gt;Does the whole &lt;strong&gt;‚Äú&lt;/strong&gt;cold steppe bare-metal neutrality&lt;strong&gt;‚Äù&lt;/strong&gt; thing sound like a real benefit or just weird marketing?&lt;/li&gt; &lt;li&gt;How many GPUs do you normally burn through and for how long? (Boss keeps saying ‚Äúeveryone wants 256-GPU clusters for three years‚Äù and I‚Äôm‚Ä¶ unconvinced.)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Landing page my designer made at 3 a.m.: &lt;a href="https://b300.fibo.cloud"&gt;https://b300.fibo.cloud&lt;/a&gt; (still WIP, don‚Äôt judge the fonts).&lt;/p&gt; &lt;p&gt;Thanks in advance, and sorry if this breaks any rules ‚Äî I read the sidebar twice üôÇ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CloudPattern1313"&gt; /u/CloudPattern1313 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbzw8f/would_you_rent_b300_blackwell_ultra_gpus_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbzw8f/would_you_rent_b300_blackwell_ultra_gpus_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pbzw8f/would_you_rent_b300_blackwell_ultra_gpus_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-02T05:08:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1pc6fks</id>
    <title>[Release] We built Step-Audio-R1: The first open-source Audio LLM that truly Reasons (CoT) and Scales ‚Äì Beats Gemini 2.5 Pro on Audio Benchmarks.</title>
    <updated>2025-12-02T11:48:13+00:00</updated>
    <author>
      <name>/u/BadgerProfessional43</name>
      <uri>https://old.reddit.com/user/BadgerProfessional43</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;üî• TL;DR:&lt;/strong&gt; We (the StepFun AI team) just released the weights for Step-Audio-R1, an audio-language model that performs Chain-of-Thought (CoT) reasoning directly on acoustic features. This solves the persistent &amp;quot;inverted scaling&amp;quot; problem in audio LLMs.&lt;/p&gt; &lt;hr /&gt; &lt;h3&gt;üëã Hello, &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; Community! (The System 2 Audio LLM)&lt;/h3&gt; &lt;p&gt;We've seen some of you discussing Step-Audio-R1 already, and we wanted to jump in as the creators to give a technical deep dive and answer any questions.&lt;/p&gt; &lt;p&gt;Most multi-modal LLMs (especially in audio) cheat: they transcribe the audio and then just reason over the &lt;em&gt;text&lt;/em&gt;. This fails when the acoustic nuance (tone, emotion, multiple speakers, sound effects) is key. We fixed this.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Step-Audio-R1 is the first audio model that successfully benefits from test-time compute scaling.&lt;/strong&gt; This means the model gets better, not worse, when given more time/tokens to think.&lt;/p&gt; &lt;h3&gt;üß† The Technical Breakthrough: Modality-Grounded Reasoning&lt;/h3&gt; &lt;p&gt;The core innovation is our training framework: &lt;strong&gt;Modality-Grounded Reasoning Distillation (MGRD)&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Traditional models rely on &lt;strong&gt;Textual Surrogate Reasoning&lt;/strong&gt;. They think like this: 1. Input Audio $\rightarrow$ 2. Transcribe to Text $\rightarrow$ 3. Reason on Text $\rightarrow$ 4. Output.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;MGRD&lt;/strong&gt; forces the model (based on Qwen2.5 32B + Qwen2 Audio Encoder) to ground its thoughts in the acoustic data itself. It generates explicit reasoning (e.g., using &lt;code&gt;&amp;lt;think&amp;gt;&lt;/code&gt; tokens) that is directly tied to the underlying sound, not just the transcript. This is how we solved the &amp;quot;inverted scaling&amp;quot; anomaly‚Äîa huge step for reliable audio intelligence.&lt;/p&gt; &lt;h3&gt;üìà Performance: Benchmarking against the Best&lt;/h3&gt; &lt;p&gt;We focused on complex audio reasoning benchmarks where this acoustic understanding is non-negotiable.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Result:&lt;/strong&gt; Step-Audio-R1 &lt;strong&gt;surpasses&lt;/strong&gt; Gemini 2.5 Pro and is comparable to Gemini 3 across comprehensive audio benchmarks. We are making extended deliberation an asset, not a liability.&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h3&gt;üíª Important: Hardware &amp;amp; Quantization (We Need Your Help!)&lt;/h3&gt; &lt;p&gt;We are committed to accessibility, but this is a large, state-of-the-art model built on a 32B parameter base.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;VRAM Requirement (FP16/BF16):&lt;/strong&gt; The base model requires approximately &lt;strong&gt;65 GB - 70 GB VRAM&lt;/strong&gt; for deployment (We tested it successfully on a 4-GPU cluster using vLLM, as detailed in our README).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;vLLM Support:&lt;/strong&gt; Inference code is included with customized vLLM support for high throughput.&lt;/li&gt; &lt;/ul&gt; &lt;h4&gt;&lt;strong&gt;Call to Action: GGUF/Quantization Request!&lt;/strong&gt;&lt;/h4&gt; &lt;p&gt;To bring Step-Audio-R1 to single-card users (e.g., those with 24GB 3090/4090s), we urgently need help from the community's expert quantizers.&lt;/p&gt; &lt;p&gt;If you are skilled in creating &lt;strong&gt;GGUF&lt;/strong&gt; or &lt;strong&gt;EXL2&lt;/strong&gt; quants, please reach out! Your work will enable thousands of local users to try the model. Feel free to tag experts like &lt;a href="/u/TheBloke"&gt;u/TheBloke&lt;/a&gt; in the comments‚Äîwe want to collaborate!&lt;/p&gt; &lt;hr /&gt; &lt;h3&gt;üîó Links and Next Steps&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GitHub Repository (Code &amp;amp; Documentation):&lt;/strong&gt; &lt;code&gt;[https://github.com/stepfun-ai/Step-Audio-R1]&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hugging Face Model Card (Weights):&lt;/strong&gt; &lt;code&gt;[https://huggingface.co/stepfun-ai/Step-Audio-R1]&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Technical Report (arXiv):&lt;/strong&gt; &lt;code&gt;[https://arxiv.org/pdf/2511.15848]&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Live Demo (HF Spaces/Gradio):&lt;/strong&gt; &lt;code&gt;[https://stepaudiollm.github.io/step-audio-r1/]&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Ask us anything about MGRD, the training data, the Qwen2 integration, or the inference stack! We'll be answering questions for the next several hours.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BadgerProfessional43"&gt; /u/BadgerProfessional43 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pc6fks/release_we_built_stepaudior1_the_first_opensource/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pc6fks/release_we_built_stepaudior1_the_first_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pc6fks/release_we_built_stepaudior1_the_first_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-02T11:48:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1pcbwse</id>
    <title>Hello mistral 3 apache 2.0</title>
    <updated>2025-12-02T15:49:33+00:00</updated>
    <author>
      <name>/u/Loud_Possibility_148</name>
      <uri>https://old.reddit.com/user/Loud_Possibility_148</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcbwse/hello_mistral_3_apache_20/"&gt; &lt;img alt="Hello mistral 3 apache 2.0" src="https://b.thumbs.redditmedia.com/Cj0hrg2BJsZ8pqxQlSMLqUTbP7S0NWRpDhN6VS8tO5c.jpg" title="Hello mistral 3 apache 2.0" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Loud_Possibility_148"&gt; /u/Loud_Possibility_148 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pcbwse"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcbwse/hello_mistral_3_apache_20/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pcbwse/hello_mistral_3_apache_20/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-02T15:49:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1pcao54</id>
    <title>New Mistral Large 3 just dropped on AWS Bedrock! Hope it will be open source...</title>
    <updated>2025-12-02T15:01:50+00:00</updated>
    <author>
      <name>/u/aspaler</name>
      <uri>https://old.reddit.com/user/aspaler</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcao54/new_mistral_large_3_just_dropped_on_aws_bedrock/"&gt; &lt;img alt="New Mistral Large 3 just dropped on AWS Bedrock! Hope it will be open source..." src="https://preview.redd.it/enk30ruj2t4g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a4bf496f80a5708e0e237231242b3a52313dc4eb" title="New Mistral Large 3 just dropped on AWS Bedrock! Hope it will be open source..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aspaler"&gt; /u/aspaler &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/enk30ruj2t4g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcao54/new_mistral_large_3_just_dropped_on_aws_bedrock/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pcao54/new_mistral_large_3_just_dropped_on_aws_bedrock/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-02T15:01:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1pcbr10</id>
    <title>Ministral WebGPU: Run Mistral's new multimodal models 100% locally in your browser.</title>
    <updated>2025-12-02T15:43:30+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcbr10/ministral_webgpu_run_mistrals_new_multimodal/"&gt; &lt;img alt="Ministral WebGPU: Run Mistral's new multimodal models 100% locally in your browser." src="https://external-preview.redd.it/a2FpOGJodms5dDRnMVOJ9FmD9w2-LMCVXdFIiBg8ZPjaS6tgqxX1OyhMPvmT.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f21be91171f3f1c63baa540518e8447e2d1bdca9" title="Ministral WebGPU: Run Mistral's new multimodal models 100% locally in your browser." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Today, Mistral released &lt;strong&gt;Mistral 3&lt;/strong&gt;, a family of multimodal models, including three start-of-the-art dense models (3B, 8B, and 14B) and Mistral Large 3 (675B, 41B active). All Apache 2.0! ü§ó Surprisingly, the 3B is small enough to run 100% locally in your browser with WebGPU acceleration, powered by Transformers.js.&lt;/p&gt; &lt;p&gt;Link to demo: &lt;a href="https://huggingface.co/spaces/mistralai/Ministral_3B_WebGPU"&gt;https://huggingface.co/spaces/mistralai/Ministral_3B_WebGPU&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/vwrcg6vk9t4g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcbr10/ministral_webgpu_run_mistrals_new_multimodal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pcbr10/ministral_webgpu_run_mistrals_new_multimodal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-02T15:43:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1pceipb</id>
    <title>Mistral just released Mistral 3 ‚Äî a full open-weight model family from 3B all the way up to 675B parameters.</title>
    <updated>2025-12-02T17:26:06+00:00</updated>
    <author>
      <name>/u/InternationalToe2678</name>
      <uri>https://old.reddit.com/user/InternationalToe2678</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;All models are Apache 2.0 and fully usable for research + commercial work.&lt;/p&gt; &lt;p&gt;Quick breakdown:&lt;/p&gt; &lt;p&gt;‚Ä¢ Ministral 3 (3B / 8B / 14B) ‚Äì compact, multimodal, and available in base, instruct, and reasoning variants. Surprisingly strong for their size.&lt;/p&gt; &lt;p&gt;‚Ä¢ Mistral Large 3 (675B MoE) ‚Äì their new flagship. Strong multilingual performance, high efficiency, and one of the most capable open-weight instruct models released so far.&lt;/p&gt; &lt;p&gt;Why it matters: You now get a full spectrum of open models that cover everything from on-device reasoning to large enterprise-scale intelligence. The release pushes the ecosystem further toward distributed, open AI instead of closed black-box APIs.&lt;/p&gt; &lt;p&gt;Full announcement: &lt;a href="https://mistral.ai/news/mistral-3"&gt;https://mistral.ai/news/mistral-3&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InternationalToe2678"&gt; /u/InternationalToe2678 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pceipb/mistral_just_released_mistral_3_a_full_openweight/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pceipb/mistral_just_released_mistral_3_a_full_openweight/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pceipb/mistral_just_released_mistral_3_a_full_openweight/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-02T17:26:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1pc6i8v</id>
    <title>Only the real ones remember (he is still the contributor with the most likes for his models)</title>
    <updated>2025-12-02T11:52:35+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pc6i8v/only_the_real_ones_remember_he_is_still_the/"&gt; &lt;img alt="Only the real ones remember (he is still the contributor with the most likes for his models)" src="https://b.thumbs.redditmedia.com/jy0GG6iG37_fVgalRs58BxRL8j79pwfEfAMtQhgLw_c.jpg" title="Only the real ones remember (he is still the contributor with the most likes for his models)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hugging Face space by TCTF: Top Contributors To Follow - November 2025: &lt;a href="https://huggingface.co/spaces/TCTF/TCTF"&gt;https://huggingface.co/spaces/TCTF/TCTF&lt;/a&gt;&lt;br /&gt; Team mradermacher and Bartowski on the podium, legends.&lt;br /&gt; From Yaƒüƒ±z √áalƒ±k on ùïè: &lt;a href="https://x.com/Weyaxi/status/1995814979543371869"&gt;https://x.com/Weyaxi/status/1995814979543371869&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pc6i8v"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pc6i8v/only_the_real_ones_remember_he_is_still_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pc6i8v/only_the_real_ones_remember_he_is_still_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-02T11:52:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1pcbgmj</id>
    <title>mistralai/Mistral-Large-3-675B-Instruct-2512 ¬∑ Hugging Face</title>
    <updated>2025-12-02T15:32:33+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcbgmj/mistralaimistrallarge3675binstruct2512_hugging/"&gt; &lt;img alt="mistralai/Mistral-Large-3-675B-Instruct-2512 ¬∑ Hugging Face" src="https://external-preview.redd.it/tLSf_merNDQULK3Jdg8DJGTdVpkoi90eALqoBj4xphE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b646bbc99c44641ebff83b25f4515841101f565f" title="mistralai/Mistral-Large-3-675B-Instruct-2512 ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Mistral just released their biggest model!!!&lt;/p&gt; &lt;p&gt;From our family of large models, &lt;strong&gt;Mistral Large 3&lt;/strong&gt; is a state-of-the-art general-purpose &lt;strong&gt;Multimodal granular Mixture-of-Experts&lt;/strong&gt; model with &lt;strong&gt;41B active parameters&lt;/strong&gt; and &lt;strong&gt;675B total parameters&lt;/strong&gt; trained from the ground up with 3000 H200s.&lt;/p&gt; &lt;p&gt;This model is the instruct post-trained version in &lt;strong&gt;FP8&lt;/strong&gt;, fine-tuned for instruction tasks, making it ideal for chat, agentic and instruction based use cases.&lt;br /&gt; Designed for reliability and long-context comprehension - It is engineered for production-grade assistants, retrieval-augmented systems, scientific workloads, and complex enterprise workflows.&lt;/p&gt; &lt;p&gt;Learn more in our blog post &lt;a href="https://mistral.ai/news/mistral-3"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Mistral Large 3 is deployable on-premises in:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;FP8&lt;/strong&gt; on a single node of B200s or H200s.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/mistralai/Mistral-Large-3-675B-Instruct-2512-NVFP4"&gt;NVFP4&lt;/a&gt; on a single node of H100s or A100s.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We provide a &lt;a href="https://huggingface.co/mistralai/Mistral-Large-3-675B-Instruct-2512-BF16"&gt;BF16&lt;/a&gt; version if needed.&lt;/p&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/mistralai/Mistral-Large-3-675B-Instruct-2512#key-features"&gt;&lt;/a&gt;&lt;/h1&gt; &lt;h1&gt;Key Features&lt;/h1&gt; &lt;p&gt;Mistral Large 3 consists of two main architectural components:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;A Granular MoE Language Model with 673B params and 39B active&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;A 2.5B Vision Encoder&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The Mistral Large 3 Instruct model offers the following capabilities:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Vision&lt;/strong&gt;: Enables the model to analyze images and provide insights based on visual content, in addition to text.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multilingual&lt;/strong&gt;: Supports dozens of languages, including English, French, Spanish, German, Italian, Portuguese, Dutch, Chinese, Japanese, Korean, Arabic.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;System Prompt&lt;/strong&gt;: Maintains strong adherence and support for system prompts.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agentic&lt;/strong&gt;: Offers best-in-class agentic capabilities with native function calling and JSON outputting.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Frontier&lt;/strong&gt;: Delivers best-in-class performance.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Apache 2.0 License&lt;/strong&gt;: Open-source license allowing usage and modification for both commercial and non-commercial purposes.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Large Context Window&lt;/strong&gt;: Supports a 256k context window.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/mistralai/Mistral-Large-3-675B-Instruct-2512"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcbgmj/mistralaimistrallarge3675binstruct2512_hugging/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pcbgmj/mistralaimistrallarge3675binstruct2512_hugging/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-02T15:32:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1pcb50r</id>
    <title>Ministral-3 has been released</title>
    <updated>2025-12-02T15:20:25+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcb50r/ministral3_has_been_released/"&gt; &lt;img alt="Ministral-3 has been released" src="https://b.thumbs.redditmedia.com/a0DyjW1DyWh-ddE3J9WOyZjKJiBbmcXRGjqX2TH__QM.jpg" title="Ministral-3 has been released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/mistralai/Ministral-3-14B-Reasoning-2512"&gt;https://huggingface.co/mistralai/Ministral-3-14B-Reasoning-2512&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mistralai/Ministral-3-14B-Instruct-2512"&gt;https://huggingface.co/mistralai/Ministral-3-14B-Instruct-2512&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mistralai/Ministral-3-14B-Base-2512"&gt;https://huggingface.co/mistralai/Ministral-3-14B-Base-2512&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The largest model in the Ministral 3 family, &lt;strong&gt;Ministral 3 14B&lt;/strong&gt; offers frontier capabilities and performance comparable to its larger &lt;a href="https://huggingface.co/mistralai/Mistral-Small-3.2-Instruct-2506"&gt;Mistral Small 3.2 24B&lt;/a&gt; counterpart. A powerful and efficient language model with vision capabilities.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mistralai/Ministral-3-8B-Reasoning-2512"&gt;https://huggingface.co/mistralai/Ministral-3-8B-Reasoning-2512&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mistralai/Ministral-3-8B-Instruct-2512"&gt;https://huggingface.co/mistralai/Ministral-3-8B-Instruct-2512&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mistralai/Ministral-3-8B-Base-2512"&gt;https://huggingface.co/mistralai/Ministral-3-8B-Base-2512&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A balanced model in the Ministral 3 family, &lt;strong&gt;Ministral 3 8B&lt;/strong&gt; is a powerful, efficient tiny language model with vision capabilities.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mistralai/Ministral-3-3B-Reasoning-2512"&gt;https://huggingface.co/mistralai/Ministral-3-3B-Reasoning-2512&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mistralai/Ministral-3-3B-Instruct-2512"&gt;https://huggingface.co/mistralai/Ministral-3-3B-Instruct-2512&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mistralai/Ministral-3-3B-Base-2512"&gt;https://huggingface.co/mistralai/Ministral-3-3B-Base-2512&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The smallest model in the Ministral 3 family, &lt;strong&gt;Ministral 3 3B&lt;/strong&gt; is a powerful, efficient tiny language model with vision capabilities.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/471e4lma6t4g1.png?width=1078&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c23d37e6a361041132ccec451c0a03921acc6e13"&gt;https://preview.redd.it/471e4lma6t4g1.png?width=1078&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c23d37e6a361041132ccec451c0a03921acc6e13&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/c2szd14b6t4g1.png?width=1210&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3d97fc5e8626f25f8c13a5b159e6351976f45de5"&gt;https://preview.redd.it/c2szd14b6t4g1.png?width=1210&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3d97fc5e8626f25f8c13a5b159e6351976f45de5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/unsloth/Ministral-3-14B-Reasoning-2512-GGUF"&gt;https://huggingface.co/unsloth/Ministral-3-14B-Reasoning-2512-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/unsloth/Ministral-3-14B-Instruct-2512-GGUF"&gt;https://huggingface.co/unsloth/Ministral-3-14B-Instruct-2512-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/unsloth/Ministral-3-8B-Reasoning-2512-GGUF"&gt;https://huggingface.co/unsloth/Ministral-3-8B-Reasoning-2512-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/unsloth/Ministral-3-8B-Instruct-2512-GGUF"&gt;https://huggingface.co/unsloth/Ministral-3-8B-Instruct-2512-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/unsloth/Ministral-3-3B-Reasoning-2512-GGUF"&gt;https://huggingface.co/unsloth/Ministral-3-3B-Reasoning-2512-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/unsloth/Ministral-3-3B-Instruct-2512-GGUF"&gt;https://huggingface.co/unsloth/Ministral-3-3B-Instruct-2512-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcb50r/ministral3_has_been_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcb50r/ministral3_has_been_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pcb50r/ministral3_has_been_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-02T15:20:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1pcayfs</id>
    <title>Mistral 3 Blog post</title>
    <updated>2025-12-02T15:13:14+00:00</updated>
    <author>
      <name>/u/rerri</name>
      <uri>https://old.reddit.com/user/rerri</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcayfs/mistral_3_blog_post/"&gt; &lt;img alt="Mistral 3 Blog post" src="https://external-preview.redd.it/QLQU1soiMTzFAm8GzW6EPDbaX5jrcYYFqy1ql5NYoiQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4c3b97e1405ebb7916bf71d7b9a3da9a44efaea7" title="Mistral 3 Blog post" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rerri"&gt; /u/rerri &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://mistral.ai/news/mistral-3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcayfs/mistral_3_blog_post/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pcayfs/mistral_3_blog_post/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-02T15:13:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
