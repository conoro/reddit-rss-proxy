<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-07T05:36:22+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1pfqm0y</id>
    <title>Speed of DeepSeek with RAM offload</title>
    <updated>2025-12-06T14:29:05+00:00</updated>
    <author>
      <name>/u/vhthc</name>
      <uri>https://old.reddit.com/user/vhthc</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have 96GB VRAM. By far not enough to run DeepSeek 3.x - bit I could upgrade my RAM so I can have the active layers on the GPU and the rest in system RAM. Yeah the RAM prices are a catastrophe but I need to run such a large model, and I don‚Äôt want to use cloud - this is locallama!&lt;/p&gt; &lt;p&gt;Has anyone tried this? What speed can I expect with a 64kb context length in prompt processing and tokens per second?&lt;/p&gt; &lt;p&gt;It would be quite the investment so if anyone has real world data that would be great!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vhthc"&gt; /u/vhthc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfqm0y/speed_of_deepseek_with_ram_offload/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfqm0y/speed_of_deepseek_with_ram_offload/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfqm0y/speed_of_deepseek_with_ram_offload/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T14:29:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1pf0q99</id>
    <title>You will own nothing and you will be happy!</title>
    <updated>2025-12-05T17:13:24+00:00</updated>
    <author>
      <name>/u/dreamyrhodes</name>
      <uri>https://old.reddit.com/user/dreamyrhodes</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Come and put everything in to cloud. We now getting into hardware as a service. The RAM craze will impact everything to the point where consumers can't afford normal hardware anymore because it's all scraped off, locked away and put into datacenters to sell to you services to store your data. (Of course that data also will be used to train AI models to sell to you as a service as well lol.)&lt;/p&gt; &lt;p&gt;You don't need RAM anymore nor do you need SSDs. You will store and process every byte of your digital life in some datacenter and pay a monthly fee to access and process it.&lt;/p&gt; &lt;p&gt;You will own nothing and you will be happy!&lt;/p&gt; &lt;p&gt;GN: WTF Just Happened? | The Corrupt Memory Industry &amp;amp; Micron&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=9A-eeJP0J7c"&gt;https://www.youtube.com/watch?v=9A-eeJP0J7c&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dreamyrhodes"&gt; /u/dreamyrhodes &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pf0q99/you_will_own_nothing_and_you_will_be_happy/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pf0q99/you_will_own_nothing_and_you_will_be_happy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pf0q99/you_will_own_nothing_and_you_will_be_happy/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T17:13:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1pftdc6</id>
    <title>Best benchmark website</title>
    <updated>2025-12-06T16:30:46+00:00</updated>
    <author>
      <name>/u/AccomplishedStory327</name>
      <uri>https://old.reddit.com/user/AccomplishedStory327</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Which website do you use to see benchmark stats of different models, apart from using your own suite?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AccomplishedStory327"&gt; /u/AccomplishedStory327 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pftdc6/best_benchmark_website/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pftdc6/best_benchmark_website/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pftdc6/best_benchmark_website/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T16:30:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfxrv5</id>
    <title>Convert Dense into MOE model?</title>
    <updated>2025-12-06T19:30:30+00:00</updated>
    <author>
      <name>/u/pmttyji</name>
      <uri>https://old.reddit.com/user/pmttyji</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I did a quick search on this here &amp;amp; found only 2 years old &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1cgo45x/converting_dense_models_into_moes/"&gt;thread&lt;/a&gt; with less replies. That's it.&lt;/p&gt; &lt;p&gt;So still no one figured it out this yet? Totally surprised that no one brought this topic here after that old thread.&lt;/p&gt; &lt;p&gt;I know it's a very big thing. But it would be a miracle if some one comes with this precious solution.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmttyji"&gt; /u/pmttyji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfxrv5/convert_dense_into_moe_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfxrv5/convert_dense_into_moe_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfxrv5/convert_dense_into_moe_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T19:30:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1pg7naj</id>
    <title>EvalCards: A Clear, Compact Format for AI Model Evaluation Reporting</title>
    <updated>2025-12-07T03:02:12+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pg7naj/evalcards_a_clear_compact_format_for_ai_model/"&gt; &lt;img alt="EvalCards: A Clear, Compact Format for AI Model Evaluation Reporting" src="https://preview.redd.it/281ua2s85p5g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7646e79db57637352ca990e598f6ff9d2cc42a36" title="EvalCards: A Clear, Compact Format for AI Model Evaluation Reporting" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;EvalCards are concise, standardized evaluation disclosure documents designed to clearly report a model‚Äôs capability and safety evaluations. &lt;/p&gt; &lt;p&gt;They focus only on essential evaluation details like &lt;/p&gt; &lt;ul&gt; &lt;li&gt;benchmarks used, &lt;/li&gt; &lt;li&gt;metrics, &lt;/li&gt; &lt;li&gt;prompting setups, &lt;/li&gt; &lt;li&gt;modalities, and &lt;/li&gt; &lt;li&gt;languages tested.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This type of compact reporting makes results easy to understand, easy to compare, and consistently visible wherever a model is released.&lt;/p&gt; &lt;p&gt;I found this type of compact and structured reporting of AI model evaluation interesting and useful. &lt;/p&gt; &lt;p&gt;Source: &lt;a href="https://arxiv.org/abs/2511.21695"&gt;EvalCards: A Framework for Standardized Evaluation Reporting&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/281ua2s85p5g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pg7naj/evalcards_a_clear_compact_format_for_ai_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pg7naj/evalcards_a_clear_compact_format_for_ai_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-07T03:02:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1pg56he</id>
    <title>Need recommendations on training datasets</title>
    <updated>2025-12-07T00:58:59+00:00</updated>
    <author>
      <name>/u/Theotheraccounti_</name>
      <uri>https://old.reddit.com/user/Theotheraccounti_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello. I've built a model that is based on the Mixture of a Million Experts paper and trained on tinystories. &lt;/p&gt; &lt;p&gt;The thing is that I'd like to test it against models of a similar size to see if the architecture is actually good and I need a good dataset to train it on. Preferably one that is small and in question-answer pairs.&lt;/p&gt; &lt;p&gt;&lt;em&gt;I cannot use a big dataset due to being on a free colab account. *&lt;/em&gt;apologies if my english is kind of bad right now.&lt;/p&gt; &lt;p&gt;Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Theotheraccounti_"&gt; /u/Theotheraccounti_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pg56he/need_recommendations_on_training_datasets/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pg56he/need_recommendations_on_training_datasets/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pg56he/need_recommendations_on_training_datasets/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-07T00:58:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1pg0jrn</id>
    <title>Built an offline voice-to-text tool for macOS using Parakeet</title>
    <updated>2025-12-06T21:29:26+00:00</updated>
    <author>
      <name>/u/_gordonclark</name>
      <uri>https://old.reddit.com/user/_gordonclark</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pg0jrn/built_an_offline_voicetotext_tool_for_macos_using/"&gt; &lt;img alt="Built an offline voice-to-text tool for macOS using Parakeet" src="https://external-preview.redd.it/aO9ax1823LtOZGcI0aK2n4il89o5Hg6Q3c3cEopkN5Y.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dc887f665d70ccae44e7f3342c50b34faf6196e4" title="Built an offline voice-to-text tool for macOS using Parakeet" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been tinkering on a little side project called &lt;strong&gt;SilentKeys&lt;/strong&gt; and figured I‚Äôd share it here in case anyone finds it useful. &lt;/p&gt; &lt;p&gt;It‚Äôs basically &lt;strong&gt;realtime offline dictation for macOS&lt;/strong&gt;. No cloud, no accounts, nothing sent anywhere, it just listens locally and types straight into whatever app you have open. I built it because I wanted dictation that didn‚Äôt ship my voice to a server.&lt;/p&gt; &lt;p&gt;It‚Äôs still early and a bit rough around the edges, but it works surprisingly well. If you‚Äôre into privacy tools, voice workflows, accessibility stuff, or just like trying weird niche projects, I‚Äôd love to hear what you think.&lt;/p&gt; &lt;p&gt;Repo‚Äôs here: &lt;a href="https://github.com/gptguy/silentkeys"&gt;https://github.com/gptguy/silentkeys&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer questions or get roasted gently.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_gordonclark"&gt; /u/_gordonclark &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/gptguy/silentkeys"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pg0jrn/built_an_offline_voicetotext_tool_for_macos_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pg0jrn/built_an_offline_voicetotext_tool_for_macos_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T21:29:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfxl6x</id>
    <title>[D] What I learned building code RAG without embeddings</title>
    <updated>2025-12-06T19:23:00+00:00</updated>
    <author>
      <name>/u/rozetyp</name>
      <uri>https://old.reddit.com/user/rozetyp</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been building a system to give LLMs relevant code context from any repo. The idea seemed simple: let an LLM look at the file tree + function signatures and pick which files to include. No embeddings, no vector DB.&lt;/p&gt; &lt;p&gt;Sharing what I learned because I wish someone had written this before I broke my eval three different ways.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. Don‚Äôt eval on famous repos&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I started testing on Flask and FastAPI. GPT got 7/10 without any context - it was just reciting training data, not using my retrieval.&lt;/p&gt; &lt;p&gt;I switched to private repos and obscure OSS (&amp;lt;1K stars). ‚ÄúNo context‚Äù dropped to ~4.9/10. That was the real baseline!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. File paths aren‚Äôt enough&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Showing the LLM `src/auth/handler.py` doesn‚Äôt really tell it what‚Äôs inside. I added AST-extracted symbols:&lt;/p&gt; &lt;p&gt;&lt;em&gt;src/auth/handler.py [login, logout, refresh_token]&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;src/auth/middleware.py [require_auth, rate_limit]&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Retrieval quality jumped noticeably (NDCG went from ~0.85 to ~0.92). The model doesn‚Äôt need to read the full file to know ‚Äúthis smells like auth.‚Äù&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3. Same-vendor judging is inflated&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;GPT-4 judging GPT-4‚Äôs answers gave suspiciously high scores! Switching to cross-vendor (GPT generates, Gemini judges) knocked about 0.5 off the scores and the reviews &lt;em&gt;felt&lt;/em&gt; more honest. The judge was much harsher on vague, confident answers.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;4. Generic eval criteria reward BS&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;My first judge prompt used vague criteria like ‚Äúshould explain error handling‚Äù. That rewarded confident wrong answers.&lt;/p&gt; &lt;p&gt;What worked better was forcing exact hooks:&lt;/p&gt; &lt;p&gt;&lt;em&gt;&lt;del&gt;‚ÄúShould explain the request lifecycle‚Äù&lt;/del&gt;&lt;/em&gt;&lt;em&gt;, &amp;quot;Must mention `RequestContext` and `full_dispatch_request()`‚Äù&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Anchoring eval on specific symbols/files made it much easier to spot hand-wavy nonsense.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Results after fixing eval (very rough):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;LLM file picker: ~0.92 NDCG, ~8.5/10 answer quality&lt;/li&gt; &lt;li&gt;Embeddings baseline: ~0.79 NDCG, ~8.6/10 answer quality&lt;/li&gt; &lt;li&gt;No context: ~4.9/10&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So the ‚ÄúLLM looks at the tree + symbols and picks files‚Äù setup landed roughly on par with embeddings on answer quality, without the indexing infrastructure. Good enough for me to keep using it.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Caveats!&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Small sample (177 questions, 14 repos)&lt;/li&gt; &lt;li&gt;I wrote the questions - probably biased toward what my approach handles&lt;/li&gt; &lt;li&gt;Private-repo results may not generalize beyond the ones I tested&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Questions for you:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;How are you building eval sets that the model &lt;em&gt;hasn‚Äôt&lt;/em&gt; basically memorized?&lt;/li&gt; &lt;li&gt;Any tricks for making LLM-as-judge less biased when you‚Äôre judging your own system?&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rozetyp"&gt; /u/rozetyp &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfxl6x/d_what_i_learned_building_code_rag_without/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfxl6x/d_what_i_learned_building_code_rag_without/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfxl6x/d_what_i_learned_building_code_rag_without/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T19:23:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfiar0</id>
    <title>Qwen3-TTS</title>
    <updated>2025-12-06T06:21:51+00:00</updated>
    <author>
      <name>/u/Terrible_Scar_9890</name>
      <uri>https://old.reddit.com/user/Terrible_Scar_9890</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/spaces/Qwen/Qwen3-TTS-Demo"&gt;https://huggingface.co/spaces/Qwen/Qwen3-TTS-Demo&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Terrible_Scar_9890"&gt; /u/Terrible_Scar_9890 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfiar0/qwen3tts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfiar0/qwen3tts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfiar0/qwen3tts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T06:21:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfsntn</id>
    <title>convert: support Mistral 3 Large MoE by ngxson ¬∑ Pull Request #17730 ¬∑ ggml-org/llama.cpp</title>
    <updated>2025-12-06T16:00:37+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfsntn/convert_support_mistral_3_large_moe_by_ngxson/"&gt; &lt;img alt="convert: support Mistral 3 Large MoE by ngxson ¬∑ Pull Request #17730 ¬∑ ggml-org/llama.cpp" src="https://external-preview.redd.it/YXlCrbFuGSaJRzk-d-1JftjUbGO215ldNJVTXMLJQi4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4507263a618891c23289c740acf9be9cc8bee393" title="convert: support Mistral 3 Large MoE by ngxson ¬∑ Pull Request #17730 ¬∑ ggml-org/llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;You can now download GGUF&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/bartowski/mistralai_Mistral-Large-3-675B-Instruct-2512-GGUF"&gt;https://huggingface.co/bartowski/mistralai_Mistral-Large-3-675B-Instruct-2512-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;but can you run it...? &lt;/p&gt; &lt;p&gt;(that another PR is &lt;a href="https://github.com/ggml-org/llama.cpp/pull/17744"&gt;https://github.com/ggml-org/llama.cpp/pull/17744&lt;/a&gt;)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/17730"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfsntn/convert_support_mistral_3_large_moe_by_ngxson/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfsntn/convert_support_mistral_3_large_moe_by_ngxson/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T16:00:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfq0kd</id>
    <title>PaperDebugger: the Best Overleaf Companion!</title>
    <updated>2025-12-06T14:01:42+00:00</updated>
    <author>
      <name>/u/NuoJohnChen</name>
      <uri>https://old.reddit.com/user/NuoJohnChen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfq0kd/paperdebugger_the_best_overleaf_companion/"&gt; &lt;img alt="PaperDebugger: the Best Overleaf Companion!" src="https://b.thumbs.redditmedia.com/LSzFW-bVRkmLrP-afZdLmy0DNmjvCz1MK2UnMO8aqLo.jpg" title="PaperDebugger: the Best Overleaf Companion!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Chrome/APP Store: &lt;a href="https://www.paperdebugger.com/"&gt;https://www.paperdebugger.com/&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Paper: &lt;a href="https://arxiv.org/abs/2512.02589"&gt;https://arxiv.org/abs/2512.02589&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Code: &lt;a href="https://github.com/PaperDebugger/PaperDebugger"&gt;https://github.com/PaperDebugger/PaperDebugger&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Enhancer: &lt;a href="https://huggingface.co/Xtra-Computing/XtraGPT-7B"&gt;https://huggingface.co/Xtra-Computing/XtraGPT-7B&lt;/a&gt; &lt;/p&gt; &lt;p&gt;An NUS team just released &amp;quot;PaperDebugger&amp;quot;: an in-editor system that uses multiple agents (Reviewer, Researcher, Scorer) to rewrite and critique papers in real-time within Overleaf. Just simply select a rough section, and it launches the full pipeline. &lt;/p&gt; &lt;p&gt;Direct Integration: No copy-pasting. It patches the document with Git-style before/after diffs.&lt;/p&gt; &lt;p&gt;Deep Research: Can pull arXiv papers, summarize them, and generate comparison tables inline.&lt;/p&gt; &lt;p&gt;Tech Stack: Uses an MCP toolchain and Kubernetes to scale the agent reasoning.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NuoJohnChen"&gt; /u/NuoJohnChen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pfq0kd"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfq0kd/paperdebugger_the_best_overleaf_companion/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfq0kd/paperdebugger_the_best_overleaf_companion/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T14:01:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfl0d8</id>
    <title>How big an open source model can I run on 128 GB unified memory?</title>
    <updated>2025-12-06T09:13:03+00:00</updated>
    <author>
      <name>/u/nameless_me</name>
      <uri>https://old.reddit.com/user/nameless_me</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just took delivery of a Minisforum MS-S1 with AMD Ryzen Ai Max+ 395 cpu, 128 GB unified memory architecture and AMD Radeon 8060S Graphics. In the BIOS the UDMA memory for the iGPU is set to 96 GB. Running a Debian Linux terminal in WSL 2, I downloaded and ran ollama which works fine.&lt;/p&gt; &lt;p&gt;Trying a Deepseek-r1:70b model, it refused to load in ollama. I checked a few sources which ended saying this &amp;quot;&lt;strong&gt;DeepSeek-R1-70B INT4 GGUF still requires ~55‚Äì60 GB VRAM equivalent&lt;/strong&gt;. &lt;strong&gt;You cannot run this model on a single consumer APU&lt;/strong&gt;, even with ‚Äú128 GB unified memory‚Äù.&lt;/p&gt; &lt;p&gt;Is the above true? What is the largest LLM model I can run reasonably on this computer?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nameless_me"&gt; /u/nameless_me &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfl0d8/how_big_an_open_source_model_can_i_run_on_128_gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfl0d8/how_big_an_open_source_model_can_i_run_on_128_gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfl0d8/how_big_an_open_source_model_can_i_run_on_128_gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T09:13:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1pg7f00</id>
    <title>Zen CPU Performance Uplift (Epyc &amp; Strix Halo) w/ ZenDNN Backend Integration for llama.cpp</title>
    <updated>2025-12-07T02:50:50+00:00</updated>
    <author>
      <name>/u/Noble00_</name>
      <uri>https://old.reddit.com/user/Noble00_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pg7f00/zen_cpu_performance_uplift_epyc_strix_halo_w/"&gt; &lt;img alt="Zen CPU Performance Uplift (Epyc &amp;amp; Strix Halo) w/ ZenDNN Backend Integration for llama.cpp" src="https://external-preview.redd.it/NDJTzKU3ltYG49f6LU-R2hFmqhxjjyJK3XNi_UF7GlA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6d8c9b4d0929a1c550bf37e26c240c96d38c9d9c" title="Zen CPU Performance Uplift (Epyc &amp;amp; Strix Halo) w/ ZenDNN Backend Integration for llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just happened to cross this and thought this seemed interesting. Here are some benchmarks:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Test Configuration&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Hardware&lt;/strong&gt;: AMD EPYC 9004 Series (Zen 4)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Threads&lt;/strong&gt;: 96&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Batch Size&lt;/strong&gt;: 4096&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Tool&lt;/strong&gt;: llama-bench&lt;/li&gt; &lt;li&gt;&lt;strong&gt;llama.cpp version&lt;/strong&gt;: 7134&lt;/li&gt; &lt;li&gt;&lt;strong&gt;ZenDNN version&lt;/strong&gt;: 1.0.0&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Environment&lt;/strong&gt;: &lt;code&gt;ZENDNNL_MATMUL_ALGO=2&lt;/code&gt; (Blocked AOCL BLIS)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;LLaMA 3.1 8B (BF16)&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Test&lt;/th&gt; &lt;th align="left"&gt;CPU t/s&lt;/th&gt; &lt;th align="left"&gt;ZenDNN t/s&lt;/th&gt; &lt;th align="left"&gt;Speedup&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;pp128&lt;/td&gt; &lt;td align="left"&gt;341.50&lt;/td&gt; &lt;td align="left"&gt;395.58&lt;/td&gt; &lt;td align="left"&gt;1.16x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;pp256&lt;/td&gt; &lt;td align="left"&gt;382.52&lt;/td&gt; &lt;td align="left"&gt;561.94&lt;/td&gt; &lt;td align="left"&gt;1.47x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;423.40&lt;/td&gt; &lt;td align="left"&gt;624.61&lt;/td&gt; &lt;td align="left"&gt;1.48x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;pp1024&lt;/td&gt; &lt;td align="left"&gt;414.12&lt;/td&gt; &lt;td align="left"&gt;637.97&lt;/td&gt; &lt;td align="left"&gt;1.54x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;pp2048&lt;/td&gt; &lt;td align="left"&gt;338.50&lt;/td&gt; &lt;td align="left"&gt;622.08&lt;/td&gt; &lt;td align="left"&gt;1.84x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;pp4096&lt;/td&gt; &lt;td align="left"&gt;308.53&lt;/td&gt; &lt;td align="left"&gt;534.76&lt;/td&gt; &lt;td align="left"&gt;1.73x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;7.28&lt;/td&gt; &lt;td align="left"&gt;10.53&lt;/td&gt; &lt;td align="left"&gt;1.45x&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;LLaMA 3.1 8B (F32)&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Test&lt;/th&gt; &lt;th align="left"&gt;CPU t/s&lt;/th&gt; &lt;th align="left"&gt;ZenDNN t/s&lt;/th&gt; &lt;th align="left"&gt;Speedup&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;pp128&lt;/td&gt; &lt;td align="left"&gt;184.44&lt;/td&gt; &lt;td align="left"&gt;293.39&lt;/td&gt; &lt;td align="left"&gt;1.59x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;pp256&lt;/td&gt; &lt;td align="left"&gt;189.69&lt;/td&gt; &lt;td align="left"&gt;384.71&lt;/td&gt; &lt;td align="left"&gt;2.03x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;234.74&lt;/td&gt; &lt;td align="left"&gt;431.21&lt;/td&gt; &lt;td align="left"&gt;1.84x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;pp1024&lt;/td&gt; &lt;td align="left"&gt;231.49&lt;/td&gt; &lt;td align="left"&gt;451.51&lt;/td&gt; &lt;td align="left"&gt;1.95x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;pp2048&lt;/td&gt; &lt;td align="left"&gt;220.05&lt;/td&gt; &lt;td align="left"&gt;425.65&lt;/td&gt; &lt;td align="left"&gt;1.93x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;pp4096&lt;/td&gt; &lt;td align="left"&gt;189.75&lt;/td&gt; &lt;td align="left"&gt;396.73&lt;/td&gt; &lt;td align="left"&gt;2.09x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;2.69&lt;/td&gt; &lt;td align="left"&gt;7.34&lt;/td&gt; &lt;td align="left"&gt;2.73x&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Merged: &lt;a href="https://github.com/ggml-org/llama.cpp/pull/17690"&gt;https://github.com/ggml-org/llama.cpp/pull/17690&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also, while disappointingly for Epyc and STX-H only it seems, it has been able to work on the Ryzen 7940HS, perhaps uplifts can be seen on consumer desktop.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Noble00_"&gt; /u/Noble00_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/discussions/17684"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pg7f00/zen_cpu_performance_uplift_epyc_strix_halo_w/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pg7f00/zen_cpu_performance_uplift_epyc_strix_halo_w/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-07T02:50:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfg0rh</id>
    <title>The Best Open-Source 8B-Parameter LLM Built in the USA</title>
    <updated>2025-12-06T04:14:17+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfg0rh/the_best_opensource_8bparameter_llm_built_in_the/"&gt; &lt;img alt="The Best Open-Source 8B-Parameter LLM Built in the USA" src="https://preview.redd.it/r6muiibadi5g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2f50b4cb0f889ed02690c8f3ff7e90713b46562c" title="The Best Open-Source 8B-Parameter LLM Built in the USA" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Rnj-1 is a family of 8B parameter open-weight, dense models trained from scratch by Essential AI, optimized for code and STEM with capabilities on par with SOTA open-weight models.&lt;/p&gt; &lt;p&gt;These models &lt;/p&gt; &lt;ul&gt; &lt;li&gt;perform well across a range of programming languages. &lt;/li&gt; &lt;li&gt;boast strong agentic capabilities (e.g., inside agentic frameworks like mini-SWE-agent). &lt;/li&gt; &lt;li&gt;excel at tool-calling.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Both raw and instruct variants are available on &lt;a href="https://huggingface.co/collections/EssentialAI/rnj-1"&gt;Hugging Face platform&lt;/a&gt;. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Model Architecture Overview&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Rnj-1's architecture is similar to Gemma 3, except that it uses only global attention, and YaRN for long-context extension.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Training Dynamics&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;rnj-1&lt;/code&gt; was pre-trained on 8.4T tokens with an 8K context length, after which the model‚Äôs context window was extended to &lt;strong&gt;32K&lt;/strong&gt; through an additional 380B-token mid-training stage. &lt;/p&gt; &lt;p&gt;A final 150B-token SFT stage completed the training to produce &lt;code&gt;rnj-1-instruct&lt;/code&gt;. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/r6muiibadi5g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfg0rh/the_best_opensource_8bparameter_llm_built_in_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfg0rh/the_best_opensource_8bparameter_llm_built_in_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T04:14:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1pg0tbe</id>
    <title>Zebra-Llama: Towards Extremely Efficient Hybrid Models</title>
    <updated>2025-12-06T21:40:53+00:00</updated>
    <author>
      <name>/u/divide0verfl0w</name>
      <uri>https://old.reddit.com/user/divide0verfl0w</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://arxiv.org/abs/2505.17272"&gt;https://arxiv.org/abs/2505.17272&lt;/a&gt;&lt;/p&gt; &lt;p&gt;HN Link: &lt;a href="https://news.ycombinator.com/item?id=46176289"&gt;https://news.ycombinator.com/item?id=46176289&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thoughts?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/divide0verfl0w"&gt; /u/divide0verfl0w &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pg0tbe/zebrallama_towards_extremely_efficient_hybrid/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pg0tbe/zebrallama_towards_extremely_efficient_hybrid/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pg0tbe/zebrallama_towards_extremely_efficient_hybrid/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T21:40:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfwu8t</id>
    <title>Are MoE models harder to Fine-tune?</title>
    <updated>2025-12-06T18:52:21+00:00</updated>
    <author>
      <name>/u/ComplexType568</name>
      <uri>https://old.reddit.com/user/ComplexType568</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;really sorry if this is a stupid question, but ive been looking around huggingface A LOT and ive noticed a really big trend where theres a ton of dense models being fine-tuned/lora-ed, while most MoE models go untouched. are there any reasons for this? &lt;/p&gt; &lt;p&gt;i dont think its the model size, as ive seen big models like Llama 70B or even 405B turn into Hermes 4 models, Tulu, etc. while pretty good models like practically the entire Qwen3 series, GLM (besides GLM Steam), DeepSeek and Kimi are untouched, id get why DS and Kimi are untouched... but, seriously, Qwen3?? so far ive seen an ArliAI finetune only. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ComplexType568"&gt; /u/ComplexType568 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfwu8t/are_moe_models_harder_to_finetune/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfwu8t/are_moe_models_harder_to_finetune/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfwu8t/are_moe_models_harder_to_finetune/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T18:52:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfyrwm</id>
    <title>Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length</title>
    <updated>2025-12-06T20:12:22+00:00</updated>
    <author>
      <name>/u/Educational-Pound269</name>
      <uri>https://old.reddit.com/user/Educational-Pound269</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfyrwm/live_avatar_streaming_realtime_audiodriven_avatar/"&gt; &lt;img alt="Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length" src="https://external-preview.redd.it/YjQybWV5bmY1bjVnMatgKKMAYanNbnGU9s9FiIXTW5q8AYgZBBw2qwcYT6Ul.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aad98486fbb5e0f55eb92c0c79b334973e80e088" title="Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;They just dropped a REALTIME, infinite length video generator.&lt;/p&gt; &lt;p&gt;Based on Wan, 20 fps, with dialogue&lt;/p&gt; &lt;p&gt;The code will be open source in early December.&lt;br /&gt; &lt;a href="https://liveavatar.github.io/"&gt;https://liveavatar.github.io/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Educational-Pound269"&gt; /u/Educational-Pound269 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/zewd3onf5n5g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfyrwm/live_avatar_streaming_realtime_audiodriven_avatar/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfyrwm/live_avatar_streaming_realtime_audiodriven_avatar/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T20:12:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1pg1rhf</id>
    <title>Minimax M2</title>
    <updated>2025-12-06T22:22:16+00:00</updated>
    <author>
      <name>/u/SlowFail2433</name>
      <uri>https://old.reddit.com/user/SlowFail2433</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What does the community think of Minimax M2?&lt;/p&gt; &lt;p&gt;Benches surprisingly well and the Minimax team tend to be strong at RL.&lt;/p&gt; &lt;p&gt;Any experiences with this model? Any tips or preferred use-cases?&lt;/p&gt; &lt;p&gt;Particularly interested in STEM, coding and agentic but all use-cases welcome&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SlowFail2433"&gt; /u/SlowFail2433 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pg1rhf/minimax_m2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pg1rhf/minimax_m2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pg1rhf/minimax_m2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T22:22:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1pg6jp6</id>
    <title>I built a minimal Claude Code clone to understand how AI coding agents work under the hood</title>
    <updated>2025-12-07T02:07:09+00:00</updated>
    <author>
      <name>/u/Money-Coast-3905</name>
      <uri>https://old.reddit.com/user/Money-Coast-3905</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pg6jp6/i_built_a_minimal_claude_code_clone_to_understand/"&gt; &lt;img alt="I built a minimal Claude Code clone to understand how AI coding agents work under the hood" src="https://preview.redd.it/7p800vqawo5g1.gif?width=640&amp;amp;crop=smart&amp;amp;s=622a2626b9442d79ae3232b55093a1ac1669ddbc" title="I built a minimal Claude Code clone to understand how AI coding agents work under the hood" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;I've been fascinated by tools like &lt;a href="https://claude.ai/code"&gt;Claude Code&lt;/a&gt; and &lt;a href="https://github.com/langchain-ai/deepagents"&gt;deepagents&lt;/a&gt; lately. While using them, I kept wondering:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;What does the system prompt actually look like?&lt;/li&gt; &lt;li&gt;How are tool schemas structured for the API?&lt;/li&gt; &lt;li&gt;How does the message flow work between turns?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So I decided to build a minimal implementation myself to understand these internals better. It's called &lt;strong&gt;yacc&lt;/strong&gt; (Yet Another Claude Code) - a simple AI coding assistant built with pure Python + Anthropic API (no LangChain).&lt;/p&gt; &lt;h3&gt;What I learned and documented:&lt;/h3&gt; &lt;p&gt;üìù &lt;strong&gt;System Prompts&lt;/strong&gt; - How to structure instructions for planning, filesystem operations, and tool usage&lt;/p&gt; &lt;p&gt;üîß &lt;strong&gt;Tool Schemas&lt;/strong&gt; - JSON schema definitions for tools like &lt;code&gt;read_file&lt;/code&gt;, &lt;code&gt;write_file&lt;/code&gt;, &lt;code&gt;edit_file&lt;/code&gt;, &lt;code&gt;grep&lt;/code&gt;, &lt;code&gt;bash&lt;/code&gt;, etc.&lt;/p&gt; &lt;p&gt;üîÑ &lt;strong&gt;Middleware patterns&lt;/strong&gt; - Prompt caching, context summarization (when tokens exceed limits), patching dangling tool calls&lt;/p&gt; &lt;p&gt;üí¨ &lt;strong&gt;Message flow&lt;/strong&gt; - How tool_use and tool_result blocks work in the conversation&lt;/p&gt; &lt;h3&gt;Not production-ready, but...&lt;/h3&gt; &lt;p&gt;This is definitely NOT a replacement for Claude Code or deepagents. It's more of a &lt;strong&gt;learning resource&lt;/strong&gt; for anyone curious about:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;How Claude's tool calling works in practice&lt;/li&gt; &lt;li&gt;What a typical agentic system prompt contains&lt;/li&gt; &lt;li&gt;How to manage context in long-running agent sessions&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;GitHub&lt;/h3&gt; &lt;p&gt;üîó &lt;a href="https://github.com/SeungyounShin/yet-another-claude-code"&gt;https://github.com/SeungyounShin/yet-another-claude-code&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The code is pretty readable and documented. Check out: - &lt;code&gt;src/prompts/system.py&lt;/code&gt; - System prompt structure - &lt;code&gt;src/tools/definitions.py&lt;/code&gt; - Tool schemas - &lt;code&gt;src/agent.py&lt;/code&gt; - Main orchestration loop - &lt;code&gt;src/middleware/&lt;/code&gt; - Context management&lt;/p&gt; &lt;p&gt;Hope this helps someone who's curious about the internals! Happy to answer any questions.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;&lt;em&gt;Inspired by &lt;a href="https://github.com/langchain-ai/deepagents"&gt;deepagents&lt;/a&gt; from LangChain team - they have a much more complete implementation if you need something production-ready.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Money-Coast-3905"&gt; /u/Money-Coast-3905 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7p800vqawo5g1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pg6jp6/i_built_a_minimal_claude_code_clone_to_understand/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pg6jp6/i_built_a_minimal_claude_code_clone_to_understand/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-07T02:07:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfvt9e</id>
    <title>VibeVoice Realtime 0.5B - OpenAI Compatible /v1/audio/speech TTS Server</title>
    <updated>2025-12-06T18:10:19+00:00</updated>
    <author>
      <name>/u/marhensa</name>
      <uri>https://old.reddit.com/user/marhensa</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfvt9e/vibevoice_realtime_05b_openai_compatible/"&gt; &lt;img alt="VibeVoice Realtime 0.5B - OpenAI Compatible /v1/audio/speech TTS Server" src="https://b.thumbs.redditmedia.com/s91ewzR1qewE9gWdg8jOP6ycdK9l1T_UjLsYMD8uNoo.jpg" title="VibeVoice Realtime 0.5B - OpenAI Compatible /v1/audio/speech TTS Server" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Microsoft recently released &lt;a href="https://huggingface.co/microsoft/VibeVoice-Realtime-0.5B"&gt;VibeVoice-Realtime-0.5B&lt;/a&gt;, a lightweight &lt;strong&gt;&lt;em&gt;expressive&lt;/em&gt;&lt;/strong&gt; TTS model.&lt;/p&gt; &lt;p&gt;I wrapped it in an OpenAI-compatible API server so it works directly with Open WebUI's TTS settings.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/marhensa/vibevoice-realtime-openai-api.git"&gt;https://github.com/marhensa/vibevoice-realtime-openai-api.git&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Drop-in using OpenAI-compatible &lt;code&gt;/v1/audio/speech&lt;/code&gt; endpoint&lt;/li&gt; &lt;li&gt;Runs locally with Docker or Python venv (via uv)&lt;/li&gt; &lt;li&gt;Using only ~2GB of VRAM&lt;/li&gt; &lt;li&gt;CUDA-optimized (around ~1x RTF on RTX 3060 12GB)&lt;/li&gt; &lt;li&gt;Multiple voices with OpenAI name aliases (alloy, nova, etc.)&lt;/li&gt; &lt;li&gt;All models auto-download on first run&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1pfvt9e/video/7emfqdbdjm5g1/player"&gt;Video demonstration of \&amp;quot;Mike\&amp;quot; male voice. Audio üì¢ ON.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The expression and flow is better than Kokoro, imho. But Kokoro is faster.&lt;/p&gt; &lt;p&gt;But (for now) it lacks female voice model, there's just two female, and one is weirdly sounds like a male üòÖ.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6r87w5d9pm5g1.png?width=1073&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=adfd10fae1523fed7f2898c38ae92816130cbf2d"&gt;vibevoice-realtime-openai-api Settings on Open WebUI: Set chunk splitting to Paragraphs.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Contribution are welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/marhensa"&gt; /u/marhensa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfvt9e/vibevoice_realtime_05b_openai_compatible/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfvt9e/vibevoice_realtime_05b_openai_compatible/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfvt9e/vibevoice_realtime_05b_openai_compatible/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T18:10:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1pg8jtk</id>
    <title>SGLang Diffusion + Cache-DiT = 20-165% Faster Local Image/Video Generation</title>
    <updated>2025-12-07T03:48:50+00:00</updated>
    <author>
      <name>/u/Expert-Pineapple-740</name>
      <uri>https://old.reddit.com/user/Expert-Pineapple-740</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Quick heads up: SGLang Diffusion now supports Cache-DiT integration, delivering 20-165% speedup for diffusion models with basically zero effort.&lt;/p&gt; &lt;p&gt;Just add some env variables and you're getting 46%+ faster inference on models like FLUX, Qwen-Image, HunyuanVideo, etc.&lt;/p&gt; &lt;p&gt;Works with torch.compile, quantization, and all the usual optimizations. Supports pretty much every major open-source DiT model.&lt;/p&gt; &lt;p&gt;Install: &lt;code&gt;uv pip install 'sglang[diffusion]' --prerelease=allow&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Docs: &lt;a href="https://github.com/sgl-project/sglang/blob/main/python/sglang/multimodal_gen/docs/cache_dit.md"&gt;https://github.com/sgl-project/sglang/blob/main/python/sglang/multimodal_gen/docs/cache_dit.md&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Expert-Pineapple-740"&gt; /u/Expert-Pineapple-740 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pg8jtk/sglang_diffusion_cachedit_20165_faster_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pg8jtk/sglang_diffusion_cachedit_20165_faster_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pg8jtk/sglang_diffusion_cachedit_20165_faster_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-07T03:48:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1pg76jo</id>
    <title>Why local coding models are less popular than hosted coding models?</title>
    <updated>2025-12-07T02:39:17+00:00</updated>
    <author>
      <name>/u/WasteTechnology</name>
      <uri>https://old.reddit.com/user/WasteTechnology</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In theory, local coding models sound very good. You don't send your most valuable assets to another company, keep everything local and under control. However, the leading AI coding startups work with hosted models (correct me if I'm wrong). Why do you think it is so?&lt;/p&gt; &lt;p&gt;If you use one, please share your setup. Which model, which engine, which coding tool do you use?, What is your experience? Do you get productive enough with them compared to hosted options?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WasteTechnology"&gt; /u/WasteTechnology &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pg76jo/why_local_coding_models_are_less_popular_than/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pg76jo/why_local_coding_models_are_less_popular_than/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pg76jo/why_local_coding_models_are_less_popular_than/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-07T02:39:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfx3d0</id>
    <title>We need open source hardware lithography</title>
    <updated>2025-12-06T19:02:38+00:00</updated>
    <author>
      <name>/u/bennmann</name>
      <uri>https://old.reddit.com/user/bennmann</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Perhaps it's time hardware was more democratized. RISC-V is only 1 step away.&lt;/p&gt; &lt;p&gt;There are real challenges with yield at small scales, requiring a clean environment. But perhaps a small scale system could be made &amp;quot;good enough&amp;quot;, or overcome with some clever tech or small vacuum chambers.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bennmann"&gt; /u/bennmann &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfx3d0/we_need_open_source_hardware_lithography/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfx3d0/we_need_open_source_hardware_lithography/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfx3d0/we_need_open_source_hardware_lithography/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T19:02:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1pg8ix9</id>
    <title>My little decentralized Locallama setup, 216gb VRAM</title>
    <updated>2025-12-07T03:47:31+00:00</updated>
    <author>
      <name>/u/Goldkoron</name>
      <uri>https://old.reddit.com/user/Goldkoron</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pg8ix9/my_little_decentralized_locallama_setup_216gb_vram/"&gt; &lt;img alt="My little decentralized Locallama setup, 216gb VRAM" src="https://preview.redd.it/o1o7ekxycp5g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=66f62e54e1c923ea71f1a1d46415562ffdcbc1ba" title="My little decentralized Locallama setup, 216gb VRAM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Goldkoron"&gt; /u/Goldkoron &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/o1o7ekxycp5g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pg8ix9/my_little_decentralized_locallama_setup_216gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pg8ix9/my_little_decentralized_locallama_setup_216gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-07T03:47:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
