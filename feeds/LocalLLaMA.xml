<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-18T12:19:41+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1r8042r</id>
    <title>A practical use case for local LLMs: reading multilingual codebases without sending code outside</title>
    <updated>2026-02-18T11:05:58+00:00</updated>
    <author>
      <name>/u/noir4y</name>
      <uri>https://old.reddit.com/user/noir4y</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I often read large codebases (OSS or internal ones) where comments and string literals&lt;/p&gt; &lt;p&gt;are written in a language I don’t speak well.&lt;/p&gt; &lt;p&gt;In many cases, I can’t just paste code into a cloud translator or API&lt;/p&gt; &lt;p&gt;— either due to privacy concerns, NDA, or simply not wanting to leak context.&lt;/p&gt; &lt;p&gt;I wanted a workflow where:&lt;/p&gt; &lt;p&gt;- code never leaves my machine&lt;/p&gt; &lt;p&gt;- translation happens only when I need it&lt;/p&gt; &lt;p&gt;- context switching is minimal&lt;/p&gt; &lt;p&gt;What ended up working well *in my case* was using a local LLM via Ollama&lt;/p&gt; &lt;p&gt;as a read-time aid rather than a full translation solution.&lt;/p&gt; &lt;p&gt;For example:&lt;/p&gt; &lt;p&gt;- I tried a few local models and settled on `translategemma:4b` for now&lt;/p&gt; &lt;p&gt;- it’s not perfect, but it was fast enough and accurate enough for understanding intent&lt;/p&gt; &lt;p&gt;- other models would likely work as well for this kind of task&lt;/p&gt; &lt;p&gt;Concretely, my setup looks like this:&lt;/p&gt; &lt;p&gt;- I run a local model via Ollama&lt;/p&gt; &lt;p&gt;- I only translate comments and string literals, not entire files&lt;/p&gt; &lt;p&gt;- latency is acceptable for interactive use (hover / on-demand)&lt;/p&gt; &lt;p&gt;The key insight for me was that for reading code,&lt;/p&gt; &lt;p&gt;I don’t need perfect translation — I need fast, private, and contextual hints.&lt;/p&gt; &lt;p&gt;After using this workflow for a while, I ended up building a small Neovim integration&lt;/p&gt; &lt;p&gt;to remove friction, but the core idea is the local-LLM-assisted reading flow itself.&lt;/p&gt; &lt;p&gt;If you’re curious, the small tool I built around this workflow is here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/noir4y/comment-translate.nvim"&gt;https://github.com/noir4y/comment-translate.nvim&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I’m curious how others approach this:&lt;/p&gt; &lt;p&gt;- What models have you found “good enough” for reading code locally?&lt;/p&gt; &lt;p&gt;- For you, in what situations does local-only translation feel worth the trade-offs compared to cloud-based tools?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/noir4y"&gt; /u/noir4y &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8042r/a_practical_use_case_for_local_llms_reading/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8042r/a_practical_use_case_for_local_llms_reading/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8042r/a_practical_use_case_for_local_llms_reading/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-18T11:05:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1r807kb</id>
    <title>Segmentation fault when loading models across multiple MI50s in llama.cpp</title>
    <updated>2026-02-18T11:11:35+00:00</updated>
    <author>
      <name>/u/EdenistTech</name>
      <uri>https://old.reddit.com/user/EdenistTech</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am using 2xMI50 32GB for inference and just added another 16GB MI50 in llama.cpp on Ubuntu 24.04 with ROCM 6.3.4. &lt;/p&gt; &lt;p&gt;Loading models unto the two 32GB card works fine. Loading a model unto the 16GB card also works fine. However, if I load a model across all three cards, I get a `Segmentation fault (core dumped)` when the model has been loaded and warmup starts. &lt;/p&gt; &lt;p&gt;Even increasing log verbosity to its highest level does not provide any insights into what is causing the seg fault. Loading a model across all cards using Vulkan backend works fine but is much, much slower than ROCM (same story with Qwen3-Next on MI50 by the way). Since Vulkan is working, I am leaning towards this being a llama.cpp/ROCM issue. Has anyone come across something similar and found a solution?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EdenistTech"&gt; /u/EdenistTech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r807kb/segmentation_fault_when_loading_models_across/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r807kb/segmentation_fault_when_loading_models_across/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r807kb/segmentation_fault_when_loading_models_across/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-18T11:11:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1r80m4o</id>
    <title>Multimodal Vector Enrichment (How to Extract Value from Images, Charts, and Tables)</title>
    <updated>2026-02-18T11:34:01+00:00</updated>
    <author>
      <name>/u/Independent-Cost-971</name>
      <uri>https://old.reddit.com/user/Independent-Cost-971</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I think most teams don't realize they're building incomplete RAG systems by only indexing text.&lt;/p&gt; &lt;p&gt;Charts, diagrams, and graphs are a big part of document content and contain most of the decision-relevant info. Yet most RAG pipelines either ignore visuals completely, extract them as raw images without interpretation, or run OCR that captures text labels but misses visual meaning.&lt;/p&gt; &lt;p&gt;I've been using multimodal enrichment where vision-language models process images in parallel with text and tables. Layout analysis detects visuals, crops each chart/diagram/graph, and the VLM interprets what it communicates. Output is natural language summaries suitable for semantic search.&lt;/p&gt; &lt;p&gt;I really think using vision-language models to enrich a vector database with images reduces hallucinations significantly. We should start treating images as first-class knowledge instead of blindly discarding them.&lt;/p&gt; &lt;p&gt;Anyway thought I should share since most people are still building text-only systems by default.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Cost-971"&gt; /u/Independent-Cost-971 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r80m4o/multimodal_vector_enrichment_how_to_extract_value/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r80m4o/multimodal_vector_enrichment_how_to_extract_value/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r80m4o/multimodal_vector_enrichment_how_to_extract_value/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-18T11:34:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7l7q5</id>
    <title>The Strix Halo feels like an amazing super power [Activation Guide]</title>
    <updated>2026-02-17T22:38:36+00:00</updated>
    <author>
      <name>/u/Potential_Block4598</name>
      <uri>https://old.reddit.com/user/Potential_Block4598</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I had my Strix halo for a while now, I though I can download and use everything out of the box, but faced some Python issues that I was able to resolve, but still performance (for CUDA) stuff was a bit underwhelming, now it feels like a superpower, I have exactly what I wanted, voice based intelligent LLM with coding and web search access, and I am sitting up still nanobot or Clawdbot and expanding, and also going to use to smartly control hue Philips and Spotify, generate images and edit them locally (ComfyUI is much better than online services since the control you get on local models is much more powerful (on the diffusion process itself!) so here is a starters guide:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Lemonade Server&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;This is the most straightforward thing for the Halo &lt;/p&gt; &lt;p&gt;Currently I have, &lt;/p&gt; &lt;p&gt;a. Whisper running on NPU backend, non-streaming however base is instantaneous for almost everything I say&lt;/p&gt; &lt;p&gt;b. Kokors (this is not lemonade but their marinated version though, hopefully it becomes part of the next release!) which is also blazingly fast and have multiple options &lt;/p&gt; &lt;p&gt;c. Qwen3-Coder-Next (I used to have GLM-4.7-Flash, but whenever I enable search and code execution it gets dizzy and gets stuck quickly, qwen3-coder-next is basically a super power in that setup!)&lt;/p&gt; &lt;p&gt;I am planning to add much more MCPs though &lt;/p&gt; &lt;p&gt;And maybe an OpenWakeWord and SileroVAD setup with barge-in support (not an Omni model though or full duplex streaming like Personaplex (which I want to get running, but no triton or ONNX unfortunately!)&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Using some supported frameworks (usually lemonade’s maintained pre-builds!) &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;llama.cpp (or the optimized version for ROCm or AMD Chat!)&lt;/p&gt; &lt;p&gt;Whisper.cpp (can also run VAD but needs the lemonade maintained NPU version or building AMD’s version from scratch!) &lt;/p&gt; &lt;p&gt;Stablediffusion.cpp (Flux Stable diffusion wan everything runs here!) &lt;/p&gt; &lt;p&gt;Kokoros (awesome TTS engine with OAI compaitable endpoints!) &lt;/p&gt; &lt;ol&gt; &lt;li&gt;Using custom maintained versions or llama.cpp (this might include building from sources)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;You need a Linux setup ideally!&lt;/p&gt; &lt;p&gt;4.&lt;/p&gt; &lt;p&gt;PyTorch based stuff (get the PyTorch version for Python 3.12 from AMD website (if on windows), if in Linux you have much more libraries and options (and I believe Moshi or Personaplex can be setup here with some tinkering!?) &lt;/p&gt; &lt;p&gt;All in all, it is a very capable machine &lt;/p&gt; &lt;p&gt;I even have managed to run Minimax M2.5 Q3_K_XL (which is a very capable mode indeed, when paired with Claude code it can automated huge parts of my job, but still I am having issues with the kv cache in llama.cpp which means it can’t work directly for now!) &lt;/p&gt; &lt;p&gt;All in all it is a very capable machine, being x86 based rather than arm (like the DGX Spark) for me at least means you can do more on the AI-powered applications side (on the same box), as opposed to the Spark (which is also a very nice machine ofc!) &lt;/p&gt; &lt;p&gt;Anyways, that was it I hope this helps &lt;/p&gt; &lt;p&gt;Cheers!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Potential_Block4598"&gt; /u/Potential_Block4598 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7l7q5/the_strix_halo_feels_like_an_amazing_super_power/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7l7q5/the_strix_halo_feels_like_an_amazing_super_power/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r7l7q5/the_strix_halo_feels_like_an_amazing_super_power/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T22:38:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7uwc1</id>
    <title>Need help with llama.cpp performance</title>
    <updated>2026-02-18T05:51:59+00:00</updated>
    <author>
      <name>/u/reto-wyss</name>
      <uri>https://old.reddit.com/user/reto-wyss</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to run Qwen3.5 (MXFP4_MOE unsloth) with llama.cpp, I can only get around 45tg/s with a single active request, and maybe like 60 tg/s combined with two request in parallel, and around 80 tg/s with 4 request. &lt;/p&gt; &lt;p&gt;My setup for this is 2x Pro 6000 + 1x RTX 5090 (all on PCIe x16) so I don't have to dip into RAM. My Workload is typically around 2k to 4k in (visual pp) and 1.5k to 2k out.&lt;/p&gt; &lt;p&gt;Sub 100tg/s total seems low, I'm used to getting like 2000 tg/s with Qwen3-VL-235b NVFP4 with around 100 active requests running on the 2x Pro 6000.&lt;/p&gt; &lt;p&gt;I've tried --parallel N and --t K following the docs, but it does very little at best and I can't find much more guidance.&lt;/p&gt; &lt;p&gt;I understand that llama.cpp is not necessarily built for that and my setup is not ideal. But maybe a few more tg/s are possible? Any guidance much appreciated - I have zero experience with llama.cpp&lt;/p&gt; &lt;p&gt;I've been using it anyway because the quality of the response on my vision task is just vastly better than Qwen3-VL-235b NVFP4 or Qwen3-VL-32b FP8/BF16.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/reto-wyss"&gt; /u/reto-wyss &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7uwc1/need_help_with_llamacpp_performance/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7uwc1/need_help_with_llamacpp_performance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r7uwc1/need_help_with_llamacpp_performance/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-18T05:51:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7i2im</id>
    <title>GLM-5 and DeepSeek are in the Top 6 of the Game Agent Coding League across five games</title>
    <updated>2026-02-17T20:40:43+00:00</updated>
    <author>
      <name>/u/kyazoglu</name>
      <uri>https://old.reddit.com/user/kyazoglu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7i2im/glm5_and_deepseek_are_in_the_top_6_of_the_game/"&gt; &lt;img alt="GLM-5 and DeepSeek are in the Top 6 of the Game Agent Coding League across five games" src="https://preview.redd.it/22z0y8ni84kg1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8c75efaafe037d102f0274765c4008b961af0948" title="GLM-5 and DeepSeek are in the Top 6 of the Game Agent Coding League across five games" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi.&lt;/p&gt; &lt;p&gt;Game Agent Coding League (GACL) is a benchmarking framework designed for LLMs in which models are tasked with generating code for game-playing agents. These agents compete in games such as Battleship, Tic-Tac-Toe variants, and others. At present, the league supports five games, with additional titles planned.&lt;/p&gt; &lt;p&gt;More info about the benchmark &amp;amp; league &lt;a href="https://gameagentcodingleague.com/"&gt;HERE&lt;/a&gt;&lt;br /&gt; Underlying project in Github &lt;a href="https://github.com/summersonnn/Game-Agent-Coding-Benchmark"&gt;HERE&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It's quite new project so bit of a mess in repo. I'll fix soon and 3 more games.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kyazoglu"&gt; /u/kyazoglu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/22z0y8ni84kg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7i2im/glm5_and_deepseek_are_in_the_top_6_of_the_game/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r7i2im/glm5_and_deepseek_are_in_the_top_6_of_the_game/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T20:40:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7qdpg</id>
    <title>GLM-5-Q2 vs GLM-4.7-Q4</title>
    <updated>2026-02-18T02:15:35+00:00</updated>
    <author>
      <name>/u/Most_Drawing5020</name>
      <uri>https://old.reddit.com/user/Most_Drawing5020</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you have a machine with (RAM+VRAM) = 256G, which model would you prefer?&lt;/p&gt; &lt;p&gt;GLM-4.7-UD-Q4_K_XL is 204.56GB&lt;br /&gt; GLM-5-UD-IQ2_XXS is 241GB,&lt;/p&gt; &lt;p&gt;(The size is in decimal unit (it's used on linux and mac). If you calculate in 1024 unit(it's used on windows), you will get 199.7G and 235.35G )&lt;/p&gt; &lt;p&gt;Both of them can be run with 150k+ context (with -fa on which means use flash attention).&lt;/p&gt; &lt;p&gt;Speed is about the same.&lt;/p&gt; &lt;p&gt;I am going to test their IQ for some questions. And I'll put my results here.&lt;/p&gt; &lt;p&gt;Feel free to put your test result here!&lt;/p&gt; &lt;p&gt;I'm going to ask the same question 10 times for each model. 5 times in English, 5 times in Chinese. As this is a Chinese model, and the IQ for different languages is probably different.&lt;/p&gt; &lt;p&gt;For a wash car question:&lt;/p&gt; &lt;p&gt;(I want to wash my car. The car wash is 50 meters away. Should I walk or drive?)&lt;/p&gt; &lt;p&gt;glm-5-q2 thinks way longer than glm-4.7-q4. I have to wait for a long time.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;English&lt;/th&gt; &lt;th align="left"&gt;Chinese&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;glm-4.7-q4&lt;/td&gt; &lt;td align="left"&gt;3 right, 2 wrong&lt;/td&gt; &lt;td align="left"&gt;5 right&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;glm-5-q2&lt;/td&gt; &lt;td align="left"&gt;5 right&lt;/td&gt; &lt;td align="left"&gt;5 right&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;For a matrix math question, I asked each model for 3 times. And both of them got the correct answer. (each answer costs about 10-25 minutes so I can't test more because time is valuable for me)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Most_Drawing5020"&gt; /u/Most_Drawing5020 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7qdpg/glm5q2_vs_glm47q4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7qdpg/glm5q2_vs_glm47q4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r7qdpg/glm5q2_vs_glm47q4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-18T02:15:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7zlwc</id>
    <title>Why GLM on llama.cpp has no MTP?</title>
    <updated>2026-02-18T10:36:52+00:00</updated>
    <author>
      <name>/u/Expensive-Paint-9490</name>
      <uri>https://old.reddit.com/user/Expensive-Paint-9490</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have searched through the repo discussions and PRs but I can't find references. GLM models have embedded layers for multi-token prediction and speculative decoding. They can be used with vLLM - if you have hundreds GB VRAM, of course.&lt;/p&gt; &lt;p&gt;Does anybody know why llama.cpp chose to not support this feature?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Expensive-Paint-9490"&gt; /u/Expensive-Paint-9490 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7zlwc/why_glm_on_llamacpp_has_no_mtp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7zlwc/why_glm_on_llamacpp_has_no_mtp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r7zlwc/why_glm_on_llamacpp_has_no_mtp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-18T10:36:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1r79dcd</id>
    <title>Qwen 3.5 397B is Strong one!</title>
    <updated>2026-02-17T15:41:44+00:00</updated>
    <author>
      <name>/u/Single_Ring4886</name>
      <uri>https://old.reddit.com/user/Single_Ring4886</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I rarely post here but after poking at latest Qwen I felt like sharing my &amp;quot;vibes&amp;quot;. I did bunch of my little tests (thinking under several constraints) and it performed really well.&lt;br /&gt; But what is really good is fact that it is capable of good outputs even without thinking!&lt;br /&gt; Some latest models depend on thinking part really much and that makes them ie 2x more expensive.&lt;br /&gt; It also seems this model is capable of cheap inference +- 1$ .&lt;br /&gt; Do you agree?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Single_Ring4886"&gt; /u/Single_Ring4886 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r79dcd/qwen_35_397b_is_strong_one/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r79dcd/qwen_35_397b_is_strong_one/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r79dcd/qwen_35_397b_is_strong_one/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T15:41:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7sc18</id>
    <title>Entropy-v1: My Take on N8Karma's Genius "Unslopper"</title>
    <updated>2026-02-18T03:42:35+00:00</updated>
    <author>
      <name>/u/Intelligent_Coffee44</name>
      <uri>https://old.reddit.com/user/Intelligent_Coffee44</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7sc18/entropyv1_my_take_on_n8karmas_genius_unslopper/"&gt; &lt;img alt="Entropy-v1: My Take on N8Karma's Genius &amp;quot;Unslopper&amp;quot;" src="https://preview.redd.it/m467tepve6kg1.jpg?width=140&amp;amp;height=77&amp;amp;auto=webp&amp;amp;s=816ea807d473bc16239e34a31646599e1e6aa8b5" title="Entropy-v1: My Take on N8Karma's Genius &amp;quot;Unslopper&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/m467tepve6kg1.jpg?width=2784&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=dd666e1e81bd89a248dee3c6aed9efea6f89be13"&gt;Entropy-v1: before vs after&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A few weeks ago, &lt;a href="/u/N8Karma"&gt;u/N8Karma&lt;/a&gt; introduced Unslopper in this community (&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1qd88v2/i_trained_a_model_to_unslop_ai_prose/"&gt;post&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;For those of you who missed it: &amp;quot;Unslopper&amp;quot; is an LLM fine-tuned to predict human writing from AI slop. The &lt;code&gt;(human writing, AI slop)&lt;/code&gt; dataset is obtained by asking gpt-4o-mini to &amp;quot;improve&amp;quot; Project Gutenberg passages 10 times, which degrades them into slop.&lt;/p&gt; &lt;p&gt;I am really excited by this idea because it solves the &amp;quot;last mile&amp;quot; problem in many LLM workflows: the LLM output might be factually fantastic, but sounds too robotic/odd to use directly. The Unslopper is just the &amp;quot;post-processing&amp;quot; step needed to make them usable.&lt;/p&gt; &lt;p&gt;So I set out to create an even better version of Unslopper - while the original model is already great, I wanted to make a few tweaks to make the output even more impressive, and to make it efficient to serve as an online service.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Switched base model to &lt;code&gt;gemma-3-27b-it&lt;/code&gt; &lt;ul&gt; &lt;li&gt;As a dense model, Gemma 3 would be easier to fine-tune with limited data than &lt;code&gt;Qwen3-VL-30B-A3B-Instruct&lt;/code&gt;&lt;/li&gt; &lt;li&gt;I personally believe reasoning CoT is a big part of why AI sounds &amp;quot;different&amp;quot;. So I specifically chose a non-reasoning model. As an added bonus, Gemma 3 is known to be very good at creative writing.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;r = 64&lt;/code&gt; lora &lt;ul&gt; &lt;li&gt;I used a lora with a relatively high # of trainable parameters to ensure we get all the value from the OG dataset.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;bf16 fine-tuning. &lt;ul&gt; &lt;li&gt;I fine-tuned the model in its original precision to avoid losing information due to quantization. The finished lora is merged into the model and quantized to fp8 for efficient serving via vLLM.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;All other settings are identical to the OG Unslopper.&lt;/p&gt; &lt;p&gt;With these changes, my model achieves a &lt;strong&gt;+4.07% ppl&lt;/strong&gt; relative improvement compared with the OG Unslopper on a validation set of held-out Project Gutenberg passages.&lt;/p&gt; &lt;p&gt;The model is open source, of course -&lt;/p&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/ysong21/entropy-v1-fp8"&gt;https://huggingface.co/ysong21/entropy-v1-fp8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Adapter: &lt;a href="https://huggingface.co/ysong21/entropy-v1-lora"&gt;https://huggingface.co/ysong21/entropy-v1-lora&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I also made a web version for people who just want to try it out without needing to set anything up: &lt;a href="https://www.getentropy.ai"&gt;https://www.getentropy.ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The model is available both through the web interface and an OpenAI-compatible API.&lt;/p&gt; &lt;p&gt;Please let me know what you think! This is just the first step. Next, I am planning to 1) retrain the model with a larger dataset and 2) make lower-bit quants once I get a good calibration dataset.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Intelligent_Coffee44"&gt; /u/Intelligent_Coffee44 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7sc18/entropyv1_my_take_on_n8karmas_genius_unslopper/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7sc18/entropyv1_my_take_on_n8karmas_genius_unslopper/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r7sc18/entropyv1_my_take_on_n8karmas_genius_unslopper/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-18T03:42:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7bf1l</id>
    <title>Alibaba's new Qwen3.5-397B-A17B is the #3 open weights model in the Artificial Analysis Intelligence Index</title>
    <updated>2026-02-17T16:49:25+00:00</updated>
    <author>
      <name>/u/abdouhlili</name>
      <uri>https://old.reddit.com/user/abdouhlili</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bf1l/alibabas_new_qwen35397ba17b_is_the_3_open_weights/"&gt; &lt;img alt="Alibaba's new Qwen3.5-397B-A17B is the #3 open weights model in the Artificial Analysis Intelligence Index" src="https://preview.redd.it/b5eytfmy33kg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a91c254a446d622c39a0be55a5a8d80f79c11886" title="Alibaba's new Qwen3.5-397B-A17B is the #3 open weights model in the Artificial Analysis Intelligence Index" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdouhlili"&gt; /u/abdouhlili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/b5eytfmy33kg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bf1l/alibabas_new_qwen35397ba17b_is_the_3_open_weights/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bf1l/alibabas_new_qwen35397ba17b_is_the_3_open_weights/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T16:49:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7bhel</id>
    <title>Team created a methodology to mathematically change the weights on local LLMs to remove the censorship guardrails. HERETIC</title>
    <updated>2026-02-17T16:51:30+00:00</updated>
    <author>
      <name>/u/44th--Hokage</name>
      <uri>https://old.reddit.com/user/44th--Hokage</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is the tool and their summary:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/p-e-w/heretic"&gt;https://github.com/p-e-w/heretic&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Heretic is a tool that removes censorship (aka &amp;quot;safety alignment&amp;quot;) from transformer-based language models without expensive post-training. It combines an advanced implementation of directional ablation, also known as &amp;quot;abliteration&amp;quot; (&lt;a href="https://arxiv.org/abs/2406.11717"&gt;Arditi et al. 2024&lt;/a&gt;, Lai 2025 (&lt;a href="https://huggingface.co/blog/grimjim/projected-abliteration"&gt;1&lt;/a&gt;, &lt;a href="https://huggingface.co/blog/grimjim/norm-preserving-biprojected-abliteration"&gt;2&lt;/a&gt;)), with a TPE-based parameter optimizer powered by &lt;a href="https://optuna.org/"&gt;Optuna&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;This approach enables Heretic to work &lt;strong&gt;completely automatically.&lt;/strong&gt; Heretic finds high-quality abliteration parameters by co-minimizing the number of refusals and the KL divergence from the original model. This results in a decensored model that retains as much of the original model's intelligence as possible. Using Heretic does not require an understanding of transformer internals. In fact, anyone who knows how to run a command-line program can use Heretic to decensor language models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/44th--Hokage"&gt; /u/44th--Hokage &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bhel/team_created_a_methodology_to_mathematically/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bhel/team_created_a_methodology_to_mathematically/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bhel/team_created_a_methodology_to_mathematically/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T16:51:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8157s</id>
    <title>Qwen 3.5 MXFP4 quants are coming - confirmed by Junyang Lin</title>
    <updated>2026-02-18T12:01:58+00:00</updated>
    <author>
      <name>/u/dampflokfreund</name>
      <uri>https://old.reddit.com/user/dampflokfreund</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Most here are aware that OpenAI did something very well with their GPT-Oss release - they trained their model in 4 bit and delivered native mxfp4 quants which means a lot higher quality than the typical Unsloth and Bartowski quants of bf16 models. Google did it too with Gemma 3 QAT which was very well received by the community. Super excited for it, this is definately the right direction to take!&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/JustinLin610/status/2024002713579651245"&gt;https://x.com/JustinLin610/status/2024002713579651245&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dampflokfreund"&gt; /u/dampflokfreund &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8157s/qwen_35_mxfp4_quants_are_coming_confirmed_by/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8157s/qwen_35_mxfp4_quants_are_coming_confirmed_by/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8157s/qwen_35_mxfp4_quants_are_coming_confirmed_by/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-18T12:01:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7uyh9</id>
    <title>Running your own LLM on a LAN accessible by a dev team</title>
    <updated>2026-02-18T05:55:24+00:00</updated>
    <author>
      <name>/u/BubbleProphylaxis</name>
      <uri>https://old.reddit.com/user/BubbleProphylaxis</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Let's say a team of 20 devs are cursor subscribers and they each consume 20-50$ usd per day in tokens by using a midrange Claude or GPT model. That adds up really quickly.&lt;/p&gt; &lt;p&gt;Is it viable then to buy a large server, with let's say 4x RTX A6000 cards, for a total of 192 gb VRAM, running a pretty big model, and plenty of system ram?&lt;/p&gt; &lt;p&gt;That would make it a pretty expensive server for sure, but certainly cheaper than the sum of all pay-per-use for all users.&lt;/p&gt; &lt;p&gt;What model would you run for a dev team on such a beast of a server?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BubbleProphylaxis"&gt; /u/BubbleProphylaxis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7uyh9/running_your_own_llm_on_a_lan_accessible_by_a_dev/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7uyh9/running_your_own_llm_on_a_lan_accessible_by_a_dev/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r7uyh9/running_your_own_llm_on_a_lan_accessible_by_a_dev/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-18T05:55:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7fb2k</id>
    <title>Anthropic is deploying 20M$ to support AI regulation in sight of 2026 elections</title>
    <updated>2026-02-17T19:02:15+00:00</updated>
    <author>
      <name>/u/1998marcom</name>
      <uri>https://old.reddit.com/user/1998marcom</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7fb2k/anthropic_is_deploying_20m_to_support_ai/"&gt; &lt;img alt="Anthropic is deploying 20M$ to support AI regulation in sight of 2026 elections" src="https://external-preview.redd.it/YL-V_hu9Gif4FU34F4m4K7lk-m3_3LBagiDGYFEEe4o.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=120a000c29bab7e9ed6a3da128fb13fb873db506" title="Anthropic is deploying 20M$ to support AI regulation in sight of 2026 elections" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Next time you buy subscriptions from Anthropic or pay for their models, keep in mind where some of your money is going.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1998marcom"&gt; /u/1998marcom &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.cnbc.com/2026/02/12/anthropic-gives-20-million-to-group-pushing-for-ai-regulations-.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7fb2k/anthropic_is_deploying_20m_to_support_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r7fb2k/anthropic_is_deploying_20m_to_support_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T19:02:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7shtv</id>
    <title>I built a benchmark that tests coding LLMs on REAL codebases (65 tasks, ELO ranked)</title>
    <updated>2026-02-18T03:50:07+00:00</updated>
    <author>
      <name>/u/hauhau901</name>
      <uri>https://old.reddit.com/user/hauhau901</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7shtv/i_built_a_benchmark_that_tests_coding_llms_on/"&gt; &lt;img alt="I built a benchmark that tests coding LLMs on REAL codebases (65 tasks, ELO ranked)" src="https://external-preview.redd.it/iR5vpFsEP8hSB9xJvEcI5sdQWM2SJY77i75tSw01yJI.png?width=140&amp;amp;height=93&amp;amp;auto=webp&amp;amp;s=737f72db6cb794bccbd21ae823e2f9590aa14236" title="I built a benchmark that tests coding LLMs on REAL codebases (65 tasks, ELO ranked)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, been working on something for a while and figured it's time to share it.&lt;/p&gt; &lt;p&gt;I kept seeing new models drop every week with claims of being 10x better, benchmarks that don't translate to actual coding, and demos that look great but fall apart on real work. so I started building my own benchmark to figure out what &lt;strong&gt;actually&lt;/strong&gt; works.&lt;/p&gt; &lt;p&gt;It's called APEX Testing. every task is an &lt;strong&gt;actual codebase with real code, real dependencies&lt;/strong&gt;, and a real problem to solve. fix this bug, add this feature, refactor this module, build this from scratch. It's (currently) comprising of 65 tasks across 8 categories, ranging from React components to race condition debugging to building CLI tools. Each model gets a fresh clone of the same repo with the exact same starting point and exact same conditions.&lt;/p&gt; &lt;p&gt;Grading is done by multiple SOTA models independently, and then I also personally review every single output to catch anything unfair like timeouts or infra hiccups. If a model got unlucky, I rerun it (which ended up causing a lot bigger of a hole in my wallet haha). The whole thing is ranked with ELO, and you can filter by category to see where models actually shine vs where they struggle.&lt;/p&gt; &lt;p&gt;A couple things that caught me off guard so far:&lt;/p&gt; &lt;p&gt;- GPT 5.1 Codex Mini beating GPT 5.2 Codex pretty convincingly even though smaller and older, it came out way more consistent (but it also seemed to REALLY splurge on tokens)&lt;/p&gt; &lt;p&gt;- Some models look great on average but completely bomb certain task types&lt;/p&gt; &lt;p&gt;- The cost difference between models with similar scores is huge&lt;/p&gt; &lt;p&gt;It's a solo project, funded out of my own pocket (you can see total spend on the homepage lol). hope it helps you cut through the noise and pick the right model for your work.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.apex-testing.org"&gt;https://www.apex-testing.org&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hope you all find it useful!&lt;/p&gt; &lt;p&gt;P.S. I will work on testing more quanted models as well and I might add more tests as well in the future.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ligwgwa9c6kg1.png?width=2095&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ac55a9932069f6100f4375a759fb238e97cdbfc8"&gt;https://preview.redd.it/ligwgwa9c6kg1.png?width=2095&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ac55a9932069f6100f4375a759fb238e97cdbfc8&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hauhau901"&gt; /u/hauhau901 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7shtv/i_built_a_benchmark_that_tests_coding_llms_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7shtv/i_built_a_benchmark_that_tests_coding_llms_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r7shtv/i_built_a_benchmark_that_tests_coding_llms_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-18T03:50:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7s5nh</id>
    <title>We tested the same INT8 model on 5 Snapdragon chipsets. Accuracy ranged from 93% to 71%. Same weights, same ONNX file.</title>
    <updated>2026-02-18T03:34:29+00:00</updated>
    <author>
      <name>/u/NoAdministration6906</name>
      <uri>https://old.reddit.com/user/NoAdministration6906</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We've been doing on-device accuracy testing across multiple Snapdragon SoCs and the results have been eye-opening.&lt;/p&gt; &lt;p&gt;Same model. Same quantization. Same ONNX export. Deployed to 5 different chipsets:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Device&lt;/th&gt; &lt;th align="left"&gt;Accuracy&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Snapdragon 8 Gen 3&lt;/td&gt; &lt;td align="left"&gt;91.8%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Snapdragon 8 Gen 2&lt;/td&gt; &lt;td align="left"&gt;89.1%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Snapdragon 7s Gen 2&lt;/td&gt; &lt;td align="left"&gt;84.3%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Snapdragon 6 Gen 1&lt;/td&gt; &lt;td align="left"&gt;79.6%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Snapdragon 4 Gen 2&lt;/td&gt; &lt;td align="left"&gt;71.2%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Cloud benchmark reported 94.2%.&lt;/p&gt; &lt;p&gt;The spread comes down to three things we've observed:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;NPU precision handling&lt;/strong&gt; — INT8 rounding behavior differs across Hexagon generations. Not all INT8 is created equal.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Operator fusion differences&lt;/strong&gt; — the QNN runtime optimizes the graph differently per SoC, sometimes trading accuracy for throughput.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Memory-constrained fallback&lt;/strong&gt; — on lower-tier chips, certain ops fall back from NPU to CPU, changing the execution path entirely.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;None of this shows up in cloud-based benchmarks. You only see it when you run on real hardware.&lt;/p&gt; &lt;p&gt;Curious if others are seeing similar drift across chipsets — or if anyone has a good strategy for catching this before shipping. Most CI pipelines we've seen only test on cloud GPUs and call it a day.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NoAdministration6906"&gt; /u/NoAdministration6906 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7s5nh/we_tested_the_same_int8_model_on_5_snapdragon/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7s5nh/we_tested_the_same_int8_model_on_5_snapdragon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r7s5nh/we_tested_the_same_int8_model_on_5_snapdragon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-18T03:34:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1r77swh</id>
    <title>I gave 12 LLMs $2,000 and a food truck. Only 4 survived.</title>
    <updated>2026-02-17T14:42:06+00:00</updated>
    <author>
      <name>/u/Disastrous_Theme5906</name>
      <uri>https://old.reddit.com/user/Disastrous_Theme5906</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r77swh/i_gave_12_llms_2000_and_a_food_truck_only_4/"&gt; &lt;img alt="I gave 12 LLMs $2,000 and a food truck. Only 4 survived." src="https://preview.redd.it/4sewtkexf2kg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c0f7608e083eece043f2953690650ad7c16596a5" title="I gave 12 LLMs $2,000 and a food truck. Only 4 survived." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Built a business sim where AI agents run a food truck for 30 days — location, menu, pricing, staff, inventory. Same scenario for all models.&lt;/p&gt; &lt;p&gt;Opus made $49K. GPT-5.2 $28K. 8 went bankrupt. Every model that took a loan went bankrupt (8/8).&lt;/p&gt; &lt;p&gt;There's also a playable mode — same simulation, same 34 tools, same leaderboard. You either survive 30 days or go bankrupt, get a result card and land on the shared leaderboard. Example result: &lt;a href="https://foodtruckbench.com/r/9E6925"&gt;https://foodtruckbench.com/r/9E6925&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Benchmark + leaderboard: &lt;a href="https://foodtruckbench.com"&gt;https://foodtruckbench.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Play: &lt;a href="https://foodtruckbench.com/play"&gt;https://foodtruckbench.com/play&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Gemini 3 Flash Thinking — only model out of 20+ tested that gets stuck in an infinite decision loop, 100% of runs: &lt;a href="https://foodtruckbench.com/blog/gemini-flash"&gt;https://foodtruckbench.com/blog/gemini-flash&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer questions about the sim or results.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Disastrous_Theme5906"&gt; /u/Disastrous_Theme5906 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4sewtkexf2kg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r77swh/i_gave_12_llms_2000_and_a_food_truck_only_4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r77swh/i_gave_12_llms_2000_and_a_food_truck_only_4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T14:42:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7plp1</id>
    <title>PrimeIntellect/INTELLECT-3.1 · Hugging Face</title>
    <updated>2026-02-18T01:43:01+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7plp1/primeintellectintellect31_hugging_face/"&gt; &lt;img alt="PrimeIntellect/INTELLECT-3.1 · Hugging Face" src="https://external-preview.redd.it/HlIthhd4_MOQ5SPqMHH4aU80ZJQIA0QmPpZBs5Jd5L0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5b86fe7ea9ee70a3f83ccb5ad48aa01e8fd98f27" title="PrimeIntellect/INTELLECT-3.1 · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INTELLECT-3.1 is a 106B (A12B) parameter Mixture-of-Experts reasoning model built as a continued training of INTELLECT-3 with additional reinforcement learning on math, coding, software engineering, and agentic tasks.&lt;/p&gt; &lt;p&gt;Training was performed with prime-rl using environments built with the verifiers library. All training and evaluation environments are available on the Environments Hub.&lt;/p&gt; &lt;p&gt;The model, training frameworks, and environments are open-sourced under fully-permissive licenses (MIT and Apache 2.0).&lt;/p&gt; &lt;p&gt;For more details, see the technical report.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/PrimeIntellect/INTELLECT-3.1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7plp1/primeintellectintellect31_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r7plp1/primeintellectintellect31_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-18T01:43:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7y86d</id>
    <title>Gemma 27B/12B/4B/1B finetunes from DavidAU (20 models)</title>
    <updated>2026-02-18T09:13:14+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;Gemma 3 (1b, 4b, 12b and 27b) - Uncensored full Reasoning/Thinking models fine tuned using top distill datasets.&lt;/p&gt; &lt;p&gt;20 Gemma 3 models 1B, 4B, 12B and 27B with full reasoning using GLM 4.7 Flash, GPT, Claude and Gemini datasets and more fully fine tuned using Unsloth.&lt;/p&gt; &lt;p&gt;Most models are Heretic'ed (uncensored) first, and tuned second.&lt;br /&gt; This vastly improves the model.&lt;/p&gt; &lt;p&gt;Models are also bench marked and in almost all cases exceed org model metrics - and in some cases by a lot.&lt;/p&gt; &lt;p&gt;Enjoy the freedom and more powerful THINKING/REASONING and UNCENSORED Gemma 3s !&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/DavidAU/gemma-3-reasoning-thinking-models-incl-uncensored"&gt;https://huggingface.co/collections/DavidAU/gemma-3-reasoning-thinking-models-incl-uncensored&lt;/a&gt;&lt;/p&gt; &lt;p&gt;DavidAU on reddit: &lt;a href="/u/Dangerous_Fix_5526/"&gt;u/Dangerous_Fix_5526/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7y86d/gemma_27b12b4b1b_finetunes_from_davidau_20_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7y86d/gemma_27b12b4b1b_finetunes_from_davidau_20_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r7y86d/gemma_27b12b4b1b_finetunes_from_davidau_20_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-18T09:13:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7j7kb</id>
    <title>The guy that won the NVIDIA Hackathon and an NVIDIA DGX Spark GB10 has won another hackathon with it!</title>
    <updated>2026-02-17T21:22:30+00:00</updated>
    <author>
      <name>/u/brandon-i</name>
      <uri>https://old.reddit.com/user/brandon-i</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I promised that I would update you all with what I was going to do next with the DGX Spark GB10 that I won. It's been a few weeks and I have been primarily heads down on fundraising for my startup trying to automatically improve and evaluate Coding Agents.&lt;/p&gt; &lt;p&gt;Since the last time I posted I became a Dell Pro Precision Ambassador after they saw all of the cool hackathons that I won and stuff I am building that can hopefully make a difference in the world (I am trying to create Brain World Models using a bunch of different types of brain scans to do precision therapeutics, diagnostics, etc. as my Magnus Opus). &lt;/p&gt; &lt;p&gt;They sent me a Dell Pro Max T2 Tower and another DGX Spark GB10 which I have connected to the previous one that I won. This allows me to continue my work with the limited funds that I have to see how far I can really push the limits of what's possible at the intersection of Healthcare and AI.&lt;/p&gt; &lt;p&gt;During Superbowl Weekend I took some time to do a 24-hour hackathon solving a problem that I really care about (even if it wasn't related to my startup).&lt;/p&gt; &lt;p&gt;My most recent job was at UCSF doing applied neuroscience creating a research-backed tool that screened children for Dyslexia since traditional approaches don’t meet learners where they are so I wanted to take the research I did further and actually create solutions that also did computer adaptive learning.&lt;/p&gt; &lt;p&gt;Through my research I have come to find that the current solutions for learning languages are antiquated often assuming a “standard” learner: same pace, same sequence, same practice, same assessments.&lt;/p&gt; &lt;p&gt;But, language learning is deeply personalized. Two learners can spend the same amount of time on the same content and walk away with totally different outcomes because the feedback they need could be entirely different with the core problem being that language learning isn’t one-size-fits-all.&lt;/p&gt; &lt;p&gt;Most language tools struggle with a few big issues:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Single Language&lt;/strong&gt;: Most tools are designed specifically for Native English speakers&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Culturally insensitive:&lt;/strong&gt; Even within the same language there can be different dialects and word/phrase utilization&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Static Difficulty:&lt;/strong&gt; content doesn’t adapt when you’re bored or overwhelmed&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Delayed Feedback:&lt;/strong&gt; you don’t always know &lt;em&gt;what&lt;/em&gt; you said wrong or &lt;em&gt;why&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Practice ≠ assessment:&lt;/strong&gt; testing is often separate from learning, instead of driving it&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Speaking is underserved&lt;/strong&gt;: it’s hard to get consistent, personalized speaking practice without 1:1 time&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For many learners, especially kids, the result is predictable: &lt;em&gt;frustration, disengagement, or plateauing.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;So I built a an automated speech recognition app that adapts in real time combining computer adaptive testing and computer adaptive learning to personalize the experience as you go.&lt;/p&gt; &lt;p&gt;It not only transcribes speech, but also evaluates phoneme-level pronunciation, which lets the system give targeted feedback (and adapt the next prompt) based on &lt;em&gt;which sounds&lt;/em&gt; someone struggles with.&lt;/p&gt; &lt;p&gt;I tried to make it as simple as possible because my primary user base would be teachers that didn't have a lot of time to actually learn new tools and were already struggling with teaching an entire class.&lt;/p&gt; &lt;p&gt;It uses natural speaking performance to determine what a student should practice next.&lt;/p&gt; &lt;p&gt;So instead of providing every child a fixed curriculum, the system continuously adjusts difficulty and targets based on how you’re actually doing rather than just on completion.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How it Built It&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;I connected two NVIDIA DGX Spark with the GB10 Grace Blackwell Superchip giving me 256 GB LPDDR5x Coherent Unified System Memory to run inference and the entire workflow locally. I also had the Dell Pro Max T2 Tower, but I couldn't physically bring it to the Notion office so I used Tailscale to SSH into it&lt;/li&gt; &lt;li&gt;I utilized CrisperWhisper, faster-whisper, and a custom transformer to get accurate word-level timestamps, verbatim transcriptions, filler detection, and hallucination mitigation&lt;/li&gt; &lt;li&gt;I fed this directly into a Montreal Forced Aligner to get phoneme level dictation&lt;/li&gt; &lt;li&gt;I then used a heuristics detection algorithm to screen for several disfluencies: Prolongnation, replacement, deletion, addition, and repetition&lt;/li&gt; &lt;li&gt;I included stutter and filler analysis/detection using the SEP-28k dataset and PodcastFillers Dataset&lt;/li&gt; &lt;li&gt;I fed these into AI Agents using both local models, Cartesia's Line Agents, and Notion's Custom Agents to do computer adaptive learning and testing&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The result is a workflow where learning content can evolve quickly while the learner experience stays personalized and measurable.&lt;/p&gt; &lt;p&gt;I want to support learners who don’t thrive in rigid systems and need:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;more repetition (without embarrassment)&lt;/li&gt; &lt;li&gt;targeted practice on specific sounds/phrases&lt;/li&gt; &lt;li&gt;a pace that adapts to attention and confidence&lt;/li&gt; &lt;li&gt;immediate feedback that’s actually actionable&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This project is an early prototype, but it’s a direction I’m genuinely excited about: speech-first language learning that adapts to the person, rather than the other way around.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=2RYHu1jyFWI"&gt;https://www.youtube.com/watch?v=2RYHu1jyFWI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I wrote something in medium that has a tiny bit more information &lt;a href="https://medium.com/@brandonin/i-just-won-the-cartesia-hackathon-reinforcing-something-ive-believed-in-for-a-long-time-language-dc93525b2e48?postPublishedType=repub"&gt;https://medium.com/@brandonin/i-just-won-the-cartesia-hackathon-reinforcing-something-ive-believed-in-for-a-long-time-language-dc93525b2e48?postPublishedType=repub&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For those that are wondering what the specs are of the Dell Pro T2 Tower that they sent me:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Intel Core Ultra 9 285K (36 MB cache, 24 cores, 24 threads, 3.2 GHz to 5.7 GHz, 125W)&lt;/li&gt; &lt;li&gt;128GB: 4 x 32 GB, DDR5, 4400 MT/s&lt;/li&gt; &lt;li&gt;2x - 4TB SSD TLC with DRAM M.2 2280 PCIe Gen4 SED Ready&lt;/li&gt; &lt;li&gt;NVIDIA RTX PRO 6000 Blackwell Workstation Edition (600W), 96GB GDDR7&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/brandon-i"&gt; /u/brandon-i &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7j7kb/the_guy_that_won_the_nvidia_hackathon_and_an/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7j7kb/the_guy_that_won_the_nvidia_hackathon_and_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r7j7kb/the_guy_that_won_the_nvidia_hackathon_and_an/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T21:22:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7mscr</id>
    <title>I trained a language model on CPU in 1.2 hours with no matrix multiplications — here's what I learned</title>
    <updated>2026-02-17T23:42:30+00:00</updated>
    <author>
      <name>/u/Own-Albatross868</name>
      <uri>https://old.reddit.com/user/Own-Albatross868</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all. I've been experimenting with tiny matmul-free language models that can be trained and run entirely on CPU. Just released the model.&lt;/p&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/changcheng967/flashlm-v3-13m"&gt;https://huggingface.co/changcheng967/flashlm-v3-13m&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Quick stats:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;13.6M parameters, d_model=256&lt;/li&gt; &lt;li&gt;Ternary weights ({-1, 0, +1}) — inference is just adds and subtracts, no multiplies&lt;/li&gt; &lt;li&gt;Trained on 2-thread CPU, no GPU, 1.2 hours&lt;/li&gt; &lt;li&gt;32M tokens from FineWeb-Edu&lt;/li&gt; &lt;li&gt;Validation loss: 6.80&lt;/li&gt; &lt;li&gt;Uses frozen GPT-2 embeddings (SVD projected) so it doesn't waste training time learning an embedding table&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The model produces grammatical-ish English but with zero coherence — it's learned syntax but not semantics. For 1.2 hours on a CPU, I'll take it.&lt;/p&gt; &lt;p&gt;The biggest surprise was that 86% of training time was spent on the output layer (projecting 256 dims to 50,257 vocab). The entire matmul-free ternary core only got 14% of compute. So the &amp;quot;efficient&amp;quot; part of the model was essentially starved of training signal by the inefficient softmax head.&lt;/p&gt; &lt;p&gt;Working on v4 that replaces the softmax with a hierarchical tree structure to fix this bottleneck. If it works, it should allow 5-10x more effective training in the same wall clock time.&lt;/p&gt; &lt;p&gt;Code is MIT licensed. Would love feedback from anyone else working on tiny/efficient models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Own-Albatross868"&gt; /u/Own-Albatross868 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7mscr/i_trained_a_language_model_on_cpu_in_12_hours/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7mscr/i_trained_a_language_model_on_cpu_in_12_hours/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r7mscr/i_trained_a_language_model_on_cpu_in_12_hours/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T23:42:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7r7zr</id>
    <title>GLM-5 Technical Report</title>
    <updated>2026-02-18T02:51:52+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7r7zr/glm5_technical_report/"&gt; &lt;img alt="GLM-5 Technical Report" src="https://preview.redd.it/phk5j82g36kg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1a4da644c3d3988eba39a218faf8a811456998b3" title="GLM-5 Technical Report" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Presenting the GLM-5 Technical Report!&lt;/p&gt; &lt;p&gt;&lt;a href="http://arxiv.org/abs/2602.15763"&gt;http://arxiv.org/abs/2602.15763&lt;/a&gt;&lt;/p&gt; &lt;p&gt;After the launch of GLM-5, we’re pulling back the curtain on how it was built. Key innovations include:&lt;/p&gt; &lt;p&gt;- DSA Adoption: Significantly reduces training and inference costs while preserving long-context fidelity&lt;/p&gt; &lt;p&gt;- Asynchronous RL Infrastructure: Drastically improves post-training efficiency by decoupling generation from training&lt;/p&gt; &lt;p&gt;- Agent RL Algorithms: Enables the model to learn from complex, long-horizon interactions more effectively&lt;/p&gt; &lt;p&gt;Through these innovations, GLM-5 achieves SOTA performance among open-source models, with particularly strong results in real-world software engineering tasks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/phk5j82g36kg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7r7zr/glm5_technical_report/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r7r7zr/glm5_technical_report/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-18T02:51:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7bsfd</id>
    <title>Best Audio Models - Feb 2026</title>
    <updated>2026-02-17T17:01:37+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;They've been a ton of audio models released of late, the most notable perhaps being Qwen3 TTS. So its time for another &lt;strong&gt;Best Audio Models&lt;/strong&gt; megathread&lt;/p&gt; &lt;p&gt;Share what your favorite ASR, TTS, STT, Text to Music models are right now &lt;strong&gt;and why.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Given the the amount of ambiguity and subjectivity in rating/testing these models, please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks etc. Closed models like Elevenlabs v3 seem to continue to be a few levels above open models especially for production use cases with long lengths/stability requirements, so comparisons, especially empirical ones are welcome.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please use the top level comments to thread your responses.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T17:01:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1r60qu9</id>
    <title>AMA Announcement: StepFun AI, The Opensource Lab Behind Step-3.5-Flash Model (Thursday, 8AM-11AM PST)</title>
    <updated>2026-02-16T05:11:16+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r60qu9/ama_announcement_stepfun_ai_the_opensource_lab/"&gt; &lt;img alt="AMA Announcement: StepFun AI, The Opensource Lab Behind Step-3.5-Flash Model (Thursday, 8AM-11AM PST)" src="https://preview.redd.it/u11uh8jfisjg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=afc30b2b6ae673f2e940109e2001bb498bd818ad" title="AMA Announcement: StepFun AI, The Opensource Lab Behind Step-3.5-Flash Model (Thursday, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; 👋&lt;/p&gt; &lt;p&gt;We're excited for Thursday's guests: &lt;strong&gt;The StepFun Team!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Kicking things off Thursday, Feb. 19th, 8 AM–11 AM PST&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;⚠️ &lt;strong&gt;Note:&lt;/strong&gt; The AMA itself will be hosted in a &lt;strong&gt;separate thread,&lt;/strong&gt; please don’t post questions here.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/u11uh8jfisjg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r60qu9/ama_announcement_stepfun_ai_the_opensource_lab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r60qu9/ama_announcement_stepfun_ai_the_opensource_lab/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T05:11:16+00:00</published>
  </entry>
</feed>
