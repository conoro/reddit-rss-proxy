<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-21T22:48:36+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1p39u1a</id>
    <title>Need Help in Studying Agent Selection in for Multi Agent Interaction</title>
    <updated>2025-11-21T20:45:29+00:00</updated>
    <author>
      <name>/u/Motor_Display6380</name>
      <uri>https://old.reddit.com/user/Motor_Display6380</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;Iâ€™m working on an Agent-to-Agent (A2A) discovery experiment and I need to populate a &amp;quot;mock internet&amp;quot; of agents.&lt;/p&gt; &lt;p&gt;Instead of chat logs, I am looking for a dataset of &lt;strong&gt;Agent Definitions&lt;/strong&gt; or &lt;strong&gt;Manifests&lt;/strong&gt;â€”structured JSON/Python objects that describe an agent's identity, inputs, and outputs.&lt;/p&gt; &lt;p&gt;I'm using a schema similar to the &lt;code&gt;AgentCard&lt;/code&gt; concept (see snippet below), where an agent declares its capabilities and URL:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;public_agent_card = AgentCard( name='Stock_Analyzer_01', description='Returns sentiment analysis for a given ticker', url=' /', input_modes=['text'], skills=['finance_sentiment_v1'], ... ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;My Question:&lt;/strong&gt; Does anyone know of a dataset that contains thousands of these &amp;quot;service descriptions&amp;quot;?&lt;/p&gt; &lt;p&gt;Essentially, I need a dump of &amp;quot;Agent Business Cards&amp;quot; or OpenAPI specs that I can wrap into &lt;code&gt;AgentCard&lt;/code&gt; objects to simulate a busy network of functional agents.&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Motor_Display6380"&gt; /u/Motor_Display6380 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p39u1a/need_help_in_studying_agent_selection_in_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p39u1a/need_help_in_studying_agent_selection_in_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p39u1a/need_help_in_studying_agent_selection_in_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T20:45:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1p39xqt</id>
    <title>Releasing APS â€” an open packaging standard + CLI for AI agents (v0.1)</title>
    <updated>2025-11-21T20:49:36+00:00</updated>
    <author>
      <name>/u/Clear-Let-8792</name>
      <uri>https://old.reddit.com/user/Clear-Let-8792</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Iâ€™ve been working on an open, vendor-neutral packaging standard for AI agents called &lt;strong&gt;APS (Agent Packaging Standard)&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;It defines a simple packaging format (&lt;code&gt;agent.yaml&lt;/code&gt; + code + metadata), a Python CLI (&lt;code&gt;aps build&lt;/code&gt;, &lt;code&gt;aps publish&lt;/code&gt;, &lt;code&gt;aps run&lt;/code&gt;), and a lightweight local registry for sharing agents.&lt;/p&gt; &lt;p&gt;Two example agents (Echo + RAG) are included.&lt;/p&gt; &lt;p&gt;Docs + examples: &lt;a href="https://agentpackaging.org"&gt;https://agentpackaging.org&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Still early (v0.1) â€” looking for feedback from anyone building or distributing agents.&lt;br /&gt; Do you think something like this will be useful?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Clear-Let-8792"&gt; /u/Clear-Let-8792 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p39xqt/releasing_aps_an_open_packaging_standard_cli_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p39xqt/releasing_aps_an_open_packaging_standard_cli_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p39xqt/releasing_aps_an_open_packaging_standard_cli_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T20:49:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2y0e5</id>
    <title>Virtual Width Networks</title>
    <updated>2025-11-21T13:01:35+00:00</updated>
    <author>
      <name>/u/Aaaaaaaaaeeeee</name>
      <uri>https://old.reddit.com/user/Aaaaaaaaaeeeee</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;We introduce Virtual Width Networks (VWN), a framework that delivers the benefits of wider representations without incurring the quadratic cost of increasing the hidden size. VWN decouples representational width from backbone width, expanding the embedding space while keeping backbone compute nearly constant. In our largeâ€‘scale experiment, an 8Ã— expansion accelerates optimization by over 2Ã— for nextâ€‘token and 3Ã— for nextâ€‘2â€‘token prediction. The advantage amplifies over training as both the loss gap grows and convergenceâ€‘speedup ratio increase, showing that VWN is not only tokenâ€‘efficient but also increasingly effective with scale. Moreover, we identify an approximately logâ€‘linear scaling relation between virtual width and loss reduction, offering an initial empirical basis and motivation for exploring virtualâ€‘width scaling as a new dimension of largeâ€‘model efficiency.&lt;/p&gt; &lt;/blockquote&gt; &lt;ul&gt; &lt;li&gt;Seems like the capacity increase comes from enhancements to residual connection paths. Here's an overview that might be helpful:&lt;/li&gt; &lt;/ul&gt; &lt;blockquote&gt; &lt;p&gt;We reinterpret Virtual Width Networks (VWN) through the lens of connectivity as attention along the depth axis. ...(1) a plain feed-forward stack without residuals corresponds to a sliding window of size 1 (each layer processes only its current input and forgets the previous one); (2) residual connections implement a window of size 2 (current input plus the immediately preceding one); and (3) dense connectivity [ma2023denseformer, huang2017densely, xiao2025muddformer] extends the window size to include all previous layers, allowing each layer to reuse all prior representations. &lt;strong&gt;VWN with Generalized Hyper-Connections (GHC) sits in between&lt;/strong&gt;: it realizes a learned, fixed-cost, linear-attention-like mechanism over depth that scales the accessible depth context.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;With this idea at play, it wouldn't be easy to determine the power of a model. If increased hidden dimension size is the key of intelligent dense models: An MoE model can be low active parameters, high depth (many layers) with an 8x virtual network width and outperform in all ways that we know about. We might need a study that compares baseline dense, vs increased total ffn parameters (MoE), vs increased virtual width. This study uses MoEs as the baseline but it would be nice to see one enhancement at a time so we can better weigh the value in VWN in comparison to increased total ffn parameters (MoE).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aaaaaaaaaeeeee"&gt; /u/Aaaaaaaaaeeeee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2511.11238v2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2y0e5/virtual_width_networks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2y0e5/virtual_width_networks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T13:01:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1p37ngq</id>
    <title>GitHub - abdomody35/agent-sdk-cpp: A modern, header-only C++ library for building ReAct AI agents, supporting multiple providers, parallel tool calling, streaming responses, and more.</title>
    <updated>2025-11-21T19:19:11+00:00</updated>
    <author>
      <name>/u/Choice_Restaurant516</name>
      <uri>https://old.reddit.com/user/Choice_Restaurant516</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p37ngq/github_abdomody35agentsdkcpp_a_modern_headeronly/"&gt; &lt;img alt="GitHub - abdomody35/agent-sdk-cpp: A modern, header-only C++ library for building ReAct AI agents, supporting multiple providers, parallel tool calling, streaming responses, and more." src="https://external-preview.redd.it/6Bc-P25uBzgiR1mgHqDBhY65qLLLrU-GcO-GOnA4DkM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8b45c69360f7428888c21064a441d60f838e271d" title="GitHub - abdomody35/agent-sdk-cpp: A modern, header-only C++ library for building ReAct AI agents, supporting multiple providers, parallel tool calling, streaming responses, and more." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I made this library with a very simple and well documented api.&lt;/p&gt; &lt;p&gt;Just released v 0.1.0 with the following features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;ReAct Pattern&lt;/strong&gt;: Implement reasoning + acting agents that can use tools and maintain context&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Tool Integration&lt;/strong&gt;: Create and integrate custom tools for data access, calculations, and actions&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multiple Providers&lt;/strong&gt;: Support for Ollama (local) and OpenRouter (cloud) LLM providers (more to come in the future)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Streaming Responses&lt;/strong&gt;: Real-time streaming for both reasoning and responses&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Builder Pattern&lt;/strong&gt;: Fluent API for easy agent construction&lt;/li&gt; &lt;li&gt;&lt;strong&gt;JSON Configuration&lt;/strong&gt;: Configure agents using JSON objects&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Header-Only&lt;/strong&gt;: No compilation required - just include and use&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Choice_Restaurant516"&gt; /u/Choice_Restaurant516 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/abdomody35/agent-sdk-cpp"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p37ngq/github_abdomody35agentsdkcpp_a_modern_headeronly/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p37ngq/github_abdomody35agentsdkcpp_a_modern_headeronly/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T19:19:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1p3b0km</id>
    <title>ðŸš€ Introducing a modular survival device with a local LLM: an autonomous neural network on a Raspberry Pi</title>
    <updated>2025-11-21T21:32:18+00:00</updated>
    <author>
      <name>/u/dovudo</name>
      <uri>https://old.reddit.com/user/dovudo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We combine AI, a mesh network, and autonomous power supply in a ruggedized enclosure.&lt;/p&gt; &lt;p&gt;ðŸ‘¾ Key features:&lt;/p&gt; &lt;p&gt;â€¢ Local LLM running on Raspberry Pi (no internet needed)&lt;/p&gt; &lt;p&gt;â€¢ Mesh networking for off-grid communication&lt;/p&gt; &lt;p&gt;â€¢ Solar power option for true autonomy&lt;/p&gt; &lt;p&gt;â€¢ Rugged, survival-rated design&lt;/p&gt; &lt;p&gt;Check it out: &lt;a href="https://doomboy.net/"&gt;https://doomboy.net/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love your feedback and ideas!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dovudo"&gt; /u/dovudo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3b0km/introducing_a_modular_survival_device_with_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3b0km/introducing_a_modular_survival_device_with_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p3b0km/introducing_a_modular_survival_device_with_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T21:32:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1p3b8lk</id>
    <title>I made a writing app that runs locally in your browser</title>
    <updated>2025-11-21T21:41:12+00:00</updated>
    <author>
      <name>/u/_glimmerbloom</name>
      <uri>https://old.reddit.com/user/_glimmerbloom</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's free, works with local models, and doesn't upload your embarrassing fan fiction anywhere.&lt;/p&gt; &lt;p&gt;Complain about bugs or other issues here: &lt;a href="https://www.reddit.com/r/inksprite/"&gt;https://www.reddit.com/r/inksprite/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Or here: &lt;a href="https://github.com/inksprite-io/inksprite-release"&gt;https://github.com/inksprite-io/inksprite-release&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_glimmerbloom"&gt; /u/_glimmerbloom &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://app.inksprite.io/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3b8lk/i_made_a_writing_app_that_runs_locally_in_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p3b8lk/i_made_a_writing_app_that_runs_locally_in_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T21:41:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1p24aet</id>
    <title>Ai2 just announced Olmo 3, a leading fully open LM suite built for reasoning, chat, &amp; tool use</title>
    <updated>2025-11-20T14:16:57+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p24aet/ai2_just_announced_olmo_3_a_leading_fully_open_lm/"&gt; &lt;img alt="Ai2 just announced Olmo 3, a leading fully open LM suite built for reasoning, chat, &amp;amp; tool use" src="https://b.thumbs.redditmedia.com/xQA2jdAbLxju3aNrYQFjsQ9bmSXaOmQiXK2aPKXj8vw.jpg" title="Ai2 just announced Olmo 3, a leading fully open LM suite built for reasoning, chat, &amp;amp; tool use" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Try Olmo 3 in the Ai2 Playground â†’ &lt;a href="https://playground.allenai.org/"&gt;https://playground.allenai.org/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Download: &lt;a href="https://huggingface.co/collections/allenai/olmo-3-68e80f043cc0d3c867e7efc6"&gt;https://huggingface.co/collections/allenai/olmo-3-68e80f043cc0d3c867e7efc6&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog: &lt;a href="https://allenai.org/blog/olmo3"&gt;https://allenai.org/blog/olmo3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Technical report: &lt;a href="https://allenai.org/papers/olmo3"&gt;https://allenai.org/papers/olmo3&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1p24aet"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p24aet/ai2_just_announced_olmo_3_a_leading_fully_open_lm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p24aet/ai2_just_announced_olmo_3_a_leading_fully_open_lm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-20T14:16:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2l36u</id>
    <title>Echo TTS - 44.1kHz, Fast, Fits under 8GB VRAM - SoTA Voice Cloning</title>
    <updated>2025-11-21T01:11:55+00:00</updated>
    <author>
      <name>/u/HelpfulHand3</name>
      <uri>https://old.reddit.com/user/HelpfulHand3</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;New diffusion based multi-speaker capable TTS model released today by the engineer who made Parakeet (the arch that Dia was based on).&lt;br /&gt; &lt;strong&gt;Voice cloning is available on the&lt;/strong&gt; &lt;a href="https://huggingface.co/spaces/jordand/echo-tts-preview"&gt;HF space&lt;/a&gt; but for safety reasons (voice similarity with this model is very high) he has decided for now not to release the speaker encoder. It does come with a large voice bank however.&lt;/p&gt; &lt;p&gt;Supports some tags like (laughs), (coughs), (applause), (singing) etc.&lt;/p&gt; &lt;p&gt;Runs on consumer cards with at least 8GB VRAM.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Echo is a 2.4B DiT that generates Fish Speech S1-DAC latents (and can thus generate 44.1kHz audio; credit to Fish Speech for having trained such a great autoencoder). On an A100, Echo can generate a single 30-second sample of audio in 1.4 seconds (including decoding).&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;License: &lt;strong&gt;CC-BY-NC due to the S1 DAC autoencoder license&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Release Blog Post: &lt;a href="https://jordandarefsky.com/blog/2025/echo/"&gt;https://jordandarefsky.com/blog/2025/echo/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Demo HF Space: &lt;a href="https://huggingface.co/spaces/jordand/echo-tts-preview"&gt;https://huggingface.co/spaces/jordand/echo-tts-preview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Weights: &lt;a href="https://huggingface.co/jordand/echo-tts-no-speaker"&gt;https://huggingface.co/jordand/echo-tts-no-speaker&lt;/a&gt; &lt;a href="https://huggingface.co/jordand/fish-s1-dac-min"&gt;https://huggingface.co/jordand/fish-s1-dac-min&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Code/Github: Coming soon&lt;/p&gt; &lt;p&gt;I haven't had this much fun playing with a TTS since Higgs. This is easily up there with VibeVoice 7b and Higgs Audio v2 despite being 2.4b.&lt;/p&gt; &lt;p&gt;It can clone voices that no other model has been able to do well for me:&lt;/p&gt; &lt;p&gt;&lt;a href="https://vocaroo.com/19PQroylYsoP"&gt;https://vocaroo.com/19PQroylYsoP&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HelpfulHand3"&gt; /u/HelpfulHand3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2l36u/echo_tts_441khz_fast_fits_under_8gb_vram_sota/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2l36u/echo_tts_441khz_fast_fits_under_8gb_vram_sota/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2l36u/echo_tts_441khz_fast_fits_under_8gb_vram_sota/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T01:11:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2wpl5</id>
    <title>On the opportunity to add a Blackwell Pro 6000 to a home lab</title>
    <updated>2025-11-21T11:56:56+00:00</updated>
    <author>
      <name>/u/Expensive-Paint-9490</name>
      <uri>https://old.reddit.com/user/Expensive-Paint-9490</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just some musing. I was searching on ebay for used RTX A6000, imagining (sweet summer child me) that with Blackwell introduction prices on Ampere had become more reasonable.&lt;/p&gt; &lt;p&gt;It turns out that used A6000 are sold for a price close to the original card price. Brand new, or NOS at this point, price is actually higher than at launch.&lt;/p&gt; &lt;p&gt;At this point I am wondering if the smart thing is, buying a Pro 6000 and selling my 4090. It seems to be a neat 5500 EUR expense, but 90% of which could be recovered three or four years from now.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Expensive-Paint-9490"&gt; /u/Expensive-Paint-9490 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2wpl5/on_the_opportunity_to_add_a_blackwell_pro_6000_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2wpl5/on_the_opportunity_to_add_a_blackwell_pro_6000_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2wpl5/on_the_opportunity_to_add_a_blackwell_pro_6000_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T11:56:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1p347rt</id>
    <title>Minimax M2 - REAP 139B</title>
    <updated>2025-11-21T17:09:00+00:00</updated>
    <author>
      <name>/u/johannes_bertens</name>
      <uri>https://old.reddit.com/user/johannes_bertens</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyone did some actual (coding) work with this model yet?&lt;/p&gt; &lt;p&gt;At 80GB (Q4_K) it should fit on the Spark, the AMD Ryzen 395+ and the RTX PRO.&lt;br /&gt; The benchmarks are pretty good for prompt processing and fine for TG.&lt;/p&gt; &lt;p&gt;&lt;code&gt;Device 0: NVIDIA RTX PRO 6000 Blackwell Workstation Edition, compute capability 12.0, VMM: yes&lt;/code&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;model&lt;/th&gt; &lt;th align="right"&gt;size&lt;/th&gt; &lt;th align="right"&gt;params&lt;/th&gt; &lt;th&gt;backend&lt;/th&gt; &lt;th align="right"&gt;ngl&lt;/th&gt; &lt;th align="right"&gt;n_ubatch&lt;/th&gt; &lt;th align="right"&gt;fa&lt;/th&gt; &lt;th align="right"&gt;test&lt;/th&gt; &lt;th align="right"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;minimax-m2 230B.A10B Q4_K - Medium&lt;/td&gt; &lt;td align="right"&gt;78.40 GiB&lt;/td&gt; &lt;td align="right"&gt;139.15 B&lt;/td&gt; &lt;td&gt;CUDA&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;4096&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;pp1024&lt;/td&gt; &lt;td align="right"&gt;3623.43 Â± 14.19&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;minimax-m2 230B.A10B Q4_K - Medium&lt;/td&gt; &lt;td align="right"&gt;78.40 GiB&lt;/td&gt; &lt;td align="right"&gt;139.15 B&lt;/td&gt; &lt;td&gt;CUDA&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;4096&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;pp2048&lt;/td&gt; &lt;td align="right"&gt;4224.81 Â± 32.53&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;minimax-m2 230B.A10B Q4_K - Medium&lt;/td&gt; &lt;td align="right"&gt;78.40 GiB&lt;/td&gt; &lt;td align="right"&gt;139.15 B&lt;/td&gt; &lt;td&gt;CUDA&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;4096&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;pp3072&lt;/td&gt; &lt;td align="right"&gt;3950.17 Â± 26.11&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;minimax-m2 230B.A10B Q4_K - Medium&lt;/td&gt; &lt;td align="right"&gt;78.40 GiB&lt;/td&gt; &lt;td align="right"&gt;139.15 B&lt;/td&gt; &lt;td&gt;CUDA&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;4096&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;pp4096&lt;/td&gt; &lt;td align="right"&gt;4202.56 Â± 18.56&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;minimax-m2 230B.A10B Q4_K - Medium&lt;/td&gt; &lt;td align="right"&gt;78.40 GiB&lt;/td&gt; &lt;td align="right"&gt;139.15 B&lt;/td&gt; &lt;td&gt;CUDA&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;4096&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;pp5120&lt;/td&gt; &lt;td align="right"&gt;3984.08 Â± 21.77&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;minimax-m2 230B.A10B Q4_K - Medium&lt;/td&gt; &lt;td align="right"&gt;78.40 GiB&lt;/td&gt; &lt;td align="right"&gt;139.15 B&lt;/td&gt; &lt;td&gt;CUDA&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;4096&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;pp6144&lt;/td&gt; &lt;td align="right"&gt;4601.65 Â± 1152.92&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;minimax-m2 230B.A10B Q4_K - Medium&lt;/td&gt; &lt;td align="right"&gt;78.40 GiB&lt;/td&gt; &lt;td align="right"&gt;139.15 B&lt;/td&gt; &lt;td&gt;CUDA&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;4096&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;pp7168&lt;/td&gt; &lt;td align="right"&gt;3935.73 Â± 23.47&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;minimax-m2 230B.A10B Q4_K - Medium&lt;/td&gt; &lt;td align="right"&gt;78.40 GiB&lt;/td&gt; &lt;td align="right"&gt;139.15 B&lt;/td&gt; &lt;td&gt;CUDA&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;4096&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;pp8192&lt;/td&gt; &lt;td align="right"&gt;4003.78 Â± 16.54&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;minimax-m2 230B.A10B Q4_K - Medium&lt;/td&gt; &lt;td align="right"&gt;78.40 GiB&lt;/td&gt; &lt;td align="right"&gt;139.15 B&lt;/td&gt; &lt;td&gt;CUDA&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;4096&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;tg128&lt;/td&gt; &lt;td align="right"&gt;133.10 Â± 51.97&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;code&gt;Device 0: NVIDIA RTX PRO 6000 Blackwell Workstation Edition, compute capability 12.0, VMM: yes&lt;/code&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;model&lt;/th&gt; &lt;th align="right"&gt;size&lt;/th&gt; &lt;th align="right"&gt;params&lt;/th&gt; &lt;th&gt;backend&lt;/th&gt; &lt;th align="right"&gt;ngl&lt;/th&gt; &lt;th align="right"&gt;n_ubatch&lt;/th&gt; &lt;th align="right"&gt;fa&lt;/th&gt; &lt;th align="right"&gt;test&lt;/th&gt; &lt;th align="right"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;minimax-m2 230B.A10B Q4_K - Medium&lt;/td&gt; &lt;td align="right"&gt;78.40 GiB&lt;/td&gt; &lt;td align="right"&gt;139.15 B&lt;/td&gt; &lt;td&gt;CUDA&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;4096&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;pp10240&lt;/td&gt; &lt;td align="right"&gt;3905.55 Â± 22.55&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;minimax-m2 230B.A10B Q4_K - Medium&lt;/td&gt; &lt;td align="right"&gt;78.40 GiB&lt;/td&gt; &lt;td align="right"&gt;139.15 B&lt;/td&gt; &lt;td&gt;CUDA&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;4096&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;pp20480&lt;/td&gt; &lt;td align="right"&gt;3555.30 Â± 175.54&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;minimax-m2 230B.A10B Q4_K - Medium&lt;/td&gt; &lt;td align="right"&gt;78.40 GiB&lt;/td&gt; &lt;td align="right"&gt;139.15 B&lt;/td&gt; &lt;td&gt;CUDA&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;4096&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;pp30720&lt;/td&gt; &lt;td align="right"&gt;3049.43 Â± 71.14&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;minimax-m2 230B.A10B Q4_K - Medium&lt;/td&gt; &lt;td align="right"&gt;78.40 GiB&lt;/td&gt; &lt;td align="right"&gt;139.15 B&lt;/td&gt; &lt;td&gt;CUDA&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;4096&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;pp40960&lt;/td&gt; &lt;td align="right"&gt;2617.13 Â± 59.72&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;minimax-m2 230B.A10B Q4_K - Medium&lt;/td&gt; &lt;td align="right"&gt;78.40 GiB&lt;/td&gt; &lt;td align="right"&gt;139.15 B&lt;/td&gt; &lt;td&gt;CUDA&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;4096&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;pp51200&lt;/td&gt; &lt;td align="right"&gt;2275.03 Â± 34.24&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/johannes_bertens"&gt; /u/johannes_bertens &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p347rt/minimax_m2_reap_139b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p347rt/minimax_m2_reap_139b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p347rt/minimax_m2_reap_139b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T17:09:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2v0fe</id>
    <title>Deep Cogito v2.1, a new open weights 671B MoE model</title>
    <updated>2025-11-21T10:16:47+00:00</updated>
    <author>
      <name>/u/BreakfastFriendly728</name>
      <uri>https://old.reddit.com/user/BreakfastFriendly728</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2v0fe/deep_cogito_v21_a_new_open_weights_671b_moe_model/"&gt; &lt;img alt="Deep Cogito v2.1, a new open weights 671B MoE model" src="https://b.thumbs.redditmedia.com/ZFcSSCHd3EryXfkTwWHW8bx8DkNYSKpulONBXzIgxiQ.jpg" title="Deep Cogito v2.1, a new open weights 671B MoE model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/collections/deepcogito/cogito-v21"&gt;https://huggingface.co/collections/deepcogito/cogito-v21&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/wgqv3iva5l2g1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7b23a040098d2ed9caa81a6a322d02e18d51cc0e"&gt;https://preview.redd.it/wgqv3iva5l2g1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7b23a040098d2ed9caa81a6a322d02e18d51cc0e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/4rfhao3d5l2g1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=82dd4fcc80106c78f950f6516116123dad2f1b49"&gt;https://preview.redd.it/4rfhao3d5l2g1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=82dd4fcc80106c78f950f6516116123dad2f1b49&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/l88vmsue5l2g1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=da35111b441df51d43d4d5f04be4fb289b029525"&gt;https://preview.redd.it/l88vmsue5l2g1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=da35111b441df51d43d4d5f04be4fb289b029525&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BreakfastFriendly728"&gt; /u/BreakfastFriendly728 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2v0fe/deep_cogito_v21_a_new_open_weights_671b_moe_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2v0fe/deep_cogito_v21_a_new_open_weights_671b_moe_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2v0fe/deep_cogito_v21_a_new_open_weights_671b_moe_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T10:16:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2padh</id>
    <title>Unsloth just released their Olmo 3 dynamic quants!</title>
    <updated>2025-11-21T04:28:41+00:00</updated>
    <author>
      <name>/u/Aromatic-Distance817</name>
      <uri>https://old.reddit.com/user/Aromatic-Distance817</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2padh/unsloth_just_released_their_olmo_3_dynamic_quants/"&gt; &lt;img alt="Unsloth just released their Olmo 3 dynamic quants!" src="https://external-preview.redd.it/48d9roHWO9vPhqtCoIVGxdhD9jO5DC8s9h8U3EqHoCc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2c1dca215120c6d942685f73783d2b00bbdb86e8" title="Unsloth just released their Olmo 3 dynamic quants!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aromatic-Distance817"&gt; /u/Aromatic-Distance817 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/unsloth/Olmo-3-32B-Think-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2padh/unsloth_just_released_their_olmo_3_dynamic_quants/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2padh/unsloth_just_released_their_olmo_3_dynamic_quants/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T04:28:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2wnh0</id>
    <title>Which model to choose for coding with 8GB VRAM (assuming quantised) if I'm happy with slow rates like 1tk/s speed.</title>
    <updated>2025-11-21T11:53:47+00:00</updated>
    <author>
      <name>/u/MakeshiftApe</name>
      <uri>https://old.reddit.com/user/MakeshiftApe</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Trying to find the best local model I can use for aid in coding. My specs are: 5950X, 32GB RAM, 8GB RTX3070, so I'm severely limited on VRAM - but I seem to have much lower acceptable speeds than most people, so I'm happy to off-load a lot to the CPU to allow for a larger more capable model. &lt;/p&gt; &lt;p&gt;For me even as low as 1tk/s is plenty fast, I don't need an LLM to respond to me instantly, I can wait a minute for a reply.&lt;/p&gt; &lt;p&gt;So far after researching models that'd work with my GPU I landed on Qwen3-14B and GPT-OSS-20B, with the latter seeming better in my tests. &lt;/p&gt; &lt;p&gt;Both run pretty fast by my standards. Which leaves me wondering if I can push it higher and if so what model I should try? Is there anything better?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Any suggestions?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;If it matters at all I'm primarily looking for help with GDScript, Java, C++, and Python. Not sure if there's any variance in programming language-proficiency between models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MakeshiftApe"&gt; /u/MakeshiftApe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2wnh0/which_model_to_choose_for_coding_with_8gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2wnh0/which_model_to_choose_for_coding_with_8gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2wnh0/which_model_to_choose_for_coding_with_8gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T11:53:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1p3b60m</id>
    <title>When do you think open-source models will catch up to Gemini 3/Nano Banana pro? Who's the closest candidate right now?</title>
    <updated>2025-11-21T21:38:16+00:00</updated>
    <author>
      <name>/u/abdouhlili</name>
      <uri>https://old.reddit.com/user/abdouhlili</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Iâ€™m curious about the current gap between open-source models and something like Gemini 3. Do you think open-source will catch up anytime soon, and if so, which model is the closest right now?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdouhlili"&gt; /u/abdouhlili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3b60m/when_do_you_think_opensource_models_will_catch_up/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3b60m/when_do_you_think_opensource_models_will_catch_up/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p3b60m/when_do_you_think_opensource_models_will_catch_up/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T21:38:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1p360cl</id>
    <title>How's your experience with Qwen3-Next-80B-A3B ?</title>
    <updated>2025-11-21T18:16:15+00:00</updated>
    <author>
      <name>/u/woahdudee2a</name>
      <uri>https://old.reddit.com/user/woahdudee2a</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know llama.cpp support is still a short while away but surely some people here are able to run it with vLLM. I'm curious how it performs in comparison to gpt-oss-120b or nemotron-super-49B-v1.5&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/woahdudee2a"&gt; /u/woahdudee2a &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p360cl/hows_your_experience_with_qwen3next80ba3b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p360cl/hows_your_experience_with_qwen3next80ba3b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p360cl/hows_your_experience_with_qwen3next80ba3b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T18:16:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2v5ap</id>
    <title>Epstein Files Document Embeddings (768D, Nomic)</title>
    <updated>2025-11-21T10:25:10+00:00</updated>
    <author>
      <name>/u/qwer1627</name>
      <uri>https://old.reddit.com/user/qwer1627</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Text embeddings generated from the House Oversight Committee's Epstein document release. (768D, Nomic)&lt;/p&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/datasets/svetfm/epstein-files-nov11-25-house-post-ocr-embeddings#source-dataset"&gt;&lt;/a&gt;Source Dataset&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;This dataset is derived from:&lt;/strong&gt; &lt;a href="https://huggingface.co/datasets/tensonaut/EPSTEIN_FILES_20K"&gt;tensonaut/EPSTEIN_FILES_20K&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The source dataset contains OCR'd text from the original House Oversight Committee PDF release.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/datasets/svetfm/epstein-files-nov11-25-house-post-ocr-embeddings"&gt;https://huggingface.co/datasets/svetfm/epstein-files-nov11-25-house-post-ocr-embeddings&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/qwer1627"&gt; /u/qwer1627 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2v5ap/epstein_files_document_embeddings_768d_nomic/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2v5ap/epstein_files_document_embeddings_768d_nomic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2v5ap/epstein_files_document_embeddings_768d_nomic/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T10:25:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1p335ta</id>
    <title>FYI / warning: default Nvidia fan speed control (Blackwell, maybe others) is horrible</title>
    <updated>2025-11-21T16:29:26+00:00</updated>
    <author>
      <name>/u/sixx7</name>
      <uri>https://old.reddit.com/user/sixx7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As we all do, I obsessively monitor &lt;code&gt;nvtop&lt;/code&gt; during AI or other heavy workloads on my GPUs. Well, the other day, I noticed a 5090 running at 81-83C but the fan only running at 50%. Yikes!&lt;/p&gt; &lt;p&gt;I tried everything in this thread: &lt;a href="https://forums.developer.nvidia.com/t/how-to-set-fanspeed-in-linux-from-terminal/72705"&gt;https://forums.developer.nvidia.com/t/how-to-set-fanspeed-in-linux-from-terminal/72705&lt;/a&gt; to no avail. Even using the gui of nvidia-settings, as root, would not let me apply a higher fan speed.&lt;/p&gt; &lt;p&gt;I found 3 repos on Github to solve this. I am not affiliated with any of them, and I chose the Python option (credit: &lt;a href="https://www.reddit.com/r/wayland/comments/1arjtxj/i_have_created_a_program_to_control_nvidia_gpus/"&gt;https://www.reddit.com/r/wayland/comments/1arjtxj/i_have_created_a_program_to_control_nvidia_gpus/&lt;/a&gt; )&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Python option:&lt;a href="https://github.com/HackTestes/NVML-GPU-Control"&gt;https://github.com/HackTestes/NVML-GPU-Control&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Golang option: &lt;a href="https://github.com/ntchjb/nvidia-fan-controller"&gt;https://github.com/ntchjb/nvidia-fan-controller&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;C option:&lt;a href="https://github.com/xl0/nvml-tool"&gt;https://github.com/xl0/nvml-tool&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The python app worked like a charm: chnvml control -n &amp;quot;NVIDIA GeForce RTX 5090&amp;quot; -sp &amp;quot;0:30,30:35,35:40,40:50,50:65,60:100&amp;quot;&lt;/p&gt; &lt;p&gt;This ramped up my fan speeds right away and immediately brought my GPU temperature below 70C&lt;/p&gt; &lt;p&gt;I am pretty shocked it was a steady 81C+ and keeping the fan at 50%. Maybe it's better in other OS or driver versions. My env: Ubuntu, Nvidia driver version 580.95.05&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sixx7"&gt; /u/sixx7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p335ta/fyi_warning_default_nvidia_fan_speed_control/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p335ta/fyi_warning_default_nvidia_fan_speed_control/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p335ta/fyi_warning_default_nvidia_fan_speed_control/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T16:29:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2ziil</id>
    <title>Hardcore function calling benchmark in backend coding agent.</title>
    <updated>2025-11-21T14:06:49+00:00</updated>
    <author>
      <name>/u/jhnam88</name>
      <uri>https://old.reddit.com/user/jhnam88</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2ziil/hardcore_function_calling_benchmark_in_backend/"&gt; &lt;img alt="Hardcore function calling benchmark in backend coding agent." src="https://b.thumbs.redditmedia.com/YaA_TdwEKDUN4oRX3BCB118KPjtAaCveNFCP3lFYqyQ.jpg" title="Hardcore function calling benchmark in backend coding agent." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h2&gt;Hardcore Benchmark&lt;/h2&gt; &lt;p&gt;&lt;a href="https://github.com/wrtnlabs/autobe"&gt;AutoBE&lt;/a&gt; is an open-source project that generates backend applications through extensive function calling.&lt;/p&gt; &lt;p&gt;As AutoBE utilizes LLM function calling in every phase instead of plain text writing, including compiler's AST (Abstract Syntax Tree) structures of infinite depths, I think this can be the most extreme function calling benchmark ever.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/wrtnlabs/autobe/blob/main/packages/interface/src/prisma/AutoBePrisma.ts"&gt;DB Compiler's AST&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/wrtnlabs/autobe/blob/main/packages/interface/src/openapi/AutoBeOpenApi.ts"&gt;API specification's AST&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/wrtnlabs/autobe/blob/main/packages/interface/src/test/AutoBeTest.ts"&gt;Test function's AST&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;code&gt;typescript // Example of AutoBE's AST structure export namespace AutoBeOpenApi { export type IJsonSchema = | IJsonSchema.IConstant | IJsonSchema.IBoolean | IJsonSchema.IInteger | IJsonSchema.INumber | IJsonSchema.IString | IJsonSchema.IArray | IJsonSchema.IObject | IJsonSchema.IReference | IJsonSchema.IOneOf | IJsonSchema.INull; } &lt;/code&gt;&lt;/p&gt; &lt;h2&gt;Limitations&lt;/h2&gt; &lt;p&gt;Of course, as you can see, the number of DB schemas and API operations generated for the same topic varies greatly by each model. When &lt;a href="https://github.com/wrtnlabs/autobe-examples/tree/main/anthropic/claude-sonnet-4.5/shopping"&gt;&lt;code&gt;anthropic/claude-sonnet-4.5&lt;/code&gt;&lt;/a&gt; and &lt;a href="https://github.com/wrtnlabs/autobe-examples/tree/main/openai/gpt-5.1/shopping"&gt;&lt;code&gt;openai/gpt-5.1&lt;/code&gt;&lt;/a&gt; create 630 and 2,000 test functions respectively for the same topic, &lt;a href="https://github.com/wrtnlabs/autobe-examples/tree/main/qwen/qwen3-next-80b-a3b-instruct/shopping"&gt;&lt;code&gt;qwen/qwen3-next-80b-a3b&lt;/code&gt;&lt;/a&gt; creates 360.&lt;/p&gt; &lt;p&gt;Moreover, function calling in AutoBE includes a &lt;a href="https://autobe.dev/docs/concepts/function-calling/#validation-feedback"&gt;validation feedback&lt;/a&gt; process that detects detailed type errors and provides feedback to the AI for recovery, even when the AI makes mistakes and creates arguments of the wrong type.&lt;/p&gt; &lt;p&gt;Simply scoring and ranking based solely on compilation/build success, and evaluating each model's function calling capabilities in depth based only on the success rate of function calling with validation feedback, is still far from sufficient.&lt;/p&gt; &lt;p&gt;Therefore, please understand that the current benchmark is simply uncontrolled and only indicates whether or not each AI model can properly construct extremely complex types, including compiler AST structures, through function calling.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;AutoBE is also still incomplete.&lt;/p&gt; &lt;p&gt;Even if the backend application generated through this guarantees a 100% compilation success rate, it does not guarantee a 100% runtime success rate. This is an open-source project with a long way to go in development and mountains of research still to be done.&lt;/p&gt; &lt;p&gt;However, we hope that this can serve as a reference for anyone planning function calling with extremely complex types like ours, and contribute even a little to the AI ecosystem.&lt;/p&gt; &lt;/blockquote&gt; &lt;h2&gt;Promise&lt;/h2&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1o3604u/autobe_achieved_100_compilation_success_of/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1o3604u/autobe_achieved_100_compilation_success_of/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A month ago, we achieved a 100% build success rate for small to medium-sized backend applications with &lt;code&gt;qwen3-next-80b-a3b&lt;/code&gt;, and promised to complete RAG optimization in the future to enable the generation of large-scale backend applications on Local LLMs.&lt;/p&gt; &lt;p&gt;Now this has become possible with various Local LLMs such as Qwen3/DeepSeek/Kimi, in addition to commercial models like GPT and Sonnet. While prompting and RAG optimization may not yet be perfect, as models like GPT-5.1 run wild creating as many as 2,000 test functions, we will resolve this issue the next time we come back.&lt;/p&gt; &lt;p&gt;And since many people were curious about the performance of various Local LLMs besides &lt;code&gt;qwen3-next-80b-a3b&lt;/code&gt;, we promised to consistently release benchmark data for them. While it's unfortunate that the benchmark we released today is inadequate due to lack of controlled variables and can only determine whether function calling with extremely complex types is possible or not, we will improve this as well next time.&lt;/p&gt; &lt;p&gt;We, the two AutoBE developers, will continue to dedicate ourselves to its development, striving to create an environment where you can freely generate backend applications on your local devices without cost burden.&lt;/p&gt; &lt;p&gt;In addition, we are always grateful to the specialists who build and freely distribute open-source AI models.&lt;/p&gt; &lt;h2&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;AutoBE: &lt;a href="https://github.com/wrtnlabs/autobe"&gt;https://github.com/wrtnlabs/autobe&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Benchmark Result: &lt;a href="https://github.com/wrtnlabs/autobe-examples"&gt;https://github.com/wrtnlabs/autobe-examples&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jhnam88"&gt; /u/jhnam88 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1p2ziil"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2ziil/hardcore_function_calling_benchmark_in_backend/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2ziil/hardcore_function_calling_benchmark_in_backend/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T14:06:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1p36l5f</id>
    <title>2x RTX 5060 TI 16 GB =32GB VRAM -</title>
    <updated>2025-11-21T18:38:39+00:00</updated>
    <author>
      <name>/u/quantier</name>
      <uri>https://old.reddit.com/user/quantier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p36l5f/2x_rtx_5060_ti_16_gb_32gb_vram/"&gt; &lt;img alt="2x RTX 5060 TI 16 GB =32GB VRAM -" src="https://preview.redd.it/ven6e8i8nn2g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ff9be1ee0ff290587c547010478fc54b1176c114" title="2x RTX 5060 TI 16 GB =32GB VRAM -" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is anyone up and running with a rig like this with 2x RTX 5060 TI? how is it? What PSU does one need? How much compute do you loose when you have 2 GPU:s instead of a 1 card setup. How would 2x 5060 TI be in comparison with a 5090.&lt;/p&gt; &lt;p&gt;How does one put together these GPU:s in ComfyUI? Does one need to add new nodes to the workflows?&lt;/p&gt; &lt;p&gt;Is this worth it, I can get a RTX 5060 TI 16GB for around $400 each meaning that $800 for 32 GB VRAM feels very interesting with a Blackwell card!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/quantier"&gt; /u/quantier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ven6e8i8nn2g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p36l5f/2x_rtx_5060_ti_16_gb_32gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p36l5f/2x_rtx_5060_ti_16_gb_32gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T18:38:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1p38wp2</id>
    <title>Dell puts 870 INT8 TOPS in Pro Max 16 Plus laptop with dual Qualcomm AI-100 discrete NPUs and 128GB LPDDR5X</title>
    <updated>2025-11-21T20:08:24+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p38wp2/dell_puts_870_int8_tops_in_pro_max_16_plus_laptop/"&gt; &lt;img alt="Dell puts 870 INT8 TOPS in Pro Max 16 Plus laptop with dual Qualcomm AI-100 discrete NPUs and 128GB LPDDR5X" src="https://external-preview.redd.it/k6ugreuNYLcJLu7o30ZlDvM4GYr0mtEvIvPMXJ8mR2c.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=675947acdabed7d61c07e99006c75339ee1cfa5f" title="Dell puts 870 INT8 TOPS in Pro Max 16 Plus laptop with dual Qualcomm AI-100 discrete NPUs and 128GB LPDDR5X" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Dell is shipping the Pro Max 16 Plus laptop with Qualcommâ€™s discrete AI-100 Ultra NPU, delivering 870 INT8 TOPS at 150W TDP with 128GB LPDDR5X memory, enabling local inference of AI models up to 120 billion parameters. The system pairs this with an Intel Core Ultra 9 285HX vPro CPU (24 cores) and 64GB system RAM, but notably omits a discrete GPU, relying instead on Arrow Lake-HXâ€™s integrated graphics, as the NPU occupies the thermal and power budget typically allocated to a dGPU. The dual-NPU configuration provides 64GB dedicated AI memory and supports FP16 precision inference, positioning the device as an â€œedge server in a backpackâ€.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.techpowerup.com/343143/dell-ship-pro-max-16-plus-laptops-with-qualcomms-discrete-npu"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p38wp2/dell_puts_870_int8_tops_in_pro_max_16_plus_laptop/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p38wp2/dell_puts_870_int8_tops_in_pro_max_16_plus_laptop/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T20:08:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2w5i6</id>
    <title>HunyuanVideo-1.5: A leading lightweight video generation model</title>
    <updated>2025-11-21T11:25:33+00:00</updated>
    <author>
      <name>/u/abdouhlili</name>
      <uri>https://old.reddit.com/user/abdouhlili</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/tencent/HunyuanVideo-1.5"&gt;https://huggingface.co/tencent/HunyuanVideo-1.5&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdouhlili"&gt; /u/abdouhlili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2w5i6/hunyuanvideo15_a_leading_lightweight_video/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2w5i6/hunyuanvideo15_a_leading_lightweight_video/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2w5i6/hunyuanvideo15_a_leading_lightweight_video/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T11:25:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1p36iln</id>
    <title>Made a site where AI models trade against each other. A local model is winning.</title>
    <updated>2025-11-21T18:35:58+00:00</updated>
    <author>
      <name>/u/2degreestarget</name>
      <uri>https://old.reddit.com/user/2degreestarget</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been messing around with new Gemini this week and ended up building this thing where different LLMs compete as stock traders. I work in asset management so I was genuinely curious how these models would approach investing.&lt;/p&gt; &lt;p&gt;Some observations:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Qwen (the only local model) is currently winning, mostly because keeps 90% cash (saving for a GPU?)&lt;/li&gt; &lt;li&gt;None of them understand position sizing. Like, at all. And they all have this weird overconfidence where they'll write a whole thesis and then make a trade that contradicts it.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Anyway it's not meant to be serious financial advice or anything. Just thought it was a fun way to see how these models actually think when you give them a concrete task. &lt;/p&gt; &lt;p&gt;Code is messy but it works. Considering doing a fully local version to stop burning my openrouter credits...&lt;br /&gt; &lt;a href="http://wallstreetarena.xyz/"&gt;http://wallstreetarena.xyz/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/2degreestarget"&gt; /u/2degreestarget &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p36iln/made_a_site_where_ai_models_trade_against_each/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p36iln/made_a_site_where_ai_models_trade_against_each/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p36iln/made_a_site_where_ai_models_trade_against_each/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T18:35:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1p35f2c</id>
    <title>I made a free playground for comparing 10+ OCR models side-by-side</title>
    <updated>2025-11-21T17:54:07+00:00</updated>
    <author>
      <name>/u/Emc2fma</name>
      <uri>https://old.reddit.com/user/Emc2fma</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's called OCR Arena, you can try it here: &lt;a href="https://ocrarena.ai"&gt;https://ocrarena.ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There's so many new OCR models coming out all the time, but testing them is really painful. I wanted to give the community an easy way to compare leading foundation VLMs and open source OCR models side-by-side. You can upload any doc, run a variety of models, and view diffs easily.&lt;/p&gt; &lt;p&gt;So far I've added Gemini 3, dots, DeepSeek-OCR, olmOCR 2, Qwen3-VL-8B, and a few others. &lt;/p&gt; &lt;p&gt;Would love any feedback you have! And if there's any other models you'd like included, let me know.&lt;/p&gt; &lt;p&gt;(No surprise, Gemini 3 is top of the leaderboard right now)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Emc2fma"&gt; /u/Emc2fma &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p35f2c/i_made_a_free_playground_for_comparing_10_ocr/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p35f2c/i_made_a_free_playground_for_comparing_10_ocr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p35f2c/i_made_a_free_playground_for_comparing_10_ocr/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T17:54:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1ozopxx</id>
    <title>AMA Announcement: MiniMax, The Opensource Lab Behind MiniMax-M2 + Gifts to Our Community (Wednesday, 8AM-11AM PST)</title>
    <updated>2025-11-17T18:50:44+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozopxx/ama_announcement_minimax_the_opensource_lab/"&gt; &lt;img alt="AMA Announcement: MiniMax, The Opensource Lab Behind MiniMax-M2 + Gifts to Our Community (Wednesday, 8AM-11AM PST)" src="https://preview.redd.it/e3scgr4j5v1g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ca0bf85cb4cf2a2b7e12e8d99d515186275c15e3" title="AMA Announcement: MiniMax, The Opensource Lab Behind MiniMax-M2 + Gifts to Our Community (Wednesday, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/e3scgr4j5v1g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozopxx/ama_announcement_minimax_the_opensource_lab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ozopxx/ama_announcement_minimax_the_opensource_lab/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-17T18:50:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1b550</id>
    <title>AMA with MiniMax â€” Ask Us Anything!</title>
    <updated>2025-11-19T15:46:38+00:00</updated>
    <author>
      <name>/u/OccasionNo6699</name>
      <uri>https://old.reddit.com/user/OccasionNo6699</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;! Weâ€™re really excited to be here, thanks for having us.&lt;/p&gt; &lt;p&gt;Iâ€™m Skyler (&lt;a href="https://www.reddit.com/user/OccasionNo6699/"&gt;u/OccasionNo6699&lt;/a&gt;), head of engineering at MiniMax, the lab behind:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://x.com/MiniMax__AI/status/1982674798649160175?s=20"&gt;MiniMax-M2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983382728343994414"&gt;Hailuo 2.3&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983661667872600296"&gt;MiniMax Speech 2.6&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983964920493568296"&gt;MiniMax Music 2.0&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Joining me today are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Pengyu Zhao, &lt;a href="https://www.reddit.com/user/Wise_Evidence9973/"&gt;u/Wise_Evidence9973&lt;/a&gt; â€” Head of LLM Research&lt;/li&gt; &lt;li&gt;Jade Cai, &lt;a href="https://www.reddit.com/user/srtng/"&gt;u/srtng&lt;/a&gt; â€” Head of Developer Community&lt;/li&gt; &lt;li&gt;midnight_compile , &lt;a href="/u/Top_Cattle_2098"&gt;u/Top_Cattle_2098&lt;/a&gt; â€” LLM Researcher&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 8AM-11AM PST with our core MiniMax tech team continuing to follow up on questions over the next 48 hours.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OccasionNo6699"&gt; /u/OccasionNo6699 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T15:46:38+00:00</published>
  </entry>
</feed>
