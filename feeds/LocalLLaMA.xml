<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-06T19:20:24+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1na3icm</id>
    <title>today grok code fast 1 brake the 1 trillion mark in the open router history</title>
    <updated>2025-09-06T16:12:30+00:00</updated>
    <author>
      <name>/u/Select_Dream634</name>
      <uri>https://old.reddit.com/user/Select_Dream634</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1na3icm/today_grok_code_fast_1_brake_the_1_trillion_mark/"&gt; &lt;img alt="today grok code fast 1 brake the 1 trillion mark in the open router history" src="https://preview.redd.it/l35vq1trjknf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4ad02854dcca1e4396c4063e031e5a95da35a2c9" title="today grok code fast 1 brake the 1 trillion mark in the open router history" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Select_Dream634"&gt; /u/Select_Dream634 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/l35vq1trjknf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1na3icm/today_grok_code_fast_1_brake_the_1_trillion_mark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1na3icm/today_grok_code_fast_1_brake_the_1_trillion_mark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-06T16:12:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1n9hduk</id>
    <title>VibeVoice came back. Though many may not like it.</title>
    <updated>2025-09-05T21:19:36+00:00</updated>
    <author>
      <name>/u/Fresh_Sun_1017</name>
      <uri>https://old.reddit.com/user/Fresh_Sun_1017</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/microsoft/VibeVoice"&gt;VibeVoice&lt;/a&gt; has returned(&lt;em&gt;not&lt;/em&gt; VibeVoice-large); however, Microsoft plans to implement censorship due to people's &amp;quot;misuse of research&amp;quot;. Here's the quote from the repo:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;VibeVoice is an open-source research framework intended to advance collaboration in the speech synthesis community. &lt;strong&gt;After release, we discovered instances where the tool was used in ways inconsistent with the stated intent. Since responsible use of AI is one of Microsoft’s guiding principles, we have disabled this repo until we are confident that out-of-scope use is no longer possible.&lt;/strong&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;What types of censorship will be implemented? And couldn’t people just use or share older, unrestricted versions they've already downloaded? That's going to be interesting...&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Edit:&lt;/strong&gt; The VibeVoice-Large model is still available as of now, &lt;a href="https://www.modelscope.cn/models/microsoft/VibeVoice-Large/files"&gt;VibeVoice-Large · Models&lt;/a&gt; on Modelscope. It may be deleted soon.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fresh_Sun_1017"&gt; /u/Fresh_Sun_1017 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9hduk/vibevoice_came_back_though_many_may_not_like_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9hduk/vibevoice_came_back_though_many_may_not_like_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n9hduk/vibevoice_came_back_though_many_may_not_like_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-05T21:19:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1n975er</id>
    <title>Qwen 3 max</title>
    <updated>2025-09-05T14:42:28+00:00</updated>
    <author>
      <name>/u/LeatherRub7248</name>
      <uri>https://old.reddit.com/user/LeatherRub7248</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n975er/qwen_3_max/"&gt; &lt;img alt="Qwen 3 max" src="https://external-preview.redd.it/9f9JRaQTq2uR5GC3copbxq5McLsZhYSzNHSbhHCgcmg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1f89b1589b444db5310feea66a3e0335c0591fac" title="Qwen 3 max" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's out&lt;/p&gt; &lt;p&gt;&lt;a href="https://openrouter.ai/qwen/qwen3-max"&gt;https://openrouter.ai/qwen/qwen3-max&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://chat.qwen.ai/"&gt;https://chat.qwen.ai/&lt;/a&gt; (qwen 3 max preview)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/nb9wzcl9bdnf1.png?width=1254&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d22747e9fb863b0412d20782dd88e055fbb87a9f"&gt;https://preview.redd.it/nb9wzcl9bdnf1.png?width=1254&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d22747e9fb863b0412d20782dd88e055fbb87a9f&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LeatherRub7248"&gt; /u/LeatherRub7248 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n975er/qwen_3_max/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n975er/qwen_3_max/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n975er/qwen_3_max/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-05T14:42:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1n9sod7</id>
    <title>What is the most effective way to have your local LLM search the web?</title>
    <updated>2025-09-06T06:43:22+00:00</updated>
    <author>
      <name>/u/teknic111</name>
      <uri>https://old.reddit.com/user/teknic111</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I would love if I could get web results the same way ChatGPT does. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/teknic111"&gt; /u/teknic111 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9sod7/what_is_the_most_effective_way_to_have_your_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9sod7/what_is_the_most_effective_way_to_have_your_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n9sod7/what_is_the_most_effective_way_to_have_your_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-06T06:43:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1n9zuil</id>
    <title>Tested sonoma-sky-alpha on Fiction.liveBench, fantastic close to SOTA scores, currently free</title>
    <updated>2025-09-06T13:40:47+00:00</updated>
    <author>
      <name>/u/fictionlive</name>
      <uri>https://old.reddit.com/user/fictionlive</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9zuil/tested_sonomaskyalpha_on_fictionlivebench/"&gt; &lt;img alt="Tested sonoma-sky-alpha on Fiction.liveBench, fantastic close to SOTA scores, currently free" src="https://preview.redd.it/lbbwz19ssjnf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6c7f0bc4e724dc56e61b41a81da5f3d8d98fc7b0" title="Tested sonoma-sky-alpha on Fiction.liveBench, fantastic close to SOTA scores, currently free" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fictionlive"&gt; /u/fictionlive &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lbbwz19ssjnf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9zuil/tested_sonomaskyalpha_on_fictionlivebench/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n9zuil/tested_sonomaskyalpha_on_fictionlivebench/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-06T13:40:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1n9omqj</id>
    <title>Kimi K2-0905 is a powerhouse VS claude-sonnet-4 @20250514.</title>
    <updated>2025-09-06T02:55:27+00:00</updated>
    <author>
      <name>/u/klippers</name>
      <uri>https://old.reddit.com/user/klippers</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been heavily builidng with claude-sonnet-4@20250514, but threw $5 into OpenRouter and gave K2-0905 and WOW. &lt;/p&gt; &lt;p&gt;Not sure if its a “better” model, but seems to chew through tasks in a “better” way.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/klippers"&gt; /u/klippers &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9omqj/kimi_k20905_is_a_powerhouse_vs_claudesonnet4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9omqj/kimi_k20905_is_a_powerhouse_vs_claudesonnet4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n9omqj/kimi_k20905_is_a_powerhouse_vs_claudesonnet4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-06T02:55:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1na78oe</id>
    <title>Qwen3 30B A3B Q40 @ 13 tok/sec on Raspberry Pi cluster</title>
    <updated>2025-09-06T18:40:33+00:00</updated>
    <author>
      <name>/u/DealingWithIt202s</name>
      <uri>https://old.reddit.com/user/DealingWithIt202s</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1na78oe/qwen3_30b_a3b_q40_13_toksec_on_raspberry_pi/"&gt; &lt;img alt="Qwen3 30B A3B Q40 @ 13 tok/sec on Raspberry Pi cluster" src="https://external-preview.redd.it/KUWKhlT5OZYpzmuPdkrY6FyowQ4PaYe23RiUvraDVrQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4733d277f6f6e759fe47794087d1da790f8d36b7" title="Qwen3 30B A3B Q40 @ 13 tok/sec on Raspberry Pi cluster" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Distributed llama is really promising for local inference. You could run multiple boxes on different circuits in your home to deal with power limitations, and scale capacity constraints horizontally. It doesn’t solve for running large open models, but it makes smaller ones potentially way faster. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DealingWithIt202s"&gt; /u/DealingWithIt202s &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/b4rtaz/distributed-llama/discussions/255"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1na78oe/qwen3_30b_a3b_q40_13_toksec_on_raspberry_pi/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1na78oe/qwen3_30b_a3b_q40_13_toksec_on_raspberry_pi/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-06T18:40:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1na79xn</id>
    <title>MCP with Computer Use</title>
    <updated>2025-09-06T18:41:57+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1na79xn/mcp_with_computer_use/"&gt; &lt;img alt="MCP with Computer Use" src="https://external-preview.redd.it/Yms5cnp6d2lhbG5mMRWfbJ8igMn1wejE5S1jscBm4hQQlwhyyetxPIo-3e1f.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=027bb2e2722caa9f69a89cc82292781870bfca74" title="MCP with Computer Use" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;MCP Server with Computer Use Agent runs through Claude Desktop, Cursor, and other MCP clients.&lt;/p&gt; &lt;p&gt;An example use case lets try using Claude as a tutor to learn how to use Tableau.&lt;/p&gt; &lt;p&gt;The MCP Server implementation exposes CUA's full functionality through standardized tool calls. It supports single-task commands and multi-task sequences, giving Claude Desktop direct access to all of Cua's computer control capabilities.&lt;/p&gt; &lt;p&gt;This is the first MCP-compatible computer control solution that works directly with Claude Desktop's and Cursor's built-in MCP implementation. Simple configuration in your claude_desktop_config.json or cursor_config.json connects Claude or Cursor directly to your desktop environment.&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Discord: discord.gg/cua-ai&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/dk9apg8jalnf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1na79xn/mcp_with_computer_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1na79xn/mcp_with_computer_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-06T18:41:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1na61bk</id>
    <title>Struggling with GPT-OSS-20B</title>
    <updated>2025-09-06T17:53:02+00:00</updated>
    <author>
      <name>/u/Psychological_Ad8426</name>
      <uri>https://old.reddit.com/user/Psychological_Ad8426</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm really having a hard time training it. I have data that I have used on mistral for pharmacy direction conversions to a short code. Example, take one tablet orally daily to T1T PO QD. A normal pharmacy thing. Mistral model does it well but sometimes there are complex scenarios it fails on. I was thinking reasoning may help.&lt;/p&gt; &lt;p&gt;In short, it doesn't seem like GPT-OSS model is following the training. It gives random directions and rarely gets the simple ones incorrect. Below is YAML of config and happy to load sample data if anyone can provide guidance. &lt;/p&gt; &lt;p&gt;Config &lt;/p&gt; &lt;p&gt;architecture:&lt;/p&gt; &lt;p&gt;backbone_dtype: bfloat16&lt;/p&gt; &lt;p&gt;gradient_checkpointing: true&lt;/p&gt; &lt;p&gt;intermediate_dropout: 0.0&lt;/p&gt; &lt;p&gt;pretrained: true&lt;/p&gt; &lt;p&gt;pretrained_weights: ''&lt;/p&gt; &lt;p&gt;augmentation:&lt;/p&gt; &lt;p&gt;neftune_noise_alpha: 0.0&lt;/p&gt; &lt;p&gt;random_parent_probability: 0.0&lt;/p&gt; &lt;p&gt;skip_parent_probability: 0.0&lt;/p&gt; &lt;p&gt;token_mask_probability: 0.0&lt;/p&gt; &lt;p&gt;dataset:&lt;/p&gt; &lt;p&gt;add_eos_token_to_answer: true&lt;/p&gt; &lt;p&gt;add_eos_token_to_prompt: true&lt;/p&gt; &lt;p&gt;add_eos_token_to_system: true&lt;/p&gt; &lt;p&gt;answer_column: &amp;quot;output_json\r&amp;quot;&lt;/p&gt; &lt;p&gt;chatbot_author: &lt;a href="http://H2O.ai"&gt;H2O.ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;chatbot_name: h2oGPT&lt;/p&gt; &lt;p&gt;data_sample: 1.0&lt;/p&gt; &lt;p&gt;data_sample_choice:&lt;/p&gt; &lt;p&gt;- Train&lt;/p&gt; &lt;p&gt;- Validation&lt;/p&gt; &lt;p&gt;id_column: None&lt;/p&gt; &lt;p&gt;limit_chained_samples: false&lt;/p&gt; &lt;p&gt;mask_prompt_labels: true&lt;/p&gt; &lt;p&gt;only_last_answer: false&lt;/p&gt; &lt;p&gt;parent_id_column: None&lt;/p&gt; &lt;p&gt;personalize: false&lt;/p&gt; &lt;p&gt;prompt_column:&lt;/p&gt; &lt;p&gt;- instruction&lt;/p&gt; &lt;p&gt;prompt_column_separator: ''&lt;/p&gt; &lt;p&gt;system_column: system&lt;/p&gt; &lt;p&gt;text_answer_separator: ''&lt;/p&gt; &lt;p&gt;text_prompt_start: ''&lt;/p&gt; &lt;p&gt;text_system_start: ''&lt;/p&gt; &lt;p&gt;train_dataframe: /home/llmstudio/mount/data/user/Training eRx 1 v8/Training eRx&lt;/p&gt; &lt;p&gt;1 v8.csv&lt;/p&gt; &lt;p&gt;validation_dataframe: None&lt;/p&gt; &lt;p&gt;validation_size: 0.1&lt;/p&gt; &lt;p&gt;validation_strategy: automatic&lt;/p&gt; &lt;p&gt;environment:&lt;/p&gt; &lt;p&gt;compile_model: false&lt;/p&gt; &lt;p&gt;deepspeed_allgather_bucket_size: 1000000&lt;/p&gt; &lt;p&gt;deepspeed_method: ZeRO2&lt;/p&gt; &lt;p&gt;deepspeed_reduce_bucket_size: 1000000&lt;/p&gt; &lt;p&gt;deepspeed_stage3_param_persistence_threshold: 1000000&lt;/p&gt; &lt;p&gt;deepspeed_stage3_prefetch_bucket_size: 1000000&lt;/p&gt; &lt;p&gt;find_unused_parameters: false&lt;/p&gt; &lt;p&gt;gpus:&lt;/p&gt; &lt;p&gt;- '0'&lt;/p&gt; &lt;p&gt;huggingface_branch: main&lt;/p&gt; &lt;p&gt;mixed_precision: true&lt;/p&gt; &lt;p&gt;mixed_precision_dtype: bfloat16&lt;/p&gt; &lt;p&gt;number_of_workers: 8&lt;/p&gt; &lt;p&gt;seed: 42&lt;/p&gt; &lt;p&gt;trust_remote_code: true&lt;/p&gt; &lt;p&gt;use_deepspeed: false&lt;/p&gt; &lt;p&gt;experiment_name: emerald-leech&lt;/p&gt; &lt;p&gt;llm_backbone: openai/gpt-oss-20b&lt;/p&gt; &lt;p&gt;logging:&lt;/p&gt; &lt;p&gt;log_all_ranks: false&lt;/p&gt; &lt;p&gt;log_step_size: absolute&lt;/p&gt; &lt;p&gt;logger: None&lt;/p&gt; &lt;p&gt;neptune_project: ''&lt;/p&gt; &lt;p&gt;wandb_entity: ''&lt;/p&gt; &lt;p&gt;wandb_project: ''&lt;/p&gt; &lt;p&gt;output_directory: /home/llmstudio/mount/output/user/emerald-leech/&lt;/p&gt; &lt;p&gt;prediction:&lt;/p&gt; &lt;p&gt;batch_size_inference: 0&lt;/p&gt; &lt;p&gt;do_sample: false&lt;/p&gt; &lt;p&gt;max_length_inference: 256&lt;/p&gt; &lt;p&gt;max_time: 0.0&lt;/p&gt; &lt;p&gt;metric: Perplexity&lt;/p&gt; &lt;p&gt;metric_gpt_model: gpt-3.5-turbo-0301&lt;/p&gt; &lt;p&gt;metric_gpt_template: general&lt;/p&gt; &lt;p&gt;min_length_inference: 2&lt;/p&gt; &lt;p&gt;num_beams: 1&lt;/p&gt; &lt;p&gt;num_history: 4&lt;/p&gt; &lt;p&gt;repetition_penalty: 1.0&lt;/p&gt; &lt;p&gt;stop_tokens: ''&lt;/p&gt; &lt;p&gt;temperature: 0.0&lt;/p&gt; &lt;p&gt;top_k: 0&lt;/p&gt; &lt;p&gt;top_p: 1.0&lt;/p&gt; &lt;p&gt;problem_type: text_causal_language_modeling&lt;/p&gt; &lt;p&gt;tokenizer:&lt;/p&gt; &lt;p&gt;add_prompt_answer_tokens: false&lt;/p&gt; &lt;p&gt;max_length: 768&lt;/p&gt; &lt;p&gt;padding_quantile: 0.95&lt;/p&gt; &lt;p&gt;tokenizer_kwargs: '{&amp;quot;use_fast&amp;quot;: true, &amp;quot;add_prefix_space&amp;quot;: false}'&lt;/p&gt; &lt;p&gt;training:&lt;/p&gt; &lt;p&gt;attention_implementation: eager&lt;/p&gt; &lt;p&gt;batch_size: 8&lt;/p&gt; &lt;p&gt;differential_learning_rate: 1.0e-05&lt;/p&gt; &lt;p&gt;differential_learning_rate_layers: []&lt;/p&gt; &lt;p&gt;drop_last_batch: true&lt;/p&gt; &lt;p&gt;epochs: 1&lt;/p&gt; &lt;p&gt;evaluate_before_training: false&lt;/p&gt; &lt;p&gt;evaluation_epochs: 1.0&lt;/p&gt; &lt;p&gt;freeze_layers: []&lt;/p&gt; &lt;p&gt;grad_accumulation: 8&lt;/p&gt; &lt;p&gt;gradient_clip: 0.0&lt;/p&gt; &lt;p&gt;learning_rate: 0.0002&lt;/p&gt; &lt;p&gt;lora: true&lt;/p&gt; &lt;p&gt;lora_alpha: 32&lt;/p&gt; &lt;p&gt;lora_dropout: 0.05&lt;/p&gt; &lt;p&gt;lora_r: 32&lt;/p&gt; &lt;p&gt;lora_target_modules: q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj&lt;/p&gt; &lt;p&gt;lora_unfreeze_layers: []&lt;/p&gt; &lt;p&gt;loss_function: TokenAveragedCrossEntropy&lt;/p&gt; &lt;p&gt;min_learning_rate_ratio: 0.0&lt;/p&gt; &lt;p&gt;optimizer: AdamW&lt;/p&gt; &lt;p&gt;save_checkpoint: last&lt;/p&gt; &lt;p&gt;schedule: Cosine&lt;/p&gt; &lt;p&gt;train_validation_data: false&lt;/p&gt; &lt;p&gt;use_dora: false&lt;/p&gt; &lt;p&gt;use_rslora: false&lt;/p&gt; &lt;p&gt;warmup_epochs: 0.05&lt;/p&gt; &lt;p&gt;weight_decay: 0.05&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Psychological_Ad8426"&gt; /u/Psychological_Ad8426 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1na61bk/struggling_with_gptoss20b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1na61bk/struggling_with_gptoss20b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1na61bk/struggling_with_gptoss20b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-06T17:53:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1n9zrvc</id>
    <title>How do you make 3+ GPUs stable?!</title>
    <updated>2025-09-06T13:37:33+00:00</updated>
    <author>
      <name>/u/anothy1</name>
      <uri>https://old.reddit.com/user/anothy1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just got my third 3090 and the setup from 2 to 3 GPUs was a PITA as I had to now use a mining frame with these pcie x16 risers (&lt;a href="https://www.amazon.ca/dp/B0C4171HKX"&gt;https://www.amazon.ca/dp/B0C4171HKX&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;Problem is I've been dealing with constant issues of crashes and instability. For example I've been trying to preprocess datasets over night just to wake up to these messages and my system hanging:&lt;/p&gt; &lt;p&gt;&lt;code&gt;GPU 00000000:01:00.0: GPU Unavailable error occurred&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;GPU 00000000:05:00.0: GPU Recovery action event occurred&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;GPU 00000000:01:00.0: Detected Critical Xid Error&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Journalctl also shows a lot of these&lt;/p&gt; &lt;p&gt;&lt;code&gt;Sep 06 11:43:45 ml-lab1 kernel: pcieport 0000:00:01.0: PCIe Bus Error: severity=Corrected, type=Data Link Layer, (Transmitter ID)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Sep 06 11:43:45 ml-lab1 kernel: pcieport 0000:00:01.0: device [8086:a70d] error status/mask=00001000/00002000&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Sep 06 11:43:45 ml-lab1 kernel: pcieport 0000:00:01.0: [12] Timeout&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Judging from this it's most likely the risers. I do hope there's some kind of magic setting in the BIOS I'm missing that someone could point out (so far the only thing I set was above 4g decoding and force pcie gen 3) but if not I would greatly appreciate recommendations for better risers&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/anothy1"&gt; /u/anothy1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9zrvc/how_do_you_make_3_gpus_stable/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9zrvc/how_do_you_make_3_gpus_stable/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n9zrvc/how_do_you_make_3_gpus_stable/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-06T13:37:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1na0yp7</id>
    <title>Strix Halo on Ubuntu looks great - Netstatz</title>
    <updated>2025-09-06T14:29:10+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1na0yp7/strix_halo_on_ubuntu_looks_great_netstatz/"&gt; &lt;img alt="Strix Halo on Ubuntu looks great - Netstatz" src="https://external-preview.redd.it/NJZpfJ99mI8JFoVvYOBKMFZAOXMI0D90oeAZs96TxjQ.jpeg?width=216&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8bfa184dd47aead2b361177ffeb7754292954b27" title="Strix Halo on Ubuntu looks great - Netstatz" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Not the author, just sharing an article written by a GitHub contributor. I appreciate that it’s an end to end tutorial with code that includes all the problems/challenges!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://netstatz.com/strix_halo_lemonade/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1na0yp7/strix_halo_on_ubuntu_looks_great_netstatz/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1na0yp7/strix_halo_on_ubuntu_looks_great_netstatz/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-06T14:29:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1na2auz</id>
    <title>Has anyone here had any experience ordering from Tenstorrent or dealing with their customer service?</title>
    <updated>2025-09-06T15:24:01+00:00</updated>
    <author>
      <name>/u/elephantgif</name>
      <uri>https://old.reddit.com/user/elephantgif</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm a fan of Jim Keller, and love the mission behind his product, but my Blackhole cards have been stuck in customs for over a week, pending paperwork that was never sent to the carrier. Despite reaching out to them multiple times, I have yet to get any response. After digging, I found their phone number only to call and discover the voice mail hasn't even been set up. This whole experience has been disappointing and has led me to question even ordering these cards to begin with. I have never experience such a total lack of customer service, and am very frustrated at this point.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/elephantgif"&gt; /u/elephantgif &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1na2auz/has_anyone_here_had_any_experience_ordering_from/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1na2auz/has_anyone_here_had_any_experience_ordering_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1na2auz/has_anyone_here_had_any_experience_ordering_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-06T15:24:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1na7c1b</id>
    <title>OpenAI: Why Language Models Hallucinate</title>
    <updated>2025-09-06T18:44:12+00:00</updated>
    <author>
      <name>/u/onil_gova</name>
      <uri>https://old.reddit.com/user/onil_gova</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In short: LLMs hallucinate because we've inadvertently designed the training and evaluation process to reward confident, even if incorrect, answers, rather than honest admissions of uncertainty. Fixing this requires a shift in how we grade these systems to steer them towards more trustworthy behavior. &lt;/p&gt; &lt;p&gt;The Solution:&lt;/p&gt; &lt;p&gt;Explicitly stating &amp;quot;confidence targets&amp;quot; in evaluation instructions, where mistakes are penalized and admitting uncertainty (IDK) might receive 0 points, but guessing incorrectly receives a negative score. This encourages &amp;quot;behavioral calibration,&amp;quot; where the model only answers if it's sufficiently confident.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/onil_gova"&gt; /u/onil_gova &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://share.google/9SKn7X0YThlmnkZ9m"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1na7c1b/openai_why_language_models_hallucinate/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1na7c1b/openai_why_language_models_hallucinate/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-06T18:44:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1n9ong4</id>
    <title>Kimi K2 0905 is a beast at coding</title>
    <updated>2025-09-06T02:56:29+00:00</updated>
    <author>
      <name>/u/adumdumonreddit</name>
      <uri>https://old.reddit.com/user/adumdumonreddit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I've been working on this static website, just a side project where I can do some blogging or some fun javascript experiments, but I've been making this new component, basically implementing custom scrolling and pagination behaviours from scratch.&lt;/p&gt; &lt;p&gt;Anyways, I was facing a bunch of tough bugs, in complete deadlock, even tried asking Deepseek/Gemini/even went for one response from Opus, no luck. Then, decided to try the new Kimi, and bam. One try, instantly solved the issue, and did it with some tastefully commented (think somewhere between Gemini and Qwen levels of comment-ness) and good-practice code.&lt;/p&gt; &lt;p&gt;I was impressed, so I decided to just toss in my entire CSS/HTML skeleton as well as a fuck it, and when it was done, the result was so much prettier than the one I had originally. Damn, I thought, so I decided to toss it a few more problems: implement dark mode handling for the entire skeleton using only CSS and a js button, and implement another style hotswapping feature I had been thinking of.&lt;/p&gt; &lt;p&gt;Five minutes, and they both were done flawlessly.&lt;/p&gt; &lt;p&gt;I'm no javascript wiz, so I imagine all of that would probably have taken me around another two or three hours. With Kimi, I did it in like 10 minutes. What's more is that it cracked bugs that even the previous SOTA models, my go-tos, couldn't do. The consistency is also impressive: all of it was in one try, maybe two if I wanted to clarify my requirements, and all of it was well formatted, had a nice level of comments (I don't know how to explain this one, the comments were just 'good' in a way Gemini comments aren't, for example)&lt;/p&gt; &lt;p&gt;Wow. I'm impressed.&lt;/p&gt; &lt;p&gt;(Sorry, no images; the website is publicly accessible and linked to my real name, so I'd prefer not to link it to this account in any way.)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/adumdumonreddit"&gt; /u/adumdumonreddit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9ong4/kimi_k2_0905_is_a_beast_at_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9ong4/kimi_k2_0905_is_a_beast_at_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n9ong4/kimi_k2_0905_is_a_beast_at_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-06T02:56:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1n9ubmn</id>
    <title>MINISFORUM MS-S1 Max AI PC features AMD Strix Halo, 80 Gbps USB, 10 Gb LAN, and PCie x16 - Liliputing</title>
    <updated>2025-09-06T08:28:13+00:00</updated>
    <author>
      <name>/u/NewtMurky</name>
      <uri>https://old.reddit.com/user/NewtMurky</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9ubmn/minisforum_mss1_max_ai_pc_features_amd_strix_halo/"&gt; &lt;img alt="MINISFORUM MS-S1 Max AI PC features AMD Strix Halo, 80 Gbps USB, 10 Gb LAN, and PCie x16 - Liliputing" src="https://external-preview.redd.it/QuRrltSklV9MMNc9P3Jo_YeEcOYyvyeX46KjOI0goqs.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b4e531500df8dc2e276ea41601eff7d38db6a0af" title="MINISFORUM MS-S1 Max AI PC features AMD Strix Halo, 80 Gbps USB, 10 Gb LAN, and PCie x16 - Liliputing" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;AMD Ryzen AI Max+ 395 processor, 128GB of LPDDR5x-8000 quad-channel memory with 256GB/s bandwidth, and the ability to run large large language models with over 100 billion parameters locally. And, it has pretty good connectivity options: 80 Gbps USB, 10 Gb LAN, and PCie x16. &lt;/p&gt; &lt;p&gt;For comparison, the Framework Desktop has PCIe x4 only.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NewtMurky"&gt; /u/NewtMurky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://liliputing.com/minisforum-ms-s1-max-ai-pc-features-amd-strix-halo-80-gbps-usb-10-gb-lan-and-pcie-x16/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9ubmn/minisforum_mss1_max_ai_pc_features_amd_strix_halo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n9ubmn/minisforum_mss1_max_ai_pc_features_amd_strix_halo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-06T08:28:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1n9o4em</id>
    <title>ROG Ally X with RTX 6000 Pro Blackwell Max-Q as Makeshift LLM Workstation</title>
    <updated>2025-09-06T02:29:21+00:00</updated>
    <author>
      <name>/u/susmitds</name>
      <uri>https://old.reddit.com/user/susmitds</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9o4em/rog_ally_x_with_rtx_6000_pro_blackwell_maxq_as/"&gt; &lt;img alt="ROG Ally X with RTX 6000 Pro Blackwell Max-Q as Makeshift LLM Workstation" src="https://b.thumbs.redditmedia.com/JhYB2Z_qsB-pJqq4qeo2qmH8su0aVxk10f3FRgO-6QE.jpg" title="ROG Ally X with RTX 6000 Pro Blackwell Max-Q as Makeshift LLM Workstation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So my workstation motherboard stopped working and needed to be sent for replacement in warranty. Leaving my research work and LLM workflow screwed.&lt;/p&gt; &lt;p&gt;Off a random idea stuck one of my RTX 6000 Blackwell into a EGPU enclosure (Aoostar AG02) and tried it on my travel device, the ROG Ally X and it kinda blew my mind on how good this makeshift temporary setup was working. Never thought I would using my Ally for hosting 235B parameter LLM models, yet with the GPU, I was getting very good performance at 1100+ tokens/sec prefill, 25+ tokens/sec decode on Qwen3-235B-A22B-Instruct-2507 with 180K context using a custom quant I made in ik-llama.cpp (attention projections, embeddings, lm_head at q8_0, expert up/gate at iq2_kt, down at iq3_kt, total 75 GB size). Also tested GLM 4.5 Air with unsloth's Q4_K_XL, could easily run with full 128k context. I am perplexed how good the models are all running even at PCIE 4 x 4 on a eGPU.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/susmitds"&gt; /u/susmitds &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1n9o4em"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9o4em/rog_ally_x_with_rtx_6000_pro_blackwell_maxq_as/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n9o4em/rog_ally_x_with_rtx_6000_pro_blackwell_maxq_as/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-06T02:29:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1na5kmg</id>
    <title>Baseten raises $150M Series D for inference infra but where’s the real bottleneck?</title>
    <updated>2025-09-06T17:34:56+00:00</updated>
    <author>
      <name>/u/pmv143</name>
      <uri>https://old.reddit.com/user/pmv143</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Baseten just raised $150M Series D at a $2.1B valuation. They focus on inference infra like low latency serving, throughput optimization, developer experience.&lt;/p&gt; &lt;p&gt;They’ve shared benchmarks showing their embeddings inference outperforms vLLM and TEI, especially on throughput and latency. The bet is that inference infra is the pain point, not training.&lt;/p&gt; &lt;p&gt;But this raises a bigger question. what’s the real bottleneck in inference? •Baseten and others (Fireworks, Together) are competing on latency + throughput. •Some argue the bigger cost sink is cold starts and low GPU utilization , serving multiple models elastically without waste is still unsolved at scale.&lt;/p&gt; &lt;p&gt;I wonder what everyone thinks…&lt;/p&gt; &lt;p&gt;Will latency/throughput optimizations be enough to differentiate?&lt;/p&gt; &lt;p&gt;Or is utilization (how efficiently GPUs are used across workloads) the deeper bottleneck?&lt;/p&gt; &lt;p&gt;Does inference infra end up commoditized like training infra, or is there still room for defensible platforms?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmv143"&gt; /u/pmv143 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1na5kmg/baseten_raises_150m_series_d_for_inference_infra/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1na5kmg/baseten_raises_150m_series_d_for_inference_infra/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1na5kmg/baseten_raises_150m_series_d_for_inference_infra/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-06T17:34:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1na4vx4</id>
    <title>[Follow-up] A deep dive into my solo-dev narrative game engine, Synthasia (Multi-LLM &amp; Local-First!)</title>
    <updated>2025-09-06T17:07:54+00:00</updated>
    <author>
      <name>/u/orblabs</name>
      <uri>https://old.reddit.com/user/orblabs</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1na4vx4/followup_a_deep_dive_into_my_solodev_narrative/"&gt; &lt;img alt="[Follow-up] A deep dive into my solo-dev narrative game engine, Synthasia (Multi-LLM &amp;amp; Local-First!)" src="https://b.thumbs.redditmedia.com/KWUqCrK1aXeTIM-23Jp0XkZ6qFLtLUme4JdNmMgTNXQ.jpg" title="[Follow-up] A deep dive into my solo-dev narrative game engine, Synthasia (Multi-LLM &amp;amp; Local-First!)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;First off, a massive thank you to this community. Last week, I posted a few teaser images of a project I've been pouring my heart into, and the interest and kind words were genuinely motivating. As promised, now that I've hammered out a few more systems and squashed some particularly nasty bugs, I'm ready to properly introduce you to &lt;strong&gt;Synthasia&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;This is my solo-dev passion project, a journey to build an engine that allows for truly living narratives.&lt;/p&gt; &lt;h1&gt;The Ethos: Back to the Future of Gameplay&lt;/h1&gt; &lt;p&gt;I genuinely feel like we're on the verge of a new era for gameplay. To understand where we're going, I felt we had to go back to where it all began: text adventures. Those games were powered not by GPUs, but by the single most powerful graphics processor in existence: the human imagination. My goal with Synthasia is to use the very latest in AI to recapture that feeling of boundless narrative freedom.&lt;/p&gt; &lt;h1&gt;Delivering a Story as a Game&lt;/h1&gt; &lt;p&gt;At its core, Synthasia is an engine designed to deliver a story as a game, complete with light RPG mechanics, inventory, and stats. It gives creators the power to decide the balance between pre-written lore and AI-driven dynamism. You can define every detail of your world, or you can provide a high-level premise and let the AI take over, enriching characters, describing locations, and reacting to the player in ways you never planned for.&lt;/p&gt; &lt;p&gt;I have to be honest, the first time I saw an NPC I'd created get genuinely convinced through unscripted dialogue to hand over a keycard—a real, game-state-altering action—it was pure magic. It's that feeling I'm chasing.&lt;/p&gt; &lt;h1&gt;The Tech Stack (The Fun Part!)&lt;/h1&gt; &lt;p&gt;I know this is what you're all here for! The entire engine is built with a local-first philosophy and a flexible, multi-LLM design.&lt;/p&gt; &lt;h1&gt;1. The Multi-LLM Design: A &amp;quot;Creative Director&amp;quot; and a &amp;quot;Stage Manager&amp;quot;&lt;/h1&gt; &lt;p&gt;Instead of relying on a single model, Synthasia orchestrates multiple LLMs, each with a specialized role.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;The Primary LLM (The &amp;quot;Creative Director&amp;quot;):&lt;/strong&gt; This is the powerhouse for heavy, creative lifting: generating rich, atmospheric location descriptions, writing complex and nuanced NPC dialogue, and enriching the world with lore. For this role, bigger is often better for generating richer detail, but I've found that even the latest &lt;strong&gt;4B parameter models are incredibly promising&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The Secondary LLM (The &amp;quot;Stage Manager&amp;quot;):&lt;/strong&gt; This is where the local-first approach becomes incredible. The &amp;quot;Stage Manager&amp;quot; handles all the smaller, faster, high-frequency tasks where speed is critical. And here's the part that blew my mind: I'm having huge success running a tiny, blazing-fast &lt;strong&gt;1.2B model (liquid/lfm2-1.2b) locally via Ollama&lt;/strong&gt; for this. It's responsible for: &lt;ul&gt; &lt;li&gt;Summarizing conversations for an NPC's memory.&lt;/li&gt; &lt;li&gt;Generating quick, atmospheric descriptions for player actions (e.g., picking up an item).&lt;/li&gt; &lt;li&gt;Transforming conversational player input (&amp;quot;tell me more&amp;quot;) into clean queries for the RAG system.&lt;/li&gt; &lt;li&gt;Handle some game world state changes and events&lt;/li&gt; &lt;li&gt;Process combat turns&lt;/li&gt; &lt;li&gt;Extracting &amp;quot;emotions&amp;quot; from conversations so to evaluate eventual relationship improvements or worsening between NPCs and the player&lt;/li&gt; &lt;li&gt;More... &lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This design allows for a super-responsive experience while keeping costs and privacy in check. We can even add more specialized models later for different tasks.&lt;/p&gt; &lt;h1&gt;2. The RAG System: Giving the World a Memory&lt;/h1&gt; &lt;p&gt;Context windows are the final boss. My solution is a multi-stage RAG pipeline that acts as the world's memory. Before hitting the vector database with a conversational query, the local &amp;quot;Stage Manager&amp;quot; LLM rewrites it into a perfect, standalone question. This makes retrieval incredibly accurate. The RAG system also uses separate vector stores for global world knowledge and private NPC memories, so characters only know what they've personally experienced or been told.&lt;/p&gt; &lt;h1&gt;3. Game State &amp;amp; User Agency&lt;/h1&gt; &lt;p&gt;The game state is managed centrally. Before any major world-altering AI call, a complete snapshot is taken. If the AI call fails, or if the player just doesn't like the response, they can hit a &amp;quot;Regenerate&amp;quot; button. This restores the snapshot and re-runs the query, giving the user real agency over their narrative experience.&lt;/p&gt; &lt;h1&gt;Infinite Worlds, One Engine&lt;/h1&gt; &lt;p&gt;Because the core systems are genre-agnostic, Synthasia can power vastly different experiences with equal fidelity—from interactive tales for little kids to deep, complex fantasy worlds or hard sci-fi mysteries.&lt;/p&gt; &lt;h1&gt;The Road Ahead &amp;amp; A Call for Testers!&lt;/h1&gt; &lt;p&gt;This has been an incredible journey, but I know that a project like this thrives with a community. To that end, I've just set up a Discord server to gather feedback, share progress, and hopefully build a group of people excited to help shape the future of this engine.&lt;/p&gt; &lt;p&gt;We're getting very close to having something ready for an early alpha build. If you're interested in being one of the first to test it out, mess with the LLM settings, and see what kind of stories you can create, please feel free to &lt;strong&gt;DM me here on Reddit or, even better, join the Discord!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Discord Link:&lt;/strong&gt; &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fdiscord.gg%2F2wc4n2GMmn"&gt;https://discord.gg/2wc4n2GMmn&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks so much for your time and for being such an awesome and inspiring community. I can't wait to hear what you think.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/orblabs"&gt; /u/orblabs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1na4vx4"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1na4vx4/followup_a_deep_dive_into_my_solodev_narrative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1na4vx4/followup_a_deep_dive_into_my_solodev_narrative/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-06T17:07:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1n9gfpt</id>
    <title>Anthropic to pay $1.5 billion to authors in landmark AI settlement</title>
    <updated>2025-09-05T20:41:52+00:00</updated>
    <author>
      <name>/u/cpldcpu</name>
      <uri>https://old.reddit.com/user/cpldcpu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9gfpt/anthropic_to_pay_15_billion_to_authors_in/"&gt; &lt;img alt="Anthropic to pay $1.5 billion to authors in landmark AI settlement" src="https://external-preview.redd.it/2giFHQHB-5T6ma6XiIR2StAHVaV1z6nAKhfbARNarkE.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d341db7b1d508270a9cf44051e58699980b97ccb" title="Anthropic to pay $1.5 billion to authors in landmark AI settlement" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cpldcpu"&gt; /u/cpldcpu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.theverge.com/anthropic/773087/anthropic-to-pay-1-5-billion-to-authors-in-landmark-ai-settlement"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9gfpt/anthropic_to_pay_15_billion_to_authors_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n9gfpt/anthropic_to_pay_15_billion_to_authors_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-05T20:41:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1na3xkd</id>
    <title>Llama-3.3-Nemotron-Super-49B-v1.5 is very good model to summarized long text into formatted markdown (Nvidia also provided free unlimited API call with rate limit)</title>
    <updated>2025-09-06T16:29:19+00:00</updated>
    <author>
      <name>/u/dheetoo</name>
      <uri>https://old.reddit.com/user/dheetoo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been working on a project to convert medical lesson data from websites into markdown format for a RAG application. Tested several popular models including Qwen3 235B, Gemma 3 27B, and GPT-oss-120 they all performed well technically, but as someone with a medical background, the output style just didn't click with me (totally subjective, I know).&lt;/p&gt; &lt;p&gt;So I decided to experiment with some models on NVIDIA's API platform and stumbled upon &lt;strong&gt;Llama-3.3-Nemotron-Super-49B-v1.5&lt;/strong&gt; This thing is surprisingly solid for my use case. I'd tried it before in an agent setup where it didn't perform great on evals, so I had to stick with the bigger models. But for this specific summarization task, it's been excellent.&lt;/p&gt; &lt;p&gt;The output is well-written, requires minimal proofreading, and the markdown formatting is clean right out of the box. Plus it's free through NVIDIA's API (40 requests/minute limit), which is perfect for my workflow since I manually review everything anyway.&lt;/p&gt; &lt;p&gt;Definitely worth trying if you're doing similar work with medical or technical content, write a good prompt still the key though.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dheetoo"&gt; /u/dheetoo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1na3xkd/llama33nemotronsuper49bv15_is_very_good_model_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1na3xkd/llama33nemotronsuper49bv15_is_very_good_model_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1na3xkd/llama33nemotronsuper49bv15_is_very_good_model_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-06T16:29:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1na2l5b</id>
    <title>Kimi K2 0905 Official Pricing (generation, tool)</title>
    <updated>2025-09-06T15:35:30+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1na2l5b/kimi_k2_0905_official_pricing_generation_tool/"&gt; &lt;img alt="Kimi K2 0905 Official Pricing (generation, tool)" src="https://b.thumbs.redditmedia.com/5S8zZlvd7mg8FDiVJJaX0ZhoACaHNn7i2haHT6-515Y.jpg" title="Kimi K2 0905 Official Pricing (generation, tool)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Quite cheap for a model this big! Consider using the official API instead of Openrouter, it directly supports the model builders (PS: I looked for &amp;quot;non-local&amp;quot; flair and couldn't find it).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1na2l5b"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1na2l5b/kimi_k2_0905_official_pricing_generation_tool/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1na2l5b/kimi_k2_0905_official_pricing_generation_tool/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-06T15:35:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1na3u8w</id>
    <title>Built QWEN3-0.6B mini inference engine in CUDA from scratch</title>
    <updated>2025-09-06T16:25:42+00:00</updated>
    <author>
      <name>/u/yassa9</name>
      <uri>https://old.reddit.com/user/yassa9</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1na3u8w/built_qwen306b_mini_inference_engine_in_cuda_from/"&gt; &lt;img alt="Built QWEN3-0.6B mini inference engine in CUDA from scratch" src="https://external-preview.redd.it/ZXpiNW9wenhra25mMbxpIYt75C5r2fT6cxhEXwgg3zD3iMqrP6b9J5eZI0o1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e8266ac29eb4dbc6dde7ee5d798cb3890af08d38" title="Built QWEN3-0.6B mini inference engine in CUDA from scratch" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm into CUDA and GPGPU programming much, didn't get into LLMs or NLP at all, so tried build that side project as as a hands-on way to learn about LLMs while practicing my CUDA programming.&lt;/p&gt; &lt;p&gt;chose that cute tiny model of qwen3-600m &lt;/p&gt; &lt;p&gt;Static configured, with suckless philosophy in code as much as possible, no deps to build beyond cuBLAS, CUB, std IO libs&lt;/p&gt; &lt;p&gt;I know that im missing smth but in benchmarking with greedy sampling (temp=0) on my RTX 3050, I get 3x speed of hf with flash-attn inference and extremely comparable speed with llama.cpp&lt;/p&gt; &lt;p&gt;My guess is the slight edge over llama.cpp comes from being hyper-specialized for just one model, allowing for more compile-time optimizations with no runtime branching.&lt;/p&gt; &lt;p&gt;feel free to check github if you want: &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/yassa9/qwen600"&gt;https://github.com/yassa9/qwen600&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yassa9"&gt; /u/yassa9 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/xh5qjozxkknf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1na3u8w/built_qwen306b_mini_inference_engine_in_cuda_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1na3u8w/built_qwen306b_mini_inference_engine_in_cuda_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-06T16:25:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1na0mw3</id>
    <title>Qwen3 30B A3B Hits 13 token/s on 4x Raspberry Pi 5</title>
    <updated>2025-09-06T14:15:15+00:00</updated>
    <author>
      <name>/u/vibjelo</name>
      <uri>https://old.reddit.com/user/vibjelo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1na0mw3/qwen3_30b_a3b_hits_13_tokens_on_4x_raspberry_pi_5/"&gt; &lt;img alt="Qwen3 30B A3B Hits 13 token/s on 4x Raspberry Pi 5" src="https://external-preview.redd.it/KUWKhlT5OZYpzmuPdkrY6FyowQ4PaYe23RiUvraDVrQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4733d277f6f6e759fe47794087d1da790f8d36b7" title="Qwen3 30B A3B Hits 13 token/s on 4x Raspberry Pi 5" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vibjelo"&gt; /u/vibjelo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/b4rtaz/distributed-llama/discussions/255"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1na0mw3/qwen3_30b_a3b_hits_13_tokens_on_4x_raspberry_pi_5/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1na0mw3/qwen3_30b_a3b_hits_13_tokens_on_4x_raspberry_pi_5/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-06T14:15:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1n9x1ho</id>
    <title>So I tried Qwen 3 Max skills for programming</title>
    <updated>2025-09-06T11:19:58+00:00</updated>
    <author>
      <name>/u/TruckUseful4423</name>
      <uri>https://old.reddit.com/user/TruckUseful4423</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9x1ho/so_i_tried_qwen_3_max_skills_for_programming/"&gt; &lt;img alt="So I tried Qwen 3 Max skills for programming" src="https://external-preview.redd.it/_Qhoi5tM5uwwG8h9pFbHe7_wEttk4KG4M_-539ZjdPE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7d6c7a702febc40739857dbd7082314de38697a5" title="So I tried Qwen 3 Max skills for programming" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;So I Tried Qwen 3 Max for Programming — Project VMP (Visualized Music Player)&lt;/h1&gt; &lt;p&gt;I wanted to see how far Qwen 3 Max could go when tasked with building a full project from a very detailed specification. The result: VMP — Visualized Music Player, a cyberpunk-style music player with FFT-based visualizations, crossfade playback, threading, and even a web terminal.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Prompt&lt;/strong&gt;&lt;/p&gt; &lt;h1&gt;Tech Stack &amp;amp; Dependencies&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Python 3.11&lt;/li&gt; &lt;li&gt;pygame, numpy, mutagen, pydub, websockets&lt;/li&gt; &lt;li&gt;Requires FFmpeg in PATH&lt;/li&gt; &lt;li&gt;Runs with a simple BAT file on Windows&lt;/li&gt; &lt;li&gt;SDL hints set for Windows: &lt;ul&gt; &lt;li&gt;SDL_RENDER_DRIVER=direct3d&lt;/li&gt; &lt;li&gt;SDL_HINT_RENDER_SCALE_QUALITY=1&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Core Features&lt;/h1&gt; &lt;h1&gt;Configuration&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;AudioCfg, VisualCfg, UiCfg dataclasses with sane defaults&lt;/li&gt; &lt;li&gt;Global instances: AUDIO, VIS, UI&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Logging&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Custom logger vmp with console + rotating file handler&lt;/li&gt; &lt;li&gt;Optional WebTermHandler streams logs to connected websocket clients&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;FFmpeg Integration&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Automatic FFmpeg availability check&lt;/li&gt; &lt;li&gt;On-demand decode with ffmpeg -ss ... -t ... into raw PCM&lt;/li&gt; &lt;li&gt;Reliable seeking via decoded segments&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Music Library&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Recursive scan for .mp3, .wav, .flac, .ogg, .m4a&lt;/li&gt; &lt;li&gt;Metadata via mutagen (fallback to smart filename guessing)&lt;/li&gt; &lt;li&gt;Sortable, with directory ignore list&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;DSP &amp;amp; Analysis&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Stereo EQ (low shelf, peaking, high shelf) + softclip limiter&lt;/li&gt; &lt;li&gt;FFT analysis with Hann windows, band mapping, adaptive beat detection&lt;/li&gt; &lt;li&gt;Analysis LRU cache (capacity 64) for performance&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Visualization&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Cyberpunk ring with dotted ticks, glow halos, progress arc&lt;/li&gt; &lt;li&gt;Outward 64-band bars + central vocal pulse disc&lt;/li&gt; &lt;li&gt;Smooth envelopes, beat halos, ~60% transparent overlays&lt;/li&gt; &lt;li&gt;Fonts: cyberpunk.ttf if present, otherwise Segoe/Arial&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Playback Model&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;pygame.mixer at 44.1 kHz stereo&lt;/li&gt; &lt;li&gt;Dual-channel system for precise seeking and crossfade overlap&lt;/li&gt; &lt;li&gt;Smooth cosine crossfade without freezing visuals&lt;/li&gt; &lt;li&gt;Modes: &lt;ul&gt; &lt;li&gt;Music = standard streaming&lt;/li&gt; &lt;li&gt;Channel = decoded segment playback (reliable seek)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Window &amp;amp; UI&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Resizable window, optional fake fullscreen&lt;/li&gt; &lt;li&gt;Backgrounds with dark overlay, cache per resolution&lt;/li&gt; &lt;li&gt;Topmost toggle, drag-window mode (Windows)&lt;/li&gt; &lt;li&gt;Presets for HUD/FPS/TIME/TITLE (keys 1–5, V, F2)&lt;/li&gt; &lt;li&gt;Help overlay (H) shows all controls&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Controls&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Playback: Space pause/resume, N/P next/prev, S shuffle, R repeat-all&lt;/li&gt; &lt;li&gt;Seek: ←/→ −5s / +5s&lt;/li&gt; &lt;li&gt;Window/UI: F fake fullscreen, T topmost, B toggle backgrounds, [/] prev/next BG&lt;/li&gt; &lt;li&gt;Volume: Mouse wheel; volume display fades quickly&lt;/li&gt; &lt;li&gt;Quit: Esc / Q&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Web Terminal&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Optional --webterm flag&lt;/li&gt; &lt;li&gt;Websocket server on ws://localhost:3030&lt;/li&gt; &lt;li&gt;Streams logs + accepts remote commands (n, p, space, etc.)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Performance&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Low-CPU visualization mode (--viz-lowcpu)&lt;/li&gt; &lt;li&gt;Heavy operations skipped while paused&lt;/li&gt; &lt;li&gt;Preallocated NumPy buffers &amp;amp; surface caches&lt;/li&gt; &lt;li&gt;Threaded FFT + loader workers, priority queue for analysis&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;CLI Options&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;--music-dir Path to your music library --backgrounds Path to background images --debug Verbose logging --shuffle Enable shuffle mode --repeat-all Repeat entire playlist --no-fft Disable FFT --viz-lowcpu Low CPU visualization --ext File extensions to include --ignore Ignore directories --no-tags Skip metadata tags --webterm Enable websocket terminal &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Results&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Crossfade works seamlessly, with no visual freeze&lt;/li&gt; &lt;li&gt;Seek is reliable thanks to FFmpeg segment decoding&lt;/li&gt; &lt;li&gt;Visualizations scale cleanly across windowed and fake-fullscreen modes&lt;/li&gt; &lt;li&gt;Handles unknown tags gracefully by guessing titles from filenames&lt;/li&gt; &lt;li&gt;Everything runs as a single script, no external modules beyond listed deps&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;👉 Full repo: &lt;a href="https://github.com/feckom/vmp"&gt;github.com/feckom/vmp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Results&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/wixd9wdhzinf1.jpg?width=1282&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=6b1a18941410cb3a7f4b0da54f36003298180dca"&gt;https://preview.redd.it/wixd9wdhzinf1.jpg?width=1282&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=6b1a18941410cb3a7f4b0da54f36003298180dca&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/m6chuvdhzinf1.jpg?width=1282&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=0c0df79e54b59b2ab064e4f7c791bb7984297a8b"&gt;https://preview.redd.it/m6chuvdhzinf1.jpg?width=1282&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=0c0df79e54b59b2ab064e4f7c791bb7984297a8b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/bma8vwdhzinf1.jpg?width=1282&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=bfe32593e27d63fd9e533c6202979bc9da6d8330"&gt;https://preview.redd.it/bma8vwdhzinf1.jpg?width=1282&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=bfe32593e27d63fd9e533c6202979bc9da6d8330&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TruckUseful4423"&gt; /u/TruckUseful4423 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9x1ho/so_i_tried_qwen_3_max_skills_for_programming/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9x1ho/so_i_tried_qwen_3_max_skills_for_programming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n9x1ho/so_i_tried_qwen_3_max_skills_for_programming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-06T11:19:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1na3f1s</id>
    <title>Renting GPUs is hilariously cheap</title>
    <updated>2025-09-06T16:08:44+00:00</updated>
    <author>
      <name>/u/-p-e-w-</name>
      <uri>https://old.reddit.com/user/-p-e-w-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1na3f1s/renting_gpus_is_hilariously_cheap/"&gt; &lt;img alt="Renting GPUs is hilariously cheap" src="https://preview.redd.it/dhtzimf7jknf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1bca94832d9e6b8fb7b8faf80d61387d12889d7f" title="Renting GPUs is hilariously cheap" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A 140 GB monster GPU that costs $30k to buy, plus the rest of the system, plus electricity, plus maintenance, plus a multi-Gbps uplink, for a little over 2 bucks per hour.&lt;/p&gt; &lt;p&gt;If you use it for 5 hours per day, 7 days per week, and factor in auxiliary costs and interest rates, buying that GPU today vs. renting it when you need it will only pay off in 2035 or later. That’s a tough sell.&lt;/p&gt; &lt;p&gt;Owning a GPU is great for privacy and control, and obviously, many people who have such GPUs run them nearly around the clock, but for quick experiments, renting is often the best option.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-p-e-w-"&gt; /u/-p-e-w- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/dhtzimf7jknf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1na3f1s/renting_gpus_is_hilariously_cheap/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1na3f1s/renting_gpus_is_hilariously_cheap/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-06T16:08:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7j5z2</id>
    <title>Our 2nd AMA: Hugging Face Science Team, Creators of SmolLM, SmolVLM, and more! (Tomorrow, 8AM-11AM PST)</title>
    <updated>2025-09-03T16:14:51+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7j5z2/our_2nd_ama_hugging_face_science_team_creators_of/"&gt; &lt;img alt="Our 2nd AMA: Hugging Face Science Team, Creators of SmolLM, SmolVLM, and more! (Tomorrow, 8AM-11AM PST)" src="https://preview.redd.it/wdx4ivdw3zmf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=876855c03867ead70389d15b60f24b91d478f835" title="Our 2nd AMA: Hugging Face Science Team, Creators of SmolLM, SmolVLM, and more! (Tomorrow, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wdx4ivdw3zmf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7j5z2/our_2nd_ama_hugging_face_science_team_creators_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7j5z2/our_2nd_ama_hugging_face_science_team_creators_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T16:14:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1n8c3l2</id>
    <title>AMA with Hugging Face Science, the team behind SmolLM, SmolVLM, Fineweb and more.</title>
    <updated>2025-09-04T14:43:01+00:00</updated>
    <author>
      <name>/u/eliebakk</name>
      <uri>https://old.reddit.com/user/eliebakk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8c3l2/ama_with_hugging_face_science_the_team_behind/"&gt; &lt;img alt="AMA with Hugging Face Science, the team behind SmolLM, SmolVLM, Fineweb and more." src="https://external-preview.redd.it/y8IJElEOEd_2568MHNUZQsP7_aRTCAzyzXUKpDJwl1Y.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4e377887ea8d7eae841499cc497b90b82aa97816" title="AMA with Hugging Face Science, the team behind SmolLM, SmolVLM, Fineweb and more." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We're super excited to do this AMA. Come ask your questions to the researchers behind &lt;strong&gt;SmolLM, SmolVLM, FineWeb&lt;/strong&gt;, and more. You can learn more about our work at &lt;a href="http://hf.co/science"&gt;hf.co/science&lt;/a&gt; 🤗&lt;/p&gt; &lt;p&gt;If you want to get started in ML, a good place is &lt;a href="https://hf.co/learn"&gt;https://hf.co/learn&lt;/a&gt;&lt;/p&gt; &lt;p&gt;To celebrate the AMA, we release a new &lt;strong&gt;FineVision&lt;/strong&gt; dataset, check it out! &lt;a href="https://huggingface.co/datasets/HuggingFaceM4/FineVision"&gt;https://huggingface.co/datasets/HuggingFaceM4/FineVision&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Our participants:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/eliebak"&gt;Elie Bakouch&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/eliebakk"&gt;u/eliebakk&lt;/a&gt; (SmolLM)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/loubnabnl"&gt;Loubna Ben Allal&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/loubnabnl"&gt;u/loubnabnl&lt;/a&gt; (SmolLM)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/nouamanetazi"&gt;Nouamane Tazi&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/Norlax"&gt;u/Norlax&lt;/a&gt;_42 (Nanotron/SmolLM)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/lvwerra"&gt;Leandro von Werra&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/lvwerra"&gt;u/lvwerra&lt;/a&gt; (Head of Research)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/edbeeching"&gt;Edward Beeching&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/edbeeching"&gt;u/edbeeching&lt;/a&gt; (Post Training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/cmpatino"&gt;Carlos Miguel Patiño&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/cmpatino"&gt;u/cmpatino&lt;/a&gt;_ (Post Training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/kashif"&gt;Kashif Rasul&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/krasul"&gt;u/krasul&lt;/a&gt; (Post Training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/lewtun"&gt;Lewis Tunstall&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/lewtun"&gt;u/lewtun&lt;/a&gt; (Post Training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/qgallouedec"&gt;Quentin Gallouédec&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/qgallouedec"&gt;u/qgallouedec&lt;/a&gt; (Post Training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/clefourrier"&gt;Clémentine Fourrier&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/clefourrier"&gt;u/clefourrier&lt;/a&gt; (Eval)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/SaylorTwift"&gt;Nathan Habib&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/HauntingMoment"&gt;u/HauntingMoment&lt;/a&gt; (Eval)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/lusxvr"&gt;Luis Wiedmann&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/luswd"&gt;u/luswd&lt;/a&gt; (Multimodal)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/andito"&gt;Andres Marafioti&lt;/a&gt;, &lt;a href="/u/futterneid"&gt;u/futterneid&lt;/a&gt; (Multimodal)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/guipenedo"&gt;Guilherme Penedo&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/PhilipsNostrum"&gt;u/PhilipsNostrum&lt;/a&gt; (Data)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/hynky"&gt;Hynek Kydlíček&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/Other"&gt;u/Other&lt;/a&gt;_Housing8453 (Data)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/reach-vb"&gt;Vaibhav Srivastav,&lt;/a&gt; &lt;a href="/u/vaibhavs10"&gt;u/vaibhavs10&lt;/a&gt; (Head of Developer Experience and Community)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/BrigitteTousi"&gt;Brigitte Tousignant&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/BriggieSmalls1992"&gt;u/BriggieSmalls1992&lt;/a&gt; (Comms)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Xenova"&gt;Xenova&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/xenovatech"&gt;u/xenovatech&lt;/a&gt; (Transformers.js)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/craffel"&gt;Colin Raffel&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/craffel"&gt;u/craffel&lt;/a&gt; (Research)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/ngxson"&gt;Xuan Son Nguyen&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/MediocreProgrammer99"&gt;u/MediocreProgrammer99&lt;/a&gt; (llama.cpp)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you are passionate about open source and open science like us, apply at &lt;a href="https://hf.co/jobs"&gt;https://hf.co/jobs&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 8 AM – 11 AM PST, with the Hugging Face team continuing to follow up on questions over the next 24 hours.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/o6moshv0u5nf1.png?width=2013&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ee6a9392c3da8651e8a1425264ed855a51b69135"&gt;https://preview.redd.it/o6moshv0u5nf1.png?width=2013&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ee6a9392c3da8651e8a1425264ed855a51b69135&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Thanks everyone for joining our AMA. The live part has ended but we will still answer question async for the next 24h. Follow our &lt;a href="https://hf.co/science"&gt;Hugging Face Science Org&lt;/a&gt; to be aware of our latest release! 🤗&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eliebakk"&gt; /u/eliebakk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8c3l2/ama_with_hugging_face_science_the_team_behind/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8c3l2/ama_with_hugging_face_science_the_team_behind/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n8c3l2/ama_with_hugging_face_science_the_team_behind/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T14:43:01+00:00</published>
  </entry>
</feed>
