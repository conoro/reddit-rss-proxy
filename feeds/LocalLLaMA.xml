<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-23T12:26:30+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1p4ayly</id>
    <title>Did a crazy speculative decoding experiment, which gave very bad results</title>
    <updated>2025-11-23T02:16:59+00:00</updated>
    <author>
      <name>/u/StomachWonderful615</name>
      <uri>https://old.reddit.com/user/StomachWonderful615</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have using Appleâ€™s mlx-lm to run my local inference for a while. I have two machines, an 8GB M2 Macbook Pro, and a 128GB M4 Macbook Studio. I usually run the bigger models like Qwen3 30b or Llama3 70b on Mac Studio and connect through API. I am also able to do speculative decoding with smaller models like Llama3 1b on Mac Studio.&lt;/p&gt; &lt;p&gt;Here are my general metrics: Llama 70b on Mac Studio - 48 tokens per sec Llama 70b target and 1b draft on Mac Studio - 55 tokens per sec Llama 1b model on Macbook Pro - 70 tokens per sec&lt;/p&gt; &lt;p&gt;I wanted to create an experimental approach of doing disaggregated speculative decoding, where draft model runs locally and target validation and rejection sampling runs on Mac Studio remotely, with draft sending draft tokens to remote server. After lot of experimentation, able to get acceptance rate to around 60%, but I am getting about 2 tokens per sec with this approach on Macbook ðŸ˜­&lt;/p&gt; &lt;p&gt;I was hoping to speed up and get good quality output, instead I am getting worse speed.&lt;/p&gt; &lt;p&gt;Is my experiment thought process wrong, or should I consider something in my implementation.&lt;/p&gt; &lt;p&gt;My original thought for this experiment - Teams can have normal sized Macbooks, able to run small models for quick generation, but validated with a bigger Model on a local server to achieve both speed and quality.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StomachWonderful615"&gt; /u/StomachWonderful615 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4ayly/did_a_crazy_speculative_decoding_experiment_which/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4ayly/did_a_crazy_speculative_decoding_experiment_which/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p4ayly/did_a_crazy_speculative_decoding_experiment_which/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T02:16:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4gttn</id>
    <title>Where is the strongest local model going to come from next?</title>
    <updated>2025-11-23T07:40:24+00:00</updated>
    <author>
      <name>/u/MrMrsPotts</name>
      <uri>https://old.reddit.com/user/MrMrsPotts</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I mean a model that clearly beats glm 4.6 and Kimi k2.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrMrsPotts"&gt; /u/MrMrsPotts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4gttn/where_is_the_strongest_local_model_going_to_come/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4gttn/where_is_the_strongest_local_model_going_to_come/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p4gttn/where_is_the_strongest_local_model_going_to_come/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T07:40:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4bqko</id>
    <title>Should local ai be used as a dungeon master?</title>
    <updated>2025-11-23T02:56:17+00:00</updated>
    <author>
      <name>/u/No_Strawberry_8719</name>
      <uri>https://old.reddit.com/user/No_Strawberry_8719</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ive heard some people have various ai be a dungeon master but does it actually work that way or should ai dm's be avoided?&lt;/p&gt; &lt;p&gt;Im very curious as i have a hard time finding trust worthy groups also what does the player setup look like on the computer/device? Have any of you tried ai dm's?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Strawberry_8719"&gt; /u/No_Strawberry_8719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4bqko/should_local_ai_be_used_as_a_dungeon_master/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4bqko/should_local_ai_be_used_as_a_dungeon_master/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p4bqko/should_local_ai_be_used_as_a_dungeon_master/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T02:56:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4evyr</id>
    <title>Most Economical Way to Run GPT-OSS-120B for ~10 Users</title>
    <updated>2025-11-23T05:42:47+00:00</updated>
    <author>
      <name>/u/theSavviestTechDude</name>
      <uri>https://old.reddit.com/user/theSavviestTechDude</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Iâ€™m planning to self-host gpt-oss-120B for about 10 concurrent users and want to figure out the most economical setup that still performs reasonably well.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/theSavviestTechDude"&gt; /u/theSavviestTechDude &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4evyr/most_economical_way_to_run_gptoss120b_for_10_users/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4evyr/most_economical_way_to_run_gptoss120b_for_10_users/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p4evyr/most_economical_way_to_run_gptoss120b_for_10_users/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T05:42:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4augy</id>
    <title>Writingway 2: An open source tool for AI-assisted writing</title>
    <updated>2025-11-23T02:11:19+00:00</updated>
    <author>
      <name>/u/Clueless_Nooblet</name>
      <uri>https://old.reddit.com/user/Clueless_Nooblet</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wrote a freeware version of sites like NovelCrafter or Sudowrite. Runs on your machine, costs zero, nothing gets saved on some obscure server, and you could even run it with a local model completely without internet access.&lt;/p&gt; &lt;p&gt;Of course FOSS.&lt;/p&gt; &lt;p&gt;Here's my blog post about it: &lt;a href="https://aomukai.com/2025/11/23/writingway-2-now-plug-and-play/"&gt;https://aomukai.com/2025/11/23/writingway-2-now-plug-and-play/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Clueless_Nooblet"&gt; /u/Clueless_Nooblet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4augy/writingway_2_an_open_source_tool_for_aiassisted/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4augy/writingway_2_an_open_source_tool_for_aiassisted/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p4augy/writingway_2_an_open_source_tool_for_aiassisted/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T02:11:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4b6ti</id>
    <title>V100 vs 5060ti vs 3090 - Some numbers</title>
    <updated>2025-11-23T02:28:10+00:00</updated>
    <author>
      <name>/u/dompazz</name>
      <uri>https://old.reddit.com/user/dompazz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi I'm new here. Ive been hosting servers on Vast for years, and finally started playing with running models locally. This site has been a great resource.&lt;/p&gt; &lt;p&gt;I've seen a couple of posts in the last few days on each of the GPUs in the title. I have machines with all of them and decided to run some benchmarks and hopefully add something back.&lt;/p&gt; &lt;p&gt;Machines:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;8x V100 SXM2 16G. This was the machine that I started on Vast with. Picked it up post ETH mining craze for dirt cheap. 2x E5-2690 v4 (56 threads) 512G RAM&lt;/li&gt; &lt;li&gt;8x 5060ti 16G. Got the board and processors from a guy in the CPU mining community. Cards are running via MCIO cables and risers - Gen 5x8. 2x EPYC 9654 (384 threads) 384G RAM&lt;/li&gt; &lt;li&gt;4x 3090, 2 NVLINK Pairs. Older processors 2x E5-2695 v3 (56 threads) 512G RAM&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So the V100 and 5060ti are about the best setup you can get with those cards. The 3090 rig could use newer hardware, they are running Gen3 PCI-E and the topology requires the pairs to cross the numa nodes to talk to each other which runs around gen3 x4 speed.&lt;/p&gt; &lt;p&gt;Speed specs put the 3090 in first place in raw compute&lt;/p&gt; &lt;ul&gt; &lt;li&gt;3090 - 35.6 TFlops FP16 (936Gb/s bandwidth)&lt;/li&gt; &lt;li&gt;V100 - 31.3 TFlops FP16 (897 Gb/s bandwidth)&lt;/li&gt; &lt;li&gt;5060ti - 23.7 TFlops FP16 (448 Gb/s bandwidth)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Worth noting the 3090 and 5060ti cards should be able to do double that TFlops, but for Nvidia nerf-ing them...&lt;/p&gt; &lt;p&gt;Ran llama-bench with llama3.1 70B Instruct Q4 model with n_gen set to 256 (ran n_prompt numbers as well but they are just silly)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;3090 - 19.09 T/s&lt;/li&gt; &lt;li&gt;V100 - 16.68 T/s&lt;/li&gt; &lt;li&gt;5060ti - 9.66 T/s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Numbers wise, the generation is roughly in line with the compute capacity (edited out badly formatted table, see comment for numbers)&lt;/p&gt; &lt;p&gt;Are there other numbers I should be running here?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dompazz"&gt; /u/dompazz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4b6ti/v100_vs_5060ti_vs_3090_some_numbers/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4b6ti/v100_vs_5060ti_vs_3090_some_numbers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p4b6ti/v100_vs_5060ti_vs_3090_some_numbers/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T02:28:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4jdwo</id>
    <title>I built my own AI Coding Agent as an Electron app, and the best part? It plugs right into regular AI chat interfaces, so I get all the power without burning through those precious token fees.</title>
    <updated>2025-11-23T10:22:04+00:00</updated>
    <author>
      <name>/u/Commercial-Gold4988</name>
      <uri>https://old.reddit.com/user/Commercial-Gold4988</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4jdwo/i_built_my_own_ai_coding_agent_as_an_electron_app/"&gt; &lt;img alt="I built my own AI Coding Agent as an Electron app, and the best part? It plugs right into regular AI chat interfaces, so I get all the power without burning through those precious token fees." src="https://external-preview.redd.it/OGI1cGI0aGJnejJnMaCPpLNF2QWthicAyURC7M2Bq9XFkDUS_tMWuFvky3Pg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7d4a6a485fcbd2a5eb23a72cc4a1230f0be52683" title="I built my own AI Coding Agent as an Electron app, and the best part? It plugs right into regular AI chat interfaces, so I get all the power without burning through those precious token fees." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Iâ€™ve been experimenting with ways to streamline my development workflow, and I finally built something Iâ€™m excited to share. I created my own &lt;strong&gt;AI Coding Agent&lt;/strong&gt; as an Electron app, designed to work directly with AI chat interfaces instead of relying on expensive API calls.&lt;/p&gt; &lt;p&gt;The result?&lt;br /&gt; A fast, flexible coding assistant that feels native, boosts productivity, and saves a &lt;em&gt;lot&lt;/em&gt; on token fees.&lt;/p&gt; &lt;p&gt;It handles file edits, diffs, context syncing, and moreâ€”without locking me into a proprietary system. Just clean integration, full control, and way fewer costs.&lt;/p&gt; &lt;p&gt;Super excited about how much this improves my daily coding flow. ðŸš€&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Commercial-Gold4988"&gt; /u/Commercial-Gold4988 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/5o7nr5hbgz2g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4jdwo/i_built_my_own_ai_coding_agent_as_an_electron_app/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p4jdwo/i_built_my_own_ai_coding_agent_as_an_electron_app/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T10:22:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4l5b8</id>
    <title>Is a fine-tuned model smaller? Will it be faster then?</title>
    <updated>2025-11-23T12:07:25+00:00</updated>
    <author>
      <name>/u/CSEliot</name>
      <uri>https://old.reddit.com/user/CSEliot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For example, fine-tuning Qwen3-Coder to only hold c++ code.&lt;/p&gt; &lt;p&gt;Apologies if it's a dumb question! I think I have a good grasp on this tech now but it's always teh problem of &amp;quot;you don't know what you don't know&amp;quot;.&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CSEliot"&gt; /u/CSEliot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4l5b8/is_a_finetuned_model_smaller_will_it_be_faster/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4l5b8/is_a_finetuned_model_smaller_will_it_be_faster/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p4l5b8/is_a_finetuned_model_smaller_will_it_be_faster/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T12:07:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1p3xv28</id>
    <title>Deep Research Agent, an autonomous research agent system</title>
    <updated>2025-11-22T16:46:04+00:00</updated>
    <author>
      <name>/u/martian7r</name>
      <uri>https://old.reddit.com/user/martian7r</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3xv28/deep_research_agent_an_autonomous_research_agent/"&gt; &lt;img alt="Deep Research Agent, an autonomous research agent system" src="https://external-preview.redd.it/a2ZpajA0cDE4dTJnMXYKxvwpmRJR_6Wuut5rPoqfAX7yC2Fpp67_z2jaY8Dw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=05d7bafd92f1750858ecd8c0b51b365c4edcb407" title="Deep Research Agent, an autonomous research agent system" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Repository: &lt;a href="https://github.com/tarun7r/deep-research-agent"&gt;https://github.com/tarun7r/deep-research-agent&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Most &amp;quot;research&amp;quot; agents just summarise the top 3 web search results. I wanted something better. I wanted an agent that could plan, verify, and synthesize information like a human analyst.&lt;/p&gt; &lt;p&gt;How it works (The Architecture): Instead of a single LLM loop, this system orchestrates four specialised agents:&lt;/p&gt; &lt;p&gt;1. The Planner: Analyzes the topic and generates a strategic research plan.&lt;/p&gt; &lt;p&gt;2. The Searcher: An autonomous agent that dynamically decides what to query and when to extract deep content.&lt;/p&gt; &lt;p&gt;3. The Synthesizer: Aggregates findings, prioritizing sources based on credibility scores.&lt;/p&gt; &lt;p&gt;4. The Writer: Drafts the final report with proper citations (APA/MLA/IEEE) and self-corrects if sections are too short.&lt;/p&gt; &lt;p&gt;The &amp;quot;Secret Sauce&amp;quot;: Credibility Scoring One of the biggest challenges with AI research is hallucinations. To solve this, I implemented an automated scoring system. It evaluates sources (0-100) based on domain authority (.edu, .gov) and academic patterns before the LLM ever summarizes them&lt;/p&gt; &lt;p&gt;Built With: Python, LangGraph &amp;amp; LangChain, Google Gemini API, Chainlit&lt;/p&gt; &lt;p&gt;Iâ€™ve attached a demo video below showing the agents in action as they tackle a complex topic from scratch.&lt;/p&gt; &lt;p&gt;Check out the code, star the repo, and contribute&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/martian7r"&gt; /u/martian7r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/tkn2fiy18u2g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3xv28/deep_research_agent_an_autonomous_research_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p3xv28/deep_research_agent_an_autonomous_research_agent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-22T16:46:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4bme5</id>
    <title>[P] Me and my uncle released a new open-source retrieval library. Full reproducibility + TREC DL 2019 benchmarks.</title>
    <updated>2025-11-23T02:50:14+00:00</updated>
    <author>
      <name>/u/Cromline</name>
      <uri>https://old.reddit.com/user/Cromline</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Over the past 8 months I have been working on a retrieval library and wanted to share if anyone is interested! It replaces ANN search and dense embeddings with full scan frequency and resonance scoring. There are few similarities to HAM (Holographic Associative Memory).&lt;/p&gt; &lt;p&gt;The repo includes an encoder, a full-scan resonance searcher, reproducible TREC DL 2019 benchmarks, a usage guide, and reported metrics.&lt;/p&gt; &lt;p&gt;MRR@10: ~.90 and Ndcg@10: ~ .75&lt;/p&gt; &lt;p&gt;Repo:&lt;br /&gt; &lt;a href="https://github.com/JLNuijens/NOS-IRv3"&gt;https://github.com/JLNuijens/NOS-IRv3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Open to questions, discussion, or critique.&lt;/p&gt; &lt;p&gt;Oops i put the [P] in there lol for the machine learning community.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cromline"&gt; /u/Cromline &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4bme5/p_me_and_my_uncle_released_a_new_opensource/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4bme5/p_me_and_my_uncle_released_a_new_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p4bme5/p_me_and_my_uncle_released_a_new_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T02:50:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4k9is</id>
    <title>LLMSnap - fast model swapping for vLLM using sleep mode</title>
    <updated>2025-11-23T11:15:48+00:00</updated>
    <author>
      <name>/u/Camvizioneer</name>
      <uri>https://old.reddit.com/user/Camvizioneer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When I saw the release of vLLM sleep mode providing second-ish swap times, I was very intrigued - it was exactly what I needed. Previous non-sleep vLLM model swapping was unusable for frequent model swaps, with startup times around 1 minute each.&lt;/p&gt; &lt;p&gt;I started looking for an existing lightweight model router with vLLM sleep mode support but couldn't find any. I found what seemed like a perfect project to add this functionality - llama-swap. I implemented vLLM sleep support and opened a PR, but it was closed with the reasoning that most llama-swap users use llama.cpp and don't need this feature. That's how &lt;a href="https://github.com/napmany/llmsnap"&gt;llmsnap&lt;/a&gt; was born!&lt;/p&gt; &lt;p&gt;I'm going to continue working on llmsnap with a focus on making LLM model swapping faster and more resource-effective, without limiting or tight coupling to any one inference server - even though only vLLM took its spot in the title for now :)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/napmany/llmsnap"&gt;https://github.com/napmany/llmsnap&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You can install and use it with brew, docker, release binaries, or from source.&lt;/p&gt; &lt;p&gt;Questions and feedback are very welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Camvizioneer"&gt; /u/Camvizioneer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4k9is/llmsnap_fast_model_swapping_for_vllm_using_sleep/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4k9is/llmsnap_fast_model_swapping_for_vllm_using_sleep/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p4k9is/llmsnap_fast_model_swapping_for_vllm_using_sleep/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T11:15:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4jfb4</id>
    <title>ComfyUI Raylight Parallelism Benchmark, 5090 vs Dual 2000 Ada (4060 Ti-ish). Also I enable CFG Parallel, so SDXL and SD1.5 can be parallelized.</title>
    <updated>2025-11-23T10:24:23+00:00</updated>
    <author>
      <name>/u/Altruistic_Heat_9531</name>
      <uri>https://old.reddit.com/user/Altruistic_Heat_9531</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4jfb4/comfyui_raylight_parallelism_benchmark_5090_vs/"&gt; &lt;img alt="ComfyUI Raylight Parallelism Benchmark, 5090 vs Dual 2000 Ada (4060 Ti-ish). Also I enable CFG Parallel, so SDXL and SD1.5 can be parallelized." src="https://preview.redd.it/9z32gcsngz2g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e9b23ff50d19900019d39d7d051f985c46fa61d9" title="ComfyUI Raylight Parallelism Benchmark, 5090 vs Dual 2000 Ada (4060 Ti-ish). Also I enable CFG Parallel, so SDXL and SD1.5 can be parallelized." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Someone asked about 5090 vs dual 5070/5060 16GB perf benchmark for Raylight, so here it is.&lt;/p&gt; &lt;p&gt;Take it with a grain of salt ofc.&lt;br /&gt; &lt;strong&gt;TLDR&lt;/strong&gt;: 5090 had, is, and will demolish dual 4060Ti. That is as true as asking if the sky is blue. But again, my project is for people who can buy a second 4060Ti, not necessarily for people buying a 5090 or 4090.&lt;/p&gt; &lt;p&gt;Runs purely on RunPod. Anyway have a nice day.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/komikndr/raylight/tree/main"&gt;https://github.com/komikndr/raylight/tree/main&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Altruistic_Heat_9531"&gt; /u/Altruistic_Heat_9531 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9z32gcsngz2g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4jfb4/comfyui_raylight_parallelism_benchmark_5090_vs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p4jfb4/comfyui_raylight_parallelism_benchmark_5090_vs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T10:24:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1p40bne</id>
    <title>I got frustrated with existing web UIs for local LLMs, so I built something different</title>
    <updated>2025-11-22T18:23:31+00:00</updated>
    <author>
      <name>/u/alphatrad</name>
      <uri>https://old.reddit.com/user/alphatrad</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p40bne/i_got_frustrated_with_existing_web_uis_for_local/"&gt; &lt;img alt="I got frustrated with existing web UIs for local LLMs, so I built something different" src="https://b.thumbs.redditmedia.com/YiyI_H_VHB3bA6O2O8UbKRAEKLauUG-4ruRmJAfsWvA.jpg" title="I got frustrated with existing web UIs for local LLMs, so I built something different" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been running local models for a while now, and like many of you, I tried Open WebUI. The feature list looked great, but in practice... it felt bloated. Slow. Overengineered. And then there is the license restrictions. WTF this isn't truly &amp;quot;open&amp;quot; in the way I expected.&lt;/p&gt; &lt;p&gt;So I built &lt;a href="https://github.com/1337hero/faster-chat"&gt;Faster Chat&lt;/a&gt; - a privacy-first, actually-MIT-licensed alternative that gets out of your way.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/nfbihjytou2g1.png?width=2226&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=43b1fc93ddc80569a95e8bce2999bd237ee6c846"&gt;https://preview.redd.it/nfbihjytou2g1.png?width=2226&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=43b1fc93ddc80569a95e8bce2999bd237ee6c846&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;3KB Preact runtime (NO BLOAT)&lt;/li&gt; &lt;li&gt;Privacy first: conversations stay in your browser&lt;/li&gt; &lt;li&gt;MIT license (actually open source, not copyleft)&lt;/li&gt; &lt;li&gt;Works offline with Ollama/LM Studio/llama.cpp&lt;/li&gt; &lt;li&gt;Multi-provider: OpenAI, Anthropic, Groq, or local models&lt;/li&gt; &lt;li&gt;Docker deployment in one command&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The honest version:&lt;/strong&gt; This is alpha. I'm a frontend dev, not a designer, so some UI quirks exist. Built it because I wanted something fast and private for myself and figued others might want the same.&lt;/p&gt; &lt;p&gt;Docker deployment works. Multi-user auth works. File attachments work. Streaming works. The core is solid.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What's still rough:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;UI polish (seriously, if you're a designer, please help)&lt;/li&gt; &lt;li&gt;Some mobile responsiveness issues&lt;/li&gt; &lt;li&gt;Tool calling is infrastructure-ready but not fully implemented&lt;/li&gt; &lt;li&gt;Documentation could be better&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I've seen the threads about Open WebUI frustrations, and I felt that pain too. So if you're looking for something lighter, faster, and actually open source, give it a shot. And if you hate it, let me know why - I'm here to improve it.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/1337hero/faster-chat"&gt;https://github.com/1337hero/faster-chat&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Questions/feedback welcome.&lt;/p&gt; &lt;p&gt;Or just roast me and dunk on me. That's cool too.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/alphatrad"&gt; /u/alphatrad &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p40bne/i_got_frustrated_with_existing_web_uis_for_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p40bne/i_got_frustrated_with_existing_web_uis_for_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p40bne/i_got_frustrated_with_existing_web_uis_for_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-22T18:23:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4jvvw</id>
    <title>What's the fastest OCR model / solution for a production grade pipeline ingesting 4M pages per month?</title>
    <updated>2025-11-23T10:52:48+00:00</updated>
    <author>
      <name>/u/DistinctAir8716</name>
      <uri>https://old.reddit.com/user/DistinctAir8716</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We are running an app serving 500k users, where we ingest pdf documents from users, and we have to turn them into markdown format for LLM integration.&lt;/p&gt; &lt;p&gt;Currently, we're using an OCR service that meets our needs, but it doesn't produce the highest quality results.&lt;/p&gt; &lt;p&gt;We want to switch to a VLLM like Deepseek-OCR, LightonOCR, dots.ocr, olmOCR etc.&lt;/p&gt; &lt;p&gt;The only problem is that when we go out and test these models, they're all too slow, with the best one, LightonOCR, peaking at 600 tok/s in generation.&lt;/p&gt; &lt;p&gt;We need a solution that can (e.g.) turn a 40-page PDF into markdown in ideally less than 20 seconds, while costing less than $0.10 per thousand pages.&lt;/p&gt; &lt;p&gt;We have been bashing out head on this problem for well over a month testing various models, is the route of switching to a VLLM worth it?&lt;/p&gt; &lt;p&gt;If not, what are some good alternatives or gaps we're not seeing? What would be the best way to approach this problem?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DistinctAir8716"&gt; /u/DistinctAir8716 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4jvvw/whats_the_fastest_ocr_model_solution_for_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4jvvw/whats_the_fastest_ocr_model_solution_for_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p4jvvw/whats_the_fastest_ocr_model_solution_for_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T10:52:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1p46mkt</id>
    <title>Qwen 2.5 vl 72b is the new SOTA model on SpatialBench, beating Gemini 3 pro. A new benchmark to test spatial reasoning on vlms</title>
    <updated>2025-11-22T22:51:53+00:00</updated>
    <author>
      <name>/u/gbomb13</name>
      <uri>https://old.reddit.com/user/gbomb13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p46mkt/qwen_25_vl_72b_is_the_new_sota_model_on/"&gt; &lt;img alt="Qwen 2.5 vl 72b is the new SOTA model on SpatialBench, beating Gemini 3 pro. A new benchmark to test spatial reasoning on vlms" src="https://b.thumbs.redditmedia.com/gkUZjCWCpd_R9pbF5IZULC7D5HjBa1BJsXWaY241hpQ.jpg" title="Qwen 2.5 vl 72b is the new SOTA model on SpatialBench, beating Gemini 3 pro. A new benchmark to test spatial reasoning on vlms" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We looked over its answers, the questions it got correct were the easiest ones but impressive nonetheless compared to other models. &lt;a href="https://spicylemonade.github.io/spatialbench/"&gt;https://spicylemonade.github.io/spatialbench/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gbomb13"&gt; /u/gbomb13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1p46mkt"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p46mkt/qwen_25_vl_72b_is_the_new_sota_model_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p46mkt/qwen_25_vl_72b_is_the_new_sota_model_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-22T22:51:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4h88k</id>
    <title>A neat CLI frontend for live AI dialogue!</title>
    <updated>2025-11-23T08:05:08+00:00</updated>
    <author>
      <name>/u/Rektile142</name>
      <uri>https://old.reddit.com/user/Rektile142</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4h88k/a_neat_cli_frontend_for_live_ai_dialogue/"&gt; &lt;img alt="A neat CLI frontend for live AI dialogue!" src="https://preview.redd.it/k8r9p2j5ly2g1.gif?width=640&amp;amp;crop=smart&amp;amp;s=4f57e7f993c90e823af42c5ab0abd97eb34ca02d" title="A neat CLI frontend for live AI dialogue!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Version 1.0.0 of &lt;strong&gt;Local Sage&lt;/strong&gt;, a dialogue-oriented CLI frontend for AI chat, has launched! &lt;/p&gt; &lt;p&gt;It's aimed at local inference (llama.cpp, ollama, vLLM, etc.) and hooks into any OpenAI API endpoint.&lt;/p&gt; &lt;p&gt;It's got some fun stuff!&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Conversations live in your shell&lt;/strong&gt;, rendering directly to standard output.&lt;/li&gt; &lt;li&gt;Fancy prompts with &lt;strong&gt;command completion&lt;/strong&gt; and &lt;strong&gt;in-memory history&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Context-aware file management&lt;/strong&gt;: attach, remove, and replace text-based files.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Session management&lt;/strong&gt;: load, save, delete, reset, and summarize sessions.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Profile management&lt;/strong&gt;: save, delete, and switch model profiles.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Repo is live here: &lt;a href="https://github.com/Kyleg142/localsage"&gt;https://github.com/Kyleg142/localsage&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You can install Local Sage with &lt;strong&gt;uv&lt;/strong&gt; to give it a spin: &lt;code&gt;uv tool install localsage&lt;/code&gt;&lt;/p&gt; &lt;p&gt;The project is MIT open-source as well! Please let me know what you guys think!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rektile142"&gt; /u/Rektile142 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/k8r9p2j5ly2g1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4h88k/a_neat_cli_frontend_for_live_ai_dialogue/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p4h88k/a_neat_cli_frontend_for_live_ai_dialogue/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T08:05:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4hkaf</id>
    <title>Qwen3-2B-VL for OCR is actually insane. Dockerized Set Up + GitHub</title>
    <updated>2025-11-23T08:26:46+00:00</updated>
    <author>
      <name>/u/exaknight21</name>
      <uri>https://old.reddit.com/user/exaknight21</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been trying to find an efficient model to perform OCR for my use case for a while. I created &lt;a href="https://github.com/ikantkode/exaOCR"&gt;exaOCR&lt;/a&gt; - and when I pushed the code, I can swear on all that is holy that it was working. BUT, for some reason, I simply cannot fix it anymore. It uses OCRMyPDF and the error is literally unsolvable by any models (ChatGPT, DeepSeek, Claude, Grok) and I threw in the towel until I guess I can make enough friends that are actual coders. (If you are able to contribute, please do.)&lt;/p&gt; &lt;p&gt;My entire purpose in using AI to create these crappy streamlit apps is to test the usability for my use case and then essentially go from there. As such, I could never get DeepSeek OCR to work, but someone posted about their project (ocrarena.ai) and I was able to try the models. Not very impressed + the general chatter around it.&lt;/p&gt; &lt;p&gt;I am a huge fan of the Qwen Team and not because they publish everything Open Source, but the fact that they are working towards an efficient AI model that *some* of us peasants can run.&lt;/p&gt; &lt;p&gt;Brings me to the main point. I got a T5610 for $239, I had a 3060 12 GB laying around and I got another for $280 also 12 GB, I threw them both together and they are able to help me experiment. The Qwen3-2B-VL for OCR is actually insane... I mean, deploy it and look for yourself. Just a heads up, my friend tried it on his 10 GB 3080, and vLLM threw an error, you will want to reduce the **--max-model-len from 16384 to probably 8000 **. Remember, I am using dual 3060s giving me more VRAM to play with.&lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/ikantkode/qwen3-2b-ocr-app"&gt;https://github.com/ikantkode/qwen3-2b-ocr-app&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In any event, here is a short video of it working: &lt;a href="https://youtu.be/anjhfOc7RqA"&gt;https://youtu.be/anjhfOc7RqA&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/exaknight21"&gt; /u/exaknight21 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4hkaf/qwen32bvl_for_ocr_is_actually_insane_dockerized/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4hkaf/qwen32bvl_for_ocr_is_actually_insane_dockerized/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p4hkaf/qwen32bvl_for_ocr_is_actually_insane_dockerized/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T08:26:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1p3xqsu</id>
    <title>Qwen-image-edit-2511 coming next week</title>
    <updated>2025-11-22T16:41:15+00:00</updated>
    <author>
      <name>/u/abdouhlili</name>
      <uri>https://old.reddit.com/user/abdouhlili</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3xqsu/qwenimageedit2511_coming_next_week/"&gt; &lt;img alt="Qwen-image-edit-2511 coming next week" src="https://preview.redd.it/yeofdp077u2g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ef09997e54c4c481e545ec2dd4183f65163c8a73" title="Qwen-image-edit-2511 coming next week" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdouhlili"&gt; /u/abdouhlili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/yeofdp077u2g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3xqsu/qwenimageedit2511_coming_next_week/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p3xqsu/qwenimageedit2511_coming_next_week/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-22T16:41:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1p48d7f</id>
    <title>Strix Halo, Debian 13@6.16.12&amp;6.17.8, Qwen3Coder-Q8 CTX&lt;=131k, llama.cpp@Vulkan&amp;ROCm, Power &amp; Efficiency</title>
    <updated>2025-11-23T00:10:33+00:00</updated>
    <author>
      <name>/u/Educational_Sun_8813</name>
      <uri>https://old.reddit.com/user/Educational_Sun_8813</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p48d7f/strix_halo_debian_13616126178_qwen3coderq8/"&gt; &lt;img alt="Strix Halo, Debian 13@6.16.12&amp;amp;6.17.8, Qwen3Coder-Q8 CTX&amp;lt;=131k, llama.cpp@Vulkan&amp;amp;ROCm, Power &amp;amp; Efficiency" src="https://preview.redd.it/hg69ko66fw2g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2fb55f92f9b2aa5fc700b98933b28eb18462d618" title="Strix Halo, Debian 13@6.16.12&amp;amp;6.17.8, Qwen3Coder-Q8 CTX&amp;lt;=131k, llama.cpp@Vulkan&amp;amp;ROCm, Power &amp;amp; Efficiency" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, i wanted to check kernel improvement in support of strix halo under Debian GNU/Linux, while latest minor versions of 6.16.x improved GTT wanted to check if can be even better. So i tested it on Debian 13 with latest kernel from testing &lt;code&gt;6.16.12+deb14+1-amd64&lt;/code&gt;, and one precompiled performance optimized kernel &lt;code&gt;6.17.8-x64v3-xanmod1&lt;/code&gt;. I ran tests agains &lt;code&gt;Qwen3-Coder-Q8&lt;/code&gt; in full context, but i did benchmark up to &lt;code&gt;131k&lt;/code&gt;. Llama.cpp versions i used for tests: &lt;code&gt;Vulkan build: 5be353ec4 (7109)&lt;/code&gt; and &lt;code&gt;ROCm TheROCK precompiled build: 416e7c7 (1)&lt;/code&gt;. Side notice i managed to compile finally llama.cpp with external libs from AMD for HIP support, so from now one i will use same build for Vulkan and ROCM. Since i wanted also to find sweet spot in energy efficiency so i tried to capture also power usage, and compare it with computing performance. So in the end i tested that model with two backends, and kernels, changing context in few steps, to find out.&lt;/p&gt; &lt;p&gt;In the end seems that latest kernel from testing &lt;code&gt;6.16.12&lt;/code&gt; works just great! Performance kernel speed is maybe fraction better (max 2%). Besides stock kernel had 4W in idle (in &lt;code&gt;balanced&lt;/code&gt; mode), while performance kernel had always minimum 9-10W. And i use fans with 0RPM &amp;lt;= PWM 5% so it's completly silent when idle. And audible under heavy load especially with ROCm. Anyway most optimal power setting for computations is &lt;code&gt;latency-performance&lt;/code&gt; and it's not worth to use &lt;code&gt;accelerator-performance&lt;/code&gt; in the long run.&lt;/p&gt; &lt;p&gt;Here just notice for strix halo Debian users (and other distros probably too, but current Arch and Fedora have newer kernel), you need to use at least &lt;code&gt;6.16.x&lt;/code&gt; to have better experience with that platform. For Debian GNU/Linux easiest way is to install newer kernel from backports, or move to testing for the latest one. I just noticed that with &lt;code&gt;apt update&lt;/code&gt; just now that there is &lt;code&gt;6.16.12&lt;/code&gt; in stable, so it's great nothing to for Debian users. :) And testing moved to &lt;code&gt;6.17.8+deb14-amd64&lt;/code&gt; so great, anyway i will have now that kernel, so will test it soon again from debian branch. haha, what an irony, but it took me quite time to write it down. So update: and just tested &lt;code&gt;6.17.8+deb14-amd64&lt;/code&gt; and idle now is 6W in balance mode now, bit more, than before, but less than the custom kernel.&lt;/p&gt; &lt;p&gt;Performance wise Vulkan is faster in TG, while significantly slower in PP especially with long context. On the other hand ROCm is much faster in PP, and bit slower in TG, but overal improvement in PP is so big that it does not matter for long context (it's around &lt;strong&gt;x2.7 faster&lt;/strong&gt; in 131k CTX window). Vulkan is very fast for shorter chats, but over 32k CTX it's getting much slower. Under load (tested with &lt;code&gt;accelerator-performance&lt;/code&gt; profile in &lt;code&gt;tuned&lt;/code&gt;) ROCm can draw around &lt;code&gt;120W&lt;/code&gt; (this backend use also more CPU for PP), while Vulkan peak was around &lt;code&gt;70W&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;I found that best values for &lt;code&gt;-ub&lt;/code&gt; batch size is &lt;code&gt;512&lt;/code&gt;(it's default) for Vulkan, but &lt;code&gt;2048&lt;/code&gt; for ROCm (it's &lt;strong&gt;faster ~16%&lt;/strong&gt; than default). After that you have to increase &lt;code&gt;-b&lt;/code&gt; logical batch size to &lt;code&gt;8192&lt;/code&gt; for best performance with ROCm. For Vulkan just leave default logical batch size.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;BONUS section&lt;/strong&gt;, agent test: After tests i wanted to check &lt;code&gt;Qwen3-coder-Q8&lt;/code&gt; model in some tooling so i tried to install &lt;code&gt;kubectl-ai&lt;/code&gt;, and connect it to my local llama-server, and perform some tasks on local kubernetes (4 nodes). Model was able based on the natural language promp install Jupyter hub from helm charts, using ~50k tokens for that. And one could run notebooks in some 8-10 minutes. That model works really good on strix halo, worth to check if you didn't yet.&lt;/p&gt; &lt;p&gt;I hope someone will find it valuable, and diagram clear enough. :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Educational_Sun_8813"&gt; /u/Educational_Sun_8813 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hg69ko66fw2g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p48d7f/strix_halo_debian_13616126178_qwen3coderq8/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p48d7f/strix_halo_debian_13616126178_qwen3coderq8/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T00:10:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4ii26</id>
    <title>Long term users of this sub - where have you gone to discuss SOTA models, ideas and AI in general?</title>
    <updated>2025-11-23T09:26:58+00:00</updated>
    <author>
      <name>/u/kpodkanowicz</name>
      <uri>https://old.reddit.com/user/kpodkanowicz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Seems like this sub has become mainstream, and in that mainstream only focus on local models. I see that key people are no longer posting or commenting, so I assume community moved somewhere... where are you now? For those that you left behind (like me) it feels lonely :D&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kpodkanowicz"&gt; /u/kpodkanowicz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4ii26/long_term_users_of_this_sub_where_have_you_gone/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4ii26/long_term_users_of_this_sub_where_have_you_gone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p4ii26/long_term_users_of_this_sub_where_have_you_gone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T09:26:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4ijay</id>
    <title>Making an offline STS (speech to speech) AI that runs under 2GB RAM. But do people even need offline AI now?</title>
    <updated>2025-11-23T09:29:08+00:00</updated>
    <author>
      <name>/u/Automatic_Finish8598</name>
      <uri>https://old.reddit.com/user/Automatic_Finish8598</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Iâ€™m building a full speech to speech AI that runs totally offline. Everything stays on the device. STT, LLM inference and TTS all running locally in under 2GB RAM. I already have most of the architecture working and a basic MVP.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The part Iâ€™m thinking a lot about is the bigger question. With models like Gemini, ChatGPT and Llama becoming cheaper and extremely accessible, why would anyone still want to use something fully offline?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;My reason is simple. I want an AI that can work completely on personal or sensitive data without sending anything outside. Something you can use in hospitals, rural government centers, developer setups, early startups, labs, or places where internet isnâ€™t stable or cloud isnâ€™t allowed. Basically an AI you own fully, with no external calls.&lt;/p&gt; &lt;p&gt;My idea is to make a proper offline autonomous assistant that behaves like a personal AI layer. It should handle voice, do local reasoning, search your files, automate stuff, summarize documents, all of that, without depending on the internet or any external service.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Iâ€™m curious what others think about this direction. Is offline AI still valuable when cloud AI is getting so cheap? Are there use cases Iâ€™m not thinking about or is this something only a niche group will ever care about?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Would love to hear your thoughts.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Automatic_Finish8598"&gt; /u/Automatic_Finish8598 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4ijay/making_an_offline_sts_speech_to_speech_ai_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4ijay/making_an_offline_sts_speech_to_speech_ai_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p4ijay/making_an_offline_sts_speech_to_speech_ai_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T09:29:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4ftd5</id>
    <title>Physical documentation for LLMs in Shenzhen bookstore selling guides for DeepSeek, Doubao, Kimi, and ChatGPT.</title>
    <updated>2025-11-23T06:36:53+00:00</updated>
    <author>
      <name>/u/abdouhlili</name>
      <uri>https://old.reddit.com/user/abdouhlili</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4ftd5/physical_documentation_for_llms_in_shenzhen/"&gt; &lt;img alt="Physical documentation for LLMs in Shenzhen bookstore selling guides for DeepSeek, Doubao, Kimi, and ChatGPT." src="https://preview.redd.it/94nizo5acy2g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b2a7cc2846512c02a0c4119a338567cbcbcc8574" title="Physical documentation for LLMs in Shenzhen bookstore selling guides for DeepSeek, Doubao, Kimi, and ChatGPT." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdouhlili"&gt; /u/abdouhlili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/94nizo5acy2g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4ftd5/physical_documentation_for_llms_in_shenzhen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p4ftd5/physical_documentation_for_llms_in_shenzhen/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T06:36:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4jyrv</id>
    <title>No way kimi gonna release new model !!</title>
    <updated>2025-11-23T10:57:51+00:00</updated>
    <author>
      <name>/u/Independent-Wind4462</name>
      <uri>https://old.reddit.com/user/Independent-Wind4462</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4jyrv/no_way_kimi_gonna_release_new_model/"&gt; &lt;img alt="No way kimi gonna release new model !!" src="https://preview.redd.it/1ezldlbumz2g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8ad6c7d5d0f6a6e2b160c885a08a82d80d71ef81" title="No way kimi gonna release new model !!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Wind4462"&gt; /u/Independent-Wind4462 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1ezldlbumz2g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4jyrv/no_way_kimi_gonna_release_new_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p4jyrv/no_way_kimi_gonna_release_new_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T10:57:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1ozopxx</id>
    <title>AMA Announcement: MiniMax, The Opensource Lab Behind MiniMax-M2 + Gifts to Our Community (Wednesday, 8AM-11AM PST)</title>
    <updated>2025-11-17T18:50:44+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozopxx/ama_announcement_minimax_the_opensource_lab/"&gt; &lt;img alt="AMA Announcement: MiniMax, The Opensource Lab Behind MiniMax-M2 + Gifts to Our Community (Wednesday, 8AM-11AM PST)" src="https://preview.redd.it/e3scgr4j5v1g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ca0bf85cb4cf2a2b7e12e8d99d515186275c15e3" title="AMA Announcement: MiniMax, The Opensource Lab Behind MiniMax-M2 + Gifts to Our Community (Wednesday, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/e3scgr4j5v1g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozopxx/ama_announcement_minimax_the_opensource_lab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ozopxx/ama_announcement_minimax_the_opensource_lab/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-17T18:50:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1b550</id>
    <title>AMA with MiniMax â€” Ask Us Anything!</title>
    <updated>2025-11-19T15:46:38+00:00</updated>
    <author>
      <name>/u/OccasionNo6699</name>
      <uri>https://old.reddit.com/user/OccasionNo6699</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;! Weâ€™re really excited to be here, thanks for having us.&lt;/p&gt; &lt;p&gt;Iâ€™m Skyler (&lt;a href="https://www.reddit.com/user/OccasionNo6699/"&gt;u/OccasionNo6699&lt;/a&gt;), head of engineering at MiniMax, the lab behind:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://x.com/MiniMax__AI/status/1982674798649160175?s=20"&gt;MiniMax-M2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983382728343994414"&gt;Hailuo 2.3&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983661667872600296"&gt;MiniMax Speech 2.6&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983964920493568296"&gt;MiniMax Music 2.0&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Joining me today are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Pengyu Zhao, &lt;a href="https://www.reddit.com/user/Wise_Evidence9973/"&gt;u/Wise_Evidence9973&lt;/a&gt; â€” Head of LLM Research&lt;/li&gt; &lt;li&gt;Jade Cai, &lt;a href="https://www.reddit.com/user/srtng/"&gt;u/srtng&lt;/a&gt; â€” Head of Developer Community&lt;/li&gt; &lt;li&gt;midnight_compile , &lt;a href="/u/Top_Cattle_2098"&gt;u/Top_Cattle_2098&lt;/a&gt; â€” LLM Researcher&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 8AM-11AM PST with our core MiniMax tech team continuing to follow up on questions over the next 48 hours.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OccasionNo6699"&gt; /u/OccasionNo6699 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T15:46:38+00:00</published>
  </entry>
</feed>
