<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-16T21:48:59+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1o8ieap</id>
    <title>Any simple alternatives to Continue.dev?</title>
    <updated>2025-10-16T21:10:34+00:00</updated>
    <author>
      <name>/u/HEAVYlight123</name>
      <uri>https://old.reddit.com/user/HEAVYlight123</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So it seems that Continue.dev has decided to continuously make their product worse for local use, hiding the config file and now automatically truncating prompts even after going through the trouble of specifying the context length. I've tried Roo, Kilo, Cline etc. but 10k+ tokens for every request seems excessive and I don't really want an agent. Really I just want a chat window that I can @ context and that can use read-only tools to discover additional context. Anything I should check out? Continue was working great, but with the recent updates it seems like it's time to jump ship before it becomes totally unusable.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HEAVYlight123"&gt; /u/HEAVYlight123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8ieap/any_simple_alternatives_to_continuedev/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8ieap/any_simple_alternatives_to_continuedev/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8ieap/any_simple_alternatives_to_continuedev/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T21:10:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1o89znk</id>
    <title>A new, super simple LLM benchmark for testing changes across models, quants, parameters, samplers, engines, etc</title>
    <updated>2025-10-16T15:57:09+00:00</updated>
    <author>
      <name>/u/Eisenstein</name>
      <uri>https://old.reddit.com/user/Eisenstein</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o89znk/a_new_super_simple_llm_benchmark_for_testing/"&gt; &lt;img alt="A new, super simple LLM benchmark for testing changes across models, quants, parameters, samplers, engines, etc" src="https://external-preview.redd.it/0joRre5VS2PgnS7WJD0-RjSuntbOAn34cwNgbAHQMrk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=92e5b40ebc550a11ecbbf21117405dc916931b9e" title="A new, super simple LLM benchmark for testing changes across models, quants, parameters, samplers, engines, etc" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Eisenstein"&gt; /u/Eisenstein &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/jabberjabberjabber/Context-Tester/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o89znk/a_new_super_simple_llm_benchmark_for_testing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o89znk/a_new_super_simple_llm_benchmark_for_testing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T15:57:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7gpr8</id>
    <title>Got the DGX Spark - ask me anything</title>
    <updated>2025-10-15T17:02:50+00:00</updated>
    <author>
      <name>/u/sotech117</name>
      <uri>https://old.reddit.com/user/sotech117</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7gpr8/got_the_dgx_spark_ask_me_anything/"&gt; &lt;img alt="Got the DGX Spark - ask me anything" src="https://preview.redd.it/9mr835ne4bvf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=42dc8e85dcff8b55d4174e98495bb8d2d144fd7d" title="Got the DGX Spark - ask me anything" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If there’s anything you want me to benchmark (or want to see in general), let me know, and I’ll try to reply to your comment. I will be playing with this all night trying a ton of different models I’ve always wanted to run. &lt;/p&gt; &lt;p&gt;(&amp;amp; shoutout to microcenter my goats!)&lt;/p&gt; &lt;p&gt;updates: Hit it hard with Wan2.2 via ComfyUI, base template but upped the resolution to &lt;a href="mailto:720p@24fps"&gt;720p@24fps&lt;/a&gt;. Extremely easy to setup. NVIDIA-SMI queries are trolling, giving lots of N/A.&lt;/p&gt; &lt;p&gt;Max-acpi-temp: 91.8 C (&lt;a href="https://drive.mfoi.dev/s/pDZm9F3axRnoGca"&gt;https://drive.mfoi.dev/s/pDZm9F3axRnoGca&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;Max-gpu-tdp: 101 W (&lt;a href="https://drive.mfoi.dev/s/LdwLdzQddjiQBKe"&gt;https://drive.mfoi.dev/s/LdwLdzQddjiQBKe&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;Max-watt-consumption (from-wall): 195.5 W (&lt;a href="https://drive.mfoi.dev/s/643GLEgsN5sBiiS"&gt;https://drive.mfoi.dev/s/643GLEgsN5sBiiS&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;final-output: &lt;a href="https://drive.mfoi.dev/s/rWe9yxReqHxB9Py"&gt;https://drive.mfoi.dev/s/rWe9yxReqHxB9Py&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Physical observations: Under heavy load, it gets uncomfortably hot to the touch, and the fan noise is prevalent and almost makes a grinding sound (?). Unfortunately, mine has some coil whine during computation (, which is may more noticeable than the fan noise). It's really not a &amp;quot;on your desk machine&amp;quot; - makes more sense in a server rack using ssh and/or webtools.&lt;/p&gt; &lt;p&gt;coil-whine: &lt;a href="https://drive.mfoi.dev/s/eGcxiMXZL3NXQYT"&gt;https://drive.mfoi.dev/s/eGcxiMXZL3NXQYT&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Still setting up a ton of requested models (mostly LLMs) currently. Should have more llm and image/video gen numbers tonight. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sotech117"&gt; /u/sotech117 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9mr835ne4bvf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7gpr8/got_the_dgx_spark_ask_me_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7gpr8/got_the_dgx_spark_ask_me_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T17:02:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8cz1h</id>
    <title>Exo linking Mac studio with DGX</title>
    <updated>2025-10-16T17:46:00+00:00</updated>
    <author>
      <name>/u/Badger-Purple</name>
      <uri>https://old.reddit.com/user/Badger-Purple</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8cz1h/exo_linking_mac_studio_with_dgx/"&gt; &lt;img alt="Exo linking Mac studio with DGX" src="https://external-preview.redd.it/hyPfbB2-0S9_BpSzWHTEzbVGWtAk9YMYdbHv82PBK64.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bdf78665a24bb3ff417303dab346ea60afc9e667" title="Exo linking Mac studio with DGX" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;EXO's newest demo combines two of NVIDIA's DGX Spark systems with Apple's M3 Ultra–powered Mac Studio to make use of the disparate strengths of each machine: Spark has more raw compute muscle, while the Mac Studio can move data around much faster. EXO 1.0, currently in early access, blends the two into a single inference pipeline, and it apparently works shockingly well.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Badger-Purple"&gt; /u/Badger-Purple &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.tomshardware.com/software/two-nvidia-dgx-spark-systems-combined-with-m3-ultra-mac-studio-to-create-blistering-llm-system-exo-labs-demonstrates-disaggregated-ai-inference-and-achieves-a-2-8-benchmark-boost"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8cz1h/exo_linking_mac_studio_with_dgx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8cz1h/exo_linking_mac_studio_with_dgx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T17:46:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8gpm0</id>
    <title>Help Us Choose Our Next Open-source Local AI App</title>
    <updated>2025-10-16T20:05:09+00:00</updated>
    <author>
      <name>/u/badgerbadgerbadgerWI</name>
      <uri>https://old.reddit.com/user/badgerbadgerbadgerWI</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We’re picking &lt;strong&gt;one&lt;/strong&gt; fully open-source app to build next with &lt;a href="https://github.com/llama-farm/llamafarm"&gt;Llamafarm's&lt;/a&gt; local AI development tools. It’ll run great on a laptop and be easy for anyone to use. No accounts. Clean UX. Real docs. One-click run. 100% local - models, RAG, runtime, app all local - (Google, OpenAI, ISP doesn't get any info).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Healthcare Assistant.&lt;/strong&gt;&lt;br /&gt; Drag in labs, CCD/Blue Button exports, or portal PDFs. It translates jargon, highlights “out of range” items, and drafts questions for your next visit. Optional modules for medication interactions and guideline lookups. I hate looking up terms in Google or OpenAI and getting ads for a month. Offline-friendly and fast on everyday hardware.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Legal Aid.&lt;/strong&gt;&lt;br /&gt; Multi-language plain guidance for immigration paperwork, divorce/custody, housing, and small claims. It maps your situation to the right forms, creates a prep checklist, and generates letter/filing drafts with citations to public sources. Those questions you don't want the world to know.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Financial Helper.&lt;/strong&gt;&lt;br /&gt; Ask about taxes, budgeting, entity setup (LLC vs S-Corp), and “what changed this year.” Import a &lt;strong&gt;local&lt;/strong&gt; CSV/ledger to get categorized insights, cash-flow flags, and draft checklists for filings. Plus explain-like-I’m-five summaries with links to official rules. Ask the questions you may be embarrassed to ask a friend. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Image Fixer.&lt;/strong&gt;&lt;br /&gt; On-device touch-ups: blemish removal, background cleanup, face/plate blur, smart crop, and batch processing. Side-by-side before/after, history panel with undo, and simple presets (headshot, marketplace, family album). No uploads, just quick results. Please don't send your family photos to OpenAI; keep them local.&lt;/p&gt; &lt;p&gt;What would &lt;em&gt;you&lt;/em&gt; actually use every week? If it’s none of these, tell us what would be—teacher prep kit, research brief builder, local dev helper for code search, small-biz ops toolkit, something else?&lt;/p&gt; &lt;p&gt;If we do this, we’ll do it right: open source, one-click run, clear docs, tests, evals, and a tidy UI—built to showcase the power and potential of local AI.&lt;/p&gt; &lt;p&gt;Drop your vote and one line on why. Add one must-have and one deal-breaker. If you’re up for feedback or safe sample data, say so and we’ll follow up.&lt;/p&gt; &lt;p&gt;Which one should we ship first?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/badgerbadgerbadgerWI"&gt; /u/badgerbadgerbadgerWI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8gpm0/help_us_choose_our_next_opensource_local_ai_app/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8gpm0/help_us_choose_our_next_opensource_local_ai_app/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8gpm0/help_us_choose_our_next_opensource_local_ai_app/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T20:05:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1o86h5j</id>
    <title>What MoE model sizes and capabilities are currently missing in the open weight ecosystem?</title>
    <updated>2025-10-16T13:41:40+00:00</updated>
    <author>
      <name>/u/eliebakk</name>
      <uri>https://old.reddit.com/user/eliebakk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As someone who trains models, I’d love to know if you have specific requests for model size or capabilities you’d like to see in a (fully) open MoE model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eliebakk"&gt; /u/eliebakk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o86h5j/what_moe_model_sizes_and_capabilities_are/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o86h5j/what_moe_model_sizes_and_capabilities_are/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o86h5j/what_moe_model_sizes_and_capabilities_are/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T13:41:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1o83b2i</id>
    <title>Qwen3 Next 80b FP8 with vllm on Pro 6000 Blackwell</title>
    <updated>2025-10-16T11:11:58+00:00</updated>
    <author>
      <name>/u/notaDestroyer</name>
      <uri>https://old.reddit.com/user/notaDestroyer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o83b2i/qwen3_next_80b_fp8_with_vllm_on_pro_6000_blackwell/"&gt; &lt;img alt="Qwen3 Next 80b FP8 with vllm on Pro 6000 Blackwell" src="https://b.thumbs.redditmedia.com/YJxZs33mPxgOa7fkc4zIPMKaCT3QAzLt14Sr270bH7A.jpg" title="Qwen3 Next 80b FP8 with vllm on Pro 6000 Blackwell" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GPU: NVIDIA RTX Pro 6000 Blackwell Edition (96GB VRAM)&lt;/p&gt; &lt;p&gt;- Driver: 580.95.05&lt;/p&gt; &lt;p&gt;- CUDA: 13.0&lt;/p&gt; &lt;p&gt;- Compute Capability: 9.0 (Blackwell)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/tf7qkz8ligvf1.png?width=7569&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=48977ab9548affc46738478260262bbf19184782"&gt;https://preview.redd.it/tf7qkz8ligvf1.png?width=7569&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=48977ab9548affc46738478260262bbf19184782&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Software:&lt;/p&gt; &lt;p&gt;- vLLM: v0.11.1rc2.dev72+gf7d318de2 (nightly)&lt;/p&gt; &lt;p&gt;- Attention Backend: **FlashInfer** (with JIT autotuning)&lt;/p&gt; &lt;p&gt;- Quantization: FP8 W8A8&lt;/p&gt; &lt;p&gt;- Python: 3.12.12&lt;/p&gt; &lt;p&gt;- PyTorch with CUDA 12.4 backend (forward compatible with CUDA 13.0 driver)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/notaDestroyer"&gt; /u/notaDestroyer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o83b2i/qwen3_next_80b_fp8_with_vllm_on_pro_6000_blackwell/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o83b2i/qwen3_next_80b_fp8_with_vllm_on_pro_6000_blackwell/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o83b2i/qwen3_next_80b_fp8_with_vllm_on_pro_6000_blackwell/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T11:11:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8hh1n</id>
    <title>Qwen3-VL-30B in llama.cpp</title>
    <updated>2025-10-16T20:33:57+00:00</updated>
    <author>
      <name>/u/egomarker</name>
      <uri>https://old.reddit.com/user/egomarker</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This release of llama.cpp can be used to run yairpatch/qwen3-vl-30b-a3b- GGUFs.&lt;br /&gt; Builds are pre-release, so issues are possible. But the overall state is very useable, so hopefully we will soon see it merged into llama.cpp.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Thireus/llama.cpp/releases/tag/tr-qwen3-vl-3-b6981-ab45b1a"&gt;https://github.com/Thireus/llama.cpp/releases/tag/tr-qwen3-vl-3-b6981-ab45b1a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also if you rename release to e.g. llama-b6981-bin-macos-arm64.zip, you will be able to install it as a backend into Jan. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/egomarker"&gt; /u/egomarker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8hh1n/qwen3vl30b_in_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8hh1n/qwen3vl30b_in_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8hh1n/qwen3vl30b_in_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T20:33:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1o88w9c</id>
    <title>ARM Partners with Meta</title>
    <updated>2025-10-16T15:16:10+00:00</updated>
    <author>
      <name>/u/Illustrious-Swim9663</name>
      <uri>https://old.reddit.com/user/Illustrious-Swim9663</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o88w9c/arm_partners_with_meta/"&gt; &lt;img alt="ARM Partners with Meta" src="https://preview.redd.it/wpf7pikaqhvf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8ab9b69847afb2486fa0aa6d707cbb6b7c878439" title="ARM Partners with Meta" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ARM Partners with Meta for Data Center and Next Generation Software, Collaboration May Be Interesting Info : &lt;a href="https://x.com/Arm/status/1978494349966025044?t=9tw4dYon0ecqebNQfE5rsQ&amp;amp;s=19"&gt;https://x.com/Arm/status/1978494349966025044?t=9tw4dYon0ecqebNQfE5rsQ&amp;amp;s=19&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Illustrious-Swim9663"&gt; /u/Illustrious-Swim9663 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wpf7pikaqhvf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o88w9c/arm_partners_with_meta/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o88w9c/arm_partners_with_meta/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T15:16:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8clx0</id>
    <title>mtmd : support home-cooked Mistral Small Omni by ngxson · Pull Request #14928 · ggml-org/llama.cpp</title>
    <updated>2025-10-16T17:32:29+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8clx0/mtmd_support_homecooked_mistral_small_omni_by/"&gt; &lt;img alt="mtmd : support home-cooked Mistral Small Omni by ngxson · Pull Request #14928 · ggml-org/llama.cpp" src="https://external-preview.redd.it/LDnY4VtL8nEMhrRnSn7xRje529JikFQ9xlCA17hsv2k.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=43014e497d8695214852c7b34ef392748272ec68" title="mtmd : support home-cooked Mistral Small Omni by ngxson · Pull Request #14928 · ggml-org/llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Support a home-cooked version of Mistral Small which can take &lt;strong&gt;both audio and image&lt;/strong&gt; as input&lt;/p&gt; &lt;p&gt;Link to GGUF: &lt;a href="https://huggingface.co/ngxson/Home-Cook-Mistral-Small-Omni-24B-2507-GGUF"&gt;https://huggingface.co/ngxson/Home-Cook-Mistral-Small-Omni-24B-2507-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(This is a multimodal model created by merging Mistral Small 2506 (with vision capabilities) and Voxtral 2507 (with audio capabilities) using a modified version of the &lt;code&gt;mergekit&lt;/code&gt; tool.)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/14928"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8clx0/mtmd_support_homecooked_mistral_small_omni_by/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8clx0/mtmd_support_homecooked_mistral_small_omni_by/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T17:32:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8hrd6</id>
    <title>Helloo, 96GB GPU from Huawei for $1400, slower than NVIDIA but the VRAM (GN)</title>
    <updated>2025-10-16T20:45:16+00:00</updated>
    <author>
      <name>/u/UniqueAttourney</name>
      <uri>https://old.reddit.com/user/UniqueAttourney</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8hrd6/helloo_96gb_gpu_from_huawei_for_1400_slower_than/"&gt; &lt;img alt="Helloo, 96GB GPU from Huawei for $1400, slower than NVIDIA but the VRAM (GN)" src="https://external-preview.redd.it/h0AXu77KNbLZvwy9Mngk-UUEgeAjkwRBjGOdlujFn7U.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=436c9e5446c362e3900f824dd945b76349b991ff" title="Helloo, 96GB GPU from Huawei for $1400, slower than NVIDIA but the VRAM (GN)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/UniqueAttourney"&gt; /u/UniqueAttourney &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=qGe_fq68x-Q"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8hrd6/helloo_96gb_gpu_from_huawei_for_1400_slower_than/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8hrd6/helloo_96gb_gpu_from_huawei_for_1400_slower_than/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T20:45:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7pe1u</id>
    <title>gigaResearch</title>
    <updated>2025-10-15T22:34:35+00:00</updated>
    <author>
      <name>/u/vladlearns</name>
      <uri>https://old.reddit.com/user/vladlearns</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7pe1u/gigaresearch/"&gt; &lt;img alt="gigaResearch" src="https://preview.redd.it/nb2hmgqircvf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=71c101f2683e8df117cbc2a9abd685bcac5cbce0" title="gigaResearch" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vladlearns"&gt; /u/vladlearns &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nb2hmgqircvf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7pe1u/gigaresearch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7pe1u/gigaresearch/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T22:34:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8frim</id>
    <title>This is interesting…</title>
    <updated>2025-10-16T19:29:14+00:00</updated>
    <author>
      <name>/u/nicoracarlo</name>
      <uri>https://old.reddit.com/user/nicoracarlo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A new release from Andrej Karpathy. Train your own model with $100&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/karpathy/nanochat/discussions/1"&gt;https://github.com/karpathy/nanochat/discussions/1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nicoracarlo"&gt; /u/nicoracarlo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8frim/this_is_interesting/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8frim/this_is_interesting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8frim/this_is_interesting/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T19:29:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1o88j9i</id>
    <title>HuggingChat Omni: new chat app by Hugging Face</title>
    <updated>2025-10-16T15:02:18+00:00</updated>
    <author>
      <name>/u/paf1138</name>
      <uri>https://old.reddit.com/user/paf1138</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;HuggingChat is back! the main new feature is auto-routing to the best open source model for your query. Making it competitive and often better than base chatgpt. &lt;/p&gt; &lt;p&gt;more info about it: &lt;a href="https://x.com/victormustar/status/1978817795312808065?s=46"&gt;https://x.com/victormustar/status/1978817795312808065?s=46&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paf1138"&gt; /u/paf1138 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/chat/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o88j9i/huggingchat_omni_new_chat_app_by_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o88j9i/huggingchat_omni_new_chat_app_by_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T15:02:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8bt83</id>
    <title>The model apocalypse is coming, which one do you chose to save and what other software ?</title>
    <updated>2025-10-16T17:03:31+00:00</updated>
    <author>
      <name>/u/HumanDrone8721</name>
      <uri>https://old.reddit.com/user/HumanDrone8721</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So the year is ${current_year} + X, a totalitarian world government is in power and decides the local running &amp;quot;unapproved&amp;quot; and &amp;quot;unaligned&amp;quot; LLMa are a danger to them (also is for the public interest, the terrorists may use them), as well as the associated software to use and train them (you can have guns but they are useless if you don't have ammunition), you mange to send a message in the past: &amp;quot;You have an 8TB SSD, you have to back-up the most useful models and software for the future&amp;quot;, what is your list of &amp;quot;must have&amp;quot; models and software, post it here to save the future ? (Yes, I do have an 8TB SSD and I foresee something like this happening and I want to have a nice selection of models and SW)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HumanDrone8721"&gt; /u/HumanDrone8721 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8bt83/the_model_apocalypse_is_coming_which_one_do_you/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8bt83/the_model_apocalypse_is_coming_which_one_do_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8bt83/the_model_apocalypse_is_coming_which_one_do_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T17:03:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1o82kta</id>
    <title>NVIDIA DGX Spark – A Non-Sponsored Review (Strix Halo Comparison, Pros &amp; Cons)</title>
    <updated>2025-10-16T10:30:22+00:00</updated>
    <author>
      <name>/u/Corylus-Core</name>
      <uri>https://old.reddit.com/user/Corylus-Core</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;NVIDIA DGX Spark – A Non-Sponsored Review (Strix Halo Comparison, Pros &amp;amp; Cons)&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=Pww8rIzr1pg"&gt;https://www.youtube.com/watch?v=Pww8rIzr1pg&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Corylus-Core"&gt; /u/Corylus-Core &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o82kta/nvidia_dgx_spark_a_nonsponsored_review_strix_halo/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o82kta/nvidia_dgx_spark_a_nonsponsored_review_strix_halo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o82kta/nvidia_dgx_spark_a_nonsponsored_review_strix_halo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T10:30:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8hacw</id>
    <title>Since DGX Spark is a disappointment... What is the best value for money hardware today?</title>
    <updated>2025-10-16T20:26:47+00:00</updated>
    <author>
      <name>/u/goto-ca</name>
      <uri>https://old.reddit.com/user/goto-ca</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My current compute box (2×1080 Ti) is failing, so I’ve been renting GPUs by the hour. I’d been waiting for DGX Spark, but early reviews look disappointing for the price/perf.&lt;/p&gt; &lt;p&gt;I’m ready to build a new PC and I’m torn between a &lt;strong&gt;single&lt;/strong&gt; high-end GPU or &lt;strong&gt;dual&lt;/strong&gt; mid/high GPUs. What’s the &lt;strong&gt;best price/performance configuration&lt;/strong&gt; I can build &lt;strong&gt;for ≤ $3,999&lt;/strong&gt; (tower, not a rack server)?&lt;/p&gt; &lt;p&gt;I don't care about RGBs and things like that - it will be kept in the basement and not looked at. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/goto-ca"&gt; /u/goto-ca &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8hacw/since_dgx_spark_is_a_disappointment_what_is_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8hacw/since_dgx_spark_is_a_disappointment_what_is_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8hacw/since_dgx_spark_is_a_disappointment_what_is_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T20:26:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1o84b36</id>
    <title>Qwen3-30B-A3B FP8 on RTX Pro 6000 blackwell with vllm</title>
    <updated>2025-10-16T12:04:11+00:00</updated>
    <author>
      <name>/u/notaDestroyer</name>
      <uri>https://old.reddit.com/user/notaDestroyer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o84b36/qwen330ba3b_fp8_on_rtx_pro_6000_blackwell_with/"&gt; &lt;img alt="Qwen3-30B-A3B FP8 on RTX Pro 6000 blackwell with vllm" src="https://b.thumbs.redditmedia.com/tevD9Ijs-RE6cQcddGazJZY--0Lzdu42Yn4uKrDPRjI.jpg" title="Qwen3-30B-A3B FP8 on RTX Pro 6000 blackwell with vllm" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Power limit set to 450w &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Short Context (1K tokens):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Single user: 88.4 tok/s&lt;/li&gt; &lt;li&gt;10 concurrent users: &lt;strong&gt;652 tok/s&lt;/strong&gt; throughput&lt;/li&gt; &lt;li&gt;Latency: 5.65s → 7.65s (1→10 users)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Long Context (256K tokens):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Single user: 22.0 tok/s&lt;/li&gt; &lt;li&gt;10 concurrent users: &lt;strong&gt;115.5 tok/s&lt;/strong&gt; throughput&lt;/li&gt; &lt;li&gt;Latency: 22.7s → 43.2s (1→10 users)&lt;/li&gt; &lt;li&gt;Still able to handle 10 concurrent requests!&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Sweet Spot (32K-64K context):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;64K @ 10 users: 311 tok/s total, 31 tok/s per user&lt;/li&gt; &lt;li&gt;32K @ 10 users: 413 tok/s total, 41 tok/s per user&lt;/li&gt; &lt;li&gt;Best balance of context length and throughput&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;FP8 quantization really shines here - getting 115 tok/s aggregate at 256K context with 10 users is wild, even with the power constraint. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/x9t4ttsvrgvf1.png?width=7590&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0c86bf3cc42032a595ee4d02b2c78986da150836"&gt;https://preview.redd.it/x9t4ttsvrgvf1.png?width=7590&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0c86bf3cc42032a595ee4d02b2c78986da150836&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/notaDestroyer"&gt; /u/notaDestroyer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o84b36/qwen330ba3b_fp8_on_rtx_pro_6000_blackwell_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o84b36/qwen330ba3b_fp8_on_rtx_pro_6000_blackwell_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o84b36/qwen330ba3b_fp8_on_rtx_pro_6000_blackwell_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T12:04:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1o81rvs</id>
    <title>Google C2S-Scale 27B (based on Gemma) built with Yale generated a novel hypothesis about cancer cellular behavior - Model + resources are now on Hugging Face and GitHub</title>
    <updated>2025-10-16T09:41:04+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o81rvs/google_c2sscale_27b_based_on_gemma_built_with/"&gt; &lt;img alt="Google C2S-Scale 27B (based on Gemma) built with Yale generated a novel hypothesis about cancer cellular behavior - Model + resources are now on Hugging Face and GitHub" src="https://b.thumbs.redditmedia.com/h0BE1gNO8S-6xv6b1X5IIoHb8CSHWZoS7YxS0LFbuxA.jpg" title="Google C2S-Scale 27B (based on Gemma) built with Yale generated a novel hypothesis about cancer cellular behavior - Model + resources are now on Hugging Face and GitHub" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Blog post: How a Gemma model helped discover a new potential cancer therapy pathway - We’re launching a new 27 billion parameter foundation model for single-cell analysis built on the Gemma family of open models.: &lt;a href="https://blog.google/technology/ai/google-gemma-ai-cancer-therapy-discovery/"&gt;https://blog.google/technology/ai/google-gemma-ai-cancer-therapy-discovery/&lt;/a&gt;&lt;br /&gt; Hugging Face: &lt;a href="https://huggingface.co/vandijklab/C2S-Scale-Gemma-2-27B"&gt;https://huggingface.co/vandijklab/C2S-Scale-Gemma-2-27B&lt;/a&gt;&lt;br /&gt; Scientific preprint on bioRxiv: &lt;a href="https://www.biorxiv.org/content/10.1101/2025.04.14.648850v2"&gt;https://www.biorxiv.org/content/10.1101/2025.04.14.648850v2&lt;/a&gt;&lt;br /&gt; Code on GitHub: &lt;a href="https://github.com/vandijklab/cell2sentence"&gt;https://github.com/vandijklab/cell2sentence&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1o81rvs"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o81rvs/google_c2sscale_27b_based_on_gemma_built_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o81rvs/google_c2sscale_27b_based_on_gemma_built_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T09:41:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8d3gt</id>
    <title>Internship with local LLMs at AMD!</title>
    <updated>2025-10-16T17:50:36+00:00</updated>
    <author>
      <name>/u/dholanda_amd</name>
      <uri>https://old.reddit.com/user/dholanda_amd</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi folks! &lt;/p&gt; &lt;p&gt;My team and I at AMD have been having a lot of fun developing agents, building next-gen apps for local LLMs, fine-tuning models, and posting a lot of that here on &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;) . We’re now looking for a (ideally grad) student who loves hands-on local AI for an internship on our team.&lt;/p&gt; &lt;p&gt;Our team really tries to contribute quite a bit to the open source community. One of our key projects is &lt;a href="https://github.com/lemonade-sdk/lemonade"&gt;Lemonade &lt;/a&gt;(Ollama-like local app with a really cool Discord community).&lt;/p&gt; &lt;p&gt;Here is the rough description of what we envision for this position:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Develop an agentic LLM framework, designed to operate effectively on client devices&lt;/li&gt; &lt;li&gt;Build and refine the framework by developing a focused application (from computer use to database reasoning - your choice!)&lt;/li&gt; &lt;li&gt;Experiment with fine-tuning, LoRAs, RAG, and agent architectures&lt;/li&gt; &lt;li&gt;Work side-by-side with the Lemonade team =D&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Experience with some of the above (e.g., fine-tuning) is a huge bonus. We also love people who are active on open-source GitHub projects, Hugging Face, and of course &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; ;)&lt;/p&gt; &lt;p&gt;If you’re excited about this opportunity with local AI, let’s chat! Please apply using the link below. Please also feel free to ask questions here or DM me on Discord (look for Daniel H).&lt;/p&gt; &lt;p&gt;Excited to hear from this community!&lt;/p&gt; &lt;p&gt;Details here: careers (dot) amd (dot) com/careers-home/jobs/70208&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dholanda_amd"&gt; /u/dholanda_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8d3gt/internship_with_local_llms_at_amd/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8d3gt/internship_with_local_llms_at_amd/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8d3gt/internship_with_local_llms_at_amd/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T17:50:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8h9ye</id>
    <title>China's GPU Competition: 96GB Huawei Atlas 300I Duo Dual-GPU Tear-Down</title>
    <updated>2025-10-16T20:26:18+00:00</updated>
    <author>
      <name>/u/sub_RedditTor</name>
      <uri>https://old.reddit.com/user/sub_RedditTor</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8h9ye/chinas_gpu_competition_96gb_huawei_atlas_300i_duo/"&gt; &lt;img alt="China's GPU Competition: 96GB Huawei Atlas 300I Duo Dual-GPU Tear-Down" src="https://external-preview.redd.it/h0AXu77KNbLZvwy9Mngk-UUEgeAjkwRBjGOdlujFn7U.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=436c9e5446c362e3900f824dd945b76349b991ff" title="China's GPU Competition: 96GB Huawei Atlas 300I Duo Dual-GPU Tear-Down" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We need benchmarks ..&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sub_RedditTor"&gt; /u/sub_RedditTor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/qGe_fq68x-Q?si=0w_3qjLvmHTXPWxI"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8h9ye/chinas_gpu_competition_96gb_huawei_atlas_300i_duo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8h9ye/chinas_gpu_competition_96gb_huawei_atlas_300i_duo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T20:26:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8c9ta</id>
    <title>new 1B LLM by meta</title>
    <updated>2025-10-16T17:20:06+00:00</updated>
    <author>
      <name>/u/BreakfastFriendly728</name>
      <uri>https://old.reddit.com/user/BreakfastFriendly728</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8c9ta/new_1b_llm_by_meta/"&gt; &lt;img alt="new 1B LLM by meta" src="https://external-preview.redd.it/jpWPa_miOIzMa-yE2f-cKYYuEKEP1Bglx0QJxru_lGs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7e2417b6932d5a1fcaf49ac7d2561558b6a9873e" title="new 1B LLM by meta" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/5adjo04ecivf1.png?width=825&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c09b47aee7f7597d580d6dbe1eec7e6429504d41"&gt;https://preview.redd.it/5adjo04ecivf1.png?width=825&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c09b47aee7f7597d580d6dbe1eec7e6429504d41&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/facebook/MobileLLM-Pro"&gt;facebook/MobileLLM-Pro · Hugging Face&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BreakfastFriendly728"&gt; /u/BreakfastFriendly728 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8c9ta/new_1b_llm_by_meta/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8c9ta/new_1b_llm_by_meta/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8c9ta/new_1b_llm_by_meta/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T17:20:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8b00e</id>
    <title>GLM 4.6 is hilarious, I wish I could run this on my own PC lol</title>
    <updated>2025-10-16T16:34:48+00:00</updated>
    <author>
      <name>/u/Cool-Chemical-5629</name>
      <uri>https://old.reddit.com/user/Cool-Chemical-5629</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Prompt:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;I have a fun task for you! Search the web to find the most (infamously) popular annoying LLM clichés (aka AI slop) in roleplay and creative writing and generate a sentence using only those phrases. Make the sentence as long as possible to cover them all.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;GLM 4.6 response:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Based on the search results, I've compiled the most notoriously annoying LLM clichés in roleplay and creative writing. Here's a sentence that combines as many of these overused phrases as possible:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&amp;quot;In a world where the tapestry of human emotions unfolds like a delicate symphony, I'm sorry, but as an AI language model, I cannot delve into unlocking your full potential without first embarking on a journey to harness the vast landscape of creativity, though I'm just an AI and don't have feelings, but I'm here to help you explore the intricate dance of storytelling where every thread weaves a new chapter in the grand narrative of imagination.&amp;quot;&lt;/strong&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cool-Chemical-5629"&gt; /u/Cool-Chemical-5629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8b00e/glm_46_is_hilarious_i_wish_i_could_run_this_on_my/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8b00e/glm_46_is_hilarious_i_wish_i_could_run_this_on_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8b00e/glm_46_is_hilarious_i_wish_i_could_run_this_on_my/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T16:34:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1o866vl</id>
    <title>PaddleOCR-VL, is better than private models</title>
    <updated>2025-10-16T13:29:48+00:00</updated>
    <author>
      <name>/u/Illustrious-Swim9663</name>
      <uri>https://old.reddit.com/user/Illustrious-Swim9663</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o866vl/paddleocrvl_is_better_than_private_models/"&gt; &lt;img alt="PaddleOCR-VL, is better than private models" src="https://b.thumbs.redditmedia.com/X4U2z8D2mUefEIEuBe11hVIGCJrPT-oF1EOiA8c0dyw.jpg" title="PaddleOCR-VL, is better than private models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/PaddlePaddle/status/1978809999263781290?t=mcHYAF7osq3MmicjMLi0IQ&amp;amp;s=19"&gt;https://x.com/PaddlePaddle/status/1978809999263781290?t=mcHYAF7osq3MmicjMLi0IQ&amp;amp;s=19&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Illustrious-Swim9663"&gt; /u/Illustrious-Swim9663 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1o866vl"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o866vl/paddleocrvl_is_better_than_private_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o866vl/paddleocrvl_is_better_than_private_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T13:29:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8ay34</id>
    <title>GLM 4.6 air when?</title>
    <updated>2025-10-16T16:32:48+00:00</updated>
    <author>
      <name>/u/gamblingapocalypse</name>
      <uri>https://old.reddit.com/user/gamblingapocalypse</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8ay34/glm_46_air_when/"&gt; &lt;img alt="GLM 4.6 air when?" src="https://preview.redd.it/1bhgri4w3ivf1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=660b0d931d85bfb508e26c3e8647b3b8a75f8708" title="GLM 4.6 air when?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gamblingapocalypse"&gt; /u/gamblingapocalypse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1bhgri4w3ivf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8ay34/glm_46_air_when/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8ay34/glm_46_air_when/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T16:32:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
