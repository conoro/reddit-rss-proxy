<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-24T07:09:27+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1rc59ze</id>
    <title>Qwen3's most underrated feature: Voice embeddings</title>
    <updated>2026-02-23T02:28:32+00:00</updated>
    <author>
      <name>/u/k_means_clusterfuck</name>
      <uri>https://old.reddit.com/user/k_means_clusterfuck</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rc59ze/qwen3s_most_underrated_feature_voice_embeddings/"&gt; &lt;img alt="Qwen3's most underrated feature: Voice embeddings" src="https://preview.redd.it/zmcs7iysm5lg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=796016e685c536fbab1ce49b5fec35afeb75f40e" title="Qwen3's most underrated feature: Voice embeddings" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Did you know that Qwen3 TTS utilizes voice embedding for voice cloning?&lt;br /&gt; Your voice is turned into a vector of 1024 dimensions (or 2048 for 1.7b), and based on this vector alone you can get your custom voice.&lt;/p&gt; &lt;p&gt;But the coolest part is that this means that you can use math to modify voices, average voices. You can swap gender, pitch, mix and match voices, and even create an emotion space! This also enables semantic voice search!&lt;/p&gt; &lt;p&gt;The voice embedding model is actually just a tiny encoder with just a few million parameters. I've ripped it out of the voice embedding model so you can use the embedding model standalone. Check out my collection! :D I also have onnx models for optimized web / front-end inference.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/marksverdhei/qwen3-voice-embedding"&gt;https://huggingface.co/collections/marksverdhei/qwen3-voice-embedding&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Voice embedings can be used for inference in my vllm-omni fork until it is supported in upstream: &lt;a href="https://github.com/heiervang-technologies/ht-vllm-omni"&gt;https://github.com/heiervang-technologies/ht-vllm-omni&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/k_means_clusterfuck"&gt; /u/k_means_clusterfuck &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zmcs7iysm5lg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rc59ze/qwen3s_most_underrated_feature_voice_embeddings/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rc59ze/qwen3s_most_underrated_feature_voice_embeddings/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T02:28:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcc2fa</id>
    <title>An open-source framework to achieve Gemini 3 Deep Think / GPT-5.2 Pro level performance with local models scaffolding</title>
    <updated>2026-02-23T08:33:22+00:00</updated>
    <author>
      <name>/u/Ryoiki-Tokuiten</name>
      <uri>https://old.reddit.com/user/Ryoiki-Tokuiten</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcc2fa/an_opensource_framework_to_achieve_gemini_3_deep/"&gt; &lt;img alt="An open-source framework to achieve Gemini 3 Deep Think / GPT-5.2 Pro level performance with local models scaffolding" src="https://preview.redd.it/wfpxmbhlc7lg1.png?width=140&amp;amp;height=99&amp;amp;auto=webp&amp;amp;s=48d6dec8f7b1fd1a11e8a1b5afbd51040b6a5021" title="An open-source framework to achieve Gemini 3 Deep Think / GPT-5.2 Pro level performance with local models scaffolding" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ryoiki-Tokuiten"&gt; /u/Ryoiki-Tokuiten &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1rcc2fa"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcc2fa/an_opensource_framework_to_achieve_gemini_3_deep/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rcc2fa/an_opensource_framework_to_achieve_gemini_3_deep/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T08:33:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1rco9v7</id>
    <title>RWKV-7: O(1) memory inference, 16.39 tok/s on ARM Cortex-A76, beats LLaMA 3.2 3B. The local-first architecture nobody is talking about...</title>
    <updated>2026-02-23T17:45:31+00:00</updated>
    <author>
      <name>/u/Sensitive-Two9732</name>
      <uri>https://old.reddit.com/user/Sensitive-Two9732</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Wrote a deep-dive specifically because the deployment numbers don't get enough attention.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;FREE MEDIUM LINK&lt;/strong&gt;: &lt;a href="https://ai.gopubby.com/rwkv-7-beats-llama-3-2-rnn-constant-memory-46064bbf1f64?sk=c2e60e9b74b726d8697dbabc220cbbf4"&gt;https://ai.gopubby.com/rwkv-7-beats-llama-3-2-rnn-constant-memory-46064bbf1f64?sk=c2e60e9b74b726d8697dbabc220cbbf4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The headline stats for local inference:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;O(1) memory per token, no KV cache at all. Context length does not affect VRAM usage.&lt;/li&gt; &lt;li&gt;16.39 tok/s on ARM Cortex-A76 (7B model). That's a mid-range Android chip.&lt;/li&gt; &lt;li&gt;28.7 tok/s on Snapdragon X Elite (7B). Current-gen Windows on ARM.&lt;/li&gt; &lt;li&gt;RWKV-X hybrid: 1.37x faster than Flash Attention v3 at 128K context.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Microsoft already ships Eagle v5 (RWKV-based) on ~1.5 billion Windows machines for on-device tasks. No cloud round-trip.&lt;/p&gt; &lt;p&gt;The compression stack: 4-bit quantized RWKV-7 0.1B runs on microcontrollers. The state size is fixed regardless of how long the conversation runs. For local-first deployment this is a fundamentally different proposition than fitting a Transformer's growing KV cache into limited VRAM.&lt;/p&gt; &lt;p&gt;Weights (Apache 2.0): &lt;a href="https://huggingface.co/collections/RWKV/rwkv-v7"&gt;https://huggingface.co/collections/RWKV/rwkv-v7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to discuss about this. :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sensitive-Two9732"&gt; /u/Sensitive-Two9732 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://medium.com/ai-advances/rwkv-7-beats-llama-3-2-rnn-constant-memory-46064bbf1f64"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rco9v7/rwkv7_o1_memory_inference_1639_toks_on_arm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rco9v7/rwkv7_o1_memory_inference_1639_toks_on_arm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T17:45:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1rd8nr7</id>
    <title>Andrej Karpathy survived the weekend with the claws</title>
    <updated>2026-02-24T06:21:50+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rd8nr7/andrej_karpathy_survived_the_weekend_with_the/"&gt; &lt;img alt="Andrej Karpathy survived the weekend with the claws" src="https://preview.redd.it/zi27d0r9ydlg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4005111dda515a576e1b3fee84166b2abc69893d" title="Andrej Karpathy survived the weekend with the claws" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;reference: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1raq23i/they_have_karpathy_we_are_doomed/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1raq23i/they_have_karpathy_we_are_doomed/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zi27d0r9ydlg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rd8nr7/andrej_karpathy_survived_the_weekend_with_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rd8nr7/andrej_karpathy_survived_the_weekend_with_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T06:21:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1rd7kw7</id>
    <title>LLM vs LLM harness</title>
    <updated>2026-02-24T05:25:29+00:00</updated>
    <author>
      <name>/u/chitown160</name>
      <uri>https://old.reddit.com/user/chitown160</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rd7kw7/llm_vs_llm_harness/"&gt; &lt;img alt="LLM vs LLM harness" src="https://preview.redd.it/umedeqhzndlg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aa6e250fe08179dba92d9ff97f79b9bd1973be08" title="LLM vs LLM harness" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We have the capable distillations - let's continue to build out the harnesses &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chitown160"&gt; /u/chitown160 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/umedeqhzndlg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rd7kw7/llm_vs_llm_harness/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rd7kw7/llm_vs_llm_harness/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T05:25:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcuaip</id>
    <title>Serious question: do you think Dario (or any other major AI players or political players) have enough power and influence that they will get Chinese local AI and/or local AI in general banned in the U.S.? What do you think the odds are?</title>
    <updated>2026-02-23T21:19:33+00:00</updated>
    <author>
      <name>/u/DeepOrangeSky</name>
      <uri>https://old.reddit.com/user/DeepOrangeSky</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I guess I'll put Dario in the title, since he's the most relevant hater of the day, and I guess fairly powerful in regards to this as far as any one specific guy goes, but, obviously if something like this happened, it would involve a lot more people combining their powers than just Dario alone.&lt;/p&gt; &lt;p&gt;Anyway, curious what you think the odds are that this actually happens. And if you were puttings odds per timescale, what would you say (like odds it happens in 2026, vs happens in next 2 years, vs next 3 years, vs never happens at all).&lt;/p&gt; &lt;p&gt;And you can divide the scenarios, like just specifically Chinese local AI (but not non-Chinese local AI) vs just all local AI of any kind (even American), etc.&lt;/p&gt; &lt;p&gt;I wonder if there is about to be a huge run on Seagate and WD hdds where they sell out like crazy that dwarfs even that big openclaw-related run on Mac minis that happened a few weeks ago, as everyone starts trying to hoard a bunch of different quants of all the best open models and even a bunch of quants and versions of all the biggest DeepSeek, GLM, and Kimi ones that they don't even necessarily have enough ram to run yet to future-proof in case it all goes away? Time to buy a bunch of Seagate stock?&lt;/p&gt; &lt;p&gt;Kind of joking about the Seagate aspect, since not that many people use open-weights ai rn, obv, but, anyway, wondering how serious you all think the odds are about the local stuff getting banned&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeepOrangeSky"&gt; /u/DeepOrangeSky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcuaip/serious_question_do_you_think_dario_or_any_other/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcuaip/serious_question_do_you_think_dario_or_any_other/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rcuaip/serious_question_do_you_think_dario_or_any_other/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T21:19:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcyy8j</id>
    <title>What models are you eagerly anticipating or wishing for?</title>
    <updated>2026-02-24T00:17:50+00:00</updated>
    <author>
      <name>/u/jinnyjuice</name>
      <uri>https://old.reddit.com/user/jinnyjuice</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just out of curiosity, I've been wishing for three particular LLMs, and curious what other people are wishing for also.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jinnyjuice"&gt; /u/jinnyjuice &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcyy8j/what_models_are_you_eagerly_anticipating_or/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcyy8j/what_models_are_you_eagerly_anticipating_or/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rcyy8j/what_models_are_you_eagerly_anticipating_or/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T00:17:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcy5wv</id>
    <title>Qwen 3 coder next ud-q8-xl F16 filling up the two orin rpc mesh!</title>
    <updated>2026-02-23T23:47:24+00:00</updated>
    <author>
      <name>/u/braydon125</name>
      <uri>https://old.reddit.com/user/braydon125</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcy5wv/qwen_3_coder_next_udq8xl_f16_filling_up_the_two/"&gt; &lt;img alt="Qwen 3 coder next ud-q8-xl F16 filling up the two orin rpc mesh!" src="https://external-preview.redd.it/aWVhZHNnaXl6YmxnMWrUBUxyMMPidJm6SSrNb-W9WQcIAZj84NetEddKYJ3y.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=676552353f7d7812a39c29ee412cf53fe6f31836" title="Qwen 3 coder next ud-q8-xl F16 filling up the two orin rpc mesh!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;running great and as you can see here llama.cpp -fit is doing a great job at splitting this evenly . the largest piece of traffic between these two during initial tensor transfer was &amp;lt;5Gbps&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/braydon125"&gt; /u/braydon125 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/hvlsxvdyzblg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcy5wv/qwen_3_coder_next_udq8xl_f16_filling_up_the_two/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rcy5wv/qwen_3_coder_next_udq8xl_f16_filling_up_the_two/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T23:47:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1rclyvf</id>
    <title>Portable Workstation for Inference</title>
    <updated>2026-02-23T16:24:21+00:00</updated>
    <author>
      <name>/u/neintailedfoxx</name>
      <uri>https://old.reddit.com/user/neintailedfoxx</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rclyvf/portable_workstation_for_inference/"&gt; &lt;img alt="Portable Workstation for Inference" src="https://preview.redd.it/j59qyq8sq9lg1.jpg?width=140&amp;amp;height=64&amp;amp;auto=webp&amp;amp;s=f979ae081b50b775630080b53aa9f61d422aa5ed" title="Portable Workstation for Inference" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Built a new portable workstation for gaming/AI workloads. One of the fans is a 12018 fan bought from aliexpress derived from a fan on the 4090FE, allowing it to provide airflow equivalent to normal 25mm thick fans despite only being 18mm in thickness.&lt;/p&gt; &lt;p&gt;Would've loved to get a Threadripper for additional memory bandwidth, but sadly there aren't any itx Threadripper boards :(&lt;/p&gt; &lt;p&gt;Getting around 150-165 tok/sec running GPT OSS 120B with max context length in LM Studio (Using windows, haven't had time to test in linux yet)&lt;/p&gt; &lt;p&gt;CPU is undervolted using the curve optimizer (-25/-30 per CCD CO) with a +200MHz PBO clock offset, RAM is tuned to 6000MT/s CL28-36-35-30 @ 2233MHz FCLK, and the GPU is undervolted to 0.89v@2700MHz and power limited to 500w.&lt;/p&gt; &lt;p&gt;Temps are good, with the cpu reaching a max temp of around 75c and the GPU never going above 80c even during extremely heavy workloads. Top fans are set to intake, providing airflow to the flipped GPU.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Case:&lt;/strong&gt; FormD T1 2.5 Gunmetal w/ Flipped Travel Kit&lt;/p&gt; &lt;p&gt;&lt;strong&gt;CPU:&lt;/strong&gt; AMD Ryzen 9 9950X3D&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GPU:&lt;/strong&gt; NVIDIA RTX PRO 6000 Workstation Edition&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Motherboard:&lt;/strong&gt; MSI MPG X870I EDGE TI EVO WIFI&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Ram:&lt;/strong&gt; TEAMGROUP T-Force Delta RGB 96 GB DDR5-6800 CL36&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Storage:&lt;/strong&gt; Crucial T710 4TB, Samsung 990 Pro 4TB, WD Black SN850X 8TB, TEAMGROUP CX2 2TB (Used drives from my previous build since I definitely won't be able to afford all this storage at current prices)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;PSU:&lt;/strong&gt; Corsair SF1000&lt;/p&gt; &lt;p&gt;&lt;strong&gt;PSU Cables:&lt;/strong&gt; Custom Cables from Dreambigbyray&lt;/p&gt; &lt;p&gt;&lt;strong&gt;CPU Cooler:&lt;/strong&gt; CM Masterliquid 240 ATMOS Stealth&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/neintailedfoxx"&gt; /u/neintailedfoxx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1rclyvf"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rclyvf/portable_workstation_for_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rclyvf/portable_workstation_for_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T16:24:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1rd2cdu</id>
    <title>Round 2: Quick MoE quantization comparison: LFM2-8B-A1B, OLMoE-1B-7B-0924-Instruct, granite-4.0-h-tiny</title>
    <updated>2026-02-24T02:28:32+00:00</updated>
    <author>
      <name>/u/TitwitMuffbiscuit</name>
      <uri>https://old.reddit.com/user/TitwitMuffbiscuit</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rd2cdu/round_2_quick_moe_quantization_comparison/"&gt; &lt;img alt="Round 2: Quick MoE quantization comparison: LFM2-8B-A1B, OLMoE-1B-7B-0924-Instruct, granite-4.0-h-tiny" src="https://external-preview.redd.it/-Vukn_kZs7j8mYYOkjXqzzndQMLeXSsRtbwQcSicIyc.png?width=140&amp;amp;height=70&amp;amp;auto=webp&amp;amp;s=4298210d6107231f989dfd747860dafb3a1948da" title="Round 2: Quick MoE quantization comparison: LFM2-8B-A1B, OLMoE-1B-7B-0924-Instruct, granite-4.0-h-tiny" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I chose three small, recent, and different MoE models that fit my VRAM for a quick assessment (these are not models I actually use).&lt;/p&gt; &lt;p&gt;The goal is to check on MXFP4 and evaluate the smallest quantization variants.&lt;/p&gt; &lt;p&gt;For the non initiated:&lt;/p&gt; &lt;p&gt;KLD (KL Divergence): Measures &amp;quot;Faithfulness.&amp;quot; It shows how much the quantized model's probability distribution drifts from the original baseline. Lower = closer.&lt;/p&gt; &lt;p&gt;PPL (Perplexity): Measures &amp;quot;Certainty.&amp;quot; It’s the average uncertainty the model feels when predicting the next token. It is derived from the total information loss (Cross Entropy). Lower = more confident&lt;/p&gt; &lt;p&gt;They are correlated. Perplexity measures the total error, KLD measures the relative error. This relationship helps in determining information loss (or gain when training).&lt;/p&gt; &lt;p&gt;Models are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;LFM2-8B-A1B has 4 experts active out of 32.&lt;/li&gt; &lt;li&gt;OLMoE-1B-7B-0924-Instruct has 8 experts active out of 64.&lt;/li&gt; &lt;li&gt;granite-4.0-h-tiny has 6 experts active out of 64.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Conclusion:&lt;/h1&gt; &lt;p&gt;MXFP4 is probably great for QAT (Quantization Aware Training), but it underperforms on speed and quality.&lt;/p&gt; &lt;p&gt;There is no &amp;quot;go-to&amp;quot; quant. If a bunch of them are really close in terms of sizes, &lt;a href="https://github.com/ggml-org/llama.cpp/pull/5076#issue-2093613239"&gt;ideally you'd proceed as is:&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-perplexity -m &amp;lt;fp16_model&amp;gt; -f wiki.test.raw --kl-divergence-base &amp;lt;file_name&amp;gt; [other parameters] llama-perplexity -m &amp;lt;quantized_model&amp;gt; --kl-divergence-base &amp;lt;file_name&amp;gt; --kl-divergence [other parameters] &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Most Desirable Quantization&lt;/h1&gt; &lt;p&gt;The Efficiency Score is the distance to a 'perfect' model (zero size, zero error), the VRAM sweet spot. Efficiency Score: √ (Normalized Size² + Normalized KLD²)&lt;/p&gt; &lt;h1&gt;Model: LFM2-8B-A1B&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Category&lt;/th&gt; &lt;th align="left"&gt;Quantization&lt;/th&gt; &lt;th align="left"&gt;Size (GiB)&lt;/th&gt; &lt;th align="left"&gt;KLD Score&lt;/th&gt; &lt;th align="left"&gt;Eff. Score&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;2-bit&lt;/td&gt; &lt;td align="left"&gt;LFM2-8B-A1B-IQ2_S&lt;/td&gt; &lt;td align="left"&gt;2.327&lt;/td&gt; &lt;td align="left"&gt;0.642566&lt;/td&gt; &lt;td align="left"&gt;0.4002&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;3-bit&lt;/td&gt; &lt;td align="left"&gt;LFM2-8B-A1B-IQ3_M&lt;/td&gt; &lt;td align="left"&gt;3.416&lt;/td&gt; &lt;td align="left"&gt;0.238139&lt;/td&gt; &lt;td align="left"&gt;0.4365&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;4-bit&lt;/td&gt; &lt;td align="left"&gt;LFM2-8B-A1B-Q4_K_S&lt;/td&gt; &lt;td align="left"&gt;4.426&lt;/td&gt; &lt;td align="left"&gt;0.093833&lt;/td&gt; &lt;td align="left"&gt;0.3642&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;5-bit&lt;/td&gt; &lt;td align="left"&gt;LFM2-8B-A1B-Q5_K_S&lt;/td&gt; &lt;td align="left"&gt;5.364&lt;/td&gt; &lt;td align="left"&gt;0.053178&lt;/td&gt; &lt;td align="left"&gt;0.3513&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Model: OLMoE-1B-7B-0924-Instruct&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Category&lt;/th&gt; &lt;th align="left"&gt;Quantization&lt;/th&gt; &lt;th align="left"&gt;Size (GiB)&lt;/th&gt; &lt;th align="left"&gt;KLD Score&lt;/th&gt; &lt;th align="left"&gt;Eff. Score&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;2-bit&lt;/td&gt; &lt;td align="left"&gt;OLMoE-1B-7B-0924-Instruct-IQ2_S&lt;/td&gt; &lt;td align="left"&gt;1.985&lt;/td&gt; &lt;td align="left"&gt;0.438407&lt;/td&gt; &lt;td align="left"&gt;0.4806&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;3-bit&lt;/td&gt; &lt;td align="left"&gt;OLMoE-1B-7B-0924-Instruct-IQ3_M&lt;/td&gt; &lt;td align="left"&gt;2.865&lt;/td&gt; &lt;td align="left"&gt;0.122599&lt;/td&gt; &lt;td align="left"&gt;0.5011&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;4-bit&lt;/td&gt; &lt;td align="left"&gt;OLMoE-1B-7B-0924-Instruct-IQ4_XS&lt;/td&gt; &lt;td align="left"&gt;3.460&lt;/td&gt; &lt;td align="left"&gt;0.052616&lt;/td&gt; &lt;td align="left"&gt;0.3509&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;5-bit&lt;/td&gt; &lt;td align="left"&gt;OLMoE-1B-7B-0924-Instruct-Q5_K_S&lt;/td&gt; &lt;td align="left"&gt;4.452&lt;/td&gt; &lt;td align="left"&gt;0.019071&lt;/td&gt; &lt;td align="left"&gt;0.3044&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Model: granite-4.0-h-tiny&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Category&lt;/th&gt; &lt;th align="left"&gt;Quantization&lt;/th&gt; &lt;th align="left"&gt;Size (GiB)&lt;/th&gt; &lt;th align="left"&gt;KLD Score&lt;/th&gt; &lt;th align="left"&gt;Eff. Score&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;2-bit&lt;/td&gt; &lt;td align="left"&gt;granite-4.0-h-tiny-IQ2_S&lt;/td&gt; &lt;td align="left"&gt;1.967&lt;/td&gt; &lt;td align="left"&gt;0.519907&lt;/td&gt; &lt;td align="left"&gt;0.4871&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;3-bit&lt;/td&gt; &lt;td align="left"&gt;granite-4.0-h-tiny-IQ3_XS&lt;/td&gt; &lt;td align="left"&gt;2.716&lt;/td&gt; &lt;td align="left"&gt;0.156308&lt;/td&gt; &lt;td align="left"&gt;0.4064&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;4-bit&lt;/td&gt; &lt;td align="left"&gt;granite-4.0-h-tiny-Q4_K_S&lt;/td&gt; &lt;td align="left"&gt;3.721&lt;/td&gt; &lt;td align="left"&gt;0.044464&lt;/td&gt; &lt;td align="left"&gt;0.4086&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;5-bit&lt;/td&gt; &lt;td align="left"&gt;granite-4.0-h-tiny-Q5_K_S&lt;/td&gt; &lt;td align="left"&gt;4.480&lt;/td&gt; &lt;td align="left"&gt;0.020204&lt;/td&gt; &lt;td align="left"&gt;0.2934&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fhljt1hisclg1.png?width=2779&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=75ec60955714ab6bcfdd0093a6ad7950b7d82e1b"&gt;https://preview.redd.it/fhljt1hisclg1.png?width=2779&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=75ec60955714ab6bcfdd0093a6ad7950b7d82e1b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ans3msbjsclg1.png?width=2779&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=89dd1c56310e5e3f3a21dc8e6299a879d0d344b7"&gt;https://preview.redd.it/ans3msbjsclg1.png?width=2779&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=89dd1c56310e5e3f3a21dc8e6299a879d0d344b7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/4kl1epyjsclg1.png?width=2780&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0b5c46e618b04fd756b93141f3a8999689ba7cc5"&gt;https://preview.redd.it/4kl1epyjsclg1.png?width=2780&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0b5c46e618b04fd756b93141f3a8999689ba7cc5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/h2tplhoksclg1.png?width=2496&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=900b52f0ece7d7abfa39081f2fd08380ff964b77"&gt;https://preview.redd.it/h2tplhoksclg1.png?width=2496&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=900b52f0ece7d7abfa39081f2fd08380ff964b77&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/asfqio9lsclg1.png?width=2496&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bdf1dbb1316a958ea59fb4d1a241aa906f0cc5c9"&gt;https://preview.redd.it/asfqio9lsclg1.png?width=2496&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bdf1dbb1316a958ea59fb4d1a241aa906f0cc5c9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/lj6ih2plsclg1.png?width=2496&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=72ad13d1354a0f26bf79162d5a33d7c83b9299ca"&gt;https://preview.redd.it/lj6ih2plsclg1.png?width=2496&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=72ad13d1354a0f26bf79162d5a33d7c83b9299ca&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Data:&lt;/h1&gt; &lt;h1&gt;LFM2-8B-A1B&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Quantization&lt;/th&gt; &lt;th align="left"&gt;Size (GiB)&lt;/th&gt; &lt;th align="left"&gt;PPL Score&lt;/th&gt; &lt;th align="left"&gt;KLD Score&lt;/th&gt; &lt;th align="left"&gt;Prompt (t/s)&lt;/th&gt; &lt;th align="left"&gt;Gen (t/s)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;LFM2-8B-A1B-IQ1_S&lt;/td&gt; &lt;td align="left"&gt;1.608&lt;/td&gt; &lt;td align="left"&gt;45.621441&lt;/td&gt; &lt;td align="left"&gt;1.974797&lt;/td&gt; &lt;td align="left"&gt;3590.05&lt;/td&gt; &lt;td align="left"&gt;228.60&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;LFM2-8B-A1B-IQ1_M&lt;/td&gt; &lt;td align="left"&gt;1.784&lt;/td&gt; &lt;td align="left"&gt;29.489175&lt;/td&gt; &lt;td align="left"&gt;1.472739&lt;/td&gt; &lt;td align="left"&gt;2288.06&lt;/td&gt; &lt;td align="left"&gt;208.50&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;LFM2-8B-A1B-IQ2_XXS&lt;/td&gt; &lt;td align="left"&gt;2.076&lt;/td&gt; &lt;td align="left"&gt;23.013295&lt;/td&gt; &lt;td align="left"&gt;1.053110&lt;/td&gt; &lt;td align="left"&gt;3830.70&lt;/td&gt; &lt;td align="left"&gt;206.69&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;LFM2-8B-A1B-IQ2_XS&lt;/td&gt; &lt;td align="left"&gt;2.31&lt;/td&gt; &lt;td align="left"&gt;19.658691&lt;/td&gt; &lt;td align="left"&gt;0.798374&lt;/td&gt; &lt;td align="left"&gt;3301.04&lt;/td&gt; &lt;td align="left"&gt;204.26&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;LFM2-8B-A1B-IQ2_S&lt;/td&gt; &lt;td align="left"&gt;2.327&lt;/td&gt; &lt;td align="left"&gt;17.572654&lt;/td&gt; &lt;td align="left"&gt;0.642566&lt;/td&gt; &lt;td align="left"&gt;3336.55&lt;/td&gt; &lt;td align="left"&gt;203.08&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;LFM2-8B-A1B-IQ2_M&lt;/td&gt; &lt;td align="left"&gt;2.561&lt;/td&gt; &lt;td align="left"&gt;17.607493&lt;/td&gt; &lt;td align="left"&gt;0.509741&lt;/td&gt; &lt;td align="left"&gt;3351.58&lt;/td&gt; &lt;td align="left"&gt;201.59&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;LFM2-8B-A1B-Q2_K_S&lt;/td&gt; &lt;td align="left"&gt;2.65&lt;/td&gt; &lt;td align="left"&gt;16.463740&lt;/td&gt; &lt;td align="left"&gt;0.640123&lt;/td&gt; &lt;td align="left"&gt;2938.68&lt;/td&gt; &lt;td align="left"&gt;208.57&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;LFM2-8B-A1B-Q2_K&lt;/td&gt; &lt;td align="left"&gt;2.868&lt;/td&gt; &lt;td align="left"&gt;16.676304&lt;/td&gt; &lt;td align="left"&gt;0.511999&lt;/td&gt; &lt;td align="left"&gt;3068.25&lt;/td&gt; &lt;td align="left"&gt;185.35&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;LFM2-8B-A1B-IQ3_XXS&lt;/td&gt; &lt;td align="left"&gt;3.019&lt;/td&gt; &lt;td align="left"&gt;15.865102&lt;/td&gt; &lt;td align="left"&gt;0.358869&lt;/td&gt; &lt;td align="left"&gt;3784.91&lt;/td&gt; &lt;td align="left"&gt;197.37&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;LFM2-8B-A1B-IQ3_XS&lt;/td&gt; &lt;td align="left"&gt;3.208&lt;/td&gt; &lt;td align="left"&gt;19.160402&lt;/td&gt; &lt;td align="left"&gt;0.390083&lt;/td&gt; &lt;td align="left"&gt;3743.55&lt;/td&gt; &lt;td align="left"&gt;190.98&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;LFM2-8B-A1B-IQ3_S&lt;/td&gt; &lt;td align="left"&gt;3.394&lt;/td&gt; &lt;td align="left"&gt;19.454378&lt;/td&gt; &lt;td align="left"&gt;0.372152&lt;/td&gt; &lt;td align="left"&gt;3718.99&lt;/td&gt; &lt;td align="left"&gt;186.42&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;LFM2-8B-A1B-Q3_K_S&lt;/td&gt; &lt;td align="left"&gt;3.394&lt;/td&gt; &lt;td align="left"&gt;17.166892&lt;/td&gt; &lt;td align="left"&gt;0.314452&lt;/td&gt; &lt;td align="left"&gt;3439.32&lt;/td&gt; &lt;td align="left"&gt;146.93&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;LFM2-8B-A1B-IQ3_M&lt;/td&gt; &lt;td align="left"&gt;3.416&lt;/td&gt; &lt;td align="left"&gt;16.149280&lt;/td&gt; &lt;td align="left"&gt;0.238139&lt;/td&gt; &lt;td align="left"&gt;3715.21&lt;/td&gt; &lt;td align="left"&gt;187.17&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;LFM2-8B-A1B-Q3_K_M&lt;/td&gt; &lt;td align="left"&gt;3.723&lt;/td&gt; &lt;td align="left"&gt;16.100256&lt;/td&gt; &lt;td align="left"&gt;0.208292&lt;/td&gt; &lt;td align="left"&gt;3537.28&lt;/td&gt; &lt;td align="left"&gt;162.56&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;LFM2-8B-A1B-Q3_K_L&lt;/td&gt; &lt;td align="left"&gt;4.029&lt;/td&gt; &lt;td align="left"&gt;16.613555&lt;/td&gt; &lt;td align="left"&gt;0.202567&lt;/td&gt; &lt;td align="left"&gt;3510.97&lt;/td&gt; &lt;td align="left"&gt;161.20&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;LFM2-8B-A1B-IQ4_XS&lt;/td&gt; &lt;td align="left"&gt;4.17&lt;/td&gt; &lt;td align="left"&gt;15.570913&lt;/td&gt; &lt;td align="left"&gt;0.116939&lt;/td&gt; &lt;td align="left"&gt;4001.26&lt;/td&gt; &lt;td align="left"&gt;223.19&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;LFM2-8B-A1B-IQ4_NL&lt;/td&gt; &lt;td align="left"&gt;4.409&lt;/td&gt; &lt;td align="left"&gt;15.736384&lt;/td&gt; &lt;td align="left"&gt;0.122198&lt;/td&gt; &lt;td align="left"&gt;3949.16&lt;/td&gt; &lt;td align="left"&gt;226.59&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;LFM2-8B-A1B-Q4_0&lt;/td&gt; &lt;td align="left"&gt;4.417&lt;/td&gt; &lt;td align="left"&gt;15.083245&lt;/td&gt; &lt;td align="left"&gt;0.141351&lt;/td&gt; &lt;td align="left"&gt;3845.05&lt;/td&gt; &lt;td align="left"&gt;227.72&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;LFM2-8B-A1B-MXFP4_MOE&lt;/td&gt; &lt;td align="left"&gt;4.424&lt;/td&gt; &lt;td align="left"&gt;14.813420&lt;/td&gt; &lt;td align="left"&gt;0.097272&lt;/td&gt; &lt;td align="left"&gt;3834.64&lt;/td&gt; &lt;td align="left"&gt;193.85&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;LFM2-8B-A1B-Q4_K_S&lt;/td&gt; &lt;td align="left"&gt;4.426&lt;/td&gt; &lt;td align="left"&gt;14.975323&lt;/td&gt; &lt;td align="left"&gt;0.093833&lt;/td&gt; &lt;td align="left"&gt;3753.01&lt;/td&gt; &lt;td align="left"&gt;215.15&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;LFM2-8B-A1B-Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;4.698&lt;/td&gt; &lt;td align="left"&gt;15.344388&lt;/td&gt; &lt;td align="left"&gt;0.090284&lt;/td&gt; &lt;td align="left"&gt;3718.73&lt;/td&gt; &lt;td align="left"&gt;208.65&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;LFM2-8B-A1B-Q4_1&lt;/td&gt; &lt;td align="left"&gt;4.886&lt;/td&gt; &lt;td align="left"&gt;15.993623&lt;/td&gt; &lt;td align="left"&gt;0.101227&lt;/td&gt; &lt;td align="left"&gt;3690.23&lt;/td&gt; &lt;td align="left"&gt;227.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;LFM2-8B-A1B-Q5_K_S&lt;/td&gt; &lt;td align="left"&gt;5.364&lt;/td&gt; &lt;td align="left"&gt;15.730543&lt;/td&gt; &lt;td align="left"&gt;0.053178&lt;/td&gt; &lt;td align="left"&gt;3657.42&lt;/td&gt; &lt;td align="left"&gt;204.26&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;LFM2-8B-A1B-Q5_0&lt;/td&gt; &lt;td align="left"&gt;5.372&lt;/td&gt; &lt;td align="left"&gt;14.653431&lt;/td&gt; &lt;td align="left"&gt;0.059156&lt;/td&gt; &lt;td align="left"&gt;3754.58&lt;/td&gt; &lt;td align="left"&gt;210.17&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;LFM2-8B-A1B-Q5_K_M&lt;/td&gt; &lt;td align="left"&gt;5.513&lt;/td&gt; &lt;td align="left"&gt;15.897327&lt;/td&gt; &lt;td align="left"&gt;0.052972&lt;/td&gt; &lt;td align="left"&gt;3635.63&lt;/td&gt; &lt;td align="left"&gt;199.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;LFM2-8B-A1B-Q5_1&lt;/td&gt; &lt;td align="left"&gt;5.841&lt;/td&gt; &lt;td align="left"&gt;15.679663&lt;/td&gt; &lt;td align="left"&gt;0.049940&lt;/td&gt; &lt;td align="left"&gt;3634.15&lt;/td&gt; &lt;td align="left"&gt;205.19&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;LFM2-8B-A1B-Q6_K&lt;/td&gt; &lt;td align="left"&gt;6.379&lt;/td&gt; &lt;td align="left"&gt;15.512109&lt;/td&gt; &lt;td align="left"&gt;0.026724&lt;/td&gt; &lt;td align="left"&gt;3496.41&lt;/td&gt; &lt;td align="left"&gt;172.28&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;LFM2-8B-A1B-Q8_0&lt;/td&gt; &lt;td align="left"&gt;8.259&lt;/td&gt; &lt;td align="left"&gt;15.193068&lt;/td&gt; &lt;td align="left"&gt;0.015443&lt;/td&gt; &lt;td align="left"&gt;3881.61&lt;/td&gt; &lt;td align="left"&gt;159.66&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;OLMoE-1B-7B-0924-Instruct&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Quantization&lt;/th&gt; &lt;th align="left"&gt;Size (GiB)&lt;/th&gt; &lt;th align="left"&gt;PPL Score&lt;/th&gt; &lt;th align="left"&gt;KLD Score&lt;/th&gt; &lt;th align="left"&gt;Prompt (t/s)&lt;/th&gt; &lt;th align="left"&gt;Gen (t/s)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;OLMoE-1B-7B-0924-Instruct-IQ1_S&lt;/td&gt; &lt;td align="left"&gt;1.388&lt;/td&gt; &lt;td align="left"&gt;27.711222&lt;/td&gt; &lt;td align="left"&gt;1.321738&lt;/td&gt; &lt;td align="left"&gt;3666.10&lt;/td&gt; &lt;td align="left"&gt;247.87&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;OLMoE-1B-7B-0924-Instruct-IQ1_M&lt;/td&gt; &lt;td align="left"&gt;1.526&lt;/td&gt; &lt;td align="left"&gt;21.665126&lt;/td&gt; &lt;td align="left"&gt;1.065891&lt;/td&gt; &lt;td align="left"&gt;2346.14&lt;/td&gt; &lt;td align="left"&gt;229.39&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;OLMoE-1B-7B-0924-Instruct-IQ2_XXS&lt;/td&gt; &lt;td align="left"&gt;1.755&lt;/td&gt; &lt;td align="left"&gt;15.855999&lt;/td&gt; &lt;td align="left"&gt;0.687041&lt;/td&gt; &lt;td align="left"&gt;3850.88&lt;/td&gt; &lt;td align="left"&gt;228.62&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;OLMoE-1B-7B-0924-Instruct-IQ2_XS&lt;/td&gt; &lt;td align="left"&gt;1.941&lt;/td&gt; &lt;td align="left"&gt;14.034858&lt;/td&gt; &lt;td align="left"&gt;0.531707&lt;/td&gt; &lt;td align="left"&gt;3438.66&lt;/td&gt; &lt;td align="left"&gt;226.46&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;OLMoE-1B-7B-0924-Instruct-IQ2_S&lt;/td&gt; &lt;td align="left"&gt;1.985&lt;/td&gt; &lt;td align="left"&gt;13.358345&lt;/td&gt; &lt;td align="left"&gt;0.438407&lt;/td&gt; &lt;td align="left"&gt;3463.65&lt;/td&gt; &lt;td align="left"&gt;223.97&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;OLMoE-1B-7B-0924-Instruct-IQ2_M&lt;/td&gt; &lt;td align="left"&gt;2.168&lt;/td&gt; &lt;td align="left"&gt;12.205082&lt;/td&gt; &lt;td align="left"&gt;0.324686&lt;/td&gt; &lt;td align="left"&gt;3512.47&lt;/td&gt; &lt;td align="left"&gt;222.87&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;OLMoE-1B-7B-0924-Instruct-Q2_K_S&lt;/td&gt; &lt;td align="left"&gt;2.23&lt;/td&gt; &lt;td align="left"&gt;13.969774&lt;/td&gt; &lt;td align="left"&gt;0.514164&lt;/td&gt; &lt;td align="left"&gt;3121.66&lt;/td&gt; &lt;td align="left"&gt;236.74&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;OLMoE-1B-7B-0924-Instruct-Q2_K&lt;/td&gt; &lt;td align="left"&gt;2.387&lt;/td&gt; &lt;td align="left"&gt;12.359235&lt;/td&gt; &lt;td align="left"&gt;0.325934&lt;/td&gt; &lt;td align="left"&gt;3235.95&lt;/td&gt; &lt;td align="left"&gt;207.06&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;OLMoE-1B-7B-0924-Instruct-IQ3_XXS&lt;/td&gt; &lt;td align="left"&gt;2.505&lt;/td&gt; &lt;td align="left"&gt;11.502814&lt;/td&gt; &lt;td align="left"&gt;0.229131&lt;/td&gt; &lt;td align="left"&gt;3803.35&lt;/td&gt; &lt;td align="left"&gt;216.86&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;OLMoE-1B-7B-0924-Instruct-IQ3_XS&lt;/td&gt; &lt;td align="left"&gt;2.669&lt;/td&gt; &lt;td align="left"&gt;11.158494&lt;/td&gt; &lt;td align="left"&gt;0.172658&lt;/td&gt; &lt;td align="left"&gt;3801.89&lt;/td&gt; &lt;td align="left"&gt;211.81&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;OLMoE-1B-7B-0924-Instruct-IQ3_S&lt;/td&gt; &lt;td align="left"&gt;2.815&lt;/td&gt; &lt;td align="left"&gt;11.006107&lt;/td&gt; &lt;td align="left"&gt;0.144768&lt;/td&gt; &lt;td align="left"&gt;3770.79&lt;/td&gt; &lt;td align="left"&gt;206.03&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;OLMoE-1B-7B-0924-Instruct-Q3_K_S&lt;/td&gt; &lt;td align="left"&gt;2.815&lt;/td&gt; &lt;td align="left"&gt;10.942114&lt;/td&gt; &lt;td align="left"&gt;0.164096&lt;/td&gt; &lt;td align="left"&gt;3531.76&lt;/td&gt; &lt;td align="left"&gt;172.25&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;OLMoE-1B-7B-0924-Instruct-IQ3_M&lt;/td&gt; &lt;td align="left"&gt;2.865&lt;/td&gt; &lt;td align="left"&gt;10.816384&lt;/td&gt; &lt;td align="left"&gt;0.122599&lt;/td&gt; &lt;td align="left"&gt;3767.94&lt;/td&gt; &lt;td align="left"&gt;211.11&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;OLMoE-1B-7B-0924-Instruct-Q3_K_M&lt;/td&gt; &lt;td align="left"&gt;3.114&lt;/td&gt; &lt;td align="left"&gt;10.577075&lt;/td&gt; &lt;td align="left"&gt;0.095189&lt;/td&gt; &lt;td align="left"&gt;3612.93&lt;/td&gt; &lt;td align="left"&gt;195.99&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;OLMoE-1B-7B-0924-Instruct-Q3_K_L&lt;/td&gt; &lt;td align="left"&gt;3.363&lt;/td&gt; &lt;td align="left"&gt;10.516405&lt;/td&gt; &lt;td align="left"&gt;0.082414&lt;/td&gt; &lt;td align="left"&gt;3588.45&lt;/td&gt; &lt;td align="left"&gt;194.13&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;OLMoE-1B-7B-0924-Instruct-IQ4_XS&lt;/td&gt; &lt;td align="left"&gt;3.46&lt;/td&gt; &lt;td align="left"&gt;10.387316&lt;/td&gt; &lt;td align="left"&gt;0.052616&lt;/td&gt; &lt;td align="left"&gt;4007.51&lt;/td&gt; &lt;td align="left"&gt;243.45&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;OLMoE-1B-7B-0924-Instruct-IQ4_NL&lt;/td&gt; &lt;td align="left"&gt;3.658&lt;/td&gt; &lt;td align="left"&gt;10.390324&lt;/td&gt; &lt;td align="left"&gt;0.051451&lt;/td&gt; &lt;td align="left"&gt;3958.14&lt;/td&gt; &lt;td align="left"&gt;251.91&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;OLMoE-1B-7B-0924-Instruct-MXFP4_MOE&lt;/td&gt; &lt;td align="left"&gt;3.667&lt;/td&gt; &lt;td align="left"&gt;10.899335&lt;/td&gt; &lt;td align="left"&gt;0.076083&lt;/td&gt; &lt;td align="left"&gt;3857.25&lt;/td&gt; &lt;td align="left"&gt;226.36&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;OLMoE-1B-7B-0924-Instruct-Q4_0&lt;/td&gt; &lt;td align="left"&gt;3.674&lt;/td&gt; &lt;td align="left"&gt;10.442592&lt;/td&gt; &lt;td align="left"&gt;0.065409&lt;/td&gt; &lt;td align="left"&gt;3867.65&lt;/td&gt; &lt;td align="left"&gt;247.41&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;OLMoE-1B-7B-0924-Instruct-Q4_K_S&lt;/td&gt; &lt;td align="left"&gt;3.691&lt;/td&gt; &lt;td align="left"&gt;10.368422&lt;/td&gt; &lt;td align="left"&gt;0.045454&lt;/td&gt; &lt;td align="left"&gt;3798.78&lt;/td&gt; &lt;td align="left"&gt;240.97&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;OLMoE-1B-7B-0924-Instruct-Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;3.924&lt;/td&gt; &lt;td align="left"&gt;10.362959&lt;/td&gt; &lt;td align="left"&gt;0.039932&lt;/td&gt; &lt;td align="left"&gt;3766.81&lt;/td&gt; &lt;td align="left"&gt;230.96&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;OLMoE-1B-7B-0924-Instruct-Q4_1&lt;/td&gt; &lt;td align="left"&gt;4.055&lt;/td&gt; &lt;td align="left"&gt;10.386061&lt;/td&gt; &lt;td align="left"&gt;0.046667&lt;/td&gt; &lt;td align="left"&gt;3745.30&lt;/td&gt; &lt;td align="left"&gt;253.62&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;OLMoE-1B-7B-0924-Instruct-Q5_K_S&lt;/td&gt; &lt;td align="left"&gt;4.452&lt;/td&gt; &lt;td align="left"&gt;10.263814&lt;/td&gt; &lt;td align="left"&gt;0.019071&lt;/td&gt; &lt;td align="left"&gt;3716.41&lt;/td&gt; &lt;td align="left"&gt;230.90&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;OLMoE-1B-7B-0924-Instruct-Q5_0&lt;/td&gt; &lt;td align="left"&gt;4.467&lt;/td&gt; &lt;td align="left"&gt;10.295836&lt;/td&gt; &lt;td align="left"&gt;0.023216&lt;/td&gt; &lt;td align="left"&gt;3803.06&lt;/td&gt; &lt;td align="left"&gt;237.34&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;OLMoE-1B-7B-0924-Instruct-Q5_K_M&lt;/td&gt; &lt;td align="left"&gt;4.588&lt;/td&gt; &lt;td align="left"&gt;10.264499&lt;/td&gt; &lt;td align="left"&gt;0.017257&lt;/td&gt; &lt;td align="left"&gt;3694.75&lt;/td&gt; &lt;td align="left"&gt;222.57&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;OLMoE-1B-7B-0924-Instruct-Q5_1&lt;/td&gt; &lt;td align="left"&gt;4.848&lt;/td&gt; &lt;td align="left"&gt;10.236555&lt;/td&gt; &lt;td align="left"&gt;0.018163&lt;/td&gt; &lt;td align="left"&gt;3692.16&lt;/td&gt; &lt;td align="left"&gt;233.59&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;OLMoE-1B-7B-0924-Instruct-Q6_K&lt;/td&gt; &lt;td align="left"&gt;5.294&lt;/td&gt; &lt;td align="left"&gt;10.209423&lt;/td&gt; &lt;td align="left"&gt;0.008738&lt;/td&gt; &lt;td align="left"&gt;3575.76&lt;/td&gt; &lt;td align="left"&gt;195.96&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;OLMoE-1B-7B-0924-Instruct-Q8_0&lt;/td&gt; &lt;td align="left"&gt;6.854&lt;/td&gt; &lt;td align="left"&gt;10.194440&lt;/td&gt; &lt;td align="left"&gt;0.004393&lt;/td&gt; &lt;td align="left"&gt;3890.05&lt;/td&gt; &lt;td align="left"&gt;187.82&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;granite-4.0-h-tiny&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Quantization&lt;/th&gt; &lt;th align="left"&gt;Size (GiB)&lt;/th&gt; &lt;th align="left"&gt;PPL Score&lt;/th&gt; &lt;th align="left"&gt;KLD Score&lt;/th&gt; &lt;th align="left"&gt;Prompt (t/s)&lt;/th&gt; &lt;th align="left"&gt;Gen (t/s)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;granite-4.0-h-tiny-IQ1_S&lt;/td&gt; &lt;td align="left"&gt;1.374&lt;/td&gt; &lt;td align="left"&gt;110.820345&lt;/td&gt; &lt;td align="left"&gt;2.936454&lt;/td&gt; &lt;td align="left"&gt;2684.17&lt;/td&gt; &lt;td align="left"&gt;127.39&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;granite-4.0-h-tiny-IQ1_M&lt;/td&gt; &lt;td align="left"&gt;1.518&lt;/td&gt; &lt;td align="left"&gt;30.016785&lt;/td&gt; &lt;td align="left"&gt;1.549064&lt;/td&gt; &lt;td align="left"&gt;1525.57&lt;/td&gt; &lt;td align="left"&gt;120.35&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;granite-4.0-h-tiny-IQ2_XXS&lt;/td&gt; &lt;td align="left"&gt;1.759&lt;/td&gt; &lt;td align="left"&gt;15.664424&lt;/td&gt; &lt;td align="left"&gt;0.815403&lt;/td&gt; &lt;td align="left"&gt;2823.29&lt;/td&gt; &lt;td align="left"&gt;118.23&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;granite-4.0-h-tiny-IQ2_XS&lt;/td&gt; &lt;td align="left"&gt;1.952&lt;/td&gt; &lt;td align="left"&gt;12.432497&lt;/td&gt; &lt;td align="left"&gt;0.544306&lt;/td&gt; &lt;td align="left"&gt;2517.37&lt;/td&gt; &lt;td align="left"&gt;118.33&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;granite-4.0-h-tiny-IQ2_S&lt;/td&gt; &lt;td align="left"&gt;1.967&lt;/td&gt; &lt;td align="left"&gt;12.192808&lt;/td&gt; &lt;td align="left"&gt;0.519907&lt;/td&gt; &lt;td align="left"&gt;2520.13&lt;/td&gt; &lt;td align="left"&gt;117.53&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;granite-4.0-h-tiny-IQ2_M&lt;/td&gt; &lt;td align="left"&gt;2.16&lt;/td&gt; &lt;td align="left"&gt;11.086195&lt;/td&gt; &lt;td align="left"&gt;0.394922&lt;/td&gt; &lt;td align="left"&gt;2516.28&lt;/td&gt; &lt;td align="left"&gt;115.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;granite-4.0-h-tiny-Q2_K_S&lt;/td&gt; &lt;td align="left"&gt;2.267&lt;/td&gt; &lt;td align="left"&gt;11.205483&lt;/td&gt; &lt;td align="left"&gt;0.422444&lt;/td&gt; &lt;td align="left"&gt;2253.11&lt;/td&gt; &lt;td align="left"&gt;126.12&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;granite-4.0-h-tiny-Q2_K&lt;/td&gt; &lt;td align="left"&gt;2.408&lt;/td&gt; &lt;td align="left"&gt;10.631549&lt;/td&gt; &lt;td align="left"&gt;0.348718&lt;/td&gt; &lt;td align="left"&gt;2295.69&lt;/td&gt; &lt;td align="left"&gt;118.05&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;granite-4.0-h-tiny-IQ3_XXS&lt;/td&gt; &lt;td align="left"&gt;2.537&lt;/td&gt; &lt;td align="left"&gt;9.878346&lt;/td&gt; &lt;td align="left"&gt;0.213335&lt;/td&gt; &lt;td align="left"&gt;2777.70&lt;/td&gt; &lt;td align="left"&gt;113.24&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;granite-4.0-h-tiny-IQ3_XS&lt;/td&gt; &lt;td align="left"&gt;2.716&lt;/td&gt; &lt;td align="left"&gt;9.414560&lt;/td&gt; &lt;td align="left"&gt;0.156308&lt;/td&gt; &lt;td align="left"&gt;2761.83&lt;/td&gt; &lt;td align="left"&gt;109.35&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;granite-4.0-h-tiny-IQ3_S&lt;/td&gt; &lt;td align="left"&gt;2.852&lt;/td&gt; &lt;td align="left"&gt;9.382415&lt;/td&gt; &lt;td align="left"&gt;0.140855&lt;/td&gt; &lt;td align="left"&gt;2748.22&lt;/td&gt; &lt;td align="left"&gt;108.30&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;granite-4.0-h-tiny-Q3_K_S&lt;/td&gt; &lt;td align="left"&gt;2.852&lt;/td&gt; &lt;td align="left"&gt;9.561864&lt;/td&gt; &lt;td align="left"&gt;0.163152&lt;/td&gt; &lt;td align="left"&gt;2560.96&lt;/td&gt; &lt;td align="left"&gt;100.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;granite-4.0-h-tiny-IQ3_M&lt;/td&gt; &lt;td align="left"&gt;2.886&lt;/td&gt; &lt;td align="left"&gt;9.348140&lt;/td&gt; &lt;td align="left"&gt;0.133007&lt;/td&gt; &lt;td align="left"&gt;2731.59&lt;/td&gt; &lt;td align="left"&gt;108.90&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;granite-4.0-h-tiny-Q3_K_M&lt;/td&gt; &lt;td align="left"&gt;3.123&lt;/td&gt; &lt;td align="left"&gt;9.398343&lt;/td&gt; &lt;td align="left"&gt;0.132221&lt;/td&gt; &lt;td align="left"&gt;2594.59&lt;/td&gt; &lt;td align="left"&gt;105.79&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;granite-4.0-h-tiny-Q3_K_L&lt;/td&gt; &lt;td align="left"&gt;3.354&lt;/td&gt; &lt;td align="left"&gt;9.371429&lt;/td&gt; &lt;td align="left"&gt;0.126633&lt;/td&gt; &lt;td align="left"&gt;2581.32&lt;/td&gt; &lt;td align="left"&gt;105.51&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;granite-4.0-h-tiny-IQ4_XS&lt;/td&gt; &lt;td align="left"&gt;3.493&lt;/td&gt; &lt;td align="left"&gt;8.884567&lt;/td&gt; &lt;td align="left"&gt;0.051232&lt;/td&gt; &lt;td align="left"&gt;2884.92&lt;/td&gt; &lt;td align="left"&gt;123.81&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;granite-4.0-h-tiny-IQ4_NL&lt;/td&gt; &lt;td align="left"&gt;3.691&lt;/td&gt; &lt;td align="left"&gt;8.899413&lt;/td&gt; &lt;td align="left"&gt;0.049923&lt;/td&gt; &lt;td align="left"&gt;2851.58&lt;/td&gt; &lt;td align="left"&gt;133.11&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;granite-4.0-h-tiny-Q4_0&lt;/td&gt; &lt;td align="left"&gt;3.706&lt;/td&gt; &lt;td align="left"&gt;9.012316&lt;/td&gt; &lt;td align="left"&gt;0.065076&lt;/td&gt; &lt;td align="left"&gt;2800.86&lt;/td&gt; &lt;td align="left"&gt;129.84&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;granite-4.0-h-tiny-Q4_K_S&lt;/td&gt; &lt;td align="left"&gt;3.721&lt;/td&gt; &lt;td align="left"&gt;8.887182&lt;/td&gt; &lt;td align="left"&gt;0.044464&lt;/td&gt; &lt;td align="left"&gt;2745.58&lt;/td&gt; &lt;td align="left"&gt;127.33&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;granite-4.0-h-tiny-MXFP4_MOE&lt;/td&gt; &lt;td align="left"&gt;3.895&lt;/td&gt; &lt;td align="left"&gt;8.825372&lt;/td&gt; &lt;td align="left"&gt;0.049953&lt;/td&gt; &lt;td align="left"&gt;2789.90&lt;/td&gt; &lt;td align="left"&gt;112.43&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;granite-4.0-h-tiny-Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;3.94&lt;/td&gt; &lt;td align="left"&gt;8.890295&lt;/td&gt; &lt;td align="left"&gt;0.041203&lt;/td&gt; &lt;td align="left"&gt;2719.64&lt;/td&gt; &lt;td align="left"&gt;124.52&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;granite-4.0-h-tiny-Q4_1&lt;/td&gt; &lt;td align="left"&gt;4.085&lt;/td&gt; &lt;td align="left"&gt;8.904143&lt;/td&gt; &lt;td align="left"&gt;0.045120&lt;/td&gt; &lt;td align="left"&gt;2679.63&lt;/td&gt; &lt;td align="left"&gt;134.15&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;granite-4.0-h-tiny-Q5_K_S&lt;/td&gt; &lt;td align="left"&gt;4.48&lt;/td&gt; &lt;td align="left"&gt;8.777425&lt;/td&gt; &lt;td align="left"&gt;0.020204&lt;/td&gt; &lt;td align="left"&gt;2694.01&lt;/td&gt; &lt;td align="left"&gt;124.06&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;granite-4.0-h-tiny-Q5_0&lt;/td&gt; &lt;td align="left"&gt;4.495&lt;/td&gt; &lt;td align="left"&gt;8.807001&lt;/td&gt; &lt;td align="left"&gt;0.023354&lt;/td&gt; &lt;td align="left"&gt;2749.84&lt;/td&gt; &lt;td align="left"&gt;127.54&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;granite-4.0-h-tiny-Q5_K_M&lt;/td&gt; &lt;td align="left"&gt;4.609&lt;/td&gt; &lt;td align="left"&gt;8.791519&lt;/td&gt; &lt;td align="left"&gt;0.018896&lt;/td&gt; &lt;td align="left"&gt;2632.96&lt;/td&gt; &lt;td align="left"&gt;119.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;granite-4.0-h-tiny-Q5_1&lt;/td&gt; &lt;td align="left"&gt;4.875&lt;/td&gt; &lt;td align="left"&gt;8.785323&lt;/td&gt; &lt;td align="left"&gt;0.019145&lt;/td&gt; &lt;td align="left"&gt;2661.61&lt;/td&gt; &lt;td align="left"&gt;127.36&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;granite-4.0-h-tiny-Q6_K&lt;/td&gt; &lt;td align="left"&gt;5.319&lt;/td&gt; &lt;td align="left"&gt;8.765266&lt;/td&gt; &lt;td align="left"&gt;0.009882&lt;/td&gt; &lt;td align="left"&gt;2566.16&lt;/td&gt; &lt;td align="left"&gt;110.06&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;granite-4.0-h-tiny-Q8_0&lt;/td&gt; &lt;td align="left"&gt;6.883&lt;/td&gt; &lt;td align="left"&gt;8.741198&lt;/td&gt; &lt;td align="left"&gt;0.004901&lt;/td&gt; &lt;td align="left"&gt;2804.95&lt;/td&gt; &lt;td align="left"&gt;103.00&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Setup:&lt;/h1&gt; &lt;p&gt;CPU: Intel Core i3-12100F.&lt;/p&gt; &lt;p&gt;RAM: 64gb of DDR4 3200, dual channel.&lt;/p&gt; &lt;p&gt;GPU: RTX 3060 12gb (GPU clock fixed at 1882 MHz via a curve, VRAM at 8210 MHz, stable).&lt;/p&gt; &lt;p&gt;OS: Windows 11, Nvidia drivers 591.74.&lt;/p&gt; &lt;p&gt;Build: llama.cpp b8123 (f75c4e8bf) for CUDA 13.1 precompiled.&lt;/p&gt; &lt;h1&gt;Details:&lt;/h1&gt; &lt;p&gt;LFM2-8B-A1B-BF16.gguf from &lt;a href="https://huggingface.co/unsloth/LFM2-8B-A1B-GGUF"&gt;unsloth/LFM2-8B-A1B-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;OLMoE-1B-7B-0924-Instruct-f16.gguf from &lt;a href="https://huggingface.co/bartowski/OLMoE-1B-7B-0924-Instruct-GGUF"&gt;bartowski/OLMoE-1B-7B-0924-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;granite-4.0-h-tiny-BF16.gguf from &lt;a href="https://huggingface.co/unsloth/granite-4.0-h-tiny-GGUF"&gt;unsloth/granite-4.0-h-tiny-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;All quants have been created using &lt;a href="https://gist.github.com/tristandruyen/9e207a95c7d75ddf37525d353e00659c"&gt;tristandruyen/calibration_data_v5_rc.txt&lt;/a&gt;&lt;/p&gt; &lt;p&gt;PPL is calculated with wiki.test.raw with a context of 512 tokens, while t/s are calculated for 2048 tokens generated with a context of 8192 tokens.&lt;/p&gt; &lt;h1&gt;Notes:&lt;/h1&gt; &lt;p&gt;These quants are just meant to represent what's mostly available on Hugging Face and have not been optimized with a custom recipe.&lt;/p&gt; &lt;p&gt;This sweep simply ranks them from least to most faithful to the original weights.&lt;/p&gt; &lt;p&gt;The figures at low bit-per-weight quantization might not be representative of the quality of the quantization scheme when applied to a larger model.&lt;/p&gt; &lt;p&gt;This is not supposed to tell what quantization scheme is best suited for your particular task or language.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TitwitMuffbiscuit"&gt; /u/TitwitMuffbiscuit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rd2cdu/round_2_quick_moe_quantization_comparison/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rd2cdu/round_2_quick_moe_quantization_comparison/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rd2cdu/round_2_quick_moe_quantization_comparison/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T02:28:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcnv9h</id>
    <title>GLM-5 is the new top open-weights model on the Extended NYT Connections benchmark, with a score of 81.8, edging out Kimi K2.5 Thinking (78.3)</title>
    <updated>2026-02-23T17:31:02+00:00</updated>
    <author>
      <name>/u/zero0_one1</name>
      <uri>https://old.reddit.com/user/zero0_one1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcnv9h/glm5_is_the_new_top_openweights_model_on_the/"&gt; &lt;img alt="GLM-5 is the new top open-weights model on the Extended NYT Connections benchmark, with a score of 81.8, edging out Kimi K2.5 Thinking (78.3)" src="https://preview.redd.it/t89mf46o4alg1.png?width=140&amp;amp;height=89&amp;amp;auto=webp&amp;amp;s=6d3e242b7e37ab99694926c9cefb58fae2a90e45" title="GLM-5 is the new top open-weights model on the Extended NYT Connections benchmark, with a score of 81.8, edging out Kimi K2.5 Thinking (78.3)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;More info: &lt;a href="https://github.com/lechmazur/nyt-connections/"&gt;https://github.com/lechmazur/nyt-connections/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zero0_one1"&gt; /u/zero0_one1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1rcnv9h"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcnv9h/glm5_is_the_new_top_openweights_model_on_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rcnv9h/glm5_is_the_new_top_openweights_model_on_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T17:31:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcs9vr</id>
    <title>Talking to my to-do list</title>
    <updated>2026-02-23T20:05:37+00:00</updated>
    <author>
      <name>/u/llo7d</name>
      <uri>https://old.reddit.com/user/llo7d</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcs9vr/talking_to_my_todo_list/"&gt; &lt;img alt="Talking to my to-do list" src="https://external-preview.redd.it/YnFzdm9lejd2YWxnMWY-tuy7HWwE5y0N4mja7xeEwkxeCiovLgSs8XbE5sB8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7139ad7399515809748a8bd26139c3d328ee50f5" title="Talking to my to-do list" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been testing feeding all my to-do list and productivity and having this kinda of desk robot thing as a screen to talk to? all the stuff happens on the pc, the screen is just a display and still for now it is a cloud based ai but I can definitely see this all happening locally in the future &lt;em&gt;(also better for privacy stuff)&lt;/em&gt; man the future is going to be awesome&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/llo7d"&gt; /u/llo7d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/xplqhdz7valg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcs9vr/talking_to_my_todo_list/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rcs9vr/talking_to_my_todo_list/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T20:05:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1rd80gx</id>
    <title>I just saw something amazing</title>
    <updated>2026-02-24T05:49:17+00:00</updated>
    <author>
      <name>/u/ayanami0011</name>
      <uri>https://old.reddit.com/user/ayanami0011</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rd80gx/i_just_saw_something_amazing/"&gt; &lt;img alt="I just saw something amazing" src="https://preview.redd.it/rr17jgdksdlg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8c7fff37ff972da0293a348d64378188d1acef13" title="I just saw something amazing" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.asus.com/displays-desktops/workstations/performance/expertcenter-pro-et900n-g3/"&gt;https://www.asus.com/displays-desktops/workstations/performance/expertcenter-pro-et900n-g3/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.azken.com/Workstations/nvidia-series/Asus-ExpertCenter-Pro-ET900N-G3?utm%5C_source=chatgpt.com"&gt;https://www.azken.com/Workstations/nvidia-series/Asus-ExpertCenter-Pro-ET900N-G3?utm\_source=chatgpt.com&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ayanami0011"&gt; /u/ayanami0011 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rr17jgdksdlg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rd80gx/i_just_saw_something_amazing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rd80gx/i_just_saw_something_amazing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T05:49:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1rd1tj9</id>
    <title>Exclusive: China's DeepSeek trained AI model on Nvidia's best chip despite US ban, official says</title>
    <updated>2026-02-24T02:05:11+00:00</updated>
    <author>
      <name>/u/blahblahsnahdah</name>
      <uri>https://old.reddit.com/user/blahblahsnahdah</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rd1tj9/exclusive_chinas_deepseek_trained_ai_model_on/"&gt; &lt;img alt="Exclusive: China's DeepSeek trained AI model on Nvidia's best chip despite US ban, official says" src="https://external-preview.redd.it/LwC39wQsKjPNUsKdGmLUh6SkmdTxf4euiX9LEkSLsqY.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b1529094f7ffd1b18c8afe7ddd7efa27261ad7f5" title="Exclusive: China's DeepSeek trained AI model on Nvidia's best chip despite US ban, official says" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/blahblahsnahdah"&gt; /u/blahblahsnahdah &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reuters.com/world/china/chinas-deepseek-trained-ai-model-nvidias-best-chip-despite-us-ban-official-says-2026-02-24/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rd1tj9/exclusive_chinas_deepseek_trained_ai_model_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rd1tj9/exclusive_chinas_deepseek_trained_ai_model_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T02:05:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1rd8cfw</id>
    <title>Anthropic's recent distillation blog should make anyone only ever want to use local open-weight models; it's scary and dystopian</title>
    <updated>2026-02-24T06:07:02+00:00</updated>
    <author>
      <name>/u/obvithrowaway34434</name>
      <uri>https://old.reddit.com/user/obvithrowaway34434</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rd8cfw/anthropics_recent_distillation_blog_should_make/"&gt; &lt;img alt="Anthropic's recent distillation blog should make anyone only ever want to use local open-weight models; it's scary and dystopian" src="https://preview.redd.it/086f3wnavdlg1.png?width=140&amp;amp;height=66&amp;amp;auto=webp&amp;amp;s=3125bb81f69aa57e4305e4471c6284c4a9a52a12" title="Anthropic's recent distillation blog should make anyone only ever want to use local open-weight models; it's scary and dystopian" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's quite ironic that they went for the censorship and authoritarian angles here.&lt;/p&gt; &lt;p&gt;Full blog: &lt;a href="https://www.anthropic.com/news/detecting-and-preventing-distillation-attacks"&gt;https://www.anthropic.com/news/detecting-and-preventing-distillation-attacks&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/obvithrowaway34434"&gt; /u/obvithrowaway34434 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1rd8cfw"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rd8cfw/anthropics_recent_distillation_blog_should_make/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rd8cfw/anthropics_recent_distillation_blog_should_make/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T06:07:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcrb2k</id>
    <title>Hypocrisy?</title>
    <updated>2026-02-23T19:31:17+00:00</updated>
    <author>
      <name>/u/pmv143</name>
      <uri>https://old.reddit.com/user/pmv143</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcrb2k/hypocrisy/"&gt; &lt;img alt="Hypocrisy?" src="https://preview.redd.it/jxutlq8bqalg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=59d78bab536255787ed1f0bc277f2a7f6d5aea3b" title="Hypocrisy?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmv143"&gt; /u/pmv143 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jxutlq8bqalg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcrb2k/hypocrisy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rcrb2k/hypocrisy/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T19:31:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1rd1lmz</id>
    <title>American vs Chinese AI is a false narrative.</title>
    <updated>2026-02-24T01:57:22+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; The real war (&lt;strong&gt;&lt;em&gt;IF&lt;/em&gt;&lt;/strong&gt; there is one) is between closed source and open source. Don't fall for/propagate the America vs China narrative. That's just tactics to get investors to loosen pursestrings and lawmakers/politicians to acquiesce to demands. &lt;/p&gt; &lt;hr /&gt; &lt;p&gt;There's been an uptick of nationalistic posts (mostly in defense of Chinese AI) on this sub and I think its very important to stop false narratives and reset it to the right framing. &lt;/p&gt; &lt;p&gt;Demonize a foreign enemy as a call for action - it was Russia for the space race, and now China. Except the world has changed immeasurably with globalization and national lines make less and less sense everyday - hell I'd wager most of OpenAI/Anthropic AI research teams are Chinese origin. Propagandizing and controlling media narratives is a time honored tradition for moneyed interests. I hope that the relatively more sophisticated folk in this sub can see past this. Yes it is true that the best open source models right now are almost all Chinese. That is resulting in people loosely using those terms as interchangeable but its a false equivalency and should not be spread. &lt;/p&gt; &lt;p&gt;Chinese labs are open sourcing their stuff &lt;em&gt;for now&lt;/em&gt;. But all of those companies are also for-profit - just like OpenAI and Anthropic. The most likely reason they are open sourcing is to stay relevant in the market and prevent platform seizure a la format wars of previous tech shifts (think Blu Ray). Also, the reality is that they are not only not as good as closed source SOTA. But even if they were at parity, most of the world would not trust them purely because of the fact that there is a strong prejudice against China. Thus, its a marketing and sales funnel channel - not some sort of magnanimity. &lt;/p&gt; &lt;p&gt;When the tides shift, as they always do (remember Llama?), Chinese companies could very well go closed source. In fact, we already saw Alibaba try that with Qwen3-Max. &lt;/p&gt; &lt;p&gt;So its very crucial that &lt;strong&gt;we reframe it to the correct axis - closed vs open source.&lt;/strong&gt; I dont think I need to preach to the choir here but this is the enormously critical battle. And if we lose it, I think its going to be worse than the SaaS/cloud/everything is a subscription hell we are currently in. Correct framing is crucial in keeping focus on the right things and prevents the water muddying tactics political players use to get their way.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rd1lmz/american_vs_chinese_ai_is_a_false_narrative/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rd1lmz/american_vs_chinese_ai_is_a_false_narrative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rd1lmz/american_vs_chinese_ai_is_a_false_narrative/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T01:57:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcmlwk</id>
    <title>so is OpenClaw local or not</title>
    <updated>2026-02-23T16:47:01+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcmlwk/so_is_openclaw_local_or_not/"&gt; &lt;img alt="so is OpenClaw local or not" src="https://preview.redd.it/5rolok0mw9lg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d0bdebee8fd3b3c91999b3592892a73daf47142e" title="so is OpenClaw local or not" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Reading the comments, I’m guessing you didn’t bother to read this:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&amp;quot;Safety and alignment at Meta Superintelligence.&amp;quot;&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5rolok0mw9lg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcmlwk/so_is_openclaw_local_or_not/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rcmlwk/so_is_openclaw_local_or_not/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T16:47:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcseh1</id>
    <title>Fun fact: Anthropic has never open-sourced any LLMs</title>
    <updated>2026-02-23T20:10:06+00:00</updated>
    <author>
      <name>/u/InternationalAsk1490</name>
      <uri>https://old.reddit.com/user/InternationalAsk1490</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been working on a little side project comparing tokenizer efficiency across different companies’ models for multilingual encoding.&lt;/p&gt; &lt;p&gt;Then I saw Anthropic’s announcement today and suddenly realized: there’s no way to analyze claude’s tokenizer lmao!&lt;/p&gt; &lt;p&gt;edit: Google once mentioned in a paper that Gemma and Gemini share the same tokenizer. OpenAI has already open‑sourced their tokenizers (and gpt‑oss). And don’t even get me started on Llama (Llama 5 pls 😭). &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InternationalAsk1490"&gt; /u/InternationalAsk1490 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcseh1/fun_fact_anthropic_has_never_opensourced_any_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcseh1/fun_fact_anthropic_has_never_opensourced_any_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rcseh1/fun_fact_anthropic_has_never_opensourced_any_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T20:10:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1rd2x61</id>
    <title>People are getting it wrong; Anthropic doesn't care about the distillation, they just want to counter the narrative about Chinese open-source models catching up with closed-source frontier models</title>
    <updated>2026-02-24T02:54:22+00:00</updated>
    <author>
      <name>/u/obvithrowaway34434</name>
      <uri>https://old.reddit.com/user/obvithrowaway34434</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rd2x61/people_are_getting_it_wrong_anthropic_doesnt_care/"&gt; &lt;img alt="People are getting it wrong; Anthropic doesn't care about the distillation, they just want to counter the narrative about Chinese open-source models catching up with closed-source frontier models" src="https://preview.redd.it/1ulaheylwclg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7333a0173119c9f64b93f296b5b27a05c6260830" title="People are getting it wrong; Anthropic doesn't care about the distillation, they just want to counter the narrative about Chinese open-source models catching up with closed-source frontier models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Why would they care about distillation when they probably have done the same with OpenAI models and the Chinese labs are paying for the tokens? This is just their attempt to explain to investors and the US government that cheap Chinese models will never be as good as their models without distillation or stealing model weights from them. And they need to put more restrictions on China to prevent the technology transfer.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/obvithrowaway34434"&gt; /u/obvithrowaway34434 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1ulaheylwclg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rd2x61/people_are_getting_it_wrong_anthropic_doesnt_care/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rd2x61/people_are_getting_it_wrong_anthropic_doesnt_care/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T02:54:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1rd64c5</id>
    <title>meanwhile in China</title>
    <updated>2026-02-24T04:42:30+00:00</updated>
    <author>
      <name>/u/Tiny_Judge_2119</name>
      <uri>https://old.reddit.com/user/Tiny_Judge_2119</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rd64c5/meanwhile_in_china/"&gt; &lt;img alt="meanwhile in China" src="https://external-preview.redd.it/bmE5aWI2Mm5nZGxnMf036yKzUhZ8EQqJaE3HIdg_QOMox8iiJVO5Ps1DTMuW.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6ac1d35442369f202ba6345ddfe517d7a7fac8f2" title="meanwhile in China" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tiny_Judge_2119"&gt; /u/Tiny_Judge_2119 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/j4ujf22ngdlg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rd64c5/meanwhile_in_china/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rd64c5/meanwhile_in_china/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T04:42:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcvimv</id>
    <title>Distillation when you do it. Training when we do it.</title>
    <updated>2026-02-23T22:04:41+00:00</updated>
    <author>
      <name>/u/Xhehab_</name>
      <uri>https://old.reddit.com/user/Xhehab_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcvimv/distillation_when_you_do_it_training_when_we_do_it/"&gt; &lt;img alt="Distillation when you do it. Training when we do it." src="https://preview.redd.it/9rc0jqbohblg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=05481c4cef786a02ca1e5d0b968e61114727348f" title="Distillation when you do it. Training when we do it." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xhehab_"&gt; /u/Xhehab_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9rc0jqbohblg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcvimv/distillation_when_you_do_it_training_when_we_do_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rcvimv/distillation_when_you_do_it_training_when_we_do_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T22:04:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcpmwn</id>
    <title>Anthropic: "We’ve identified industrial-scale distillation attacks on our models by DeepSeek, Moonshot AI, and MiniMax." 🚨</title>
    <updated>2026-02-23T18:32:45+00:00</updated>
    <author>
      <name>/u/KvAk_AKPlaysYT</name>
      <uri>https://old.reddit.com/user/KvAk_AKPlaysYT</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcpmwn/anthropic_weve_identified_industrialscale/"&gt; &lt;img alt="Anthropic: &amp;quot;We’ve identified industrial-scale distillation attacks on our models by DeepSeek, Moonshot AI, and MiniMax.&amp;quot; 🚨" src="https://preview.redd.it/94fbimavfalg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c2ad159232448ffd7033d6be4fa96582b674e461" title="Anthropic: &amp;quot;We’ve identified industrial-scale distillation attacks on our models by DeepSeek, Moonshot AI, and MiniMax.&amp;quot; 🚨" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KvAk_AKPlaysYT"&gt; /u/KvAk_AKPlaysYT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/94fbimavfalg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcpmwn/anthropic_weve_identified_industrialscale/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rcpmwn/anthropic_weve_identified_industrialscale/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T18:32:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7bsfd</id>
    <title>Best Audio Models - Feb 2026</title>
    <updated>2026-02-17T17:01:37+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;They've been a ton of audio models released of late, the most notable perhaps being Qwen3 TTS. So its time for another &lt;strong&gt;Best Audio Models&lt;/strong&gt; megathread&lt;/p&gt; &lt;p&gt;Share what your favorite ASR, TTS, STT, Text to Music models are right now &lt;strong&gt;and why.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Given the the amount of ambiguity and subjectivity in rating/testing these models, please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks etc. Closed models like Elevenlabs v3 seem to continue to be a few levels above open models especially for production use cases with long lengths/stability requirements, so comparisons, especially empirical ones are welcome.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please use the top level comments to thread your responses.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T17:01:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8snay</id>
    <title>AMA with StepFun AI - Ask Us Anything</title>
    <updated>2026-02-19T07:15:35+00:00</updated>
    <author>
      <name>/u/StepFun_ai</name>
      <uri>https://old.reddit.com/user/StepFun_ai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt; &lt;img alt="AMA with StepFun AI - Ask Us Anything" src="https://external-preview.redd.it/bqLigXK_A-XO-amDfdyeQenKCxR18nhChWpDnQXYmr4.png?width=140&amp;amp;height=75&amp;amp;auto=webp&amp;amp;s=72c79bc533c167d8f77ecf629e3b7af53ee1e6b6" title="AMA with StepFun AI - Ask Us Anything" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/w8274fg1jekg1.png?width=1785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fadbd0ec26a56e60900f9ed667ae808217d70cf2"&gt;https://preview.redd.it/w8274fg1jekg1.png?width=1785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fadbd0ec26a56e60900f9ed667ae808217d70cf2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; !&lt;/p&gt; &lt;p&gt;We are &lt;strong&gt;StepFun&lt;/strong&gt;, the team behind the &lt;strong&gt;Step&lt;/strong&gt; family models, including &lt;a href="https://huggingface.co/collections/stepfun-ai/step-35-flash"&gt;&lt;strong&gt;Step 3.5 Flash&lt;/strong&gt;&lt;/a&gt; and &lt;a href="https://huggingface.co/collections/stepfun-ai/step3-vl-10b"&gt;&lt;strong&gt;Step-3-VL-10B&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;We are super excited to host our first AMA tomorrow in this community. Our participants include CEO, CTO, Chief Scientist, LLM Researchers.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Participants&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Ok_Reach_5122"&gt;u/Ok_Reach_5122&lt;/a&gt; (Co-founder &amp;amp; CEO of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/bobzhuyb"&gt;u/bobzhuyb&lt;/a&gt; (Co-founder &amp;amp; CTO of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/user/Lost-Nectarine1016"&gt;u/Lost-Nectarine1016&lt;/a&gt; (Co-founder &amp;amp; Chief Scientist of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Elegant-Sale-1328"&gt;u/Elegant-Sale-1328&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/SavingsConclusion298"&gt;u/SavingsConclusion298&lt;/a&gt; (Post-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Spirited_Spirit3387"&gt;u/Spirited_Spirit3387&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/These-Nothing-8564/"&gt;u/These-Nothing-8564&lt;/a&gt; (Technical Project Manager)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Either-Beyond-7395"&gt;u/Either-Beyond-7395&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Human_Ad_162"&gt;u/Human_Ad_162&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Icy_Dare_3866"&gt;u/Icy_Dare_3866&lt;/a&gt; (Post-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Big-Employee5595"&gt;u/Big-Employee5595&lt;/a&gt; (Agent Algorithms Lead&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run 8 - 11 AM PST, Feburary 19th. The StepFun team will monitor and answer questions over the 24 hours after the live session.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StepFun_ai"&gt; /u/StepFun_ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T07:15:35+00:00</published>
  </entry>
</feed>
