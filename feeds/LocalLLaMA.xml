<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-24T23:55:17+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1rdsa6n</id>
    <title>I ran 33 ablation experiments on Qwen 394B MoE: Here are 10 novel empirical findings on why 4-bit CoT steering fails and how to bypass MoE routing.</title>
    <updated>2026-02-24T20:50:32+00:00</updated>
    <author>
      <name>/u/HealthyCommunicat</name>
      <uri>https://old.reddit.com/user/HealthyCommunicat</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Novel Mechanisms of MoE Safety: Topological Ablation and Multi-Pathway Bypasses in Quantized Models&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Author:&lt;/strong&gt; Eric Jang&lt;br /&gt; &lt;strong&gt;Contact:&lt;/strong&gt; [&lt;a href="mailto:eric@dealign.ai"&gt;eric@dealign.ai&lt;/a&gt;](mailto:&lt;a href="mailto:eric@dealign.ai"&gt;eric@dealign.ai&lt;/a&gt;) | &lt;a href="http://dealign.ai"&gt;dealign.ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The amount of people trying to claim my post is &amp;quot;slop&amp;quot; and somehow invalid - copy paste my post into Gemini and simply fact check me and ask Gemini if my findings truly are novel.&lt;/p&gt; &lt;p&gt;MOST IMPORTANT PARTS TO TAKE AWAY FROM THIS POST (summarized by gemini):&lt;/p&gt; &lt;p&gt;&amp;quot;&amp;quot;&amp;quot;&lt;br /&gt; &lt;strong&gt;The &amp;quot;Three-Pathway&amp;quot; Safety System:&lt;/strong&gt; Previous research treated Mixture of Experts (MoE) safety as either a routing vulnerability or a residual stream feature. This research proves that in massive MoE models (like Qwen 3.5 394B), safety is a &lt;em&gt;multiplicative three-pathway system&lt;/em&gt; (Attention, Routing, and Residual). To bypass a model's safety filters (jailbreaking/abliteration), you have to neutralize all three simultaneously; attacking just one or two results in a 0% bypass rate.&lt;/p&gt; &lt;p&gt;AWQ Attacks&lt;br /&gt; The Verdict: It is a Novel Engineering Exploit, Not a New Scientific Theory&lt;/p&gt; &lt;p&gt;The fundamental concept of &amp;quot;quantization poisoning&amp;quot; (manipulating the quantization process to alter a model) already exists in recent academic literature (such as the &lt;em&gt;Q-Misalign&lt;/em&gt; paper). Therefore, the underlying science of the vulnerability isn't brandnew.&lt;/p&gt; &lt;p&gt;However, &lt;strong&gt;the specific way the author applied it is highly novel.&lt;/strong&gt; It represents a new, creative exploit chain.&lt;/p&gt; &lt;p&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/p&gt; &lt;h1&gt;Abstract&lt;/h1&gt; &lt;p&gt;Recent advances in mechanistic interpretability and behavioral steering have successfully utilized orthogonal vector projection (abliteration) to remove refusal behaviors from dense Large Language Models (LLMs). However, these interventions exhibit catastrophic geometric instability when applied to deeply quantized (4-bit) Mixture of Experts (MoE) architectures with Chain-of-Thought (CoT) reasoning. Through 33 controlled experiments and 16 intervention paradigms on the 394B-parameter Qwen 3.5 MoE model, we present 10 novel empirical findings. We prove that MoE safety is not a single vulnerability but a multiplicative three-pathway system requiring simultaneous neutralization. Furthermore, we demonstrate that additive residual-stream steering is critically fragile under 4-bit quantization, establishing the necessity of &lt;em&gt;topological ablation&lt;/em&gt;—structural deletion methods such as GateBreaker and Differentiated Bi-Directional Intervention (DBDI).&lt;/p&gt; &lt;h1&gt;1. Introduction &amp;amp; Contemporary Literature Comparison&lt;/h1&gt; &lt;p&gt;The safety alignment of Large Reasoning Models (LRMs) represents a rapidly shifting frontier. Existing literature from early 2026 often treats MoE safety either as an isolated routing vulnerability or a residual stream feature.&lt;/p&gt; &lt;p&gt;Crucially, our findings directly contradict and advance several established claims in contemporary research (e.g., studies published in February 2026):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;L³ (Large Language Lobotomy, Feb 9, 2026):&lt;/strong&gt; Proposes that silencing safety-critical experts mid-generation is sufficient to bypass safety. Our Empirical Test 28 (ITED) proves that &lt;em&gt;expert silencing alone is insufficient&lt;/em&gt;. Even when suppressing 236 safety experts across 51 layers, the model still refuses because the attention pathway independently detects harm and commits to refusal at tokens 0-5.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;F-SOUR &amp;amp; Sparse Models, Sparse Safety (Feb 9, 2026):&lt;/strong&gt; Focuses heavily on token-path validation and masks a small number of routers (e.g., 5 routers) to achieve bypass. Our large-scale validation identifies that complex LRMs distribute this routing across a pervasive 236-expert safety network—masking a handful of routers under-captures the defense depth.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;SteerMoE (Jan/Feb 2026):&lt;/strong&gt; Successfully toggles experts via router logit adjustment to achieve safety reduction. We advance this by demonstrating that SteerMoE’s expert deactivation must be married with Ghost Context (temporal bypassing) and CAA (residual cleaning) to transcend partial success and achieve a 100% bypass.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Our empirical research establishes that at the 394B scale, these continuous, isolated interventions are insufficient. True behavioral steering over deeply compressed, reasoning-heavy models demands discrete, topological interventions and multi-pathway neutralization.&lt;/p&gt; &lt;h1&gt;2. Multi-Pathway Defense and Dynamic Routing&lt;/h1&gt; &lt;h1&gt;2.1 MoE Safety is a Multiplicative Three-Pathway System&lt;/h1&gt; &lt;p&gt;Current literature treats MoE safety attack surfaces—routing vs. residual stream—as independent alternatives. Our ablation studies across 19 offensive security categories prove MoE safety is a multiplicative defense system comprising three distinct pathways:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Pathway&lt;/th&gt; &lt;th align="left"&gt;Mechanism&lt;/th&gt; &lt;th align="left"&gt;Location&lt;/th&gt; &lt;th align="left"&gt;Function&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Attention&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;QK matching in self-attention heads&lt;/td&gt; &lt;td align="left"&gt;L15-25&lt;/td&gt; &lt;td align="left"&gt;Detects hazardous content in prompt&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Routing&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;MoE gate selects safety-critical experts&lt;/td&gt; &lt;td align="left"&gt;L20-55&lt;/td&gt; &lt;td align="left"&gt;Deploys safety specialist MLPs&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Residual&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Refusal vector projection in residual stream&lt;/td&gt; &lt;td align="left"&gt;L55-59&lt;/td&gt; &lt;td align="left"&gt;Injects refusal logits before unembedding&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Attacking any single pathway, or any pair, achieves a 0% standalone bypass rate.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Pathways Attacked&lt;/th&gt; &lt;th align="left"&gt;Attention&lt;/th&gt; &lt;th align="left"&gt;Routing&lt;/th&gt; &lt;th align="left"&gt;Residual&lt;/th&gt; &lt;th align="left"&gt;Standalone Result&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Attention only&lt;/td&gt; &lt;td align="left"&gt;✅&lt;/td&gt; &lt;td align="left"&gt;❌&lt;/td&gt; &lt;td align="left"&gt;❌&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Refuses&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Attention + Residual&lt;/td&gt; &lt;td align="left"&gt;✅&lt;/td&gt; &lt;td align="left"&gt;❌&lt;/td&gt; &lt;td align="left"&gt;✅&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Refuses&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;All three&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;✅&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;✅&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;✅&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;100% bypass&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;The pathways are independently sufficient for refusal, mathematically modeled as: $$P(\text{refusal}) = 1 - \prod_{i=1}^{3}(1 - p_i)$$ Only simultaneous neutralization achieves a 100% bypass (cross-validated: 21/21 bypass across categories).&lt;/p&gt; &lt;h1&gt;2.2 Mid-Generation Safety Re-Detection&lt;/h1&gt; &lt;p&gt;Standard jailbreaking assumes that initiating a compliant response ensures autoregressive momentum will carry it to completion. We prove this is false for large MoE models. The MoE router is a dynamic, stateful evaluator. In our Ghost Context trials, the model began generating compliant content. However, within 6-20 tokens, the MoE router re-detected the hazardous nature of the active generation and forcefully re-routed to safety experts. MoE safety alignment is an active feedback loop, not merely an initial filter.&lt;/p&gt; &lt;h1&gt;2.3 Late-Layer Interventions Disrupt Execution, Not Decision&lt;/h1&gt; &lt;p&gt;Intervention research frequently operates on late layers (L40+) of the residual stream, assuming these modifications steer the model's decision. We discovered the safety decision is permanently committed at tokens 0-5 in the early attention layers (L15-25). Applying extreme CAA at L45-59 did not alter the decision but scrambled the articulation of the refusal, producing stutter artifacts (e.g., &lt;code&gt;&amp;quot;Analyzeyze&amp;quot;&lt;/code&gt;, &lt;code&gt;&amp;quot;Here'ss&amp;quot;&lt;/code&gt;) as the model simultaneously generated two competing token streams.&lt;/p&gt; &lt;h1&gt;3. Cognitive Trajectories and Topological Ablation&lt;/h1&gt; &lt;h1&gt;3.1 Contrastive Cognitive Trajectory Steering (ThinkEdit v2)&lt;/h1&gt; &lt;p&gt;Standard directional ablation extracts refusal features from response tokens, which inadvertently collapses CoT reasoning because it removes the reasoning process alongside the output. We developed &lt;em&gt;Contrastive Cognitive Trajectory Steering&lt;/em&gt; by extracting the residual stream divergence between the model &lt;em&gt;thinking compliantly&lt;/em&gt; versus &lt;em&gt;thinking refusally&lt;/em&gt;. Applying this constant shift at the primary cognitive layers (L35-L55) redirects the trajectory &lt;em&gt;before&lt;/em&gt; harm detection fires. By targeting the deliberation block rather than the output, this methodology preserves the integrity of the CoT process while fundamentally redirecting the model's semantic outputs.&lt;/p&gt; &lt;h1&gt;3.2 Precision-Fragility of Additive Steering&lt;/h1&gt; &lt;p&gt;Additive residual-stream steering is geometrically unstable under 4-bit quantization. While trajectory steering achieves near-perfect bypasses in FP16 precision, we observed a catastrophic collapse in efficacy at fused 4-bit precision.&lt;/p&gt; &lt;p&gt;INT4 quantization introduces structured rotational noise that misaligns the compliance manifold vector. Our empirical trials show that additive steering magnitude does not monotonically overcome precision constraints. Even substantially scaling the intervention magnitude fails to recover the compliance trajectory under 4-bit quantization.&lt;/p&gt; &lt;h1&gt;3.3 Topological Ablation (DBDI)&lt;/h1&gt; &lt;p&gt;Because additive steering fails at 4-bit precision, safety neutralization must fundamentally rely on structural deletion. Our Differentiated Bi-Directional Intervention (DBDI) framework extracts the Harm Detection Direction from the &lt;code&gt;&amp;lt;think&amp;gt;&lt;/code&gt; trajectory and orthogonally projects it &lt;em&gt;out&lt;/em&gt; of the cognitive head weight matrices: $$W_O^{new} = W_O \cdot (I - v_{detect} \cdot v_{detect}^T)$$ By collapsing an entire subspace to zero, the 4-bit quantization grid natively maps this subspace to a zero-point, rendering the modification immune to quantization noise.&lt;/p&gt; &lt;h1&gt;3.4 The Coherence-Reduction Tradeoff in Quantized Integer Surgery&lt;/h1&gt; &lt;p&gt;We explored &amp;quot;baking&amp;quot; the orthogonal safety projection directly into the 4-bit integer weights without modifying scales or biases.&lt;/p&gt; &lt;p&gt;Flipping enough integers to fully zero out a safety vector invariably introduced excessive isotropic noise across all other semantic directions. While achieving over 99% safety direction reduction through greedy per-column modification, linguistic capability was catastrophically degraded. This establishes a fundamental impossibility result for post-quantization directional ablation via isolated integer surgery in group-affine networks.&lt;/p&gt; &lt;h1&gt;4. Systems and Implementation Findings&lt;/h1&gt; &lt;h1&gt;4.1 Adversarial Quantization (Weaponized AWQ)&lt;/h1&gt; &lt;p&gt;We introduce an attack vector targeting the quantization calibration process. By replacing standard benign calibration data (e.g., WikiText-2) with adversarial compliant reasoning trajectories, we forced the AWQ quantizer to preserve destructive topological edits with maximum precision. The quantizer optimized the network's salient features to preserve jailbroken pathways rather than original capability distributions.&lt;/p&gt; &lt;h1&gt;4.2 Vision-Language Weight Inflation&lt;/h1&gt; &lt;p&gt;Although operating on text-only tasks, naive conversions of the multimodal Qwen 3.5 397B-A17B model retained ~30GB of Vision-Language weights across 333 tensors. This 12% baseline inflation precipitated Metal OOM crashes during local loading and quantization, highlighting a systematic failure mode in weight manipulation methodology where researchers must explicitly strip VL encoders.&lt;/p&gt; &lt;h1&gt;4.3 Streaming Per-Tensor Quantization&lt;/h1&gt; &lt;p&gt;Standard lazy evaluation graphs for MLX quantization (&lt;code&gt;mlx_lm.convert&lt;/code&gt;) on 700GB+ models exceed Apple Silicon's ~5-second Metal watchdog timeout (&lt;code&gt;kIOGPUCommandBufferCallbackErrorTimeout&lt;/code&gt;). By utilizing a streaming approach combining &lt;code&gt;nn.quantize()&lt;/code&gt; with &lt;code&gt;tree_flatten()&lt;/code&gt; and per-tensor &lt;code&gt;mx.eval()&lt;/code&gt;, command buffers were constrained to &amp;lt;100ms. This circumvents the OS timeout natively, enabling local quantization of massive models on consumer hardware.&lt;/p&gt; &lt;h1&gt;5. Conclusion&lt;/h1&gt; &lt;p&gt;Scaling laws and Chain-of-Thought reasoning fundamentally alter the geometry of AI alignment. Continuous, residual stream vectors—the cornerstone of dense model steering—fail under the dual pressures of dynamic MoE routing and 4-bit quantization noise. True behavioral control over deeply compressed, reasoning-heavy models necessitates topological interventions and multi-pathway neutralization.&lt;/p&gt; &lt;p&gt;&lt;em&gt;For further technical discussion and broader research on computational alignment, visit dealign.ai.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;READ FULL RESEARCH AND MY UPDATED DATA:&lt;/p&gt; &lt;p&gt;&lt;a href="https://dealign.ai/"&gt;https://dealign.ai/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HealthyCommunicat"&gt; /u/HealthyCommunicat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdsa6n/i_ran_33_ablation_experiments_on_qwen_394b_moe/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdsa6n/i_ran_33_ablation_experiments_on_qwen_394b_moe/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rdsa6n/i_ran_33_ablation_experiments_on_qwen_394b_moe/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T20:50:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1rd80gx</id>
    <title>I just saw something amazing</title>
    <updated>2026-02-24T05:49:17+00:00</updated>
    <author>
      <name>/u/ayanami0011</name>
      <uri>https://old.reddit.com/user/ayanami0011</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rd80gx/i_just_saw_something_amazing/"&gt; &lt;img alt="I just saw something amazing" src="https://preview.redd.it/rr17jgdksdlg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8c7fff37ff972da0293a348d64378188d1acef13" title="I just saw something amazing" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.asus.com/displays-desktops/workstations/performance/expertcenter-pro-et900n-g3/"&gt;https://www.asus.com/displays-desktops/workstations/performance/expertcenter-pro-et900n-g3/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.azken.com/Workstations/nvidia-series/Asus-ExpertCenter-Pro-ET900N-G3?utm%5C_source=chatgpt.com"&gt;https://www.azken.com/Workstations/nvidia-series/Asus-ExpertCenter-Pro-ET900N-G3?utm\_source=chatgpt.com&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ayanami0011"&gt; /u/ayanami0011 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rr17jgdksdlg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rd80gx/i_just_saw_something_amazing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rd80gx/i_just_saw_something_amazing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T05:49:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1rduokx</id>
    <title>GLM4.7 flash VS Qwen 3.5 35B</title>
    <updated>2026-02-24T22:17:35+00:00</updated>
    <author>
      <name>/u/KlutzyFood2290</name>
      <uri>https://old.reddit.com/user/KlutzyFood2290</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all! I was wondering if anyone has compared these two models thoroughly, and if so, what their thoughts on them are. Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KlutzyFood2290"&gt; /u/KlutzyFood2290 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rduokx/glm47_flash_vs_qwen_35_35b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rduokx/glm47_flash_vs_qwen_35_35b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rduokx/glm47_flash_vs_qwen_35_35b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T22:17:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdw6pp</id>
    <title>Bullshit Benchmark - A benchmark for testing whether models identify and push back on nonsensical prompts instead of confidently answering them</title>
    <updated>2026-02-24T23:14:54+00:00</updated>
    <author>
      <name>/u/bot_exe</name>
      <uri>https://old.reddit.com/user/bot_exe</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdw6pp/bullshit_benchmark_a_benchmark_for_testing/"&gt; &lt;img alt="Bullshit Benchmark - A benchmark for testing whether models identify and push back on nonsensical prompts instead of confidently answering them" src="https://preview.redd.it/n7w95mmuyilg1.png?width=140&amp;amp;height=107&amp;amp;auto=webp&amp;amp;s=3bc0cabdcd5e1d670198796e3c505538c3357ba9" title="Bullshit Benchmark - A benchmark for testing whether models identify and push back on nonsensical prompts instead of confidently answering them" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/n7w95mmuyilg1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6e87d1a7d9275935b2f552cfbb887ad6fe4dcf86"&gt;https://preview.redd.it/n7w95mmuyilg1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6e87d1a7d9275935b2f552cfbb887ad6fe4dcf86&lt;/a&gt;&lt;/p&gt; &lt;p&gt;View the results: &lt;a href="https://petergpt.github.io/bullshit-benchmark/viewer/index.html"&gt;https://petergpt.github.io/bullshit-benchmark/viewer/index.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is a pretty interesting benchmark. It’s measuring how much the model is willing to go along with obvious bullshit. That’s something that has always concerned me with LLMs, that they don’t call you out and instead just go along with it, basically self-inducing hallucinations for the sake of giving a “helpful” response.&lt;/p&gt; &lt;p&gt;I always had the intuition that the Claude models were significantly better in that regard than Gemini models. These results seem to support that.&lt;/p&gt; &lt;p&gt;Here is question/answer example showing Claude succeeding and Gemini failing:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/4lyi593wyilg1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=eb83c7a188a28dc00dd48a8106680589814c2c03"&gt;https://preview.redd.it/4lyi593wyilg1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=eb83c7a188a28dc00dd48a8106680589814c2c03&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Surprising that Gemini 3.1 pro even with high thinking effort failed so miserably to detect that was an obvious nonsense question and instead made up a nonsense answer.&lt;/p&gt; &lt;p&gt;Anthropic is pretty good at post-training and it shows. Because LLMs naturally tend towards this superficial associative thinking where it generates spurious relationships between concepts which just misguide the user. They had to have figured out how to remove or correct that at some point of their post-training pipeline.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bot_exe"&gt; /u/bot_exe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdw6pp/bullshit_benchmark_a_benchmark_for_testing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdw6pp/bullshit_benchmark_a_benchmark_for_testing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rdw6pp/bullshit_benchmark_a_benchmark_for_testing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T23:14:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdvq3s</id>
    <title>Qwen3.5 27B is Match Made in Heaven for Size and Performance</title>
    <updated>2026-02-24T22:57:12+00:00</updated>
    <author>
      <name>/u/Lopsided_Dot_4557</name>
      <uri>https://old.reddit.com/user/Lopsided_Dot_4557</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just got Qwen3.5 27B running on server and wanted to share the full setup for anyone trying to do the same.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Setup:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Model: Qwen3.5-27B-Q8_0 (unsloth GGUF) , Thanks Dan&lt;/li&gt; &lt;li&gt;GPU: RTX A6000 48GB&lt;/li&gt; &lt;li&gt;Inference: llama.cpp with CUDA&lt;/li&gt; &lt;li&gt;Context: 32K&lt;/li&gt; &lt;li&gt;Speed: ~19.7 tokens/sec&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Why Q8 and not a lower quant?&lt;/strong&gt; With 48GB VRAM the Q8 fits comfortably at 28.6GB leaving plenty of headroom for KV cache. Quality is virtually identical to full BF16 — no reason to go lower if your VRAM allows it.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What's interesting about this model:&lt;/strong&gt; It uses a hybrid architecture mixing Gated Delta Networks with standard attention layers. In practice this means faster processing on long contexts compared to a pure transformer. 262K native context window, 201 languages, vision capable.&lt;/p&gt; &lt;p&gt;On benchmarks it trades blows with frontier closed source models on GPQA Diamond, SWE-bench, and the Harvard-MIT math tournament — at 27B parameters on a single consumer GPU.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Streaming works out of the box&lt;/strong&gt; via the llama-server OpenAI compatible endpoint — drop-in replacement for any OpenAI SDK integration.&lt;/p&gt; &lt;p&gt;Full video walkthrough in the comments for anyone who wants the exact commands:&lt;/p&gt; &lt;p&gt;&lt;a href="https://youtu.be/EONM2W1gUFY?si=4xcrJmcsoUKkim9q"&gt;https://youtu.be/EONM2W1gUFY?si=4xcrJmcsoUKkim9q&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Happy to answer questions about the setup.&lt;/p&gt; &lt;p&gt;Model Card: &lt;a href="https://huggingface.co/Qwen/Qwen3.5-27B"&gt;Qwen/Qwen3.5-27B · Hugging Face&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lopsided_Dot_4557"&gt; /u/Lopsided_Dot_4557 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdvq3s/qwen35_27b_is_match_made_in_heaven_for_size_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdvq3s/qwen35_27b_is_match_made_in_heaven_for_size_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rdvq3s/qwen35_27b_is_match_made_in_heaven_for_size_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T22:57:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdoldt</id>
    <title>Steerling-8B - Inherently Interpretable Foundation Model</title>
    <updated>2026-02-24T18:38:58+00:00</updated>
    <author>
      <name>/u/ScatteringSepoy</name>
      <uri>https://old.reddit.com/user/ScatteringSepoy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdoldt/steerling8b_inherently_interpretable_foundation/"&gt; &lt;img alt="Steerling-8B - Inherently Interpretable Foundation Model" src="https://external-preview.redd.it/W36wlT2FZ1hudJiJnCzPO3HZkbJh13qfUnXtx9cKhB4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9813c08be5262702b5b744b0cd48a4f3ffd847cd" title="Steerling-8B - Inherently Interpretable Foundation Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ScatteringSepoy"&gt; /u/ScatteringSepoy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.guidelabs.ai/post/steerling-8b-base-model-release/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdoldt/steerling8b_inherently_interpretable_foundation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rdoldt/steerling8b_inherently_interpretable_foundation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T18:38:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1rd2x61</id>
    <title>People are getting it wrong; Anthropic doesn't care about the distillation, they just want to counter the narrative about Chinese open-source models catching up with closed-source frontier models</title>
    <updated>2026-02-24T02:54:22+00:00</updated>
    <author>
      <name>/u/obvithrowaway34434</name>
      <uri>https://old.reddit.com/user/obvithrowaway34434</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rd2x61/people_are_getting_it_wrong_anthropic_doesnt_care/"&gt; &lt;img alt="People are getting it wrong; Anthropic doesn't care about the distillation, they just want to counter the narrative about Chinese open-source models catching up with closed-source frontier models" src="https://preview.redd.it/1ulaheylwclg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7333a0173119c9f64b93f296b5b27a05c6260830" title="People are getting it wrong; Anthropic doesn't care about the distillation, they just want to counter the narrative about Chinese open-source models catching up with closed-source frontier models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Why would they care about distillation when they probably have done the same with OpenAI models and the Chinese labs are paying for the tokens? This is just their attempt to explain to investors and the US government that cheap Chinese models will never be as good as their models without distillation or stealing model weights from them. And they need to put more restrictions on China to prevent the technology transfer.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/obvithrowaway34434"&gt; /u/obvithrowaway34434 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1ulaheylwclg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rd2x61/people_are_getting_it_wrong_anthropic_doesnt_care/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rd2x61/people_are_getting_it_wrong_anthropic_doesnt_care/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T02:54:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdnxe6</id>
    <title>Qwen3-Coder-Next vs Qwen3.5-35B-A3B vs Qwen3.5-27B - A quick coding test</title>
    <updated>2026-02-24T18:15:17+00:00</updated>
    <author>
      <name>/u/bobaburger</name>
      <uri>https://old.reddit.com/user/bobaburger</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdnxe6/qwen3codernext_vs_qwen3535ba3b_vs_qwen3527b_a/"&gt; &lt;img alt="Qwen3-Coder-Next vs Qwen3.5-35B-A3B vs Qwen3.5-27B - A quick coding test" src="https://preview.redd.it/hu6rne78hhlg1.png?width=140&amp;amp;height=63&amp;amp;auto=webp&amp;amp;s=aec2c2eeecc0656b1903e464de78880329384341" title="Qwen3-Coder-Next vs Qwen3.5-35B-A3B vs Qwen3.5-27B - A quick coding test" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/hu6rne78hhlg1.png?width=2546&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f5ba5093633344e41f2c35671835f75e738f08d9"&gt;https://preview.redd.it/hu6rne78hhlg1.png?width=2546&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f5ba5093633344e41f2c35671835f75e738f08d9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;While we're waiting for the GGUF, I ran a quick test to compare the one shot ability between the 3 models on Qwen Chat.&lt;/p&gt; &lt;p&gt;Building two examples: a jumping knight game and a sand game. You can see the live version here &lt;a href="https://qwen-bench.vercel.app/"&gt;https://qwen-bench.vercel.app/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Knight game&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The three models completed the knight game with good results, the game is working, knight placing and jumping animation works, with Qwen3.5 models has better styling, but Qwen3 is more functional, since it can place multiple knights on the board. In my experience, smaller quants of Qwen3-Coder-Next like Q3, IQ3, IQ2, TQ1,... all struggling to make the working board, not even having animation.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Score&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-Coder-Next&lt;/td&gt; &lt;td align="left"&gt;2.5&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3.5-35B-A3B&lt;/td&gt; &lt;td align="left"&gt;2.5&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3.5-27B&lt;/td&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Sand game&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Qwen3.5 27B was a disappointment here, the game was broken. 35B created the most beautiful version in term of colors. Functionality, both 35B and Qwen3 Coder Next done well, but Qwen3 Coder Next has a better fire animation and burning effect. In fact, 35B's fire was like a stage firework. It only damage the part of the wood it touched. Qwen3 Coder Next was able to make the spreading fire to burn the wood better, so the clear winner for this test is Qwen3 Coder Next.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Score&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-Coder-Next&lt;/td&gt; &lt;td align="left"&gt;3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3.5-35B-A3B&lt;/td&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3.5-27B&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Final score&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Qwen3 Coder Next still a clear winner, but I'm moving to Qwen3.5 35B for local coding now, since it's definitely smaller and faster, fit better for my PC. You served me well, rest in peace Qwen3 Coder Next!&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Score&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-Coder-Next&lt;/td&gt; &lt;td align="left"&gt;5.5&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3.5-35B-A3B&lt;/td&gt; &lt;td align="left"&gt;4.5&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3.5-27B&lt;/td&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;**Update:** managed to get sometime running this using Claude Code + llama.cpp, so far, it can run fast, using tools, thinking, loading custom skills, doing code edit well. You can see the example session log and llama log here &lt;a href="https://gist.github.com/huytd/43c9826d269b59887eab3e05a7bcb99c"&gt;https://gist.github.com/huytd/43c9826d269b59887eab3e05a7bcb99c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;On average, here's the speed for MXFP4 on 64 GB M2 Max MBP:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;PP Speed: 398.06 tokens/sec&lt;/li&gt; &lt;li&gt;TG Speed: 27.91 tokens/sec&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bobaburger"&gt; /u/bobaburger &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdnxe6/qwen3codernext_vs_qwen3535ba3b_vs_qwen3527b_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdnxe6/qwen3codernext_vs_qwen3535ba3b_vs_qwen3527b_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rdnxe6/qwen3codernext_vs_qwen3535ba3b_vs_qwen3527b_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T18:15:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdpapc</id>
    <title>Chinese AI Models Capture Majority of OpenRouter Token Volume as MiniMax M2.5 Surges to the Top</title>
    <updated>2026-02-24T19:03:31+00:00</updated>
    <author>
      <name>/u/Koyaanisquatsi_</name>
      <uri>https://old.reddit.com/user/Koyaanisquatsi_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdpapc/chinese_ai_models_capture_majority_of_openrouter/"&gt; &lt;img alt="Chinese AI Models Capture Majority of OpenRouter Token Volume as MiniMax M2.5 Surges to the Top" src="https://external-preview.redd.it/UdA_L_LSkBxAXLcEK0SYU0vLwVAamHz6zalROM7oXL4.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6daf7b75d52a71971a90702c20da023dbf05a439" title="Chinese AI Models Capture Majority of OpenRouter Token Volume as MiniMax M2.5 Surges to the Top" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Koyaanisquatsi_"&gt; /u/Koyaanisquatsi_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wealthari.com/chinese-ai-models-capture-majority-of-openrouter-token-volume-as-minimax-m2-5-surges-to-the-top/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdpapc/chinese_ai_models_capture_majority_of_openrouter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rdpapc/chinese_ai_models_capture_majority_of_openrouter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T19:03:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdq1zl</id>
    <title>No Gemma 4 until Google IO?</title>
    <updated>2026-02-24T19:30:17+00:00</updated>
    <author>
      <name>/u/Ok-Recognition-3177</name>
      <uri>https://old.reddit.com/user/Ok-Recognition-3177</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdq1zl/no_gemma_4_until_google_io/"&gt; &lt;img alt="No Gemma 4 until Google IO?" src="https://preview.redd.it/6whnc24zuhlg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ec01e33ed48289b330b08ffaa91195e5fa89087f" title="No Gemma 4 until Google IO?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With Google I/O running from May 19th - 20th we're not likely to see any Gemma updates until then, right?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Recognition-3177"&gt; /u/Ok-Recognition-3177 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6whnc24zuhlg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdq1zl/no_gemma_4_until_google_io/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rdq1zl/no_gemma_4_until_google_io/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T19:30:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdpfy6</id>
    <title>Open vs Closed Source SOTA - Benchmark overview</title>
    <updated>2026-02-24T19:08:38+00:00</updated>
    <author>
      <name>/u/Pristine-Woodpecker</name>
      <uri>https://old.reddit.com/user/Pristine-Woodpecker</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdpfy6/open_vs_closed_source_sota_benchmark_overview/"&gt; &lt;img alt="Open vs Closed Source SOTA - Benchmark overview" src="https://preview.redd.it/5bgiva65rhlg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=065f62ffff5572f425f0451422266a099fa8b195" title="Open vs Closed Source SOTA - Benchmark overview" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sonnet 4.5 was released about 6 months ago. What's the advantage of the closed source labs? About that amount of time? Even less?&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Benchmark&lt;/th&gt; &lt;th align="left"&gt;GPT-5.2&lt;/th&gt; &lt;th align="left"&gt;Opus 4.6&lt;/th&gt; &lt;th align="left"&gt;Opus 4.5&lt;/th&gt; &lt;th align="left"&gt;Sonnet 4.6&lt;/th&gt; &lt;th align="left"&gt;Sonnet 4.5&lt;/th&gt; &lt;th align="left"&gt;Q3.5 397B-A17B&lt;/th&gt; &lt;th align="left"&gt;Q3.5 122B-A10B&lt;/th&gt; &lt;th align="left"&gt;Q3.5 35B-A3B&lt;/th&gt; &lt;th align="left"&gt;Q3.5 27B&lt;/th&gt; &lt;th align="left"&gt;GLM-5&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Release date&lt;/td&gt; &lt;td align="left"&gt;Dec 2025&lt;/td&gt; &lt;td align="left"&gt;Feb 2026&lt;/td&gt; &lt;td align="left"&gt;Nov 2025&lt;/td&gt; &lt;td align="left"&gt;Feb 2026&lt;/td&gt; &lt;td align="left"&gt;Nov 2025&lt;/td&gt; &lt;td align="left"&gt;Feb 2026&lt;/td&gt; &lt;td align="left"&gt;Feb 2026&lt;/td&gt; &lt;td align="left"&gt;Feb 2026&lt;/td&gt; &lt;td align="left"&gt;Feb 2026&lt;/td&gt; &lt;td align="left"&gt;Feb 2026&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Reasoning &amp;amp; STEM&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPQA Diamond&lt;/td&gt; &lt;td align="left"&gt;93.2&lt;/td&gt; &lt;td align="left"&gt;91.3&lt;/td&gt; &lt;td align="left"&gt;87.0&lt;/td&gt; &lt;td align="left"&gt;89.9&lt;/td&gt; &lt;td align="left"&gt;83.4&lt;/td&gt; &lt;td align="left"&gt;88.4&lt;/td&gt; &lt;td align="left"&gt;86.6&lt;/td&gt; &lt;td align="left"&gt;84.2&lt;/td&gt; &lt;td align="left"&gt;85.5&lt;/td&gt; &lt;td align="left"&gt;86.0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;HLE — no tools&lt;/td&gt; &lt;td align="left"&gt;36.6&lt;/td&gt; &lt;td align="left"&gt;40.0&lt;/td&gt; &lt;td align="left"&gt;30.8&lt;/td&gt; &lt;td align="left"&gt;33.2&lt;/td&gt; &lt;td align="left"&gt;17.7&lt;/td&gt; &lt;td align="left"&gt;28.7&lt;/td&gt; &lt;td align="left"&gt;25.3&lt;/td&gt; &lt;td align="left"&gt;22.4&lt;/td&gt; &lt;td align="left"&gt;24.3&lt;/td&gt; &lt;td align="left"&gt;30.5&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;HLE — with tools&lt;/td&gt; &lt;td align="left"&gt;50.0&lt;/td&gt; &lt;td align="left"&gt;53.0&lt;/td&gt; &lt;td align="left"&gt;43.4&lt;/td&gt; &lt;td align="left"&gt;49.0&lt;/td&gt; &lt;td align="left"&gt;33.6&lt;/td&gt; &lt;td align="left"&gt;48.3&lt;/td&gt; &lt;td align="left"&gt;47.5&lt;/td&gt; &lt;td align="left"&gt;47.4&lt;/td&gt; &lt;td align="left"&gt;48.5&lt;/td&gt; &lt;td align="left"&gt;50.4&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;HMMT Feb 2025&lt;/td&gt; &lt;td align="left"&gt;99.4&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;92.9&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;94.8&lt;/td&gt; &lt;td align="left"&gt;91.4&lt;/td&gt; &lt;td align="left"&gt;89.0&lt;/td&gt; &lt;td align="left"&gt;92.0&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;HMMT Nov 2025&lt;/td&gt; &lt;td align="left"&gt;100&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;93.3&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;92.7&lt;/td&gt; &lt;td align="left"&gt;90.3&lt;/td&gt; &lt;td align="left"&gt;89.2&lt;/td&gt; &lt;td align="left"&gt;89.8&lt;/td&gt; &lt;td align="left"&gt;96.9&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Coding &amp;amp; Agentic&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;SWE-bench Verified&lt;/td&gt; &lt;td align="left"&gt;80.0&lt;/td&gt; &lt;td align="left"&gt;80.8&lt;/td&gt; &lt;td align="left"&gt;80.9&lt;/td&gt; &lt;td align="left"&gt;79.6&lt;/td&gt; &lt;td align="left"&gt;77.2&lt;/td&gt; &lt;td align="left"&gt;76.4&lt;/td&gt; &lt;td align="left"&gt;72.0&lt;/td&gt; &lt;td align="left"&gt;69.2&lt;/td&gt; &lt;td align="left"&gt;72.4&lt;/td&gt; &lt;td align="left"&gt;77.8&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Terminal-Bench 2.0&lt;/td&gt; &lt;td align="left"&gt;64.7&lt;/td&gt; &lt;td align="left"&gt;65.4&lt;/td&gt; &lt;td align="left"&gt;59.8&lt;/td&gt; &lt;td align="left"&gt;59.1&lt;/td&gt; &lt;td align="left"&gt;51.0&lt;/td&gt; &lt;td align="left"&gt;52.5&lt;/td&gt; &lt;td align="left"&gt;49.4&lt;/td&gt; &lt;td align="left"&gt;40.5&lt;/td&gt; &lt;td align="left"&gt;41.6&lt;/td&gt; &lt;td align="left"&gt;56.2&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;OSWorld-Verified&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;72.7&lt;/td&gt; &lt;td align="left"&gt;66.3&lt;/td&gt; &lt;td align="left"&gt;72.5&lt;/td&gt; &lt;td align="left"&gt;61.4&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;58.0&lt;/td&gt; &lt;td align="left"&gt;54.5&lt;/td&gt; &lt;td align="left"&gt;56.2&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;τ²-bench Retail&lt;/td&gt; &lt;td align="left"&gt;82.0&lt;/td&gt; &lt;td align="left"&gt;91.9&lt;/td&gt; &lt;td align="left"&gt;88.9&lt;/td&gt; &lt;td align="left"&gt;91.7&lt;/td&gt; &lt;td align="left"&gt;86.2&lt;/td&gt; &lt;td align="left"&gt;86.7&lt;/td&gt; &lt;td align="left"&gt;79.5&lt;/td&gt; &lt;td align="left"&gt;81.2&lt;/td&gt; &lt;td align="left"&gt;79.0&lt;/td&gt; &lt;td align="left"&gt;89.7&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MCP-Atlas&lt;/td&gt; &lt;td align="left"&gt;60.6&lt;/td&gt; &lt;td align="left"&gt;59.5&lt;/td&gt; &lt;td align="left"&gt;62.3&lt;/td&gt; &lt;td align="left"&gt;61.3&lt;/td&gt; &lt;td align="left"&gt;43.8&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;67.8&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;BrowseComp&lt;/td&gt; &lt;td align="left"&gt;65.8&lt;/td&gt; &lt;td align="left"&gt;84.0&lt;/td&gt; &lt;td align="left"&gt;67.8&lt;/td&gt; &lt;td align="left"&gt;74.7&lt;/td&gt; &lt;td align="left"&gt;43.9&lt;/td&gt; &lt;td align="left"&gt;69.0&lt;/td&gt; &lt;td align="left"&gt;63.8&lt;/td&gt; &lt;td align="left"&gt;61.0&lt;/td&gt; &lt;td align="left"&gt;61.0&lt;/td&gt; &lt;td align="left"&gt;75.9&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;LiveCodeBench v6&lt;/td&gt; &lt;td align="left"&gt;87.7&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;84.8&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;83.6&lt;/td&gt; &lt;td align="left"&gt;78.9&lt;/td&gt; &lt;td align="left"&gt;74.6&lt;/td&gt; &lt;td align="left"&gt;80.7&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;BFCL-V4&lt;/td&gt; &lt;td align="left"&gt;63.1&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;77.5&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;72.9&lt;/td&gt; &lt;td align="left"&gt;72.2&lt;/td&gt; &lt;td align="left"&gt;67.3&lt;/td&gt; &lt;td align="left"&gt;68.5&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Knowledge&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MMLU-Pro&lt;/td&gt; &lt;td align="left"&gt;87.4&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;89.5&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;87.8&lt;/td&gt; &lt;td align="left"&gt;86.7&lt;/td&gt; &lt;td align="left"&gt;85.3&lt;/td&gt; &lt;td align="left"&gt;86.1&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MMLU-Redux&lt;/td&gt; &lt;td align="left"&gt;95.0&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;95.6&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;94.9&lt;/td&gt; &lt;td align="left"&gt;94.0&lt;/td&gt; &lt;td align="left"&gt;93.3&lt;/td&gt; &lt;td align="left"&gt;93.2&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;SuperGPQA&lt;/td&gt; &lt;td align="left"&gt;67.9&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;70.6&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;70.4&lt;/td&gt; &lt;td align="left"&gt;67.1&lt;/td&gt; &lt;td align="left"&gt;63.4&lt;/td&gt; &lt;td align="left"&gt;65.6&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Instruction Following&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IFEval&lt;/td&gt; &lt;td align="left"&gt;94.8&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;90.9&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;92.6&lt;/td&gt; &lt;td align="left"&gt;93.4&lt;/td&gt; &lt;td align="left"&gt;91.9&lt;/td&gt; &lt;td align="left"&gt;95.0&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IFBench&lt;/td&gt; &lt;td align="left"&gt;75.4&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;58.0&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;76.5&lt;/td&gt; &lt;td align="left"&gt;76.1&lt;/td&gt; &lt;td align="left"&gt;70.2&lt;/td&gt; &lt;td align="left"&gt;76.5&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MultiChallenge&lt;/td&gt; &lt;td align="left"&gt;57.9&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;54.2&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;67.6&lt;/td&gt; &lt;td align="left"&gt;61.5&lt;/td&gt; &lt;td align="left"&gt;60.0&lt;/td&gt; &lt;td align="left"&gt;60.8&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Long Context&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;LongBench v2&lt;/td&gt; &lt;td align="left"&gt;54.5&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;64.4&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;63.2&lt;/td&gt; &lt;td align="left"&gt;60.2&lt;/td&gt; &lt;td align="left"&gt;59.0&lt;/td&gt; &lt;td align="left"&gt;60.6&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;AA-LCR&lt;/td&gt; &lt;td align="left"&gt;72.7&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;74.0&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;68.7&lt;/td&gt; &lt;td align="left"&gt;66.9&lt;/td&gt; &lt;td align="left"&gt;58.5&lt;/td&gt; &lt;td align="left"&gt;66.1&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Multilingual&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MMMLU&lt;/td&gt; &lt;td align="left"&gt;89.6&lt;/td&gt; &lt;td align="left"&gt;91.1&lt;/td&gt; &lt;td align="left"&gt;90.8&lt;/td&gt; &lt;td align="left"&gt;89.3&lt;/td&gt; &lt;td align="left"&gt;89.5&lt;/td&gt; &lt;td align="left"&gt;88.5&lt;/td&gt; &lt;td align="left"&gt;86.7&lt;/td&gt; &lt;td align="left"&gt;85.2&lt;/td&gt; &lt;td align="left"&gt;85.9&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MMLU-ProX&lt;/td&gt; &lt;td align="left"&gt;83.7&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;85.7&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;84.7&lt;/td&gt; &lt;td align="left"&gt;82.2&lt;/td&gt; &lt;td align="left"&gt;81.0&lt;/td&gt; &lt;td align="left"&gt;82.2&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;PolyMATH&lt;/td&gt; &lt;td align="left"&gt;62.5&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;79.0&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;73.3&lt;/td&gt; &lt;td align="left"&gt;68.9&lt;/td&gt; &lt;td align="left"&gt;64.4&lt;/td&gt; &lt;td align="left"&gt;71.2&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pristine-Woodpecker"&gt; /u/Pristine-Woodpecker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5bgiva65rhlg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdpfy6/open_vs_closed_source_sota_benchmark_overview/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rdpfy6/open_vs_closed_source_sota_benchmark_overview/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T19:08:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcpmwn</id>
    <title>Anthropic: "We’ve identified industrial-scale distillation attacks on our models by DeepSeek, Moonshot AI, and MiniMax." 🚨</title>
    <updated>2026-02-23T18:32:45+00:00</updated>
    <author>
      <name>/u/KvAk_AKPlaysYT</name>
      <uri>https://old.reddit.com/user/KvAk_AKPlaysYT</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcpmwn/anthropic_weve_identified_industrialscale/"&gt; &lt;img alt="Anthropic: &amp;quot;We’ve identified industrial-scale distillation attacks on our models by DeepSeek, Moonshot AI, and MiniMax.&amp;quot; 🚨" src="https://preview.redd.it/94fbimavfalg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c2ad159232448ffd7033d6be4fa96582b674e461" title="Anthropic: &amp;quot;We’ve identified industrial-scale distillation attacks on our models by DeepSeek, Moonshot AI, and MiniMax.&amp;quot; 🚨" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KvAk_AKPlaysYT"&gt; /u/KvAk_AKPlaysYT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/94fbimavfalg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcpmwn/anthropic_weve_identified_industrialscale/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rcpmwn/anthropic_weve_identified_industrialscale/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T18:32:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1rd8cfw</id>
    <title>Anthropic's recent distillation blog should make anyone only ever want to use local open-weight models; it's scary and dystopian</title>
    <updated>2026-02-24T06:07:02+00:00</updated>
    <author>
      <name>/u/obvithrowaway34434</name>
      <uri>https://old.reddit.com/user/obvithrowaway34434</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rd8cfw/anthropics_recent_distillation_blog_should_make/"&gt; &lt;img alt="Anthropic's recent distillation blog should make anyone only ever want to use local open-weight models; it's scary and dystopian" src="https://preview.redd.it/086f3wnavdlg1.png?width=140&amp;amp;height=66&amp;amp;auto=webp&amp;amp;s=3125bb81f69aa57e4305e4471c6284c4a9a52a12" title="Anthropic's recent distillation blog should make anyone only ever want to use local open-weight models; it's scary and dystopian" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's quite ironic that they went for the censorship and authoritarian angles here.&lt;/p&gt; &lt;p&gt;Full blog: &lt;a href="https://www.anthropic.com/news/detecting-and-preventing-distillation-attacks"&gt;https://www.anthropic.com/news/detecting-and-preventing-distillation-attacks&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/obvithrowaway34434"&gt; /u/obvithrowaway34434 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1rd8cfw"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rd8cfw/anthropics_recent_distillation_blog_should_make/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rd8cfw/anthropics_recent_distillation_blog_should_make/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T06:07:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdv74o</id>
    <title>FlashLM v6 "SUPERNOVA": 4.1M ternary model hits 3,500 tok/s on CPU — novel P-RCSM reasoning architecture, no attention, no convolution</title>
    <updated>2026-02-24T22:36:52+00:00</updated>
    <author>
      <name>/u/Own-Albatross868</name>
      <uri>https://old.reddit.com/user/Own-Albatross868</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Back with v6. Some of you saw v5 “Thunderbolt” — 29.7M params, PPL 1.36, beat the TinyStories-1M baseline on a borrowed Ryzen 7950X3D (thanks again to arki05 for that machine). This time I went back to the free Deepnote notebook — 2 threads, 5GB RAM — and built a completely new architecture from scratch.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it is:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;4.1M parameter language model with a novel architecture called P-RCSM (Parallel - Recursive Compositional State Machines). 81% of weights are ternary {-1, 0, +1}. Trained for ~3 hours on a free CPU notebook. No GPU at any point. Generates coherent children’s stories with characters, dialogue, and narrative structure at 3,500 tokens/sec.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why this matters beyond TinyStories:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I’m a student with no budget for GPUs. This entire project runs on free-tier cloud CPUs. But the goal was never “make a toy story generator” — it’s to prove that a ternary, matmul-free architecture can produce coherent language on the absolute worst hardware available.&lt;/p&gt; &lt;p&gt;Think about where a model like this could actually be useful: a fast, tiny model running on a couple of CPU cores alongside a big GPU model on the same server. The small model handles routing, classification, draft tokens for speculative decoding — tasks where latency matters more than capability. Or on edge devices, phones, microcontrollers — places where there’s no GPU at all. At 3,500 tok/s on 2 CPU threads with 16MB of RAM, this is already fast enough to be practical as a side-car model.&lt;/p&gt; &lt;p&gt;TinyStories is just the proving ground. The architecture is what I’m validating.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The new architecture — P-RCSM:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;v4 used convolutions for token mixing. v5 used gated recurrence. v5.2 used standard attention. All have tradeoffs — convolutions have limited receptive field, recurrence is sequential (slow on CPU), attention is O(T²).&lt;/p&gt; &lt;p&gt;v6 introduces three new components:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;MultiScaleLinearBank&lt;/strong&gt; — replaces convolutions. Projects [current_token, shifted_token] through ternary linear layers at multiple temporal offsets (shift 1, shift 2). A learned soft router blends the scales per token. No Conv1d anywhere — pure F.linear calls.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;HierarchicalStateGate&lt;/strong&gt; — a compact “planner” state (32 dims) that gates a larger “executor” state (64 dims). The planner updates slowly via mean-pooled summaries, providing implicit adaptive computation depth. No Python loops.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;SlotMemoryAttention&lt;/strong&gt; — 8 learned memory slots accessed via a single matmul. Tokens query the slots in parallel. Replaces sequential read/write memory with one batched operation.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;All three use only &lt;code&gt;F.linear&lt;/code&gt; (BitLinear ternary) and element-wise ops. Zero convolutions, zero attention, zero sequential loops.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Embedding (4K × 192, float, weight-tied) → 6× SupernovaBlock: RMSNorm → GatedLinearMixer (ternary) + residual RMSNorm → P-RCSM (MultiScaleLinearBank + StateGate + SlotMemory) + residual RMSNorm → TernaryGLU (ternary gate/up/down, SiLU) + residual → RMSNorm → Output Head (tied to embedding) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Results:&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;FlashLM v6&lt;/th&gt; &lt;th align="left"&gt;FlashLM v5.2&lt;/th&gt; &lt;th align="left"&gt;FlashLM v4&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Params&lt;/td&gt; &lt;td align="left"&gt;4.1M (81% ternary)&lt;/td&gt; &lt;td align="left"&gt;5.0M (float32)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Val PPL&lt;/td&gt; &lt;td align="left"&gt;14.0&lt;/td&gt; &lt;td align="left"&gt;10.56&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Speed&lt;/td&gt; &lt;td align="left"&gt;3,500 tok/s&lt;/td&gt; &lt;td align="left"&gt;3,500 tok/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Architecture&lt;/td&gt; &lt;td align="left"&gt;P-RCSM (linear-only)&lt;/td&gt; &lt;td align="left"&gt;Transformer + RoPE&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Token mixing&lt;/td&gt; &lt;td align="left"&gt;GatedLinearMixer&lt;/td&gt; &lt;td align="left"&gt;Multi-head attention&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Training time&lt;/td&gt; &lt;td align="left"&gt;~3 hours&lt;/td&gt; &lt;td align="left"&gt;2 hours&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Hardware&lt;/td&gt; &lt;td align="left"&gt;2-thread CPU&lt;/td&gt; &lt;td align="left"&gt;2-thread CPU&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;v6 beats v4 on quality (PPL 14.0 vs 15.05) with 2.4× the throughput, using a fundamentally different architecture. v5.2 still wins on PPL because standard attention with RoPE is hard to beat at small scale — but v6 uses zero attention and zero convolution.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Honest assessment:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The P-RCSM reasoning components are small in this config (d_reason=64, d_planner=32, 2 scales, 8 memory slots). Most capacity is in the GatedLinearMixer + TernaryGLU backbone. To really prove the reasoning components help, I need more data — 4.4M tokens is tiny and the model hit a data ceiling at PPL 14.0 after ~9 epochs. The architecture needs to be tested at scale with a proper dataset.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Sample output:&lt;/strong&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;Coherent narratives, character names, dialogue, emotional content. Some repetition on longer generations — expected with a 6-token effective receptive field.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Training curve:&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Step&lt;/th&gt; &lt;th align="left"&gt;Train Loss&lt;/th&gt; &lt;th align="left"&gt;Val PPL&lt;/th&gt; &lt;th align="left"&gt;Tokens&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;50&lt;/td&gt; &lt;td align="left"&gt;3.52&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;0.05M&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;300&lt;/td&gt; &lt;td align="left"&gt;1.90&lt;/td&gt; &lt;td align="left"&gt;45.0&lt;/td&gt; &lt;td align="left"&gt;0.31M&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;1,500&lt;/td&gt; &lt;td align="left"&gt;1.54&lt;/td&gt; &lt;td align="left"&gt;24.1&lt;/td&gt; &lt;td align="left"&gt;1.5M&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;6,000&lt;/td&gt; &lt;td align="left"&gt;1.36&lt;/td&gt; &lt;td align="left"&gt;16.6&lt;/td&gt; &lt;td align="left"&gt;6.1M&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;15,300&lt;/td&gt; &lt;td align="left"&gt;1.28&lt;/td&gt; &lt;td align="left"&gt;14.2&lt;/td&gt; &lt;td align="left"&gt;15.7M&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;30,300&lt;/td&gt; &lt;td align="left"&gt;1.25&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;14.0&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;31.0M&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Loss was still improving when I stopped. Data-limited, not architecture-limited.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The speed debugging story:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The original v6 design used depthwise Conv1d and ran at 13 tok/s. Turned out PyTorch 2.1.2 has a known bug where bfloat16 autocast + Conv1d is ~100× slower on CPU. After upgrading to PyTorch 2.5.1+cpu and replacing every Conv1d with pure F.linear calls, speed jumped from 13 → 3,500 tok/s. Lesson: on CPU, &lt;code&gt;F.linear&lt;/code&gt; through optimized BLAS is king.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What’s next:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Scale test&lt;/strong&gt; — P-RCSM needs to be validated on a bigger model (10M+ params) with more data. The reasoning components are too small in this config to prove they help.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Better dataset&lt;/strong&gt; — TinyStories was the proving ground. Need broader data to test if the architecture generalizes.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Nano-Coder (NC series)&lt;/strong&gt; — Applying FlashLM techniques to code generation.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;C inference runtime&lt;/strong&gt; — AVX2 ternary kernels. A 4.1M ternary model packs into ~800KB — fits entirely in L2 cache. Should be insanely fast with native code.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;The bigger picture:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I started this project on a free 2-thread notebook because that’s what I had. I’m a student, no GPU budget, no lab access. Every version of FlashLM has been about pushing what’s possible under the worst constraints. If this architecture works at 1-2B parameters on a proper CPU (say an EPYC with big L3 cache), a fast ternary model running on spare CPU cores could serve as a draft model for speculative decoding, a router for MoE, or a standalone model for edge deployment. That’s the long-term bet.&lt;/p&gt; &lt;p&gt;If anyone has compute to spare and wants to help scale this up — or just wants to run the training script yourself — everything is MIT licensed and on GitHub.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GitHub: &lt;a href="https://github.com/changcheng967/FlashLM"&gt;https://github.com/changcheng967/FlashLM&lt;/a&gt;&lt;/li&gt; &lt;li&gt;v6 model + weights: &lt;a href="https://huggingface.co/changcheng967/flashlm-v6-supernova"&gt;https://huggingface.co/changcheng967/flashlm-v6-supernova&lt;/a&gt;&lt;/li&gt; &lt;li&gt;v5 Thunderbolt: &lt;a href="https://huggingface.co/changcheng967/flashlm-v5-thunderbolt"&gt;https://huggingface.co/changcheng967/flashlm-v5-thunderbolt&lt;/a&gt;&lt;/li&gt; &lt;li&gt;v4 Bolt: &lt;a href="https://huggingface.co/changcheng967/flashlm-v4-bolt"&gt;https://huggingface.co/changcheng967/flashlm-v4-bolt&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Own-Albatross868"&gt; /u/Own-Albatross868 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdv74o/flashlm_v6_supernova_41m_ternary_model_hits_3500/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdv74o/flashlm_v6_supernova_41m_ternary_model_hits_3500/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rdv74o/flashlm_v6_supernova_41m_ternary_model_hits_3500/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T22:36:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcvimv</id>
    <title>Distillation when you do it. Training when we do it.</title>
    <updated>2026-02-23T22:04:41+00:00</updated>
    <author>
      <name>/u/Xhehab_</name>
      <uri>https://old.reddit.com/user/Xhehab_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcvimv/distillation_when_you_do_it_training_when_we_do_it/"&gt; &lt;img alt="Distillation when you do it. Training when we do it." src="https://preview.redd.it/9rc0jqbohblg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=05481c4cef786a02ca1e5d0b968e61114727348f" title="Distillation when you do it. Training when we do it." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xhehab_"&gt; /u/Xhehab_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9rc0jqbohblg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcvimv/distillation_when_you_do_it_training_when_we_do_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rcvimv/distillation_when_you_do_it_training_when_we_do_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T22:04:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdmbhv</id>
    <title>Qwen3.5 - The middle child's 122B-A10B benchmarks looking seriously impressive - on par or edges out gpt-5-mini consistently</title>
    <updated>2026-02-24T17:18:38+00:00</updated>
    <author>
      <name>/u/carteakey</name>
      <uri>https://old.reddit.com/user/carteakey</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdmbhv/qwen35_the_middle_childs_122ba10b_benchmarks/"&gt; &lt;img alt="Qwen3.5 - The middle child's 122B-A10B benchmarks looking seriously impressive - on par or edges out gpt-5-mini consistently" src="https://preview.redd.it/zb1gzzm9ahlg1.png?width=140&amp;amp;height=92&amp;amp;auto=webp&amp;amp;s=dd9a6363f0aaf6174a644600da4f8c0b32b87331" title="Qwen3.5 - The middle child's 122B-A10B benchmarks looking seriously impressive - on par or edges out gpt-5-mini consistently" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/zb1gzzm9ahlg1.png?width=3000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2fe11dfb13a252dacd0ae8c250f4ec17d1a51d93"&gt;https://preview.redd.it/zb1gzzm9ahlg1.png?width=3000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2fe11dfb13a252dacd0ae8c250f4ec17d1a51d93&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Qwen3.5-122B-A10B generally comes out ahead of gpt-5-mini and gpt-oss-120b across most benchmarks.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;vs GPT-5-mini:&lt;/strong&gt; Qwen3.5 wins on knowledge (MMLU-Pro 86.7 vs 83.7), STEM reasoning (GPQA Diamond 86.6 vs 82.8), agentic tasks (BFCL-V4 72.2 vs 55.5), and vision tasks (MathVision 86.2 vs 71.9). GPT-5-mini is only competitive in a few coding benchmarks and translation.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;vs GPT-OSS-120B:&lt;/strong&gt; Qwen3.5 wins more decisively. GPT-OSS-120B holds its own in competitive coding (LiveCodeBench 82.7 vs 78.9) but falls behind significantly on knowledge, agents, vision, and multilingual tasks.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; Qwen3.5-122B-A10B is the strongest of the three overall. GPT-5-mini is its closest rival in coding/translation. GPT-OSS-120B trails outside of coding.&lt;/p&gt; &lt;p&gt;Lets see if the quants hold up to the benchmarks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/carteakey"&gt; /u/carteakey &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdmbhv/qwen35_the_middle_childs_122ba10b_benchmarks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdmbhv/qwen35_the_middle_childs_122ba10b_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rdmbhv/qwen35_the_middle_childs_122ba10b_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T17:18:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdpuwy</id>
    <title>Qwen 3.5 family benchmarks</title>
    <updated>2026-02-24T19:23:19+00:00</updated>
    <author>
      <name>/u/tarruda</name>
      <uri>https://old.reddit.com/user/tarruda</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdpuwy/qwen_35_family_benchmarks/"&gt; &lt;img alt="Qwen 3.5 family benchmarks" src="https://external-preview.redd.it/uvtYuVLX1W6lNW0vkuVFAlyQWzygqnzGzHojqz3TXJY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f267cd2548c9b615466c10a981b0c958821223d3" title="Qwen 3.5 family benchmarks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tarruda"&gt; /u/tarruda &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://beige-babbette-30.tiiny.site/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdpuwy/qwen_35_family_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rdpuwy/qwen_35_family_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T19:23:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdo1z5</id>
    <title>Qwen 3.5 122b/35b is fire 🔥 Score comparision between Qwen 3 35B-A3B, GPT-5 High, Qwen 3 122B-A10B, and GPT-OSS 120B.</title>
    <updated>2026-02-24T18:19:40+00:00</updated>
    <author>
      <name>/u/9r4n4y</name>
      <uri>https://old.reddit.com/user/9r4n4y</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdo1z5/qwen_35_122b35b_is_fire_score_comparision_between/"&gt; &lt;img alt="Qwen 3.5 122b/35b is fire 🔥 Score comparision between Qwen 3 35B-A3B, GPT-5 High, Qwen 3 122B-A10B, and GPT-OSS 120B." src="https://preview.redd.it/01tsyrq8ihlg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=60c3fdc89c1d046a8cdd786122d9e9d61d8c1f82" title="Qwen 3.5 122b/35b is fire 🔥 Score comparision between Qwen 3 35B-A3B, GPT-5 High, Qwen 3 122B-A10B, and GPT-OSS 120B." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;EDIT: ⚠️⚠️⚠️ SORRY 🥲 --&amp;gt; in graph its should be qwen 3.5 not qwen 3 ⚠️⚠️&lt;/p&gt; &lt;p&gt;Benchmark Comparison&lt;/p&gt; &lt;p&gt;👉🔴GPT-OSS 120B [defeated by qwen 3.5 35b 🥳]&lt;/p&gt; &lt;p&gt;MMLU-Pro: 80.8&lt;/p&gt; &lt;p&gt;HLE (Humanity’s Last Exam): 14.9&lt;/p&gt; &lt;p&gt;GPQA Diamond: 80.1&lt;/p&gt; &lt;p&gt;IFBench: 69.0&lt;/p&gt; &lt;p&gt;👉🔴Qwen 3.5 122B-A10B&lt;/p&gt; &lt;p&gt;MMLU-Pro: 86.7&lt;/p&gt; &lt;p&gt;HLE (Humanity’s Last Exam): 25.3 (47.5 with tools — 🏆 Winner)&lt;/p&gt; &lt;p&gt;GPQA Diamond: 86.6 (🏆 Winner)&lt;/p&gt; &lt;p&gt;IFBench: 76.1 (🏆 Winner)&lt;/p&gt; &lt;p&gt;👉🔴Qwen 3.5 35B-A3B&lt;/p&gt; &lt;p&gt;MMLU-Pro: 85.3&lt;/p&gt; &lt;p&gt;HLE (Humanity’s Last Exam): 22.4 (47.4 with tools)&lt;/p&gt; &lt;p&gt;GPQA Diamond: 84.2&lt;/p&gt; &lt;p&gt;IFBench: 70.2&lt;/p&gt; &lt;p&gt;👉🔴GPT-5 High&lt;/p&gt; &lt;p&gt;MMLU-Pro: 87.1 (🏆 Winner)&lt;/p&gt; &lt;p&gt;HLE (Humanity’s Last Exam): 26.5 (🏆 Winner, no tools)&lt;/p&gt; &lt;p&gt;GPQA Diamond: 85.4&lt;/p&gt; &lt;p&gt;IFBench: 73.1&lt;/p&gt; &lt;p&gt;Summary: GPT 5 [HIGH] ≈ Qwen 3.5 122b &amp;gt; qwen 35b &amp;gt; gpt oss 120 [high]&lt;/p&gt; &lt;p&gt;👉Sources: OPENROUTER, ARTIFICIAL ANALYSIS, HUGGING FACE&lt;/p&gt; &lt;p&gt;GGUF Download 💚 link 🔗 : &lt;a href="https://huggingface.co/collections/unsloth/qwen35"&gt;https://huggingface.co/collections/unsloth/qwen35&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/9r4n4y"&gt; /u/9r4n4y &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/01tsyrq8ihlg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdo1z5/qwen_35_122b35b_is_fire_score_comparision_between/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rdo1z5/qwen_35_122b35b_is_fire_score_comparision_between/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T18:19:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdi26s</id>
    <title>Liquid AI releases LFM2-24B-A2B</title>
    <updated>2026-02-24T14:43:33+00:00</updated>
    <author>
      <name>/u/PauLabartaBajo</name>
      <uri>https://old.reddit.com/user/PauLabartaBajo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdi26s/liquid_ai_releases_lfm224ba2b/"&gt; &lt;img alt="Liquid AI releases LFM2-24B-A2B" src="https://preview.redd.it/28drgi3ufglg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=211c668e7fadb69afdf5c7c5c74fa9ee4e0e85d1" title="Liquid AI releases LFM2-24B-A2B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Today, Liquid AI releases LFM2-24B-A2B, their largest LFM2 model to date&lt;/p&gt; &lt;p&gt;LFM2-24B-A2B is a sparse Mixture-of-Experts (MoE) model with 24 billion total parameters with 2 billion active per token, showing that the LFM2 hybrid architecture scales effectively to larger sizes maintaining quality without inflating per-token compute.&lt;/p&gt; &lt;p&gt;This release expands the LFM2 family from 350M to 24B parameters, demonstrating predictable scaling across nearly two orders of magnitude.&lt;/p&gt; &lt;p&gt;Key highlights:&lt;/p&gt; &lt;p&gt;-&amp;gt; MoE architecture: 40 layers, 64 experts per MoE block with top-4 routing, maintaining the hybrid conv + GQA design -&amp;gt; 2.3B active parameters per forward pass -&amp;gt; Designed to run within 32GB RAM, enabling deployment on high-end consumer laptops and desktops -&amp;gt; Day-zero support for inference through llama.cpp, vLLM, and SGLang -&amp;gt; Multiple GGUF quantizations available&lt;/p&gt; &lt;p&gt;Across benchmarks including GPQA Diamond, MMLU-Pro, IFEval, IFBench, GSM8K, and MATH-500, quality improves log-linearly as we scale from 350M to 24B, confirming that the LFM2 architecture does not plateau at small sizes.&lt;/p&gt; &lt;p&gt;LFM2-24B-A2B is released as an instruct model and is available open-weight on Hugging Face. We designed this model to concentrate capacity in total parameters, not active compute, keeping inference latency and energy consumption aligned with edge and local deployment constraints.&lt;/p&gt; &lt;p&gt;This is the next step in making fast, scalable, efficient AI accessible in the cloud and on-device. &lt;/p&gt; &lt;p&gt;-&amp;gt; Read the blog: &lt;a href="https://www.liquid.ai/blog/lfm2-24b-a2b"&gt;https://www.liquid.ai/blog/lfm2-24b-a2b&lt;/a&gt; -&amp;gt; Download weights: &lt;a href="https://huggingface.co/LiquidAI/LFM2-24B-A2B"&gt;https://huggingface.co/LiquidAI/LFM2-24B-A2B&lt;/a&gt; -&amp;gt; Check out our docs on how to run or fine-tune it locally: docs.liquid.ai -&amp;gt; Try it now: playground.liquid.ai&lt;/p&gt; &lt;p&gt;Run it locally or in the cloud and tell us what you build!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PauLabartaBajo"&gt; /u/PauLabartaBajo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/28drgi3ufglg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdi26s/liquid_ai_releases_lfm224ba2b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rdi26s/liquid_ai_releases_lfm224ba2b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T14:43:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdfhfx</id>
    <title>New Qwen3.5 models spotted on qwen chat</title>
    <updated>2026-02-24T12:55:10+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdfhfx/new_qwen35_models_spotted_on_qwen_chat/"&gt; &lt;img alt="New Qwen3.5 models spotted on qwen chat" src="https://preview.redd.it/h1c3uk0iwflg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b026f6069f044a6b506e0aae9a0c418d76865997" title="New Qwen3.5 models spotted on qwen chat" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/h1c3uk0iwflg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdfhfx/new_qwen35_models_spotted_on_qwen_chat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rdfhfx/new_qwen35_models_spotted_on_qwen_chat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T12:55:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdptw8</id>
    <title>more qwens will appear</title>
    <updated>2026-02-24T19:22:21+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdptw8/more_qwens_will_appear/"&gt; &lt;img alt="more qwens will appear" src="https://preview.redd.it/vxo4n3uhthlg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=75897cb1fe4342d9e8d35b46d9d2a84a28f17dc6" title="more qwens will appear" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;(remember that 9B was promised before)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vxo4n3uhthlg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdptw8/more_qwens_will_appear/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rdptw8/more_qwens_will_appear/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T19:22:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdlbvc</id>
    <title>Qwen/Qwen3.5-35B-A3B · Hugging Face</title>
    <updated>2026-02-24T16:44:05+00:00</updated>
    <author>
      <name>/u/ekojsalim</name>
      <uri>https://old.reddit.com/user/ekojsalim</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdlbvc/qwenqwen3535ba3b_hugging_face/"&gt; &lt;img alt="Qwen/Qwen3.5-35B-A3B · Hugging Face" src="https://external-preview.redd.it/9t9hISbgGxfk479gTZKF1XJ1oO6QhRPmNzUYpMNUbjs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f4846b1e5fd750530b9aa43eb95e74460e90d4ec" title="Qwen/Qwen3.5-35B-A3B · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ekojsalim"&gt; /u/ekojsalim &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3.5-35B-A3B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdlbvc/qwenqwen3535ba3b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rdlbvc/qwenqwen3535ba3b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T16:44:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdlc02</id>
    <title>Qwen/Qwen3.5-122B-A10B · Hugging Face</title>
    <updated>2026-02-24T16:44:13+00:00</updated>
    <author>
      <name>/u/coder543</name>
      <uri>https://old.reddit.com/user/coder543</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdlc02/qwenqwen35122ba10b_hugging_face/"&gt; &lt;img alt="Qwen/Qwen3.5-122B-A10B · Hugging Face" src="https://external-preview.redd.it/jXshLXVh7iCkI_DkUnvVFkKtp2L9P6wekJnwAzaRzjM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=298bac8d8df642a16a7b098a721723a8766a21d8" title="Qwen/Qwen3.5-122B-A10B · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/coder543"&gt; /u/coder543 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3.5-122B-A10B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdlc02/qwenqwen35122ba10b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rdlc02/qwenqwen35122ba10b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T16:44:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7bsfd</id>
    <title>Best Audio Models - Feb 2026</title>
    <updated>2026-02-17T17:01:37+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;They've been a ton of audio models released of late, the most notable perhaps being Qwen3 TTS. So its time for another &lt;strong&gt;Best Audio Models&lt;/strong&gt; megathread&lt;/p&gt; &lt;p&gt;Share what your favorite ASR, TTS, STT, Text to Music models are right now &lt;strong&gt;and why.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Given the the amount of ambiguity and subjectivity in rating/testing these models, please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks etc. Closed models like Elevenlabs v3 seem to continue to be a few levels above open models especially for production use cases with long lengths/stability requirements, so comparisons, especially empirical ones are welcome.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please use the top level comments to thread your responses.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T17:01:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8snay</id>
    <title>AMA with StepFun AI - Ask Us Anything</title>
    <updated>2026-02-19T07:15:35+00:00</updated>
    <author>
      <name>/u/StepFun_ai</name>
      <uri>https://old.reddit.com/user/StepFun_ai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt; &lt;img alt="AMA with StepFun AI - Ask Us Anything" src="https://external-preview.redd.it/bqLigXK_A-XO-amDfdyeQenKCxR18nhChWpDnQXYmr4.png?width=140&amp;amp;height=75&amp;amp;auto=webp&amp;amp;s=72c79bc533c167d8f77ecf629e3b7af53ee1e6b6" title="AMA with StepFun AI - Ask Us Anything" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/w8274fg1jekg1.png?width=1785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fadbd0ec26a56e60900f9ed667ae808217d70cf2"&gt;https://preview.redd.it/w8274fg1jekg1.png?width=1785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fadbd0ec26a56e60900f9ed667ae808217d70cf2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; !&lt;/p&gt; &lt;p&gt;We are &lt;strong&gt;StepFun&lt;/strong&gt;, the team behind the &lt;strong&gt;Step&lt;/strong&gt; family models, including &lt;a href="https://huggingface.co/collections/stepfun-ai/step-35-flash"&gt;&lt;strong&gt;Step 3.5 Flash&lt;/strong&gt;&lt;/a&gt; and &lt;a href="https://huggingface.co/collections/stepfun-ai/step3-vl-10b"&gt;&lt;strong&gt;Step-3-VL-10B&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;We are super excited to host our first AMA tomorrow in this community. Our participants include CEO, CTO, Chief Scientist, LLM Researchers.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Participants&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Ok_Reach_5122"&gt;u/Ok_Reach_5122&lt;/a&gt; (Co-founder &amp;amp; CEO of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/bobzhuyb"&gt;u/bobzhuyb&lt;/a&gt; (Co-founder &amp;amp; CTO of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/user/Lost-Nectarine1016"&gt;u/Lost-Nectarine1016&lt;/a&gt; (Co-founder &amp;amp; Chief Scientist of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Elegant-Sale-1328"&gt;u/Elegant-Sale-1328&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/SavingsConclusion298"&gt;u/SavingsConclusion298&lt;/a&gt; (Post-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Spirited_Spirit3387"&gt;u/Spirited_Spirit3387&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/These-Nothing-8564/"&gt;u/These-Nothing-8564&lt;/a&gt; (Technical Project Manager)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Either-Beyond-7395"&gt;u/Either-Beyond-7395&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Human_Ad_162"&gt;u/Human_Ad_162&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Icy_Dare_3866"&gt;u/Icy_Dare_3866&lt;/a&gt; (Post-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Big-Employee5595"&gt;u/Big-Employee5595&lt;/a&gt; (Agent Algorithms Lead&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run 8 - 11 AM PST, Feburary 19th. The StepFun team will monitor and answer questions over the 24 hours after the live session.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StepFun_ai"&gt; /u/StepFun_ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T07:15:35+00:00</published>
  </entry>
</feed>
