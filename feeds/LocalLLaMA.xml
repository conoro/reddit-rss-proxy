<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-28T22:45:59+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qp4ftj</id>
    <title>I made a Coding Eval, and ran it against 49 different coding agent/model combinations, including Kimi K2.5.</title>
    <updated>2026-01-28T07:08:34+00:00</updated>
    <author>
      <name>/u/lemon07r</name>
      <uri>https://old.reddit.com/user/lemon07r</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;You may remember me from my &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1o77ag4/a_guide_to_the_best_agentic_tools_and_the_best/"&gt;A guide to the best agentic tools and the best way to use them on the cheap, locally or free&lt;/a&gt; post from 3 months ago. Where I submitted a big wall of text at 4 am in stream of consciousness format. For some reason, I still get random replies on it, about not putting enough effort in to format it. Well I'm back, and this time I've written my own benchmarking tool for evaluating coding agent/model ability, and ran it against (as of writing) 49 different coding agent and model combinations. Are you guys entertained now?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Coding Eval - SanityHarness&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This is my purpose-made coding eval, that I wanted to be agent-agnostic as possible to use (I've run a lot of other coding evals and some of them are a pain in the butt to get working with many agents). I carefully curated and put together tasks across 6 different languages, specifically focusing on problems for measuring model understanding and agent capability rather than training data regurgitation. If you're interested in the implementation or want to run it yourself, check it out on &lt;a href="https://github.com/lemon07r/SanityHarness"&gt;GitHub | lemon07r/SanityHarness&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Coding Agent Leaderboard - SanityBoard&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Now, for the part you‚Äôre probably most interested in, and where I invested too many hours: &lt;a href="https://sanityboard.lr7.dev/"&gt;https://sanityboard.lr7.dev/&lt;/a&gt; (source available on GH &lt;a href="https://github.com/lemon07r/SanityBoard"&gt;here&lt;/a&gt;). There are currently 49 entries, and &lt;strong&gt;many&lt;/strong&gt; more still being added. I tried to provide as much relevant data as possible, and present it in an easy to digest format with sort/filter controls and report pages with the full run data. This includes run dates, agent version numbers, etc, things that I feel are important but often left out in some leaderboards.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Join the Discord Server! Also consider giving my GH repos a star&lt;/strong&gt; ‚òÜ&lt;/p&gt; &lt;p&gt;Consider leaving a star in my github repos, as I did put in a lot of work in these projects, and will continue doing so. If any of you would like to see a specific agent or model tested (or retested), need any help running the eval, or have any other questions about the eval or leaderboard consider joining my &lt;a href="https://discord.gg/rXNQXCTWDt"&gt;Discord&lt;/a&gt; server (I am looking for more peeps to discuss ai and coding related topics with!)&lt;/p&gt; &lt;h1&gt;Some Extra Stuff, and Future Plans&lt;/h1&gt; &lt;p&gt;This post started out as another big block of text, but I've decided to spare you guys and re-wrote most of it to separate all the extra stuff as optional reading below. This includes some usage cost analysis' and some pretty cool stuff I have planned for the future.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;MCP Server Evals&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;For one, you might have noticed an &amp;quot;MCP&amp;quot; column on my leaderboard. That's right, I will eventually do runs with MCP tools enabled, but before this I have something even cooler planned. I'm going to be testing different MCP tools to see which ones make any difference (if it all), and which MCP tools are the best in their respective categories (web search, code indexing + semantic retrieval, etc), then afterwards, the best MCP combinations. I will be testing all of these in my evals; the goal is to figure what MCP tools and tool combination is best, and to see which ones might even negatively impact coding ability.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Agent Skills&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Also going to do evals against different skills files to see if they actually help and which ones are best (these are obviously very project/task dependant but I hope we can still figure out some good blanket-use ones).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;More Agents and Models to Test&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;There will be more coding agents tested. And models. Oh-My-Opencode is on my radar, I want to try testing a few different configurations to see if it's actually any better than vanilla opencode, or if it's all smoke and mirrors.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Usage, Cost and why Some Agents Were Left Off&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;AI credit plans suck. The coding agents that only support these monetization models are horrible. They wont support BYOK for a reason; they know their monetization models are downright horrendous and predatory. I was able to confirm this while monitoring the usage of some of my runs. Some agents that didn't make the cut because of this include Warp, Letta Code and Codebuff. Seriously, just support BYOK. Or at least have a decent value plan or free usage.&lt;/p&gt; &lt;p&gt;Here is a good example of how horrible some of these guys can be. Codebuff. 100 credits = $1. When I ran my tests against codebuff, my eval got through ONLY 9 of my 26 tasks, burning through $7.5 worth of credits. They even advertise how they use 30% less tokens than claude code or something like that. So you're telling me with codebuff you get to spend more money to use less tokens? I cannot explain how terrible this is. Maybe you'll have an idea of how bad it is, when you see below how much usage other plans or providers will give you (yes even AMP free, gives you more usage daily than you get from two months of free Codebuff credits).&lt;/p&gt; &lt;ul&gt; &lt;li&gt;AMP Smart Mode (mixed) - $6.53&lt;/li&gt; &lt;li&gt;AMP Rush Mode (mixed) - $3.8~&lt;/li&gt; &lt;li&gt;Copilot CLI GPT 5.2 High - 26 Premium Requests (basically $0.86 on pro plan)&lt;/li&gt; &lt;li&gt;Copilot CLI Opus - 78 Premium Requests (expensive, no reasoning or gimped somehow, use something else)&lt;/li&gt; &lt;li&gt;Codex GPT 5.2-Codex xhigh - 65% of daily, 20% of weekly (business seat)&lt;/li&gt; &lt;li&gt;Codex GPT 5.2 xhigh - 100% of daily, 30% of weekly (business seat)&lt;/li&gt; &lt;li&gt;Factory Gemini 3 Flash High - 1m tokens (these are all &amp;quot;Factory&amp;quot; tokens, 1m = $1)&lt;/li&gt; &lt;li&gt;Factory GLM 4.7 High - 0.7m tokens&lt;/li&gt; &lt;li&gt;Factory K2.5 - 0.8m tokens&lt;/li&gt; &lt;li&gt;Factory Gemini 3 Pro High - 2m tokens&lt;/li&gt; &lt;li&gt;Factory GPT 5.2 Codex xhigh - 2m tokens&lt;/li&gt; &lt;li&gt;Factory GPT 5.1 Codex Max xhigh - 2m tokens&lt;/li&gt; &lt;li&gt;Factory GPT 5.2 xhigh - 2.4m tokens&lt;/li&gt; &lt;li&gt;Factory Opus 4.5 High - 3m tokens&lt;/li&gt; &lt;li&gt;Kim For Coding Plan (K2.5) - Around 120-130 Req each run on OpenCode, Claude Code and Kimi CLI (with 2k weekly limit on $19 plan, this is essentially $0.30 a run).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;API Credits, Keys, And Integrity&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I'm accepting API credits/keys for testing more models and agents otherwise I will be limited to what I have access to currently (DM me). If you are an official provider for your model/agent, or have your own coding agent, feel free to reach out to me to get your stuff on my leaderboard.&lt;/p&gt; &lt;p&gt;Full disclosure, I do not do any manipulation of any kind and try to keep things completely fair, bias free, etc. Droid did provide me extra usage to run my evals, and Minimax has provided me a Coding Max Plan, but as you can see from my leaderboard that will not save some of them from having poor results.&lt;/p&gt; &lt;p&gt;I keep all my runs and can provide the entirety of them on request if anyone wants to see them for improving their model, agent or to see how valid my runs are (I do thoroughly check each of them for issues and have done complete reruns of every model and agent when I found any issues that needed fixing).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Future Updated Model and Agent Guide&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I am going to make a revised and updated guide soon. This will cover the best coding models and agents, covering various different grounds, like best open weight models, best open source agents, best free tier setups (including both open and closed options), and best value/bang for your buck setups. I will provide some actual analysis on my coding eval results and other data, including some behind the scenes stuff and experience, or other knowledge I've gathered from talking to experienced people in the field. There are a lot of insights and things to be gathered from outside evals and leaderboards, these results don't tell the full story.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lemon07r"&gt; /u/lemon07r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qp4ftj/i_made_a_coding_eval_and_ran_it_against_49/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qp4ftj/i_made_a_coding_eval_and_ran_it_against_49/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qp4ftj/i_made_a_coding_eval_and_ran_it_against_49/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T07:08:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpp13g</id>
    <title>Building opensource Zero Server Code Intelligence Engine</title>
    <updated>2026-01-28T21:44:35+00:00</updated>
    <author>
      <name>/u/DeathShot7777</name>
      <uri>https://old.reddit.com/user/DeathShot7777</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpp13g/building_opensource_zero_server_code_intelligence/"&gt; &lt;img alt="Building opensource Zero Server Code Intelligence Engine" src="https://external-preview.redd.it/c251Yzk2ZXBxNWdnMX8xZ7UL_WAflwTq0BqeDmN95WRo5Ajh0fAkuzEXaT9M.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=62456e5ae99c59466ae938995d3e10a9ac656404" title="Building opensource Zero Server Code Intelligence Engine" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, guys, I m building GitNexus, an opensource Code Intelligence Engine which works fully client sided in-browser. There have been lot of progress since I last posted. &lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/abhigyanpatwari/GitNexus"&gt;https://github.com/abhigyanpatwari/GitNexus&lt;/a&gt; ( ‚≠ê would help so much, u have no idea!! )&lt;br /&gt; Try: &lt;a href="https://gitnexus.vercel.app/"&gt;https://gitnexus.vercel.app/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It creates a Knowledge Graph from github repos and exposes an Agent with specially designed tools and also MCP support. Idea is to solve the project wide context issue in tools like cursor, claude code, etc and have a shared code intelligence layer for multiple agents. It provides a reliable way to retrieve full context important for codebase audits, blast radius detection of code changes and deep architectural understanding of the codebase for both humans and LLM. ( Ever encountered the issue where cursor updates some part of the codebase but fails to adapt other dependent functions around it ? this should solve it )&lt;/p&gt; &lt;p&gt;&lt;strong&gt;I tested it using cursor through MCP. Even without the impact tool and LLM enrichment feature, haiku 4.5 model was able to produce better Architecture documentation compared to opus 4.5 without MCP on PyBamm repo ( its a complex battery modelling repo )&lt;/strong&gt;. &lt;/p&gt; &lt;p&gt;Opus 4.5 was asked to get into as much detail as possible but haiku had a simple prompt asking it to explain the architecture. The output files were compared in chatgpt 5.2 chat link: &lt;a href="https://chatgpt.com/share/697a7a2c-9524-8009-8112-32b83c6c9fe4"&gt;https://chatgpt.com/share/697a7a2c-9524-8009-8112-32b83c6c9fe4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;( IK its not a good enough benchmark but still promising )&lt;/p&gt; &lt;p&gt;Quick tech jargon: &lt;/p&gt; &lt;p&gt;- Everything including db engine, embeddings model, all works in-browser client sided&lt;/p&gt; &lt;p&gt;- The project architecture flowchart u can see in the video is generated without LLM during repo ingestion so is reliable.&lt;/p&gt; &lt;p&gt;- Creates clusters ( using leidens algo ) and process maps during ingestion.&lt;/p&gt; &lt;p&gt;- It has all the usual tools like grep, semantic search, etc but enhanced majorly using process maps and clusters making the tool themselves smart hence a lot of the decisions the LLM had to make to retrieve context is offloaded into the tools, making it much more reliable even with non sota models.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What I need help with:&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;- To convert it into a actually useful product do u think I should make it like a CLI tool that keeps track of local code changes and updating the graph?&lt;/p&gt; &lt;p&gt;- Is there some way to get some free API credits or sponsorship or something so that I can test gitnexus with multiple providers&lt;/p&gt; &lt;p&gt;- Some insights into enterprise code problems like security audits or dead code detection or any other potential usecase I can tune gitnexus for?&lt;/p&gt; &lt;p&gt;Any cool idea and suggestion helps a lot. The comments on previous post helped a LOT, thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeathShot7777"&gt; /u/DeathShot7777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/b3d93edpq5gg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpp13g/building_opensource_zero_server_code_intelligence/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qpp13g/building_opensource_zero_server_code_intelligence/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T21:44:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpgrlg</id>
    <title>MiMo V2 Flash &amp; Kimi K2.5: How Chinese Models Are Democratizing AI</title>
    <updated>2026-01-28T16:52:06+00:00</updated>
    <author>
      <name>/u/prakersh</name>
      <uri>https://old.reddit.com/user/prakersh</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For years, the AI narrative has been simple: OpenAI, Google, and Anthropic build the best models, everyone else catches up. You pay premium API prices, accept their terms, and hope your data stays private.&lt;/p&gt; &lt;p&gt;That narrative is breaking down. Fast.&lt;/p&gt; &lt;p&gt;In the past few weeks, two Chinese labs dropped open-weight models that rival‚Äîand in some cases beat‚Äîthe best from Silicon Valley. Xiaomi's MiMo V2 Flash and Moonshot AI's Kimi K2.5 aren't just catching up. They're reshaping what &amp;quot;accessible AI&amp;quot; actually means. &lt;a href="https://onllm.dev/blog/2-mimo-v2-flash-kimi-k25-democratizing"&gt;https://onllm.dev/blog/2-mimo-v2-flash-kimi-k25-democratizing&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/prakersh"&gt; /u/prakersh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://onllm.dev/blog/2-mimo-v2-flash-kimi-k25-democratizing"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpgrlg/mimo_v2_flash_kimi_k25_how_chinese_models_are/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qpgrlg/mimo_v2_flash_kimi_k25_how_chinese_models_are/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T16:52:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpoi1y</id>
    <title>Olmo/Bolmo: Why is remote code needed?</title>
    <updated>2026-01-28T21:24:44+00:00</updated>
    <author>
      <name>/u/FrozenBuffalo25</name>
      <uri>https://old.reddit.com/user/FrozenBuffalo25</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When I went to try Bolmo-1B in vLLM, I got a message saying I need to enable 'trust remote code.' Which code? For what purpose? This should be explained in the model card, or preferably the requisite functionality should be just a PR into vLLM rather than (potentially) allowing arbitrary code execution.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FrozenBuffalo25"&gt; /u/FrozenBuffalo25 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpoi1y/olmobolmo_why_is_remote_code_needed/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpoi1y/olmobolmo_why_is_remote_code_needed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qpoi1y/olmobolmo_why_is_remote_code_needed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T21:24:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1qppjo4</id>
    <title>Assistant_Pepe_8B, 1-M context, zero slop</title>
    <updated>2026-01-28T22:03:49+00:00</updated>
    <author>
      <name>/u/Sicarius_The_First</name>
      <uri>https://old.reddit.com/user/Sicarius_The_First</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;This is a project that was a long time in the making because I wanted to get it right. I'm still not fully satisfied, as there are some rough corners to sand, but for now, this would do.&lt;/p&gt; &lt;p&gt;The goal was to &lt;strong&gt;maximize shitpostness&lt;/strong&gt; along with &lt;strong&gt;helpfulness&lt;/strong&gt;, without glazing the user for every retarded idea. Not an easy needle to thread.&lt;/p&gt; &lt;p&gt;This amphibious AI has learned the ways of /g/, and speaks &lt;strong&gt;fluent brainrot&lt;/strong&gt;, but will also help you out with just about anything you'll need, and won't be ashamed to roast you while at it.&lt;/p&gt; &lt;p&gt;For those who remember &lt;a href="https://huggingface.co/SicariusSicariiStuff/Oni_Mitsubishi_12B"&gt;Oni_Mitsubishi_12B&lt;/a&gt; - it was &lt;strong&gt;so overtly toxic&lt;/strong&gt; that it made me worry at first (only to quickly be verified as not even that uncensored). I could do better. So now I did.&lt;/p&gt; &lt;p&gt;This model is a &lt;strong&gt;significant refinement&lt;/strong&gt; of the idea, with a cleaned dataset, better curation, and with much more intelligence (also &lt;strong&gt;one million tokens of contexts&lt;/strong&gt;, theoretically). &lt;/p&gt; &lt;p&gt;It is much less (overtly) toxic, and much smarter, while also being very helpful (and imo much more funny too, because the skies are blue due to the chemtrails and neurlink that feeds this simulation)&lt;/p&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/SicariusSicariiStuff/Assistant_Pepe_8B#but-why"&gt;&lt;/a&gt;But why?&lt;/h1&gt; &lt;p&gt;It's now late &lt;strong&gt;January&lt;/strong&gt;, &lt;strong&gt;2026&lt;/strong&gt;, open source is crushing closed frontier (&lt;a href="https://huggingface.co/moonshotai/Kimi-K2.5"&gt;Kimi K2.5&lt;/a&gt; was recently released, &lt;strong&gt;1T&lt;/strong&gt; params that &lt;strong&gt;beats frontier models&lt;/strong&gt;), but has anyone released a &lt;strong&gt;helpful shitposting AI yet?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Yeah, didn't think so.&lt;/p&gt; &lt;p&gt;If it &lt;strong&gt;shitposts too hard&lt;/strong&gt;, it is often not that &lt;strong&gt;helpful&lt;/strong&gt;; if it's '&lt;strong&gt;helpful enough&lt;/strong&gt;, the &lt;strong&gt;shitposting ability is often lacking&lt;/strong&gt;. You just couldn't win. &lt;strong&gt;Until now&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Oh, and &lt;strong&gt;no system prompt is needed&lt;/strong&gt;. Just don't let it get stuck in a greentext loop. I might have overcooked the frog a tad bit too fast in the pot for this one.&lt;/p&gt; &lt;p&gt;P.S It writes &lt;strong&gt;HILARIOUS STORIES&lt;/strong&gt;, nothing like a typical AI assistant, see the examples below for details.&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/SicariusSicariiStuff/Assistant_Pepe_8B#tldr"&gt;&lt;/a&gt;TL;DR&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Top tier shitposting&lt;/strong&gt; absolutely unhinged, funny, and witty. Sometimes cringe too; nothing is perfect.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Helpful!&lt;/strong&gt; will actually get shit done.&lt;/li&gt; &lt;li&gt;Will &lt;strong&gt;100% roast you&lt;/strong&gt; for being dumb, thanks to a subtle &lt;strong&gt;negativity bias infusion&lt;/strong&gt;. Very &lt;strong&gt;refreshing!&lt;/strong&gt; ü§å&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Deep insights&lt;/strong&gt; (when it doesn't delve into absolutely unhinged conspiracy theories about how the water makes the frogs gay).&lt;/li&gt; &lt;li&gt;Built on my &lt;a href="https://huggingface.co/SicariusSicariiStuff/Llama-3.1-Nemotron-8B-UltraLong-1M-Instruct_Abliterated"&gt;UltraLong-1M-Instruct_Abliterated&lt;/a&gt; model, fulfill your dream of a &lt;strong&gt;million-token-long&lt;/strong&gt; shitpost.&lt;/li&gt; &lt;li&gt;Say goodbye to &lt;strong&gt;GPT-isms&lt;/strong&gt; and say hello to &lt;strong&gt;truly creative stories!&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Ships code.&lt;/li&gt; &lt;li&gt;Inclusive towards amphibians.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://huggingface.co/SicariusSicariiStuff/Assistant_Pepe_8B"&gt;https://huggingface.co/SicariusSicariiStuff/Assistant_Pepe_8B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sicarius_The_First"&gt; /u/Sicarius_The_First &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qppjo4/assistant_pepe_8b_1m_context_zero_slop/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qppjo4/assistant_pepe_8b_1m_context_zero_slop/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qppjo4/assistant_pepe_8b_1m_context_zero_slop/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T22:03:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpcdjg</id>
    <title>Should data centers be required to include emergency shutdown mechanisms as we have with nuclear power plants?</title>
    <updated>2026-01-28T14:10:43+00:00</updated>
    <author>
      <name>/u/FinnFarrow</name>
      <uri>https://old.reddit.com/user/FinnFarrow</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpcdjg/should_data_centers_be_required_to_include/"&gt; &lt;img alt="Should data centers be required to include emergency shutdown mechanisms as we have with nuclear power plants?" src="https://external-preview.redd.it/dnQ2eHBuOWNsM2dnMbsCN6d3L6_HXnnRFCSSTAWqyWY23lOHKBvxCRMllsYF.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c21745e2d5cd89c46aea64639f6fc8d263fdf360" title="Should data centers be required to include emergency shutdown mechanisms as we have with nuclear power plants?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FinnFarrow"&gt; /u/FinnFarrow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/xvo60r3cl3gg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpcdjg/should_data_centers_be_required_to_include/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qpcdjg/should_data_centers_be_required_to_include/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T14:10:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1qoty38</id>
    <title>Kimi K2.5 costs almost 10% of what Opus costs at a similar performance</title>
    <updated>2026-01-27T23:10:16+00:00</updated>
    <author>
      <name>/u/Odd_Tumbleweed574</name>
      <uri>https://old.reddit.com/user/Odd_Tumbleweed574</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qoty38/kimi_k25_costs_almost_10_of_what_opus_costs_at_a/"&gt; &lt;img alt="Kimi K2.5 costs almost 10% of what Opus costs at a similar performance" src="https://preview.redd.it/xz7okply3zfg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=74631e6bf621619eca977768f8c7287dc6164e45" title="Kimi K2.5 costs almost 10% of what Opus costs at a similar performance" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been trying out Kimi k2.5 and this is the first time that I feel an open model is truly competitive with SOTA closed models.&lt;/p&gt; &lt;p&gt;Compared to GLM, Kimi is a bit better, specially when it comes to non-website tasks.&lt;/p&gt; &lt;p&gt;Have you tried it? What's your take?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Odd_Tumbleweed574"&gt; /u/Odd_Tumbleweed574 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xz7okply3zfg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qoty38/kimi_k25_costs_almost_10_of_what_opus_costs_at_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qoty38/kimi_k25_costs_almost_10_of_what_opus_costs_at_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-27T23:10:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpkbb4</id>
    <title>Running Kimi K2.5 at 24 token/s with 2 x 512GB M3 Ultra Mac Studios</title>
    <updated>2026-01-28T18:53:08+00:00</updated>
    <author>
      <name>/u/Zestyclose_Slip_6467</name>
      <uri>https://old.reddit.com/user/Zestyclose_Slip_6467</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpkbb4/running_kimi_k25_at_24_tokens_with_2_x_512gb_m3/"&gt; &lt;img alt="Running Kimi K2.5 at 24 token/s with 2 x 512GB M3 Ultra Mac Studios" src="https://b.thumbs.redditmedia.com/e1osIFOLvrOqDjp2BbW21abJU9c9ylrd1K0acMpLc_w.jpg" title="Running Kimi K2.5 at 24 token/s with 2 x 512GB M3 Ultra Mac Studios" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/p7jc0fkqz4gg1.jpg?width=1182&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=184e9a714d225a7eaa870d649f682df8b3220f3b"&gt;https://preview.redd.it/p7jc0fkqz4gg1.jpg?width=1182&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=184e9a714d225a7eaa870d649f682df8b3220f3b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So Cooooool!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zestyclose_Slip_6467"&gt; /u/Zestyclose_Slip_6467 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpkbb4/running_kimi_k25_at_24_tokens_with_2_x_512gb_m3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpkbb4/running_kimi_k25_at_24_tokens_with_2_x_512gb_m3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qpkbb4/running_kimi_k25_at_24_tokens_with_2_x_512gb_m3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T18:53:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1qp814d</id>
    <title>Sam Altman Says OpenAI Is Slashing Its Hiring Pace as Financial Crunch Tightens</title>
    <updated>2026-01-28T10:43:40+00:00</updated>
    <author>
      <name>/u/EchoOfOppenheimer</name>
      <uri>https://old.reddit.com/user/EchoOfOppenheimer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qp814d/sam_altman_says_openai_is_slashing_its_hiring/"&gt; &lt;img alt="Sam Altman Says OpenAI Is Slashing Its Hiring Pace as Financial Crunch Tightens" src="https://external-preview.redd.it/js5rUnxnssRKiIPCHK8jH1gE4HhRk8kF6Ui2iEIglL4.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=00c0125d100a3eaf17a548840fc4935c5b5cac3c" title="Sam Altman Says OpenAI Is Slashing Its Hiring Pace as Financial Crunch Tightens" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In a livestreamed town hall, Sam Altman admitted OpenAI is 'dramatically slowing down' hiring as the company faces increasing financial pressure. This follows reports of an internal 'Code Red' memo urging staff to fix ChatGPT as competitors gain ground. With analysts warning of an 'Enron-like' cash crunch within 18 months and the company resorting to ads for revenue, the era of unlimited AI spending appears to be hitting a wall.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EchoOfOppenheimer"&gt; /u/EchoOfOppenheimer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://futurism.com/artificial-intelligence/sam-altman-openai-slashing-hiring"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qp814d/sam_altman_says_openai_is_slashing_its_hiring/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qp814d/sam_altman_says_openai_is_slashing_its_hiring/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T10:43:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpdn1t</id>
    <title>FASHN VTON v1.5: Apache-2.0 virtual try-on model, runs on consumer GPUs (~8GB VRAM), ~1B params</title>
    <updated>2026-01-28T14:59:46+00:00</updated>
    <author>
      <name>/u/JYP_Scouter</name>
      <uri>https://old.reddit.com/user/JYP_Scouter</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpdn1t/fashn_vton_v15_apache20_virtual_tryon_model_runs/"&gt; &lt;img alt="FASHN VTON v1.5: Apache-2.0 virtual try-on model, runs on consumer GPUs (~8GB VRAM), ~1B params" src="https://external-preview.redd.it/dzB2eTU3dTB0M2dnMQAcCp4XjN-WlzBmxKbhH4j6EHXSl-nuFCETM3brGZzN.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aa21a6beabb7206bc0fa0f9980387312dbb7c8ce" title="FASHN VTON v1.5: Apache-2.0 virtual try-on model, runs on consumer GPUs (~8GB VRAM), ~1B params" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We just open-sourced FASHN VTON v1.5, a virtual try-on model that generates photorealistic images of people wearing garments. We've been running this as a production API for the past year, and now we're releasing the weights and inference code under Apache-2.0.&lt;/p&gt; &lt;h1&gt;Why we're releasing this&lt;/h1&gt; &lt;p&gt;Most open-source VTON models are either research prototypes that require significant engineering to deploy, or they're locked behind restrictive licenses. As state-of-the-art capabilities consolidate into massive generalist models, we think there's value in releasing focused, efficient models that researchers and developers can actually own, study, and extend commercially.&lt;/p&gt; &lt;p&gt;We also want to demonstrate that competitive results in this domain don't require massive compute budgets. Total training cost was in the $5-10k range on rented A100s.&lt;/p&gt; &lt;p&gt;This follows our &lt;a href="https://www.reddit.com/r/MachineLearning/comments/1qax221/p_opensourcing_a_human_parsing_model_trained_on/"&gt;human parser release&lt;/a&gt; from a couple weeks ago.&lt;/p&gt; &lt;h1&gt;Specs&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Parameters:&lt;/strong&gt; 972M&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Architecture:&lt;/strong&gt; Custom MMDiT&lt;/li&gt; &lt;li&gt;&lt;strong&gt;VRAM:&lt;/strong&gt; ~8GB minimum&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hardware:&lt;/strong&gt; Runs on consumer GPUs (RTX 30xx/40xx)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Latency:&lt;/strong&gt; ~5 seconds on H100&lt;/li&gt; &lt;li&gt;&lt;strong&gt;License:&lt;/strong&gt; Apache-2.0 (fully permissive, commercial use allowed)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Technical highlights&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Pixel-space operation:&lt;/strong&gt; Unlike most diffusion models that work in VAE latent space, we operate directly on RGB pixels. This avoids lossy VAE encoding/decoding that can blur fine garment details like textures, patterns, and text.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Maskless inference:&lt;/strong&gt; No segmentation mask required on the target person. The model learns where clothing boundaries should be rather than being told.&lt;/p&gt; &lt;h1&gt;Links&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/fashn-AI/fashn-vton-1.5"&gt;fashn-AI/fashn-vton-1.5&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;HuggingFace:&lt;/strong&gt; &lt;a href="https://huggingface.co/fashn-ai/fashn-vton-1.5"&gt;fashn-ai/fashn-vton-1.5&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Project page:&lt;/strong&gt; &lt;a href="https://fashn.ai/research/vton-1-5"&gt;fashn.ai/research/vton-1-5&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Quick example&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;from fashn_vton import TryOnPipeline from PIL import Image pipeline = TryOnPipeline(weights_dir=&amp;quot;./weights&amp;quot;) person = Image.open(&amp;quot;person.jpg&amp;quot;).convert(&amp;quot;RGB&amp;quot;) garment = Image.open(&amp;quot;garment.jpg&amp;quot;).convert(&amp;quot;RGB&amp;quot;) result = pipeline( person_image=person, garment_image=garment, category=&amp;quot;tops&amp;quot;, ) result.images[0].save(&amp;quot;output.png&amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Coming soon&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;HuggingFace Space:&lt;/strong&gt; Online demo&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Technical paper:&lt;/strong&gt; Architecture decisions, training methodology, and design rationale&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Happy to answer questions about running this locally or the implementation.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JYP_Scouter"&gt; /u/JYP_Scouter &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/x689lnt0t3gg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpdn1t/fashn_vton_v15_apache20_virtual_tryon_model_runs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qpdn1t/fashn_vton_v15_apache20_virtual_tryon_model_runs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T14:59:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpjc4a</id>
    <title>Add self‚Äëspeculative decoding (no draft model required) by srogmann ¬∑ Pull Request #18471 ¬∑ ggml-org/llama.cpp</title>
    <updated>2026-01-28T18:19:15+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;tl;dr: potential &lt;strong&gt;t/s boost&lt;/strong&gt; for all (non-reasoning) models &lt;/p&gt; &lt;p&gt;This looks really interesting, but needs more investigation.&lt;br /&gt; Speculative decoding uses a smaller draft model to speed up a bigger one.&lt;br /&gt; &lt;strong&gt;Self-speculative decoding&lt;/strong&gt; uses no extra model at all, the model is helping itself.&lt;br /&gt; It only speeds up certain workloads with a lot of repetition, should be especially useful for coding and refactoring tasks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/18471"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpjc4a/add_selfspeculative_decoding_no_draft_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qpjc4a/add_selfspeculative_decoding_no_draft_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T18:19:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpneiq</id>
    <title>AMD Strix Halo GMTEK 128GB Unified ROCKS!</title>
    <updated>2026-01-28T20:43:57+00:00</updated>
    <author>
      <name>/u/MSBStudio</name>
      <uri>https://old.reddit.com/user/MSBStudio</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been running a MAX+ 395 as my daily workstation ‚Äî the unified memory architecture &lt;/p&gt; &lt;p&gt;is a game-changer for AI/ML workloads. Being able to allocate 96GB+ to the GPU without the PCIe bottleneck makes local LLM. DeepSeek 70B *12 tokens/s, gpt-oss faster, comfyui with LTX2 12 s/it this is a game changer...no quants not hassle. In if you need check out my GIT I have step by step &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/bkpaine1"&gt;https://github.com/bkpaine1&lt;/a&gt; have some comfyui nodes for AMD and walk throughs to get beast cranking! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MSBStudio"&gt; /u/MSBStudio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpneiq/amd_strix_halo_gmtek_128gb_unified_rocks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpneiq/amd_strix_halo_gmtek_128gb_unified_rocks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qpneiq/amd_strix_halo_gmtek_128gb_unified_rocks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T20:43:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpllhm</id>
    <title>ACE-Step 1.5 dropping in days - "Commercial grade OSS music gen" with quality between Suno v4.5 and v5 (8GB VRAM)</title>
    <updated>2026-01-28T19:38:05+00:00</updated>
    <author>
      <name>/u/ExcellentTrust4433</name>
      <uri>https://old.reddit.com/user/ExcellentTrust4433</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those who haven't been following the AI music generation space, ACE-Step is about to have its &amp;quot;Stable Diffusion moment.&amp;quot;&lt;/p&gt; &lt;h2&gt;What's Happening&lt;/h2&gt; &lt;p&gt;According to [@realmrfakename on X](&lt;a href="https://x.com/realmrfakename/status/2016274138701476040"&gt;https://x.com/realmrfakename/status/2016274138701476040&lt;/a&gt;) (7K+ views), ACE-Step 1.5 is coming in days with early access already rolling out.&lt;/p&gt; &lt;p&gt;**Key claims:** - Quality &amp;quot;somewhere between Suno v4.5 and v5&amp;quot; - &amp;quot;Far better than HeartMuLa or DiffRhythm&amp;quot; - &amp;quot;We finally have commercial grade OSS music gen&amp;quot;&lt;/p&gt; &lt;h2&gt;Why This Matters for Local AI&lt;/h2&gt; &lt;p&gt;**ACE-Step v1** already runs on **8GB VRAM** with CPU offload. It's a 3.5B parameter model that generates full songs with vocals + instrumentals + lyrics in 19 languages.&lt;/p&gt; &lt;p&gt;**Speed:** 4 minutes of music in ~20 seconds on A100, ~1.7s on RTX 4090&lt;/p&gt; &lt;p&gt;If v1.5 delivers on the quality claims while keeping the same hardware requirements, this could be huge for: - Local music generation without cloud dependencies - LoRA fine-tuning for custom voices/styles - Integration into creative workflows&lt;/p&gt; &lt;h2&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;[GitHub](&lt;a href="https://github.com/ace-step/ACE-Step"&gt;https://github.com/ace-step/ACE-Step&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;[HuggingFace](&lt;a href="https://huggingface.co/ACE-Step/ACE-Step-v1-3.5B"&gt;https://huggingface.co/ACE-Step/ACE-Step-v1-3.5B&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;[Demo Space](&lt;a href="https://huggingface.co/spaces/ACE-Step/ACE-Step"&gt;https://huggingface.co/spaces/ACE-Step/ACE-Step&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;[Technical Report](&lt;a href="https://arxiv.org/abs/2506.00045"&gt;https://arxiv.org/abs/2506.00045&lt;/a&gt;)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Also created &lt;a href="/r/ACEStepGen"&gt;r/ACEStepGen&lt;/a&gt; for dedicated discussions if anyone's interested.&lt;/p&gt; &lt;p&gt;Anyone here tried the current v1? Curious about real-world experiences with quality and inference speed.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ExcellentTrust4433"&gt; /u/ExcellentTrust4433 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpllhm/acestep_15_dropping_in_days_commercial_grade_oss/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpllhm/acestep_15_dropping_in_days_commercial_grade_oss/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qpllhm/acestep_15_dropping_in_days_commercial_grade_oss/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T19:38:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpkem2</id>
    <title>Image generation is now available alongside LLMs and Whisper in Lemonade v9.2</title>
    <updated>2026-01-28T18:56:13+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpkem2/image_generation_is_now_available_alongside_llms/"&gt; &lt;img alt="Image generation is now available alongside LLMs and Whisper in Lemonade v9.2" src="https://preview.redd.it/sbfse0xez4gg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=344e447c355af53401fc4c3ee2b177014b36bc61" title="Image generation is now available alongside LLMs and Whisper in Lemonade v9.2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We're on a mission to make local generative AI supremely easy for users and devs. Today, Lemonade has taken a big step by introducing image generation into our unified local API.&lt;/p&gt; &lt;p&gt;This means our one-click installer gets you LLMs, Whisper, and Stable Diffusion and makes them all available on the same base URL.&lt;/p&gt; &lt;p&gt;We'll use these capabilities to build local apps and agents that are more powerful and natural to interact with. What would a unified multi-modal server help you build?&lt;/p&gt; &lt;p&gt;Load models:&lt;/p&gt; &lt;p&gt;&lt;code&gt; lemonade-server run SD-Turbo lemonade-server run Whisper-Large-v3 lemonade-server run GLM-4.7-Flash-GGUF &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Endpoints:&lt;/p&gt; &lt;p&gt;&lt;code&gt; /api/v1/images/generations /api/v1/audio/transcriptions /api/v1/chat/completions &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Today is just the beginning, introducing the fundamental capability and enabling the endpoints. Future work to enable multi-modal local AI apps includes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Add Z-Image and other SOTA models to &lt;code&gt;images/generations&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;Add ROCm, Vulkan, and AMD NPU builds for &lt;code&gt;images/generations&lt;/code&gt; and &lt;code&gt;audio/transcriptions&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;Streaming input support for &lt;code&gt;audio/transcriptions&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;Introduce a text-to-speech endpoint.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you like what we're doing, please support the project with a star on the &lt;a href="https://github.com/lemonade-sdk/lemonade"&gt;lemonade GitHub&lt;/a&gt; and come hang out with us on &lt;a href="https://discord.gg/5xXzkMu8Zk"&gt;Discord&lt;/a&gt;!&lt;/p&gt; &lt;p&gt;PS. as always huge thanks to the maintainers of llama.cpp, stablediffusion.cpp, whisper.cpp, and the other tools lemonade builds on.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sbfse0xez4gg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpkem2/image_generation_is_now_available_alongside_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qpkem2/image_generation_is_now_available_alongside_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T18:56:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpde5g</id>
    <title>Backup those models, because of calls for regulations</title>
    <updated>2026-01-28T14:50:06+00:00</updated>
    <author>
      <name>/u/ProfessionalSpend589</name>
      <uri>https://old.reddit.com/user/ProfessionalSpend589</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.euronews.com/next/2026/01/28/humanity-needs-to-wake-up-to-ai-threats-anthropic-ceo-says"&gt;‚ÄòHumanity needs to wake up‚Äô to AI threats, Anthropic CEO says&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&amp;gt; Dario Amodei, the CEO of Anthropic, says that humanity needs to regulate the use of AI,‚Ä¶&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ProfessionalSpend589"&gt; /u/ProfessionalSpend589 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpde5g/backup_those_models_because_of_calls_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpde5g/backup_those_models_because_of_calls_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qpde5g/backup_those_models_because_of_calls_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T14:50:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpj0i1</id>
    <title>Introducing LM Studio 0.4.0</title>
    <updated>2026-01-28T18:08:04+00:00</updated>
    <author>
      <name>/u/sleepingsysadmin</name>
      <uri>https://old.reddit.com/user/sleepingsysadmin</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpj0i1/introducing_lm_studio_040/"&gt; &lt;img alt="Introducing LM Studio 0.4.0" src="https://external-preview.redd.it/ONJ57JrCEXk7COEJSakptLDGIozjYb7vtKbjnKraCHs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a3acf1e85d0e36eae10cc58480d7a5498c2ec4e4" title="Introducing LM Studio 0.4.0" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Testing out Parralel setting, default is 4, i tried 2, i tried 40. Overall no change at all in performance for me. &lt;/p&gt; &lt;p&gt;I havent changed unified kv cache, on by default. Seems to be fine. &lt;/p&gt; &lt;p&gt;New UI moved the runtimes into settings, but they are hidden unless you enable developer in settings. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sleepingsysadmin"&gt; /u/sleepingsysadmin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://lmstudio.ai/blog/0.4.0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpj0i1/introducing_lm_studio_040/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qpj0i1/introducing_lm_studio_040/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T18:08:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpm48y</id>
    <title>ByteDance-Seed/Stable-DiffCoder-8B-Instruct ¬∑ Hugging Face</title>
    <updated>2026-01-28T19:56:52+00:00</updated>
    <author>
      <name>/u/FullstackSensei</name>
      <uri>https://old.reddit.com/user/FullstackSensei</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpm48y/bytedanceseedstablediffcoder8binstruct_hugging/"&gt; &lt;img alt="ByteDance-Seed/Stable-DiffCoder-8B-Instruct ¬∑ Hugging Face" src="https://external-preview.redd.it/NHXAooAa4KDswUx_-kPck6PlutEBvd6amAoPNHHGh0s.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8ee5c6bc1687d34fcebba3988d582836917613a0" title="ByteDance-Seed/Stable-DiffCoder-8B-Instruct ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Diffusion text/coding models are finally tricking in!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullstackSensei"&gt; /u/FullstackSensei &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/ByteDance-Seed/Stable-DiffCoder-8B-Instruct"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpm48y/bytedanceseedstablediffcoder8binstruct_hugging/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qpm48y/bytedanceseedstablediffcoder8binstruct_hugging/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T19:56:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1qphkd8</id>
    <title>[Release] BitMamba-2-1B: I trained a 1.58-bit Mamba-2 model from scratch on 150B tokens (Runs on CPU @ 50+ tok/s)</title>
    <updated>2026-01-28T17:19:36+00:00</updated>
    <author>
      <name>/u/Positive-Violinist90</name>
      <uri>https://old.reddit.com/user/Positive-Violinist90</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;I‚Äôve been working on scaling efficient architectures and just released &lt;strong&gt;BitMamba-2&lt;/strong&gt;, a hybrid model combining &lt;strong&gt;Mamba-2 SSM with BitNet 1.58-bit quantization.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The goal was to prove that ternary scaling laws hold up even for SSMs, and to enable decent inference on legacy hardware/edge devices without heavy GPUs.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Specs:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Architecture:&lt;/strong&gt; Mamba-2 + BitNet b1.58 (Ternary weights {-1, 0, 1})&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Training:&lt;/strong&gt; Trained from scratch on 150B tokens (FineWeb-Edu, Cosmopedia, Stack-Dedup) using Google TPU v6e-8.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Performance:&lt;/strong&gt; The 1B model beats the 255M baseline significantly, validating the scaling laws (You can check the loss curves in the repo).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I wrote a custom C++ inference engine for this. On a consumer &lt;strong&gt;Intel Core i3-12100F (CPU only)&lt;/strong&gt;, I'm getting:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;BitMamba-2-1B:&lt;/strong&gt; ~53 tokens/sec (621 MB RAM)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;BitMamba-2-255M:&lt;/strong&gt; ~146 tokens/sec (252 MB RAM)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It‚Äôs fully open-source (Apache/MIT). I‚Äôd love for you guys to test it and let me know what you think about the generation quality vs. pure transformers.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Paper (Zenodo):&lt;/strong&gt; &lt;a href="https://zenodo.org/records/18394665"&gt;https://zenodo.org/records/18394665&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hugging Face (Weights):&lt;/strong&gt; &lt;a href="https://huggingface.co/Zhayr1/BitMamba-2-1B"&gt;https://huggingface.co/Zhayr1/BitMamba-2-1B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GitHub (JAX Code):&lt;/strong&gt; &lt;a href="https://github.com/Zhayr1/BitMamba-2"&gt;https://github.com/Zhayr1/BitMamba-2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GitHub (C++ Inference):&lt;/strong&gt; &lt;a href="https://github.com/Zhayr1/bitmamba.cpp"&gt;https://github.com/Zhayr1/bitmamba.cpp&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Let me know if you have questions about the training dynamics or the C++ implementation.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Positive-Violinist90"&gt; /u/Positive-Violinist90 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qphkd8/release_bitmamba21b_i_trained_a_158bit_mamba2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qphkd8/release_bitmamba21b_i_trained_a_158bit_mamba2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qphkd8/release_bitmamba21b_i_trained_a_158bit_mamba2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T17:19:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpi8d4</id>
    <title>meituan-longcat/LongCat-Flash-Lite</title>
    <updated>2026-01-28T17:42:14+00:00</updated>
    <author>
      <name>/u/windows_error23</name>
      <uri>https://old.reddit.com/user/windows_error23</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpi8d4/meituanlongcatlongcatflashlite/"&gt; &lt;img alt="meituan-longcat/LongCat-Flash-Lite" src="https://external-preview.redd.it/SP0mBN3iaWZga0fzz80Atyk8zYq_GP0yB1tyV6wtLlw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bfbc45df0f4f6a1d3dd5267993dbfc464f761f81" title="meituan-longcat/LongCat-Flash-Lite" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/windows_error23"&gt; /u/windows_error23 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/meituan-longcat/LongCat-Flash-Lite"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpi8d4/meituanlongcatlongcatflashlite/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qpi8d4/meituanlongcatlongcatflashlite/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T17:42:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1qp6rm5</id>
    <title>API pricing is in freefall. What's the actual case for running local now beyond privacy?</title>
    <updated>2026-01-28T09:27:55+00:00</updated>
    <author>
      <name>/u/Distinct-Expression2</name>
      <uri>https://old.reddit.com/user/Distinct-Expression2</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;K2.5 just dropped at roughly 10% of Opus pricing with competitive benchmarks. Deepseek is practically free. Gemini has a massive free tier. Every month the API cost floor drops another 50%.&lt;/p&gt; &lt;p&gt;Meanwhile running a 70B locally still means either a k+ GPU or dealing with quantization tradeoffs and 15 tok/s on consumer hardware.&lt;/p&gt; &lt;p&gt;I've been running local for about a year now and I'm genuinely starting to question the math. The three arguments I keep hearing:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Privacy&lt;/strong&gt; ‚Äî legit, no argument. If you're processing sensitive data, local is the only option.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;No rate limits&lt;/strong&gt; ‚Äî fair, but most providers have pretty generous limits now unless you're doing something unusual.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;&amp;quot;It's free after hardware costs&amp;quot;&lt;/strong&gt; ‚Äî this one aged poorly. That 3090 isn't free, electricity isn't free, and your time configuring and optimizing isn't free. At current API rates you'd need to run millions of tokens before breaking even.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The argument I never hear but actually find compelling: &lt;strong&gt;latency control and customization&lt;/strong&gt;. If you need a fine-tuned model for a specific domain with predictable latency, local still wins. But that's a pretty niche use case.&lt;/p&gt; &lt;p&gt;What's keeping you all running local at this point? Genuinely curious if I'm missing something or if the calculus has actually shifted.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Distinct-Expression2"&gt; /u/Distinct-Expression2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qp6rm5/api_pricing_is_in_freefall_whats_the_actual_case/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qp6rm5/api_pricing_is_in_freefall_whats_the_actual_case/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qp6rm5/api_pricing_is_in_freefall_whats_the_actual_case/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T09:27:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpmay0</id>
    <title>I just got my Dell DGX Spark GB10 that I won from the hackathon!</title>
    <updated>2026-01-28T20:03:20+00:00</updated>
    <author>
      <name>/u/brandon-i</name>
      <uri>https://old.reddit.com/user/brandon-i</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpmay0/i_just_got_my_dell_dgx_spark_gb10_that_i_won_from/"&gt; &lt;img alt="I just got my Dell DGX Spark GB10 that I won from the hackathon!" src="https://preview.redd.it/af2u39y6a5gg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d772153c2ea4d039aff7359c493d9106f7a82aae" title="I just got my Dell DGX Spark GB10 that I won from the hackathon!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Please don't mind the breadcrumbs... &lt;/p&gt; &lt;p&gt;But they pretty much overnighted the Dell DGX Spark GB10.&lt;/p&gt; &lt;p&gt;I think the first thing I am going to try and do is figure out how to get a robot arm to do some sort of shape matching using transfer learning to stick particular shapes in the correct holes. I think that might be easy enough? (I am naive because I haven't done transfer learning or physical AI yet)&lt;/p&gt; &lt;p&gt;I also want to try using LTX and see if it can recreate the ending for How I Met Your Mother or Game of Thrones (if it is able to do that). Might honestly be difficult because I haven't worked with vision models other than image creation using Fal.ai. I wonder if this machine can handle it.&lt;/p&gt; &lt;p&gt;Otherwise, I am going to keep hammering at figuring out better ways of solving the Social Determinants of Health problem. There are a lot of correlations that I wasn't able to completely finish within the limited amount of time for example:&lt;/p&gt; &lt;p&gt;Crime, lack of parks, and food insecurity increases chronic disease risk because people do not feel safe to leave their homes and exercise or walk and often times default to junk food as there are no other culturally sensitive alternatives leading to obesity and higher cardiovascular.&lt;/p&gt; &lt;p&gt;It would be also great if my AI Agents can go through some research paper and identify some of the most crucial ones that I can at least bake into the platform as a baseline that might be effecting other cities.&lt;/p&gt; &lt;p&gt;Also since I have 4 TB SSD I can potentially add the data from a bunch of different cities and start doing some pattern matching/correlation detection between this generally siloed data and see if I could suggest specific campaigns for the cities that would help unrepresented people get better access to care. &lt;/p&gt; &lt;p&gt;One of my passions (and I know this sounds really nerdy) is to create really good multi-turn evaluation harnesses that can use Process Supervised Reward Models to better train complex AI agents and self-heal.&lt;/p&gt; &lt;p&gt;If anyone has advice on any of this I would love to hear it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/brandon-i"&gt; /u/brandon-i &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/af2u39y6a5gg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpmay0/i_just_got_my_dell_dgx_spark_gb10_that_i_won_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qpmay0/i_just_got_my_dell_dgx_spark_gb10_that_i_won_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T20:03:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpfse6</id>
    <title>Run Kimi K2.5 Locally</title>
    <updated>2026-01-28T16:17:45+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpfse6/run_kimi_k25_locally/"&gt; &lt;img alt="Run Kimi K2.5 Locally" src="https://preview.redd.it/rxqfj5os74gg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2606f30079a77f14bb28c31413be651c092abaa9" title="Run Kimi K2.5 Locally" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Kimi-K2.5 achieves SOTA performance in vision, coding, agentic and chat tasks. &lt;/p&gt; &lt;p&gt;The 1T parameter hybrid reasoning model requires 600GB of disk space, while the quantized &lt;strong&gt;Unsloth Dynamic 1.8-bit&lt;/strong&gt; version reduces this to &lt;strong&gt;240GB (-60% size).&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Model:&lt;/strong&gt; &lt;a href="https://huggingface.co/unsloth/Kimi-K2.5-GGUF"&gt;&lt;strong&gt;Kimi-K2.5-GGUF&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Official Guide:&lt;/strong&gt; &lt;a href="https://unsloth.ai/docs/models/kimi-k2.5"&gt;&lt;strong&gt;https://unsloth.ai/docs/models/kimi-k2.5&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rxqfj5os74gg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpfse6/run_kimi_k25_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qpfse6/run_kimi_k25_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T16:17:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1qp87tk</id>
    <title>Kimi K2.5 is the best open model for coding</title>
    <updated>2026-01-28T10:54:13+00:00</updated>
    <author>
      <name>/u/npc_gooner</name>
      <uri>https://old.reddit.com/user/npc_gooner</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qp87tk/kimi_k25_is_the_best_open_model_for_coding/"&gt; &lt;img alt="Kimi K2.5 is the best open model for coding" src="https://preview.redd.it/unxlhercm2gg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=23f59fb1153598db9b9c8a3b5ce9067435dfba28" title="Kimi K2.5 is the best open model for coding" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;they really cooked&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/npc_gooner"&gt; /u/npc_gooner &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/unxlhercm2gg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qp87tk/kimi_k25_is_the_best_open_model_for_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qp87tk/kimi_k25_is_the_best_open_model_for_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T10:54:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1qp46za</id>
    <title>AMA Announcement: Moonshot AI, The Opensource Frontier Lab Behind Kimi K2.5 SoTA Model (Wednesday, 8AM-11AM PST)</title>
    <updated>2026-01-28T06:54:28+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qp46za/ama_announcement_moonshot_ai_the_opensource/"&gt; &lt;img alt="AMA Announcement: Moonshot AI, The Opensource Frontier Lab Behind Kimi K2.5 SoTA Model (Wednesday, 8AM-11AM PST)" src="https://preview.redd.it/y2qj7ancf1gg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7bb1df11c9d46ca94be0db3438449dc28e2dd48e" title="AMA Announcement: Moonshot AI, The Opensource Frontier Lab Behind Kimi K2.5 SoTA Model (Wednesday, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; üëã&lt;/p&gt; &lt;p&gt;We're excited for Wednesday's guests, &lt;strong&gt;The Moonshot AI Lab Team!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Kicking things off Wednesday, Jan. 28th, 8 AM‚Äì11 AM PST&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;‚ö†Ô∏è &lt;strong&gt;Note:&lt;/strong&gt; The AMA itself will be hosted in a &lt;strong&gt;separate thread,&lt;/strong&gt; please don‚Äôt post questions here.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/y2qj7ancf1gg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qp46za/ama_announcement_moonshot_ai_the_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qp46za/ama_announcement_moonshot_ai_the_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T06:54:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpewj7</id>
    <title>AMA With Kimi, The Open-source Frontier Lab Behind Kimi K2.5 Model</title>
    <updated>2026-01-28T15:46:40+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"&gt; &lt;img alt="AMA With Kimi, The Open-source Frontier Lab Behind Kimi K2.5 Model" src="https://b.thumbs.redditmedia.com/Tdp5mXDQmwe_e0sKIEY1hjWX6FrN679odChZp8YL2Rk.jpg" title="AMA With Kimi, The Open-source Frontier Lab Behind Kimi K2.5 Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Today we are having &lt;strong&gt;Kimi&lt;/strong&gt;, the research lab behind the &lt;strong&gt;Kimi&lt;/strong&gt; &lt;strong&gt;K2.5&lt;/strong&gt;. We‚Äôre excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/ComfortableAsk4494/"&gt;u/ComfortableAsk4494&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/zxytim/"&gt;u/zxytim&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/ppwwyyxx/"&gt;u/ppwwyyxx&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 8 AM ‚Äì 11 AM PST, with the Kimi team continuing to follow up on questions over the next 24 hours.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3yq8msvp24gg1.png?width=2000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=98c89b5d86ee1197799532fead6a84da2223b389"&gt;https://preview.redd.it/3yq8msvp24gg1.png?width=2000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=98c89b5d86ee1197799532fead6a84da2223b389&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Thanks everyone for joining our AMA. The live part has ended and the Kimi team will be following up with more answers sporadically over the next 24 hours.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T15:46:40+00:00</published>
  </entry>
</feed>
