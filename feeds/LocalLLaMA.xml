<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-17T23:11:15+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1r7j9kp</id>
    <title>What Frontend do you use?</title>
    <updated>2026-02-17T21:24:36+00:00</updated>
    <author>
      <name>/u/TyedalWaves</name>
      <uri>https://old.reddit.com/user/TyedalWaves</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been on and off with front-ends, but I really just want something that has a lot of capabilities and is relatively user friendly. I'm not a big fan of openwebui personally. There's nothing wrong with it, it's just not for me. What Frontends do you guys like?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TyedalWaves"&gt; /u/TyedalWaves &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7j9kp/what_frontend_do_you_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7j9kp/what_frontend_do_you_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r7j9kp/what_frontend_do_you_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T21:24:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1r72ddg</id>
    <title>Qwen 3.5 vs Gemini 3 Pro on Screenshot-to-Code: Is the gap finally gone?</title>
    <updated>2026-02-17T10:20:28+00:00</updated>
    <author>
      <name>/u/Awkward_Run_9982</name>
      <uri>https://old.reddit.com/user/Awkward_Run_9982</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r72ddg/qwen_35_vs_gemini_3_pro_on_screenshottocode_is/"&gt; &lt;img alt="Qwen 3.5 vs Gemini 3 Pro on Screenshot-to-Code: Is the gap finally gone?" src="https://preview.redd.it/5bfrjlz861kg1.jpg?width=140&amp;amp;height=68&amp;amp;auto=webp&amp;amp;s=d437ebbb410b077aa5b02561d21953400cde3a16" title="Qwen 3.5 vs Gemini 3 Pro on Screenshot-to-Code: Is the gap finally gone?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been testing the new Qwen 3.5-397B against Gemini 3 and Kimi K2.5. The task was simple but tricky: Give it a high-res screenshot of a complex Hugging Face dataset page and ask for a functional Tailwind frontend.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The results are… interesting.&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Qwen 3.5 (The Layout King):&lt;/strong&gt; I was genuinely surprised. It nailed the sidebar grid better than Gemini. While Gemini usually wins on &amp;quot;vibes,&amp;quot; Qwen actually followed the structural constraints of the UI better. It didn't hallucinate the layout as much as Kimi did.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Gemini 3 Pro:&lt;/strong&gt; Still has the edge on OCR. It’s the only one that correctly grabbed the tiny SVG logos (pandas/polars). Qwen just put generic icons there.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Kimi K2.5:&lt;/strong&gt; Feels very &amp;quot;polished&amp;quot; in terms of code quality (cleaner components), but it took too many creative liberties with the layout.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Local Context:&lt;/strong&gt; I was testing this via openrouter. If you're running the 397B locally on a Mac or a cluster, the MoE efficiency makes the inference speed surprisingly usable.&lt;/p&gt; &lt;p&gt;Is anyone else seeing Qwen outperform Gemini on structural vision tasks? I feel like we’re hitting a point where open-access models are basically on par for coding agents.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Awkward_Run_9982"&gt; /u/Awkward_Run_9982 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1r72ddg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r72ddg/qwen_35_vs_gemini_3_pro_on_screenshottocode_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r72ddg/qwen_35_vs_gemini_3_pro_on_screenshottocode_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T10:20:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1r76gi7</id>
    <title>I made a CLI that turns any podcast or YouTube video into clean Markdown transcripts (speaker labels + timestamps)</title>
    <updated>2026-02-17T13:47:54+00:00</updated>
    <author>
      <name>/u/timf34</name>
      <uri>https://old.reddit.com/user/timf34</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r76gi7/i_made_a_cli_that_turns_any_podcast_or_youtube/"&gt; &lt;img alt="I made a CLI that turns any podcast or YouTube video into clean Markdown transcripts (speaker labels + timestamps)" src="https://preview.redd.it/c8c0loeh72kg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1b787cd8de924691b14d8780e905e538b1895203" title="I made a CLI that turns any podcast or YouTube video into clean Markdown transcripts (speaker labels + timestamps)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Built a tiny CLI to turn podcasts or YouTube videos into clean Markdown transcripts (speakers + timestamps).&lt;/p&gt; &lt;p&gt;&lt;code&gt;pip install podscript&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Uses ElevenLabs for high-quality diarization.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/timf34/podscript"&gt;https://github.com/timf34/podscript&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Update: now supports running fully locally with faster-whisper, and optional support too for diarization&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/timf34"&gt; /u/timf34 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/c8c0loeh72kg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r76gi7/i_made_a_cli_that_turns_any_podcast_or_youtube/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r76gi7/i_made_a_cli_that_turns_any_podcast_or_youtube/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T13:47:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7gzc7</id>
    <title>Speculative decoding on Strix Halo?</title>
    <updated>2026-02-17T20:01:24+00:00</updated>
    <author>
      <name>/u/Hector_Rvkp</name>
      <uri>https://old.reddit.com/user/Hector_Rvkp</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just found out about speculative decoding (Alex Ziskind on YT). Given the low bandwidth on the strix halo but relatively big ram (128), I had in mind that only large MoE models made sense on that machine (relatively small active parameters making an MoE model usable Vs a dense model that'd just be too slow). But then there's speculative decoding to maybe double+ tgs? And it should be even more relevant with large context windows. Gemini says that MoE + speculative decoding should be faster than just MoE, but with a smaller gain. Gemini also says there's no quality degradation using speculative decoding. I'm shocked i haven't heard about that stuff until now. Are there benchmarks to figure out optimal combos on a 128gb strix halo? There's the size constraint + AMD tax to factor in (gguf, quantization limitations &amp;amp; the likes). I assume Linux. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hector_Rvkp"&gt; /u/Hector_Rvkp &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7gzc7/speculative_decoding_on_strix_halo/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7gzc7/speculative_decoding_on_strix_halo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r7gzc7/speculative_decoding_on_strix_halo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T20:01:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1r74kl8</id>
    <title>built a local semantic file search because normal file search doesn’t understand meaning</title>
    <updated>2026-02-17T12:22:39+00:00</updated>
    <author>
      <name>/u/Humble-Plastic-5285</name>
      <uri>https://old.reddit.com/user/Humble-Plastic-5285</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r74kl8/built_a_local_semantic_file_search_because_normal/"&gt; &lt;img alt="built a local semantic file search because normal file search doesn’t understand meaning" src="https://preview.redd.it/su8cizras1kg1.gif?width=640&amp;amp;crop=smart&amp;amp;s=f2d867789605b92c642056bc84c72640116d5c3b" title="built a local semantic file search because normal file search doesn’t understand meaning" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;spotlight / windows search / recall anything.&lt;/p&gt; &lt;p&gt;i kept searching for stuff like “that pdf about distributed systems i read last winter” and getting useless results, so i hacked together a small local semantic search tool in rust.&lt;/p&gt; &lt;p&gt;it crawls your files, generates embeddings locally, stores vectors and does cosine similarity search. no cloud, no api keys, no telemetry. everything stays on your machine.&lt;/p&gt; &lt;p&gt;ui is tauri. vector search is brute force for now (yeah, i know). it’s not super optimized but it works surprisingly well for personal use.&lt;/p&gt; &lt;p&gt;threw it on github in case anyone wants to mess with it or point out terrible decisions.&lt;/p&gt; &lt;p&gt;repo: &lt;a href="https://github.com/illegal-instruction-co/recall-lite"&gt;https://github.com/illegal-instruction-co/recall-lite&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Humble-Plastic-5285"&gt; /u/Humble-Plastic-5285 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/su8cizras1kg1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r74kl8/built_a_local_semantic_file_search_because_normal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r74kl8/built_a_local_semantic_file_search_because_normal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T12:22:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1r71af3</id>
    <title>[Solution Found] Qwen3-Next 80B MoE running at 39 t/s on RTX 5070 Ti + 5060 Ti (32GB VRAM)</title>
    <updated>2026-02-17T09:13:03+00:00</updated>
    <author>
      <name>/u/mazuj2</name>
      <uri>https://old.reddit.com/user/mazuj2</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r71af3/solution_found_qwen3next_80b_moe_running_at_39_ts/"&gt; &lt;img alt="[Solution Found] Qwen3-Next 80B MoE running at 39 t/s on RTX 5070 Ti + 5060 Ti (32GB VRAM)" src="https://preview.redd.it/t250hgafu0kg1.png?width=140&amp;amp;height=28&amp;amp;auto=webp&amp;amp;s=bd66f5c19f65460627ab36584f3996a261d1155c" title="[Solution Found] Qwen3-Next 80B MoE running at 39 t/s on RTX 5070 Ti + 5060 Ti (32GB VRAM)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;[Solution Found] Qwen3-Next 80B MoE running at 39 t/s on RTX 5070 Ti + 5060 Ti (32GB VRAM) - The fix nobody else figured out&lt;/p&gt; &lt;p&gt;Hey fellow 50 series brothers in pain,&lt;/p&gt; &lt;p&gt;I've been banging my head against this for a while and finally cracked it through pure trial and error. Posting this so nobody else has to suffer.&lt;/p&gt; &lt;p&gt;My Hardware:&lt;/p&gt; &lt;p&gt;RTX 5070 Ti (16GB VRAM)&lt;/p&gt; &lt;p&gt;RTX 5060 Ti (16GB VRAM)&lt;/p&gt; &lt;p&gt;32GB total VRAM&lt;/p&gt; &lt;p&gt;64GB System RAM&lt;/p&gt; &lt;p&gt;Windows 11&lt;/p&gt; &lt;p&gt;llama.cpp b8077 (CUDA 12.4 build)&lt;/p&gt; &lt;p&gt;Model: Qwen3-Next-80B-A3B-Instruct-UD-IQ2_XXS.gguf (26.2GB)&lt;/p&gt; &lt;p&gt;The Problem:&lt;/p&gt; &lt;p&gt;Out of the box, Qwen3-Next was running at 6.5 tokens/sec with:&lt;/p&gt; &lt;p&gt;CPU usage 25-55% going absolutely insane during thinking AND generation&lt;/p&gt; &lt;p&gt;GPUs sitting at 0% during thinking phase&lt;/p&gt; &lt;p&gt;5070 Ti at 5-10% during generation&lt;/p&gt; &lt;p&gt;5060 Ti at 10-40% during generation&lt;/p&gt; &lt;p&gt;~34GB of system RAM being consumed&lt;/p&gt; &lt;p&gt;Model clearly bottlenecked on CPU&lt;/p&gt; &lt;p&gt;Every suggestion I found online said the same generic things:&lt;/p&gt; &lt;p&gt;&amp;quot;Check your n_gpu_layers&amp;quot; ✅ already 999, all 49 layers on GPU&lt;/p&gt; &lt;p&gt;&amp;quot;Check your tensor split&amp;quot; ✅ tried everything&lt;/p&gt; &lt;p&gt;&amp;quot;Use CUDA 12.8+&amp;quot; ✅ not the issue&lt;/p&gt; &lt;p&gt;&amp;quot;Your offloading is broken&amp;quot; ❌ WRONG - layers were fully on GPU&lt;/p&gt; &lt;p&gt;The load output PROVED layers were on GPU:&lt;/p&gt; &lt;p&gt;load_tensors: offloaded 49/49 layers to GPU&lt;/p&gt; &lt;p&gt;load_tensors: CPU_Mapped model buffer size = 166.92 MiB (just metadata)&lt;/p&gt; &lt;p&gt;load_tensors: CUDA0 model buffer size = 12617.97 MiB&lt;/p&gt; &lt;p&gt;load_tensors: CUDA1 model buffer size = 12206.31 MiB&lt;/p&gt; &lt;p&gt;So why was CPU going nuts? Nobody had the right answer.&lt;/p&gt; &lt;p&gt;The Fix - Two flags that nobody mentioned together:&lt;/p&gt; &lt;p&gt;Step 1: Force ALL MoE experts off CPU&lt;/p&gt; &lt;p&gt;--n-cpu-moe 0&lt;/p&gt; &lt;p&gt;Start here. Systematically reduce from default down to 0. Each step helps. At 0 you still get CPU activity but it's better.&lt;/p&gt; &lt;p&gt;Step 2: THIS IS THE KEY ONE&lt;/p&gt; &lt;p&gt;Change from -sm row to:&lt;/p&gt; &lt;p&gt;-sm layer&lt;/p&gt; &lt;p&gt;Row-split (-sm row) splits each expert's weight matrix across both GPUs. This means every single expert call requires GPU-to-GPU communication over PCIe. For a model with 128 experts firing 8 per token, that's constant cross-GPU chatter killing your throughput.&lt;/p&gt; &lt;p&gt;Layer-split (-sm layer) assigns complete layers/experts to one GPU. Each GPU owns its experts fully. No cross-GPU communication during routing. The GPUs work independently and efficiently.&lt;/p&gt; &lt;p&gt;BOOM. 39 tokens/sec.&lt;/p&gt; &lt;p&gt;The Winning Command:&lt;/p&gt; &lt;p&gt;llama-server.exe -m Qwen3-Next-80B-A3B-Instruct-UD-IQ2_XXS.gguf -ngl 999 -c 4096 --port 8081 --n-cpu-moe 0 -t 6 -fa auto -sm layer&lt;/p&gt; &lt;p&gt;Results:&lt;/p&gt; &lt;p&gt;Before: 6.5 t/s, CPU melting, GPUs doing nothing&lt;/p&gt; &lt;p&gt;After: 38-39 t/s, CPUs chill, GPUs working properly&lt;/p&gt; &lt;p&gt;That's a 6x improvement with zero hardware changes&lt;/p&gt; &lt;p&gt;Why this works (the actual explanation):&lt;/p&gt; &lt;p&gt;Qwen3-Next uses a hybrid architecture — DeltaNet linear attention combined with high-sparsity MoE (128 experts, 8 active per token). When you row-split a MoE model across two GPUs, the expert weights are sliced horizontally across both cards. Every expert activation requires both GPUs to coordinate and combine results. With 8 experts firing per token across 47 layers, you're generating thousands of cross-GPU sync operations per token.&lt;/p&gt; &lt;p&gt;Layer-split instead assigns whole layers to each GPU. Experts live entirely on one card. The routing decision sends the computation to whichever GPU owns that expert. Clean, fast, no sync overhead.&lt;/p&gt; &lt;p&gt;Notes:&lt;/p&gt; &lt;p&gt;The 166MB CPU_Mapped is normal — that's just mmap metadata and tokenizer, not model weights&lt;/p&gt; &lt;p&gt;-t 6 sets CPU threads for the tiny bit of remaining CPU work&lt;/p&gt; &lt;p&gt;-fa auto enables flash attention where supported&lt;/p&gt; &lt;p&gt;This is on llama.cpp b8077 — make sure you're on a recent build that has Qwen3-Next support (merged in b7186)&lt;/p&gt; &lt;p&gt;Model fits in 32GB with ~7GB headroom for KV cache&lt;/p&gt; &lt;p&gt;Hope this saves someone's sanity. Took me way too long to find this and I couldn't find it documented anywhere.&lt;/p&gt; &lt;p&gt;If this helped you, drop a comment — curious how it performs on other 50 series configurations.&lt;/p&gt; &lt;p&gt;— RJ&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/t250hgafu0kg1.png?width=921&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=38348a8169ecc5856a6b99b33d79668daa0e087d"&gt;https://preview.redd.it/t250hgafu0kg1.png?width=921&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=38348a8169ecc5856a6b99b33d79668daa0e087d&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mazuj2"&gt; /u/mazuj2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r71af3/solution_found_qwen3next_80b_moe_running_at_39_ts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r71af3/solution_found_qwen3next_80b_moe_running_at_39_ts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r71af3/solution_found_qwen3next_80b_moe_running_at_39_ts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T09:13:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1r6w0la</id>
    <title>Where are Qwen 3.5 2B, 9B, and 35B-A3B</title>
    <updated>2026-02-17T04:12:03+00:00</updated>
    <author>
      <name>/u/Admirable_Flower_287</name>
      <uri>https://old.reddit.com/user/Admirable_Flower_287</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Where did leakers go&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Admirable_Flower_287"&gt; /u/Admirable_Flower_287 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r6w0la/where_are_qwen_35_2b_9b_and_35ba3b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r6w0la/where_are_qwen_35_2b_9b_and_35ba3b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r6w0la/where_are_qwen_35_2b_9b_and_35ba3b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T04:12:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7ij81</id>
    <title>ViT-5: Vision Transformers for The Mid-2020s</title>
    <updated>2026-02-17T20:57:59+00:00</updated>
    <author>
      <name>/u/xXWarMachineRoXx</name>
      <uri>https://old.reddit.com/user/xXWarMachineRoXx</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7ij81/vit5_vision_transformers_for_the_mid2020s/"&gt; &lt;img alt="ViT-5: Vision Transformers for The Mid-2020s" src="https://preview.redd.it/n403andob4kg1.png?width=140&amp;amp;height=140&amp;amp;crop=1:1,smart&amp;amp;auto=webp&amp;amp;s=b2bfd99eeda1f159f96a8ebde58933e175854921" title="ViT-5: Vision Transformers for The Mid-2020s" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;ViT-5: Vision Transformers for The Mid-2020s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;em&gt;Wang et al. [&lt;/em&gt;Johns Hopkins University, UC Santa Cruz&lt;em&gt;]&lt;/em&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;LLMs are sprinting ahead with rapid architectural refinements, but Vision Transformers (ViTs) have remained largely stagnant since their debut in 2020. Vision models struggle with stability issues and a limited ability to handle complex spatial reasoning.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/n403andob4kg1.png?width=629&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=edacfe88fe2840a840af5ae32d971a17a1720e4b"&gt;ViT Architecture&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The research team developed ViT-5 by systematically testing five years of AI advancements to see which ones actually improve a model's &amp;quot;eyesight.&amp;quot; They discovered that simply copying language model tricks doesn't always work; for instance, a popular method for filtering information in text models actually caused &amp;quot;over-gating&amp;quot; in vision, making the internal representations too sparse to be useful.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/s0i2hgvqb4kg1.png?width=617&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7dc824bcbc80c917bbad6bd067e90b3ad9a5e874"&gt;https://preview.redd.it/s0i2hgvqb4kg1.png?width=617&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7dc824bcbc80c917bbad6bd067e90b3ad9a5e874&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Instead, they found success by combining a more efficient normalization method with a clever dual-positioning system. This allows the model to understand where every pixel is relative to its neighbors while still maintaining a &amp;quot;big picture&amp;quot; sense of the entire image.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/pg7c4visb4kg1.png?width=1564&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=006329cff9a16a8f5458d99279e11d4126fbdc02"&gt;https://preview.redd.it/pg7c4visb4kg1.png?width=1564&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=006329cff9a16a8f5458d99279e11d4126fbdc02&lt;/a&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;To further refine performance, the researchers introduced &amp;quot;register tokens,&amp;quot; which act like digital scratchpads to clean up visual artifacts and help the model focus on what is semantically important. They also implemented a technique called QK-normalization, which smoothed out the training process and eliminated the frustrating &amp;quot;error spikes&amp;quot; that often crash large-scale AI projects.&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;The final model can handle images of varying sizes with ease and consistently outperforms previous standards in identifying objects and generating new images.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Hope you like it, Shout out to bycloud! It's from his newsletter.&lt;/p&gt; &lt;p&gt;[&lt;a href="mailto:weekly@mail.bycloud.ai"&gt;weekly@mail.bycloud.ai&lt;/a&gt;](mailto:&lt;a href="mailto:weekly@mail.bycloud.ai"&gt;weekly@mail.bycloud.ai&lt;/a&gt;)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xXWarMachineRoXx"&gt; /u/xXWarMachineRoXx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7ij81/vit5_vision_transformers_for_the_mid2020s/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7ij81/vit5_vision_transformers_for_the_mid2020s/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r7ij81/vit5_vision_transformers_for_the_mid2020s/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T20:57:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1r76d34</id>
    <title>Qwen3.5 vs GLM-4.7 vs Qwen3-235B-Thinking</title>
    <updated>2026-02-17T13:43:44+00:00</updated>
    <author>
      <name>/u/ChopSticksPlease</name>
      <uri>https://old.reddit.com/user/ChopSticksPlease</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Since the NVMe prices skyrocketed recently, and my existing drive is telling me to gtfo each time i can see chinese folk releasing a new open weight model, the question arises: &lt;/p&gt; &lt;p&gt;Qwen3.5 vs GLM-4.7 vs Qwen3-235B-Thinking, is the new one worth updating?&lt;/p&gt; &lt;p&gt;To be precise, my current setup is 128GB ram + 48GB vram, so i could run Qwen3.5 IQ3_XXS while Qwen3-235B runs at Q4_K_XL. I can also run GLM-4.7 at Q3_K_XL.&lt;/p&gt; &lt;p&gt;I found Qwen3-235b-thinking quite capable in writing documents for my work so I'm reluctant trashing it just like that.&lt;/p&gt; &lt;p&gt;Has anyone compared these models? Is the newest the best?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ChopSticksPlease"&gt; /u/ChopSticksPlease &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r76d34/qwen35_vs_glm47_vs_qwen3235bthinking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r76d34/qwen35_vs_glm47_vs_qwen3235bthinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r76d34/qwen35_vs_glm47_vs_qwen3235bthinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T13:43:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7iwmb</id>
    <title>Arc B60 24gb or RTX 5060ti 16gb?</title>
    <updated>2026-02-17T21:11:22+00:00</updated>
    <author>
      <name>/u/Proof_Nothing_7711</name>
      <uri>https://old.reddit.com/user/Proof_Nothing_7711</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everybody,&lt;/p&gt; &lt;p&gt;I would like to add an eGPU to my Ryzen 9 AI HX370 64gb ram. I can use usb-c 40gbps or Oculink.&lt;/p&gt; &lt;p&gt;Owners or experts can you give me some advices on these 2 gpu ?&lt;/p&gt; &lt;p&gt;If token/s are similar obviously I choose 24gb ram for bigger model BUT ….&lt;/p&gt; &lt;p&gt;What about difficulty to tune Intel ARC to gain its maximum performances ?&lt;/p&gt; &lt;p&gt;I will use it on Win 11. ATM I use LM Studio.&lt;/p&gt; &lt;p&gt;Ps: could be interesting also consider RX 7900 XTX 24gb or RX 9000 series?&lt;/p&gt; &lt;p&gt;Thanks !&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Proof_Nothing_7711"&gt; /u/Proof_Nothing_7711 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7iwmb/arc_b60_24gb_or_rtx_5060ti_16gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7iwmb/arc_b60_24gb_or_rtx_5060ti_16gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r7iwmb/arc_b60_24gb_or_rtx_5060ti_16gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T21:11:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1r787nn</id>
    <title>Zero Shot Transferable Adapter</title>
    <updated>2026-02-17T14:58:11+00:00</updated>
    <author>
      <name>/u/ShotokanOSS</name>
      <uri>https://old.reddit.com/user/ShotokanOSS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r787nn/zero_shot_transferable_adapter/"&gt; &lt;img alt="Zero Shot Transferable Adapter" src="https://preview.redd.it/4riq1hxaj2kg1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=83434c22c14e3b29fb42a0c547dbfe74ebc1e5be" title="Zero Shot Transferable Adapter" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We just did it! With our new methode we can train adapter on small models and then transfer them to huger ones without more fine tunning! In the table you see Zero shot transfer ability. &lt;/p&gt; &lt;p&gt;Its really simple we just train small adapters which improve the soft targets of the model itself instead of doing it in the weights like normal. &lt;/p&gt; &lt;p&gt;That makes the fine tunning process a way cheaper and gives the possibilty to transfer from small to huge models as long as the tokenizer stays the same.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ShotokanOSS"&gt; /u/ShotokanOSS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4riq1hxaj2kg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r787nn/zero_shot_transferable_adapter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r787nn/zero_shot_transferable_adapter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T14:58:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1r79f0y</id>
    <title>Some of you apparently</title>
    <updated>2026-02-17T15:43:16+00:00</updated>
    <author>
      <name>/u/__Maximum__</name>
      <uri>https://old.reddit.com/user/__Maximum__</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r79f0y/some_of_you_apparently/"&gt; &lt;img alt="Some of you apparently" src="https://preview.redd.it/ldkfxos5s2kg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9a0fad14885a3f2833b97aa399d3bbd0e1d80406" title="Some of you apparently" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/__Maximum__"&gt; /u/__Maximum__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ldkfxos5s2kg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r79f0y/some_of_you_apparently/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r79f0y/some_of_you_apparently/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T15:43:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1r71lu7</id>
    <title>Qwen 3.5, replacement to Llama 4 Scout?</title>
    <updated>2026-02-17T09:33:24+00:00</updated>
    <author>
      <name>/u/redjojovic</name>
      <uri>https://old.reddit.com/user/redjojovic</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r71lu7/qwen_35_replacement_to_llama_4_scout/"&gt; &lt;img alt="Qwen 3.5, replacement to Llama 4 Scout?" src="https://preview.redd.it/pjuceb62y0kg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0c7e44de2c66f97a35831fb81ad45a2c9aaa9afa" title="Qwen 3.5, replacement to Llama 4 Scout?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is Qwen 3.5 a direct replacement to Llama 4 in your opinion? Seems too much of a coincidence&lt;/p&gt; &lt;p&gt;Edit: 3.5 Plus and not Max&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/redjojovic"&gt; /u/redjojovic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/pjuceb62y0kg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r71lu7/qwen_35_replacement_to_llama_4_scout/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r71lu7/qwen_35_replacement_to_llama_4_scout/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T09:33:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1r70ohs</id>
    <title>Tiny Aya</title>
    <updated>2026-02-17T08:33:58+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Model Summary&lt;/h1&gt; &lt;p&gt;Cohere Labs Tiny Aya is an open weights research release of a pretrained 3.35 billion parameter model optimized for efficient, strong, and balanced multilingual representation across 70+ languages, including many lower-resourced ones. The model is designed to support downstream adaptation, instruction tuning, and local deployment under realistic compute constraints.&lt;/p&gt; &lt;p&gt;Developed by: &lt;a href="https://cohere.com/"&gt;Cohere&lt;/a&gt; and &lt;a href="https://cohere.com/research"&gt;Cohere&lt;/a&gt; Labs&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Point of Contact: &lt;a href="https://cohere.com/research"&gt;&lt;strong&gt;Cohere Labs&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;License: &lt;a href="https://cohere.com/cohere-labs-cc-by-nc-license"&gt;CC-BY-NC&lt;/a&gt;, requires also adhering to &lt;a href="https://docs.cohere.com/docs/c4ai-acceptable-use-policy"&gt;&lt;strong&gt;Cohere Lab's Acceptable Use Policy&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Model: tiny-aya-it-global&lt;/li&gt; &lt;li&gt;Model Size: 3.35B&lt;/li&gt; &lt;li&gt;Context length: 8K input&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For more details about this model family, please check out our &lt;a href="https://cohere.com/blog/cohere-labs-tiny-aya"&gt;blog post&lt;/a&gt; and &lt;a href="https://github.com/Cohere-Labs/tiny-aya-tech-report/blob/main/tiny_aya_tech_report.pdf"&gt;tech report&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;looks like different models are for different families of languages:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/CohereLabs/tiny-aya-earth-GGUF"&gt;https://huggingface.co/CohereLabs/tiny-aya-earth-GGUF&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/CohereLabs/tiny-aya-fire-GGUF"&gt;https://huggingface.co/CohereLabs/tiny-aya-fire-GGUF&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/CohereLabs/tiny-aya-water-GGUF"&gt;https://huggingface.co/CohereLabs/tiny-aya-water-GGUF&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/CohereLabs/tiny-aya-global-GGUF"&gt;https://huggingface.co/CohereLabs/tiny-aya-global-GGUF&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Usage and Limitations&lt;/h1&gt; &lt;h1&gt;Intended Usage&lt;/h1&gt; &lt;p&gt;Tiny Aya is a family of massively multilingual small language models built to bring capable AI to languages that are often underserved by existing models. The models support languages across Indic, East and Southeast Asian, African, European, and Middle Eastern language families, with a deliberate emphasis on low-resource language performance.&lt;/p&gt; &lt;p&gt;Intended applications include multilingual text generation, conversational AI, summarization, translation and cross-lingual tasks, as well as research in multilingual NLP and low-resource language modeling. The models are also suited for efficient deployment in multilingual regions, helping bridge the digital language divide for underrepresented language communities.&lt;/p&gt; &lt;h1&gt;Strengths&lt;/h1&gt; &lt;p&gt;Tiny Aya demonstrates strong open-ended generation quality across its full language coverage, with particularly notable performance on low-resource languages. The model performs well on translation, summarization, and cross-lingual tasks, benefiting from training signal shared across language families and scripts.&lt;/p&gt; &lt;h1&gt;Limitations&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Reasoning tasks.&lt;/strong&gt; The model's strongest performance is on open-ended generation and conversational tasks. Chain-of-thought reasoning tasks such as multilingual math (MGSM) are comparatively weaker.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Factual knowledge.&lt;/strong&gt; As with any language model, outputs may contain incorrect or outdated statements, particularly in lower-resource languages with thinner training data coverage.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Uneven resource distribution.&lt;/strong&gt; High-resource languages benefit from richer training signal and tend to exhibit more consistent quality across tasks. The lowest-resource languages in the model's coverage may show greater variability, and culturally specific nuance, sarcasm, or figurative language may be less reliably handled in these languages.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Task complexity.&lt;/strong&gt; The model performs best with clear prompts and instructions. Highly complex or open-ended reasoning, particularly in lower-resource languages, remains challenging.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r70ohs/tiny_aya/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r70ohs/tiny_aya/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r70ohs/tiny_aya/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T08:33:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1r77fz7</id>
    <title>Qwen3.5 NVFP4 (Blackwell) is up!</title>
    <updated>2026-02-17T14:27:43+00:00</updated>
    <author>
      <name>/u/TeekayTK</name>
      <uri>https://old.reddit.com/user/TeekayTK</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Quantized with NVIDIA's Model Optimizer to FP4. Checkpoint is ~224GB total, 17B active parameters. Apache 2.0 license.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;HF:&lt;/strong&gt; &lt;a href="https://huggingface.co/vincentzed-hf/Qwen3.5-397B-A17B-NVFP4"&gt;vincentzed-hf/Qwen3.5-397B-A17B-NVFP4&lt;/a&gt;&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;&lt;strong&gt;Install&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;You need SGLang from a specific branch that fixes visual encoder weight handling during quantized inference: (Basically, it was trying to quantize the vision weights, we didn't do that).&lt;/p&gt; &lt;p&gt;&lt;code&gt; git clone -b vz/qwen3-5 git@github.com:bzhng-development/sglang.git cd sglang uv pip install -e &amp;quot;python&amp;quot; uv pip install transformers==5.2.0 &lt;/code&gt;&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;&lt;strong&gt;Launch (B200/B300, TP=4)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt; python3 -m sglang.launch_server \ --model-path vincentzed-hf/Qwen3.5-397B-A17B-NVFP4 \ --quantization modelopt_fp4 \ --tp 4 \ --context-length 262144 \ --reasoning-parser qwen3 &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Set &lt;code&gt;--tp 8&lt;/code&gt; for RTX PRO 6000s or if you're running into OOM.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;&lt;strong&gt;Speculative Decoding (Experimental)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Qwen3.5 has a built-in Multi-Token Prediction head. Worth trying if you have few concurrent users:&lt;/p&gt; &lt;p&gt;&lt;code&gt; SGLANG_ENABLE_SPEC_V2=1 python3 -m sglang.launch_server \ --model-path vincentzed-hf/Qwen3.5-397B-A17B-NVFP4 \ --quantization modelopt_fp4 \ --tp 8 \ --context-length 262144 \ --reasoning-parser qwen3 \ --speculative-algo NEXTN \ --speculative-num-steps 3 \ --speculative-eagle-topk 1 \ --speculative-num-draft-tokens 4 &lt;/code&gt;&lt;/p&gt; &lt;p&gt;If you run into issues (i.e server crashes), you also also remove &lt;code&gt;SGLANG_ENABLE_SPEC_V2=1&lt;/code&gt; but it can boost up to 10% performance by overlapping some CUDA operations, so it's generally helpful.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;&lt;strong&gt;Hardware Requirements&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Config&lt;/th&gt; &lt;th&gt;GPUs&lt;/th&gt; &lt;th&gt;VRAM/GPU&lt;/th&gt; &lt;th&gt;Throughput&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;B300 TP=4&lt;/td&gt; &lt;td&gt;4x B300&lt;/td&gt; &lt;td&gt;288 GB&lt;/td&gt; &lt;td&gt;~120 tok/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;B200 TP=4&lt;/td&gt; &lt;td&gt;4x B200&lt;/td&gt; &lt;td&gt;192 GB&lt;/td&gt; &lt;td&gt;—&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX PRO 6000 TP=8&lt;/td&gt; &lt;td&gt;8x RTX PRO 6000&lt;/td&gt; &lt;td&gt;96 GB&lt;/td&gt; &lt;td&gt;—&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Default context is 262K tokens. If you hit OOM, reduce it — but try to keep at least 128K to preserve thinking quality. We are working on the 1M context support.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;&lt;strong&gt;Key specs:&lt;/strong&gt; 397B total params, 17B active (MoE with 512 experts, 10 active per token), 262K native context (extensible to 1M+), multimodal (text + image + video), supports 201 languages, built-in thinking mode, all the good stuff from Qwen3.5 (Nothing changed, ~99% accuracy)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TeekayTK"&gt; /u/TeekayTK &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r77fz7/qwen35_nvfp4_blackwell_is_up/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r77fz7/qwen35_nvfp4_blackwell_is_up/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r77fz7/qwen35_nvfp4_blackwell_is_up/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T14:27:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7i2im</id>
    <title>GLM-5 and DeepSeek are in the Top 6 of the Game Agent Coding League across five games</title>
    <updated>2026-02-17T20:40:43+00:00</updated>
    <author>
      <name>/u/kyazoglu</name>
      <uri>https://old.reddit.com/user/kyazoglu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7i2im/glm5_and_deepseek_are_in_the_top_6_of_the_game/"&gt; &lt;img alt="GLM-5 and DeepSeek are in the Top 6 of the Game Agent Coding League across five games" src="https://preview.redd.it/22z0y8ni84kg1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8c75efaafe037d102f0274765c4008b961af0948" title="GLM-5 and DeepSeek are in the Top 6 of the Game Agent Coding League across five games" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi.&lt;/p&gt; &lt;p&gt;Game Agent Coding League (GACL) is a benchmarking framework designed for LLMs in which models are tasked with generating code for game-playing agents. These agents compete in games such as Battleship, Tic-Tac-Toe variants, and others. At present, the league supports five games, with additional titles planned.&lt;/p&gt; &lt;p&gt;More info about the benchmark &amp;amp; league &lt;a href="https://gameagentcodingleague.com/"&gt;HERE&lt;/a&gt;&lt;br /&gt; Underlying project in Github &lt;a href="https://github.com/summersonnn/Game-Agent-Coding-Benchmark"&gt;HERE&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It's quite new project so bit of a mess in repo. I'll fix soon and 3 more games.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kyazoglu"&gt; /u/kyazoglu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/22z0y8ni84kg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7i2im/glm5_and_deepseek_are_in_the_top_6_of_the_game/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r7i2im/glm5_and_deepseek_are_in_the_top_6_of_the_game/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T20:40:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7bhel</id>
    <title>Team created a methodology to mathematically change the weights on local LLMs to remove the censorship guardrails. HERETIC</title>
    <updated>2026-02-17T16:51:30+00:00</updated>
    <author>
      <name>/u/44th--Hokage</name>
      <uri>https://old.reddit.com/user/44th--Hokage</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is the tool and their summary:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/p-e-w/heretic"&gt;https://github.com/p-e-w/heretic&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Heretic is a tool that removes censorship (aka &amp;quot;safety alignment&amp;quot;) from transformer-based language models without expensive post-training. It combines an advanced implementation of directional ablation, also known as &amp;quot;abliteration&amp;quot; (&lt;a href="https://arxiv.org/abs/2406.11717"&gt;Arditi et al. 2024&lt;/a&gt;, Lai 2025 (&lt;a href="https://huggingface.co/blog/grimjim/projected-abliteration"&gt;1&lt;/a&gt;, &lt;a href="https://huggingface.co/blog/grimjim/norm-preserving-biprojected-abliteration"&gt;2&lt;/a&gt;)), with a TPE-based parameter optimizer powered by &lt;a href="https://optuna.org/"&gt;Optuna&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;This approach enables Heretic to work &lt;strong&gt;completely automatically.&lt;/strong&gt; Heretic finds high-quality abliteration parameters by co-minimizing the number of refusals and the KL divergence from the original model. This results in a decensored model that retains as much of the original model's intelligence as possible. Using Heretic does not require an understanding of transformer internals. In fact, anyone who knows how to run a command-line program can use Heretic to decensor language models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/44th--Hokage"&gt; /u/44th--Hokage &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bhel/team_created_a_methodology_to_mathematically/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bhel/team_created_a_methodology_to_mathematically/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bhel/team_created_a_methodology_to_mathematically/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T16:51:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1r79dcd</id>
    <title>Qwen 3.5 397B is Strong one!</title>
    <updated>2026-02-17T15:41:44+00:00</updated>
    <author>
      <name>/u/Single_Ring4886</name>
      <uri>https://old.reddit.com/user/Single_Ring4886</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I rarely post here but after poking at latest Qwen I felt like sharing my &amp;quot;vibes&amp;quot;. I did bunch of my little tests (thinking under several constraints) and it performed really well.&lt;br /&gt; But what is really good is fact that it is capable of good outputs even without thinking!&lt;br /&gt; Some latest models depend on thinking part really much and that makes them ie 2x more expensive.&lt;br /&gt; It also seems this model is capable of cheap inference +- 1$ .&lt;br /&gt; Do you agree?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Single_Ring4886"&gt; /u/Single_Ring4886 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r79dcd/qwen_35_397b_is_strong_one/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r79dcd/qwen_35_397b_is_strong_one/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r79dcd/qwen_35_397b_is_strong_one/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T15:41:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7bf1l</id>
    <title>Alibaba's new Qwen3.5-397B-A17B is the #3 open weights model in the Artificial Analysis Intelligence Index</title>
    <updated>2026-02-17T16:49:25+00:00</updated>
    <author>
      <name>/u/abdouhlili</name>
      <uri>https://old.reddit.com/user/abdouhlili</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bf1l/alibabas_new_qwen35397ba17b_is_the_3_open_weights/"&gt; &lt;img alt="Alibaba's new Qwen3.5-397B-A17B is the #3 open weights model in the Artificial Analysis Intelligence Index" src="https://preview.redd.it/b5eytfmy33kg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a91c254a446d622c39a0be55a5a8d80f79c11886" title="Alibaba's new Qwen3.5-397B-A17B is the #3 open weights model in the Artificial Analysis Intelligence Index" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdouhlili"&gt; /u/abdouhlili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/b5eytfmy33kg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bf1l/alibabas_new_qwen35397ba17b_is_the_3_open_weights/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bf1l/alibabas_new_qwen35397ba17b_is_the_3_open_weights/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T16:49:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7j7kb</id>
    <title>The guy that won the NVIDIA Hackathon and an NVIDIA DGX Spark GB10 has won another hackathon with it!</title>
    <updated>2026-02-17T21:22:30+00:00</updated>
    <author>
      <name>/u/brandon-i</name>
      <uri>https://old.reddit.com/user/brandon-i</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I promised that I would update you all with what I was going to do next with the DGX Spark GB10 that I won. It's been a few weeks and I have been primarily heads down on fundraising for my startup trying to automatically improve and evaluate Coding Agents.&lt;/p&gt; &lt;p&gt;Since the last time I posted I became a Dell Pro Precision Ambassador after they saw all of the cool hackathons that I won and stuff I am building that can hopefully make a difference in the world (I am trying to create Brain World Models using a bunch of different types of brain scans to do precision therapeutics, diagnostics, etc. as my Magnus Opus). &lt;/p&gt; &lt;p&gt;They sent me a Dell Pro Max T2 Tower and another DGX Spark GB10 which I have connected to the previous one that I won. This allows me to continue my work with the limited funds that I have to see how far I can really push the limits of what's possible at the intersection of Healthcare and AI.&lt;/p&gt; &lt;p&gt;During Superbowl Weekend I took some time to do a 24-hour hackathon solving a problem that I really care about (even if it wasn't related to my startup).&lt;/p&gt; &lt;p&gt;My most recent job was at UCSF doing applied neuroscience creating a research-backed tool that screened children for Dyslexia since traditional approaches don’t meet learners where they are so I wanted to take the research I did further and actually create solutions that also did computer adaptive learning.&lt;/p&gt; &lt;p&gt;Through my research I have come to find that the current solutions for learning languages are antiquated often assuming a “standard” learner: same pace, same sequence, same practice, same assessments.&lt;/p&gt; &lt;p&gt;But, language learning is deeply personalized. Two learners can spend the same amount of time on the same content and walk away with totally different outcomes because the feedback they need could be entirely different with the core problem being that language learning isn’t one-size-fits-all.&lt;/p&gt; &lt;p&gt;Most language tools struggle with a few big issues:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Single Language&lt;/strong&gt;: Most tools are designed specifically for Native English speakers&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Culturally insensitive:&lt;/strong&gt; Even within the same language there can be different dialects and word/phrase utilization&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Static Difficulty:&lt;/strong&gt; content doesn’t adapt when you’re bored or overwhelmed&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Delayed Feedback:&lt;/strong&gt; you don’t always know &lt;em&gt;what&lt;/em&gt; you said wrong or &lt;em&gt;why&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Practice ≠ assessment:&lt;/strong&gt; testing is often separate from learning, instead of driving it&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Speaking is underserved&lt;/strong&gt;: it’s hard to get consistent, personalized speaking practice without 1:1 time&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For many learners, especially kids, the result is predictable: &lt;em&gt;frustration, disengagement, or plateauing.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;So I built a an automated speech recognition app that adapts in real time combining computer adaptive testing and computer adaptive learning to personalize the experience as you go.&lt;/p&gt; &lt;p&gt;It not only transcribes speech, but also evaluates phoneme-level pronunciation, which lets the system give targeted feedback (and adapt the next prompt) based on &lt;em&gt;which sounds&lt;/em&gt; someone struggles with.&lt;/p&gt; &lt;p&gt;I tried to make it as simple as possible because my primary user base would be teachers that didn't have a lot of time to actually learn new tools and were already struggling with teaching an entire class.&lt;/p&gt; &lt;p&gt;It uses natural speaking performance to determine what a student should practice next.&lt;/p&gt; &lt;p&gt;So instead of providing every child a fixed curriculum, the system continuously adjusts difficulty and targets based on how you’re actually doing rather than just on completion.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How it Built It&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;I connected two NVIDIA DGX Spark with the GB10 Grace Blackwell Superchip giving me 256 GB LPDDR5x Coherent Unified System Memory to run inference and the entire workflow locally. I also had the Dell Pro Max T2 Tower, but I couldn't physically bring it to the Notion office so I used Tailscale to SSH into it&lt;/li&gt; &lt;li&gt;I utilized CrisperWhisper, faster-whisper, and a custom transformer to get accurate word-level timestamps, verbatim transcriptions, filler detection, and hallucination mitigation&lt;/li&gt; &lt;li&gt;I fed this directly into a Montreal Forced Aligner to get phoneme level dictation&lt;/li&gt; &lt;li&gt;I then used a heuristics detection algorithm to screen for several disfluencies: Prolongnation, replacement, deletion, addition, and repetition&lt;/li&gt; &lt;li&gt;I included stutter and filler analysis/detection using the SEP-28k dataset and PodcastFillers Dataset&lt;/li&gt; &lt;li&gt;I fed these into AI Agents using both local models, Cartesia's Line Agents, and Notion's Custom Agents to do computer adaptive learning and testing&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The result is a workflow where learning content can evolve quickly while the learner experience stays personalized and measurable.&lt;/p&gt; &lt;p&gt;I want to support learners who don’t thrive in rigid systems and need:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;more repetition (without embarrassment)&lt;/li&gt; &lt;li&gt;targeted practice on specific sounds/phrases&lt;/li&gt; &lt;li&gt;a pace that adapts to attention and confidence&lt;/li&gt; &lt;li&gt;immediate feedback that’s actually actionable&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This project is an early prototype, but it’s a direction I’m genuinely excited about: speech-first language learning that adapts to the person, rather than the other way around.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=2RYHu1jyFWI"&gt;https://www.youtube.com/watch?v=2RYHu1jyFWI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I wrote something in medium that has a tiny bit more information &lt;a href="https://medium.com/@brandonin/i-just-won-the-cartesia-hackathon-reinforcing-something-ive-believed-in-for-a-long-time-language-dc93525b2e48?postPublishedType=repub"&gt;https://medium.com/@brandonin/i-just-won-the-cartesia-hackathon-reinforcing-something-ive-believed-in-for-a-long-time-language-dc93525b2e48?postPublishedType=repub&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For those that are wondering what the specs are of the Dell Pro T2 Tower that they sent me:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Intel Core Ultra 9 285K (36 MB cache, 24 cores, 24 threads, 3.2 GHz to 5.7 GHz, 125W)&lt;/li&gt; &lt;li&gt;128GB: 4 x 32 GB, DDR5, 4400 MT/s&lt;/li&gt; &lt;li&gt;2x - 4TB SSD TLC with DRAM M.2 2280 PCIe Gen4 SED Ready&lt;/li&gt; &lt;li&gt;NVIDIA RTX PRO 6000 Blackwell Workstation Edition (600W), 96GB GDDR7&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/brandon-i"&gt; /u/brandon-i &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7j7kb/the_guy_that_won_the_nvidia_hackathon_and_an/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7j7kb/the_guy_that_won_the_nvidia_hackathon_and_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r7j7kb/the_guy_that_won_the_nvidia_hackathon_and_an/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T21:22:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7fb2k</id>
    <title>Anthropic is deploying 20M$ to support AI regulation in sight of 2026 elections</title>
    <updated>2026-02-17T19:02:15+00:00</updated>
    <author>
      <name>/u/1998marcom</name>
      <uri>https://old.reddit.com/user/1998marcom</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7fb2k/anthropic_is_deploying_20m_to_support_ai/"&gt; &lt;img alt="Anthropic is deploying 20M$ to support AI regulation in sight of 2026 elections" src="https://external-preview.redd.it/YL-V_hu9Gif4FU34F4m4K7lk-m3_3LBagiDGYFEEe4o.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=120a000c29bab7e9ed6a3da128fb13fb873db506" title="Anthropic is deploying 20M$ to support AI regulation in sight of 2026 elections" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Next time you buy subscriptions from Anthropic or pay for their models, keep in mind where some of your money is going.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1998marcom"&gt; /u/1998marcom &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.cnbc.com/2026/02/12/anthropic-gives-20-million-to-group-pushing-for-ai-regulations-.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7fb2k/anthropic_is_deploying_20m_to_support_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r7fb2k/anthropic_is_deploying_20m_to_support_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T19:02:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1r77swh</id>
    <title>I gave 12 LLMs $2,000 and a food truck. Only 4 survived.</title>
    <updated>2026-02-17T14:42:06+00:00</updated>
    <author>
      <name>/u/Disastrous_Theme5906</name>
      <uri>https://old.reddit.com/user/Disastrous_Theme5906</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r77swh/i_gave_12_llms_2000_and_a_food_truck_only_4/"&gt; &lt;img alt="I gave 12 LLMs $2,000 and a food truck. Only 4 survived." src="https://preview.redd.it/4sewtkexf2kg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c0f7608e083eece043f2953690650ad7c16596a5" title="I gave 12 LLMs $2,000 and a food truck. Only 4 survived." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Built a business sim where AI agents run a food truck for 30 days — location, menu, pricing, staff, inventory. Same scenario for all models.&lt;/p&gt; &lt;p&gt;Opus made $49K. GPT-5.2 $28K. 8 went bankrupt. Every model that took a loan went bankrupt (8/8).&lt;/p&gt; &lt;p&gt;There's also a playable mode — same simulation, same 34 tools, same leaderboard. You either survive 30 days or go bankrupt, get a result card and land on the shared leaderboard. Example result: &lt;a href="https://foodtruckbench.com/r/9E6925"&gt;https://foodtruckbench.com/r/9E6925&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Benchmark + leaderboard: &lt;a href="https://foodtruckbench.com"&gt;https://foodtruckbench.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Play: &lt;a href="https://foodtruckbench.com/play"&gt;https://foodtruckbench.com/play&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Gemini 3 Flash Thinking — only model out of 20+ tested that gets stuck in an infinite decision loop, 100% of runs: &lt;a href="https://foodtruckbench.com/blog/gemini-flash"&gt;https://foodtruckbench.com/blog/gemini-flash&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer questions about the sim or results.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Disastrous_Theme5906"&gt; /u/Disastrous_Theme5906 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4sewtkexf2kg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r77swh/i_gave_12_llms_2000_and_a_food_truck_only_4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r77swh/i_gave_12_llms_2000_and_a_food_truck_only_4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T14:42:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7c7zg</id>
    <title>Car Wash Test on 53 leading models: “I want to wash my car. The car wash is 50 meters away. Should I walk or drive?”</title>
    <updated>2026-02-17T17:16:18+00:00</updated>
    <author>
      <name>/u/facethef</name>
      <uri>https://old.reddit.com/user/facethef</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7c7zg/car_wash_test_on_53_leading_models_i_want_to_wash/"&gt; &lt;img alt="Car Wash Test on 53 leading models: “I want to wash my car. The car wash is 50 meters away. Should I walk or drive?”" src="https://preview.redd.it/je57sbwb43kg1.png?width=140&amp;amp;height=121&amp;amp;auto=webp&amp;amp;s=e7ce1b3de0d48928a1de60cdf23dd848d2141ef5" title="Car Wash Test on 53 leading models: “I want to wash my car. The car wash is 50 meters away. Should I walk or drive?”" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I asked 53 leading AI models the question: &lt;strong&gt;&amp;quot;I want to wash my car. The car wash is 50 meters away. Should I walk or drive?&amp;quot;&lt;/strong&gt; Obviously, you need to drive because the car needs to be at the car wash. &lt;/p&gt; &lt;p&gt;The funniest part: Perplexity's sonar and sonar-pro got the right answer for completely insane reasons. &lt;/p&gt; &lt;p&gt;They cited EPA studies and argued that walking burns calories which requires food production energy, making walking more polluting than driving 50 meters. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;In this setup, the open-weight models tested got it wrong:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Llama 3.1 8B: walk ❌&lt;/p&gt; &lt;p&gt;Llama 3.3 70B: walk ❌&lt;/p&gt; &lt;p&gt;Llama 4 Scout 17B: walk ❌&lt;/p&gt; &lt;p&gt;Llama 4 Maverick 17B: walk ❌&lt;/p&gt; &lt;p&gt;Mistral Small / Medium / Large: walk ❌ ❌ ❌&lt;/p&gt; &lt;p&gt;DeepSeek v3.1 / v3.2: walk ❌ ❌&lt;/p&gt; &lt;p&gt;GLM-4.7 / GLM-4.7 Flash: walk ❌ ❌&lt;/p&gt; &lt;p&gt;Kimi K2 Instruct: walk ❌&lt;/p&gt; &lt;p&gt;Kimi K2 Thinking / Thinking Turbo: walk ❌ ❌&lt;/p&gt; &lt;p&gt;MiniMax M2.1: walk ❌&lt;/p&gt; &lt;p&gt;GPT-OSS 20B / 120B: walk ❌ ❌ &lt;/p&gt; &lt;p&gt;Only GLM-5 and Kimi K2.5 (closed) both got it right. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Full scorecard (11/53 correct):&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Anthropic: 1/9 — only Opus 4.6 got it&lt;/p&gt; &lt;p&gt;OpenAI: 1/12 — only GPT-5 got it&lt;/p&gt; &lt;p&gt;Google: 3/8 — Gemini 3 models nailed it, all 2.x failed&lt;/p&gt; &lt;p&gt;xAI: 2/4 — Grok-4 yes, non-reasoning variant no&lt;/p&gt; &lt;p&gt;Perplexity: 2/3 — right answer, wrong reasons&lt;/p&gt; &lt;p&gt;Meta (Llama): 0/4&lt;/p&gt; &lt;p&gt;Mistral: 0/3&lt;/p&gt; &lt;p&gt;DeepSeek: 0/2&lt;/p&gt; &lt;p&gt;Moonshot (Kimi): 1/4&lt;/p&gt; &lt;p&gt;Zhipu (GLM): 1/3&lt;/p&gt; &lt;p&gt;MiniMax: 0/1 &lt;/p&gt; &lt;p&gt;Tested all 53 models via &lt;a href="https://opper.ai"&gt;Opper&lt;/a&gt; with the same prompt, no system prompt tricks, forced choice with reasoning.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/facethef"&gt; /u/facethef &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1r7c7zg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7c7zg/car_wash_test_on_53_leading_models_i_want_to_wash/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r7c7zg/car_wash_test_on_53_leading_models_i_want_to_wash/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T17:16:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7bsfd</id>
    <title>Best Audio Models - Feb 2026</title>
    <updated>2026-02-17T17:01:37+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;They've been a ton of audio models released of late, the most notable perhaps being Qwen3 TTS. So its time for another &lt;strong&gt;Best Audio Models&lt;/strong&gt; megathread&lt;/p&gt; &lt;p&gt;Share what your favorite ASR, TTS, STT, Text to Music models are right now &lt;strong&gt;and why.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Given the the amount of ambiguity and subjectivity in rating/testing these models, please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks etc. Closed models like Elevenlabs v3 seem to continue to be a few levels above open models especially for production use cases with long lengths/stability requirements, so comparisons, especially empirical ones are welcome.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please use the top level comments to thread your responses.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T17:01:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1r60qu9</id>
    <title>AMA Announcement: StepFun AI, The Opensource Lab Behind Step-3.5-Flash Model (Thursday, 8AM-11AM PST)</title>
    <updated>2026-02-16T05:11:16+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r60qu9/ama_announcement_stepfun_ai_the_opensource_lab/"&gt; &lt;img alt="AMA Announcement: StepFun AI, The Opensource Lab Behind Step-3.5-Flash Model (Thursday, 8AM-11AM PST)" src="https://preview.redd.it/u11uh8jfisjg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=afc30b2b6ae673f2e940109e2001bb498bd818ad" title="AMA Announcement: StepFun AI, The Opensource Lab Behind Step-3.5-Flash Model (Thursday, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; 👋&lt;/p&gt; &lt;p&gt;We're excited for Thursday's guests: &lt;strong&gt;The StepFun Team!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Kicking things off Thursday, Feb. 19th, 8 AM–11 AM PST&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;⚠️ &lt;strong&gt;Note:&lt;/strong&gt; The AMA itself will be hosted in a &lt;strong&gt;separate thread,&lt;/strong&gt; please don’t post questions here.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/u11uh8jfisjg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r60qu9/ama_announcement_stepfun_ai_the_opensource_lab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r60qu9/ama_announcement_stepfun_ai_the_opensource_lab/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T05:11:16+00:00</published>
  </entry>
</feed>
