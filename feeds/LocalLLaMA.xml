<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-28T00:27:36+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ohtard</id>
    <title>Parallels Virtualization for Local AI on Macbook w/eGPU connected (TB4/5)</title>
    <updated>2025-10-27T22:56:33+00:00</updated>
    <author>
      <name>/u/Super_Revolution3966</name>
      <uri>https://old.reddit.com/user/Super_Revolution3966</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone tried using a Mac to run Windows through Parallels and then used that Windows instance to run local LLMs while connected via Thunderbolt 4/5 to use an eGPU or your main PC to boost performance? Is that possible?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Super_Revolution3966"&gt; /u/Super_Revolution3966 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohtard/parallels_virtualization_for_local_ai_on_macbook/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohtard/parallels_virtualization_for_local_ai_on_macbook/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohtard/parallels_virtualization_for_local_ai_on_macbook/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T22:56:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1oh6k6u</id>
    <title>Some usage notes on low-end CPU LLMs and home applications (/r/frugal meets /r/localLlama)</title>
    <updated>2025-10-27T05:44:43+00:00</updated>
    <author>
      <name>/u/___positive___</name>
      <uri>https://old.reddit.com/user/___positive___</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So a few weeks ago I discovered that Qwen3-4b is actually usable on any old laptop with CPU-only inference. Since then, I've been working on getting a simple home smart station set up using small LLMs. These are some notes on the LLMs and their usage that will hopefully be useful for anyone else thinking of doing similar hobby projects with dirt cheap components.&lt;/p&gt; &lt;p&gt;I scored a used Thinkpad for $200 with a Ryzen 4650U and 32GB DDR4 3200, perfect cosmetic condition. The key here is the 32GB RAM. I installed Ubuntu 24.04. I'm not a big Linux guy but it was painless and everything worked perfectly on the first try. The idea is to have a small self-contained system with a built-in monitor and keyboard to act like a smart whiteboard + Alexa.&lt;/p&gt; &lt;p&gt;Here are some inference numbers , pardon the plain formatting, all run with llama.cpp built for CPU only, all q4, using short test prompts:&lt;/p&gt; &lt;p&gt;Qwen3-4B-Instruct-2507 (q4): 29 tok/sec (PP), 11 tok/sec (TG), 1 sec (model load time). Running in Balanced Mode versus Performance Mode power settings had negligible difference.&lt;/p&gt; &lt;p&gt;Qwen3-30B-A3B-Instruct-2507 (q4): 38 tok/sec (PP), 15 tok/sec (TG), 26 sec (model load time) for Balanced Mode. 44 tok/sec (PP), 15 tok/sec (TG), 17 sec (model load time) for Performance Mode.&lt;/p&gt; &lt;p&gt;Mistral-Small-3.2-24B-Instruct-2506 (q4): 5 tok/sec (PP), 2 tok/sec (TG), 12 sec (model load time) for Balanced mode. 5 tok/sec (PP), 2 tok/sec (TG), 4 sec (model load time) for Performance Mode.&lt;/p&gt; &lt;p&gt;Qwen3-30b-a3b is actually FASTER than Qwen3-4b and also performed better in my benchmarks for relevant tasks. But you need a lot of RAM to load it, which is why I specifically looked for the cheapest 32GB RAM laptop. Also, in my testing I found that the Qwen3-4b Thinking model would think for 3000 tokens to give a final 100 token result, which gave an effective generation rate of 0.1-0.2 tok/sec. So I would actually prefer a super slow non-instruct model like Mistral 24b at 2 tok/sec to a thinking model. However, Qwen3-30b-a3b is a nice compromise between speed and reliability.&lt;/p&gt; &lt;p&gt;Most of my use cases are non-interactive, like giving it an email to process and update a calendar. I do not need real time responses. For that reason, I didn't care about slow inference times within reason.&lt;/p&gt; &lt;p&gt;To get reliable performance, I had to split up tasks into simple subtasks. For example, I will ask the LLM to simply list all the topics from an email in the first step. In a second step, I ask the LLM to evaluate the relevancy of each topic in small batches. Then, I ask the LLM to extract JSON structures for each relevant event in order to update the calendar. On a 1000 word email with very high topic density (like a newsletter), Qwen3-30b-a3b would take roughly 9 minutes to process the entire workflow. I tweaked the workflow with various optimizations and could cut it down to about half. That's good enough for me.&lt;/p&gt; &lt;p&gt;I want to keep the power usage low, which means I'm not keeping the models warm. (I also stick to Balanced Mode.) That's why I wanted to record model load times as well. Again, most use cases are non-interactive. If I input a single event, like type &amp;quot;add this event on this time at this date&amp;quot;, the LLM will spin up and add it in under a minute.&lt;/p&gt; &lt;p&gt;I do have some light interactive uses. An example of that is asking for a timer while cooking. I might say &amp;quot;Alexa, set the timer for five minutes.&amp;quot; So here are some notes on that.&lt;/p&gt; &lt;p&gt;First, I use Openwakeword to trigger the whole process so that my laptop is not always running models and recording sound. Openwakeword is pre-tuned for a few wake words, which is why I am using &amp;quot;Alexa&amp;quot; as the wake word for now. I believe this can be tuned in the future. As soon as the wake word is detected, I immediately fire up faster-distil-whisper-small.en and LFM2-8b-a1b. They only take a second each to load, and I'm talking for a few seconds, so there is no lag this way.&lt;/p&gt; &lt;p&gt;LFM2-8b-a1b loads in about 1 second for me and runs at about 25 tok/sec TG (forgot to write down the PP but it is fast too). It is much faster than the other models but not as good with anything requiring reasoning. However, I was surprised at how well it performs in two tasks: topic identification and JSON extraction. So in a 1000 word newsletter filled with 18 topics, LFM2-8b-a1b can reliably extract all 18 topics pretty much as well as Qwen3-30b-a3b. So it's great at summarization, essentially. LFM2-8b-a1b can also reliably form JSON structures. By the way, I am using the model at q8. q4 definitely performs worse. This model, however, is not good at reasoning. For example, if I ask the model to determine if a certain event is relevant or not, it does not perform well. So it is good for fast topic identification and JSON extraction.&lt;/p&gt; &lt;p&gt;I tried various whisper models. I ended up finding the faster-distil-whisper-small.en to be a good compromise between speed and reliability. A sentence like &amp;quot;Alexa, set the timer for 5 minutes&amp;quot; will get parsed in 1 sec, but not as well as I would like. However, if I set the beam_size to 10 (5 is the default, typically), then it takes 2 seconds but with decent reliability. The medium model is too slow, around 5+ seconds even with reduced beam_size, and the base model has horrible accuracy. So that worked for me.&lt;/p&gt; &lt;p&gt;However, to boost the reliability further, I take the output from faster-distil-whisper-small.en and pass it to LFM2-8b-a1b, which gives me a JSON with an action field and a parameter field or two. That gets used to trigger the downstream python script. The LFM2 inference adds about an additional second or so. I don't care about waiting a tiny amount in this case, so that works for me.&lt;/p&gt; &lt;p&gt;For voice commands for adding reminders or calendar events, I will use the LFM2 JSON extraction to trigger re-transcription of the recorded voice message with whisper-largev3. Then, throw it to Qwen3-30b-a3b for processing, since quality is more important than speed.&lt;/p&gt; &lt;p&gt;I almost forgot! Super important, but the built-in mic quality isn't great on laptops. I ended getting a cheap USB wired conference speakerphone for &amp;lt;$20 off ebay. The brand is EMEET, but I think any modern one probably works. Python interacts with the microphone using Pipewire. The microphone made a big difference in transcription quality. It has hardware level sound processing, noise cancellation, etc.&lt;/p&gt; &lt;p&gt;Basically, I am using Qwen3-30b-a3b to process messy inputs (typing, voice, emails) slowly and LFM2-8b-a1b to process messy voice transcription quickly. Again, this all runs on a dirt cheap, old 4650U processor.&lt;/p&gt; &lt;p&gt;This is an ongoing hobby project. I want to eventually see if I can take pictures with the built-in webcam of physical mail or receipts and get one of the VL models or an OCR model to process it. There are trivial things to add, like verbal commands to check the weather and such. A whole bunch of other ideas.&lt;/p&gt; &lt;p&gt;I am loving the low-end LLM ecosystem. The cool part is that the stuff you make actually affects people around you! Like it actually gets used! The Qwen3 and LFM2 models I use are my favorites so far.&lt;/p&gt; &lt;p&gt;Okay, now back to you guys with your 8 x H100 basement setups...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/___positive___"&gt; /u/___positive___ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oh6k6u/some_usage_notes_on_lowend_cpu_llms_and_home/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oh6k6u/some_usage_notes_on_lowend_cpu_llms_and_home/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oh6k6u/some_usage_notes_on_lowend_cpu_llms_and_home/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T05:44:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohjyoc</id>
    <title>Dataset streaming for distributed SOTA model training</title>
    <updated>2025-10-27T17:00:31+00:00</updated>
    <author>
      <name>/u/qlhoest</name>
      <uri>https://old.reddit.com/user/qlhoest</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;Streaming datasets: 100x More Efficient&amp;quot; is a new blog post sharing improvements on dataset streaming to train AI models.&lt;/p&gt; &lt;p&gt;Link: &lt;a href="https://huggingface.co/blog/streaming-datasets"&gt;https://huggingface.co/blog/streaming-datasets&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Summary of the blog post:&lt;/p&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;There is also a 1min video explaining the impact of this: &lt;a href="https://x.com/andimarafioti/status/1982829207471419879"&gt;https://x.com/andimarafioti/status/1982829207471419879&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/qlhoest"&gt; /u/qlhoest &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohjyoc/dataset_streaming_for_distributed_sota_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohjyoc/dataset_streaming_for_distributed_sota_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohjyoc/dataset_streaming_for_distributed_sota_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T17:00:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohfk05</id>
    <title>Made my own Local AI Research Agent | Need suggestions how to improve prompt/execution</title>
    <updated>2025-10-27T14:15:10+00:00</updated>
    <author>
      <name>/u/FriendshipCreepy8045</name>
      <uri>https://old.reddit.com/user/FriendshipCreepy8045</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohfk05/made_my_own_local_ai_research_agent_need/"&gt; &lt;img alt="Made my own Local AI Research Agent | Need suggestions how to improve prompt/execution" src="https://preview.redd.it/adft1ikiwnxf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5103180332a6e1cf251a0358796ec069d99e5ed5" title="Made my own Local AI Research Agent | Need suggestions how to improve prompt/execution" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone!&lt;br /&gt; So, in short I built my own local AI research assistant in Python ü¶ä. &lt;/p&gt; &lt;p&gt;It reads Wikipedia, Arxiv, and news, then outputs professional research summaries directly in the terminal. Everything runs fully offline using Ollama! This is my first time exploring the agentic world, understanding how tool-calling and reasoning flow actually work. &lt;/p&gt; &lt;p&gt;I‚Äôve always been a frontend engineer, and honestly, I didn‚Äôt realize how far the AI world had come ‚Äî the progress is unbelievable. After just 7 days of studying and 1 day of building, I made this small project. It‚Äôs definitely not perfect. &lt;/p&gt; &lt;p&gt;I‚Äôm still using pre-built tools instead of making things from scratch, but the outcome feels like a light version of ChatGPT, running locally!&lt;br /&gt; I‚Äôd really love to hear your thoughts and suggestions on how I can improve this or what I should learn next to move closer to becoming an AI Engineer.&lt;br /&gt; Here‚Äôs the GitHub link: &lt;a href="https://github.com/vedas-dixit/LocalAgent"&gt;https://github.com/vedas-dixit/LocalAgent&lt;/a&gt; If you try it locally, let me know what you think! &lt;/p&gt; &lt;p&gt;Thanks in advance :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FriendshipCreepy8045"&gt; /u/FriendshipCreepy8045 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/adft1ikiwnxf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohfk05/made_my_own_local_ai_research_agent_need/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohfk05/made_my_own_local_ai_research_agent_need/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T14:15:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohrvrm</id>
    <title>Which small models are best for fine-tuning? (most adaptive)</title>
    <updated>2025-10-27T21:58:18+00:00</updated>
    <author>
      <name>/u/Empty-Tourist3083</name>
      <uri>https://old.reddit.com/user/Empty-Tourist3083</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Which ones were most &amp;quot;flexible&amp;quot; (achieved biggest performance gains) when fine-tuned on the same dataset?&lt;/p&gt; &lt;p&gt;Do you have an idea how it differs depending on different sizes? (ex. 0.5-1B; 3-4B; 7-8B)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Empty-Tourist3083"&gt; /u/Empty-Tourist3083 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohrvrm/which_small_models_are_best_for_finetuning_most/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohrvrm/which_small_models_are_best_for_finetuning_most/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohrvrm/which_small_models_are_best_for_finetuning_most/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T21:58:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohmado</id>
    <title>LM Studio Local Server hidden and always running</title>
    <updated>2025-10-27T18:25:30+00:00</updated>
    <author>
      <name>/u/JustSayin_thatuknow</name>
      <uri>https://old.reddit.com/user/JustSayin_thatuknow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys, can someone else confirm that LM Studio, even if you have local server turned off, it is actively listening to localhost port 41343? How is this possible? If you're on windows, try this cmd &amp;quot;netstat -ano | findstr 41343&amp;quot; (if on other OS you'll know how to do it). Mine outputs this &amp;quot;TCP 127.0.0.1:41343 0.0.0.0:0 LISTENING 17200&amp;quot; so when I run this &amp;quot;tasklist /FI &amp;quot;PID eq 17200&amp;quot;&amp;quot; it returns this &amp;quot;LM Studio.exe 17200 Console 1 97,804 K&amp;quot; so I went digging everywhere and can't find anyone with this same issue.. Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JustSayin_thatuknow"&gt; /u/JustSayin_thatuknow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohmado/lm_studio_local_server_hidden_and_always_running/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohmado/lm_studio_local_server_hidden_and_always_running/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohmado/lm_studio_local_server_hidden_and_always_running/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T18:25:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohu19n</id>
    <title>Flagship LLM on 128GB</title>
    <updated>2025-10-27T23:27:24+00:00</updated>
    <author>
      <name>/u/EffectiveGlove1651</name>
      <uri>https://old.reddit.com/user/EffectiveGlove1651</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello ! Running an M4 Max Mac Studio with 128GB RAM. Currently using OSS20B but wondering if I should go bigger for better performance. What models do you recommend for this setup? Worth stepping up in size? Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EffectiveGlove1651"&gt; /u/EffectiveGlove1651 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohu19n/flagship_llm_on_128gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohu19n/flagship_llm_on_128gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohu19n/flagship_llm_on_128gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T23:27:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohoi4n</id>
    <title>Finetuning a LLM (~20B) for Binary Classification ‚Äì Need Advice on Dataset Design</title>
    <updated>2025-10-27T19:48:52+00:00</updated>
    <author>
      <name>/u/United_Demand</name>
      <uri>https://old.reddit.com/user/United_Demand</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm planning to finetune a language model (‚â§20B parameters) for a binary classification task in the healthcare insurance domain. I have around 10M records (won‚Äôt use all for training), and my input data consists of 4 JSON files per sample.&lt;/p&gt; &lt;p&gt;Given the complexity of the domain, I was thinking of embedding &lt;strong&gt;rules&lt;/strong&gt; into the training data to guide the model better. My idea is to structure the dataset using instruction-response format like:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;### Instruction: [Task description + domain-specific rules] ### Input: {...json1...} --- {...json2...} --- {...json3...} --- {...json4...} ### Response: [Binary label] &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;My questions:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Is it a good idea to include rules directly in the instruction part of each sample?&lt;/li&gt; &lt;li&gt;If yes, should I repeat the same rules across all samples, or rephrase them to add variety?&lt;/li&gt; &lt;li&gt;Are there better approaches for incorporating domain knowledge into finetuning?&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/United_Demand"&gt; /u/United_Demand &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohoi4n/finetuning_a_llm_20b_for_binary_classification/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohoi4n/finetuning_a_llm_20b_for_binary_classification/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohoi4n/finetuning_a_llm_20b_for_binary_classification/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T19:48:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohrn20</id>
    <title>Investigating Apple's new "Neural Accelerators" in each GPU core (A19 Pro vs M4 Pro vs M4 vs RTX 3080 - Local LLM Speed Test!)</title>
    <updated>2025-10-27T21:48:31+00:00</updated>
    <author>
      <name>/u/TechExpert2910</name>
      <uri>https://old.reddit.com/user/TechExpert2910</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone :D&lt;/p&gt; &lt;p&gt;I thought it‚Äôd be really interesting to compare how Apple's new A19 Pro (and in turn, the M5) with its fancy &lt;strong&gt;new &amp;quot;neural accelerators&amp;quot; in each GPU core&lt;/strong&gt; compare to other GPUs!&lt;/p&gt; &lt;p&gt;I ran Gemma 3n 4B on each of these devices, outputting ~the same 100-word story (at a temp of 0). I used the most optimal inference framework for each to give each their best shot.&lt;/p&gt; &lt;p&gt;Here're the results!&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;GPU&lt;/th&gt; &lt;th align="left"&gt;Device&lt;/th&gt; &lt;th align="left"&gt;Inference Set-Up&lt;/th&gt; &lt;th align="left"&gt;Tokens / Sec&lt;/th&gt; &lt;th align="left"&gt;Time to First Token&lt;/th&gt; &lt;th align="left"&gt;Perf / GPU Core&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;A19 Pro&lt;/td&gt; &lt;td align="left"&gt;6 GPU cores; iPhone 17 Pro Max&lt;/td&gt; &lt;td align="left"&gt;MLX? (‚ÄúLocal Chat‚Äù app)&lt;/td&gt; &lt;td align="left"&gt;23.5 tok/s&lt;/td&gt; &lt;td align="left"&gt;0.4 s üëÄ&lt;/td&gt; &lt;td align="left"&gt;3.92&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;M4&lt;/td&gt; &lt;td align="left"&gt;10 GPU cores, iPad Pro 13‚Äù&lt;/td&gt; &lt;td align="left"&gt;MLX? (‚ÄúLocal Chat‚Äù app)&lt;/td&gt; &lt;td align="left"&gt;33.4 tok/s&lt;/td&gt; &lt;td align="left"&gt;1.1 s&lt;/td&gt; &lt;td align="left"&gt;3.34&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;RTX 3080&lt;/td&gt; &lt;td align="left"&gt;10 GB VRAM; paired with a Ryzen 5 7600 + 32 GB DDR5&lt;/td&gt; &lt;td align="left"&gt;CUDA 12 llama.cpp (LM Studio)&lt;/td&gt; &lt;td align="left"&gt;59.1 tok/s&lt;/td&gt; &lt;td align="left"&gt;0.02 s&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;M4 Pro&lt;/td&gt; &lt;td align="left"&gt;16 GPU cores, MacBook Pro 14‚Äù, 48 GB unified memory&lt;/td&gt; &lt;td align="left"&gt;MLX (LM Studio)&lt;/td&gt; &lt;td align="left"&gt;60.5 tok/s üëë&lt;/td&gt; &lt;td align="left"&gt;0.31 s&lt;/td&gt; &lt;td align="left"&gt;3.69&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Super Interesting Notes:&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;1. The neural accelerators didn't make much of a difference. Here's why!&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;First off, they do indeed significantly accelerate compute! &lt;a href="https://tzakharko.github.io/apple-neural-accelerators-benchmark/#:%7E:text=Metal%20Shading%20Language.-,Key%20Takeaways%3A,-Operation"&gt;Taras Zakharko found that&lt;/a&gt; Matrix FP16 and Matrix INT8 are already accelerated by 4x and 7x respectively!!!&lt;/li&gt; &lt;li&gt;BUT, when the LLM spits out tokens, we're limited by memory bandwidth, NOT compute. This is especially true with Apple's iGPUs using the comparatively low-memory-bandwith system RAM as VRAM.&lt;/li&gt; &lt;li&gt;Still, there is one stage of inference that is compute-bound: prompt pre-processing! That's why we see the A19 Pro has ~3x faster Time to First Token vs the M4.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://www.macstories.net/linked/max-weinbach-on-the-m5s-neural-accelerators/"&gt;Max Weinbach's testing&lt;/a&gt; also corroborates what I found. And it's also worth noting that MLX hasn't been updated (yet) to take full advantage of the new neural accelerators!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. My M4 Pro as fast as my RTX 3080!!! It's crazy - 350 w vs 35 w&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;When you use an MLX model + MLX on Apple Silicon, you get some really remarkable performance. Note that the 3080 also had ~its best shot with CUDA optimized llama cpp!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TechExpert2910"&gt; /u/TechExpert2910 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohrn20/investigating_apples_new_neural_accelerators_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohrn20/investigating_apples_new_neural_accelerators_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohrn20/investigating_apples_new_neural_accelerators_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T21:48:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohjayo</id>
    <title>Llama.cpp New Ram halves inference speed at a higher context</title>
    <updated>2025-10-27T16:36:05+00:00</updated>
    <author>
      <name>/u/easyrider99</name>
      <uri>https://old.reddit.com/user/easyrider99</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I am just starting to debug this and wondered if anyone else has run into this issue.&lt;/p&gt; &lt;p&gt;I am running a W7-3455 ( Xeon 8 channel DDR5 ). I recently upgraded from 8x64GB DDR5 to 8x96GB. The original kit was a high performance V-color kit with lower CL timings, so the performance on MLC is about a ~5% decrease. In any case, the speed is very good according to MLC ( ~ 240GB/s ).&lt;/p&gt; &lt;p&gt;When running the same parameters with llama-server, I initially get the same inference speeds. However, at about 25K context, the inference speed just drops by half.&lt;/p&gt; &lt;p&gt;Example running DeepSeekV3.1-Terminus at Q4_K_XL:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;srv params_from_: Chat format: DeepSeek V3.1 slot get_availabl: id 0 | task 0 | selected slot by LRU, t_last = 55080165780 slot launch_slot_: id 0 | task 138 | processing task slot update_slots: id 0 | task 138 | new prompt, n_ctx_slot = 164096, n_keep = 0, n_prompt_tokens = 24619 slot update_slots: id 0 | task 138 | n_past = 2, memory_seq_rm [2, end) slot update_slots: id 0 | task 138 | prompt processing progress, n_past = 2050, n_tokens = 2048, progress = 0.083188 slot update_slots: id 0 | task 138 | n_past = 2050, memory_seq_rm [2050, end) slot update_slots: id 0 | task 138 | prompt processing progress, n_past = 4098, n_tokens = 2048, progress = 0.166376 slot update_slots: id 0 | task 138 | n_past = 4098, memory_seq_rm [4098, end) slot update_slots: id 0 | task 138 | prompt processing progress, n_past = 6146, n_tokens = 2048, progress = 0.249563 slot update_slots: id 0 | task 138 | n_past = 6146, memory_seq_rm [6146, end) slot update_slots: id 0 | task 138 | prompt processing progress, n_past = 8194, n_tokens = 2048, progress = 0.332751 slot update_slots: id 0 | task 138 | n_past = 8194, memory_seq_rm [8194, end) slot update_slots: id 0 | task 138 | prompt processing progress, n_past = 10242, n_tokens = 2048, progress = 0.415939 slot update_slots: id 0 | task 138 | n_past = 10242, memory_seq_rm [10242, end) slot update_slots: id 0 | task 138 | prompt processing progress, n_past = 12290, n_tokens = 2048, progress = 0.499127 slot update_slots: id 0 | task 138 | n_past = 12290, memory_seq_rm [12290, end) slot update_slots: id 0 | task 138 | prompt processing progress, n_past = 14338, n_tokens = 2048, progress = 0.582314 slot update_slots: id 0 | task 138 | n_past = 14338, memory_seq_rm [14338, end) slot update_slots: id 0 | task 138 | prompt processing progress, n_past = 16386, n_tokens = 2048, progress = 0.665502 slot update_slots: id 0 | task 138 | n_past = 16386, memory_seq_rm [16386, end) slot update_slots: id 0 | task 138 | prompt processing progress, n_past = 18434, n_tokens = 2048, progress = 0.748690 slot update_slots: id 0 | task 138 | n_past = 18434, memory_seq_rm [18434, end) slot update_slots: id 0 | task 138 | prompt processing progress, n_past = 20482, n_tokens = 2048, progress = 0.831878 slot update_slots: id 0 | task 138 | n_past = 20482, memory_seq_rm [20482, end) slot update_slots: id 0 | task 138 | prompt processing progress, n_past = 22530, n_tokens = 2048, progress = 0.915066 slot update_slots: id 0 | task 138 | n_past = 22530, memory_seq_rm [22530, end) slot update_slots: id 0 | task 138 | prompt processing progress, n_past = 24578, n_tokens = 2048, progress = 0.998253 slot update_slots: id 0 | task 138 | n_past = 24578, memory_seq_rm [24578, end) slot update_slots: id 0 | task 138 | prompt processing progress, n_past = 24619, n_tokens = 41, progress = 0.999919 slot update_slots: id 0 | task 138 | prompt done, n_past = 24619, n_tokens = 41 slot release: id 0 | task 138 | stop processing: n_past = 25332, truncated = 0 slot print_timing: id 0 | task 138 | prompt eval time = 977896.21 ms / 24617 tokens ( 39.72 ms per token, 25.17 tokens per second) eval time = 88448.57 ms / 714 tokens ( 123.88 ms per token, 8.07 tokens per second) total time = 1066344.78 ms / 25331 tokens &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then the following prompt:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;srv update_slots: all slots are idle srv log_server_r: request: POST /v1/chat/completions 10.0.0.40 200 srv params_from_: Chat format: DeepSeek V3.1 slot get_availabl: id 0 | task 138 | selected slot by lcs similarity, lcs_len = 24618, similarity = 0.972 (&amp;gt; 0.100 thold) slot launch_slot_: id 0 | task 865 | processing task slot update_slots: id 0 | task 865 | new prompt, n_ctx_slot = 164096, n_keep = 0, n_prompt_tokens = 25756 slot update_slots: id 0 | task 865 | n_past = 24618, memory_seq_rm [24618, end) slot update_slots: id 0 | task 865 | prompt processing progress, n_past = 25756, n_tokens = 1138, progress = 0.044184 slot update_slots: id 0 | task 865 | prompt done, n_past = 25756, n_tokens = 1138 slot release: id 0 | task 865 | stop processing: n_past = 26212, truncated = 0 slot print_timing: id 0 | task 865 | prompt eval time = 51948.00 ms / 1138 tokens ( 45.65 ms per token, 21.91 tokens per second) eval time = 94955.55 ms / 457 tokens ( 207.78 ms per token, 4.81 tokens per second) total time = 146903.55 ms / 1595 tokens &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This never happened with my previous RAM kit. The inference speed would decrease as context increased, but rather linearly rather than this huge drop. &lt;/p&gt; &lt;p&gt;Any tips?&lt;/p&gt; &lt;p&gt;My current llama-server command:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;numactl --interleave=all ./build/bin/llama-server --model /mnt/home_extend/models/unsloth_DeepSeek-V3.1-Terminus-GGUF/UD-Q4_K_XL/DeepSeek-V3.1-Terminus-UD-Q4_K_XL-00001-of-00008.gguf --alias DeepSeek-V3.1 --threads 44 --ctx-size 120000 --n-gpu-layers 99 --cpu-moe --temp 0.6 --top-p 0.95 -fa 1 --host 0.0.0.0 --jinja --port 8099 --threads 48 --no-host &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/easyrider99"&gt; /u/easyrider99 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohjayo/llamacpp_new_ram_halves_inference_speed_at_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohjayo/llamacpp_new_ram_halves_inference_speed_at_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohjayo/llamacpp_new_ram_halves_inference_speed_at_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T16:36:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohfuea</id>
    <title>Last week in Multimodal AI - Local Edition</title>
    <updated>2025-10-27T14:26:44+00:00</updated>
    <author>
      <name>/u/Vast_Yak_4147</name>
      <uri>https://old.reddit.com/user/Vast_Yak_4147</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohfuea/last_week_in_multimodal_ai_local_edition/"&gt; &lt;img alt="Last week in Multimodal AI - Local Edition" src="https://external-preview.redd.it/GG_nI8HdJYOWQ3BUPyYaxU74wbTZUS40_TzupervzGM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a7b79f3bb4c84db3fbd07ad6d0a3cdb30315acbb" title="Last week in Multimodal AI - Local Edition" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I curate a weekly newsletter on multimodal AI. Here are the local/edge highlights from last week:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;DeepSeek OCR - Efficient Document Parsing&lt;/strong&gt;&lt;br /&gt; ‚Ä¢ Uses optical 2D mapping with lossy compression for 97% OCR accuracy at 10x compression.&lt;br /&gt; ‚Ä¢ Processes 200k+ pages daily on a single A100 GPU, ideal for local document digitization.&lt;br /&gt; ‚Ä¢ &lt;a href="https://github.com/deepseek-ai/DeepSeek-OCR"&gt;GitHub&lt;/a&gt; | &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-OCR"&gt;Hugging Face&lt;/a&gt; | &lt;a href="https://arxiv.org/abs/2510.18234"&gt;Paper&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8mt2da5wynxf1.png?width=1456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=09575812d8fc3336db32cda7148fb8fbc9c0857c"&gt;https://preview.redd.it/8mt2da5wynxf1.png?width=1456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=09575812d8fc3336db32cda7148fb8fbc9c0857c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;LightOnOCR-1B - Multimodal OCR for Edge&lt;/strong&gt;&lt;br /&gt; ‚Ä¢ 1B parameter model transcribes full pages to Markdown at 5.71 pages/second on an H100.&lt;br /&gt; ‚Ä¢ Distilled from a 72B teacher, optimized for low-resource local setups with SOTA efficiency.&lt;br /&gt; ‚Ä¢ &lt;a href="https://huggingface.co/lightonai/LightOnOCR-1B-1025"&gt;Hugging Face&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Tencent Hunyuan World 1.1 (WorldMirror)&lt;/strong&gt;&lt;br /&gt; ‚Ä¢ Feed-forward 3D reconstruction from video or multi-view, running on a single GPU.&lt;br /&gt; ‚Ä¢ Delivers production-ready 3D assets in seconds for local VR and gaming workflows.&lt;br /&gt; ‚Ä¢ &lt;a href="https://3d-models.hunyuan.tencent.com/world/"&gt;Project Page&lt;/a&gt; | &lt;a href="https://github.com/Tencent-Hunyuan/HunyuanWorld-Mirror"&gt;GitHub&lt;/a&gt; | &lt;a href="https://huggingface.co/tencent/HunyuanWorld-Mirror"&gt;Hugging Face&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1ohfuea/video/1arpw5h6znxf1/player"&gt;https://reddit.com/link/1ohfuea/video/1arpw5h6znxf1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Krea Realtime - Real-Time Video Generation&lt;/strong&gt;&lt;br /&gt; ‚Ä¢ 14B model generates video at 11 fps on a single B200 GPU.&lt;br /&gt; ‚Ä¢ Enables real-time interactive video for edge-based creative applications.&lt;br /&gt; ‚Ä¢ &lt;a href="https://huggingface.co/krea/krea-realtime-video"&gt;Hugging Face&lt;/a&gt; | &lt;a href="https://x.com/krea_ai/status/1980358158376988747?s=42"&gt;Announcement&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1ohfuea/video/ula998hcznxf1/player"&gt;https://reddit.com/link/1ohfuea/video/ula998hcznxf1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;AGILE - Agentic Jigsaw Interaction Learning&lt;/strong&gt;&lt;br /&gt; ‚Ä¢ Trains VLMs via trial-and-error puzzle solving, boosting accuracy from 9.5% to 82.8%.&lt;br /&gt; ‚Ä¢ Lightweight and interactive, ideal for edge-based vision task improvement.&lt;br /&gt; ‚Ä¢ &lt;a href="https://yuzeng0-0.github.io/AGILE/"&gt;Project Page&lt;/a&gt; | &lt;a href="https://arxiv.org/abs/2510.01304"&gt;Paper&lt;/a&gt; | &lt;a href="https://github.com/yuzeng0-0/AGILE"&gt;GitHub&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/cqdgb04gznxf1.jpg?width=1456&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=2790f8e9b1e4627202fd96de5485540f4c6456ca"&gt;https://preview.redd.it/cqdgb04gznxf1.jpg?width=1456&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=2790f8e9b1e4627202fd96de5485540f4c6456ca&lt;/a&gt;&lt;/p&gt; &lt;p&gt;See the full newsletter for more demos, papers, and more resources: &lt;a href="https://open.substack.com/pub/thelivingedge/p/multimodal-monday-30-smarter-agents"&gt;https://open.substack.com/pub/thelivingedge/p/multimodal-monday-30-smarter-agents&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vast_Yak_4147"&gt; /u/Vast_Yak_4147 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohfuea/last_week_in_multimodal_ai_local_edition/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohfuea/last_week_in_multimodal_ai_local_edition/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohfuea/last_week_in_multimodal_ai_local_edition/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T14:26:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohq5bc</id>
    <title>GLM-4.6 vs Minimax-M2</title>
    <updated>2025-10-27T20:50:13+00:00</updated>
    <author>
      <name>/u/baykarmehmet</name>
      <uri>https://old.reddit.com/user/baykarmehmet</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;I've been using the GLM Coding Plan and it works well&lt;/strong&gt; ‚Äî not quite Sonnet 4.5 performance, but with clear prompts it gets the job done.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;However, everyone's hyping Minimax M2&lt;/strong&gt;, claiming it crushes every benchmark. The problem? I haven't seen any real-world coding examples or projects using it.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Has anyone here actually used Minimax M2 for development work?&lt;/strong&gt; If so:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;How does it compare to other models in practice?&lt;/li&gt; &lt;li&gt;Is it worth switching to?&lt;/li&gt; &lt;li&gt;Any specific use cases where it excels or falls short?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Would love to hear some hands-on experiences beyond the benchmark numbers.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/baykarmehmet"&gt; /u/baykarmehmet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohq5bc/glm46_vs_minimaxm2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohq5bc/glm46_vs_minimaxm2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohq5bc/glm46_vs_minimaxm2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T20:50:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohqulc</id>
    <title>Looking for a local llm thats good with warhammer 40k lore, Preferably below 10B</title>
    <updated>2025-10-27T21:17:41+00:00</updated>
    <author>
      <name>/u/Hakukh123</name>
      <uri>https://old.reddit.com/user/Hakukh123</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone&lt;/p&gt; &lt;p&gt;So i work in places with spotty/no internet pretty often and im new to &lt;strong&gt;40k lore&lt;/strong&gt;. been trying to find a decent local llm that knows its stuff about &lt;strong&gt;warhammer lore&lt;/strong&gt; so i can ask questions, brainstorm some stuff, or just chat about the setting when im bored.&lt;/p&gt; &lt;p&gt;ive tried a few models through lm studio but they seem pretty hit or miss with the lore - like they know the basic stuff (emperor, chaos, space marines) but when you get into specifics they start making things up or mixing factions.&lt;/p&gt; &lt;p&gt;wondering if anyone here has found a model that actually handles specialized lore well? or if anyone has fine-tuned something for 40k specifically? not looking for anything crazy powerful, just something that can run offline and actually knows the difference between a custodes and a primaris lol.&lt;/p&gt; &lt;p&gt;my setup can handle up to maybe 8b comfortably, could push 10b if its really worth it&lt;/p&gt; &lt;p&gt;any recommendations appreciated, thanks. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hakukh123"&gt; /u/Hakukh123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohqulc/looking_for_a_local_llm_thats_good_with_warhammer/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohqulc/looking_for_a_local_llm_thats_good_with_warhammer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohqulc/looking_for_a_local_llm_thats_good_with_warhammer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T21:17:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1oh57ys</id>
    <title>MiniMaxAI/MiniMax-M2 ¬∑ Hugging Face</title>
    <updated>2025-10-27T04:23:41+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oh57ys/minimaxaiminimaxm2_hugging_face/"&gt; &lt;img alt="MiniMaxAI/MiniMax-M2 ¬∑ Hugging Face" src="https://external-preview.redd.it/UWFNDndMvPJsO1Z9iKM9CbvnTGrRp8W6-SXVbMO4N1g.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0f00892c38fccd0c77d2f3f510b6bc20576cdae9" title="MiniMaxAI/MiniMax-M2 ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/MiniMaxAI/MiniMax-M2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oh57ys/minimaxaiminimaxm2_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oh57ys/minimaxaiminimaxm2_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T04:23:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohnuxy</id>
    <title>How are you preventing production AI agents from going rogue? (Cost overruns, unsafe tool use, etc.)</title>
    <updated>2025-10-27T19:24:09+00:00</updated>
    <author>
      <name>/u/ClearstoneDev</name>
      <uri>https://old.reddit.com/user/ClearstoneDev</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My team is moving our LangChain/LangGraph agents from prototype to production, and we're looking at risks of autonomous execution.&lt;/p&gt; &lt;p&gt;We're trying to solve problems like:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Preventing an agent from getting stuck in a loop and blowing our OpenAI budget.&lt;/li&gt; &lt;li&gt;Enforcing strict rules about which tools certain user roles can trigger (e.g., guests can't use a delete_files tool).&lt;/li&gt; &lt;li&gt;Requiring manual human approval before an agent performs a high-stakes action (like for example a financial transaction).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Right now, our code is getting messy with if/else checks for permissions and budget limits. It feels brittle and hard to audit... How are you all handling this in production?&lt;/p&gt; &lt;p&gt;Are you using framework features (like LangChain's new middleware), external tools (like OPA), or just building custom logic? What are the trade-offs you've found (especially around latency and complexity)?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ClearstoneDev"&gt; /u/ClearstoneDev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohnuxy/how_are_you_preventing_production_ai_agents_from/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohnuxy/how_are_you_preventing_production_ai_agents_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohnuxy/how_are_you_preventing_production_ai_agents_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T19:24:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1oh5asg</id>
    <title>üöÄ New Model from the MiniMax team: MiniMax-M2, an impressive 230B-A10B LLM.</title>
    <updated>2025-10-27T04:28:24+00:00</updated>
    <author>
      <name>/u/chenqian615</name>
      <uri>https://old.reddit.com/user/chenqian615</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oh5asg/new_model_from_the_minimax_team_minimaxm2_an/"&gt; &lt;img alt="üöÄ New Model from the MiniMax team: MiniMax-M2, an impressive 230B-A10B LLM." src="https://a.thumbs.redditmedia.com/b3_JzXejnThTVzO8xn7-iIWxAt3NTQCnaQo4XEvZSC0.jpg" title="üöÄ New Model from the MiniMax team: MiniMax-M2, an impressive 230B-A10B LLM." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Officially positioned as an ‚Äúend-to-end coding + tool-using agent.‚Äù From the public evaluations and model setup, it looks well-suited for teams that need end to end development and toolchain agents, prioritizing lower latency and higher throughput. For real engineering workflows that advance in small but continuous steps, it should offer strong cost-effectiveness. I‚Äôve collected a few points to help with evaluation:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;End-to-end workflow oriented, emphasizing multi-file editing, code, run, fix loops, testing/verification, and long-chain tool orchestration across terminal/browser/retrieval/code execution. These capabilities matter more than just chatting when deploying agents.&lt;/li&gt; &lt;li&gt;Publicly described as ‚Äú~10B activated parameters (total ~200B).‚Äù The design aims to reduce inference latency and per unit cost while preserving coding and tool-calling capabilities, making it suitable for high concurrency and batch sampling.&lt;/li&gt; &lt;li&gt;Benchmark coverage spans end-to-end software engineering (SWE-bench, Terminal-Bench, ArtifactsBench), browsing/retrieval tasks (BrowseComp, FinSearchComp), and holistic intelligence profiling (AA Intelligence).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Position in public benchmarks (not the absolute strongest, but well targeted)&lt;/p&gt; &lt;p&gt;Here are a few developer-relevant metrics I pulled from public tables:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;SWE-bench Verified: 69.4&lt;/li&gt; &lt;li&gt;Terminal-Bench: 46.3&lt;/li&gt; &lt;li&gt;ArtifactsBench: 66.8&lt;/li&gt; &lt;li&gt;BrowseComp: 44.0 (BrowseComp-zh in Chinese: 48.5)&lt;/li&gt; &lt;li&gt;œÑ¬≤-Bench: 77.2&lt;/li&gt; &lt;li&gt;FinSearchComp-global: 65.5&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;From the scores, on tasks that require real toolchain collaboration, this model looks like a balanced choice prioritizing efficiency and stability. Some closed-source models score higher on certain benchmarks, but for end to end development/ agent pipelines, its price performance orientation is appealing. On SWE-bench / Multi-SWE-Bench, steadily completing the modify test modify again loop is often more important than a one-shot perfect fix. These scores and its positioning suggest it can keep pushing the loop toward a runnable solution. A Terminal-Bench score of 46.3 indicates decent robustness in command execution, error recovery, and retries worth trying in a real CI sandbox for small-scale tasks.&lt;/p&gt; &lt;p&gt;References&lt;/p&gt; &lt;p&gt;HF:&lt;a href="https://huggingface.co/MiniMaxAI/MiniMax-M2"&gt;https://huggingface.co/MiniMaxAI/MiniMax-M2&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chenqian615"&gt; /u/chenqian615 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1oh5asg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oh5asg/new_model_from_the_minimax_team_minimaxm2_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oh5asg/new_model_from_the_minimax_team_minimaxm2_an/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T04:28:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohox4r</id>
    <title>Kiln Agent Builder (new): Build agentic systems in minutes with tools, sub-agents, RAG, and context management [Kiln]</title>
    <updated>2025-10-27T20:04:04+00:00</updated>
    <author>
      <name>/u/davernow</name>
      <uri>https://old.reddit.com/user/davernow</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohox4r/kiln_agent_builder_new_build_agentic_systems_in/"&gt; &lt;img alt="Kiln Agent Builder (new): Build agentic systems in minutes with tools, sub-agents, RAG, and context management [Kiln]" src="https://external-preview.redd.it/d3Zva2N5a2ZucHhmMTkufcVC4ejmCLafkThKi0kTFLLjDsIWZAXwIzmkOAnA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a0861e31fca0f585073e8017454429b3c233ac06" title="Kiln Agent Builder (new): Build agentic systems in minutes with tools, sub-agents, RAG, and context management [Kiln]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We just added an interactive Agent builder to &lt;a href="https://github.com/Kiln-AI/Kiln"&gt;the GitHub project Kiln&lt;/a&gt;. With it you can build agentic systems in under 10 minutes. You can do it all through our UI, or use our python library.&lt;/p&gt; &lt;p&gt;What is it? Well ‚Äúagentic‚Äù is just about the most overloaded term in AI, but Kiln supports everything you need to build agents:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://docs.kiln.tech/docs/agents#tool-use"&gt;Tool Use&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.kiln.tech/docs/agents#multi-actor-interaction-aka-subtasks"&gt;Multi-Actor Interaction (aka subtasks)&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.kiln.tech/docs/agents#goal-directed-autonomy-and-reasoning"&gt;Goal Directed, Autonomous Looping &amp;amp; Reasoning&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.kiln.tech/docs/agents#state-and-memory"&gt;State &amp;amp; Memory&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Context Management with Subtasks (aka Multi-Actor Pattern)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Context management is the process of curating the model's context (chat/tool history) to ensure it has the right data, at the right time, in the right level of detail to get the job done.&lt;/p&gt; &lt;p&gt;With Kiln you can implement context management by dividing your agent tasks into subtasks, making context management easy. Each subtask can focus within its own context, then compress/summarize for the parent task. This can make the system faster, cheaper and higher quality. See our &lt;a href="https://docs.kiln.tech/docs/agents#context-management"&gt;docs on context management&lt;/a&gt; for more details.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Eval &amp;amp; Optimize Agent Performance&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Kiln agents work with &lt;a href="https://docs.kiln.tech/docs/evaluations"&gt;Kiln evals&lt;/a&gt; so you can measure and improve agent performance:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Find the ideal model to use, balancing quality, cost and speed&lt;/li&gt; &lt;li&gt;Test different prompts&lt;/li&gt; &lt;li&gt;Evaluate end-to-end quality, or focus on the quality of subtasks&lt;/li&gt; &lt;li&gt;Compare different agent system designs: more/fewer subtasks&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Links and Docs&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Some links to the repo and guides:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/Kiln-AI/Kiln"&gt;Kiln AI on Github - 4k stars&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.kiln.tech/docs/agents"&gt;Docs for Kiln Agents&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://getkiln.ai/discord"&gt;Kiln Discord&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://kiln.tech/"&gt;Homepage&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Feedback and suggestions are very welcome! We‚Äôre already working on custom evals to inspect the trace, and ensure the right tools are used at the right times. What else would be helpful? Any other agent memory patterns you‚Äôd want to see?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/davernow"&gt; /u/davernow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/71g2oykfnpxf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohox4r/kiln_agent_builder_new_build_agentic_systems_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohox4r/kiln_agent_builder_new_build_agentic_systems_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T20:04:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1oht9pw</id>
    <title>Z.ai release Glyph weight</title>
    <updated>2025-10-27T22:55:19+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oht9pw/zai_release_glyph_weight/"&gt; &lt;img alt="Z.ai release Glyph weight" src="https://a.thumbs.redditmedia.com/69TaqDj6bS-Vs7O_YYHmJygT9-976J0J5KJcduszmX8.jpg" title="Z.ai release Glyph weight" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Glyph: Scaling Context Windows via Visual-Text Compression&lt;/p&gt; &lt;p&gt;Paper: arxiv.org/abs/2510.17800 Weights: huggingface.co/zai-org/Glyph Repo: github.com/thu-coai/Glyph&lt;/p&gt; &lt;p&gt;Glyph is a framework for scaling the context length through visual-text compression. It renders long textual sequences into images and processes them using vision‚Äìlanguage models.&lt;/p&gt; &lt;p&gt;This design transforms the challenge of long-context modeling into a multimodal problem, substantially reducing computational and memory costs while preserving semantic information.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1oht9pw"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oht9pw/zai_release_glyph_weight/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oht9pw/zai_release_glyph_weight/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T22:55:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohbcu1</id>
    <title>Experience with the new model MiniMax M2 and some cost saving tips</title>
    <updated>2025-10-27T11:00:41+00:00</updated>
    <author>
      <name>/u/thalacque</name>
      <uri>https://old.reddit.com/user/thalacque</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohbcu1/experience_with_the_new_model_minimax_m2_and_some/"&gt; &lt;img alt="Experience with the new model MiniMax M2 and some cost saving tips" src="https://b.thumbs.redditmedia.com/UVUhaaqNSDCk6qFXVB2lhPVQVQLnJtZQw5XfM0IrY1I.jpg" title="Experience with the new model MiniMax M2 and some cost saving tips" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I saw the discussion about MiniMax M2 in the group chat a couple of days ago, and since their API and agent are free to use, I thought I‚Äôd test it out. First, the conclusion: in my own use, M2 delivers better than expected efficiency and stability. You can feel the team has pushed the model‚Äôs strengths close to top closed models. In some scenarios it reaches top results at clearly lower cost, so it fits as the default executor, with closed models kept for final polish when needed.&lt;/p&gt; &lt;p&gt;My comparison across models:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;A three service monorepo dependency and lock file mess (Node.js + Express). The three services used different versions of jsonwebtoken and had lock file conflicts. The goal was to unify versions, upgrade jwt.verify from callback to Promise, and add an npm run bootstrap script for one click dependency setup and alignment.&lt;/li&gt; &lt;/ol&gt; &lt;ul&gt; &lt;li&gt;M2: breaks down todos, understands the task well, reads files first, lists a plan, then edits step by step. It detects three version drifts and proposes an alignment strategy, adds the bootstrap script, runs one round of install and startup checks. Small fixes are quick, friendly to regression runs, and it feels ready to drop into a pipeline for repeated runs. Claude: strong first pass, but cross service consistency sometimes needed repeated reminders, took more rounds, and usage cost was higher. GLM/Kimi: can get the main path working, but more likely to leave rough edges in lock files and scripts that I had to clean up.&lt;/li&gt; &lt;/ul&gt; &lt;ol&gt; &lt;li&gt;An online 3x3 Rubik‚Äôs Cube (a small front end interaction project): rotate a layer to a target angle, buttons to choose a face, show the 3x3 color grid.&lt;/li&gt; &lt;/ol&gt; &lt;ul&gt; &lt;li&gt;M2: To be honest, the first iteration wasn‚Äôt great, major issues like text occlusion and non-functional rotation weren‚Äôt addressed. The bright spot is that interaction bugs (e.g., rotation state desynchronization) could be fixed in a single pass once pointed out, without introducing new regressions. After subsequent rounds of refinement, the final result actually became the most usable and presentable, fully supporting 3D dragging. GLM/Kimi: The first round results were decent, but both ran into problems in the second round. GLM didn‚Äôt resolve the Rubik‚Äôs Cube floating/hover position issue, and Kimi, after the second round feedback, ended up not being three-dimensional. Claude performed excellently after the first round of prompts, with all features working normally, but even after multiple later rounds it still didn‚Äôt demonstrate an understanding of a 3D cube (in the image, Claude‚Äôs Rubik‚Äôs Cube is flat and the view can‚Äôt be rotated).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Metrics echo this feel: SWE bench Verified 69.4, Terminal Bench 46.3, ArtifactsBench 66.8, BrowseComp 44.0, FinSearchComp global 65.5. It is not first in every category, but on the runnable and fixable engineering loop, the structure score looks better. From my use, the strengths are proposing a plan, checking its own work, and favoring short fast iterations that clear blockers one by one.&lt;/p&gt; &lt;p&gt;Replace most closed model usage without sacrificing the reliability of the engineering loop. M2 is already enough and surprisingly handy. Set it as the default executor and run regressions for two days; the difference will be clear. After putting it into the pipeline, with the same budget you can run more in parallel, and you do save money.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/MiniMaxAI/MiniMax-M2"&gt;https://huggingface.co/MiniMaxAI/MiniMax-M2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/MiniMax-AI/MiniMax-M2"&gt;https://github.com/MiniMax-AI/MiniMax-M2&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thalacque"&gt; /u/thalacque &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ohbcu1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohbcu1/experience_with_the_new_model_minimax_m2_and_some/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohbcu1/experience_with_the_new_model_minimax_m2_and_some/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T11:00:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohlhdx</id>
    <title>Phoronix benchmarks single and dual AMD R9700 GPUs against a single NVIDIA RTX 6000 Ada GPU</title>
    <updated>2025-10-27T17:56:13+00:00</updated>
    <author>
      <name>/u/Brian-Puccio</name>
      <uri>https://old.reddit.com/user/Brian-Puccio</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Brian-Puccio"&gt; /u/Brian-Puccio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.phoronix.com/review/amd-radeon-ai-pro-r9700"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohlhdx/phoronix_benchmarks_single_and_dual_amd_r9700/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohlhdx/phoronix_benchmarks_single_and_dual_amd_r9700/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T17:56:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohpqvy</id>
    <title>Radeon R9700 Dual GPU First Look ‚Äî AI/vLLM plus creative tests with Nuke &amp; the Adobe Suite</title>
    <updated>2025-10-27T20:34:58+00:00</updated>
    <author>
      <name>/u/atape_1</name>
      <uri>https://old.reddit.com/user/atape_1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohpqvy/radeon_r9700_dual_gpu_first_look_aivllm_plus/"&gt; &lt;img alt="Radeon R9700 Dual GPU First Look ‚Äî AI/vLLM plus creative tests with Nuke &amp;amp; the Adobe Suite" src="https://external-preview.redd.it/nujocFBKcx2R2jDqpB8QfazaXJG6_pPL0crrme1qkLM.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=166ad90b700b767ba9a03c0fc586c47d3289ab0d" title="Radeon R9700 Dual GPU First Look ‚Äî AI/vLLM plus creative tests with Nuke &amp;amp; the Adobe Suite" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/atape_1"&gt; /u/atape_1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=efQPFhZmhAo&amp;amp;embeds_referring_euri=https%3A%2F%2Fwww.reddit.com%2F"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohpqvy/radeon_r9700_dual_gpu_first_look_aivllm_plus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohpqvy/radeon_r9700_dual_gpu_first_look_aivllm_plus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T20:34:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohm80t</id>
    <title>Newegg has 32gb AMD r9700 for $1,300</title>
    <updated>2025-10-27T18:23:03+00:00</updated>
    <author>
      <name>/u/Terminator857</name>
      <uri>https://old.reddit.com/user/Terminator857</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://videocardz.com/newz/amd-radeon-pro-ai-r9700-is-now-available-32gb-memory-and-full-navi-48-gpu"&gt;https://videocardz.com/newz/amd-radeon-pro-ai-r9700-is-now-available-32gb-memory-and-full-navi-48-gpu&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Phoronix did a poor job of benchmarking it. Would prefer benchmarking a 30gb model like qwen3 coder, but instead focuses on 8gb model: &lt;a href="https://www.phoronix.com/review/amd-radeon-ai-pro-r9700"&gt;https://www.phoronix.com/review/amd-radeon-ai-pro-r9700&lt;/a&gt; Doesn't bother to compare it to 4090/5090. This video does gaming benchmarks: &lt;a href="https://www.youtube.com/watch?v=x0YJ32Q0mNw"&gt;https://www.youtube.com/watch?v=x0YJ32Q0mNw&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Guessing 30 tokens per second (TPS) for qwen3 coder.&lt;/p&gt; &lt;p&gt;Also found at: &lt;a href="https://www.amazon.com/XFX-Radeon-R9700-GDDR6-RX-97XPROAIY/dp/B0FXTRGHL9"&gt;https://www.amazon.com/XFX-Radeon-R9700-GDDR6-RX-97XPROAIY/dp/B0FXTRGHL9&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Terminator857"&gt; /u/Terminator857 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohm80t/newegg_has_32gb_amd_r9700_for_1300/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohm80t/newegg_has_32gb_amd_r9700_for_1300/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohm80t/newegg_has_32gb_amd_r9700_for_1300/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T18:23:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohihvo</id>
    <title>Another Banger from Inclusion AI: Ming-flash-omni-Preview</title>
    <updated>2025-10-27T16:06:14+00:00</updated>
    <author>
      <name>/u/Finanzamt_Endgegner</name>
      <uri>https://old.reddit.com/user/Finanzamt_Endgegner</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohihvo/another_banger_from_inclusion_ai/"&gt; &lt;img alt="Another Banger from Inclusion AI: Ming-flash-omni-Preview" src="https://external-preview.redd.it/PFGMqHZG1FenJLDckxpcToXwao333pejl_fZNW4bWqk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=04ce1f6abd38ec8e123f358b2f5b41d3b28a30d6" title="Another Banger from Inclusion AI: Ming-flash-omni-Preview" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/inclusionAI/Ming-flash-omni-Preview"&gt;https://huggingface.co/inclusionAI/Ming-flash-omni-Preview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Based on &lt;a href="https://huggingface.co/inclusionAI/Ling-flash-2.0"&gt;Ling-Flash-2.0&lt;/a&gt; this model has 100b total parameters and 6b active ones and supports context aware asr, text to speech, image generation and editing, segmentation etc (well its an omni modal model so you know the drill). Since its fairly sparse it is very efficient and while I couldn't test it myself the benchmarks seem promising, and it also supports voice cloning (;&lt;/p&gt; &lt;p&gt;It says it can do dialect-aware ASR, though im not sure if that will only work with Chinese ü§î&lt;/p&gt; &lt;p&gt;Anyways, if im not mistaken this is the biggest open sourced omni modal model yet so thanks to the mad lads at inclusion ai!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/qml9ai33goxf1.png?width=2972&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=29df775ad390dc4e6cb4306e302540f231bdf556"&gt;https://preview.redd.it/qml9ai33goxf1.png?width=2972&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=29df775ad390dc4e6cb4306e302540f231bdf556&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1ohihvo/video/oh86jahegoxf1/player"&gt;https://reddit.com/link/1ohihvo/video/oh86jahegoxf1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1ohihvo/video/zbxb11vnhoxf1/player"&gt;https://reddit.com/link/1ohihvo/video/zbxb11vnhoxf1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Finanzamt_Endgegner"&gt; /u/Finanzamt_Endgegner &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohihvo/another_banger_from_inclusion_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohihvo/another_banger_from_inclusion_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohihvo/another_banger_from_inclusion_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T16:06:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohdl9q</id>
    <title>Silicon Valley is migrating from expensive closed-source models to cheaper open-source alternatives</title>
    <updated>2025-10-27T12:53:12+00:00</updated>
    <author>
      <name>/u/xiaoruhao</name>
      <uri>https://old.reddit.com/user/xiaoruhao</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohdl9q/silicon_valley_is_migrating_from_expensive/"&gt; &lt;img alt="Silicon Valley is migrating from expensive closed-source models to cheaper open-source alternatives" src="https://external-preview.redd.it/cDlpaWtncThobnhmMYyuPjWRTezxeRfqB3upVJ5ATISaueUIVVjdl6ikWaxE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=49803f057b43fcece9390b3b966fe6ba4de209b3" title="Silicon Valley is migrating from expensive closed-source models to cheaper open-source alternatives" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Chamath Palihapitiya said his team migrated a large number of workloads to Kimi K2 because it was significantly more performant and much cheaper than both OpenAI and Anthropic.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xiaoruhao"&gt; /u/xiaoruhao &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/avwpphq8hnxf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohdl9q/silicon_valley_is_migrating_from_expensive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohdl9q/silicon_valley_is_migrating_from_expensive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T12:53:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohtp6d</id>
    <title>Bad news: DGX Spark may have only half the performance claimed.</title>
    <updated>2025-10-27T23:13:15+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohtp6d/bad_news_dgx_spark_may_have_only_half_the/"&gt; &lt;img alt="Bad news: DGX Spark may have only half the performance claimed." src="https://preview.redd.it/9b2ziei0lqxf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=de9741ebb17cdabb88dde97eb430a1c2ff563565" title="Bad news: DGX Spark may have only half the performance claimed." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There might be more bad news about the DGX Spark!&lt;/p&gt; &lt;p&gt;Before it was even released, I told everyone that this thing has a memory bandwidth problem. Although it boasts 1 PFLOPS of FP4 floating-point performance, its memory bandwidth is only 273GB/s. This will cause major stuttering when running large models (with performance being roughly only one-third of a MacStudio M2 Ultra).&lt;/p&gt; &lt;p&gt;Today, more bad news emerged: the floating-point performance doesn't even reach 1 PFLOPS.&lt;/p&gt; &lt;p&gt;Tests from two titans of the industry‚ÄîJohn Carmack (founder of id Software, developer of games like Doom, and a name every programmer should know from the legendary fast inverse square root algorithm) and Awni Hannun (the primary lead of Apple's large model framework, MLX)‚Äîhave shown that this device only achieves 480 TFLOPS of FP4 performance (approximately 60 TFLOPS BF16). That's less than half of the advertised performance.&lt;/p&gt; &lt;p&gt;Furthermore, if you run it for an extended period, it will overheat and restart.&lt;/p&gt; &lt;p&gt;It's currently unclear whether the problem is caused by the power supply, firmware, CUDA, or something else, or if the SoC is genuinely this underpowered. I hope Jensen Huang fixes this soon. The memory bandwidth issue could be excused as a calculated product segmentation decision from NVIDIA, a result of us having overly high expectations meeting his precise market strategy. However, performance not matching the advertised claims is a major integrity problem.&lt;/p&gt; &lt;p&gt;So, for all the folks who bought an NVIDIA DGX Spark, Gigabyte AI TOP Atom, or ASUS Ascent GX10, I recommend you all run some tests and see if you're indeed facing performance issues.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9b2ziei0lqxf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohtp6d/bad_news_dgx_spark_may_have_only_half_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohtp6d/bad_news_dgx_spark_may_have_only_half_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T23:13:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohdzxs</id>
    <title>AMA Announcement: Liquid AI, the team behind Liquid Foundational Models, LEAP and Apollo (Thu, Oct 30 ‚Ä¢ 10 AM ‚Äì 1 PM PDT)</title>
    <updated>2025-10-27T13:10:46+00:00</updated>
    <author>
      <name>/u/LiquidAI_Team</name>
      <uri>https://old.reddit.com/user/LiquidAI_Team</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohdzxs/ama_announcement_liquid_ai_the_team_behind_liquid/"&gt; &lt;img alt="AMA Announcement: Liquid AI, the team behind Liquid Foundational Models, LEAP and Apollo (Thu, Oct 30 ‚Ä¢ 10 AM ‚Äì 1 PM PDT)" src="https://preview.redd.it/47wfyylmlnxf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c359ceba4921b523ecd2e493f9cc84bd8b3e7881" title="AMA Announcement: Liquid AI, the team behind Liquid Foundational Models, LEAP and Apollo (Thu, Oct 30 ‚Ä¢ 10 AM ‚Äì 1 PM PDT)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;When: Thursday 10/30, 10 AM ‚Äì 1 PM PST&lt;/h1&gt; &lt;p&gt;The Liquid AI team will also continue answering questions for the following 24 hours, so jump in anytime!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Who will be there:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Jacob Marks (Data)&lt;/li&gt; &lt;li&gt;Jimmy Smith (Pre-Training)&lt;/li&gt; &lt;li&gt;Maxime Labonne (Post-Training)&lt;/li&gt; &lt;li&gt;Fernando Fernandes (Post-training)&lt;/li&gt; &lt;li&gt;Anna Banaszak (LFM2-VL)&lt;/li&gt; &lt;li&gt;Arthur B√∂√∂k (LFM2-Audio)&lt;/li&gt; &lt;li&gt;Yuri Khrustalev (Inference engine, llama.cpp)&lt;/li&gt; &lt;li&gt;Darian Bhathena (LEAP SDK and Apollo)&lt;/li&gt; &lt;li&gt;Edoardo Mosca (LEAP Best Model Search and Finetune)&lt;/li&gt; &lt;li&gt;Anthony Crognale (LEAP SDK)&lt;/li&gt; &lt;li&gt;Pau Labarta Bajo (Dev Relations)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Want to get started?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;‚Üí &lt;a href="https://leap.liquid.ai/models?utm_source=reddit&amp;amp;utm_medium=devrel"&gt;Deploy your first model on-device today&lt;/a&gt;&lt;br /&gt; ‚Üí &lt;a href="https://huggingface.co/LiquidAI?utm_source=reddit&amp;amp;utm_medium=devrel"&gt;Check out our models on Hugging Face&lt;/a&gt;&lt;br /&gt; ‚Üí &lt;a href="https://www.liquid.ai/apollo?utm_source=reddit&amp;amp;utm_medium=devrel"&gt;Play with models on Apollo&lt;/a&gt;&lt;br /&gt; ‚Üí &lt;a href="https://www.liquid.ai/company/news?utm_source=reddit&amp;amp;utm_medium=devrel"&gt;Learn more about our recent releases&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LiquidAI_Team"&gt; /u/LiquidAI_Team &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/47wfyylmlnxf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohdzxs/ama_announcement_liquid_ai_the_team_behind_liquid/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohdzxs/ama_announcement_liquid_ai_the_team_behind_liquid/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T13:10:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohqev8</id>
    <title>Best Local TTS/STT Models - October 2025</title>
    <updated>2025-10-27T21:00:42+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Share what your favorite TTS / STT models are right now &lt;strong&gt;and why.&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Given the the amount of ambiguity and subjectivity in rating/testing these models, please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc. Closed models like Elevenlabs v3 seem to continue to be a few levels above open models, so comparisons, especially empirical ones are welcome.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please use the top level TTS/STT comments to thread your responses. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohqev8/best_local_ttsstt_models_october_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohqev8/best_local_ttsstt_models_october_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohqev8/best_local_ttsstt_models_october_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T21:00:42+00:00</published>
  </entry>
</feed>
