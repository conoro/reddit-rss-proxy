<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-22T14:49:52+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1nnmqvs</id>
    <title>Topics for a hands on course on LLMs</title>
    <updated>2025-09-22T13:38:41+00:00</updated>
    <author>
      <name>/u/Top-Book2609</name>
      <uri>https://old.reddit.com/user/Top-Book2609</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; , I have been a long time reader of this community and have learnt a lot. Thank you all for the amazing information here.&lt;/p&gt; &lt;p&gt;At my University, we want to float a 4-5 month long course on LLMs focusing on applications and engineering side as compared to research or pretraining. While it is floated at a university, the audience will be mostly experienced software professionals. To make it interesting for professionals, we will have demos, labs and hands on assignments each week. I have made a rough sketch of topics to cover and your feedback on the set of topics will definitely help. Each week will have 2 classes of 1.5 hrs each&lt;/p&gt; &lt;p&gt;Topics shortlisted week wise : &lt;/p&gt; &lt;p&gt;|| || |1. LLM Foundations - Transformer Architecture - GPT 1 and 2| |2. Tokenization, Pretraining objectives, Mixture of Experts| |3. Case studies : State-of-the-art open-source LLM architectures (GPT OSS, Qwen 3, Gemma etc), Scaling Laws| |4. GPU architecture deep dive, Parallelism: Multi GPU and Multi Node, On-Prem Hardware Stack Deep Dive| |5. Inference Math and Bottlenecks, Efficient Attention &amp;amp; KV Caching| |6. Quantization Fundamentals| |7. Inference Engines and Multi GPU, Case study : Serving large models| |8. Full Fine-Tuning vs. PEFT, Data Preparation &amp;amp; Instruction Tuning| |9. Instruction tuning &amp;amp; alignment (RLHF, DPO etc)| |10. Reasoning &amp;amp; Chain-of-Thought, Prompt Engineering| |11. RAG Fundamentals, Evaluating RAG| |12. ReAct Framework, MCP introduction, Agentic RAG, Multi Agent Orchestration, Multimodal Agents| |13. Agent Evaluation, Fine Tuning for Tool calling, | |14. Evaluation, Observability &amp;amp; Monitoring| |15. Multi Modal Architecture : Image, Audio and Video models, Running Locally, Fine tuning multimodal models| |16. Edge-Optimized LLM Architectures, Case Studies, Edge Optimization techniques| |17. Security : Prompt Injection, Jailbreaking, Data Leakage, Emerging Topics: Mamba, Qwen Next, Hybrid architectures|&lt;/p&gt; &lt;p&gt;Please suggest me if we can remove any topic or add others. This will greatly help. We're planning to release the slides, notebooks and assignments on Github.&lt;/p&gt; &lt;p&gt;Thank you all again!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Top-Book2609"&gt; /u/Top-Book2609 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnmqvs/topics_for_a_hands_on_course_on_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnmqvs/topics_for_a_hands_on_course_on_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nnmqvs/topics_for_a_hands_on_course_on_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-22T13:38:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1nnipty</id>
    <title>SillyTavern for story writing?</title>
    <updated>2025-09-22T10:22:31+00:00</updated>
    <author>
      <name>/u/rdpl_</name>
      <uri>https://old.reddit.com/user/rdpl_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ST has many features well suited for story writing despite its actual use case being chat. There are &lt;a href="https://www.reddit.com/r/SillyTavernAI/comments/1ewz2e7/comment/lj2rkpv/"&gt;some &amp;quot;hacks&amp;quot;&lt;/a&gt; in order to tweak ST into this direction.&lt;/p&gt; &lt;p&gt;Since I am a bit out of the loop, should I still use ST for story writing or are there better ways nowadays or should I just use text-generation-webui and use the system message for the meta info?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rdpl_"&gt; /u/rdpl_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnipty/sillytavern_for_story_writing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnipty/sillytavern_for_story_writing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nnipty/sillytavern_for_story_writing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-22T10:22:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1nne3ra</id>
    <title>How do I disable thinking in Deepseek V3.1?</title>
    <updated>2025-09-22T05:29:09+00:00</updated>
    <author>
      <name>/u/MengerianMango</name>
      <uri>https://old.reddit.com/user/MengerianMango</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;``` llama-cli -hf unsloth/DeepSeek-V3.1-GGUF:Q5_K_XL \ --jinja --mlock \ --prio 3 -ngl 99 --cpu-moe \&lt;br /&gt; --temp 0.6 --top_p 0.95 --min_p 0.01 --ctx-size $((128*1024)) \ -t 128 -b 10240 \ -p &amp;quot;Tell me about PCA.&amp;quot; --verbose-prompt&lt;/p&gt; &lt;h1&gt;... log output&lt;/h1&gt; &lt;p&gt;main: prompt: '/no&lt;em&gt;think Tell me about PCA.' main: number of tokens in prompt = 12 0 -&amp;gt; '&amp;lt;｜begin▁of▁sentence｜&amp;gt;' 128803 -&amp;gt; '&amp;lt;｜User｜&amp;gt;' 91306 -&amp;gt; '/no' 65 -&amp;gt; '&lt;/em&gt;' 37947 -&amp;gt; 'think' 32536 -&amp;gt; ' Tell' 678 -&amp;gt; ' me' 943 -&amp;gt; ' about' 78896 -&amp;gt; ' PCA' 16 -&amp;gt; '.' 128804 -&amp;gt; '&amp;lt;｜Assistant｜&amp;gt;' 128798 -&amp;gt; '&amp;lt;think&amp;gt;'&lt;/p&gt; &lt;h1&gt;more log output&lt;/h1&gt; &lt;p&gt;Tell me about PCA.&amp;lt;think&amp;gt;Hmm, the user asked about PCA. They probably want a straightforward, jargon-free explanation without overcomplicating it. Since PCA is a technical topic, I should balance simplicity with accuracy. &lt;/p&gt; &lt;p&gt;I'll start with a high-level intuition—comparing it to photo compression—to make it relatable. Then, I'll break down the core ideas: variance, eigenvectors, and dimensionality reduction, but keep it concise. No need for deep math unless the user asks. &lt;/p&gt; &lt;p&gt;The response should end with a clear summary of pros and cons, since practical use cases matter. Avoid tangents—stick to what PCA is, why it's useful, and when to use it.&amp;lt;/think&amp;gt;Of course. Here is a straightforward explanation of Principal Component Analysis (PCA).&lt;/p&gt; &lt;h3&gt;The Core Idea in Simple Terms&lt;/h3&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;I've tried /no_think, \no_think, --reasoning-budget 0, etc. None of that seems to work.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MengerianMango"&gt; /u/MengerianMango &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nne3ra/how_do_i_disable_thinking_in_deepseek_v31/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nne3ra/how_do_i_disable_thinking_in_deepseek_v31/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nne3ra/how_do_i_disable_thinking_in_deepseek_v31/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-22T05:29:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1nnk7qj</id>
    <title>Running LLM on Orange Pi 5</title>
    <updated>2025-09-22T11:45:13+00:00</updated>
    <author>
      <name>/u/SlovenskiFemboy418</name>
      <uri>https://old.reddit.com/user/SlovenskiFemboy418</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I have Orange Pi 5 with 16 GB of RAM, 8 core CPU (4x2,4GHz and 4x1,8GHz) and NVMe SSD. &lt;/p&gt; &lt;p&gt;So I asked ChatGPT and it told me that my device could run Deepseek R1 Distilled 7B at about 3 tokens/s and the 13B version at around 1,5 tokens / second. However I have no issue if a minute is needed for it to answer or perhaps 2 minutes for a more complex topic.&lt;/p&gt; &lt;p&gt;So I wanna use this for a Discord bot that, when tagged, will provide an answer to a user's statement in my server. &lt;/p&gt; &lt;p&gt;I want it to be for general use, so providing answer to math questions, programming questions, history or food nutrition related queston or generaly anything. &lt;/p&gt; &lt;p&gt;I also plan to use RAG to feed it some books and some documents to provide answers on related topics based on those.&lt;/p&gt; &lt;p&gt;I will install heatsinks and a fan on Orange Pi so that might provide some room for CPU overclocking if I decide so in the future. &lt;/p&gt; &lt;p&gt;Do you guys have any advice for me or perhaps suggest a different model, ChatGPT compared a few models for me and came to the conclusion that its the best for me to go with Deepseek R1 Distilled 7B.&lt;/p&gt; &lt;p&gt;Regarding RAM usage, it estimated that 7B model would use up about 6 GB of RAM while it estimates that the 13B model would use up around 13 GB. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SlovenskiFemboy418"&gt; /u/SlovenskiFemboy418 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnk7qj/running_llm_on_orange_pi_5/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnk7qj/running_llm_on_orange_pi_5/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nnk7qj/running_llm_on_orange_pi_5/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-22T11:45:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1nnn6aq</id>
    <title>Best local model to feed large amounts of data to train on?</title>
    <updated>2025-09-22T13:56:33+00:00</updated>
    <author>
      <name>/u/Hiking_lover</name>
      <uri>https://old.reddit.com/user/Hiking_lover</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, I'm looking to build a system and run a LLM on locally that we can train with our own data as well. We have hundreds of thousands of datapoints from testing of thousands of different types of chemicals, alongside millions of datapoints for manufactured chemical properties, and we're looking to have a model we can use for years to help us fine tune our R&amp;amp;D. Obviously, &amp;quot;general&amp;quot; knowledge is a bit less critical here, as we really need something that can build off of the massive amounts of data we've collected over many years. Any recommendations for models that can be trained on data that then becomes part of their permanent knowledge?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hiking_lover"&gt; /u/Hiking_lover &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnn6aq/best_local_model_to_feed_large_amounts_of_data_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnn6aq/best_local_model_to_feed_large_amounts_of_data_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nnn6aq/best_local_model_to_feed_large_amounts_of_data_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-22T13:56:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1nn18k2</id>
    <title>Predicting the next "attention is all you need"</title>
    <updated>2025-09-21T19:30:36+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;NeurIPS 2025 &lt;a href="https://neurips.cc/Downloads/2025"&gt;accepted papers&lt;/a&gt; are out! If you didn't know, &amp;quot;Attention is all you Need&amp;quot; was published in &lt;a href="https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf"&gt;NeurIPS 2017&lt;/a&gt; and spawned the modern wave of Transformer-based large language models; but few would have predicted this back in 2017. Which NeurIPS 2025 paper do you think is the bext &amp;quot;Attention is all you Need&amp;quot;? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://neurips.cc/Downloads/2025"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nn18k2/predicting_the_next_attention_is_all_you_need/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nn18k2/predicting_the_next_attention_is_all_you_need/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T19:30:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1nn01bj</id>
    <title>Qwen3-Coder-480B on the M3 Ultra 512GB Mac Studio is perfect for agentic coding</title>
    <updated>2025-09-21T18:45:29+00:00</updated>
    <author>
      <name>/u/ButThatsMyRamSlot</name>
      <uri>https://old.reddit.com/user/ButThatsMyRamSlot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen3-Coder-480b runs in MLX with 8bit quantization and just barely fits the full 256k context window within 512GB.&lt;/p&gt; &lt;p&gt;With Roo code/cline, Q3C works exceptionally well when working within an existing codebase.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;RAG (with Qwen3-Embed) retrieves API documentation and code samples which eliminates hallucinations.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;The long context length can handle entire source code files for additional details.&lt;/li&gt; &lt;li&gt;Prompt adherence is great, and the subtasks in Roo work very well to gather information without saturating the main context.&lt;/li&gt; &lt;li&gt;VSCode hints are read by Roo and provide feedback about the output code.&lt;/li&gt; &lt;li&gt;Console output is read back to identify compile time and runtime errors.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Green grass is more difficult, Q3C doesn’t do the best job at architecting a solution given a generic prompt. It’s much better to explicitly provide a design or at minimum design constraints rather than just “implement X using Y”.&lt;/p&gt; &lt;p&gt;Prompt processing, especially at full 256k context, can be quite slow. For an agentic workflow, this doesn’t matter much, since I’m running it in the background. I find Q3C difficult to use as a coding &lt;em&gt;assistant&lt;/em&gt;, at least the 480b version.&lt;/p&gt; &lt;p&gt;I was on the fence about this machine 6 months ago when I ordered it, but I’m quite happy with what it can do now. An alternative option I considered was to buy an RTX Pro 6000 for my 256GB threadripper system, but the throughout benefits are far outweighed by the ability to run larger models at higher precision in my use case.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ButThatsMyRamSlot"&gt; /u/ButThatsMyRamSlot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nn01bj/qwen3coder480b_on_the_m3_ultra_512gb_mac_studio/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nn01bj/qwen3coder480b_on_the_m3_ultra_512gb_mac_studio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nn01bj/qwen3coder480b_on_the_m3_ultra_512gb_mac_studio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T18:45:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1nmzio1</id>
    <title>LongCat-Flash-Thinking</title>
    <updated>2025-09-21T18:25:42+00:00</updated>
    <author>
      <name>/u/Xhehab_</name>
      <uri>https://old.reddit.com/user/Xhehab_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmzio1/longcatflashthinking/"&gt; &lt;img alt="LongCat-Flash-Thinking" src="https://preview.redd.it/l7o00pbb9kqf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3fb8a419e2a1961bdcf7234e4eb8e33897a2904f" title="LongCat-Flash-Thinking" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;🚀 LongCat-Flash-Thinking: Smarter reasoning, leaner costs!&lt;/p&gt; &lt;p&gt;🏆 Performance: SOTA open-source models on Logic/Math/Coding/Agent tasks&lt;/p&gt; &lt;p&gt;📊 Efficiency: 64.5% fewer tokens to hit top-tier accuracy on AIME25 with native tool use, agent-friendly&lt;/p&gt; &lt;p&gt;⚙️ Infrastructure: Async RL achieves a 3x speedup over Sync frameworks&lt;/p&gt; &lt;p&gt;🔗Model: &lt;a href="https://huggingface.co/meituan-longcat/LongCat-Flash-Thinking"&gt;https://huggingface.co/meituan-longcat/LongCat-Flash-Thinking&lt;/a&gt;&lt;/p&gt; &lt;p&gt;💻 Try Now: &lt;a href="http://longcat.ai"&gt;longcat.ai&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xhehab_"&gt; /u/Xhehab_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/l7o00pbb9kqf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmzio1/longcatflashthinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nmzio1/longcatflashthinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T18:25:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1nn72ji</id>
    <title>Optimizing gpt-oss-120b local inference speed on consumer hardware</title>
    <updated>2025-09-21T23:32:32+00:00</updated>
    <author>
      <name>/u/carteakey</name>
      <uri>https://old.reddit.com/user/carteakey</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ul&gt; &lt;li&gt;Got GPT‑OSS‑120B running with llama.cpp on mid‑range hardware – i5‑12600K + RTX 4070 (12 GB) + 64 GB DDR5 – ≈191 tps prompt, ≈10 tps generation with a 24k context window.&lt;/li&gt; &lt;li&gt;Distilled &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; tips &amp;amp; community tweaks into an article (run script, benchmarks).&lt;/li&gt; &lt;li&gt;Feedback and further tuning ideas welcome!&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;em&gt;script + step‑by‑step tuning guide ➜&lt;/em&gt; &lt;a href="https://carteakey.dev/optimizing%20gpt-oss-120b-local%20inference/"&gt;https://carteakey.dev/optimizing%20gpt-oss-120b-local%20inference/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/carteakey"&gt; /u/carteakey &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://carteakey.dev/optimizing%20gpt-oss-120b-local%20inference/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nn72ji/optimizing_gptoss120b_local_inference_speed_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nn72ji/optimizing_gptoss120b_local_inference_speed_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T23:32:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1nnmfne</id>
    <title>SWE-Bench Pro released, targeting dataset contamination</title>
    <updated>2025-09-22T13:25:49+00:00</updated>
    <author>
      <name>/u/Pristine-Woodpecker</name>
      <uri>https://old.reddit.com/user/Pristine-Woodpecker</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnmfne/swebench_pro_released_targeting_dataset/"&gt; &lt;img alt="SWE-Bench Pro released, targeting dataset contamination" src="https://external-preview.redd.it/Qw0D15PigrzJYps6l8gVjMdI1NYqWX8uTNTfIGJFrdQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c93337ea63bf45258fb1db695bf6f13eef2f9d43" title="SWE-Bench Pro released, targeting dataset contamination" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pristine-Woodpecker"&gt; /u/Pristine-Woodpecker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://scale.com/research/swe_bench_pro"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnmfne/swebench_pro_released_targeting_dataset/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nnmfne/swebench_pro_released_targeting_dataset/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-22T13:25:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1nnfwmo</id>
    <title>Moving from Cursor to Qwen-code</title>
    <updated>2025-09-22T07:20:39+00:00</updated>
    <author>
      <name>/u/Honest-Debate-6863</name>
      <uri>https://old.reddit.com/user/Honest-Debate-6863</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Never been faster &amp;amp; happier, I basically live on terminal. tmux 8 panes +qwen on each with llamacpp qwen3 30b server. Definitely recommend.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Honest-Debate-6863"&gt; /u/Honest-Debate-6863 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnfwmo/moving_from_cursor_to_qwencode/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnfwmo/moving_from_cursor_to_qwencode/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nnfwmo/moving_from_cursor_to_qwencode/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-22T07:20:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1nnefs0</id>
    <title>GLM-4.5V model for local computer use</title>
    <updated>2025-09-22T05:49:19+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnefs0/glm45v_model_for_local_computer_use/"&gt; &lt;img alt="GLM-4.5V model for local computer use" src="https://external-preview.redd.it/MTdjemJhbjlubnFmMbWADQNBSkImjVESNjfi_q43l9ostHKNGAFX_QJdfnS0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6e71ff3c4f0395f4af1a821212361417ff7c59ec" title="GLM-4.5V model for local computer use" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;On OSWorld-V, it scores 35.8% - beating UI-TARS-1.5, matching Claude-3.7-Sonnet-20250219, and setting SOTA for fully open-source computer-use models.&lt;/p&gt; &lt;p&gt;Run it with Cua either: Locally via Hugging Face Remotely via OpenRouter&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/trycua"&gt;https://github.com/trycua&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Docs + examples: &lt;a href="https://docs.trycua.com/docs/agent-sdk/supported-agents/computer-use-agents#glm-45v"&gt;https://docs.trycua.com/docs/agent-sdk/supported-agents/computer-use-agents#glm-45v&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/s7ecb9y9nnqf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnefs0/glm45v_model_for_local_computer_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nnefs0/glm45v_model_for_local_computer_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-22T05:49:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1nn0u8p</id>
    <title>Why is Hugging Face blocked in China when so many open‑weight models are released by Chinese companies?</title>
    <updated>2025-09-21T19:15:37+00:00</updated>
    <author>
      <name>/u/zoxtech</name>
      <uri>https://old.reddit.com/user/zoxtech</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently learned that HF is inaccessible from mainland China. At the same time, a large share of the open‑weight LLMs are published by Chinese firms.&lt;/p&gt; &lt;p&gt;Is this a legal prohibition on publishing Chinese models, or simply a network‑level block that prevents users inside China from reaching the site?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zoxtech"&gt; /u/zoxtech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nn0u8p/why_is_hugging_face_blocked_in_china_when_so_many/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nn0u8p/why_is_hugging_face_blocked_in_china_when_so_many/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nn0u8p/why_is_hugging_face_blocked_in_china_when_so_many/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T19:15:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1nnlylf</id>
    <title>Benchmarked 2x 5090 with vLLM and Gemma-3-12b unquantized</title>
    <updated>2025-09-22T13:06:15+00:00</updated>
    <author>
      <name>/u/somealusta</name>
      <uri>https://old.reddit.com/user/somealusta</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Tested a dual 5090 setup with vLLM and Gemma-3-12b unquantized inference performance.&lt;br /&gt; Goal was to see how much more performance and tokens/s a second GPU gives when the inference engine is better than Ollama or LM-studio. &lt;/p&gt; &lt;p&gt;Test setup&lt;/p&gt; &lt;p&gt;Epyc siena 24core 64GB RAM, 1500W NZXT PSU&lt;/p&gt; &lt;p&gt;2x5090 in pcie 5.0 16X slots Both power limited to 400W&lt;/p&gt; &lt;p&gt;Benchmark command:&lt;/p&gt; &lt;p&gt;&lt;code&gt;python3 benchmark_serving.py --backend vllm --base-url &amp;quot;http://127.0.0.1:8000&amp;quot; --endpoint='/v1/completions' --model google/gemma-3-12b-it --served-model-name vllm/gemma-3 --dataset-name random --num-prompts 200 --max-concurrency 64 --request-rate inf --random-input-len 64 --random-output-len 128&lt;/code&gt;&lt;/p&gt; &lt;p&gt;(I changed the max concurrency and num-prompts values in the below tests.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Summary&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;&lt;/th&gt; &lt;th align="left"&gt;1x 5090 (total tokens/s)&lt;/th&gt; &lt;th align="left"&gt;2x 5090 (total tokens/s)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;1 requests concurrency&lt;/td&gt; &lt;td align="left"&gt;84.10&lt;/td&gt; &lt;td align="left"&gt;117.82&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;64 requests concurrency&lt;/td&gt; &lt;td align="left"&gt;2331.57&lt;/td&gt; &lt;td align="left"&gt;3749.04&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;124 requests concurrency&lt;/td&gt; &lt;td align="left"&gt;2542.67&lt;/td&gt; &lt;td align="left"&gt;4428.10&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;---- tensor-parallel = 2&lt;/strong&gt; (2 cards)&lt;/p&gt; &lt;p&gt;--num-prompts 10 --max-concurrency 1&lt;/p&gt; &lt;pre&gt;&lt;code&gt;============ Serving Benchmark Result ============ Successful requests: 10 Maximum request concurrency: 1 Benchmark duration (s): 13.89 Total input tokens: 630 Total generated tokens: 1006 Request throughput (req/s): 0.72 Output token throughput (tok/s): 72.45 Total Token throughput (tok/s): 117.82 ---------------Time to First Token---------------- Mean TTFT (ms): 20.89 Median TTFT (ms): 20.85 P99 TTFT (ms): 21.31 -----Time per Output Token (excl. 1st token)------ Mean TPOT (ms): 13.77 Median TPOT (ms): 13.72 P99 TPOT (ms): 14.12 ---------------Inter-token Latency---------------- Mean ITL (ms): 13.73 Median ITL (ms): 13.67 P99 ITL (ms): 14.55 ================================================== &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;--num-prompts 200 --max-concurrency 64&lt;/p&gt; &lt;pre&gt;&lt;code&gt;============ Serving Benchmark Result ============ Successful requests: 200 Maximum request concurrency: 64 Benchmark duration (s): 9.32 Total input tokens: 12600 Total generated tokens: 22340 Request throughput (req/s): 21.46 Output token throughput (tok/s): 2397.07 Total Token throughput (tok/s): 3749.04 ---------------Time to First Token---------------- Mean TTFT (ms): 191.26 Median TTFT (ms): 212.97 P99 TTFT (ms): 341.05 -----Time per Output Token (excl. 1st token)------ Mean TPOT (ms): 24.86 Median TPOT (ms): 22.93 P99 TPOT (ms): 53.04 ---------------Inter-token Latency---------------- Mean ITL (ms): 23.04 Median ITL (ms): 22.09 P99 ITL (ms): 47.91 ================================================== &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;--num-prompts 300 --max-concurrency 124&lt;/p&gt; &lt;pre&gt;&lt;code&gt;============ Serving Benchmark Result ============ Successful requests: 300 Maximum request concurrency: 124 Benchmark duration (s): 11.89 Total input tokens: 18898 Total generated tokens: 33750 Request throughput (req/s): 25.23 Output token throughput (tok/s): 2838.63 Total Token throughput (tok/s): 4428.10 ---------------Time to First Token---------------- Mean TTFT (ms): 263.10 Median TTFT (ms): 228.77 P99 TTFT (ms): 554.57 -----Time per Output Token (excl. 1st token)------ Mean TPOT (ms): 37.19 Median TPOT (ms): 34.55 P99 TPOT (ms): 158.76 ---------------Inter-token Latency---------------- Mean ITL (ms): 34.44 Median ITL (ms): 33.23 P99 ITL (ms): 51.66 ================================================== &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;---- tensor-parallel = 1&lt;/strong&gt; (1 card) &lt;/p&gt; &lt;p&gt;--num-prompts 10 --max-concurrency 1&lt;/p&gt; &lt;pre&gt;&lt;code&gt;============ Serving Benchmark Result ============ Successful requests: 10 Maximum request concurrency: 1 Benchmark duration (s): 19.45 Total input tokens: 630 Total generated tokens: 1006 Request throughput (req/s): 0.51 Output token throughput (tok/s): 51.71 Total Token throughput (tok/s): 84.10 ---------------Time to First Token---------------- Mean TTFT (ms): 35.58 Median TTFT (ms): 36.64 P99 TTFT (ms): 37.14 -----Time per Output Token (excl. 1st token)------ Mean TPOT (ms): 19.14 Median TPOT (ms): 19.16 P99 TPOT (ms): 19.23 ---------------Inter-token Latency---------------- Mean ITL (ms): 19.17 Median ITL (ms): 19.17 P99 ITL (ms): 19.46 ================================================== &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;--num-prompts 200 --max-concurrency 64&lt;/p&gt; &lt;pre&gt;&lt;code&gt;============ Serving Benchmark Result ============ Successful requests: 200 Maximum request concurrency: 64 Benchmark duration (s): 15.00 Total input tokens: 12600 Total generated tokens: 22366 Request throughput (req/s): 13.34 Output token throughput (tok/s): 1491.39 Total Token throughput (tok/s): 2331.57 ---------------Time to First Token---------------- Mean TTFT (ms): 332.08 Median TTFT (ms): 330.50 P99 TTFT (ms): 549.43 -----Time per Output Token (excl. 1st token)------ Mean TPOT (ms): 40.50 Median TPOT (ms): 36.66 P99 TPOT (ms): 139.68 ---------------Inter-token Latency---------------- Mean ITL (ms): 36.96 Median ITL (ms): 35.48 P99 ITL (ms): 64.42 ================================================== &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;--num-prompts 300 --max-concurrency 124&lt;/p&gt; &lt;pre&gt;&lt;code&gt;============ Serving Benchmark Result ============ Successful requests: 300 Maximum request concurrency: 124 Benchmark duration (s): 20.74 Total input tokens: 18898 Total generated tokens: 33842 Request throughput (req/s): 14.46 Output token throughput (tok/s): 1631.57 Total Token throughput (tok/s): 2542.67 ---------------Time to First Token---------------- Mean TTFT (ms): 1398.51 Median TTFT (ms): 1012.84 P99 TTFT (ms): 4301.30 -----Time per Output Token (excl. 1st token)------ Mean TPOT (ms): 57.72 Median TPOT (ms): 49.13 P99 TPOT (ms): 251.44 ---------------Inter-token Latency---------------- Mean ITL (ms): 52.97 Median ITL (ms): 35.83 P99 ITL (ms): 256.72 ================================================== &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/somealusta"&gt; /u/somealusta &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnlylf/benchmarked_2x_5090_with_vllm_and_gemma312b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnlylf/benchmarked_2x_5090_with_vllm_and_gemma312b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nnlylf/benchmarked_2x_5090_with_vllm_and_gemma312b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-22T13:06:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1nnmjt0</id>
    <title>DeepSeek-V3.1-Terminus</title>
    <updated>2025-09-22T13:30:32+00:00</updated>
    <author>
      <name>/u/Xhehab_</name>
      <uri>https://old.reddit.com/user/Xhehab_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnmjt0/deepseekv31terminus/"&gt; &lt;img alt="DeepSeek-V3.1-Terminus" src="https://preview.redd.it/ih6z5vljxpqf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=15bc33beee1daca428fa6b9ed4d7c64f51d360b6" title="DeepSeek-V3.1-Terminus" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.1-Terminus"&gt;https://huggingface.co/deepseek-ai/DeepSeek-V3.1-Terminus&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xhehab_"&gt; /u/Xhehab_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ih6z5vljxpqf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnmjt0/deepseekv31terminus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nnmjt0/deepseekv31terminus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-22T13:30:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1nncyvv</id>
    <title>baidu releases Qianfan-VL 70B/8B/3B</title>
    <updated>2025-09-22T04:23:58+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/baidu/Qianfan-VL-8B"&gt;https://huggingface.co/baidu/Qianfan-VL-8B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/baidu/Qianfan-VL-70B"&gt;https://huggingface.co/baidu/Qianfan-VL-70B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/baidu/Qianfan-VL-3B"&gt;https://huggingface.co/baidu/Qianfan-VL-3B&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Model Description&lt;/h1&gt; &lt;p&gt;Qianfan-VL is a series of general-purpose multimodal large language models enhanced for enterprise-level multimodal applications. The models offer deep optimization for high-frequency scenarios in industrial deployment while maintaining strong general capabilities.&lt;/p&gt; &lt;h1&gt;Model Variants&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Parameters&lt;/th&gt; &lt;th align="left"&gt;Context Length&lt;/th&gt; &lt;th align="left"&gt;CoT Support&lt;/th&gt; &lt;th align="left"&gt;Best For&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Qianfan-VL-3B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;3B&lt;/td&gt; &lt;td align="left"&gt;32k&lt;/td&gt; &lt;td align="left"&gt;❌&lt;/td&gt; &lt;td align="left"&gt;Edge deployment, real-time OCR&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Qianfan-VL-8B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;8B&lt;/td&gt; &lt;td align="left"&gt;32k&lt;/td&gt; &lt;td align="left"&gt;✅&lt;/td&gt; &lt;td align="left"&gt;Server-side general scenarios, fine-tuning&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Qianfan-VL-70B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;70B&lt;/td&gt; &lt;td align="left"&gt;32k&lt;/td&gt; &lt;td align="left"&gt;✅&lt;/td&gt; &lt;td align="left"&gt;Complex reasoning, data synthesis&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Architecture&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Language Model&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;Qianfan-VL-3B: Based on Qwen2.5-3B&lt;/li&gt; &lt;li&gt;Qianfan-VL-8B/70B: Based on Llama 3.1 architecture&lt;/li&gt; &lt;li&gt;Enhanced with 3T multilingual corpus&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Vision Encoder&lt;/strong&gt;: InternViT-based, supports dynamic patching up to 4K resolution&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cross-modal Fusion&lt;/strong&gt;: MLP adapter for efficient vision-language bridging&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Key Capabilities&lt;/h1&gt; &lt;h1&gt;🔍 OCR &amp;amp; Document Understanding&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Full-Scenario OCR&lt;/strong&gt;: Handwriting, formulas, natural scenes, cards/documents&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Document Intelligence&lt;/strong&gt;: Layout analysis, table parsing, chart understanding, document Q&amp;amp;A&lt;/li&gt; &lt;li&gt;&lt;strong&gt;High Precision&lt;/strong&gt;: Industry-leading performance on OCR benchmarks&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;🧮 Chain-of-Thought Reasoning (8B &amp;amp; 70B)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Complex chart analysis and reasoning&lt;/li&gt; &lt;li&gt;Mathematical problem-solving with step-by-step derivation&lt;/li&gt; &lt;li&gt;Visual reasoning and logical inference&lt;/li&gt; &lt;li&gt;Statistical computation and trend prediction&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nncyvv/baidu_releases_qianfanvl_70b8b3b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nncyvv/baidu_releases_qianfanvl_70b8b3b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nncyvv/baidu_releases_qianfanvl_70b8b3b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-22T04:23:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1nnj83s</id>
    <title>Magistral Small 2509 - Jinja Template Modification (Based on Unsloth's) - No thinking by default - straight quick answers in Mistral Small 3.2 style and quality~, need thinking? simple activation with "/think" command anywhere in the system prompt.</title>
    <updated>2025-09-22T10:52:12+00:00</updated>
    <author>
      <name>/u/-Ellary-</name>
      <uri>https://old.reddit.com/user/-Ellary-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnj83s/magistral_small_2509_jinja_template_modification/"&gt; &lt;img alt="Magistral Small 2509 - Jinja Template Modification (Based on Unsloth's) - No thinking by default - straight quick answers in Mistral Small 3.2 style and quality~, need thinking? simple activation with &amp;quot;/think&amp;quot; command anywhere in the system prompt." src="https://b.thumbs.redditmedia.com/reer8ADbaCCmPSvEFLw22uSl2QwyNkai_YlopFuEYLc.jpg" title="Magistral Small 2509 - Jinja Template Modification (Based on Unsloth's) - No thinking by default - straight quick answers in Mistral Small 3.2 style and quality~, need thinking? simple activation with &amp;quot;/think&amp;quot; command anywhere in the system prompt." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-Ellary-"&gt; /u/-Ellary- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nnj83s"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnj83s/magistral_small_2509_jinja_template_modification/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nnj83s/magistral_small_2509_jinja_template_modification/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-22T10:52:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1nnmbkz</id>
    <title>deepseek-ai/DeepSeek-V3.1-Terminus · Hugging Face</title>
    <updated>2025-09-22T13:21:04+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnmbkz/deepseekaideepseekv31terminus_hugging_face/"&gt; &lt;img alt="deepseek-ai/DeepSeek-V3.1-Terminus · Hugging Face" src="https://external-preview.redd.it/lElvQgBFLJ25eAKrUxM2O_G26a4lbsVKHgY4b4Y5JIc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1eefab6fb85acd29ad52251c7b36d0ae23c1ac6a" title="deepseek-ai/DeepSeek-V3.1-Terminus · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.1-Terminus"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnmbkz/deepseekaideepseekv31terminus_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nnmbkz/deepseekaideepseekv31terminus_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-22T13:21:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1nncssq</id>
    <title>Qwen3-Omni Promotional Video</title>
    <updated>2025-09-22T04:14:46+00:00</updated>
    <author>
      <name>/u/Mysterious_Finish543</name>
      <uri>https://old.reddit.com/user/Mysterious_Finish543</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=RRlAen2kIUU"&gt;https://www.youtube.com/watch?v=RRlAen2kIUU&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Qwen dropped a promotional video for Qwen3-Omni, looks like the weights are just around the corner!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mysterious_Finish543"&gt; /u/Mysterious_Finish543 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nncssq/qwen3omni_promotional_video/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nncssq/qwen3omni_promotional_video/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nncssq/qwen3omni_promotional_video/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-22T04:14:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1nnb8sq</id>
    <title>I'll show you mine, if you show me yours: Local AI tech stack September 2025</title>
    <updated>2025-09-22T02:53:16+00:00</updated>
    <author>
      <name>/u/JLeonsarmiento</name>
      <uri>https://old.reddit.com/user/JLeonsarmiento</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnb8sq/ill_show_you_mine_if_you_show_me_yours_local_ai/"&gt; &lt;img alt="I'll show you mine, if you show me yours: Local AI tech stack September 2025" src="https://preview.redd.it/rq2ple7trmqf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0ba6a2e65b89f81d77c39c353119c9e596157a9b" title="I'll show you mine, if you show me yours: Local AI tech stack September 2025" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JLeonsarmiento"&gt; /u/JLeonsarmiento &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rq2ple7trmqf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnb8sq/ill_show_you_mine_if_you_show_me_yours_local_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nnb8sq/ill_show_you_mine_if_you_show_me_yours_local_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-22T02:53:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1nnhlx5</id>
    <title>Official FP8-quantizion of Qwen3-Next-80B-A3B</title>
    <updated>2025-09-22T09:14:06+00:00</updated>
    <author>
      <name>/u/touhidul002</name>
      <uri>https://old.reddit.com/user/touhidul002</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Thinking-FP8"&gt;https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Thinking-FP8&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/touhidul002"&gt; /u/touhidul002 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnhlx5/official_fp8quantizion_of_qwen3next80ba3b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnhlx5/official_fp8quantizion_of_qwen3next80ba3b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nnhlx5/official_fp8quantizion_of_qwen3next80ba3b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-22T09:14:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1nnnws0</id>
    <title>Qwen 😁</title>
    <updated>2025-09-22T14:25:11+00:00</updated>
    <author>
      <name>/u/Namra_7</name>
      <uri>https://old.reddit.com/user/Namra_7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnnws0/qwen/"&gt; &lt;img alt="Qwen 😁" src="https://preview.redd.it/milakcbb7qqf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7af57edddeef91bdc75a874fb95e8ac60d2746ae" title="Qwen 😁" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Namra_7"&gt; /u/Namra_7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/milakcbb7qqf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnnws0/qwen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nnnws0/qwen/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-22T14:25:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1nnki20</id>
    <title>The DeepSeek online model has been upgraded</title>
    <updated>2025-09-22T11:59:55+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnki20/the_deepseek_online_model_has_been_upgraded/"&gt; &lt;img alt="The DeepSeek online model has been upgraded" src="https://external-preview.redd.it/fj84M-Z0L_zDI8VjgLPR-vGFwXVTTqVZFoa_h5offPs.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bf8b3f8dce31098b2bdb03126d4f6c603326511a" title="The DeepSeek online model has been upgraded" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/wk15my1bhpqf1.jpg?width=1440&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=0e007554233eee03a311b2657d714af9da13f353"&gt;https://preview.redd.it/wk15my1bhpqf1.jpg?width=1440&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=0e007554233eee03a311b2657d714af9da13f353&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The DeepSeek online model has been upgraded. The current version number is DeepSeek-V3.1-Terminus. Everyone is welcome to test it and report any issues~&lt;/p&gt; &lt;p&gt;edit:&lt;/p&gt; &lt;p&gt;&lt;a href="https://api-docs.deepseek.com/updates#deepseek-v31-terminus"&gt;https://api-docs.deepseek.com/updates#deepseek-v31-terminus&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This update maintains the model's original capabilities while addressing issues reported by users, including:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Language consistency: Reduced occurrences of Chinese-English mixing and occasional abnormal characters;&lt;/li&gt; &lt;li&gt;Agent capabilities: Further optimized the performance of the Code Agent and Search Agent.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnki20/the_deepseek_online_model_has_been_upgraded/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnki20/the_deepseek_online_model_has_been_upgraded/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nnki20/the_deepseek_online_model_has_been_upgraded/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-22T11:59:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1nnj67v</id>
    <title>too many qwens</title>
    <updated>2025-09-22T10:49:14+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnj67v/too_many_qwens/"&gt; &lt;img alt="too many qwens" src="https://preview.redd.it/z6ehmb4r4pqf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ac3c6bfbc9c3ca1495fb8ed54680114fd1e007ff" title="too many qwens" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/z6ehmb4r4pqf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnj67v/too_many_qwens/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nnj67v/too_many_qwens/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-22T10:49:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1nnmhai</id>
    <title>🚀 DeepSeek released DeepSeek-V3.1-Terminus</title>
    <updated>2025-09-22T13:27:35+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnmhai/deepseek_released_deepseekv31terminus/"&gt; &lt;img alt="🚀 DeepSeek released DeepSeek-V3.1-Terminus" src="https://preview.redd.it/729mf2l1xpqf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4e9ce1e335c2a491a3d422d6f4d70b33dbf6a25f" title="🚀 DeepSeek released DeepSeek-V3.1-Terminus" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;🚀 DeepSeek-V3.1 → DeepSeek-V3.1-Terminus The latest update builds on V3.1’s strengths while addressing key user feedback.&lt;/p&gt; &lt;p&gt;✨ What’s improved?&lt;/p&gt; &lt;p&gt;🌐 Language consistency: fewer CN/EN mix-ups &amp;amp; no more random chars.&lt;/p&gt; &lt;p&gt;🤖 Agent upgrades: stronger Code Agent &amp;amp; Search Agent performance.&lt;/p&gt; &lt;p&gt;📊 DeepSeek-V3.1-Terminus delivers more stable &amp;amp; reliable outputs across benchmarks compared to the previous version.&lt;/p&gt; &lt;p&gt;👉 Available now on: App / Web / API 🔗 Open-source weights here: &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.1-Terminus"&gt;https://huggingface.co/deepseek-ai/DeepSeek-V3.1-Terminus&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks to everyone for your feedback. It drives us to keep improving and refining the experience! 🚀&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/729mf2l1xpqf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnmhai/deepseek_released_deepseekv31terminus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nnmhai/deepseek_released_deepseekv31terminus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-22T13:27:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1njjz7j</id>
    <title>Our 4th AMA: The LMStudio Team! (Thursday, 11 AM-1 PM PDT)</title>
    <updated>2025-09-17T17:44:17+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njjz7j/our_4th_ama_the_lmstudio_team_thursday_11_am1_pm/"&gt; &lt;img alt="Our 4th AMA: The LMStudio Team! (Thursday, 11 AM-1 PM PDT)" src="https://preview.redd.it/4xt9enbairpf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1cc1ac16a9a2b96134934fcb2f81a9f3d4916b31" title="Our 4th AMA: The LMStudio Team! (Thursday, 11 AM-1 PM PDT)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4xt9enbairpf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njjz7j/our_4th_ama_the_lmstudio_team_thursday_11_am1_pm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1njjz7j/our_4th_ama_the_lmstudio_team_thursday_11_am1_pm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T17:44:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1nkft9l</id>
    <title>AMA with the LM Studio team</title>
    <updated>2025-09-18T18:12:24+00:00</updated>
    <author>
      <name>/u/yags-lms</name>
      <uri>https://old.reddit.com/user/yags-lms</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;! We're excited for this AMA. Thank you for having us here today. We got a full house from the LM Studio team: &lt;/p&gt; &lt;p&gt;- Yags &lt;a href="https://t.co/ERfA4NrR96"&gt;https://reddit.com/user/yags-lms/&lt;/a&gt; (founder)&lt;br /&gt; - Neil &lt;a href="https://t.co/KyiHVfv0QG"&gt;https://reddit.com/user/neilmehta24/&lt;/a&gt; (LLM engines and runtime)&lt;br /&gt; - Will &lt;a href="https://t.co/IjAZJL2JMK"&gt;https://reddit.com/user/will-lms/&lt;/a&gt; (LLM engines and runtime)&lt;br /&gt; - Matt &lt;a href="https://t.co/6MNkItPYnI"&gt;https://reddit.com/user/matt-lms/&lt;/a&gt; (LLM engines, runtime, and APIs)&lt;br /&gt; - Ryan &lt;a href="https://t.co/0snuNUPizo"&gt;https://reddit.com/user/ryan-lms/&lt;/a&gt; (Core system and APIs)&lt;br /&gt; - Rugved &lt;a href="https://t.co/xGtYHsJZI3"&gt;https://reddit.com/user/rugved_lms/&lt;/a&gt; (CLI and SDKs)&lt;br /&gt; - Alex &lt;a href="https://t.co/wtT2IFf0z6"&gt;https://reddit.com/user/alex-lms/&lt;/a&gt; (App)&lt;br /&gt; - Julian &lt;a href="https://www.reddit.com/user/julian-lms/"&gt;https://www.reddit.com/user/julian-lms/&lt;/a&gt; (Ops) &lt;/p&gt; &lt;p&gt;Excited to chat about: the latest local models, UX for local models, steering local models effectively, LM Studio SDK and APIs, how we support multiple LLM engines (llama.cpp, MLX, and more), privacy philosophy, why local AI matters, our open source projects (mlx-engine, lms, lmstudio-js, lmstudio-python, venvstacks), why ggerganov and Awni are the GOATs, where is TheBloke, and more. &lt;/p&gt; &lt;p&gt;Would love to hear about people's setup, which models you use, use cases that really work, how you got into local AI, what needs to improve in LM Studio and the ecosystem as a whole, how you use LM Studio, and anything in between!&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Everyone: it was awesome to see your questions here today and share replies! Thanks a lot for the welcoming AMA. We will continue to monitor this post for more questions over the next couple of days, but for now we're signing off to continue building 🔨&lt;/p&gt; &lt;p&gt;We have several marquee features we've been working on for a loong time coming out later this month that we hope you'll love and find lots of value in. And don't worry, UI for n cpu moe is on the way too :)&lt;/p&gt; &lt;p&gt;Special shoutout and thanks to ggerganov, Awni Hannun, TheBloke, Hugging Face, and all the rest of the open source AI community!&lt;/p&gt; &lt;p&gt;Thank you and see you around! - Team LM Studio 👾&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yags-lms"&gt; /u/yags-lms &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkft9l/ama_with_the_lm_studio_team/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkft9l/ama_with_the_lm_studio_team/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nkft9l/ama_with_the_lm_studio_team/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T18:12:24+00:00</published>
  </entry>
</feed>
