<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-21T09:27:14+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1rakjyx</id>
    <title>implemented a pipeline by gepa that helps your ai agent perform way better</title>
    <updated>2026-02-21T07:05:26+00:00</updated>
    <author>
      <name>/u/purealgo</name>
      <uri>https://old.reddit.com/user/purealgo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built an open source project based on gskill, a pipeline from the team behind GEPA. It takes any github repository and generates a `.claude/skills/{repo-name}/SKILL.md` file with optimized, repo-specific instructions that significantly improve an agent‚Äôs task performance. You can easily use the resulting skill file with Claude Code, Codex and other ai agents. In the blog post, gskill improved resolve rate from 24% to 93% on some repositories and completed tasks up to 47% faster. In theory, with this strategy, smaller open weight models can perform much closer to the level of sota models.&lt;/p&gt; &lt;p&gt;Try it out and feel free to contribute!&lt;/p&gt; &lt;p&gt;blog post: &lt;a href="https://gepa-ai.github.io/gepa/blog/2026/02/18/automatically-learning-skills-for-coding-agents/"&gt;https://gepa-ai.github.io/gepa/blog/2026/02/18/automatically-learning-skills-for-coding-agents/&lt;/a&gt;&lt;br /&gt; repo: &lt;a href="https://github.com/itsmostafa/gskill"&gt;https://github.com/itsmostafa/gskill&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/purealgo"&gt; /u/purealgo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rakjyx/implemented_a_pipeline_by_gepa_that_helps_your_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rakjyx/implemented_a_pipeline_by_gepa_that_helps_your_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rakjyx/implemented_a_pipeline_by_gepa_that_helps_your_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T07:05:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1ra794c</id>
    <title>Introducing a new benchmark to answer the only important question: how good are LLMs at Age of Empires 2 build orders?</title>
    <updated>2026-02-20T20:56:09+00:00</updated>
    <author>
      <name>/u/wraitii_</name>
      <uri>https://old.reddit.com/user/wraitii_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Built a simulator to craft Age of Empires 2 build orders over the past few days with a custom DSL. Then used it to create a simple LLM benchmark that isn't saturated yet.&lt;br /&gt; Models are scored on their ability to reach castle age &amp;amp; make 10 archers.&lt;/p&gt; &lt;p&gt;I think it's a pretty good benchmark at this particular point in time - there's clear separation, it's not obviously benchmaxxed by any model, and it's easy to extend and make harder in the future while also not being a &lt;em&gt;complete&lt;/em&gt; toy problem... And it's technically coding !&lt;/p&gt; &lt;p&gt;Results at &lt;a href="https://wraitii.github.io/build-order-workbench/aoe2-llm-benchmarks.html"&gt;https://wraitii.github.io/build-order-workbench/aoe2-llm-benchmarks.html&lt;/a&gt;, will potentially move it to a real website if there's interest !&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wraitii_"&gt; /u/wraitii_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ra794c/introducing_a_new_benchmark_to_answer_the_only/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ra794c/introducing_a_new_benchmark_to_answer_the_only/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ra794c/introducing_a_new_benchmark_to_answer_the_only/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T20:56:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9x0l2</id>
    <title>We replaced the LLM in a voice assistant with a fine-tuned 0.6B model. 90.9% tool call accuracy vs. 87.5% for the 120B teacher. ~40ms inference.</title>
    <updated>2026-02-20T14:37:00+00:00</updated>
    <author>
      <name>/u/party-horse</name>
      <uri>https://old.reddit.com/user/party-horse</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9x0l2/we_replaced_the_llm_in_a_voice_assistant_with_a/"&gt; &lt;img alt="We replaced the LLM in a voice assistant with a fine-tuned 0.6B model. 90.9% tool call accuracy vs. 87.5% for the 120B teacher. ~40ms inference." src="https://preview.redd.it/lh8p2xv0vnkg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fbabc4097ccac2e5d448a628acbee7068bf698ee" title="We replaced the LLM in a voice assistant with a fine-tuned 0.6B model. 90.9% tool call accuracy vs. 87.5% for the 120B teacher. ~40ms inference." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Voice assistants almost always use a cloud LLM for the &amp;quot;brain&amp;quot; stage (intent routing, slot extraction, dialogue state). The LLM stage alone adds 375-750ms per turn, which pushes total pipeline latency past the 500-800ms threshold where conversations feel natural.&lt;/p&gt; &lt;p&gt;For bounded workflows like banking, insurance, or telecom, that's a lot of unnecessary overhead. The task is not open-ended generation -- it's classifying intent and extracting structured slots from what the user said. That's exactly where fine-tuned SLMs shine.&lt;/p&gt; &lt;p&gt;We built VoiceTeller, a banking voice assistant that swaps the LLM for a locally-running fine-tuned Qwen3-0.6B. Numbers:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;Params&lt;/th&gt; &lt;th&gt;Single-Turn Tool Call Accuracy&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;GPT-oss-120B (teacher)&lt;/td&gt; &lt;td&gt;120B&lt;/td&gt; &lt;td&gt;87.5%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3-0.6B (fine-tuned)&lt;/td&gt; &lt;td&gt;0.6B&lt;/td&gt; &lt;td&gt;&lt;strong&gt;90.9%&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3-0.6B (base)&lt;/td&gt; &lt;td&gt;0.6B&lt;/td&gt; &lt;td&gt;48.7%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;And the pipeline latency breakdown:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Stage&lt;/th&gt; &lt;th&gt;Cloud LLM&lt;/th&gt; &lt;th&gt;SLM&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;ASR&lt;/td&gt; &lt;td&gt;200-350ms&lt;/td&gt; &lt;td&gt;~200ms&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Brain&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;375-750ms&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;~40ms&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;TTS&lt;/td&gt; &lt;td&gt;75-150ms&lt;/td&gt; &lt;td&gt;~75ms&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Total&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;680-1300ms&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;~315ms&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;The fine-tuned model beats the 120B teacher by ~3 points while being 200x smaller. The base model at 48.7% is unusable -- over a 3-turn conversation that compounds to about 11.6% success rate.&lt;/p&gt; &lt;p&gt;Architecture note: the SLM never generates user-facing text. It only outputs structured JSON (function name + slots). A deterministic orchestrator handles slot elicitation and response templates. This keeps latency bounded and responses well-formed regardless of what the model outputs.&lt;/p&gt; &lt;p&gt;The whole thing runs locally: Qwen3-ASR-0.6B for speech-to-text, the fine-tuned Qwen3-0.6B via llama.cpp for intent routing, Qwen3-TTS for speech synthesis. Full pipeline on Apple Silicon with MPS.&lt;/p&gt; &lt;p&gt;GitHub (code + training data + pre-trained GGUF): &lt;a href="https://github.com/distil-labs/distil-voice-assistant-banking"&gt;https://github.com/distil-labs/distil-voice-assistant-banking&lt;/a&gt;&lt;/p&gt; &lt;p&gt;HuggingFace model: &lt;a href="https://huggingface.co/distil-labs/distil-qwen3-0.6b-voice-assistant-banking"&gt;https://huggingface.co/distil-labs/distil-qwen3-0.6b-voice-assistant-banking&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog post with the full write-up: &lt;a href="https://www.distillabs.ai/blog/the-llm-in-your-voice-assistant-is-the-bottleneck-replace-it-with-an-slm"&gt;https://www.distillabs.ai/blog/the-llm-in-your-voice-assistant-is-the-bottleneck-replace-it-with-an-slm&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer questions about the training setup, the multi-turn tool calling format, or why the student beats the teacher.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/party-horse"&gt; /u/party-horse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lh8p2xv0vnkg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9x0l2/we_replaced_the_llm_in_a_voice_assistant_with_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9x0l2/we_replaced_the_llm_in_a_voice_assistant_with_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T14:37:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9tdvr</id>
    <title>Kimi K2.5 better than Opus 4.6 on hallucination benchmark in pharmaceutical domain</title>
    <updated>2026-02-20T11:54:25+00:00</updated>
    <author>
      <name>/u/aiprod</name>
      <uri>https://old.reddit.com/user/aiprod</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9tdvr/kimi_k25_better_than_opus_46_on_hallucination/"&gt; &lt;img alt="Kimi K2.5 better than Opus 4.6 on hallucination benchmark in pharmaceutical domain" src="https://preview.redd.it/c1z228f22nkg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=57a4ecba13b26df8634c1b123271ef9c3a609c4f" title="Kimi K2.5 better than Opus 4.6 on hallucination benchmark in pharmaceutical domain" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know the benchmark is mostly commercial models but Kimi K2.5 was part of it and I was actually surprised how well it did against its commercial counterparts.&lt;/p&gt; &lt;p&gt;The benchmark test 7 recent models for hallucinations on a realistic use case and data from the pharmaceutical domain.&lt;/p&gt; &lt;p&gt;Surprisingly, Opus 4.6 has the highest hallucination rate.&lt;/p&gt; &lt;p&gt;I labeled a good chunk of the data and from my impressions, it just invented clinical protocols or tests that weren‚Äôt in the source data (probably trying to be helpful).&lt;/p&gt; &lt;p&gt;Kimi K2.5 did much better (albeit still not great).&lt;/p&gt; &lt;p&gt;You can read the full benchmark here: &lt;a href="https://www.blueguardrails.com/en/blog/placebo-bench-an-llm-hallucination-benchmark-for-pharma"&gt;https://www.blueguardrails.com/en/blog/placebo-bench-an-llm-hallucination-benchmark-for-pharma&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Dataset is also available on hugging face.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aiprod"&gt; /u/aiprod &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/c1z228f22nkg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9tdvr/kimi_k25_better_than_opus_46_on_hallucination/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9tdvr/kimi_k25_better_than_opus_46_on_hallucination/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T11:54:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1rafggf</id>
    <title>Free open-source prompt compression engine ‚Äî pure text processing, no AI calls, works with any model</title>
    <updated>2026-02-21T02:40:41+00:00</updated>
    <author>
      <name>/u/bytesizei3</name>
      <uri>https://old.reddit.com/user/bytesizei3</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Built TokenShrink ‚Äî compresses prompts before you send them to any LLM. Pure text processing, no model calls in the loop. &lt;/p&gt; &lt;p&gt;How it works:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Removes verbose filler (&amp;quot;in order to&amp;quot; ‚Üí &amp;quot;to&amp;quot;, &amp;quot;due to the fact that&amp;quot; ‚Üí &amp;quot;because&amp;quot;)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Abbreviates common words (&amp;quot;function&amp;quot; ‚Üí &amp;quot;fn&amp;quot;, &amp;quot;database&amp;quot; ‚Üí &amp;quot;db&amp;quot;)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Detects repeated phrases and collapses them&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Prepends a tiny [DECODE] header so the model understands&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Stress tested up to 10K words:&lt;/p&gt; &lt;p&gt;| Size | Ratio | Tokens Saved | Time |&lt;/p&gt; &lt;p&gt;|---|---|---|---|&lt;/p&gt; &lt;p&gt;| 500 words | 1.1x | 77 | 4ms |&lt;/p&gt; &lt;p&gt;| 1,000 words | 1.2x | 259 | 4ms |&lt;/p&gt; &lt;p&gt;| 5,000 words | 1.4x | 1,775 | 10ms |&lt;/p&gt; &lt;p&gt;| 10,000 words | 1.4x | 3,679 | 18ms |&lt;/p&gt; &lt;p&gt;Especially useful if you're running local models with limited context windows ‚Äî every token counts when you're on 4K or 8K ctx.&lt;/p&gt; &lt;p&gt;Has domain-specific dictionaries for code, medical, legal, and business prompts. Auto-detects which to use.&lt;/p&gt; &lt;p&gt;Web UI: &lt;a href="https://tokenshrink.com"&gt;https://tokenshrink.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/chatde/tokenshrink"&gt;https://github.com/chatde/tokenshrink&lt;/a&gt; (MIT, 29 unit tests)&lt;/p&gt; &lt;p&gt;API: POST &lt;a href="https://tokenshrink.com/api/compress"&gt;https://tokenshrink.com/api/compress&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Free forever. No tracking, no signup, client-side processing.&lt;/p&gt; &lt;p&gt;Curious if anyone has tested compression like this with smaller models ‚Äî does the [DECODE] header confuse 3B/7B models or do they handle it fine?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bytesizei3"&gt; /u/bytesizei3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rafggf/free_opensource_prompt_compression_engine_pure/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rafggf/free_opensource_prompt_compression_engine_pure/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rafggf/free_opensource_prompt_compression_engine_pure/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T02:40:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1ra1wxm</id>
    <title>I got 45-46 tok/s on IPhone 14 Pro Max using BitNet</title>
    <updated>2026-02-20T17:37:38+00:00</updated>
    <author>
      <name>/u/Middle-Hurry4718</name>
      <uri>https://old.reddit.com/user/Middle-Hurry4718</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ra1wxm/i_got_4546_toks_on_iphone_14_pro_max_using_bitnet/"&gt; &lt;img alt="I got 45-46 tok/s on IPhone 14 Pro Max using BitNet" src="https://external-preview.redd.it/MnpoZng3cWFyb2tnMag_nQlaOiUb75GBHB5vo6hyb1PC6uSB2BeZWzIId6Ao.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=56ac4a23fdd3e309d449ba68f654ccf3d70eee7b" title="I got 45-46 tok/s on IPhone 14 Pro Max using BitNet" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I ported Microsoft‚Äôs BitNet to iOS. Getting 45 tok/s on iPhone 14 Pro Max with the 0.7B model, ~200MB memory. BitNet uses 1-bit weights (-1, 0, +1) instead of 16-bit floats so the model is tiny and runs fast. The ARM NEON kernels already worked on M-series Macs so getting it on iPhone was mostly build system wrangling. I am currently running a base model (outputs are nonsense), next step is the instruction-tuned 2B model for actual usable chat. I will open source eventually, but sooner rather than later if there‚Äôs interest.‚Äã‚Äã‚Äã‚Äã‚Äã&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Middle-Hurry4718"&gt; /u/Middle-Hurry4718 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/whlo0jrarokg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ra1wxm/i_got_4546_toks_on_iphone_14_pro_max_using_bitnet/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ra1wxm/i_got_4546_toks_on_iphone_14_pro_max_using_bitnet/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T17:37:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9uu5h</id>
    <title>Qwen3 Coder Next on 8GB VRAM</title>
    <updated>2026-02-20T13:05:21+00:00</updated>
    <author>
      <name>/u/Juan_Valadez</name>
      <uri>https://old.reddit.com/user/Juan_Valadez</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi!&lt;/p&gt; &lt;p&gt;I have a PC with 64 GB of RAM and an RTX 3060 12 GB, and I'm running Qwen3 Coder Next in MXFP4 with 131,072 context tokens.&lt;/p&gt; &lt;p&gt;I get a sustained speed of around 23 t/s throughout the entire conversation.&lt;/p&gt; &lt;p&gt;I mainly use it for front-end and back-end web development, and it works perfectly.&lt;/p&gt; &lt;p&gt;I've stopped paying for my Claude Max plan ($100 USD per month) to use only Claude Code with the following configuration:&lt;/p&gt; &lt;p&gt;&lt;code&gt;set GGML_CUDA_GRAPH_OPT=1&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;llama-server -m ../GGUF/qwen3-coder-next-mxfp4.gguf -ngl 999 -sm none -mg 0 -t 12 -fa on -cmoe -c 131072 -b 512 -ub 512 -np 1 --jinja --temp 1.0 --top-p 0.95 --top-k 40 --min-p 0.01 --repeat-penalty 1.0 --host&lt;/code&gt; &lt;a href="http://0.0.0.0"&gt;&lt;code&gt;0.0.0.0&lt;/code&gt;&lt;/a&gt; &lt;code&gt;--port 8080&lt;/code&gt;&lt;/p&gt; &lt;p&gt;I promise you it works fast enough and with incredible quality to work with complete SaaS applications (I know how to program, obviously, but I'm delegating practically everything to AI).&lt;/p&gt; &lt;p&gt;If you have at least 64 GB of RAM and 8 GB of VRAM, I recommend giving it a try; you won't regret it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Juan_Valadez"&gt; /u/Juan_Valadez &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9uu5h/qwen3_coder_next_on_8gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9uu5h/qwen3_coder_next_on_8gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9uu5h/qwen3_coder_next_on_8gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T13:05:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1rabo34</id>
    <title>Local TTS server with voice cloning + near-realtime streaming replies (ElevenLabs alternative)</title>
    <updated>2026-02-20T23:50:26+00:00</updated>
    <author>
      <name>/u/RIP26770</name>
      <uri>https://old.reddit.com/user/RIP26770</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rabo34/local_tts_server_with_voice_cloning_nearrealtime/"&gt; &lt;img alt="Local TTS server with voice cloning + near-realtime streaming replies (ElevenLabs alternative)" src="https://preview.redd.it/heh81bnslqkg1.png?width=140&amp;amp;height=69&amp;amp;auto=webp&amp;amp;s=e8760daf2610cb9721ded68d56da0aa9a90586c4" title="Local TTS server with voice cloning + near-realtime streaming replies (ElevenLabs alternative)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Built a small local-first TTS server with voice cloning and streaming audio output so your LLM can reply back in a cloned voice almost in realtime.&lt;/p&gt; &lt;p&gt;Main reason: I wanted something that could replace ElevenLabs in a fully local stack without API costs or external dependencies.&lt;/p&gt; &lt;p&gt;Works well alongside llama.cpp / OpenAI-compatible endpoints and plugs cleanly into voice bots (I‚Äôm using it for Telegram voice replies).&lt;/p&gt; &lt;p&gt;Goals were simple:&lt;/p&gt; &lt;p&gt;-fully local -streaming audio output -voice cloning -lightweight + clean API -easy integration &lt;a href="https://github.com/ai-joe-git/pocket-tts-server"&gt;Pocket-TTS-Server&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Already running it daily for voice-first bots.&lt;/p&gt; &lt;p&gt;Curious if anyone else here is building similar pipelines.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RIP26770"&gt; /u/RIP26770 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1rabo34"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rabo34/local_tts_server_with_voice_cloning_nearrealtime/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rabo34/local_tts_server_with_voice_cloning_nearrealtime/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T23:50:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9y6s8</id>
    <title>TranscriptionSuite - A fully local, private &amp; open source audio transcription for Linux, Windows &amp; macOS</title>
    <updated>2026-02-20T15:22:24+00:00</updated>
    <author>
      <name>/u/TwilightEncoder</name>
      <uri>https://old.reddit.com/user/TwilightEncoder</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9y6s8/transcriptionsuite_a_fully_local_private_open/"&gt; &lt;img alt="TranscriptionSuite - A fully local, private &amp;amp; open source audio transcription for Linux, Windows &amp;amp; macOS" src="https://external-preview.redd.it/ZjVodnR2dGoyb2tnMfrHn1-Z1IlbM1M-CdvVLf1S0fx3BvVT39BjZwD6xxr6.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8c01a41fb0c91487d97d9e6bbd7ba58c3750d09f" title="TranscriptionSuite - A fully local, private &amp;amp; open source audio transcription for Linux, Windows &amp;amp; macOS" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! This is a short presentation for my hobby project, &lt;a href="https://github.com/homelab-00/TranscriptionSuite"&gt;TranscriptionSuite&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt; A fully local &amp;amp; private Speech-To-Text app for Linux, Windows &amp;amp; macOS. Python backend + Electron frontend, utilizing faster-whisper and CUDA acceleration.&lt;/p&gt; &lt;p&gt;If you're interested in the boring dev stuff, go to the bottom section.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;I'm releasing a major UI upgrade today. Enjoy!&lt;/p&gt; &lt;p&gt;Short sales pitch:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;100% Local&lt;/strong&gt;: &lt;em&gt;Everything&lt;/em&gt; runs on your own computer, the app doesn't need internet beyond the initial setup&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Truly Multilingual&lt;/strong&gt;: Supports &lt;a href="https://github.com/openai/whisper/blob/main/whisper/tokenizer.py"&gt;90+ languages&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Fully featured GUI&lt;/strong&gt;: Electron desktop app for Linux, Windows, and macOS&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPU + CPU Mode&lt;/strong&gt;: NVIDIA CUDA acceleration (recommended), or CPU-only mode for any platform including macOS&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Longform Transcription&lt;/strong&gt;: Record as long as you want and have it transcribed in seconds&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Live Mode&lt;/strong&gt;: Real-time sentence-by-sentence transcription for continuous dictation workflows&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Speaker Diarization&lt;/strong&gt;: PyAnnote-based speaker identification&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Static File Transcription&lt;/strong&gt;: Transcribe existing audio/video files with multi-file import queue, retry, and progress tracking&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Remote Access&lt;/strong&gt;: Securely access your desktop at home running the model from anywhere (utilizing Tailscale)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Audio Notebook&lt;/strong&gt;: An Audio Notebook mode, with a calendar-based view, full-text search, and LM Studio integration (chat about your notes with the AI)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;System Tray Control&lt;/strong&gt;: Quickly start/stop a recording, plus a lot of other controls, available via the system tray.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;üìå&lt;em&gt;Half an hour of audio transcribed in under a minute (RTX 3060)!&lt;/em&gt;&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;The seed of the project was my desire to quickly and reliably interface with AI chatbots using my voice. That was about a year ago. Though less prevalent back then, still plenty of AI services like GhatGPT offered voice transcription. However the issue is that, like every other AI-infused company, they &lt;em&gt;always&lt;/em&gt; do it shittily. Yes is works fine for 30s recordings, but what if I want to ramble on for 10 minutes? The AI is smart enough to decipher what I mean and I can speak to it like a smarter rubber ducky, helping me work through the problem.&lt;/p&gt; &lt;p&gt;Well, from my testing back then speak more than 5 minutes and they all start to crap out. And you feel doubly stupid because not only did you not get your transcription but you also wasted 10 minutes talking to the wall.&lt;/p&gt; &lt;p&gt;Moreover, there's the privacy issue. They already collect a ton of text data, giving them my voice feels like too much.&lt;/p&gt; &lt;p&gt;So I first looking at any existing solutions, but couldn't find any decent option that could run locally. Then I came across &lt;a href="https://github.com/KoljaB/RealtimeSTT"&gt;RealtimeSTT&lt;/a&gt;, an extremely impressive and efficient Python project that offered real-time transcription. It's more of a library or framework with only sample implementations.&lt;/p&gt; &lt;p&gt;So I started building around that package, stripping it down to its barest of bones in order to understand how it works so that I could modify it. This whole project grew out of that idea.&lt;/p&gt; &lt;p&gt;I built this project to satisfy my needs. I thought about releasing it only when it was decent enough where someone who doesn't know anything about it can just download a thing and run it. That's why I chose to Dockerize the server portion of the code.&lt;/p&gt; &lt;p&gt;The project was originally written in pure Python. Essentially it's a fancy wrapper around &lt;code&gt;faster-whisper&lt;/code&gt;. At some point I implemented a &lt;em&gt;server-client&lt;/em&gt; architecture and added a notebook mode (think of it like calendar for your audio notes).&lt;/p&gt; &lt;p&gt;And recently I decided to upgrade the frontend UI from Python to React + Typescript. Built all in Google AI Studio - App Builder mode for free believe it or not. No need to shell out the big bucks for Lovable, daddy Google's got you covered.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Don't hesitate to contact me here or open an issue on GitHub for any technical issues or other ideas!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TwilightEncoder"&gt; /u/TwilightEncoder &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/gxbrs1rj2okg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9y6s8/transcriptionsuite_a_fully_local_private_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9y6s8/transcriptionsuite_a_fully_local_private_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T15:22:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1rad3hd</id>
    <title>I evaluated LLaMA and 100+ LLMs on real engineering reasoning for Python</title>
    <updated>2026-02-21T00:51:34+00:00</updated>
    <author>
      <name>/u/samaphp</name>
      <uri>https://old.reddit.com/user/samaphp</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rad3hd/i_evaluated_llama_and_100_llms_on_real/"&gt; &lt;img alt="I evaluated LLaMA and 100+ LLMs on real engineering reasoning for Python" src="https://preview.redd.it/jf8obilpwqkg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8e68a6c305d0b33063478f23529d27aa4fddbb79" title="I evaluated LLaMA and 100+ LLMs on real engineering reasoning for Python" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I evaluated &lt;strong&gt;100+ LLMs&lt;/strong&gt; using a fixed set of questions covering &lt;strong&gt;7 software engineering categories&lt;/strong&gt; from the perspective of a Python developer. This was &lt;strong&gt;not coding tasks&lt;/strong&gt; and not traditional benchmarks, the questions focus on practical engineering reasoning and decision-making. All models were tested against the same prompts, and the results include both qualitative evaluation and &lt;strong&gt;token generation speed&lt;/strong&gt;, because usability over time matters as much as correctness.&lt;/p&gt; &lt;p&gt;Local models were evaluated on an NVIDIA RTX 4060 Ti 16GB using LM Studio, while most cloud models were tested via OpenRouter, with some Anthropic and OpenAI models evaluated directly through their official APIs.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Methodology:&lt;/strong&gt; the evaluation questions were collaboratively designed by &lt;strong&gt;ChatGPT 5.2&lt;/strong&gt; and &lt;strong&gt;Claude Opus 4.5&lt;/strong&gt;, including an agreed list of &lt;em&gt;good&lt;/em&gt; and &lt;em&gt;bad&lt;/em&gt; behaviors for each question. Model responses were then evaluated by &lt;strong&gt;gpt-4o-mini&lt;/strong&gt;, which checked each answer against that shared list. The evaluation categories were:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Problem Understanding &amp;amp; Reasoning&lt;/li&gt; &lt;li&gt;System Design &amp;amp; Architecture&lt;/li&gt; &lt;li&gt;API, Data &amp;amp; Domain Design&lt;/li&gt; &lt;li&gt;Code Quality &amp;amp; Implementation&lt;/li&gt; &lt;li&gt;Reliability, Security &amp;amp; Operations&lt;/li&gt; &lt;li&gt;LLM Behavior &amp;amp; Professional Discipline&lt;/li&gt; &lt;li&gt;Engineering Restraint &amp;amp; Practical Judgment&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;One thing that surprised me was that some of the &lt;strong&gt;highest-performing models&lt;/strong&gt; were also among the &lt;strong&gt;slowest and most token-heavy&lt;/strong&gt;. Once models pass roughly ~95%, quality differences shrink, and &lt;strong&gt;latency and efficiency become far more important&lt;/strong&gt;. My goal was to identify models I could realistically run &lt;strong&gt;24 hours a day&lt;/strong&gt;, either locally or via a cloud provider, without excessive cost or waiting time. The models I ended up favoriting for Python developer tasks weren't always the cheapest or the top scorers; they were the ones that finished quickly, used tokens efficiently, and still showed consistently good engineering judgment. For example, &lt;strong&gt;GPT 5.1 Codex&lt;/strong&gt; isn't very cheap, but it's very fast and highly token-efficient, which makes it practical for continuous use.&lt;/p&gt; &lt;hr /&gt; &lt;h3&gt;Models I favored (efficient &amp;amp; suitable for my use case)&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Grok 4.1 Fast&lt;/strong&gt;: very fast, disciplined engineering responses&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPT OSS 120B&lt;/strong&gt;: strong reasoning with excellent efficiency&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Gemini 3 Flash Preview&lt;/strong&gt;: extremely fast and clean&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPT OSS 20B (local)&lt;/strong&gt;: fast and practical on a consumer GPU&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPT 5.1 Codex Mini&lt;/strong&gt;: low verbosity, quick turnaround&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPT 5.1 Codex&lt;/strong&gt;: not cheap, but very fast and token-efficient&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Minimax M2&lt;/strong&gt;:solid discipline with reasonable latency&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Qwen3 4B (local)&lt;/strong&gt;: small, fast, and surprisingly capable&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The full list and the test results are available on this URL: &lt;a href="https://py.eval.draftroad.com"&gt;https://py.eval.draftroad.com&lt;/a&gt;&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;‚ö†Ô∏è &lt;strong&gt;Disclaimer:&lt;/strong&gt; these results reflect my personal experience and testing methodology. I may be wrong. Results can vary based on use cases, prompting styles, and evaluation criteria. This should be viewed as a transparent comparison, not a definitive benchmark for python with LLM.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/samaphp"&gt; /u/samaphp &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jf8obilpwqkg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rad3hd/i_evaluated_llama_and_100_llms_on_real/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rad3hd/i_evaluated_llama_and_100_llms_on_real/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T00:51:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1rajez2</id>
    <title>what are your favorite lesser known models on huggingface</title>
    <updated>2026-02-21T06:01:33+00:00</updated>
    <author>
      <name>/u/EngineeringBright82</name>
      <uri>https://old.reddit.com/user/EngineeringBright82</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm a professor, I want to expand my students minds by showing them models that are not chatGPT etc. Anyone have some unique / interesting / useful models hosted on huggingface?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EngineeringBright82"&gt; /u/EngineeringBright82 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rajez2/what_are_your_favorite_lesser_known_models_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rajez2/what_are_your_favorite_lesser_known_models_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rajez2/what_are_your_favorite_lesser_known_models_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T06:01:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9qa7l</id>
    <title>Kimi has context window expansion ambitions</title>
    <updated>2026-02-20T08:54:10+00:00</updated>
    <author>
      <name>/u/omarous</name>
      <uri>https://old.reddit.com/user/omarous</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9qa7l/kimi_has_context_window_expansion_ambitions/"&gt; &lt;img alt="Kimi has context window expansion ambitions" src="https://preview.redd.it/3cvl2bdh5mkg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e22f6604997ccccf6f6215ae239ab8f8b1dd09c3" title="Kimi has context window expansion ambitions" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/omarous"&gt; /u/omarous &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3cvl2bdh5mkg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9qa7l/kimi_has_context_window_expansion_ambitions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9qa7l/kimi_has_context_window_expansion_ambitions/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T08:54:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1rafo5b</id>
    <title>[Update] Vellium v0.3.5: Massive Writing Mode upgrade, Native KoboldCpp, and OpenAI TTS</title>
    <updated>2026-02-21T02:50:43+00:00</updated>
    <author>
      <name>/u/Possible_Statement84</name>
      <uri>https://old.reddit.com/user/Possible_Statement84</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rafo5b/update_vellium_v035_massive_writing_mode_upgrade/"&gt; &lt;img alt="[Update] Vellium v0.3.5: Massive Writing Mode upgrade, Native KoboldCpp, and OpenAI TTS" src="https://preview.redd.it/j509b0ozgrkg1.png?width=140&amp;amp;height=84&amp;amp;auto=webp&amp;amp;s=8c561d0403991ae31c82f4c63a33fcc76d123703" title="[Update] Vellium v0.3.5: Massive Writing Mode upgrade, Native KoboldCpp, and OpenAI TTS" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, just pushed a pretty big update for Vellium (v0.2.8 to v0.3.5). The main focus this time was overhauling the writing mode and making local providers work much smoother.&lt;/p&gt; &lt;p&gt;The writing mode got a huge rework. We finally added a proper book bible, direct DOCX import, and cached book summaries. The sidebar is way more compact now, and the character workspace is much better ‚Äî you can even use AI to patch-edit your characters directly. We also fixed a bunch of UX stuff, so project deletion and export/download (including inline scenes) are actually reliable now.&lt;/p&gt; &lt;p&gt;For local setups, KoboldCpp integration is fully native now. It supports the &lt;code&gt;provider:memory&lt;/code&gt; field, universal tags, and n-sigma. Payload fields are finally aligned with the official API, and we fixed those annoying model loading issues. Tool calling also properly disables in the UI when KoboldCpp is active.&lt;/p&gt; &lt;p&gt;A few other cool things: we added OpenAI-compatible TTS with a separate model just for translation. There's a new Zen Chat UI mode if you want zero visual distractions. Phrase bans are working properly now, and we turned off the default badwords by default. You also get more control in settings over API parameter forwarding, like sampler forwarding.&lt;/p&gt; &lt;p&gt;Under the hood, multi-character chat is way more stable (add at least one word from char name and he answer first than another). Squashed some runtime data leaks, sorted out the server bundle resolving inside&lt;code&gt;asar&lt;/code&gt;, and added some basic security hardening for local mode. Oh, and the project is now officially MIT licensed!&lt;/p&gt; &lt;p&gt;Grab the release on GitHub: &lt;a href="https://github.com/tg-prplx/vellium"&gt;https://github.com/tg-prplx/vellium&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know if you hit any bugs or have ideas for the next updates.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Possible_Statement84"&gt; /u/Possible_Statement84 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1rafo5b"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rafo5b/update_vellium_v035_massive_writing_mode_upgrade/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rafo5b/update_vellium_v035_massive_writing_mode_upgrade/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T02:50:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9wvg4</id>
    <title>GGML and llama.cpp join HF to ensure the long-term progress of Local AI</title>
    <updated>2026-02-20T14:31:22+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9wvg4/ggml_and_llamacpp_join_hf_to_ensure_the_longterm/"&gt; &lt;img alt="GGML and llama.cpp join HF to ensure the long-term progress of Local AI" src="https://external-preview.redd.it/tLGg2WMvFn2R5w7Nf2m6oJPphAYJILLSWaWPLPoW8i4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e6bb0cd5000a00c0e28c8ae17203068e5acfb352" title="GGML and llama.cpp join HF to ensure the long-term progress of Local AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;article by Georgi Gerganov, Xuan-Son Nguyen, Aleksander Grygier, Lysandre, Victor Mustar, Julien Chaumond&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/blog/ggml-joins-hf"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9wvg4/ggml_and_llamacpp_join_hf_to_ensure_the_longterm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9wvg4/ggml_and_llamacpp_join_hf_to_ensure_the_longterm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T14:31:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1rabg6o</id>
    <title>Qwen3 coder next oddly usable at aggressive quantization</title>
    <updated>2026-02-20T23:41:01+00:00</updated>
    <author>
      <name>/u/CoolestSlave</name>
      <uri>https://old.reddit.com/user/CoolestSlave</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys,&lt;/p&gt; &lt;p&gt;I've been testing the 30b range models but i've been a little disappointed by them (qwen 30b, devstral 2, nemotron etc) as they need a lot of guidance and almost all of them can't correct some mistake they made no matter what.&lt;/p&gt; &lt;p&gt;Then i tried to use qwen next coder at q2 because i don't have enough ram for q4. Oddly enough it does not say nonsense, even better, he one shot some html front page and can correct some mistake by himself when prompting back his mistake.&lt;/p&gt; &lt;p&gt;I've only made shallow testing but it really feel like at this quant, it already surpass all 30b models without sweating.&lt;/p&gt; &lt;p&gt;Do you have any experience with this model ? why is it that good ??&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CoolestSlave"&gt; /u/CoolestSlave &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rabg6o/qwen3_coder_next_oddly_usable_at_aggressive/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rabg6o/qwen3_coder_next_oddly_usable_at_aggressive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rabg6o/qwen3_coder_next_oddly_usable_at_aggressive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T23:41:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1rabcyp</id>
    <title>A few Strix Halo benchmarks (Minimax M2.5, Step 3.5 Flash, Qwen3 Coder Next)</title>
    <updated>2026-02-20T23:37:12+00:00</updated>
    <author>
      <name>/u/spaceman_</name>
      <uri>https://old.reddit.com/user/spaceman_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rabcyp/a_few_strix_halo_benchmarks_minimax_m25_step_35/"&gt; &lt;img alt="A few Strix Halo benchmarks (Minimax M2.5, Step 3.5 Flash, Qwen3 Coder Next)" src="https://preview.redd.it/y3n05xxziqkg1.png?width=140&amp;amp;height=75&amp;amp;auto=webp&amp;amp;s=9c8cd5f0f499dbda0fd3ab30a338f120aa67e03d" title="A few Strix Halo benchmarks (Minimax M2.5, Step 3.5 Flash, Qwen3 Coder Next)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With the release of Step 3.5 and MiniMax M2.5, we've got two new options for models that barely fit in memory.&lt;/p&gt; &lt;p&gt;To help people figure out which models run best on the platform, I decided to run some llama.cpp benchmarks for a few quants of these models.&lt;/p&gt; &lt;p&gt;I also included some benchmarks for Qwen3-coder-next (since we've been seeing lots of improvement lately), GLM 4.6V &amp;amp; GLM 4.7 Flash, and a few older models like gpt-oss-120b which compete in a similar size space.&lt;/p&gt; &lt;p&gt;My ROCm benchmarks are running against ROCm 7.2 as that is what my distro provides. My device has a Ryzen AI Max+ 395 @ 70W and 128GB of memory. All benchmarks are run at a context depth of 30,000 tokens.&lt;/p&gt; &lt;p&gt;If there's interest in other models or quants, feel free to ask for them in the comments, and I'll see if I can get some running.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/spaceman_"&gt; /u/spaceman_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1rabcyp"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rabcyp/a_few_strix_halo_benchmarks_minimax_m25_step_35/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rabcyp/a_few_strix_halo_benchmarks_minimax_m25_step_35/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T23:37:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1raall0</id>
    <title>fixed parser for Qwen3-Coder-Next</title>
    <updated>2026-02-20T23:06:32+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1raall0/fixed_parser_for_qwen3codernext/"&gt; &lt;img alt="fixed parser for Qwen3-Coder-Next" src="https://external-preview.redd.it/Y3wE-GVXbELboPM9WQJZOtsZ_aPLgAL7jIOMvAV90UU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6015d17bc47511d73e5fb3850ccf0851296f278f" title="fixed parser for Qwen3-Coder-Next" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;another fix for Qwen Next!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/19765"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1raall0/fixed_parser_for_qwen3codernext/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1raall0/fixed_parser_for_qwen3codernext/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T23:06:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9vywq</id>
    <title>GGML.AI has got acquired by Huggingface</title>
    <updated>2026-02-20T13:54:26+00:00</updated>
    <author>
      <name>/u/Time_Reaper</name>
      <uri>https://old.reddit.com/user/Time_Reaper</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9vywq/ggmlai_has_got_acquired_by_huggingface/"&gt; &lt;img alt="GGML.AI has got acquired by Huggingface" src="https://external-preview.redd.it/l687iazpdDZhrDlIbQBxf8OTcfiJg6WGdsBpv03NqVo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a9e45ab199a5cdbdf8c5eb1968743c094b946e98" title="GGML.AI has got acquired by Huggingface" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Time_Reaper"&gt; /u/Time_Reaper &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/discussions/19759"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9vywq/ggmlai_has_got_acquired_by_huggingface/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9vywq/ggmlai_has_got_acquired_by_huggingface/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T13:54:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9zt8m</id>
    <title>The top 3 models on openrouter this week ( Chinese models are dominating!)</title>
    <updated>2026-02-20T16:21:50+00:00</updated>
    <author>
      <name>/u/keb_37</name>
      <uri>https://old.reddit.com/user/keb_37</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9zt8m/the_top_3_models_on_openrouter_this_week_chinese/"&gt; &lt;img alt="The top 3 models on openrouter this week ( Chinese models are dominating!)" src="https://preview.redd.it/h4l8zr4rdokg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e1cb3201433eea5c7cd862fbc8c0f259e4e6b134" title="The top 3 models on openrouter this week ( Chinese models are dominating!)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;the first time i see a model exceed 3 trillion tokens per week on openrouter!&lt;/p&gt; &lt;p&gt;the first time i see more than one model exceed a trillion token per week ( it was only grok 4 fast month ago)&lt;/p&gt; &lt;p&gt;the first time i see chinese models destroying US ones like this&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/keb_37"&gt; /u/keb_37 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/h4l8zr4rdokg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9zt8m/the_top_3_models_on_openrouter_this_week_chinese/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9zt8m/the_top_3_models_on_openrouter_this_week_chinese/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T16:21:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1ram2ov</id>
    <title>How I mapped every High Court of Australia case and their citations (1901-2025)</title>
    <updated>2026-02-21T08:36:59+00:00</updated>
    <author>
      <name>/u/Neon0asis</name>
      <uri>https://old.reddit.com/user/Neon0asis</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ram2ov/how_i_mapped_every_high_court_of_australia_case/"&gt; &lt;img alt="How I mapped every High Court of Australia case and their citations (1901-2025)" src="https://preview.redd.it/2mntthxp7tkg1.gif?width=640&amp;amp;crop=smart&amp;amp;s=2eb05b7ded68545504de00ea12ea1305b546acb8" title="How I mapped every High Court of Australia case and their citations (1901-2025)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve recently begun working on a project to convert entirety of Australian case law and legislation into a LexisNexis-style interlinked legal knowledge graph.&lt;/p&gt; &lt;p&gt;As I‚Äôve experimented with techniques to normalise case citations, I thought it would be cool to turn my work into a neat little visualisation, and explain how you could do the same with your own documents.&lt;/p&gt; &lt;p&gt;So the graph above is a visualisation of a cross-section of a legal knowledge graph I‚Äôve been developing of Australian case law.&lt;/p&gt; &lt;p&gt;Each node represents a High Court of Australia decision. The size of the node reflects how often that case has been cited by other High Court cases. The node's location and clustering comes from mapping each case‚Äôs semantic ‚Äúposition‚Äù into 3D space, based on its location in a higher-dimensional embedding space.&lt;/p&gt; &lt;h1&gt;How the dataset was built&lt;/h1&gt; &lt;p&gt;To assemble the graph, I downloaded the &lt;a href="https://huggingface.co/datasets/isaacus/open-australian-legal-corpus"&gt;Open Australian Legal Corpus &lt;/a&gt;and ran the &lt;a href="https://docs.isaacus.com/capabilities/enrichment"&gt;Kanon 2 Enricher&lt;/a&gt; to extract citations and additional metadata, such as decision dates and pinpoint references. I then used this additional metadata to repair and improve some of the dataset's missing features.&lt;/p&gt; &lt;p&gt;For roughly 90% of the corpus, I was able to recover and uniquely identify the party names, decision dates, and common aliases.&lt;/p&gt; &lt;p&gt;Using the party names and year as a composite key, I then normalised and deduplicated every citation appearing in High Court decisions. This produced ~20,000 High Court-to-High Court citations.&lt;/p&gt; &lt;p&gt;With the citations linked, I used the &lt;a href="https://docs.isaacus.com/capabilities/embedding"&gt;Kanon 2 Embedder&lt;/a&gt; to generate vector embeddings for each case, and then applied &lt;a href="https://github.com/YingfanWang/PaCMAP"&gt;PaCMAP&lt;/a&gt; (a dimensionality reduction library) to reduce those embeddings down to a 3D representation.&lt;/p&gt; &lt;p&gt;To infer clusters (i.e., broad topical groupings), I ran &lt;a href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html"&gt;K-means &lt;/a&gt;in the original embedding space. To make the clusters interpretable, I used &lt;a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"&gt;TF‚ÄìIDF&lt;/a&gt; to generate simple semantic labels based on the most characteristic terms in each cluster.&lt;/p&gt; &lt;p&gt;Finally, using the reception labels extracted by the Kanon 2 Enricher, I captured a sentiment-like signal for how cases treat the authorities they cite. Most citations are neutral (grey). Citations that overrule prior High Court authority are marked in red, while supportive citations are shown in green. Because the Enricher extracts these signals natively, that step was straightforward.&lt;/p&gt; &lt;p&gt;With the features extracted and linked, I then vibe coded a lightweight interface to render the network as an interactive node graph.&lt;/p&gt; &lt;h1&gt;What you can see in the result&lt;/h1&gt; &lt;p&gt;Even with around ~7,000 High Court cases, some patterns stand out immediately:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;The semantic geometry works surprisingly well.&lt;/strong&gt; Closely related areas of law sit near one another in 3D space. Estate law and land law, for example, tend to cluster tightly (towards the bottom of the structure) while criminal law, which is not related to these fields, occupies the top end of the grap.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;You can explore fine-grained subregions interactively.&lt;/strong&gt; In the notebook (linked at the end of the post), there‚Äôs a region where several clusters intersect that corresponds strongly to constitutional cases involving Indigenous communities. &lt;em&gt;Mabo v Queensland (No 2)&lt;/em&gt; is one of the best-known cases in that neighbourhood.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The time dimension reflects legal history.&lt;/strong&gt; You can see a shift toward citing domestic authority more heavily after the &lt;a href="https://peo.gov.au/understand-our-parliament/history-of-parliament/history-milestones/australian-parliament-history-timeline/events/australia-act-1986"&gt;Australia Acts 1986&lt;/a&gt;, which helped establish Australia‚Äôs judicial independence. Earlier High Court decisions cite UK Privy Council rulings more often and are more visibly shaped by UK common law. This is one reason the earliest cases cite Australian authorities less than you might expect.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Reproducing it&lt;/h1&gt; &lt;p&gt;All code to reproduce the results is on &lt;a href="https://github.com/isaacus-dev/cookbooks/tree/main/cookbooks/semantic-legal-citation-graph"&gt;GitHub,&lt;/a&gt; and the interactive visualisation is embedded directly in the notebook, so you can explore it without running anything locally. If you‚Äôd like a guided walkthrough, there‚Äôs also a guided tour highlighting landmark cases in Australian constitutional law I have up on &lt;a href="https://youtu.be/in76S6P9xOw?si=hBaPpb0p6HVyjelv"&gt;YouTube&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Neon0asis"&gt; /u/Neon0asis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2mntthxp7tkg1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ram2ov/how_i_mapped_every_high_court_of_australia_case/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ram2ov/how_i_mapped_every_high_court_of_australia_case/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T08:36:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1raf3dm</id>
    <title>GLM 5 seems to have a "Claude" personality</title>
    <updated>2026-02-21T02:23:22+00:00</updated>
    <author>
      <name>/u/TinyApplet</name>
      <uri>https://old.reddit.com/user/TinyApplet</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1raf3dm/glm_5_seems_to_have_a_claude_personality/"&gt; &lt;img alt="GLM 5 seems to have a &amp;quot;Claude&amp;quot; personality" src="https://preview.redd.it/7nj17cwubrkg1.jpg?width=140&amp;amp;height=78&amp;amp;auto=webp&amp;amp;s=e0025d5e56387cb178ff1a928dc4a19313407e90" title="GLM 5 seems to have a &amp;quot;Claude&amp;quot; personality" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've noticed that GLM 5 behaves significantly differently when told it is Claude, as with the following system prompt: &amp;quot;You are Claude, a large language model by Anthropic.&amp;quot; The writing style and personality changes significantly, and it even seems to bypass built-in censorship, as per my second image.&lt;/p&gt; &lt;p&gt;I've also tried a more nonsensical prompt: &amp;quot;You are Tiny, a large language model by Applet&amp;quot; (deliberately avoiding the names of any known models or companies), and, as expected, that didn't yield the same results nor bypassed the model's censorship.&lt;/p&gt; &lt;p&gt;Whether this was intentional on Zhipu's part or not, I can't say; it could be that they did, in fact, include a &amp;quot;Claude&amp;quot; personality in the training dataset, seeing as how they seem to have planned for GLM 5 to work well with Claude Code. It's also possible, of course, that this is emergent behavior, and that the personality changes are merely because GLM 5 has some information, however vague, on its dataset about what Claude is and how it's supposed to behave.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TinyApplet"&gt; /u/TinyApplet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1raf3dm"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1raf3dm/glm_5_seems_to_have_a_claude_personality/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1raf3dm/glm_5_seems_to_have_a_claude_personality/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T02:23:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9uuc6</id>
    <title>Deepseek and Gemma ??</title>
    <updated>2026-02-20T13:05:36+00:00</updated>
    <author>
      <name>/u/ZeusZCC</name>
      <uri>https://old.reddit.com/user/ZeusZCC</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9uuc6/deepseek_and_gemma/"&gt; &lt;img alt="Deepseek and Gemma ??" src="https://preview.redd.it/84ph0pirenkg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8d2b363b1900aae44bcfc12c0eeb9d8e2caa7d08" title="Deepseek and Gemma ??" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ZeusZCC"&gt; /u/ZeusZCC &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/84ph0pirenkg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9uuc6/deepseek_and_gemma/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9uuc6/deepseek_and_gemma/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T13:05:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1ra8omf</id>
    <title>"Gemma, which we will be releasing a new version of soon"</title>
    <updated>2026-02-20T21:50:51+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ra8omf/gemma_which_we_will_be_releasing_a_new_version_of/"&gt; &lt;img alt="&amp;quot;Gemma, which we will be releasing a new version of soon&amp;quot;" src="https://external-preview.redd.it/9mfj1kMXjQ4Pove4Y8zbrEpz5ffGrhmDZ-YwmsdPJeE.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=66d6fe6182877c1d801afa8a61aec7616fb8a587" title="&amp;quot;Gemma, which we will be releasing a new version of soon&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;20:17&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/P0enFK4bzLE?si=2hfjhPrT4gbqsZwk"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ra8omf/gemma_which_we_will_be_releasing_a_new_version_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ra8omf/gemma_which_we_will_be_releasing_a_new_version_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T21:50:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7bsfd</id>
    <title>Best Audio Models - Feb 2026</title>
    <updated>2026-02-17T17:01:37+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;They've been a ton of audio models released of late, the most notable perhaps being Qwen3 TTS. So its time for another &lt;strong&gt;Best Audio Models&lt;/strong&gt; megathread&lt;/p&gt; &lt;p&gt;Share what your favorite ASR, TTS, STT, Text to Music models are right now &lt;strong&gt;and why.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Given the the amount of ambiguity and subjectivity in rating/testing these models, please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks etc. Closed models like Elevenlabs v3 seem to continue to be a few levels above open models especially for production use cases with long lengths/stability requirements, so comparisons, especially empirical ones are welcome.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please use the top level comments to thread your responses.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T17:01:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8snay</id>
    <title>AMA with StepFun AI - Ask Us Anything</title>
    <updated>2026-02-19T07:15:35+00:00</updated>
    <author>
      <name>/u/StepFun_ai</name>
      <uri>https://old.reddit.com/user/StepFun_ai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt; &lt;img alt="AMA with StepFun AI - Ask Us Anything" src="https://external-preview.redd.it/bqLigXK_A-XO-amDfdyeQenKCxR18nhChWpDnQXYmr4.png?width=140&amp;amp;height=75&amp;amp;auto=webp&amp;amp;s=72c79bc533c167d8f77ecf629e3b7af53ee1e6b6" title="AMA with StepFun AI - Ask Us Anything" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/w8274fg1jekg1.png?width=1785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fadbd0ec26a56e60900f9ed667ae808217d70cf2"&gt;https://preview.redd.it/w8274fg1jekg1.png?width=1785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fadbd0ec26a56e60900f9ed667ae808217d70cf2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; !&lt;/p&gt; &lt;p&gt;We are &lt;strong&gt;StepFun&lt;/strong&gt;, the team behind the &lt;strong&gt;Step&lt;/strong&gt; family models, including &lt;a href="https://huggingface.co/collections/stepfun-ai/step-35-flash"&gt;&lt;strong&gt;Step 3.5 Flash&lt;/strong&gt;&lt;/a&gt; and &lt;a href="https://huggingface.co/collections/stepfun-ai/step3-vl-10b"&gt;&lt;strong&gt;Step-3-VL-10B&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;We are super excited to host our first AMA tomorrow in this community. Our participants include CEO, CTO, Chief Scientist, LLM Researchers.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Participants&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Ok_Reach_5122"&gt;u/Ok_Reach_5122&lt;/a&gt; (Co-founder &amp;amp; CEO of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/bobzhuyb"&gt;u/bobzhuyb&lt;/a&gt; (Co-founder &amp;amp; CTO of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/user/Lost-Nectarine1016"&gt;u/Lost-Nectarine1016&lt;/a&gt; (Co-founder &amp;amp; Chief Scientist of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Elegant-Sale-1328"&gt;u/Elegant-Sale-1328&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/SavingsConclusion298"&gt;u/SavingsConclusion298&lt;/a&gt; (Post-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Spirited_Spirit3387"&gt;u/Spirited_Spirit3387&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/These-Nothing-8564/"&gt;u/These-Nothing-8564&lt;/a&gt; (Technical Project Manager)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Either-Beyond-7395"&gt;u/Either-Beyond-7395&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Human_Ad_162"&gt;u/Human_Ad_162&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Icy_Dare_3866"&gt;u/Icy_Dare_3866&lt;/a&gt; (Post-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Big-Employee5595"&gt;u/Big-Employee5595&lt;/a&gt; (Agent Algorithms Lead&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run 8 - 11 AM PST, Feburary 19th. The StepFun team will monitor and answer questions over the 24 hours after the live session.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StepFun_ai"&gt; /u/StepFun_ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T07:15:35+00:00</published>
  </entry>
</feed>
