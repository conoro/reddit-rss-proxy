<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-16T09:06:48+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1mqox5s</id>
    <title>Meta released DINO-V3 : SOTA for any Vision task</title>
    <updated>2025-08-15T05:48:16+00:00</updated>
    <author>
      <name>/u/Technical-Love-8479</name>
      <uri>https://old.reddit.com/user/Technical-Love-8479</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Meta just released DINOv3 (upgrade over DINO-V2). It learns entirely from unlabeled images, no captions, no annotations, and still outperforms models like CLIP, SAM, and even the previous DINOv2 on dense tasks like segmentation, depth estimation, and 3D matching. They trained a 7B-parameter ViT and fixed the usual issue of feature degradation over long training with a new technique called Gram Anchoring.&lt;/p&gt; &lt;p&gt;Paper &amp;amp; weights : &lt;a href="https://ai.meta.com/dinov3/"&gt;https://ai.meta.com/dinov3/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Video explanation : &lt;a href="https://www.youtube.com/watch?v=VfYUQ2Qquxk"&gt;https://www.youtube.com/watch?v=VfYUQ2Qquxk&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Technical-Love-8479"&gt; /u/Technical-Love-8479 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqox5s/meta_released_dinov3_sota_for_any_vision_task/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqox5s/meta_released_dinov3_sota_for_any_vision_task/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mqox5s/meta_released_dinov3_sota_for_any_vision_task/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T05:48:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1mrk8sj</id>
    <title>Codedox</title>
    <updated>2025-08-16T04:06:51+00:00</updated>
    <author>
      <name>/u/getfitdotus</name>
      <uri>https://old.reddit.com/user/getfitdotus</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrk8sj/codedox/"&gt; &lt;img alt="Codedox" src="https://external-preview.redd.it/A1SgjRkmhWwrrqPQBYBttVOWWU4KzaH8F3m_DCov2wk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3c63a424d6c13a7d7ce5663c9757006d9536cf56" title="Codedox" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;CodeDox - Documentation Code Extraction &amp;amp; Search Something I created for self hosting. Similar to context7 but open source and your in control of the content. &lt;/p&gt; &lt;p&gt;A powerful system for crawling documentation websites, extracting code snippets, and providing fast search capabilities via MCP (Model Context Protocol) integration.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/getfitdotus"&gt; /u/getfitdotus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/chriswritescode-dev/codedox"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrk8sj/codedox/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mrk8sj/codedox/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T04:06:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1mrnnqu</id>
    <title>Iterative loops for quality</title>
    <updated>2025-08-16T07:02:35+00:00</updated>
    <author>
      <name>/u/No_Efficiency_1144</name>
      <uri>https://old.reddit.com/user/No_Efficiency_1144</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What have you found to be good methods for prompting an LLM, or setting up a multi-agent system, to iterate on previous responses to increase quality?&lt;/p&gt; &lt;p&gt;Did you find a limit to how many times you could do iterative loops before it broke down or got strange?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Efficiency_1144"&gt; /u/No_Efficiency_1144 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrnnqu/iterative_loops_for_quality/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrnnqu/iterative_loops_for_quality/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mrnnqu/iterative_loops_for_quality/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T07:02:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqnft3</id>
    <title>DeepSeek is better than 4o on most benchmarks at 10% of the price?</title>
    <updated>2025-08-15T04:27:25+00:00</updated>
    <author>
      <name>/u/inbiolim</name>
      <uri>https://old.reddit.com/user/inbiolim</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqnft3/deepseek_is_better_than_4o_on_most_benchmarks_at/"&gt; &lt;img alt="DeepSeek is better than 4o on most benchmarks at 10% of the price?" src="https://preview.redd.it/o5jfkiky14jf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3040aae64b79ccf04ada63a396032e3bf5085f8f" title="DeepSeek is better than 4o on most benchmarks at 10% of the price?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/inbiolim"&gt; /u/inbiolim &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/o5jfkiky14jf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqnft3/deepseek_is_better_than_4o_on_most_benchmarks_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mqnft3/deepseek_is_better_than_4o_on_most_benchmarks_at/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T04:27:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1mrjg71</id>
    <title>"toad" model on LMArena is epic</title>
    <updated>2025-08-16T03:28:58+00:00</updated>
    <author>
      <name>/u/Comprehensive_Dish_6</name>
      <uri>https://old.reddit.com/user/Comprehensive_Dish_6</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrjg71/toad_model_on_lmarena_is_epic/"&gt; &lt;img alt="&amp;quot;toad&amp;quot; model on LMArena is epic" src="https://b.thumbs.redditmedia.com/0Ag5Z41HsiPG54btL9pWP_XXFucch0CZzf1SSjL0ADg.jpg" title="&amp;quot;toad&amp;quot; model on LMArena is epic" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Does anyone know what the &amp;quot;toad&amp;quot; model is?&lt;br /&gt; I've tried battle a few times and each time toad appears I have voted for it. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/q291qgsdwajf1.png?width=2644&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3eb6a42ff6082e9b10f9f23ba8a8607a437b3e13"&gt;https://preview.redd.it/q291qgsdwajf1.png?width=2644&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3eb6a42ff6082e9b10f9f23ba8a8607a437b3e13&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Comprehensive_Dish_6"&gt; /u/Comprehensive_Dish_6 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrjg71/toad_model_on_lmarena_is_epic/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrjg71/toad_model_on_lmarena_is_epic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mrjg71/toad_model_on_lmarena_is_epic/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T03:28:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1mr173e</id>
    <title>huihui-ai/Huihui-GLM-4.5-Air-abliterated-GGUF ¬∑ Hugging Face</title>
    <updated>2025-08-15T15:31:23+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr173e/huihuiaihuihuiglm45airabliteratedgguf_hugging_face/"&gt; &lt;img alt="huihui-ai/Huihui-GLM-4.5-Air-abliterated-GGUF ¬∑ Hugging Face" src="https://external-preview.redd.it/xIP4cUl_xFw8QdJsO9wbtyJiZxAzIX4f0eGxUH-gPb0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5312fc45844644f3509dcb53e3091d546266ec52" title="huihui-ai/Huihui-GLM-4.5-Air-abliterated-GGUF ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/huihui-ai/Huihui-GLM-4.5-Air-abliterated-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr173e/huihuiaihuihuiglm45airabliteratedgguf_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mr173e/huihuiaihuihuiglm45airabliteratedgguf_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T15:31:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1mr6ojs</id>
    <title>How OpenAI Misled You on RLHF</title>
    <updated>2025-08-15T18:49:03+00:00</updated>
    <author>
      <name>/u/fpgaminer</name>
      <uri>https://old.reddit.com/user/fpgaminer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr6ojs/how_openai_misled_you_on_rlhf/"&gt; &lt;img alt="How OpenAI Misled You on RLHF" src="https://external-preview.redd.it/o02DfA1hR06T8VIFmfiB8nq6L3bMSQ3o_mYnp-HDq_s.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7014c3c86a3b96ca4ff35dbc47e42d8cfe3b95e8" title="How OpenAI Misled You on RLHF" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I hope this article is okay here, since it's related to my open source VLM (JoyCaption), and LLM training in general. The article originally started as just my usual dumping of details and insights from the Finetuning Battlefields, this time focused on RL finetuning a VLM, but I ended up adding a bunch of details on the nature of RL itself, since most people assume it's only for preference tuning or similar (it's much, much more important than that). Anyway, if you're interested in training models I hope there's something interesting or useful in there.&lt;/p&gt; &lt;p&gt;(I'll eventually get around to finishing the article on building JoyCaption itself, which covers its core dataset building and how a pure LLM like Llama 3.1 was trained to see images.)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fpgaminer"&gt; /u/fpgaminer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://aerial-toothpaste-34a.notion.site/How-OpenAI-Misled-You-on-RLHF-1f83f742d9dd80a68129d06503464aff"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr6ojs/how_openai_misled_you_on_rlhf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mr6ojs/how_openai_misled_you_on_rlhf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T18:49:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1mro1q4</id>
    <title>Where to Start with Fine-tuning</title>
    <updated>2025-08-16T07:23:21+00:00</updated>
    <author>
      <name>/u/Constant_View_197</name>
      <uri>https://old.reddit.com/user/Constant_View_197</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to make a project on fine-tuning. No idea where to start Push me in the right direction ‚¨ÜÔ∏è&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Constant_View_197"&gt; /u/Constant_View_197 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mro1q4/where_to_start_with_finetuning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mro1q4/where_to_start_with_finetuning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mro1q4/where_to_start_with_finetuning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T07:23:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqxq1v</id>
    <title>Build the buddy that gets you! We open-sourced a complete AI voice interaction system!</title>
    <updated>2025-08-15T13:23:10+00:00</updated>
    <author>
      <name>/u/Lanky-Drummer193</name>
      <uri>https://old.reddit.com/user/Lanky-Drummer193</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqxq1v/build_the_buddy_that_gets_you_we_opensourced_a/"&gt; &lt;img alt="Build the buddy that gets you! We open-sourced a complete AI voice interaction system!" src="https://preview.redd.it/1o9li0qbp6jf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a0bab69a658eb8e7805d1b0196f9f560f38d8735" title="Build the buddy that gets you! We open-sourced a complete AI voice interaction system!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, we just open-sourced Buddie: a complete, AI-powered voice interaction system we built from the ground up, so you can create your own AI buddy.&lt;/p&gt; &lt;p&gt;It's a full-stack platform for developers, hackers, and students, including custom hardware, firmware, and a mobile app. Therefore, you can use our solution to create various forms of AI devices, such as earphones, speakers, bracelets, toys, or desktop ornaments.&lt;/p&gt; &lt;p&gt;What it can do:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Live transcribe &amp;amp; summarize meetings, calls, or in-person chats. &lt;/li&gt; &lt;li&gt;Get real-time hints during conversations . &lt;/li&gt; &lt;li&gt;Talk to LLMs completely hands-free. &lt;/li&gt; &lt;li&gt;Context-aware help without needing to repeat yourself.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We've put everything on GitHub, including docs, to get you started. We're just getting started and would love to hear your ideas, questions, or even wild feature requests. Let us know what you think!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lanky-Drummer193"&gt; /u/Lanky-Drummer193 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1o9li0qbp6jf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqxq1v/build_the_buddy_that_gets_you_we_opensourced_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mqxq1v/build_the_buddy_that_gets_you_we_opensourced_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T13:23:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1mrn9it</id>
    <title>Start fine-tuning - Guidance needed</title>
    <updated>2025-08-16T06:42:03+00:00</updated>
    <author>
      <name>/u/AI-On-A-Dime</name>
      <uri>https://old.reddit.com/user/AI-On-A-Dime</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After hanging around this community a while, I finally decided to dip my feet into fine-tuning / post-training!&lt;/p&gt; &lt;p&gt;I want to fine-tune/post-train the following dataset on a small model: &lt;a href="https://huggingface.co/datasets/microsoft/rStar-Coder"&gt;https://huggingface.co/datasets/microsoft/rStar-Coder&lt;/a&gt;. &lt;/p&gt; &lt;p&gt;The benchmarks seem remarkable, so let‚Äôs see what happens.&lt;/p&gt; &lt;p&gt;The idea is to have a local llm to use with open source code assistant like roo code, kilo code and similar. However, the main purpose of this to learn.&lt;/p&gt; &lt;p&gt;I have a total of 16 GB RAM + 6 GB VRAM, so the model has to be small, ranging between Gemma 3n 270 to maximum Qwen3-8gb. &lt;/p&gt; &lt;p&gt;Which model would make most sense to fine-tune/post-train for this purpose?&lt;/p&gt; &lt;p&gt;What method do you recommend for this purpose? Lora? Or anything else? &lt;/p&gt; &lt;p&gt;Any good guides that you can share?&lt;/p&gt; &lt;p&gt;Any particular ‚Äùthis is how I would do it‚Äù suggestions are more than welcome also! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AI-On-A-Dime"&gt; /u/AI-On-A-Dime &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrn9it/start_finetuning_guidance_needed/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrn9it/start_finetuning_guidance_needed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mrn9it/start_finetuning_guidance_needed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T06:42:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1mr2i67</id>
    <title>Prompt Engineering: What Actually Works (Without the 8-Hour Hype)</title>
    <updated>2025-08-15T16:18:15+00:00</updated>
    <author>
      <name>/u/Prashant-Lakhera</name>
      <uri>https://old.reddit.com/user/Prashant-Lakhera</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve seen people drop 8-hour-long videos on prompt engineering, and honestly, my reaction is ü§¶‚Äç‚ôÇÔ∏è.&lt;/p&gt; &lt;p&gt;I won‚Äôt bore you with the obvious stuff or overcomplicate things. Instead, I want to share a few practical techniques that actually helped me write better prompts, some common sense, some hard-earned lessons. Most of what I‚Äôm sharing comes from the book Hands-On Large Language Models &lt;/p&gt; &lt;p&gt;So here‚Äôs what I‚Äôve learned that actually works:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Specificity&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;This one seems obvious, but it‚Äôs also the most commonly missed.&lt;/p&gt; &lt;p&gt;A vague prompt gives you a vague answer. The more precise you are about your goal, format, and constraints, the better the result.&lt;/p&gt; &lt;p&gt;Bad Prompt:&lt;/p&gt; &lt;p&gt;&lt;code&gt;Write something about climate change.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Good Prompt:&lt;/p&gt; &lt;p&gt;&lt;code&gt;Write a 100-word summary on how climate change affects sea levels, using simple language for a high school audience.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;See the difference? Specific inputs = Specific outputs.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Hallucination Guardrail&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;We all know that LLMs hallucinate, they confidently make stuff up.&lt;/p&gt; &lt;p&gt;A surprisingly simple trick: Tell it not to.&lt;/p&gt; &lt;p&gt;Try this prompt:&lt;/p&gt; &lt;p&gt;&lt;code&gt;If you don‚Äôt know the answer, respond with ‚ÄòI don‚Äôt know.‚Äô Don‚Äôt make anything up.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;This becomes really important when you're designing apps or knowledge assistants. It helps reduce the risk of wrong answers.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Order Matters&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;This was a surprise to me and I learned it from the book.&lt;/p&gt; &lt;p&gt;Where you place your instruction in a long prompt matters. Either put it right at the start or at the end. LLMs often forget what‚Äôs in the middle (especially in long prompts).&lt;/p&gt; &lt;p&gt;Example:&lt;/p&gt; &lt;p&gt;Here's a paragraph. Also here's a use case. Here's some random info. Now summarize.&lt;/p&gt; &lt;p&gt;&lt;code&gt;Summarize the following paragraph:&amp;quot; [then the content]&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Simple shift, big difference.&lt;/p&gt; &lt;p&gt;Other Techniques That Help Me Daily&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Persona:&lt;/p&gt; &lt;p&gt;Set the role clearly.&lt;/p&gt; &lt;p&gt;&lt;code&gt;You are an expert Python developer who writes clean code.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;This changes the behavior completely.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Audience Awareness:&lt;/p&gt; &lt;p&gt;My favorite when I want to simplify things.&lt;/p&gt; &lt;p&gt;&lt;code&gt;Explain this like I‚Äôm five.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Works brilliantly for breaking down tough concepts.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Tone:&lt;/p&gt; &lt;p&gt;Underrated but essential.&lt;/p&gt; &lt;p&gt;Want a formal reply? &lt;/p&gt; &lt;p&gt;&lt;code&gt;Write this in a professional tone for a client. vs Make this sound like I‚Äôm texting a friend.&lt;/code&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Instruction / Context:&lt;/p&gt; &lt;p&gt;Always useful.&lt;/p&gt; &lt;p&gt;&lt;code&gt;Summarize the following news article in bullet points.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Gives the model direction and expected output format.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Grammar Fixing:&lt;/p&gt; &lt;p&gt;As a non-native English speaker, this one‚Äôs gold for me.&lt;/p&gt; &lt;p&gt;&lt;code&gt;Fix the grammar and make it sound more natural.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;It has helped me immensely in writing better content, emails, blogs, even this post :-) &lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;These are the techniques I use regularly. If you have your own prompt engineering hacks, I‚Äôd love to hear them, drop them in the comments!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prashant-Lakhera"&gt; /u/Prashant-Lakhera &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr2i67/prompt_engineering_what_actually_works_without/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr2i67/prompt_engineering_what_actually_works_without/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mr2i67/prompt_engineering_what_actually_works_without/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T16:18:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqy0b1</id>
    <title>AI startup Cohere valued at $6.8 billion in latest fundraising, hires Meta exec</title>
    <updated>2025-08-15T13:34:18+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqy0b1/ai_startup_cohere_valued_at_68_billion_in_latest/"&gt; &lt;img alt="AI startup Cohere valued at $6.8 billion in latest fundraising, hires Meta exec" src="https://external-preview.redd.it/lmgG1KIrrSyLFv85NNJsP33J5KztZGiIWLsgvd8Qf8U.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e44aaba15ee33b78856b227fc344e124b981e1b3" title="AI startup Cohere valued at $6.8 billion in latest fundraising, hires Meta exec" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Why does Cohere fly under the radar. They don't seem to do much marketing and they are not discussed much on LocalLLaMA any more.&lt;/p&gt; &lt;p&gt;They made a splash with Command R and R+. Later also released Command A.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reuters.com/business/ai-startup-cohere-valued-68-billion-latest-fundraising-hires-meta-exec-2025-08-14/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqy0b1/ai_startup_cohere_valued_at_68_billion_in_latest/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mqy0b1/ai_startup_cohere_valued_at_68_billion_in_latest/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T13:34:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1mquhdc</id>
    <title>‚ÄúMind the Gap‚Äù shows the first practical backdoor attack on GGUF quantization</title>
    <updated>2025-08-15T10:59:53+00:00</updated>
    <author>
      <name>/u/juanviera23</name>
      <uri>https://old.reddit.com/user/juanviera23</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; Researchers claim the &lt;em&gt;first&lt;/em&gt; successful backdoor attack that specifically targets &lt;strong&gt;GGUF&lt;/strong&gt; quantization. They show you can make a benign FP model look clean, but after quantization to GGUF it exhibits malicious behavior (e.g., insecure code gen jumps by &lt;strong&gt;+88.7%&lt;/strong&gt; in their tests). This directly concerns anyone who downloads random GGUFs for llama.cpp/Ollama.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/juanviera23"&gt; /u/juanviera23 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.arxiv.org/pdf/2505.23786"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mquhdc/mind_the_gap_shows_the_first_practical_backdoor/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mquhdc/mind_the_gap_shows_the_first_practical_backdoor/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T10:59:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1mr4fdk</id>
    <title>Qwen 2.5 (7B/14B/32B) Finetunes Outperforming Opus 4 &amp; Sonnet 4/3.5 on Out-of-Distribution Tasks with RL --- Code, Weights, Data, and Paper Released</title>
    <updated>2025-08-15T17:27:21+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr4fdk/qwen_25_7b14b32b_finetunes_outperforming_opus_4/"&gt; &lt;img alt="Qwen 2.5 (7B/14B/32B) Finetunes Outperforming Opus 4 &amp;amp; Sonnet 4/3.5 on Out-of-Distribution Tasks with RL --- Code, Weights, Data, and Paper Released" src="https://preview.redd.it/3beo5klvv7jf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2dd6143a597d6b0048011fe35125ed52dd343f90" title="Qwen 2.5 (7B/14B/32B) Finetunes Outperforming Opus 4 &amp;amp; Sonnet 4/3.5 on Out-of-Distribution Tasks with RL --- Code, Weights, Data, and Paper Released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3beo5klvv7jf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr4fdk/qwen_25_7b14b32b_finetunes_outperforming_opus_4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mr4fdk/qwen_25_7b14b32b_finetunes_outperforming_opus_4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T17:27:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1mrp54l</id>
    <title>How do you all discover new models?</title>
    <updated>2025-08-16T08:24:24+00:00</updated>
    <author>
      <name>/u/wh33t</name>
      <uri>https://old.reddit.com/user/wh33t</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm currently trying to search Huggingface to find a model that is around 70B, has thinking built in, and is a mixture of experts. I am surprised that I can't easily select these features during the search. All that is available is the parameter count.&lt;/p&gt; &lt;p&gt;I'm feeling a bit baffled that I can't seem to figure out a way to easily search for models using a series of filters like this. &lt;/p&gt; &lt;p&gt;Am I just blind and missing something obvious? Is there a much better method for shopping for new models? Another service perhaps?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wh33t"&gt; /u/wh33t &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrp54l/how_do_you_all_discover_new_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrp54l/how_do_you_all_discover_new_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mrp54l/how_do_you_all_discover_new_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T08:24:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1mr6929</id>
    <title>Intel adds Shared GPU Memory Override feature for Core Ultra systems, enables larger VRAM for AI</title>
    <updated>2025-08-15T18:33:10+00:00</updated>
    <author>
      <name>/u/reps_up</name>
      <uri>https://old.reddit.com/user/reps_up</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr6929/intel_adds_shared_gpu_memory_override_feature_for/"&gt; &lt;img alt="Intel adds Shared GPU Memory Override feature for Core Ultra systems, enables larger VRAM for AI" src="https://external-preview.redd.it/YJQ41TIHjSIHRPnnPYpoNGj-_TlQpYrRQFRV8JkhNlo.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4dc9954302a8136bd445b302a8ce0f2c5e742b5e" title="Intel adds Shared GPU Memory Override feature for Core Ultra systems, enables larger VRAM for AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/reps_up"&gt; /u/reps_up &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://videocardz.com/newz/intel-adds-shared-gpu-memory-override-feature-for-core-ultra-systems-enables-larger-vram-for-ai"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr6929/intel_adds_shared_gpu_memory_override_feature_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mr6929/intel_adds_shared_gpu_memory_override_feature_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T18:33:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1mr8rfh</id>
    <title>Analysis on hyped Hierarchical Reasoning Model (HRM) by ARC-AGI foundation</title>
    <updated>2025-08-15T20:06:07+00:00</updated>
    <author>
      <name>/u/Snoo_64233</name>
      <uri>https://old.reddit.com/user/Snoo_64233</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr8rfh/analysis_on_hyped_hierarchical_reasoning_model/"&gt; &lt;img alt="Analysis on hyped Hierarchical Reasoning Model (HRM) by ARC-AGI foundation" src="https://preview.redd.it/30drwal3p8jf1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=587da208e46dce9fd645a884879872736fb35f43" title="Analysis on hyped Hierarchical Reasoning Model (HRM) by ARC-AGI foundation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://arcprize.org/blog/hrm-analysis"&gt;ARC AGI analysis&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Snoo_64233"&gt; /u/Snoo_64233 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/30drwal3p8jf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr8rfh/analysis_on_hyped_hierarchical_reasoning_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mr8rfh/analysis_on_hyped_hierarchical_reasoning_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T20:06:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1mrm7oz</id>
    <title>LMArena‚Äôs leaderboard can be misleading</title>
    <updated>2025-08-16T05:47:59+00:00</updated>
    <author>
      <name>/u/Beneficial_Tough_367</name>
      <uri>https://old.reddit.com/user/Beneficial_Tough_367</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;LMArena‚Äôs leaderboard can be misleading: new models with fewer votes (e.g. GPT-5) can top the chart before scores stabilize, while older models (e.g. Gemini) are based on much larger and more robust sample sizes.&lt;/p&gt; &lt;p&gt;I think we need a ‚Äúmatched sample‚Äù ranking, only compare models based on their last N votes, to get a fair picture. Otherwise, the leaderboard is systematically biased.&lt;/p&gt; &lt;p&gt;Thoughts?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Beneficial_Tough_367"&gt; /u/Beneficial_Tough_367 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrm7oz/lmarenas_leaderboard_can_be_misleading/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrm7oz/lmarenas_leaderboard_can_be_misleading/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mrm7oz/lmarenas_leaderboard_can_be_misleading/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T05:47:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1mroal8</id>
    <title>Huihui-gpt-oss-120b-BF16-abliterated</title>
    <updated>2025-08-16T07:36:52+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mroal8/huihuigptoss120bbf16abliterated/"&gt; &lt;img alt="Huihui-gpt-oss-120b-BF16-abliterated" src="https://external-preview.redd.it/Whbl3EQ8tzvwyKl63iWfJrIBTWW6XBRLW7AQQgHk37I.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6a5c4cc8d89ef3ca3df1e29ec46225752e44231a" title="Huihui-gpt-oss-120b-BF16-abliterated" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/huihui-ai/Huihui-gpt-oss-120b-BF16-abliterated/tree/main"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mroal8/huihuigptoss120bbf16abliterated/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mroal8/huihuigptoss120bbf16abliterated/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T07:36:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1mrcgcr</id>
    <title>Rival Ryzen AI Max+ 395 Mini PC 96GB for $1479.</title>
    <updated>2025-08-15T22:25:13+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrcgcr/rival_ryzen_ai_max_395_mini_pc_96gb_for_1479/"&gt; &lt;img alt="Rival Ryzen AI Max+ 395 Mini PC 96GB for $1479." src="https://external-preview.redd.it/1uympFuPK52czHQ4IvmWhNt0vnP2FK278N3meDCoUq4.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=59d72cc8f3a3871e1aa7795e415e94129d9d9c61" title="Rival Ryzen AI Max+ 395 Mini PC 96GB for $1479." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is yet another AMD Max+ 395 machine. This is unusual in that it's 96GB instead of 64GB or 128GB. At $1479 though, it's the same price as other's 64GB machines but gives you 96GB instead.&lt;/p&gt; &lt;p&gt;It looks to use the same Sixunited MB as other Max+ machines like the GMK X2 right down to the red color of the MB.&lt;/p&gt; &lt;p&gt;Update: I ran across a video of this machine being built.&lt;/p&gt; &lt;p&gt;&lt;a href="https://youtu.be/3esEHgoymCY"&gt;https://youtu.be/3esEHgoymCY&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x-plus.store/products/xrival"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrcgcr/rival_ryzen_ai_max_395_mini_pc_96gb_for_1479/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mrcgcr/rival_ryzen_ai_max_395_mini_pc_96gb_for_1479/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T22:25:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1mr7m2r</id>
    <title>LM Studio now supports llama.cpp CPU offload for MoE which is awesome</title>
    <updated>2025-08-15T19:23:30+00:00</updated>
    <author>
      <name>/u/carlosedp</name>
      <uri>https://old.reddit.com/user/carlosedp</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr7m2r/lm_studio_now_supports_llamacpp_cpu_offload_for/"&gt; &lt;img alt="LM Studio now supports llama.cpp CPU offload for MoE which is awesome" src="https://b.thumbs.redditmedia.com/8_UCLmbk5AUNXfDHLBVN5lWhMbgV6ZR8UC3ks8DeLuE.jpg" title="LM Studio now supports llama.cpp CPU offload for MoE which is awesome" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Now LM Studio (from 0.3.23 build 3) supports llama.cpp &lt;code&gt;--cpu-moe&lt;/code&gt; which allows offloading the MoE weights to the CPU leaving the GPU VRAM for layer offload.&lt;/p&gt; &lt;p&gt;Using Qwen3 30B (both thinking and instruct) on a 64GB Ryzen 7 and a RTX3070 with 8GB VRAM I've been able to use 16k context and fully offload the model's layers to GPU and got about 15 tok/s which is amazing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/carlosedp"&gt; /u/carlosedp &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mr7m2r"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr7m2r/lm_studio_now_supports_llamacpp_cpu_offload_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mr7m2r/lm_studio_now_supports_llamacpp_cpu_offload_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T19:23:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1mr6sdc</id>
    <title>Jedi code Gemma 27v vs 270m</title>
    <updated>2025-08-15T18:52:56+00:00</updated>
    <author>
      <name>/u/Skystunt</name>
      <uri>https://old.reddit.com/user/Skystunt</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr6sdc/jedi_code_gemma_27v_vs_270m/"&gt; &lt;img alt="Jedi code Gemma 27v vs 270m" src="https://preview.redd.it/4icjlje4c8jf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cecea0a1e7e78f6cb9509e2f3f9a7433184fb5a5" title="Jedi code Gemma 27v vs 270m" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Gemma 3 270m coding a jedi in existence&lt;/p&gt; &lt;p&gt;Quite interesting how bad the small model is to following instructions, this is the first semblence to doing what i said.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Skystunt"&gt; /u/Skystunt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4icjlje4c8jf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr6sdc/jedi_code_gemma_27v_vs_270m/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mr6sdc/jedi_code_gemma_27v_vs_270m/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T18:52:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1mrbtqt</id>
    <title>DINOv3 visualization tool running 100% locally in your browser on WebGPU/WASM</title>
    <updated>2025-08-15T22:00:47+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrbtqt/dinov3_visualization_tool_running_100_locally_in/"&gt; &lt;img alt="DINOv3 visualization tool running 100% locally in your browser on WebGPU/WASM" src="https://external-preview.redd.it/dm1scXBiZnU4OWpmMbd7l6YK9EDz0b8q8nzrd_PHLYbyTzK6nb4d-_lrl57d.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a680cf7593e65adcab4110d0090bab480e862303" title="DINOv3 visualization tool running 100% locally in your browser on WebGPU/WASM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;DINOv3 released yesterday, a new state-of-the-art vision backbone trained to produce rich, dense image features. I loved their demo video so much that I decided to re-create their visualization tool. &lt;/p&gt; &lt;p&gt;Everything runs locally in your browser with Transformers.js, using WebGPU if available and falling back to WASM if not. Hope you like it! &lt;/p&gt; &lt;p&gt;Link to demo + source code: &lt;a href="https://huggingface.co/spaces/webml-community/dinov3-web"&gt;https://huggingface.co/spaces/webml-community/dinov3-web&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/yhe3jbfu89jf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrbtqt/dinov3_visualization_tool_running_100_locally_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mrbtqt/dinov3_visualization_tool_running_100_locally_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T22:00:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1mrlpxd</id>
    <title>My project allows you to use the OpenAI API without an API Key (through your ChatGPT account)</title>
    <updated>2025-08-16T05:21:33+00:00</updated>
    <author>
      <name>/u/FunConversation7257</name>
      <uri>https://old.reddit.com/user/FunConversation7257</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrlpxd/my_project_allows_you_to_use_the_openai_api/"&gt; &lt;img alt="My project allows you to use the OpenAI API without an API Key (through your ChatGPT account)" src="https://external-preview.redd.it/Os4oYZsYLVlsXnga3hPOUAlxvPVzcyCPA6N9lZAIVyQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b7ea802471412bf40b6e93f29c186991e9a7c4e2" title="My project allows you to use the OpenAI API without an API Key (through your ChatGPT account)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/7qr019kqgbjf1.png?width=671&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=34985293292691f5bd4067ed3297e5fdaf6f0174"&gt;https://preview.redd.it/7qr019kqgbjf1.png?width=671&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=34985293292691f5bd4067ed3297e5fdaf6f0174&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Recently, Codex, OpenAI's coding CLI released a way to authenticate with your ChatGPT account, and use that for usage instead of api keys. I dug through the code and saw that by using Codex CLI, you can login with your account and send requests right to OpenAI, albeit restricted by slightly tougher rate limits than on the ChatGPT app.&lt;/p&gt; &lt;p&gt;However, still was decent enough for my use case, so I made a python script which allows one to login with their ChatGPT account, and then serve a OpenAI compatible endpoint you can use programmatically or via a chat app of your choice.&lt;br /&gt; Might be useful for you too for data analysis, or just chatting in a better app than the ChatGPT desktop app. It's also customisable with thinking effort, and even sends back thinking summaries, and can use tools.&lt;/p&gt; &lt;p&gt;Not strictly &amp;quot;local&amp;quot;, but brought that 2023 vibe back, and thought it was kinda cool.&lt;/p&gt; &lt;p&gt;Will try to make it a better package soon than just python files.&lt;br /&gt; Github link: &lt;a href="https://github.com/RayBytes/ChatMock"&gt;https://github.com/RayBytes/ChatMock&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FunConversation7257"&gt; /u/FunConversation7257 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrlpxd/my_project_allows_you_to_use_the_openai_api/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrlpxd/my_project_allows_you_to_use_the_openai_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mrlpxd/my_project_allows_you_to_use_the_openai_api/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T05:21:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1mrfqsd</id>
    <title>Epoch AI data shows that on benchmarks, local LLMs only lag the frontier by about 9 months</title>
    <updated>2025-08-16T00:40:29+00:00</updated>
    <author>
      <name>/u/timfduffy</name>
      <uri>https://old.reddit.com/user/timfduffy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrfqsd/epoch_ai_data_shows_that_on_benchmarks_local_llms/"&gt; &lt;img alt="Epoch AI data shows that on benchmarks, local LLMs only lag the frontier by about 9 months" src="https://preview.redd.it/kbdu3pyq1ajf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e6766455308e18a9b20204df7a38e2406f44eff0" title="Epoch AI data shows that on benchmarks, local LLMs only lag the frontier by about 9 months" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/timfduffy"&gt; /u/timfduffy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kbdu3pyq1ajf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrfqsd/epoch_ai_data_shows_that_on_benchmarks_local_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mrfqsd/epoch_ai_data_shows_that_on_benchmarks_local_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T00:40:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjf5ol</id>
    <title>r/LocalLlama is looking for moderators</title>
    <updated>2025-08-06T20:06:34+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/application/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T20:06:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
