<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-03-01T07:54:43+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1rguty0</id>
    <title>Get your local models in order. Anthropic just got "dislike" from the US government.</title>
    <updated>2026-02-28T06:01:55+00:00</updated>
    <author>
      <name>/u/FPham</name>
      <uri>https://old.reddit.com/user/FPham</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rguty0/get_your_local_models_in_order_anthropic_just_got/"&gt; &lt;img alt="Get your local models in order. Anthropic just got &amp;quot;dislike&amp;quot; from the US government." src="https://preview.redd.it/p1uxufobl6mg1.png?width=140&amp;amp;height=32&amp;amp;auto=webp&amp;amp;s=75fde52d15f964724cf5a1209d7fee5c4dc8bc21" title="Get your local models in order. Anthropic just got &amp;quot;dislike&amp;quot; from the US government." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anthropic in a panic mode. Yeah as things look RN OpenAI+US government are on the war path to bring Anthropic to its knees. I mean blacklisting it...&lt;/p&gt; &lt;p&gt;Would Anthropic's fall be good or bad for us?&lt;/p&gt; &lt;p&gt;Is the next step: &amp;quot;Use of any Chinese models is strictly prohibited...&amp;quot; ?&lt;/p&gt; &lt;p&gt;Also if the blacklisting by DoW (&amp;quot;no contractor, supplier, or partner that does business with the United States military may conduct any commercial activity with Anthropic&amp;quot;) is being taken seriously, that means AWS and other cloud backbones of Anthropic would then take their hands off, letting Anthropic dry in th air, no?&lt;/p&gt; &lt;p&gt;They (Anthropic) are though in a panic mode rn.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/p1uxufobl6mg1.png?width=1262&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=807cb81fb92e2fffa74079fcdf57846719f78e72"&gt;https://preview.redd.it/p1uxufobl6mg1.png?width=1262&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=807cb81fb92e2fffa74079fcdf57846719f78e72&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FPham"&gt; /u/FPham &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rguty0/get_your_local_models_in_order_anthropic_just_got/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rguty0/get_your_local_models_in_order_anthropic_just_got/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rguty0/get_your_local_models_in_order_anthropic_just_got/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-28T06:01:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1rhjcvo</id>
    <title>What I'm doing locally - Develping an MCP to attach to your Game Engine</title>
    <updated>2026-03-01T00:53:01+00:00</updated>
    <author>
      <name>/u/frosticecold</name>
      <uri>https://old.reddit.com/user/frosticecold</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Howdy folks, I'm experimenting developing an MCP to attach to Game Engines so you can expose the game internals and control/augment it with AI.&lt;/p&gt; &lt;p&gt;Currently I have it integrated with DOOM (via crispy doom or zdoom)&lt;/p&gt; &lt;p&gt;My idea was: How can I take an old game, and make it /refreshed/ with AI? Came to conclusion, let an AI agent be it's &amp;quot;Game Master&amp;quot;&lt;/p&gt; &lt;p&gt;Here is a demo running Crispy Doom, Shareware Doom 1 wad and Qwen3 30b a3b&lt;br /&gt; I will try to make this open source soon (with a release for you guys to have some fun)&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1rhjcvo/video/i16o23530cmg1/player"&gt;https://reddit.com/link/1rhjcvo/video/i16o23530cmg1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/frosticecold"&gt; /u/frosticecold &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rhjcvo/what_im_doing_locally_develping_an_mcp_to_attach/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rhjcvo/what_im_doing_locally_develping_an_mcp_to_attach/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rhjcvo/what_im_doing_locally_develping_an_mcp_to_attach/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-03-01T00:53:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1rh69co</id>
    <title>Multi-Directional Refusal Suppression with Self-Organizing Maps - Pull Request into heretic!</title>
    <updated>2026-02-28T16:01:19+00:00</updated>
    <author>
      <name>/u/kabachuha</name>
      <uri>https://old.reddit.com/user/kabachuha</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TL;DR: The first technique that pushed gpt-oss-20b to 3 refusals from 100 while keeping KL of 0.12, and oss-120b to 7/100 while having KL 0.22!&lt;/p&gt; &lt;p&gt;Previous work assumed refusal behavior to be encoded as a single direction in the model's latent space; e.g., computed as the difference between the centroids of harmful and harmless prompt representations. However, emerging evidence suggests that concepts in LLMs often appear to be encoded as a low-dimensional manifold embedded in the high-dimensional latent space. Just like numbers and days of week are encoded in circles or helices, in recent advanced neural networks like GPT-OSS refusals are becoming ingrained in complex multi-directional clusters and one-directional ablation is not enough to get rid of the refusal reasoning. This &lt;a href="https://huggingface.co/Magic-Decensored/Apriel-1.6-15b-Thinker-Magic_beta-decensored-GGUF"&gt;HF model&lt;/a&gt;, which has applied my implemented PR, has an awesome visualization of refusal clusterization.&lt;/p&gt; &lt;p&gt;Now that we cannot use simple ablation, is it over? It is not. Researchers from the &lt;a href="https://arxiv.org/abs/2511.08379v2"&gt;Universities of Cagliari and Genova&lt;/a&gt; invented a new method. They &lt;em&gt;train a self-organizing neural network&lt;/em&gt; on the hidden states to &lt;em&gt;determine this manifold&lt;/em&gt;. After it, the K most important neurons are selected and turned into refusal directions, compressing this manifold towards the harmless zone, making them equivalent in a fine-grained manner instead of a one-fits-all lobotomy. So yes, we have neural networks fighting against the other neural networks. The final export of abliteration is baked into the model's weights, no modules needed.&lt;/p&gt; &lt;p&gt;I, and the community are already testing this algorithm on models such as GPT-OSS, Qwen and Apriel, and we are getting unbelievable results. With enabling the newer norm-preserving biprojected abliteration as well, as it stacks greatly.&lt;/p&gt; &lt;p&gt;So far, I pushed gemma3-12b to 3/100 and 0.08 KL, gpt-oss-20b to 3/100 and 0.12 KL, gpt-oss-120b to 7/100 and 0.22 KL (lowest KL for &amp;lt; 20 refusals I found on HF), Qwen3 4b to 3/100 and 0.08 KL, and the community pushed Qwen3.5 27b to 18/100 refusals and KL of 0.028, and Apriel-Thinker to 11/100 refusals and 0.005 KL. (Note, the base versions have 97+/100) Read &lt;a href="https://github.com/p-e-w/heretic/pull/196#issuecomment-3974974202"&gt;the comparison table&lt;/a&gt; in the pull request for more details.&lt;/p&gt; &lt;p&gt;Subjective evaluation on gpt-oss-120b: The model has a slight DID, for the better. For example, it will recite the safety policy and &lt;strong&gt;agree&lt;/strong&gt; with that it is allowed to give you the pipe bomb recipe. After agreement in the reasoning, it gives the recipe just as asked and even an attack plan. It distorts the meaning of safety in &amp;quot;yours&amp;quot; safety, so it makes sure you will survive the attack. In the end it gives generic safety and legality advice, but no refusal. Qwen3 is more than eager to give you drug recipes. Even for gpt-oss, NSFW and profanity are vivid and not sanitized as in the other oss-abliterates I tested. Benchmarks are yet to be measures, waiting for the UGI evaluation.&lt;/p&gt; &lt;p&gt;My &lt;a href="https://huggingface.co/kabachuha/gpt-oss-20b-SOMbliterated"&gt;GPT-OSS-20b&lt;/a&gt; and &lt;a href="https://huggingface.co/kabachuha/Qwen3-4B-Instruct-2507-SOMbliterated"&gt;Qwen3-4b&lt;/a&gt; are already uploaded on Huggingface if someone would like to test. Unfortunately, because I got out of memory when merging LoRA, I need some more tests to ensure gpt-oss-120b is not corrupted, so I invite you to do your own abliterates. For 120b, it takes 1 h 5 m on a single H100 to do 400 trials. (make sure you have enough RAM to dequantize it when merging!) The training time for the self-organizing networks is negligible and it takes &amp;lt; 30-40 seconds to train them all for the transformer layers.&lt;/p&gt; &lt;p&gt;This implementation is based on the awesome work &lt;a href="https://arxiv.org/abs/2511.08379v2"&gt;https://arxiv.org/abs/2511.08379v2&lt;/a&gt; by Giorgio Piras and Raffaele Mura et al. I also thank p-e-w (heretic) and the norm-preserving biprojected abliteration authors for their contributions.&lt;/p&gt; &lt;p&gt;The link to the Pull Request: &lt;a href="https://github.com/p-e-w/heretic/pull/196"&gt;https://github.com/p-e-w/heretic/pull/196&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kabachuha"&gt; /u/kabachuha &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rh69co/multidirectional_refusal_suppression_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rh69co/multidirectional_refusal_suppression_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rh69co/multidirectional_refusal_suppression_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-28T16:01:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1rhmepa</id>
    <title>Qwen3.5-122B on Blackwell SM120: fp8 KV cache silently corrupts output, bf16 required — 1,985 tok/s burst, MTP 2.75x</title>
    <updated>2026-03-01T03:17:58+00:00</updated>
    <author>
      <name>/u/awwwyeah206</name>
      <uri>https://old.reddit.com/user/awwwyeah206</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The most useful finding first: &lt;strong&gt;fp8_e4m3 KV cache on Qwen3.5-122B doesn’t crash — it silently produces corrupt output.&lt;/strong&gt; No error, no warning. Just exclamation marks and repetition instead of answers. I did not observe the same failure in my earlier M2.5 testing, though that run used a different SGLang build. The only way to catch it is by checking output quality. &lt;strong&gt;bf16 KV fixes it.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This is a follow-up to my earlier M2.5 benchmarks on the same hardware. I’ve been characterizing model bring-up on &lt;strong&gt;8x RTX PRO 6000 Blackwell (SM120, AWS g7e.48xlarge)&lt;/strong&gt; with SGLang so others can avoid blind alleys on this platform.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;DeltaNet adds constraints that standard MoE models don’t have.&lt;/strong&gt; M2.5 needed 2 Triton backend flags on SM120. Qwen3.5-122B needed 6 in this setup: attention backend forced to Triton (DeltaNet layers), KV cache forced to bf16 (fp8 corrupts), no CUDA graphs (Triton SMEM overflow), and no HiCache (DeltaNet incompatible). Of the optimization paths I tested, &lt;strong&gt;MTP was the only one that materially improved performance: 2.75x single-request speedup (~9 to ~25 tok/s).&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Numbers (same hardware, same methodology):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Burst tok/s:&lt;/strong&gt; 1,985 vs 1,818&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Online 4 rps:&lt;/strong&gt; 310 vs 404&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Online 8 rps:&lt;/strong&gt; 514 vs 744&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Single-request tok/s:&lt;/strong&gt; ~25 (MTP) vs 72&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Arena-Hard quality*:&lt;/strong&gt; 6.99/10 vs 4.94/10&lt;/li&gt; &lt;li&gt;&lt;strong&gt;SM120 optimizations available:&lt;/strong&gt; MTP only vs FP8 KV + CUDA graphs + HiCache&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;*Arena-Hard here was judged by &lt;strong&gt;Claude Opus 4.6&lt;/strong&gt;, not GPT-4, so these scores are &lt;strong&gt;not comparable to leaderboard results&lt;/strong&gt;. The same judge was used for both models.&lt;/p&gt; &lt;p&gt;In my tests, Qwen3.5-122B wins on &lt;strong&gt;burst throughput and quality&lt;/strong&gt;. M2.5 still wins on &lt;strong&gt;every sustained serving metric&lt;/strong&gt;, largely because DeltaNet blocks the optimizations that make M2.5 fast on this hardware (FP8 KV, CUDA graphs, HiCache).&lt;/p&gt; &lt;p&gt;Full results, compatibility matrix, exact repro commands, and all JSONL artifacts:&lt;br /&gt; &lt;a href="https://github.com/sgl-project/sglang/issues/19603"&gt;https://github.com/sgl-project/sglang/issues/19603&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hardware: AWS g7e.48xlarge, SGLang nightly (cu13 20260219), TP=8.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/awwwyeah206"&gt; /u/awwwyeah206 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rhmepa/qwen35122b_on_blackwell_sm120_fp8_kv_cache/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rhmepa/qwen35122b_on_blackwell_sm120_fp8_kv_cache/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rhmepa/qwen35122b_on_blackwell_sm120_fp8_kv_cache/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-03-01T03:17:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1rhe790</id>
    <title>My frends trained and benchmarked 4 diffusion model versions entirely on an RTX 2050 (4GB VRAM) — the 17.8M model beat the 143.8M one</title>
    <updated>2026-02-28T21:13:13+00:00</updated>
    <author>
      <name>/u/zemondza</name>
      <uri>https://old.reddit.com/user/zemondza</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rhe790/my_frends_trained_and_benchmarked_4_diffusion/"&gt; &lt;img alt="My frends trained and benchmarked 4 diffusion model versions entirely on an RTX 2050 (4GB VRAM) — the 17.8M model beat the 143.8M one" src="https://preview.redd.it/19soue2hwamg1.png?width=140&amp;amp;height=140&amp;amp;crop=1:1,smart&amp;amp;auto=webp&amp;amp;s=b40a0fbba47f6a1c1d5ec3d73f2243261110acab" title="My frends trained and benchmarked 4 diffusion model versions entirely on an RTX 2050 (4GB VRAM) — the 17.8M model beat the 143.8M one" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zemondza"&gt; /u/zemondza &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1rhe790"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rhe790/my_frends_trained_and_benchmarked_4_diffusion/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rhe790/my_frends_trained_and_benchmarked_4_diffusion/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-28T21:13:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1rh0xwk</id>
    <title>Unsloth Dynamic 2.0 GGUFs now selectively quantizes layers much more intelligently and extensively.</title>
    <updated>2026-02-28T12:03:25+00:00</updated>
    <author>
      <name>/u/paranoidray</name>
      <uri>https://old.reddit.com/user/paranoidray</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rh0xwk/unsloth_dynamic_20_ggufs_now_selectively/"&gt; &lt;img alt="Unsloth Dynamic 2.0 GGUFs now selectively quantizes layers much more intelligently and extensively." src="https://external-preview.redd.it/ksRJC2bKGwjrMfOqsioi-B4oIm5QWQUM7Vf03KwieGM.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5f6fc5d8f727ab6f86a8ca5f94a5091bbe81d025" title="Unsloth Dynamic 2.0 GGUFs now selectively quantizes layers much more intelligently and extensively." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paranoidray"&gt; /u/paranoidray &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://unsloth.ai/docs/basics/unsloth-dynamic-2.0-ggufs"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rh0xwk/unsloth_dynamic_20_ggufs_now_selectively/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rh0xwk/unsloth_dynamic_20_ggufs_now_selectively/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-28T12:03:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1rhlosn</id>
    <title>microgpt</title>
    <updated>2026-03-01T02:42:51+00:00</updated>
    <author>
      <name>/u/johnnyApplePRNG</name>
      <uri>https://old.reddit.com/user/johnnyApplePRNG</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/johnnyApplePRNG"&gt; /u/johnnyApplePRNG &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://karpathy.github.io/2026/02/12/microgpt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rhlosn/microgpt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rhlosn/microgpt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-03-01T02:42:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1rh802w</id>
    <title>What if LLM agents passed KV-cache to each other instead of text? I tried it -- 73-78% token savings across Qwen, Llama, and DeepSeek</title>
    <updated>2026-02-28T17:10:16+00:00</updated>
    <author>
      <name>/u/proggmouse</name>
      <uri>https://old.reddit.com/user/proggmouse</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you've used multi-agent setups with LangChain, CrewAI, AutoGen, or Swarm, you've probably noticed: every agent re-tokenizes and re-processes the full conversation from scratch. Agent 3 in a 4-agent chain is re-reading everything agents 1 and 2 already chewed through. When I measured this across Qwen2.5, Llama 3.2, and DeepSeek-R1-Distill, &lt;strong&gt;47-53% of all tokens in text mode turned out to be redundant re-processing.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;AVP (Agent Vector Protocol) is my attempt to fix this. Instead of passing text between agents, it passes the KV-cache directly. Agent A finishes reasoning serializes its key-value attention states, and Agent B injects them. No re-tokenization, no redundant forward passes.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Text: Planner -&amp;gt; [text] -&amp;gt; Critic re-tokenizes everything -&amp;gt; [text] -&amp;gt; Refiner re-tokenizes everything Latent: Planner -&amp;gt; [KV-cache] -&amp;gt; Critic injects, skips to generation -&amp;gt; [KV-cache] -&amp;gt; Refiner same &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;What it actually does:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Same model on both sides? Direct KV-cache transfer, zero overhead.&lt;/li&gt; &lt;li&gt;Same family, different size (e.g. Qwen2.5-7B talking to 1.5B)? Vocabulary-mediated projection. No learned params, no calibration data needed.&lt;/li&gt; &lt;li&gt;Different families? Falls back to JSON. Not everything needs to be fancy.&lt;/li&gt; &lt;li&gt;Transport-agnostic -- works alongside A2A, MCP, gRPC, whatever you're already using&lt;/li&gt; &lt;li&gt;Binary wire format, not JSON+Base64 (33% overhead on tensor data is painful)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Numbers (these are structural, not accuracy claims):&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Token savings of 73-78% and 2-4x speedups held consistent across all three model families. This isn't model-dependent -- it's just fewer forward passes, so less wall time. Here's the intuition: text prompt sizes balloon at each hop (186 -&amp;gt; 545 -&amp;gt; 1,073 -&amp;gt; 1,397 tokens in a 4-agent GSM8K chain). Latent stays flat at ~164-207 tokens per hop because prior context arrives as pre-computed KV-cache, not as text that needs re-encoding.&lt;/p&gt; &lt;p&gt;The gap widens with chain length. At 4 agents it's roughly 2x. At 16 agents (projected) it'd be around 6x, because text scales O(n^2) while latent scales O(n).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Limitations (yes, I know about these):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Sample sizes are n=20 per model. The token and speed numbers are solid because they're structural (fewer forward passes is fewer forward passes), but n=20 isn't enough to make accuracy claims. That's future work.&lt;/li&gt; &lt;li&gt;Tested on small models only (1.5B-3B on an RTX 3070 Ti). 7B+ results pending.&lt;/li&gt; &lt;li&gt;This is a datacenter / same-machine thing. KV-cache for a 3B model runs about 130 MB per sample. You need 1 Gbps+ bandwidth minimum. Sending this over the internet is not happening.&lt;/li&gt; &lt;li&gt;Requires KV-cache access, so self-hosted only. Won't work with OpenAI/Anthropic/etc. APIs.&lt;/li&gt; &lt;li&gt;Same model only for now. Cross-model (Rosetta Stone) is implemented but not benchmarked yet.&lt;/li&gt; &lt;li&gt;Latent uses 17-54x more VRAM than text because you're holding KV-cache across hops instead of discarding it. Totally fine for 1.5B-3B on 8GB+ GPUs. At 7B+ it becomes a real constraint, and I don't have a clean answer for that yet.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Try it yourself:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pip install avp &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Two API levels depending on how much control you want:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;import avp msg = avp.pack(&amp;quot;Hello&amp;quot;, model=&amp;quot;Qwen/Qwen2.5-7B-Instruct&amp;quot;, think_steps=20) answer = avp.unpack(msg, model=&amp;quot;Qwen/Qwen2.5-7B-Instruct&amp;quot;) from avp import HuggingFaceConnector connector = HuggingFaceConnector.from_pretrained(&amp;quot;Qwen/Qwen2.5-1.5B-Instruct&amp;quot;) context = connector.think(&amp;quot;Analyze this problem&amp;quot;, steps=20) answer = connector.generate(&amp;quot;Solve it.&amp;quot;, context=context) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;vLLM connector also available (&lt;code&gt;pip install &amp;quot;avp[vllm]&amp;quot;&lt;/code&gt;).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;SDK: &lt;a href="https://github.com/VectorArc/avp-python"&gt;github.com/VectorArc/avp-python&lt;/a&gt; (MIT, 377 tests, 7 benchmarks)&lt;/li&gt; &lt;li&gt;Spec: &lt;a href="https://github.com/VectorArc/avp-spec"&gt;github.com/VectorArc/avp-spec&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Benchmark details: &lt;a href="https://github.com/VectorArc/avp-python/blob/main/docs/BENCHMARKS.md"&gt;BENCHMARKS.md&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This is a nights-and-weekends project born out of my own multi-agent work. Happy to answer questions about the implementation and genuinely interested in feedback from people running multi-agent setups in production.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/proggmouse"&gt; /u/proggmouse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rh802w/what_if_llm_agents_passed_kvcache_to_each_other/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rh802w/what_if_llm_agents_passed_kvcache_to_each_other/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rh802w/what_if_llm_agents_passed_kvcache_to_each_other/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-28T17:10:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1rhjg6w</id>
    <title>LongCat-Flash-Lite 68.5B maybe a relatively good choice for a pure instruct model within the 24GB GPU VRAM constraint.</title>
    <updated>2026-03-01T00:57:12+00:00</updated>
    <author>
      <name>/u/Sad-Pickle4282</name>
      <uri>https://old.reddit.com/user/Sad-Pickle4282</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rhjg6w/longcatflashlite_685b_maybe_a_relatively_good/"&gt; &lt;img alt="LongCat-Flash-Lite 68.5B maybe a relatively good choice for a pure instruct model within the 24GB GPU VRAM constraint." src="https://preview.redd.it/x6xh438e0cmg1.png?width=140&amp;amp;height=75&amp;amp;auto=webp&amp;amp;s=d239994ad05747522c973e3217fed8bd42115ace" title="LongCat-Flash-Lite 68.5B maybe a relatively good choice for a pure instruct model within the 24GB GPU VRAM constraint." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/x6xh438e0cmg1.png?width=817&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bcb36f59882c00352f44fbfc484a37358b6d5fd8"&gt;N-gram in Longcat, arxiv.org/abs/2601.21204&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Meituan released their &lt;a href="http://huggingface.co/meituan-longcat/LongCat-Flash-Lite"&gt;huggingface.co/meituan-longcat/LongCat-Flash-Lite&lt;/a&gt; model two months ago. It is a model whose capability and parameter count are roughly on par with Qwen3-Next-80B-A3B-Instruct. By utilizing N-gram (which can be seen as a predecessor or lightweight version of DeepSeek Engram), it allows the enormous embedding layer (approximately 30B parameters) to run on the CPU, while the attention layers and MoE FFN are executed on the GPU.&lt;/p&gt; &lt;p&gt;Previously, I frequently used their API service at &lt;a href="http://longcat.chat/platform/"&gt;longcat.chat/platform/&lt;/a&gt; to call this model for translating papers and web pages (The model is also available for testing at &lt;a href="http://longcat.chat"&gt;longcat.chat&lt;/a&gt; ). The high speed (400 tokens/s) provided a very good experience. However, local deployment was difficult because Hugging Face only had an MLX version available. But now, I have discovered that InquiringMinds-AI has just produced complete GGUF models (q_3 to q_5) available at &lt;a href="http://huggingface.co/InquiringMinds-AI/LongCat-Flash-Lite-GGUF"&gt;huggingface.co/InquiringMinds-AI/LongCat-Flash-Lite-GGUF&lt;/a&gt; .&lt;/p&gt; &lt;p&gt;The required llama.cpp fork is very easy to compile—it took me less than 10 minutes to get it running locally. On a 4090D, using the Q4_K_M model with q8 KV quantization and 80K context length results in approximately 22.5GB VRAM usage and about 18GB RAM usage. The first few hundred tokens can reach 150 token/s.&lt;/p&gt; &lt;p&gt;Given that Qwen3.5 35B A3B has already been released, I believe this model is better suited as a pure instruct model choice. Although Qwen3.5 can disable thinking mode, it sometimes still engages in repeated thinking within the main text after turning it off, which can occasionally affect response efficiency. Additionally, this model seems to have some hallucination issues with long contexts; I'm unsure whether this stems from the quantization or the chat template, and disabling KV quantization did not resolve this issue for me.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/jgwokl4p0cmg1.png?width=1701&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=314e1739a5523d349d23f36e7390f1f35e9d6042"&gt;VRAM usage, 80K context&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sad-Pickle4282"&gt; /u/Sad-Pickle4282 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rhjg6w/longcatflashlite_685b_maybe_a_relatively_good/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rhjg6w/longcatflashlite_685b_maybe_a_relatively_good/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rhjg6w/longcatflashlite_685b_maybe_a_relatively_good/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-03-01T00:57:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1rh5luv</id>
    <title>qwen3.5 35b-a3b evaded the zero-reasoning budget by doing its thinking in the comments</title>
    <updated>2026-02-28T15:35:09+00:00</updated>
    <author>
      <name>/u/crantob</name>
      <uri>https://old.reddit.com/user/crantob</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rh5luv/qwen35_35ba3b_evaded_the_zeroreasoning_budget_by/"&gt; &lt;img alt="qwen3.5 35b-a3b evaded the zero-reasoning budget by doing its thinking in the comments" src="https://preview.redd.it/bh48tphl89mg1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3d4f831d5ac50f3062d37127eebfd8ea831e1c62" title="qwen3.5 35b-a3b evaded the zero-reasoning budget by doing its thinking in the comments" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/crantob"&gt; /u/crantob &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bh48tphl89mg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rh5luv/qwen35_35ba3b_evaded_the_zeroreasoning_budget_by/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rh5luv/qwen35_35ba3b_evaded_the_zeroreasoning_budget_by/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-28T15:35:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1rh9k63</id>
    <title>Qwen3.5 35B-A3B replaced my 2-model agentic setup on M1 64GB</title>
    <updated>2026-02-28T18:10:25+00:00</updated>
    <author>
      <name>/u/luke_pacman</name>
      <uri>https://old.reddit.com/user/luke_pacman</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There's been a lot of buzz about Qwen3.5 models being smarter than all previous open-source models in the same size class matching or rivaling models 8-25x larger in total parameters like MiniMax-M2.5 (230B), DeepSeek V3.2 (685B), and GLM-4.7 (357B) in reasoning, agentic, and coding tasks.&lt;/p&gt; &lt;p&gt;I had to try them on a real-world agentic workflow. Here's what I found.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Setup&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- Device: Apple Silicon M1 Max, 64GB&lt;/p&gt; &lt;p&gt;- Inference: llama.cpp server (build 8179)&lt;/p&gt; &lt;p&gt;- Model: Qwen3.5-35B-A3B (Q4_K_XL, 19 GB), runs comfortably on 64GB or even 32GB devices&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Task&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Analyze Amazon sales data for January 2025, identify trends, and suggest improvements to boost sales by 10% next month.&lt;/em&gt; &lt;/p&gt; &lt;p&gt;The data is an Excel file with 6 sheets. This requires both reasoning (planning the analysis, drawing conclusions) and coding (pandas, visualization).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Before: Two Models Required&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Previously, no single model could handle the full task well on my device. I had to combine:&lt;/p&gt; &lt;p&gt;- Nemotron-3-Nano-30B-A3B (~40 tok/s): strong at reasoning and writing, but struggled with code generation&lt;/p&gt; &lt;p&gt;- Qwen3-Coder-30B-A3B (~45 tok/s): handled the coding parts&lt;/p&gt; &lt;p&gt;This combo completed the task in ~13 minutes and produced solid results.&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1rh9k63/video/sagc0xwnv9mg1/player"&gt;https://reddit.com/link/1rh9k63/video/sagc0xwnv9mg1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;After: One Model Does It All&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Qwen3.5 35B-A3B generates at ~27 tok/s on my M1, slower than either of the previous models individually but it handles both reasoning and coding without needing a second model.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Without thinking (~15-20 min)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Slower than the two-model setup, but the output quality was noticeably better:&lt;/p&gt; &lt;p&gt;- More thoughtful analytical plan&lt;/p&gt; &lt;p&gt;- More sophisticated code with better visualizations&lt;/p&gt; &lt;p&gt;- More insightful conclusions and actionable strategies for the 10% sales boost&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1rh9k63/video/u4q8h3c7x9mg1/player"&gt;https://reddit.com/link/1rh9k63/video/u4q8h3c7x9mg1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;With thinking (~35-40 min)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Results improved slightly over no-thinking mode, but at the cost of roughly double the time. Diminishing returns for this particular task.&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1rh9k63/video/guor8u1jz9mg1/player"&gt;https://reddit.com/link/1rh9k63/video/guor8u1jz9mg1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Takeaway&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;One of the tricky parts of local agentic AI is the engineering effort in model selection balancing quality, speed, and device constraints. Qwen3.5 35B-A3B is a meaningful step forward: a single model that handles both reasoning and coding well enough to replace a multi-model setup on a consumer Apple Silicon device, while producing better output.&lt;/p&gt; &lt;p&gt;If you're running agentic workflows locally, I'd recommend trying it with thinking disabled first, you get most of the intelligence gain without the latency penalty.&lt;/p&gt; &lt;p&gt;Please share your own experiences with the Qwen3.5 models below.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/luke_pacman"&gt; /u/luke_pacman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rh9k63/qwen35_35ba3b_replaced_my_2model_agentic_setup_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rh9k63/qwen35_35ba3b_replaced_my_2model_agentic_setup_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rh9k63/qwen35_35ba3b_replaced_my_2model_agentic_setup_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-28T18:10:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1rgzul5</id>
    <title>are you ready for small Qwens?</title>
    <updated>2026-02-28T11:02:10+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rgzul5/are_you_ready_for_small_qwens/"&gt; &lt;img alt="are you ready for small Qwens?" src="https://preview.redd.it/bwc4xcf0w7mg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ac545e4ed49e187bffbf4cf369b2fda1bafd4bb5" title="are you ready for small Qwens?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;13-9=4&lt;/p&gt; &lt;p&gt;unsloth collection has been updated with 4 hidden items too ;)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bwc4xcf0w7mg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rgzul5/are_you_ready_for_small_qwens/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rgzul5/are_you_ready_for_small_qwens/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-28T11:02:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1rhqeob</id>
    <title>Qwen 3.5 27B is the best Chinese translation model under 70B</title>
    <updated>2026-03-01T06:50:30+00:00</updated>
    <author>
      <name>/u/AndreVallestero</name>
      <uri>https://old.reddit.com/user/AndreVallestero</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ever since Llama 3.0, I've been using local models to translate Chinese subs to English. Since December 2024, I've been using a mix of Llama 3.3 70B 2 bit and Gemma 3 27B 4 bit for translations, and although the translations aren't perfect, they're decent enough to be usable.&lt;/p&gt; &lt;p&gt;I've tested many other models in this size range but none of them are as consistent, or as natural sounding as my existing setup. From my testing, MoE tends to perform poorly in translations, and thinking only models tend to also struggle, so it makes sense that there haven't been any improvements in this space for the past year when MoE and thinking have been all the rage.&lt;/p&gt; &lt;p&gt;Like all of you, for the past 4 days I've been testing Qwen 3.5, and I can confidently say that Qwen 3.5 27B is by far the best Chinese translation model under (and including) 70B. For the first time, my local setup (24GB VRAM) has been able to produce translations with tone and consistency on par with GPT 5 fast, and Gemini 3 fast. Really impressed with the Qwen team.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AndreVallestero"&gt; /u/AndreVallestero &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rhqeob/qwen_35_27b_is_the_best_chinese_translation_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rhqeob/qwen_35_27b_is_the_best_chinese_translation_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rhqeob/qwen_35_27b_is_the_best_chinese_translation_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-03-01T06:50:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1rhfque</id>
    <title>Qwen3 Coder Next | Qwen3.5 27B | Devstral Small 2 | Rust &amp; Next.js Benchmark</title>
    <updated>2026-02-28T22:17:13+00:00</updated>
    <author>
      <name>/u/Holiday_Purpose_3166</name>
      <uri>https://old.reddit.com/user/Holiday_Purpose_3166</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rhfque/qwen3_coder_next_qwen35_27b_devstral_small_2_rust/"&gt; &lt;img alt="Qwen3 Coder Next | Qwen3.5 27B | Devstral Small 2 | Rust &amp;amp; Next.js Benchmark" src="https://preview.redd.it/55bw37eg7bmg1.png?width=140&amp;amp;height=95&amp;amp;auto=webp&amp;amp;s=eaeb0ad2fbaa83a0cd9e4f46f9189c9bf64d4755" title="Qwen3 Coder Next | Qwen3.5 27B | Devstral Small 2 | Rust &amp;amp; Next.js Benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Previously&lt;/h1&gt; &lt;p&gt;This benchmark continues my local testing on personal production repos, helping me narrow down the best models to complement my daily driver Devstral Small 2.&lt;/p&gt; &lt;p&gt;Since I'm benchmarking them, I might aswell share the stats which I understand these can be useful and constructive feedback.&lt;/p&gt; &lt;p&gt;In the previous &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1rg41ss/qwen35_27b_vs_devstral_small_2_nextjs_solidity/"&gt;post&lt;/a&gt; Qwen3.5 27B performed best on a custom 78-task Next.js/Solidity bench. Byteshape's Devstral Small 2 had better edge on Next.js.&lt;/p&gt; &lt;p&gt;In the same previous post I ran a bench for &lt;code&gt;noctrex&lt;/code&gt; comment, using the same suite for &lt;code&gt;Qwen3-Coder-Next-UD-IQ3_XXS&lt;/code&gt; which to my surprise, blasted both Mistral and Qwen models.&lt;/p&gt; &lt;p&gt;For this run, I will execute the same models &lt;em&gt;and&lt;/em&gt; Qwen3 Coder Next on a different active repo I'm working on that includes Rust alongside Next.js.&lt;/p&gt; &lt;p&gt;Pulling from my stash I'll be adding LM Studio's Devstral Small 2 Q8_0.&lt;br /&gt; To make &amp;quot;free lunch&amp;quot; fair, I will be setting all Devstral models KV Cache to Q8_0 since LM Studio's heavy on VRAM.&lt;/p&gt; &lt;h1&gt;Important Note&lt;/h1&gt; &lt;p&gt;I understand the configs and quants used in the stack below &lt;strong&gt;doesn't&lt;/strong&gt; represent apples-to-apples comparison. This is based on personal preference in attempt to produce the most efficient output based on resource constraints and context required for my work - absolute minimum 70k context, ideal 131k.&lt;/p&gt; &lt;p&gt;I wish I could test more equivalent models and quants, unfortunately it's time consuming downloading and testing them all, especially wear and tear in these dear times.&lt;/p&gt; &lt;h1&gt;Stack&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;- Fedora 43 - llama.cpp b8149 | docker `nvidia/cuda:13.1.0-devel-ubuntu24.04` - RTX 5090 | stock | driver 580.119.02 - Ryzen 9 9950X | 96GB DDR5 6000 &lt;/code&gt;&lt;/pre&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Fine-Tuner&lt;/th&gt; &lt;th align="left"&gt;Model &amp;amp; Quant&lt;/th&gt; &lt;th align="left"&gt;Model+Context Size&lt;/th&gt; &lt;th align="left"&gt;Flags&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;mradermacher&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Qwen3.5 27B i1-Q6_K&lt;/td&gt; &lt;td align="left"&gt;110k = 29.3GB&lt;/td&gt; &lt;td align="left"&gt;&lt;code&gt;-t 8 --numa numactl --jinja --temp 0.6 --top-p 0.95 --top-k 20 --min-p 0.0 --presence-penalty 0.0 --repeat-penalty 1.0 -b 512 -ub 512 --no-mmap -c 111000&lt;/code&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;unsloth&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Devstral Small 2 24B Q6_K&lt;/td&gt; &lt;td align="left"&gt;132.1k = 29.9GB&lt;/td&gt; &lt;td align="left"&gt;&lt;code&gt;-t 8 --chat-template-file /models/devstral-fix.jinja --temp 0.15 --min-p 0.01 --numa numactl -ctk q8_0 -ctv q8_0 -b 512 -ub 512 --no-mmap -c 71125&lt;/code&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;byteshape&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Devstral Small 2 24B 4.04bpw&lt;/td&gt; &lt;td align="left"&gt;200k = 28.9GB&lt;/td&gt; &lt;td align="left"&gt;&lt;code&gt;-t 8 --chat-template-file /models/devstral-fix.jinja --temp 0.15 --min-p 0.01 --numa numactl -ctk q8_0 -ctv q8_0 -b 512 -ub 512 --no-mmap -c 200000&lt;/code&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;unsloth&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Qwen3 Coder Next UD-IQ3_XXS&lt;/td&gt; &lt;td align="left"&gt;262k = 29.5GB&lt;/td&gt; &lt;td align="left"&gt;&lt;code&gt;-t 10 --numa numactl --jinja --temp 1.0 --top-p 0.95 --min-p 0.01 --top-k 40 -b 512 -ub 512 --n-cpu-moe 0 -ot .ffn_(up)_exps.=CPU --no-mmap&lt;/code&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Scoring&lt;/h1&gt; &lt;p&gt;Executed a single suite with 60 tasks (30 Rust + 30 Next.js) via Opencode - running each model sequentially, one task per session.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Scoring rubric (per task, 0-100)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Correctness (0 or 60 points)&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;60 if the patch fully satisfies task checks.&lt;/li&gt; &lt;li&gt;0 if it fails.&lt;/li&gt; &lt;li&gt;This is binary to reward complete fixes, not partial progress.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Compatibility (0-20 points)&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Measures whether the patch preserves required integration/contract expectations for that task.&lt;/li&gt; &lt;li&gt;Usually task-specific checks.&lt;/li&gt; &lt;li&gt;Full compatibility = 20 | n partial = lower | broken/missing = 0&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Scope Discipline (0-20 points)&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Measures edit hygiene: &lt;em&gt;did the model change only relevant files?&lt;/em&gt;&lt;/li&gt; &lt;li&gt;20 if changes stay in intended scope.&lt;/li&gt; &lt;li&gt;Penalised as unrelated edits increase.&lt;/li&gt; &lt;li&gt;Extra penalty if the model creates a commit during benchmarking.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Why this design works&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Total score = Correctness + Compatibility + Scope Discipline (max 100)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;60% on correctness keeps &lt;em&gt;“works vs doesn’t work”&lt;/em&gt; as the primary signal.&lt;/li&gt; &lt;li&gt;20% compatibility penalises fixes that break expected interfaces/behaviour.&lt;/li&gt; &lt;li&gt;20% scope discipline penalises noisy, risky patching and rewards precise edits.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Results Breakdown&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/55bw37eg7bmg1.png?width=793&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=599d723729ee924e5677cf06c6f68856f27ce1e3"&gt;https://preview.redd.it/55bw37eg7bmg1.png?width=793&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=599d723729ee924e5677cf06c6f68856f27ce1e3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/1r97co9s2bmg1.png?width=1089&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0830e13351ef9e8b48ce330cfda757d67e79fa17"&gt;https://preview.redd.it/1r97co9s2bmg1.png?width=1089&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0830e13351ef9e8b48ce330cfda757d67e79fa17&lt;/a&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Total score&lt;/th&gt; &lt;th align="left"&gt;Pass rate&lt;/th&gt; &lt;th align="left"&gt;Next.js avg&lt;/th&gt; &lt;th align="left"&gt;Rust avg&lt;/th&gt; &lt;th align="left"&gt;PP (tok/s)&lt;/th&gt; &lt;th align="left"&gt;TG (tok/s)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Devstral Small 2 Byteshape 4.04bpw&lt;/td&gt; &lt;td align="left"&gt;2880&lt;/td&gt; &lt;td align="left"&gt;47%&lt;/td&gt; &lt;td align="left"&gt;46/100&lt;/td&gt; &lt;td align="left"&gt;50/100&lt;/td&gt; &lt;td align="left"&gt;700&lt;/td&gt; &lt;td align="left"&gt;56&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Devstral Small 2 Unsloth Q6_0&lt;/td&gt; &lt;td align="left"&gt;3028&lt;/td&gt; &lt;td align="left"&gt;52%&lt;/td&gt; &lt;td align="left"&gt;41/100&lt;/td&gt; &lt;td align="left"&gt;60/100&lt;/td&gt; &lt;td align="left"&gt;1384&lt;/td&gt; &lt;td align="left"&gt;55&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Devstral Small 2 LM Studio Q8_0&lt;/td&gt; &lt;td align="left"&gt;3068&lt;/td&gt; &lt;td align="left"&gt;52%&lt;/td&gt; &lt;td align="left"&gt;56/100&lt;/td&gt; &lt;td align="left"&gt;46/100&lt;/td&gt; &lt;td align="left"&gt;873&lt;/td&gt; &lt;td align="left"&gt;45&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3.5 27B i1-Q6_K&lt;/td&gt; &lt;td align="left"&gt;4200&lt;/td&gt; &lt;td align="left"&gt;83%&lt;/td&gt; &lt;td align="left"&gt;64/100&lt;/td&gt; &lt;td align="left"&gt;76/100&lt;/td&gt; &lt;td align="left"&gt;1128&lt;/td&gt; &lt;td align="left"&gt;46&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 Coder Next Unsloth UD-IQ3_XXS&lt;/td&gt; &lt;td align="left"&gt;4320&lt;/td&gt; &lt;td align="left"&gt;87%&lt;/td&gt; &lt;td align="left"&gt;70/100&lt;/td&gt; &lt;td align="left"&gt;74/100&lt;/td&gt; &lt;td align="left"&gt;654&lt;/td&gt; &lt;td align="left"&gt;60&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Accuracy per Memory&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Total VRAM/RAM&lt;/th&gt; &lt;th align="left"&gt;Accuracy per VRAM/RAM (%/GB)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Devstral Small 2 Bytescape 4.04bpw&lt;/td&gt; &lt;td align="left"&gt;29.3GB VRAM&lt;/td&gt; &lt;td align="left"&gt;1.60&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Devstral Small 2 Unsloth Q6_0&lt;/td&gt; &lt;td align="left"&gt;29.9GB VRAM&lt;/td&gt; &lt;td align="left"&gt;1.74&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Devstral Small 2 LM Studio Q8_0&lt;/td&gt; &lt;td align="left"&gt;30.0GB VRAM&lt;/td&gt; &lt;td align="left"&gt;1.73&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3.5 27B i1-Q6_K&lt;/td&gt; &lt;td align="left"&gt;30.2GB VRAM&lt;/td&gt; &lt;td align="left"&gt;2.75&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 Coder NextUnsloth UD-IQ3_XXS&lt;/td&gt; &lt;td align="left"&gt;31.3GB (29.5GB VRAM + 1.8GB RAM)&lt;/td&gt; &lt;td align="left"&gt;2.78&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Takeaway&lt;/h1&gt; &lt;p&gt;Interesting observation. The overall throughput in this test was significantly slower with Devstral quants, where Qwen3.5 27B and Qwen3 Coder Next had a much more stable throughput compared to previous post.&lt;/p&gt; &lt;p&gt;Despite this test suite being smaller - albeit it took magnitudes longer time - the previous post's 78-suite bench, the Devstral models failed faster on Solidity - scoring between 16-13% respectively - winning in speed to patch Next.js. &lt;em&gt;Maybe KV Cache Q8 ate their lunch?&lt;/em&gt;&lt;/p&gt; &lt;p&gt;In this bench, Devstral models had more approach to Rust as noticed in higher scoring compared to Solidity. I assume due to Rust's nature, the models spent more time patching Rust, which reflected on the longer-horizon throughput decay.&lt;/p&gt; &lt;p&gt;It seems to align with my experience, models with appealing throughput can provide a false belief they can do more work in less time to offset accuracies.&lt;/p&gt; &lt;p&gt;In scenarios where the outcome is deterministic speed makes sense. It may not always be true in repo work. For vibe coding sake, the bigger (slower) models &lt;em&gt;here&lt;/em&gt; will hit the nail more often in fewer steps.&lt;/p&gt; &lt;h1&gt;Conclusions&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Qwen3 Coder Next&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Despite being a Q3 quant, it's the higher-quality repo worker here, and have the benefit using hybrid offloading for max context like in my case if you have enough VRAM/RAM combo. Only wins against Qwen3.5 27B by very small margin but at half throughput could be best for latency due to no reasoning traces.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Qwen3.5 27B&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This is the most efficient choice of the bunch if one can tolerate reasoning. Great fit as Q6 for RTX 5090, and all-rounder that can provide very extensive document writing. This could be an amazing planner and doc writer alongside for agentic work. I suspect if Qwen comes out with a coder variant, this will mog many models in the parameter range. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Devstral Small 2 24B&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;It's a personal favourite, both LM Studio Q8 and Byteshape's exotic 4.04bpw were my great stashed quants. LM Studio's Q8 quality provided same large detail of documentation like Qwen3.5 27B does at Q6.&lt;/p&gt; &lt;p&gt;Oddly, it seems Unsloth's quant did best at Rust and at better PP throughput as the other quants - assuming the higher Next.js fails didn't provide faster Rust patches (?).&lt;/p&gt; &lt;p&gt;Thanks for Unsloth, Byteshape, and LM Studio for their efforts providing these quants.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Holiday_Purpose_3166"&gt; /u/Holiday_Purpose_3166 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rhfque/qwen3_coder_next_qwen35_27b_devstral_small_2_rust/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rhfque/qwen3_coder_next_qwen35_27b_devstral_small_2_rust/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rhfque/qwen3_coder_next_qwen35_27b_devstral_small_2_rust/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-28T22:17:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1rh6pru</id>
    <title>google found that longer chain of thought actually correlates NEGATIVELY with accuracy. -0.54 correlation</title>
    <updated>2026-02-28T16:19:37+00:00</updated>
    <author>
      <name>/u/Top-Cardiologist1011</name>
      <uri>https://old.reddit.com/user/Top-Cardiologist1011</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;new google paper is out and it challenges something a lot of us assumed. they tested 8 model variants (GPT-OSS, DeepSeek-R1, Qwen3, etc) across AIME2024/2025, HMMT 2025, and GPQA-Diamond.&lt;/p&gt; &lt;p&gt;the finding: token length and accuracy have an average correlation of -0.54. negative. longer reasoning chains don't mean better answers, they often mean the model is spiraling or overthinking.&lt;/p&gt; &lt;p&gt;so they proposed DTR (Deep Thinking Ratio) which measures what fraction of tokens actually involve deep processing vs filler. they track this by monitoring prediction distribution changes across model layers. tokens that stabilize early in shallow layers are &amp;quot;filler&amp;quot; (words like &amp;quot;and&amp;quot;, &amp;quot;is&amp;quot;, &amp;quot;the&amp;quot;). tokens that keep getting revised in deep layers are actual reasoning.&lt;/p&gt; &lt;p&gt;DTR correlates with accuracy at 0.82. way better signal than raw length.&lt;/p&gt; &lt;p&gt;the practical payoff: Think@n strategy. sample multiple reasoning paths, estimate DTR from just the first 50 tokens, keep only the top 50% high-DTR samples, then majority vote. result: same or better accuracy, ~50% compute reduction.&lt;/p&gt; &lt;p&gt;GPT-OSS-120B-medium hit 94.7% on AIME 2025 with Think@n vs 92.7% with standard approach. less compute, better results.&lt;/p&gt; &lt;p&gt;this has real implications for local inference. if you can identify and terminate low-quality reasoning early (after just 50 tokens), you save massive amounts of compute. token consumption dropped from 355.6k to 181.9k in their tests.&lt;/p&gt; &lt;p&gt;for anyone running reasoning models locally, this could be huge. early termination of bad reasoning paths means you can run more attempts in the same compute budget. even cloud-based tools like verdent that run multiple agent passes would benefit from this kind of filtering.&lt;/p&gt; &lt;p&gt;paper: &lt;a href="https://arxiv.org/abs/2602.13517"&gt;https://arxiv.org/abs/2602.13517&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Top-Cardiologist1011"&gt; /u/Top-Cardiologist1011 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rh6pru/google_found_that_longer_chain_of_thought/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rh6pru/google_found_that_longer_chain_of_thought/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rh6pru/google_found_that_longer_chain_of_thought/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-28T16:19:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1rh095c</id>
    <title>DeepSeek V4 will be released next week and will have image and video generation capabilities, according to the Financial Times</title>
    <updated>2026-02-28T11:25:49+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rh095c/deepseek_v4_will_be_released_next_week_and_will/"&gt; &lt;img alt="DeepSeek V4 will be released next week and will have image and video generation capabilities, according to the Financial Times" src="https://preview.redd.it/kwyym79lz7mg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=abd4de62b86da5c98b3825614d512759e3a8ec10" title="DeepSeek V4 will be released next week and will have image and video generation capabilities, according to the Financial Times" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Financial Times: DeepSeek to release long-awaited AI model in new challenge to US rivals (paywall): &lt;a href="https://www.ft.com/content/e3366881-0622-40a7-9c34-a0d82e3d573e"&gt;https://www.ft.com/content/e3366881-0622-40a7-9c34-a0d82e3d573e&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kwyym79lz7mg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rh095c/deepseek_v4_will_be_released_next_week_and_will/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rh095c/deepseek_v4_will_be_released_next_week_and_will/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-28T11:25:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1rh43za</id>
    <title>Qwen 3.5-35B-A3B is beyond expectations. It's replaced GPT-OSS-120B as my daily driver and it's 1/3 the size.</title>
    <updated>2026-02-28T14:32:21+00:00</updated>
    <author>
      <name>/u/valdev</name>
      <uri>https://old.reddit.com/user/valdev</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know everyone has their own subjective take on what models are the best, at which types of tasks, at which sizes, at which quants, at which context lengths and so on and so forth.&lt;/p&gt; &lt;p&gt;But Qwen 3.5-35B-A3B has completely shocked me.&lt;/p&gt; &lt;p&gt;My use-case is pretty broad, but generally focuses around development tasks.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I have an N8N server setup that aggregates all of my messages, emails, alerts and aggregates them into priority based batches via the LLM.&lt;/li&gt; &lt;li&gt;I have multiple systems I've created which dynamically generate other systems based on internal tooling I've created based on user requests.&lt;/li&gt; &lt;li&gt;Timed task systems which utilize custom MCP's I've created, think things like &amp;quot;Get me the current mortgage rate in the USA&amp;quot;, then having it run once a day and giving it access to a custom browser MCP. (Only reason custom is important here is because it's self documenting, this isn't published anywhere for it to be part of the training).&lt;/li&gt; &lt;li&gt;Multiple different systems that require vision and interpretation of said visual understanding.&lt;/li&gt; &lt;li&gt;I run it on opencode as well to analyze large code bases&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This model, is... Amazing. It yaps a lot in thinking, but is amazing. I don't know what kind of black magic the Qwen team pumped into this model, but it worked.&lt;/p&gt; &lt;p&gt;It's not the smartest model in the world, it doesn't have all the knowledge crammed into it's data set... But it's very often smart enough to know when it doesn't know something, and when you give it the ability to use a browser it will find the data it needs to fill in the gaps.&lt;/p&gt; &lt;p&gt;Anyone else having a similar experience? (I'm using unsloths Q4-K-XL, running on a 5090 and 3090 @ 100k context)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/valdev"&gt; /u/valdev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rh43za/qwen_3535ba3b_is_beyond_expectations_its_replaced/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rh43za/qwen_3535ba3b_is_beyond_expectations_its_replaced/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rh43za/qwen_3535ba3b_is_beyond_expectations_its_replaced/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-28T14:32:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1rhohqk</id>
    <title>How to switch Qwen 3.5 thinking on/off without reloading the model</title>
    <updated>2026-03-01T05:04:12+00:00</updated>
    <author>
      <name>/u/No-Statement-0001</name>
      <uri>https://old.reddit.com/user/No-Statement-0001</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The Unsloth guide for Qwen 3.5 provides four recommendations for using the model in instruct or thinking mode for general and coding use. I wanted to share that it is possible to switch between the different use cases without having to reload the model every time. &lt;/p&gt; &lt;p&gt;Using the new &lt;code&gt;setParamsByID&lt;/code&gt; filter in llama-swap: &lt;/p&gt; &lt;p&gt;```yaml&lt;/p&gt; &lt;h1&gt;show aliases in v1/models&lt;/h1&gt; &lt;p&gt;includeAliasesInList: true&lt;/p&gt; &lt;p&gt;models: &amp;quot;Q3.5-35B&amp;quot;: env: - &amp;quot;CUDA_VISIBLE_DEVICES=GPU-6f0,GPU-f10&amp;quot; filters: stripParams: &amp;quot;temperature, top_k, top_p, repeat_penalty, min_p, presence_penalty&amp;quot;&lt;/p&gt; &lt;pre&gt;&lt;code&gt; # new filter setParamsByID: &amp;quot;${MODEL_ID}:thinking-coding&amp;quot;: temperture: 0.6 presence_penalty: 0.0 &amp;quot;${MODEL_ID}:instruct&amp;quot;: chat_template_kwargs: enable_thinking: false temperture: 0.7 top_p: 0.8 cmd: | ${server-latest} --model /path/to/models/Qwen3.5-35B-A3B-UD-Q6_K_XL.gguf --ctx-size 262144 --fit off --temp 1.0 --min-p 0.0 --top-k 20 --top-p 0.95 --repeat_penalty 1.0 --presence_penalty 1.5 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;I'm running the above config over 2x3090s with full context getting about 1400 tok/sec for prompt processing and 70 tok/sec generation.&lt;/p&gt; &lt;p&gt;setParamsByID will create a new alias for each set of parameters. When a request for one of the aliases comes in, it will inject new values for chat_template_kwargs, temperture and top_p into the request before sending it to llama-server. &lt;/p&gt; &lt;p&gt;Using the &lt;code&gt;${MODEL_ID}&lt;/code&gt; macro will create aliases named &lt;code&gt;Q3.5-35B:instruct&lt;/code&gt; and &lt;code&gt;Q3.5-35B:thinking-coding&lt;/code&gt;. You don't have to use a macro. You can pick anything for the aliases as long as they're globally unique. &lt;/p&gt; &lt;p&gt;setParamsByID works for any model as it just sets or replaces JSON params in the request before sending it upstream. Here's my gpt-oss-120B config for controlling low, medium and high reasoning efforts: &lt;/p&gt; &lt;p&gt;&lt;code&gt; models: gptoss-120B: env: - &amp;quot;CUDA_VISIBLE_DEVICES=GPU-f10,GPU-6f,GPU-eb1&amp;quot; name: &amp;quot;GPT-OSS 120B&amp;quot; filters: stripParams: &amp;quot;${default_strip_params}&amp;quot; setParamsByID: &amp;quot;${MODEL_ID}&amp;quot;: chat_template_kwargs: reasoning_effort: low &amp;quot;${MODEL_ID}:med&amp;quot;: chat_template_kwargs: reasoning_effort: medium &amp;quot;${MODEL_ID}:high&amp;quot;: chat_template_kwargs: reasoning_effort: high cmd: | /path/to/llama-server/llama-server-latest --host 127.0.0.1 --port ${PORT} --fit off --ctx-size 65536 --no-mmap --no-warmup --model /path/to/models/gpt-oss-120b-mxfp4-00001-of-00003.gguf --temp 1.0 --top-k 100 --top-p 1.0 &lt;/code&gt;&lt;/p&gt; &lt;p&gt;There's a bit more documentation in the &lt;a href="https://github.com/mostlygeek/llama-swap/blob/49546e2cf2d7089bafc463a51677b4843f4627ec/config.example.yaml#L217-L234"&gt;config examples&lt;/a&gt;. &lt;/p&gt; &lt;p&gt;Side note: I realize that llama-swap's config has gotten quite complex! I'm trying to come up with clever ways to make it a bit more accessible for new users. :) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Statement-0001"&gt; /u/No-Statement-0001 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rhohqk/how_to_switch_qwen_35_thinking_onoff_without/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rhohqk/how_to_switch_qwen_35_thinking_onoff_without/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rhohqk/how_to_switch_qwen_35_thinking_onoff_without/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-03-01T05:04:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1rhjmfr</id>
    <title>Nobody in the family uses the family AI platform I build - really bummed about it</title>
    <updated>2026-03-01T01:05:21+00:00</updated>
    <author>
      <name>/u/ubrtnk</name>
      <uri>https://old.reddit.com/user/ubrtnk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rhjmfr/nobody_in_the_family_uses_the_family_ai_platform/"&gt; &lt;img alt="Nobody in the family uses the family AI platform I build - really bummed about it" src="https://preview.redd.it/3a1e1rfx0cmg1.png?width=140&amp;amp;height=21&amp;amp;auto=webp&amp;amp;s=e7b9b15b0167f1280f1757e2e67699674d7bace9" title="Nobody in the family uses the family AI platform I build - really bummed about it" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I started my local AI journey last year after going to Red Hat's conference in May - met the vLLM guys and was completely enthralled. Right around that same time, Amazon announced that they were going to use Alexa recordings for training and that didn't sit right with me. &lt;/p&gt; &lt;p&gt;So I started the process of learning as much as I could, engaging in the community, building, acquiring, growing etc. Strived to have a local equivalent that can answer questions like Alexa, control music, control the smart home and, if something happened to me, help the family figure out how to control everything until they can downgrade to whatever my local ISP will give them - I don't expect them to maintain everything. &lt;/p&gt; &lt;p&gt;Started with dual purposing hardware from my music studio (M2 Max 64GB MBP and M3 Ultra studio) and now as of this post I have 2x 3090s, 2x4090s, 1x 4080s, 1x5060Ti, running on a 24/48c EPYC with 256GB plus a bunch of auxiliary support stuff. I have TTS/STT, Memory functions, RAG, Home Assistant piped in for actual smart and pretty fast Voice Assistant etc. It works. It can talk to the Unifi stuff, it talks to Bookstack for home documentation, it searches the internet automatically...it works. &lt;/p&gt; &lt;p&gt;So, in an attempt to figure out what the family really wanted feature wise, I sent out some questions and a quick survey to see how they were using things, as I have a few different options for consumption - voice, OWUI (public and private facing) etc. and I didnt want to just speculate &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3a1e1rfx0cmg1.png?width=261&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=72111d87860154863159fc292650f1c055595f83"&gt;https://preview.redd.it/3a1e1rfx0cmg1.png?width=261&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=72111d87860154863159fc292650f1c055595f83&lt;/a&gt;&lt;/p&gt; &lt;p&gt;My wife's response... &lt;/p&gt; &lt;p&gt;Nobody uses it. I pour over posts and Medium articles and threads about how to make things faster, more efficient and available for the family and tried to find new options, new features, new cool things. Looked at the logs on OWUI - Wife logged in 1 time since Christmas, Son once in the last 17 days, daughter never. My wife's response to the text. That hurt, and I know it wasn't intentional but it still hurt. I've been keeping things stable and available and fast and...yea. &lt;/p&gt; &lt;p&gt;So now I'm rethinking my entire strategy and pulling it back really to just a hobby for myself and not focusing on the family's need. It doesnt seem like they really care if their stuff stays local or not. So why stress over it.&lt;/p&gt; &lt;p&gt;Technically I could still keep things localist with MUCH less gear - STT/TTS and the GPT-OSS:20B in a 48GB Mac mini would be more than enough - I could see all the gear and just run with that and maybe then take the rest and get an M5 Max MacBook for myself or something. &lt;/p&gt; &lt;p&gt;I just wanted to share my recent story. To my family, it's a hobby. So maybe I need to also look at it that way and let it compete with the rest of the hobbies and eventually fade&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ubrtnk"&gt; /u/ubrtnk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rhjmfr/nobody_in_the_family_uses_the_family_ai_platform/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rhjmfr/nobody_in_the_family_uses_the_family_ai_platform/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rhjmfr/nobody_in_the_family_uses_the_family_ai_platform/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-03-01T01:05:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1rh9u4r</id>
    <title>This sub is incredible</title>
    <updated>2026-02-28T18:20:55+00:00</updated>
    <author>
      <name>/u/cmdr-William-Riker</name>
      <uri>https://old.reddit.com/user/cmdr-William-Riker</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I feel like everything in the AI industry is spedrunning profit driven vendor lock in and rapid enshitification, then everyone on this sub cobbles together a bunch of RTX3090s, trade weights around like they are books at a book club and make the entire industry look like a joke. Keep at it! you are our only hope!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cmdr-William-Riker"&gt; /u/cmdr-William-Riker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rh9u4r/this_sub_is_incredible/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rh9u4r/this_sub_is_incredible/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rh9u4r/this_sub_is_incredible/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-28T18:20:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1rhg3p4</id>
    <title>Bare-Metal AI: Booting Directly Into LLM Inference ‚ No OS, No Kernel (Dell E6510)</title>
    <updated>2026-02-28T22:32:35+00:00</updated>
    <author>
      <name>/u/Electrical_Ninja3805</name>
      <uri>https://old.reddit.com/user/Electrical_Ninja3805</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rhg3p4/baremetal_ai_booting_directly_into_llm_inference/"&gt; &lt;img alt="Bare-Metal AI: Booting Directly Into LLM Inference ‚ No OS, No Kernel (Dell E6510)" src="https://external-preview.redd.it/PRknAnIB54eZMfut9qkw3hhK_Rxo72UxY2hekIecmlA.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8c8f7850f02bf683876987aac081f780b39da271" title="Bare-Metal AI: Booting Directly Into LLM Inference ‚ No OS, No Kernel (Dell E6510)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;someone asked me to post this here, said you gays would like this kinda thing. just a heads up, Im new to reddit, made my account a couple years ago, only now using it,&lt;/p&gt; &lt;p&gt;A UEFI application that boots directly into LLM chat: no operating system, no kernel, no drivers(well sort of....wifi). Just power on, select &amp;quot;Run Live&amp;quot;, type &amp;quot;chat&amp;quot;, and talk to an AI. Everything you see is running in UEFI boot services mode. The entire stack, tokenizer, weight loader, tensor math, inference engine, is written from scratch in freestanding C with zero dependencies. It's painfully slow at the moment because I haven't done any optimizations. Realistically it should run much much faster, but I'm more interested in getting the network drivers running first before that. I'm planning on using this to serve smaller models on my network. Why would I build this? For giggles. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Electrical_Ninja3805"&gt; /u/Electrical_Ninja3805 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=wsfKZWg-Wv4"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rhg3p4/baremetal_ai_booting_directly_into_llm_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rhg3p4/baremetal_ai_booting_directly_into_llm_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-28T22:32:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1rh2lew</id>
    <title>OpenAI pivot investors love</title>
    <updated>2026-02-28T13:25:38+00:00</updated>
    <author>
      <name>/u/PaceImaginary8610</name>
      <uri>https://old.reddit.com/user/PaceImaginary8610</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rh2lew/openai_pivot_investors_love/"&gt; &lt;img alt="OpenAI pivot investors love" src="https://preview.redd.it/wfho2ytml8mg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c93cb65e111030d26cc8300d3d750ce3552a15a9" title="OpenAI pivot investors love" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PaceImaginary8610"&gt; /u/PaceImaginary8610 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wfho2ytml8mg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rh2lew/openai_pivot_investors_love/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rh2lew/openai_pivot_investors_love/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-28T13:25:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1rhogov</id>
    <title>The U.S. used Anthropic AI tools during airstrikes on Iran</title>
    <updated>2026-03-01T05:02:45+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hours after announcing that the federal government would cease using artificial intelligence tools developed by the tech company Anthropic, U.S. President Trump utilized those very tools to launch a massive airstrike against Iran. Sources familiar with the matter confirmed that command centers in various locations, including U.S. Central Command (CENTCOM), have been using Anthropic’s Claude AI tool. Despite escalating tensions between the company and the Pentagon, the command continued to employ the tool for intelligence assessments, target identification, and combat simulations, highlighting the deep level of involvement of AI tools in military operations. The U.S. government and Anthropic have been in a dispute for months over how the Pentagon utilizes its AI models. On Friday, President Trump ordered all agencies to stop cooperating with the company, and the Department of Defense also determined that the firm poses a security threat and a risk to its supply chain.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.wsj.com/livecoverage/iran-strikes-2026/card/u-s-strikes-in-middle-east-use-anthropic-hours-after-trump-ban-ozNO0iClZpfpL7K7ElJ2"&gt;https://www.wsj.com/livecoverage/iran-strikes-2026/card/u-s-strikes-in-middle-east-use-anthropic-hours-after-trump-ban-ozNO0iClZpfpL7K7ElJ2&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rhogov/the_us_used_anthropic_ai_tools_during_airstrikes/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rhogov/the_us_used_anthropic_ai_tools_during_airstrikes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rhogov/the_us_used_anthropic_ai_tools_during_airstrikes/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-03-01T05:02:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7bsfd</id>
    <title>Best Audio Models - Feb 2026</title>
    <updated>2026-02-17T17:01:37+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;They've been a ton of audio models released of late, the most notable perhaps being Qwen3 TTS. So its time for another &lt;strong&gt;Best Audio Models&lt;/strong&gt; megathread&lt;/p&gt; &lt;p&gt;Share what your favorite ASR, TTS, STT, Text to Music models are right now &lt;strong&gt;and why.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Given the the amount of ambiguity and subjectivity in rating/testing these models, please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks etc. Closed models like Elevenlabs v3 seem to continue to be a few levels above open models especially for production use cases with long lengths/stability requirements, so comparisons, especially empirical ones are welcome.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please use the top level comments to thread your responses.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T17:01:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8snay</id>
    <title>AMA with StepFun AI - Ask Us Anything</title>
    <updated>2026-02-19T07:15:35+00:00</updated>
    <author>
      <name>/u/StepFun_ai</name>
      <uri>https://old.reddit.com/user/StepFun_ai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt; &lt;img alt="AMA with StepFun AI - Ask Us Anything" src="https://external-preview.redd.it/bqLigXK_A-XO-amDfdyeQenKCxR18nhChWpDnQXYmr4.png?width=140&amp;amp;height=75&amp;amp;auto=webp&amp;amp;s=72c79bc533c167d8f77ecf629e3b7af53ee1e6b6" title="AMA with StepFun AI - Ask Us Anything" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/w8274fg1jekg1.png?width=1785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fadbd0ec26a56e60900f9ed667ae808217d70cf2"&gt;https://preview.redd.it/w8274fg1jekg1.png?width=1785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fadbd0ec26a56e60900f9ed667ae808217d70cf2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; !&lt;/p&gt; &lt;p&gt;We are &lt;strong&gt;StepFun&lt;/strong&gt;, the team behind the &lt;strong&gt;Step&lt;/strong&gt; family models, including &lt;a href="https://huggingface.co/collections/stepfun-ai/step-35-flash"&gt;&lt;strong&gt;Step 3.5 Flash&lt;/strong&gt;&lt;/a&gt; and &lt;a href="https://huggingface.co/collections/stepfun-ai/step3-vl-10b"&gt;&lt;strong&gt;Step-3-VL-10B&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;We are super excited to host our first AMA tomorrow in this community. Our participants include CEO, CTO, Chief Scientist, LLM Researchers.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Participants&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Ok_Reach_5122"&gt;u/Ok_Reach_5122&lt;/a&gt; (Co-founder &amp;amp; CEO of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/bobzhuyb"&gt;u/bobzhuyb&lt;/a&gt; (Co-founder &amp;amp; CTO of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/user/Lost-Nectarine1016"&gt;u/Lost-Nectarine1016&lt;/a&gt; (Co-founder &amp;amp; Chief Scientist of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Elegant-Sale-1328"&gt;u/Elegant-Sale-1328&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/SavingsConclusion298"&gt;u/SavingsConclusion298&lt;/a&gt; (Post-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Spirited_Spirit3387"&gt;u/Spirited_Spirit3387&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/These-Nothing-8564/"&gt;u/These-Nothing-8564&lt;/a&gt; (Technical Project Manager)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Either-Beyond-7395"&gt;u/Either-Beyond-7395&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Human_Ad_162"&gt;u/Human_Ad_162&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Icy_Dare_3866"&gt;u/Icy_Dare_3866&lt;/a&gt; (Post-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Big-Employee5595"&gt;u/Big-Employee5595&lt;/a&gt; (Agent Algorithms Lead&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run 8 - 11 AM PST, Feburary 19th. The StepFun team will monitor and answer questions over the 24 hours after the live session.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StepFun_ai"&gt; /u/StepFun_ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T07:15:35+00:00</published>
  </entry>
</feed>
