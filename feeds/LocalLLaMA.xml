<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-28T17:06:07+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1pxbgyf</id>
    <title>How to get SOTA opensource models (GLM 4.7, Kimi K2) to do multistep coding automatically? On Claude Code? They keep stopping after 2 or 3 steps...</title>
    <updated>2025-12-27T23:11:47+00:00</updated>
    <author>
      <name>/u/FigZestyclose7787</name>
      <uri>https://old.reddit.com/user/FigZestyclose7787</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Honest question: How do you guys get K2, GLM 4.7 models to work automatically on Coding AGents such as Claude Code? I know these models are plenty powerful, and may work well on other harnesses (roocode, cline, etc) but on Claude Code they hang... on need me to type continue after every 2 or 3 interactions, making it completely unusable. What has your experience been like? Is there a configuration I'm missing? (I'm using Claude Code Router, and or a custom built endpoint/ api key injector). &lt;/p&gt; &lt;p&gt;I really want to give these models a fair try but I simply can't make it work well enough. Glm 4.7 is slightly better than k2 for multistep iteration, but it also stops after a few steps. &lt;/p&gt; &lt;p&gt;So far, only minimax m2.1 (not 2) has worked well enough to get to completion of tasks on its own. &lt;/p&gt; &lt;p&gt;But I'm sure there's wisdom out there that I might be missing. Please share your tips and experience. Tks. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FigZestyclose7787"&gt; /u/FigZestyclose7787 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxbgyf/how_to_get_sota_opensource_models_glm_47_kimi_k2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxbgyf/how_to_get_sota_opensource_models_glm_47_kimi_k2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pxbgyf/how_to_get_sota_opensource_models_glm_47_kimi_k2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-27T23:11:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1pxv4tb</id>
    <title>Llms for local use on rtx5080?</title>
    <updated>2025-12-28T16:14:54+00:00</updated>
    <author>
      <name>/u/Fabulous-Courage819</name>
      <uri>https://old.reddit.com/user/Fabulous-Courage819</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, so i recently bought rtx 5080 and i really want to use it at its full potential, i already tried qwen 8/14B, nemotron3 14B, mistral reasoning 14B, gpt oss 20B. &lt;/p&gt; &lt;p&gt;What other models i should use which are good at tool calling and with some good context window like 32K+&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fabulous-Courage819"&gt; /u/Fabulous-Courage819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxv4tb/llms_for_local_use_on_rtx5080/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxv4tb/llms_for_local_use_on_rtx5080/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pxv4tb/llms_for_local_use_on_rtx5080/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-28T16:14:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1pxvcys</id>
    <title>Gen 3D with local llm</title>
    <updated>2025-12-28T16:24:02+00:00</updated>
    <author>
      <name>/u/mukhayy</name>
      <uri>https://old.reddit.com/user/mukhayy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been working on an exciting project that pushes open source llm like LLaMA 3.1 into new territory: generating 3D furniture models. The goal was to see if LLaMA could move beyond text and learn to create complex 3D mesh structures.&lt;/p&gt; &lt;p&gt;The key challenge was handling the complexity of furniture geometry. Unlike simple 3D objects, furniture pieces have intricate mesh structures that require rich geometric detail. To solve this, I fine-tuned LLaMA 3.1 with a 20k token context length, which gave the model enough capacity to learn and generate these detailed mesh representations. I curated a specialized dataset from open soure, filtering specifically for furniture categories including sofas, cabinets, chairs, and tables. After preparing the data to work with mesh formats, I trained the model using verda.com 's GPU infrastructure.&lt;/p&gt; &lt;p&gt;The results are showcased on a demo website at llm3d.space , where you can see examples of generated furniture across different categories. While it's currently in testing mode due to GPU hosting costs, the initial outputs demonstrate that LLaMA can successfully bridge natural language understanding with 3D content creation—opening possibilities for e-commerce, interior design, AR/VR applications and Gaming.&lt;/p&gt; &lt;p&gt;Exciting to listen for feedback!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mukhayy"&gt; /u/mukhayy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxvcys/gen_3d_with_local_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxvcys/gen_3d_with_local_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pxvcys/gen_3d_with_local_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-28T16:24:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwyw36</id>
    <title>MiniMaxAI/MiniMax-M2.1 seems to be the strongest model per param</title>
    <updated>2025-12-27T14:19:07+00:00</updated>
    <author>
      <name>/u/SlowFail2433</name>
      <uri>https://old.reddit.com/user/SlowFail2433</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Going by the Artifical Analysis benchaes, MiniMaxAI/MiniMax-M2.1 can compete with Kimi K2 Thinking, Deepseek 3.2 and GLM 4.7 in performance.&lt;/p&gt; &lt;p&gt;But what feels especially notable is that MiniMaxAI/MiniMax-M2.1 is only 229B param which is around half of GLM 4.7, around a third of Deepseek 3.2 and around a fifth of Kimi K2 Thinking&lt;/p&gt; &lt;p&gt;What this means is that MiniMaxAI/MiniMax-M2.1 seems to be the best value model now&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SlowFail2433"&gt; /u/SlowFail2433 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwyw36/minimaxaiminimaxm21_seems_to_be_the_strongest/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwyw36/minimaxaiminimaxm21_seems_to_be_the_strongest/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwyw36/minimaxaiminimaxm21_seems_to_be_the_strongest/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-27T14:19:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1pxvp4t</id>
    <title>Anyone running 4x RTX Pro 6000s stacked directly on top of each other?</title>
    <updated>2025-12-28T16:37:35+00:00</updated>
    <author>
      <name>/u/Comfortable-Plate467</name>
      <uri>https://old.reddit.com/user/Comfortable-Plate467</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxvp4t/anyone_running_4x_rtx_pro_6000s_stacked_directly/"&gt; &lt;img alt="Anyone running 4x RTX Pro 6000s stacked directly on top of each other?" src="https://a.thumbs.redditmedia.com/ORhdMeatICbcQ3aVXDfxgI0ZMabVn4Bvql6IkdtE5D4.jpg" title="Anyone running 4x RTX Pro 6000s stacked directly on top of each other?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/ovmd5a522z9g1.jpg?width=4000&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=ec42305e26873179763f77c1d3d2a1bf972623e4"&gt;https://preview.redd.it/ovmd5a522z9g1.jpg?width=4000&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=ec42305e26873179763f77c1d3d2a1bf972623e4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Is anyone here actually running a quad RTX Pro 6000 setup with the cards sandwiched together? I’ve got two right now and I’m looking to add two more. My thinking is that since the cool air should flow from bottom to top through each card, the thermals might be manageable. Has anyone tried this? I really want to avoid using riser cables—they’re such a mess and a total pain to deal with&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Comfortable-Plate467"&gt; /u/Comfortable-Plate467 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxvp4t/anyone_running_4x_rtx_pro_6000s_stacked_directly/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxvp4t/anyone_running_4x_rtx_pro_6000s_stacked_directly/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pxvp4t/anyone_running_4x_rtx_pro_6000s_stacked_directly/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-28T16:37:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1pxb6oo</id>
    <title>China issues draft rules to regulate AI with human-like interaction.</title>
    <updated>2025-12-27T22:59:09+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxb6oo/china_issues_draft_rules_to_regulate_ai_with/"&gt; &lt;img alt="China issues draft rules to regulate AI with human-like interaction." src="https://external-preview.redd.it/UOe8-3-UzVIzXY9pBpuX6xxhZKsMl0kFOZlkgCirjwM.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9193d747727ddaf97e85a229c20398a79d44528d" title="China issues draft rules to regulate AI with human-like interaction." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wonder if this will have any impact on all the models coming out of China.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reuters.com/world/asia-pacific/china-issues-drafts-rules-regulate-ai-with-human-like-interaction-2025-12-27/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxb6oo/china_issues_draft_rules_to_regulate_ai_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pxb6oo/china_issues_draft_rules_to_regulate_ai_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-27T22:59:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1pxl6c9</id>
    <title>[Strix Halo] Unable to load 120B model on Ryzen AI Max+ 395 (128GB RAM) - "Unable to allocate ROCm0 buffer"</title>
    <updated>2025-12-28T07:14:06+00:00</updated>
    <author>
      <name>/u/Wrong-Policy-5612</name>
      <uri>https://old.reddit.com/user/Wrong-Policy-5612</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxl6c9/strix_halo_unable_to_load_120b_model_on_ryzen_ai/"&gt; &lt;img alt="[Strix Halo] Unable to load 120B model on Ryzen AI Max+ 395 (128GB RAM) - &amp;quot;Unable to allocate ROCm0 buffer&amp;quot;" src="https://b.thumbs.redditmedia.com/BfYcjvJg4-Xby41gb5LwhAkWAi-0QHFoSnniKNs2jAY.jpg" title="[Strix Halo] Unable to load 120B model on Ryzen AI Max+ 395 (128GB RAM) - &amp;quot;Unable to allocate ROCm0 buffer&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I am running a Ryzen AI Max+ 395 (Strix Halo) with 128 GB of RAM. I have set my BIOS/Driver &amp;quot;Variable Graphics Memory&amp;quot; (VGM) to High, so Windows reports 96 GB Dedicated VRAM and ~32 GB System RAM.&lt;/p&gt; &lt;p&gt;I am trying to load gpt-oss-120b-Q4_K_M.gguf (approx 64 GB) in LM Studio 0.3.36.&lt;/p&gt; &lt;p&gt;The Issue: No matter what settings I try, I get an allocation error immediately upon loading: error loading model: unable to allocate ROCm0 buffer (I also tried Vulkan and got unable to allocate Vulkan0 buffer).&lt;/p&gt; &lt;p&gt;My Settings:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Model: gpt-oss-120b-Q4_K_M.gguf (63.66 GB)&lt;/li&gt; &lt;li&gt;Engine: ROCm / Vulkan (Tried both)&lt;/li&gt; &lt;li&gt;Context Length: Reduced to 8192 (and even 2048)&lt;/li&gt; &lt;li&gt;GPU Offload: Max (36/36) and Partial (30/36)&lt;/li&gt; &lt;li&gt;mmap: OFF (Crucial, otherwise it checks system RAM)&lt;/li&gt; &lt;li&gt;Flash Attention: OFF&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/t06q2wcoaw9g1.png?width=1038&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0e118bd60a96faac9195d52d02b158fde0e39fab"&gt;https://preview.redd.it/t06q2wcoaw9g1.png?width=1038&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0e118bd60a96faac9195d52d02b158fde0e39fab&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Observations:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The VRAM usage graph shows it loads about 25% (24GB) and then crashes.&lt;/li&gt; &lt;li&gt;It seems like the Windows driver refuses to allocate a single large contiguous chunk, even though I have 96 GB empty VRAM.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Has anyone with Strix Halo or high-VRAM AMD cards (7900 XTX) encountered this buffer limit on Windows? Do I need a specific boot flag or driver setting to allow &amp;gt;24GB allocations?&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wrong-Policy-5612"&gt; /u/Wrong-Policy-5612 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxl6c9/strix_halo_unable_to_load_120b_model_on_ryzen_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxl6c9/strix_halo_unable_to_load_120b_model_on_ryzen_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pxl6c9/strix_halo_unable_to_load_120b_model_on_ryzen_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-28T07:14:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1pxr6sl</id>
    <title>Looking for a specific Fine-tune/Paper: Model that mastered "Analog Clocks" and "Exact Counting"</title>
    <updated>2025-12-28T13:20:08+00:00</updated>
    <author>
      <name>/u/hyperschlauer</name>
      <uri>https://old.reddit.com/user/hyperschlauer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I’m trying to track down a specific research project or model release I saw recently (a few weeks ago). It featured a developer (or research team) who successfully fine-tuned an image generation model to solve two classic &amp;quot;AI fails&amp;quot;:&lt;/p&gt; &lt;p&gt;1) Correct Time on Analog Clocks: The model could represent specific, requested times on clock faces accurately.&lt;/p&gt; &lt;p&gt;2) Exact Counting: It could generate the precise number of people or objects requested (e.g., &amp;quot;exactly five people&amp;quot; and actually showing five).&lt;/p&gt; &lt;p&gt;I remember seeing side-by-side comparison examples showing the base model failing and their version getting it right every time. I believe it might have been shared here on Reddit or via a technical blog post recently. Does anyone have the link to the paper, the GitHub repo, or the original Reddit thread? Any leads would be greatly appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hyperschlauer"&gt; /u/hyperschlauer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxr6sl/looking_for_a_specific_finetunepaper_model_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxr6sl/looking_for_a_specific_finetunepaper_model_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pxr6sl/looking_for_a_specific_finetunepaper_model_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-28T13:20:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1px940g</id>
    <title>Running MiniMax-M2.1 Locally with Claude Code and vLLM on Dual RTX Pro 6000</title>
    <updated>2025-12-27T21:28:12+00:00</updated>
    <author>
      <name>/u/zmarty</name>
      <uri>https://old.reddit.com/user/zmarty</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Run Claude Code with your own local MiniMax-M2.1 model using vLLM's native Anthropic API endpoint support.&lt;/p&gt; &lt;h2&gt;Hardware Used&lt;/h2&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Component&lt;/th&gt; &lt;th&gt;Specification&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;CPU&lt;/td&gt; &lt;td&gt;AMD Ryzen 9 7950X3D 16-Core Processor&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Motherboard&lt;/td&gt; &lt;td&gt;ROG CROSSHAIR X670E HERO&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;GPU&lt;/td&gt; &lt;td&gt;Dual NVIDIA RTX Pro 6000 (96 GB VRAM each)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RAM&lt;/td&gt; &lt;td&gt;192 GB DDR5 5200 (note the model does not use the RAM, it fits into VRAM entirely)&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;hr /&gt; &lt;h2&gt;Install vLLM Nightly&lt;/h2&gt; &lt;p&gt;&lt;strong&gt;Prerequisite:&lt;/strong&gt; &lt;a href="https://forum.level1techs.com/t/wip-blackwell-rtx-6000-pro-max-q-quickie-setup-guide-on-ubuntu-24-04-lts-25-04/230521"&gt;Ubuntu 24.04 and the proper NVIDIA drivers&lt;/a&gt;&lt;/p&gt; &lt;p&gt;```bash mkdir vllm-nightly cd vllm-nightly uv venv --python 3.12 --seed source .venv/bin/activate&lt;/p&gt; &lt;p&gt;uv pip install -U vllm \ --torch-backend=auto \ --extra-index-url &lt;a href="https://wheels.vllm.ai/nightly"&gt;https://wheels.vllm.ai/nightly&lt;/a&gt; ```&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Download MiniMax-M2.1&lt;/h2&gt; &lt;p&gt;Set up a separate environment for downloading models:&lt;/p&gt; &lt;p&gt;```bash mkdir /models cd /models uv venv --python 3.12 --seed source .venv/bin/activate&lt;/p&gt; &lt;p&gt;pip install huggingface_hub ```&lt;/p&gt; &lt;p&gt;Download the AWQ-quantized MiniMax-M2.1 model:&lt;/p&gt; &lt;p&gt;&lt;code&gt;bash mkdir /models/awq huggingface-cli download cyankiwi/MiniMax-M2.1-AWQ-4bit \ --local-dir /models/awq/cyankiwi-MiniMax-M2.1-AWQ-4bit &lt;/code&gt;&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Start vLLM Server&lt;/h2&gt; &lt;p&gt;From your vLLM environment, launch the server with the Anthropic-compatible endpoint:&lt;/p&gt; &lt;p&gt;```bash cd ~/vllm-nightly source .venv/bin/activate&lt;/p&gt; &lt;p&gt;vllm serve \ /models/awq/cyankiwi-MiniMax-M2.1-AWQ-4bit \ --served-model-name MiniMax-M2.1-AWQ \ --max-num-seqs 10 \ --max-model-len 128000 \ --gpu-memory-utilization 0.95 \ --tensor-parallel-size 2 \ --pipeline-parallel-size 1 \ --enable-auto-tool-choice \ --tool-call-parser minimax_m2 \ --reasoning-parser minimax_m2_append_think \ --trust-remote-code \ --host 0.0.0.0 \ --port 8000 ```&lt;/p&gt; &lt;p&gt;The server exposes &lt;code&gt;/v1/messages&lt;/code&gt; (Anthropic-compatible) at &lt;code&gt;http://localhost:8000&lt;/code&gt;.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Install Claude Code&lt;/h2&gt; &lt;p&gt;Install Claude Code on macOS, Linux, or WSL:&lt;/p&gt; &lt;p&gt;&lt;code&gt;bash curl -fsSL https://claude.ai/install.sh | bash &lt;/code&gt;&lt;/p&gt; &lt;p&gt;See the &lt;a href="https://code.claude.com/docs/en/overview"&gt;official Claude Code documentation&lt;/a&gt; for more details.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Configure Claude Code&lt;/h2&gt; &lt;h3&gt;Create settings.json&lt;/h3&gt; &lt;p&gt;Create or edit &lt;code&gt;~/.claude/settings.json&lt;/code&gt;:&lt;/p&gt; &lt;p&gt;&lt;code&gt;json { &amp;quot;env&amp;quot;: { &amp;quot;ANTHROPIC_BASE_URL&amp;quot;: &amp;quot;http://localhost:8000&amp;quot;, &amp;quot;ANTHROPIC_AUTH_TOKEN&amp;quot;: &amp;quot;dummy&amp;quot;, &amp;quot;API_TIMEOUT_MS&amp;quot;: &amp;quot;3000000&amp;quot;, &amp;quot;CLAUDE_CODE_DISABLE_NONESSENTIAL_TRAFFIC&amp;quot;: &amp;quot;1&amp;quot;, &amp;quot;ANTHROPIC_MODEL&amp;quot;: &amp;quot;MiniMax-M2.1-AWQ&amp;quot;, &amp;quot;ANTHROPIC_SMALL_FAST_MODEL&amp;quot;: &amp;quot;MiniMax-M2.1-AWQ&amp;quot;, &amp;quot;ANTHROPIC_DEFAULT_SONNET_MODEL&amp;quot;: &amp;quot;MiniMax-M2.1-AWQ&amp;quot;, &amp;quot;ANTHROPIC_DEFAULT_OPUS_MODEL&amp;quot;: &amp;quot;MiniMax-M2.1-AWQ&amp;quot;, &amp;quot;ANTHROPIC_DEFAULT_HAIKU_MODEL&amp;quot;: &amp;quot;MiniMax-M2.1-AWQ&amp;quot; } } &lt;/code&gt;&lt;/p&gt; &lt;h3&gt;Skip Onboarding (Workaround for Bug)&lt;/h3&gt; &lt;p&gt;Due to a &lt;a href="https://github.com/anthropics/claude-code/issues/13827"&gt;known bug in Claude Code 2.0.65+&lt;/a&gt;, fresh installs may ignore &lt;code&gt;settings.json&lt;/code&gt; during onboarding. Add &lt;code&gt;hasCompletedOnboarding&lt;/code&gt; to &lt;code&gt;~/.claude.json&lt;/code&gt;:&lt;/p&gt; &lt;p&gt;```bash&lt;/p&gt; &lt;h1&gt;If ~/.claude.json doesn't exist, create it:&lt;/h1&gt; &lt;p&gt;echo '{&amp;quot;hasCompletedOnboarding&amp;quot;: true}' &amp;gt; ~/.claude.json&lt;/p&gt; &lt;h1&gt;If it exists, add the field manually or use jq:&lt;/h1&gt; &lt;p&gt;jq '. + {&amp;quot;hasCompletedOnboarding&amp;quot;: true}' ~/.claude.json &amp;gt; tmp.json &amp;amp;&amp;amp; mv tmp.json ~/.claude.json ```&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Run Claude Code&lt;/h2&gt; &lt;p&gt;With vLLM running in one terminal, open another and run:&lt;/p&gt; &lt;p&gt;&lt;code&gt;bash claude &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Claude Code will now use your local MiniMax-M2.1 model! If you also want to configure the Claude Code VSCode extension, see &lt;a href="https://platform.minimax.io/docs/guides/text-ai-coding-tools#use-m2-1-in-claude-code-extension-for-vs-code"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;References&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/vllm-project/vllm/issues/21313"&gt;vLLM Anthropic API Support (GitHub Issue #21313)&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://platform.minimax.io/docs/guides/text-ai-coding-tools"&gt;MiniMax M2.1 for AI Coding Tools&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/cyankiwi/MiniMax-M2.1-AWQ-4bit"&gt;cyankiwi/MiniMax-M2.1-AWQ-4bit on Hugging Face&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Cross-posted from my blog: &lt;a href="https://www.ovidiudan.com/2025/12/27/running-claude-code-with-minimax-m2-1.html"&gt;Running MiniMax-M2.1 Locally with Claude Code on Dual RTX Pro 6000&lt;/a&gt; (I am not selling or promoting anything)&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zmarty"&gt; /u/zmarty &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1px940g/running_minimaxm21_locally_with_claude_code_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1px940g/running_minimaxm21_locally_with_claude_code_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1px940g/running_minimaxm21_locally_with_claude_code_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-27T21:28:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1px1c41</id>
    <title>Head of Engineering @MiniMax__AI on MiniMax M2 int4 QAT</title>
    <updated>2025-12-27T16:06:19+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1px1c41/head_of_engineering_minimax_ai_on_minimax_m2_int4/"&gt; &lt;img alt="Head of Engineering @MiniMax__AI on MiniMax M2 int4 QAT" src="https://preview.redd.it/1e9anmnmsr9g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7456cd2a6f5b63217ca62ea494cdbf87700184fa" title="Head of Engineering @MiniMax__AI on MiniMax M2 int4 QAT" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1e9anmnmsr9g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1px1c41/head_of_engineering_minimax_ai_on_minimax_m2_int4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1px1c41/head_of_engineering_minimax_ai_on_minimax_m2_int4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-27T16:06:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1pxbg4x</id>
    <title>SOCAMM2 - new(ish), screwable (replaceable, non soldered) LPDDR5X RAM standard intended for AI data centers.</title>
    <updated>2025-12-27T23:10:47+00:00</updated>
    <author>
      <name>/u/-InformalBanana-</name>
      <uri>https://old.reddit.com/user/-InformalBanana-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Samsung introduces SOCAMM2 LPDDR5X memory module for AI data centers — new standard set to offer reduced power consumption and &lt;strong&gt;double the bandwidth&lt;/strong&gt; versus DDR5 RDIMMs.&lt;/p&gt; &lt;p&gt;The SOCAMM2 LPDDR5X-based module is being positioned as a standardized, serviceable alternative to soldered memory as AI servers chase higher bandwidth.&lt;/p&gt; &lt;p&gt;Hopefully this gets represented and used more in the consumer market.&lt;/p&gt; &lt;p&gt;More info:&lt;/p&gt; &lt;p&gt;&lt;a href="https://semiconductor.samsung.com/news-events/tech-blog/introducing-samsungs-socamm2-new-lpddr-memory-module-empowering-next-generation-ai-infrastructure/"&gt;https://semiconductor.samsung.com/news-events/tech-blog/introducing-samsungs-socamm2-new-lpddr-memory-module-empowering-next-generation-ai-infrastructure/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.tomshardware.com/tech-industry/samsung-introduces-socamm2-lpddr5x-memory-module-for-ai-data-centers"&gt;https://www.tomshardware.com/tech-industry/samsung-introduces-socamm2-lpddr5x-memory-module-for-ai-data-centers&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-InformalBanana-"&gt; /u/-InformalBanana- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxbg4x/socamm2_newish_screwable_replaceable_non_soldered/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxbg4x/socamm2_newish_screwable_replaceable_non_soldered/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pxbg4x/socamm2_newish_screwable_replaceable_non_soldered/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-27T23:10:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1pxp9rg</id>
    <title>Seeking advice from developers building apps with ML/DL integration</title>
    <updated>2025-12-28T11:32:29+00:00</updated>
    <author>
      <name>/u/hemahariharansamson</name>
      <uri>https://old.reddit.com/user/hemahariharansamson</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I am planning to build apps and websites that solve real-world problems. My goal is not just to create normal CRUD or UI-focused apps, but also to gradually integrate my own machine learning and deep learning models into these products and services.&lt;/p&gt; &lt;p&gt;I’ve been experimenting with AI-assisted development tools like Cursor to speed up design and coding, but I want to learn from the community about what works best in practice.&lt;/p&gt; &lt;p&gt;I’d love to hear from you:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;What is your go-to AI tool for development like Cursor?&lt;/li&gt; &lt;li&gt;What subscription plan or setup do you use?&lt;/li&gt; &lt;li&gt;Any tips for integrating custom ML/DL models into real apps?&lt;/li&gt; &lt;li&gt;Recommended tech stacks, workflows, or common pitfalls for beginners building production-ready apps&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Looking forward to your advice. Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hemahariharansamson"&gt; /u/hemahariharansamson &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxp9rg/seeking_advice_from_developers_building_apps_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxp9rg/seeking_advice_from_developers_building_apps_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pxp9rg/seeking_advice_from_developers_building_apps_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-28T11:32:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1pxs4us</id>
    <title>What's a good small model for generating tags from text content?</title>
    <updated>2025-12-28T14:05:34+00:00</updated>
    <author>
      <name>/u/ghulamalchik</name>
      <uri>https://old.reddit.com/user/ghulamalchik</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm using Karakeep which is a bookmark system for links. They offer using Ollama/Open Router/OpenAI for auto generating tag.&lt;/p&gt; &lt;p&gt;First of all, are tiny models capable of doing this task? By tiny I mean maybe 200m, 500m. If not, what could be the best smallest option? I'm currently using Mistral 7b, it's not the best, but it's not bad either.&lt;/p&gt; &lt;p&gt;I wonder if I can get better results with another model, and if it can be smaller too.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ghulamalchik"&gt; /u/ghulamalchik &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxs4us/whats_a_good_small_model_for_generating_tags_from/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxs4us/whats_a_good_small_model_for_generating_tags_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pxs4us/whats_a_good_small_model_for_generating_tags_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-28T14:05:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1pxr47l</id>
    <title>MCP servers are hard to debug and impossible to test, so I built Syrin</title>
    <updated>2025-12-28T13:16:33+00:00</updated>
    <author>
      <name>/u/hack_the_developer</name>
      <uri>https://old.reddit.com/user/hack_the_developer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, &lt;/p&gt; &lt;p&gt;I’ve been building MCP servers and kept running into the same issues:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;No visibility into why an LLM picked a tool&lt;/li&gt; &lt;li&gt;Tool calls looping or failing silently&lt;/li&gt; &lt;li&gt;No deterministic way to test MCP behaviour&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So I built &lt;strong&gt;Syrin,&lt;/strong&gt; a local-first &lt;strong&gt;CLI debugger and test runner for MCP servers&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it does (v1.0.0):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;CLI commands: &lt;code&gt;syrin init&lt;/code&gt;, &lt;code&gt;doctor&lt;/code&gt;, &lt;code&gt;test&lt;/code&gt;, &lt;code&gt;list&lt;/code&gt;, &lt;code&gt;dev&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Full MCP protocol support (tools, resources, prompts, validation)&lt;/li&gt; &lt;li&gt;Multi-LLM support: OpenAI, Claude, Ollama (auto-manages Ollama)&lt;/li&gt; &lt;li&gt;Safe-by-default execution (preview mode + full event tracing)&lt;/li&gt; &lt;li&gt;YAML config, HTTP + stdio transport&lt;/li&gt; &lt;li&gt;TypeScript, npm package, npx-friendly&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What I’m working on next:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Deterministic &lt;strong&gt;unit tests for tools&lt;/strong&gt; (was it called? with what args?)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Workflow testing&lt;/strong&gt; for multi-step tool chains with dependencies&lt;/li&gt; &lt;li&gt;Assertions on runtime events, not model text&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/ankan-labs/syrin"&gt;&lt;strong&gt;https://github.com/ankan-labs/syrin&lt;/strong&gt;&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;NPM:&lt;/strong&gt; &lt;a href="https://www.npmjs.com/package/@ankan-ai/syrin"&gt;&lt;strong&gt;https://www.npmjs.com/package/@ankan-ai/syrin&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you’re building MCP servers, I’d love feedback or contributors.&lt;br /&gt; If this is the wrong approach, tell me why.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hack_the_developer"&gt; /u/hack_the_developer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxr47l/mcp_servers_are_hard_to_debug_and_impossible_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxr47l/mcp_servers_are_hard_to_debug_and_impossible_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pxr47l/mcp_servers_are_hard_to_debug_and_impossible_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-28T13:16:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1pxtwn2</id>
    <title>Which is the best embedding model for production use?</title>
    <updated>2025-12-28T15:24:55+00:00</updated>
    <author>
      <name>/u/Hari-Prasad-12</name>
      <uri>https://old.reddit.com/user/Hari-Prasad-12</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've done my research for embedding models for a critical production job. I've read a lot about bge m3 since I can't use a closed source model like text emmedings 3 or something properitry I'm seeking your experience working with these open source models. &lt;/p&gt; &lt;p&gt;To put it simply, which one of these works the best in production:&lt;br /&gt; 1. bge m3&lt;br /&gt; 2. embeddinggemma-300m&lt;br /&gt; 3. qwen3-embedding-0.6b&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hari-Prasad-12"&gt; /u/Hari-Prasad-12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxtwn2/which_is_the_best_embedding_model_for_production/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxtwn2/which_is_the_best_embedding_model_for_production/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pxtwn2/which_is_the_best_embedding_model_for_production/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-28T15:24:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1pxuk38</id>
    <title>Fix for Nvidia Nemotron Nano 3's forced thinking – now it can be toggled on and off!</title>
    <updated>2025-12-28T15:51:54+00:00</updated>
    <author>
      <name>/u/Substantial_Swan_144</name>
      <uri>https://old.reddit.com/user/Substantial_Swan_144</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, everyone,&lt;/p&gt; &lt;p&gt;if you downloaded NVidia Nemotron 3, you are probably aware that the instruction 'detailed thinking off' doesn't work. This is because the automatic Jinja template on Lmstudio has a bug that forces thinking.&lt;/p&gt; &lt;p&gt;However, I'm postining a workaround here: this template has a bugfix which makes thinking on by default, but it can be toggled off by typing /nothink at the system prompt (like you do with Qwen). I pasted it on Pastebin to make this post clean: &lt;a href="https://pastebin.com/y5g3X2Ex"&gt;https://pastebin.com/y5g3X2Ex&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Enjoy!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Substantial_Swan_144"&gt; /u/Substantial_Swan_144 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxuk38/fix_for_nvidia_nemotron_nano_3s_forced_thinking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxuk38/fix_for_nvidia_nemotron_nano_3s_forced_thinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pxuk38/fix_for_nvidia_nemotron_nano_3s_forced_thinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-28T15:51:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1pxo9y5</id>
    <title>What non-Asian based models do you recommend at the end of 2025?</title>
    <updated>2025-12-28T10:31:00+00:00</updated>
    <author>
      <name>/u/thealliane96</name>
      <uri>https://old.reddit.com/user/thealliane96</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Background:&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;Building agentic stuff so tool calling has to be good (gpt oss has been the most reliable one in my, admittedly anecdotal, experience)&lt;/li&gt; &lt;li&gt;Work with and do work for certain organizations where I can’t:&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;- Use frontier models (or any hosted models for that matter)&lt;/p&gt; &lt;p&gt;- Use models released by Chinese, Taiwanese, etc based companies (maybe it’s dumb, okay it’s probably dumb, but unfortunately I don’t make the rules lol)&lt;/p&gt; &lt;p&gt;So I come to yall ask for your recommendations going into 2026.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note 1:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;I’m aware there’s some other similar posts but since they’re somewhat dated and a lot has happened since, I figured it wouldn’t be&lt;/em&gt; &lt;strong&gt;&lt;em&gt;too&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;egregious to throw mine up. Hope it’s okay &amp;lt;3&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note 2:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;While I am hoping to get recs for models I haven’t considered that will actually be effective, I’m also hoping just to find some new stuff to try regardless &amp;lt;3&lt;/em&gt;&lt;/p&gt; &lt;h1&gt;Models Tried&lt;/h1&gt; &lt;p&gt;- llama3.1 8B&lt;/p&gt; &lt;p&gt;- mistral Nemo&lt;/p&gt; &lt;p&gt;- Nemo fine tuned on my dataset&lt;/p&gt; &lt;p&gt;- mistral small 3.1 / 3.2 24b&lt;/p&gt; &lt;p&gt;- gpt-oss 20b and 120b&lt;/p&gt; &lt;p&gt;- several other mistral and devstral variants&lt;/p&gt; &lt;p&gt;- some phi models&lt;/p&gt; &lt;p&gt;- Gemma 3 27B (been so long and didn’t try it as much as the others)&lt;/p&gt; &lt;h1&gt;Unorganized Thoughts Regarding Models Tried&lt;/h1&gt; &lt;p&gt;From my experience testing them:&lt;/p&gt; &lt;p&gt;- All are generally good with raw text output (except Nemo, Nemo just sucks ass in my opinion)&lt;/p&gt; &lt;p&gt;- Tool calling wise **gpt-oss** is leagues ahead of all the others, at least in my experience using them&lt;/p&gt; &lt;p&gt;- llama3.1 8B is surprising good for raw text output and summarization and it has a oddly pleasing writing style? Maybe that’s just me.&lt;/p&gt; &lt;p&gt;- Mistral models in general never fail to be underwhelming for me. Quite liked Small 3.2, but when I slotted it into a (honestly) quite simple agent setup it got stuck in loops and would fuck up tool calls whereas gpt-oss-20b did it perfectly fine.&lt;/p&gt; &lt;p&gt;- devstral, mixtral, all those mistral variants I’ve found to also be incredibly underwhelming&lt;/p&gt; &lt;p&gt;- Phi models were, in my experience, utterly useless&lt;/p&gt; &lt;p&gt;- Gemma 3 honestly don’t remember, planning to try it out again soon&lt;/p&gt; &lt;h1&gt;On GPT-OSS&lt;/h1&gt; &lt;p&gt;While the answer is somewhat obviously “just use gpt oss”, there’s 2 negatives I find with it, neither are really deal breaking but they can be annoying plus sometimes you just want to try different stuff.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Negative 1:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I sometimes find it can maybe be a bit &lt;em&gt;too good&lt;/em&gt; at following instructions?&lt;/p&gt; &lt;p&gt;It’ll kind of, well, follow them to the letter including making things up to produce an output I’ve asked for.&lt;/p&gt; &lt;p&gt;I’ve gotten around this by instructing it to only output things it’s seen directly in tool results or directly from some external context it was given and that’s worked quite well but still.&lt;/p&gt; &lt;p&gt;It also suffers from what I like to call &lt;em&gt;context window snowballing&lt;/em&gt; where it gets stuck on one path and becomes very narrow minded (all the previous tokens influencing the next token basically, so without some type of intervention it’ll snowball down that same path).&lt;/p&gt; &lt;p&gt;Again I have ways getting around this where I’ll intentionally stop it after a certain percentage of the context window is full and then have it break down what it did and what the next steps should be and then I’ll throw that into a new run with a clear context window and instructing to rethink through the task and what it’s next steps should be. It’s a lot of work around but it works decently well.&lt;/p&gt; &lt;p&gt;I also haven’t found 120b to really be all that better than 20b, honestly sometimes 120b… kinda performs &lt;em&gt;worse&lt;/em&gt;?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Negative Number 2:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;For the work I’m doing I have to abliterate it (de-censor it).&lt;/p&gt; &lt;p&gt;It’d get stuck in a reasoning loop of trying to decide whether it could answer or not until eventually it’d just time out or I’d kill it. And what I’m asking it to do is not even against policy, it’s just been so heavily censored.&lt;/p&gt; &lt;p&gt;This isn’t that big of a deal as it’s been made quite easy by heretic, but still one of those annoyances where you just kind of wish you didn’t have to do it.&lt;/p&gt; &lt;p&gt;—-&lt;/p&gt; &lt;p&gt;Anyway enough of my rambling, anyone who read through it all, you’re a real one!&lt;/p&gt; &lt;h1&gt;TL;DR&lt;/h1&gt; &lt;p&gt;Can’t use models from either Chinese or other Asia-based companies/orgs.&lt;/p&gt; &lt;p&gt;Looking for recommendations for American/Canadian/European models that are good at tool calling that aren’t within the list of ones I’ve already tried.&lt;/p&gt; &lt;p&gt;Edit:&lt;/p&gt; &lt;p&gt;Guess markdown formatting isn’t supported on mobile lol&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thealliane96"&gt; /u/thealliane96 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxo9y5/what_nonasian_based_models_do_you_recommend_at/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxo9y5/what_nonasian_based_models_do_you_recommend_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pxo9y5/what_nonasian_based_models_do_you_recommend_at/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-28T10:31:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1pxl1k1</id>
    <title>I built a frontend for stable-diffusion.cpp for local image generation</title>
    <updated>2025-12-28T07:06:16+00:00</updated>
    <author>
      <name>/u/fabricio3g</name>
      <uri>https://old.reddit.com/user/fabricio3g</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built a Front End of stable-diffusion-cpp to run localy Z-Image Turbo on my old vulkan compatible integrated GPU using stable-diffusion.cpp. &lt;/p&gt; &lt;p&gt;The code is a messy but works for my needs. Some features aren’t fully tested due to my weak GPU. The project is open source and open to contributions.&lt;/p&gt; &lt;p&gt;Currently: Run with npm start&lt;/p&gt; &lt;p&gt;Windows build not working &lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/fabricio3g/FlaxeoUI"&gt;https://github.com/fabricio3g/FlaxeoUI&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fabricio3g"&gt; /u/fabricio3g &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxl1k1/i_built_a_frontend_for_stablediffusioncpp_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxl1k1/i_built_a_frontend_for_stablediffusioncpp_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pxl1k1/i_built_a_frontend_for_stablediffusioncpp_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-28T07:06:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1pxj4lv</id>
    <title>Day 20: 21 Days of Building a Small Language Model: Activation Functions</title>
    <updated>2025-12-28T05:18:48+00:00</updated>
    <author>
      <name>/u/Prashant-Lakhera</name>
      <uri>https://old.reddit.com/user/Prashant-Lakhera</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Welcome to Day 20 of 21 Days of Building a Small Language Model. The topic for today is activation functions, the components that give neural networks their ability to learn complex, non-linear patterns. Yesterday we explored residual connections and how they enable deep networks. Today, we'll discover how activation functions work, why they're essential, and how modern choices like SwiGLU have become the standard in state-of-the-art language models.&lt;/p&gt; &lt;h1&gt;Why Activation Functions matter&lt;/h1&gt; &lt;p&gt;Before we dive into specific activation functions, let's understand why they're essential. A neural network without activation functions is just a series of matrix multiplications. No matter how many layers you stack, the result is always a linear transformation. This means the network can only learn linear relationships, which is extremely limiting.&lt;/p&gt; &lt;p&gt;Activation functions introduce non-linearity, allowing networks to learn complex patterns. They're what enable neural networks to approximate any function, recognize images, understand language, and solve problems that linear models cannot. Without activation functions, neural networks would be no more powerful than simple linear regression.&lt;/p&gt; &lt;p&gt;But not all activation functions are created equal. The choice of activation function affects training stability, convergence speed, gradient flow, and final model performance. This is why understanding activation functions is crucial for building effective language models.&lt;/p&gt; &lt;h1&gt;Evolution: From ReLU to SwiGLU&lt;/h1&gt; &lt;p&gt;The history of activation functions in deep learning shows a clear evolution toward smoother, more effective functions. Let's trace this journey:&lt;/p&gt; &lt;h1&gt;ReLU&lt;/h1&gt; &lt;p&gt;ReLU (Rectified Linear Unit) was the breakthrough that made deep learning practical. It's defined as:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;ReLU(x) = max(0, x)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;ReLU is simple: if the input is positive, output the input; if negative, output zero. This simplicity made it fast to compute and helped with the vanishing gradient problem that plagued earlier activation functions like sigmoid and tanh.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why ReLU worked:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Fast computation (just a max operation)&lt;/li&gt; &lt;li&gt;Helps with vanishing gradients (gradient is 1 for positive inputs)&lt;/li&gt; &lt;li&gt;Sparse activations (many neurons output zero, creating sparsity)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Limitations:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Dead neurons: Once a neuron outputs zero, it may never recover (dying ReLU problem)&lt;/li&gt; &lt;li&gt;Not smooth: The function has a sharp corner at zero, which can cause issues&lt;/li&gt; &lt;li&gt;Zero gradient for negative inputs: No learning happens for negative values&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;GELU&lt;/h1&gt; &lt;p&gt;GELU (Gaussian Error Linear Unit) addressed some of ReLU's limitations by being smooth and differentiable everywhere. It's defined as:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GELU(x) = x × Φ(x)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;where Φ(x) is the cumulative distribution function of the standard normal distribution. GELU is smooth, meaning it has no sharp corners, which helps with gradient flow.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why GELU became popular:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Smooth and differentiable everywhere&lt;/li&gt; &lt;li&gt;Better gradient flow than ReLU&lt;/li&gt; &lt;li&gt;Works well in transformers (used in BERT, GPT-2)&lt;/li&gt; &lt;li&gt;More stable training, especially for language models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;GELU's characteristics:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Smooth transition instead of sharp cutoff&lt;/li&gt; &lt;li&gt;Allows small negative values to pass through (unlike ReLU)&lt;/li&gt; &lt;li&gt;Better for tasks requiring fine-grained control&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Swish/SiLU&lt;/h1&gt; &lt;p&gt;Swish (also called SiLU, Sigmoid Linear Unit) is defined as:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Swish(x) = x × sigmoid(x) = x × (1 / (1 + e^(-x)))&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Swish is smooth like GELU but has been shown to work better in many applications. It's non-monotonic (can decrease for negative inputs), which gives it more flexibility than ReLU.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why Swish works well:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Smooth and differentiable everywhere&lt;/li&gt; &lt;li&gt;Non-monotonic behavior provides more expressiveness&lt;/li&gt; &lt;li&gt;Better gradient flow than ReLU&lt;/li&gt; &lt;li&gt;Proven effective in modern language models&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;SwiGLU&lt;/h1&gt; &lt;p&gt;SwiGLU (Swish-Gated Linear Unit) takes Swish and adds a gating mechanism. Instead of just applying Swish to a transformation, SwiGLU uses two parallel paths:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;SwiGLU(x) = Swish(W_gate × x) ⊙ (W_up × x)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;where ⊙ is element-wise multiplication. The key innovation is the gating mechanism: one path gets activated (the gate), and the other doesn't (the up projection). The gate controls how much of the unactivated path passes through.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why SwiGLU is powerful:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Gating mechanism allows more complex transformations&lt;/li&gt; &lt;li&gt;The gate can selectively pass or block information&lt;/li&gt; &lt;li&gt;More expressive than simple activation functions&lt;/li&gt; &lt;li&gt;Has become the standard in modern models like Qwen, LLaMA, and GPT&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;My Experience&lt;/h1&gt; &lt;p&gt;From working with different activation functions in practice, here's what I've learned:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;For small models:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GELU is often the safe, reliable choice. It provides good stability and performance without the extra parameters of gated variants.&lt;/li&gt; &lt;li&gt;SwiGLU can provide better performance but requires more parameters. For small models where every parameter counts, the trade-off isn't always worth it.&lt;/li&gt; &lt;li&gt;ReLU can work but is less stable, especially early in training. I avoid it unless I have a specific reason.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;For Larger models:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;SwiGLU has become the standard. The extra parameters are worth it for the improved expressiveness and performance.&lt;/li&gt; &lt;li&gt;The gating mechanism provides significant benefits in larger models where parameter count is less constrained.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Training Stability:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I've discovered that activation function choice can dramatically affect training stability. GELU and Swish provide better stability than ReLU, especially for small models.&lt;/li&gt; &lt;li&gt;The smoothness of these functions helps with gradient flow, which is critical for stable training.&lt;/li&gt; &lt;li&gt;I've had training runs that failed with ReLU but succeeded with GELU, even with identical architectures and hyperparameters.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;My Decision Framework:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;For most small models, I use GELU it's the safe, reliable choice that works well.&lt;/li&gt; &lt;li&gt;If I have parameter budget and want to maximize performance, I use SwiGLU.&lt;/li&gt; &lt;li&gt;I only consider alternatives like ReLU if I have a specific reason or constraint.&lt;/li&gt; &lt;li&gt;Activation function isn't usually the bottleneck for small models, so I don't spend too much time optimizing it. GELU works, and that's often enough.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Summary&lt;/h1&gt; &lt;p&gt;Today we explored activation functions, the components that give neural networks their non-linear power. We learned how activation functions evolved from simple ReLU to sophisticated SwiGLU, and how they affect training stability and model performance.&lt;/p&gt; &lt;p&gt;Understanding activation functions is crucial because they're fundamental to how neural networks learn. The choice of activation function can mean the difference between a model that trains stably and one that fails, between a model that converges quickly and one that struggles. In the context of language models, activation functions work together with normalization, residual connections, and attention mechanisms to create the powerful architectures we use today.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prashant-Lakhera"&gt; /u/Prashant-Lakhera &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxj4lv/day_20_21_days_of_building_a_small_language_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxj4lv/day_20_21_days_of_building_a_small_language_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pxj4lv/day_20_21_days_of_building_a_small_language_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-28T05:18:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1pxm29c</id>
    <title>Unsloth GLM-4.7-GGUF?</title>
    <updated>2025-12-28T08:08:13+00:00</updated>
    <author>
      <name>/u/UnknownDude360</name>
      <uri>https://old.reddit.com/user/UnknownDude360</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all! I’m really excited to test out GLM-4.7 and I’ve been specifically waiting for Unsloth’s quants because they always cook!&lt;/p&gt; &lt;p&gt;Well, I’m a little confused. Which is “technically” better? I mean higher average bits? Less lossy. &lt;/p&gt; &lt;p&gt;Q3_K_M @ 171GB vs Q3_K_XL @ 159GB?&lt;/p&gt; &lt;p&gt;I have 48GB VRAM + 128GB DDR4 = 176GB absolute maximum ideally. &lt;/p&gt; &lt;p&gt;I would expect it be obvious, the _XL should be better than the _M… right?&lt;/p&gt; &lt;p&gt;However the more lossy quant is somehow bigger? Can someone help me reconcile this discrepancy? I feel kinda dumb overthinking this…&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/UnknownDude360"&gt; /u/UnknownDude360 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxm29c/unsloth_glm47gguf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxm29c/unsloth_glm47gguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pxm29c/unsloth_glm47gguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-28T08:08:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1pxsdnm</id>
    <title>XiaomiMiMo/MiMo-V2-Flash Under-rated?</title>
    <updated>2025-12-28T14:17:17+00:00</updated>
    <author>
      <name>/u/SlowFail2433</name>
      <uri>https://old.reddit.com/user/SlowFail2433</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;XiaomiMiMo/MiMo-V2-Flash has 310B param and top benches.&lt;/p&gt; &lt;p&gt;Seems to compete well with KimiK2Thinking, GLM4.7, MinimaxM2.1, Deepseek3.2&lt;/p&gt; &lt;p&gt;What do you think of this model?&lt;/p&gt; &lt;p&gt;Any use-cases welcome but particularly math, coding and agentic&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SlowFail2433"&gt; /u/SlowFail2433 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxsdnm/xiaomimimomimov2flash_underrated/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxsdnm/xiaomimimomimov2flash_underrated/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pxsdnm/xiaomimimomimov2flash_underrated/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-28T14:17:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1pxad0k</id>
    <title>NVIDIA Drops Pascal Support On Linux, Causing Chaos On Arch Linux</title>
    <updated>2025-12-27T22:22:21+00:00</updated>
    <author>
      <name>/u/HumanDrone8721</name>
      <uri>https://old.reddit.com/user/HumanDrone8721</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxad0k/nvidia_drops_pascal_support_on_linux_causing/"&gt; &lt;img alt="NVIDIA Drops Pascal Support On Linux, Causing Chaos On Arch Linux" src="https://external-preview.redd.it/Z1W-jCS5853m4eyzALlzqsbFjQ8v2fOj2tdMfCsU0J8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=232d0328fa3a116bc0a1917deae0e1763f4b6c47" title="NVIDIA Drops Pascal Support On Linux, Causing Chaos On Arch Linux" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HumanDrone8721"&gt; /u/HumanDrone8721 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://hackaday.com/2025/12/26/nvidia-drops-pascal-support-on-linux-causing-chaos-on-arch-linux/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxad0k/nvidia_drops_pascal_support_on_linux_causing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pxad0k/nvidia_drops_pascal_support_on_linux_causing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-27T22:22:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1pxss0m</id>
    <title>Senator in Tennessee introduces bill to felonize making AI "act as a companion" or "mirror human interactions"</title>
    <updated>2025-12-28T14:35:58+00:00</updated>
    <author>
      <name>/u/CanineAssBandit</name>
      <uri>https://old.reddit.com/user/CanineAssBandit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Call (202) 224-3121 for the Capitol switchboard to contact your representative.&lt;/p&gt; &lt;p&gt;The bill:&lt;br /&gt; &lt;a href="https://legiscan.com/TN/bill/SB1493/2025"&gt;https://legiscan.com/TN/bill/SB1493/2025&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Quotes from the bill (emphasis mine):&lt;/p&gt; &lt;p&gt;It is an offense for a person to knowingly train artificial intelligence to:&lt;br /&gt; (3) Provide emotional support, &lt;strong&gt;including through open-ended conversations&lt;/strong&gt; with a user;&lt;br /&gt; (4) Develop an emotional relationship with, or otherwise &lt;strong&gt;act as a companion&lt;/strong&gt; to, an individual;&lt;br /&gt; (6) Otherwise act as a sentient human or &lt;strong&gt;mirror interactions that a human user might have with another human user&lt;/strong&gt;, such that an individual would feel that the individual could develop a friendship or other relationship with the artificial intelligence;&lt;br /&gt; (8) &lt;strong&gt;Simulate a human being&lt;/strong&gt;, including in appearance, voice, or other mannerisms.&lt;/p&gt; &lt;p&gt;&amp;quot;Train&amp;quot;:&lt;br /&gt; (A) Means utilizing sets of data and other information to teach an artificial intelligence system to perceive, interpret, and learn from data, such that the A.I. will later be capable of &lt;strong&gt;making decisions based on information or other inputs&lt;/strong&gt; provided to the A.I.&lt;br /&gt; (B) Includes development of a large language model when the person developing the large language model knows that the model will be used to teach the A.I.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CanineAssBandit"&gt; /u/CanineAssBandit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxss0m/senator_in_tennessee_introduces_bill_to_felonize/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxss0m/senator_in_tennessee_introduces_bill_to_felonize/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pxss0m/senator_in_tennessee_introduces_bill_to_felonize/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-28T14:35:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptxm3x</id>
    <title>AMA With Z.AI, The Lab Behind GLM-4.7</title>
    <updated>2025-12-23T16:04:23+00:00</updated>
    <author>
      <name>/u/zixuanlimit</name>
      <uri>https://old.reddit.com/user/zixuanlimit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi r/LocalLLaMA&lt;/p&gt; &lt;p&gt;Today we are having &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt;, the research lab behind the GLM 4.7. We’re excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Yuxuan Zhang, &lt;a href="/u/YuxuanZhangzR"&gt;u/YuxuanZhangzR&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Qinkai Zheng, &lt;a href="/u/QinkaiZheng"&gt;u/QinkaiZheng&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Aohan Zeng, &lt;a href="/u/Sengxian"&gt;u/Sengxian&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Zhenyu Hou, &lt;a href="/u/ZhenyuHou"&gt;u/ZhenyuHou&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Xin Lv, &lt;a href="/u/davidlvxin"&gt;u/davidlvxin&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The AMA will run from 8 AM – 11 AM PST, with the &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt; team continuing to follow up on questions over the next 48 hours.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zixuanlimit"&gt; /u/zixuanlimit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T16:04:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwh0q9</id>
    <title>Best Local LLMs - 2025</title>
    <updated>2025-12-26T22:31:28+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Year end thread for the best LLMs of 2025!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;2025 is almost done! Its been &lt;strong&gt;a wonderful year&lt;/strong&gt; for us Open/Local AI enthusiasts. And its looking like Xmas time brought some great gifts in the shape of Minimax M2.1 and GLM4.7 that are touting frontier model performance. Are we there already? are we at parity with proprietary models?!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The standard spiel:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Share what your favorite models are right now &lt;strong&gt;and why.&lt;/strong&gt; Given the nature of the beast in evaluating LLMs (untrustworthiness of benchmarks, immature tooling, intrinsic stochasticity), please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Only open weights models&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;em&gt;Please thread your responses in the top level comments for each Application below to enable readability&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Applications&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;General&lt;/strong&gt;: Includes practical guidance, how to, encyclopedic QnA, search engine replacement/augmentation&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agentic/Agentic Coding/Tool Use/Coding&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Creative Writing/RP&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Speciality&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;If a category is missing, please create a top level comment under the Speciality comment&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Notes&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Useful breakdown of how folk are using LLMs: &lt;a href="https://preview.redd.it/i8td7u8vcewf1.png?width=1090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d"&gt;https://preview.redd.it/i8td7u8vcewf1.png?width=1090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d&lt;/a&gt; &lt;/p&gt; &lt;p&gt;A good suggestion for last time, breakdown/classify your recommendation by model memory footprint: (you can and should be using multiple models in each size range for different tasks)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Unlimited: &amp;gt;128GB VRAM &lt;/li&gt; &lt;li&gt;Medium: 8 to 128GB VRAM&lt;/li&gt; &lt;li&gt;Small: &amp;lt;8GB VRAM&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T22:31:28+00:00</published>
  </entry>
</feed>
