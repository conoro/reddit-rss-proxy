<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-15T09:49:21+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1o6iwrd</id>
    <title>Performance of llama.cpp on NVIDIA DGX Spark ¬∑ ggml-org/llama.cpp ¬∑ Discussion #16578</title>
    <updated>2025-10-14T15:38:49+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6iwrd/performance_of_llamacpp_on_nvidia_dgx_spark/"&gt; &lt;img alt="Performance of llama.cpp on NVIDIA DGX Spark ¬∑ ggml-org/llama.cpp ¬∑ Discussion #16578" src="https://external-preview.redd.it/jHQdSuZPiOdCRrmqghCS01mFwWiPh61nOi8HvEEUkiw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2fd9276c6ee8e677f95f5cd9bcd71ed7dde0ad2a" title="Performance of llama.cpp on NVIDIA DGX Spark ¬∑ ggml-org/llama.cpp ¬∑ Discussion #16578" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/discussions/16578"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6iwrd/performance_of_llamacpp_on_nvidia_dgx_spark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6iwrd/performance_of_llamacpp_on_nvidia_dgx_spark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T15:38:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1o72d6e</id>
    <title>Amd 8845HS (or same family) and max vram ?</title>
    <updated>2025-10-15T05:14:38+00:00</updated>
    <author>
      <name>/u/ResearcherNeither132</name>
      <uri>https://old.reddit.com/user/ResearcherNeither132</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I‚Äôm want to use a mini PC with an AMD Ryzen 7 8845HS and the integrated Radeon 780M GPU for LLM.&lt;br /&gt; I know that the VRAM is shared from system RAM (UMA), and in the BIOS I can set the UMA Frame Buffer Size up to 16 GB.&lt;/p&gt; &lt;p&gt;it possible to increase the VRAM allocation beyond 16 GB ‚Äî for example, if I have 128 or 256 GB ?&lt;/p&gt; &lt;p&gt;Or is 16 GB the hard limit ?&lt;/p&gt; &lt;p&gt;Also, does the GPU dynamically use more than that 16 GB when needed (through UMA), or is it really capped at that value?&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearcherNeither132"&gt; /u/ResearcherNeither132 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o72d6e/amd_8845hs_or_same_family_and_max_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o72d6e/amd_8845hs_or_same_family_and_max_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o72d6e/amd_8845hs_or_same_family_and_max_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T05:14:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6izz2</id>
    <title>DGX Spark vs AI Max 395+</title>
    <updated>2025-10-14T15:42:08+00:00</updated>
    <author>
      <name>/u/Responsible-Let9423</name>
      <uri>https://old.reddit.com/user/Responsible-Let9423</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyone has fair comparison between two tiny AI PCs.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Responsible-Let9423"&gt; /u/Responsible-Let9423 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6izz2/dgx_spark_vs_ai_max_395/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6izz2/dgx_spark_vs_ai_max_395/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6izz2/dgx_spark_vs_ai_max_395/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T15:42:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6yz59</id>
    <title>[WebGPU Demo] Granite Docling 258M ‚Äî document parsing 100% in-browser (HF Space)</title>
    <updated>2025-10-15T02:18:14+00:00</updated>
    <author>
      <name>/u/freesysck</name>
      <uri>https://old.reddit.com/user/freesysck</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Run IBM‚Äôs &lt;strong&gt;Granite-Docling-258M&lt;/strong&gt; entirely in your browser via &lt;strong&gt;WebGPU + Transformers.js&lt;/strong&gt; to convert scanned pages/images into structured &lt;strong&gt;HTML&lt;/strong&gt;‚Äîno data leaves your machine. &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Upload &lt;strong&gt;PNG/JPG/WEBP&lt;/strong&gt; ‚Üí get clean HTML. &lt;/li&gt; &lt;li&gt;Local/WebGPU execution = privacy-friendly.&lt;/li&gt; &lt;li&gt;Link: &lt;a href="https://huggingface.co/spaces/ibm-granite/granite-docling-258M-WebGPU"&gt;&lt;code&gt;https://huggingface.co/spaces/ibm-granite/granite-docling-258M-WebGPU&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/freesysck"&gt; /u/freesysck &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6yz59/webgpu_demo_granite_docling_258m_document_parsing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6yz59/webgpu_demo_granite_docling_258m_document_parsing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6yz59/webgpu_demo_granite_docling_258m_document_parsing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T02:18:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6rqay</id>
    <title>I got fed up with Open WebUI/LibreChat for local LLMs so I made an open source tool to turn my GPU server into an always-on assistant</title>
    <updated>2025-10-14T21:03:41+00:00</updated>
    <author>
      <name>/u/ai-christianson</name>
      <uri>https://old.reddit.com/user/ai-christianson</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all, I've been running local LLMs since the beginning and have always felt like LLM chat interfaces like Open WebUI/LibreChat/SillyTavern are great, but there must be so much more that we can do with local LLMs. I paid a lot for my GPU servers, so I actually want them to &lt;em&gt;do work&lt;/em&gt; for me.&lt;/p&gt; &lt;p&gt;Furthermore, local LLMs are generally higher latency than cloud services. It's a bit annoying to have to wait for a local LLM to fully generate a response, even though the response can be really good. I've always wanted the LLM to keep churning for me overnight, long after I've closed the chat tab. I don't care if it generates at 5 toks/sec if it is always doing work for me in the background.&lt;/p&gt; &lt;p&gt;Then there's the aspect that inference engines like vllm can get much higher batch throughput, but it hurts the latency a bit. It would be great to stack up many concurrent LLM requests. This would let me really extract the most &lt;em&gt;productivity&lt;/em&gt; out of my GPU servers over time.&lt;/p&gt; &lt;p&gt;So it put all the best ideas together, including all the lessons learned from the open source coding agent I previously built (RA.Aid), and built an open source platform for running agents that are always on.&lt;/p&gt; &lt;p&gt;The heart of the system is the incredible &lt;a href="https://github.com/browser-use/browser-use"&gt;browser-use&lt;/a&gt; project. So right of the bat we get web browsing agents, which is one of keys to being able to do productive work. The agents can access websites, web apps, and interact with them the way a human would.&lt;/p&gt; &lt;p&gt;But the big challenge with browser-use is that it requires writing custom code for each agent, and the agents don't run 24/7, and they lack high level planning and orchestration. I want to just tell my GPU server what I want it to do and &lt;em&gt;put it to work&lt;/em&gt; and have it get back to me when the job is done.&lt;/p&gt; &lt;p&gt;So that's exactly what I've built, and it's OSS (MIT licensed). You can check it out at &lt;a href="https://github.com/gobii-ai/gobii-platform"&gt;https://github.com/gobii-ai/gobii-platform&lt;/a&gt;&lt;/p&gt; &lt;p&gt;To get it running, all you have to do is clone the repo and run: &lt;strong&gt;docker compose up --build&lt;/strong&gt;. It will take a minute to get set up, then a web UI will be available at localhost:8000. You can configure the key settings using the graphical config wizard, which is basically just the default account username/password and your local LLM inference endpoint.&lt;/p&gt; &lt;p&gt;Once it's running, you'll see a big text box at localhost:8000. Just type what you want it to do, like &amp;quot;find me the best priced 3090s on ebay from sellers that have good reviews&amp;quot; and it will do everything, including spawning a full chrome instance in an xvfb environment. It will set its own schedule, or you can ask it explicitly to check every 3 hours, for example.&lt;/p&gt; &lt;p&gt;The best part? If your hardware is not super fast for running local LLMs, you can configure it with an email account using SMTP/IMAP and it will &lt;strong&gt;automatically contact you when it has the results&lt;/strong&gt;, e.g. when it finds the 3090s you're looking for on ebay, it will email you links to them. You don't have to sit there waiting for your hardware to churn out the tokens.&lt;/p&gt; &lt;p&gt;And here's where it gets really cool: you can spin up as many of these agents as you want &lt;strong&gt;and you can link them together&lt;/strong&gt; so they can DM one another and work as a team. This means if you're running an inference server like vllm, it will actually turn that massive concurrent token throughput into &lt;em&gt;productive work&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;I hope you all like this as it took quite a bit of effort to put together. The whole idea here is to mine as much actual productive work as possible out of the expensive GPUs you already have. You can literally turn that GPU server into an &lt;em&gt;always-on team of assistants&lt;/em&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ai-christianson"&gt; /u/ai-christianson &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6rqay/i_got_fed_up_with_open_webuilibrechat_for_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6rqay/i_got_fed_up_with_open_webuilibrechat_for_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6rqay/i_got_fed_up_with_open_webuilibrechat_for_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T21:03:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1o61gzs</id>
    <title>Nvidia breakthrough gives 4-bit pretraining technique the accuracy of FP8</title>
    <updated>2025-10-14T00:47:06+00:00</updated>
    <author>
      <name>/u/dionisioalcaraz</name>
      <uri>https://old.reddit.com/user/dionisioalcaraz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o61gzs/nvidia_breakthrough_gives_4bit_pretraining/"&gt; &lt;img alt="Nvidia breakthrough gives 4-bit pretraining technique the accuracy of FP8" src="https://preview.redd.it/fjr53w0m4zuf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b1986eb8662405e67e0522e5d8d37f03ea577ffc" title="Nvidia breakthrough gives 4-bit pretraining technique the accuracy of FP8" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;-NVFP4 is a way to store numbers for training large models using just 4 bits instead of 8 or 16. This makes training faster and use less memory&lt;/p&gt; &lt;p&gt;-NVFP4 shows 4-bit pretraining of a 12B Mamba Transformer on 10T tokens can match FP8 accuracy while cutting compute and memory.&lt;/p&gt; &lt;p&gt;-The validation loss stays within 1% of FP8 for most of training and grows to about 1.5% late during learning rate decay. &lt;/p&gt; &lt;p&gt;-Task scores stay close, for example MMLU Pro 62.58% vs 62.62%, while coding dips a bit like MBPP+ 55.91% vs 59.11%.&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/godofprompt/status/1977678347879714912"&gt;X thread&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="http://arxiv.org/abs/2509.25149"&gt;Arxiv paper&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dionisioalcaraz"&gt; /u/dionisioalcaraz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fjr53w0m4zuf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o61gzs/nvidia_breakthrough_gives_4bit_pretraining/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o61gzs/nvidia_breakthrough_gives_4bit_pretraining/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T00:47:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6oaa4</id>
    <title>Those who reserved Nvidia's DGX Spark are starting to receive purchase invitation emails</title>
    <updated>2025-10-14T18:55:40+00:00</updated>
    <author>
      <name>/u/sketharapu</name>
      <uri>https://old.reddit.com/user/sketharapu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6oaa4/those_who_reserved_nvidias_dgx_spark_are_starting/"&gt; &lt;img alt="Those who reserved Nvidia's DGX Spark are starting to receive purchase invitation emails" src="https://preview.redd.it/7w1yhhrhj4vf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=31ba63d90457b18277246650e0e8756589cac761" title="Those who reserved Nvidia's DGX Spark are starting to receive purchase invitation emails" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just received this email&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sketharapu"&gt; /u/sketharapu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7w1yhhrhj4vf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6oaa4/those_who_reserved_nvidias_dgx_spark_are_starting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6oaa4/those_who_reserved_nvidias_dgx_spark_are_starting/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T18:55:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1o74ufs</id>
    <title>Coding assistant with web search?</title>
    <updated>2025-10-15T07:49:40+00:00</updated>
    <author>
      <name>/u/ramendik</name>
      <uri>https://old.reddit.com/user/ramendik</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Was anyone successful at getting any open source coding assistant to offer web search tools and to get the model to actually use them when tricky library/framework/etc questions arise? If so I'd appreciate the configuration details.&lt;/p&gt; &lt;p&gt;Asking after chasing an Alpine.js UI glitch in endless circles until I went to Gemini web, which has built in search grounding. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ramendik"&gt; /u/ramendik &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o74ufs/coding_assistant_with_web_search/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o74ufs/coding_assistant_with_web_search/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o74ufs/coding_assistant_with_web_search/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T07:49:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1o75bah</id>
    <title>best local model for article analysis and summarization</title>
    <updated>2025-10-15T08:20:53+00:00</updated>
    <author>
      <name>/u/Luke1144</name>
      <uri>https://old.reddit.com/user/Luke1144</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i‚Äôm early in my testing journey of determining the best local model for my use case. &lt;/p&gt; &lt;p&gt;in this particular instance i‚Äôm trying to find a local model that can ingest article data and output structured responses around key points, impact analysis, and things of that nature.&lt;/p&gt; &lt;p&gt;is there a model that you think would best suit this kind of work?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Luke1144"&gt; /u/Luke1144 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o75bah/best_local_model_for_article_analysis_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o75bah/best_local_model_for_article_analysis_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o75bah/best_local_model_for_article_analysis_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T08:20:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6s89n</id>
    <title>Tested 9 RAG query transformation techniques ‚Äì HydE is absurdly underrated</title>
    <updated>2025-10-14T21:22:30+00:00</updated>
    <author>
      <name>/u/Best-Information2493</name>
      <uri>https://old.reddit.com/user/Best-Information2493</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6s89n/tested_9_rag_query_transformation_techniques_hyde/"&gt; &lt;img alt="Tested 9 RAG query transformation techniques ‚Äì HydE is absurdly underrated" src="https://preview.redd.it/fq5i6e8q95vf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f8db07dad84a6951edf7b8129992c0a4a7da454f" title="Tested 9 RAG query transformation techniques ‚Äì HydE is absurdly underrated" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Your RAG system isn't bad. Your queries are.&lt;/p&gt; &lt;p&gt;I just tested 9 query transformation techniques. Here's what actually moved the needle:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Top 3:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;HydE&lt;/strong&gt; ‚Äì Generate a hypothetical answer, search for docs similar to &lt;em&gt;that&lt;/em&gt;. Sounds dumb, works incredibly well. Solves the semantic gap problem.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RAG-Fusion&lt;/strong&gt; ‚Äì Multi-query + reranking. Simple, effective, production-ready.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Step-Back&lt;/strong&gt; ‚Äì Ask abstract questions first. &amp;quot;What is photosynthesis?&amp;quot; before &amp;quot;How do C4 plants fix carbon?&amp;quot;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Meh tier:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Multi-Query: Good baseline, nothing special&lt;/li&gt; &lt;li&gt;Decomposition: Works but adds complexity&lt;/li&gt; &lt;li&gt;Recursive: Slow, minimal quality gain for simple queries&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Key insight:&lt;/strong&gt; You're spending time optimizing embeddings when your query formulation is the actual bottleneck.&lt;/p&gt; &lt;p&gt;Notebook: &lt;a href="https://colab.research.google.com/drive/1HXhEudDjJsXCvP3tO4G7cAC15OyKW3nM?usp=sharing"&gt;https://colab.research.google.com/drive/1HXhEudDjJsXCvP3tO4G7cAC15OyKW3nM?usp=sharing&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What techniques are you using? Anyone else seeing HydE results this good?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Best-Information2493"&gt; /u/Best-Information2493 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fq5i6e8q95vf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6s89n/tested_9_rag_query_transformation_techniques_hyde/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6s89n/tested_9_rag_query_transformation_techniques_hyde/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T21:22:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6hjgw</id>
    <title>[Open Source] We built a production-ready GenAI framework after deploying 50+ agents. Here's what we learned üçï</title>
    <updated>2025-10-14T14:48:01+00:00</updated>
    <author>
      <name>/u/mario_candela</name>
      <uri>https://old.reddit.com/user/mario_candela</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; ! üëã&lt;/p&gt; &lt;p&gt;After building and deploying 50+ GenAI solutions in production, we got tired of fighting with bloated frameworks, debugging black boxes, and dealing with vendor lock-in. So we built Datapizza AI - a Python framework that actually respects your time.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Problem We Solved&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Most LLM frameworks give you two bad options:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Too much magic ‚Üí You have no idea why your agent did what it did&lt;/li&gt; &lt;li&gt;Too little structure ‚Üí You're rebuilding the same patterns over and over&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We wanted something that's predictable, debuggable, and production-ready from day one.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What Makes It Different&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;üîç Built-in Observability: OpenTelemetry tracing out of the box. See exactly what your agents are doing, track token usage, and debug performance issues without adding extra libraries.&lt;/p&gt; &lt;p&gt;ü§ù Multi-Agent Collaboration: Agents can call other specialized agents. Build a trip planner that coordinates weather experts and web researchers - it just works.&lt;/p&gt; &lt;p&gt;üìö Production-Grade RAG: From document ingestion to reranking, we handle the entire pipeline. No more duct-taping 5 different libraries together.&lt;/p&gt; &lt;p&gt;üîå Vendor Agnostic: Start with OpenAI, switch to Claude, add Gemini - same code. We support OpenAI, Anthropic, Google, Mistral, and Azure.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why We're Sharing This&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;We believe in less abstraction, more control. If you've ever been frustrated by frameworks that hide too much or provide too little, this might be for you.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;üêô GitHub: &lt;a href="https://github.com/datapizza-labs/datapizza-ai"&gt;https://github.com/datapizza-labs/datapizza-ai&lt;/a&gt;&lt;/li&gt; &lt;li&gt;üìñ Docs: &lt;a href="https://docs.datapizza.ai"&gt;https://docs.datapizza.ai&lt;/a&gt;&lt;/li&gt; &lt;li&gt;üè† Website: &lt;a href="https://datapizza.tech/en/ai-framework/"&gt;https://datapizza.tech/en/ai-framework/&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;We Need Your Help! üôè&lt;/h1&gt; &lt;p&gt;We're actively developing this and would love to hear:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;What features would make this useful for YOUR use case?&lt;/li&gt; &lt;li&gt;What problems are you facing with current LLM frameworks?&lt;/li&gt; &lt;li&gt;Any bugs or issues you encounter (we respond fast!)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Star us on GitHub if you find this interesting,&lt;/strong&gt; it genuinely helps us understand if we're solving real problems.&lt;/p&gt; &lt;p&gt;Happy to answer any questions in the comments! üçï&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mario_candela"&gt; /u/mario_candela &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6hjgw/open_source_we_built_a_productionready_genai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6hjgw/open_source_we_built_a_productionready_genai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6hjgw/open_source_we_built_a_productionready_genai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T14:48:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6h8jn</id>
    <title>We tested Claude Sonnet 4.5, GPT-5-codex, Qwen3-Coder, GLM and other 25+ models on fresh SWE-Bench like tasks from September 2025</title>
    <updated>2025-10-14T14:36:10+00:00</updated>
    <author>
      <name>/u/Fabulous_Pollution10</name>
      <uri>https://old.reddit.com/user/Fabulous_Pollution10</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, I‚Äôm Ibragim from Nebius.&lt;/p&gt; &lt;p&gt;We‚Äôve updated the &lt;strong&gt;SWE-rebench&lt;/strong&gt; leaderboard with September runs on &lt;strong&gt;49 fresh GitHub PR bug-fix tasks&lt;/strong&gt; (last-month PR issues only). It‚Äôs a SWE-bench‚Äìstyle setup: models read real PR issues, run tests, edit code, and must make the suite pass.&lt;/p&gt; &lt;p&gt;Models: &lt;strong&gt;Sonnet-4.5, GPT-5-Codex, Grok Code Fast 1, GLM, Qwen, Kimi&lt;/strong&gt; and others&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Claude Sonnet 4.5 achieved the highest &lt;em&gt;pass@5&lt;/em&gt; (&lt;strong&gt;55.1%&lt;/strong&gt;) and uniquely solving several instances that &lt;strong&gt;no other model&lt;/strong&gt; on the leaderboard managed to resolve: &lt;a href="https://github.com/python-trio/trio/pull/3334"&gt;&lt;strong&gt;python-trio/trio-3334&lt;/strong&gt;&lt;/a&gt;, &lt;a href="https://github.com/cubed-dev/cubed/pull/799"&gt;&lt;strong&gt;cubed-dev/cubed-799&lt;/strong&gt;&lt;/a&gt;, &lt;a href="https://github.com/canopen-python/canopen/pull/613"&gt;&lt;strong&gt;canopen-python/canopen-613&lt;/strong&gt;&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Qwen3-Coder&lt;/strong&gt; is the &lt;strong&gt;best open-source performer&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;All models on the leaderboard were evaluated using the ChatCompletions API, except for &lt;a href="https://platform.openai.com/docs/models/gpt-5-codex"&gt;&lt;strong&gt;gpt-5-codex&lt;/strong&gt;&lt;/a&gt; and &lt;a href="https://platform.openai.com/docs/models/gpt-oss-120b"&gt;&lt;strong&gt;gpt-oss-120b&lt;/strong&gt;&lt;/a&gt;, which are only accessible via the Responses API.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please check the leaderboard, the insights, and write if you want to request some models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fabulous_Pollution10"&gt; /u/Fabulous_Pollution10 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://swe-rebench.com/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6h8jn/we_tested_claude_sonnet_45_gpt5codex_qwen3coder/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6h8jn/we_tested_claude_sonnet_45_gpt5codex_qwen3coder/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T14:36:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6u5o4</id>
    <title>gpt-oss20/120b AMD Strix Halo vs NVIDIA DGX Spark benchmark</title>
    <updated>2025-10-14T22:40:20+00:00</updated>
    <author>
      <name>/u/Educational_Sun_8813</name>
      <uri>https://old.reddit.com/user/Educational_Sun_8813</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;[EDIT] seems, that their results are way off, and for real performance values check: &lt;a href="https://github.com/ggml-org/llama.cpp/discussions/16578"&gt;https://github.com/ggml-org/llama.cpp/discussions/16578&lt;/a&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Metric&lt;/th&gt; &lt;th align="left"&gt;NVIDIA DGX Spark (ollama)&lt;/th&gt; &lt;th align="left"&gt;Strix Halo (llama.cpp)&lt;/th&gt; &lt;th align="left"&gt;Winner&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;gpt-oss 20b&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Prompt Processing (Prefill)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;2,053.98 t/s&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;1,332.70 t/s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;NVIDIA DGX Spark&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;gpt-oss 20b&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Token Generation (Decode)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;49.69 t/s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;72.87 t/s&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Strix Halo&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;gpt-oss 120b&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Prompt Processing (Prefill)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;94.67 t/s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;526.15 t/s&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Strix Halo&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;gpt-oss 120b&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Token Generation (Decode)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;11.66 t/s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;51.39 t/s&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Strix Halo&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Educational_Sun_8813"&gt; /u/Educational_Sun_8813 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6u5o4/gptoss20120b_amd_strix_halo_vs_nvidia_dgx_spark/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6u5o4/gptoss20120b_amd_strix_halo_vs_nvidia_dgx_spark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6u5o4/gptoss20120b_amd_strix_halo_vs_nvidia_dgx_spark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T22:40:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6vb48</id>
    <title>Quick Guide: Running Qwen3-Next-80B-A3B-Instruct-Q4_K_M Locally with FastLLM (Windows)</title>
    <updated>2025-10-14T23:29:26+00:00</updated>
    <author>
      <name>/u/ThetaCursed</name>
      <uri>https://old.reddit.com/user/ThetaCursed</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6vb48/quick_guide_running_qwen3next80ba3binstructq4_k_m/"&gt; &lt;img alt="Quick Guide: Running Qwen3-Next-80B-A3B-Instruct-Q4_K_M Locally with FastLLM (Windows)" src="https://b.thumbs.redditmedia.com/GVz5Ej45qwNz7Z-7_HggplD-Q37GuOg6QSgfhll7Bek.jpg" title="Quick Guide: Running Qwen3-Next-80B-A3B-Instruct-Q4_K_M Locally with FastLLM (Windows)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;Nailed it first try with &lt;strong&gt;FastLLM&lt;/strong&gt;! No fuss.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Setup &amp;amp; Perf&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Required&lt;/strong&gt;: ~6 GB VRAM (for some reason it wasn't using my GPU to its maximum) + 48 GB RAM&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Speed&lt;/strong&gt;: ~8 t/s&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThetaCursed"&gt; /u/ThetaCursed &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1o6vb48"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6vb48/quick_guide_running_qwen3next80ba3binstructq4_k_m/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6vb48/quick_guide_running_qwen3next80ba3binstructq4_k_m/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T23:29:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1o734qe</id>
    <title>Running Qwen3-4B on a 6-Year-Old AMD APU? Yes, and It Works Surprisingly Well!</title>
    <updated>2025-10-15T06:00:03+00:00</updated>
    <author>
      <name>/u/rtsov</name>
      <uri>https://old.reddit.com/user/rtsov</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Running Qwen3-4B on a 6-Year-Old AMD APU? Yes, and It Works Surprisingly Well!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I just successfully ran &lt;strong&gt;unsloth/Qwen3-4B-Instruct-2507-UD-Q4_K_XL.gguf&lt;/strong&gt; on a modest home server with the following specs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;CPU&lt;/strong&gt;: AMD Ryzen 5 2400G (8) @ 3.600GHz&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RAM&lt;/strong&gt;: 16 GB (2 √ó 8 GiB DDR4-2133, unbuffered, unregistered)&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;iGPU&lt;/strong&gt;: Radeon Vega 11 (with 2 GB of VRAM allocated in BIOS)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;And the results?&lt;br /&gt; ‚úÖ &lt;strong&gt;Prompt processing&lt;/strong&gt;: &lt;strong&gt;25.9 tokens/sec&lt;/strong&gt; (24 tokens)&lt;br /&gt; ‚úÖ &lt;strong&gt;Text generation&lt;/strong&gt;: &lt;strong&gt;9.76 tokens/sec&lt;/strong&gt; (1,264 tokens)&lt;/p&gt; &lt;p&gt;This is honestly &lt;strong&gt;unexpected&lt;/strong&gt;‚Äîbut it turns out that the Vega 11 iGPU, often overlooked for AI workloads, can actually handle &lt;strong&gt;lightweight LLM tasks&lt;/strong&gt; like news summarization or simple agent workflows quite effectively‚Äîeven on hardware from 2018!&lt;/p&gt; &lt;h3&gt;Key Setup Details&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;BIOS&lt;/strong&gt;: 2 GB of system RAM allocated to integrated graphics&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Debian 12 with kernel (6.1.0-40-amd64) parameters&lt;/strong&gt;:&lt;br /&gt; &lt;code&gt;text GRUB_CMDLINE_LINUX_DEFAULT=&amp;quot;amdgpu.gttsize=8192&amp;quot; &lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Runtime&lt;/strong&gt;: &lt;code&gt;llama.cpp&lt;/code&gt; with &lt;strong&gt;Vulkan backend&lt;/strong&gt;, running inside a Docker container:&lt;br /&gt; &lt;a href="https://github.com/mostlygeek/llama-swap"&gt;&lt;code&gt;ghcr.io/mostlygeek/llama-swap:vulkan&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;Docker Compose&lt;/h3&gt; &lt;p&gt;&lt;code&gt;yaml services: llama-swap: container_name: llama-swap image: ghcr.io/mostlygeek/llama-swap:vulkan devices: - /dev/kfd - /dev/dri group_add: - &amp;quot;video&amp;quot; security_opt: - seccomp=unconfined shm_size: 2g environment: - AMD_VISIBLE_DEVICES=all command: /app/llama-swap -config /app/config.yaml -watch-config &lt;/code&gt;&lt;/p&gt; &lt;h3&gt;llama-swap Config (&lt;code&gt;config.yaml&lt;/code&gt;)&lt;/h3&gt; &lt;p&gt;```yaml macros: &amp;quot;llama-server-default&amp;quot;: | /app/llama-server --port ${PORT} --flash-attn on --no-webui&lt;/p&gt; &lt;p&gt;models: &amp;quot;qwen3-4b-instruct-2507&amp;quot;: name: &amp;quot;qwen3-4b-instruct-2507&amp;quot; cmd: | ${llama-server-default} --model /models/Qwen3-4B-Instruct-2507-UD-Q4_K_XL.gguf --ctx-size 4096 --temp 0.7 --top-k 20 --top-p 0.8 --min-p 0.0 --repeat-penalty 1.05 --cache-type-k q8_0 --cache-type-v q8_0 --jinja ttl: 60 ```&lt;/p&gt; &lt;h3&gt;Takeaway&lt;/h3&gt; &lt;p&gt;You &lt;strong&gt;don‚Äôt need a high-end GPU&lt;/strong&gt; to experiment with modern 4B-parameter models. With the right optimizations (Vulkan + llama.cpp + proper iGPU tuning), even aging AMD APUs can serve as capable local LLM endpoints for everyday tasks.&lt;/p&gt; &lt;p&gt;If you‚Äôve got an old Ryzen desktop lying around‚Äîgive it a try! üöÄ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rtsov"&gt; /u/rtsov &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o734qe/running_qwen34b_on_a_6yearold_amd_apu_yes_and_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o734qe/running_qwen34b_on_a_6yearold_amd_apu_yes_and_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o734qe/running_qwen34b_on_a_6yearold_amd_apu_yes_and_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T06:00:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6ofr9</id>
    <title>Intel Crescent Island GPU: 160GB of LPDDR5X memory</title>
    <updated>2025-10-14T19:01:14+00:00</updated>
    <author>
      <name>/u/On1ineAxeL</name>
      <uri>https://old.reddit.com/user/On1ineAxeL</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;About the GPU:&lt;/strong&gt; The new data center GPU code-named Crescent Island is being designed to be power and cost-optimized for air-cooled enterprise servers and to incorporate large amounts of memory capacity and bandwidth, optimized for inference workflows. &lt;/p&gt; &lt;p&gt;Key features include: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Xe3P microarchitecture with optimized performance-per-watt &lt;/li&gt; &lt;li&gt;160GB of LPDDR5X memory &lt;/li&gt; &lt;li&gt;Support for a broad range of data types, ideal for ‚Äútokens-as-a-service‚Äù providers and inference use cases &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://videocardz.com/newz/intel-confirms-xe3p-architecture-to-power-new-crescent-island-data-center-gpu-with-160gb-lpddr5x-memory"&gt;https://videocardz.com/newz/intel-confirms-xe3p-architecture-to-power-new-crescent-island-data-center-gpu-with-160gb-lpddr5x-memory&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://newsroom.intel.com/artificial-intelligence/intel-to-expand-ai-accelerator-portfolio-with-new-gpu"&gt;https://newsroom.intel.com/artificial-intelligence/intel-to-expand-ai-accelerator-portfolio-with-new-gpu&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/On1ineAxeL"&gt; /u/On1ineAxeL &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6ofr9/intel_crescent_island_gpu_160gb_of_lpddr5x_memory/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6ofr9/intel_crescent_island_gpu_160gb_of_lpddr5x_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6ofr9/intel_crescent_island_gpu_160gb_of_lpddr5x_memory/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T19:01:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6wfpy</id>
    <title>GPT-OSS-20b TAKE THE WHEEL!</title>
    <updated>2025-10-15T00:20:31+00:00</updated>
    <author>
      <name>/u/teachersecret</name>
      <uri>https://old.reddit.com/user/teachersecret</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6wfpy/gptoss20b_take_the_wheel/"&gt; &lt;img alt="GPT-OSS-20b TAKE THE WHEEL!" src="https://external-preview.redd.it/EYfYrBdrbHJnRl1EvzhfVjhkBpr2GL8UU-8scxe6WCU.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0d3d6c023c0a8855a48f130b4207e4980d5b4f49" title="GPT-OSS-20b TAKE THE WHEEL!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In this experiment, I use a single 4090 hooked up to VLLM and a batching GPT-OSS-20b model set up with prefill prompts that explain the current game state (direction/velocity/location of asteroids and the direction/velocity/location of our ship in relation to them), and the LLM is forced to make a control decision to either turn left 25%, turn right 25%, thrust forward, reverse (turn 180 degrees and thrust), or fire. Since I'm only generating one token per generation, I am able to get latency down under 20ms, allowing the AI to make rapid fire decisions (multiple-per-second) and to apply them as control inputs to the spaceship.&lt;/p&gt; &lt;p&gt;As it runs, it's generating a high speed continuous stream of 20ms responses to input thanks to the continuous batching VLLM server (a largely prefix cached prompt with a bit of information updating the current game-state so it can make an input decision in near-realtime). It's able to successfully autopilot the ship around. I also gave it some instructions and a reward (higher points) for flying closer to asteroids and 'hot dogging' which made its chosen flightpath a bit more interesting.&lt;/p&gt; &lt;p&gt;I know it's just a silly experiment, and yes, it would be absolutely trivial to make a simple algorithm that could fly this ship around safely without needing hundreds of watts of screaming GPU, but I thought someone might appreciate making OSS 20b into a little autopilot that knows what's going on around it and controls the ship like it's using a game controller at latency that makes it a fairly competent pilot.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/teachersecret"&gt; /u/teachersecret &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=NY6htCUWFqI"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6wfpy/gptoss20b_take_the_wheel/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6wfpy/gptoss20b_take_the_wheel/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T00:20:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6pmxt</id>
    <title>Real-time study buddy that sees your screen and talks back</title>
    <updated>2025-10-14T19:46:04+00:00</updated>
    <author>
      <name>/u/Weary-Wing-6806</name>
      <uri>https://old.reddit.com/user/Weary-Wing-6806</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6pmxt/realtime_study_buddy_that_sees_your_screen_and/"&gt; &lt;img alt="Real-time study buddy that sees your screen and talks back" src="https://external-preview.redd.it/ZXlwZW5pYTNuNHZmMUsYDHUptOq0sYO1cNNkCl_tbC9KzkSKWyT6VTZxxWFL.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ff99a7daf7c346698fa23df7509fc456a4b3edc3" title="Real-time study buddy that sees your screen and talks back" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Built a real-time learning assistant that sees your screen, talks, and learns alongside you. All open models (Qwen3-VL, Parakeet, Orpheus) wired together. &lt;/p&gt; &lt;p&gt;I shared a biology site on cell structure to see if it could describe the page, identify the diagram, and answer targeted questions about the mitochondria. &lt;/p&gt; &lt;p&gt;These text and vision models are getting so good. Wiring them together levels them all up. Next step: going to try running it across multiple sites and have it auto-summarize my learnings into a study guide or PDF after.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Weary-Wing-6806"&gt; /u/Weary-Wing-6806 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ctp0k9a3n4vf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6pmxt/realtime_study_buddy_that_sees_your_screen_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6pmxt/realtime_study_buddy_that_sees_your_screen_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T19:46:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6zg97</id>
    <title>[Update] Qwen3-VL cookbooks coming ‚Äî recognition, localization, doc parsing, video</title>
    <updated>2025-10-15T02:41:33+00:00</updated>
    <author>
      <name>/u/freesysck</name>
      <uri>https://old.reddit.com/user/freesysck</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6zg97/update_qwen3vl_cookbooks_coming_recognition/"&gt; &lt;img alt="[Update] Qwen3-VL cookbooks coming ‚Äî recognition, localization, doc parsing, video" src="https://external-preview.redd.it/sMJBVR2ChB4qgLOpxT2QxURyjiN_Zh_hva5OCRGa7Ls.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=17e9d23803b4ee9beb0893f3fff3d9a55771c058" title="[Update] Qwen3-VL cookbooks coming ‚Äî recognition, localization, doc parsing, video" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/ny9qsbphu6vf1.png?width=955&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=89a0a9878dd4c655c5d082543041d9d2c36b22fb"&gt;https://preview.redd.it/ny9qsbphu6vf1.png?width=955&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=89a0a9878dd4c655c5d082543041d9d2c36b22fb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;cookbooks&lt;/strong&gt; for a bunch of real-world capabilities‚Äî&lt;strong&gt;recognition&lt;/strong&gt;, &lt;strong&gt;localization&lt;/strong&gt;, &lt;strong&gt;document parsing&lt;/strong&gt;, &lt;strong&gt;video understanding&lt;/strong&gt;, &lt;strong&gt;key information extraction&lt;/strong&gt;, and more&lt;/p&gt; &lt;h1&gt;Cookbooks&lt;/h1&gt; &lt;p&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL#cookbooks"&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We are preparing &lt;a href="https://github.com/QwenLM/Qwen3-VL/tree/main/cookbooks"&gt;cookbooks&lt;/a&gt; for many capabilities, including recognition, localization, document parsing, video understanding, key information extraction, and more. Welcome to learn more!&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Cookbook&lt;/th&gt; &lt;th align="left"&gt;Description&lt;/th&gt; &lt;th align="left"&gt;Open&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/blob/main/cookbooks/omni_recognition.ipynb"&gt;Omni Recognition&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Not only identify animals, plants, people, and scenic spots but also recognize various objects such as cars and merchandise.&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/omni_recognition.ipynb"&gt;&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/blob/main/cookbooks/document_parsing.ipynb"&gt;Powerful Document Parsing Capabilities&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;The parsing of documents has reached a higher level, including not only text but also layout position information and our Qwen HTML format.&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/document_parsing.ipynb"&gt;&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/blob/main/cookbooks/2d_grounding.ipynb"&gt;Precise Object Grounding Across Formats&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Using relative position coordinates, it supports both boxes and points, allowing for diverse combinations of positioning and labeling tasks.&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/2d_grounding.ipynb"&gt;&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/blob/main/cookbooks/ocr.ipynb"&gt;General OCR and Key Information Extraction&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Stronger text recognition capabilities in natural scenes and multiple languages, supporting diverse key information extraction needs.&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/ocr.ipynb"&gt;&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/blob/main/cookbooks/video_understanding.ipynb"&gt;Video Understanding&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Better video OCR, long video understanding, and video grounding.&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/video_understanding.ipynb"&gt;&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/blob/main/cookbooks/mobile_agent.ipynb"&gt;Mobile Agent&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Locate and think for mobile phone control.&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/mobile_agent.ipynb"&gt;&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/blob/main/cookbooks/computer_use.ipynb"&gt;Computer-Use Agent&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Locate and think for controlling computers and Web.&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/computer_use.ipynb"&gt;&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/blob/main/cookbooks/3d_grounding.ipynb"&gt;3D Grounding&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Provide accurate 3D bounding boxes for both indoor and outdoor objects.&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/3d_grounding.ipynb"&gt;&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/blob/main/cookbooks/think_with_images.ipynb"&gt;Thinking with Images&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Utilize image_zoom_in_tool and search_tool to facilitate the model‚Äôs precise comprehension of fine-grained visual details within images.&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/think_with_images.ipynb"&gt;&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/blob/main/cookbooks/mmcode.ipynb"&gt;MultiModal Coding&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Generate accurate code based on rigorous comprehension of multimodal information.&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/mmcode.ipynb"&gt;&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/blob/main/cookbooks/long_document_understanding.ipynb"&gt;Long Document Understanding&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Achieve rigorous semantic comprehension of ultra-long documents.&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/long_document_understanding.ipynb"&gt;&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/blob/main/cookbooks/spatial_understanding.ipynb"&gt;Spatial Understanding&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;See, understand and reason about the spatial information&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/spatial_understanding.ipynb"&gt;&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/freesysck"&gt; /u/freesysck &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6zg97/update_qwen3vl_cookbooks_coming_recognition/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6zg97/update_qwen3vl_cookbooks_coming_recognition/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6zg97/update_qwen3vl_cookbooks_coming_recognition/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T02:41:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1o70fa7</id>
    <title>Sharing a few image transcriptions from Qwen3-VL-8B-Instruct</title>
    <updated>2025-10-15T03:28:58+00:00</updated>
    <author>
      <name>/u/Hoppss</name>
      <uri>https://old.reddit.com/user/Hoppss</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o70fa7/sharing_a_few_image_transcriptions_from/"&gt; &lt;img alt="Sharing a few image transcriptions from Qwen3-VL-8B-Instruct" src="https://b.thumbs.redditmedia.com/Lka0yz7i3ahGr9QVYGE_4FpjZ35ZZZnacxxqUOVbwHI.jpg" title="Sharing a few image transcriptions from Qwen3-VL-8B-Instruct" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hoppss"&gt; /u/Hoppss &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1o70fa7"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o70fa7/sharing_a_few_image_transcriptions_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o70fa7/sharing_a_few_image_transcriptions_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T03:28:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6kchz</id>
    <title>Qwen3-VL-4B and 8B Instruct &amp; Thinking are here</title>
    <updated>2025-10-14T16:31:24+00:00</updated>
    <author>
      <name>/u/AlanzhuLy</name>
      <uri>https://old.reddit.com/user/AlanzhuLy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-VL-4B-Thinking"&gt;https://huggingface.co/Qwen/Qwen3-VL-4B-Thinking&lt;/a&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/Qwen/Qwen3-VL-8B-Thinking"&gt;https://huggingface.co/Qwen/Qwen3-VL-8B-Thinking&lt;/a&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/Qwen/Qwen3-VL-8B-Instruct"&gt;https://huggingface.co/Qwen/Qwen3-VL-8B-Instruct&lt;/a&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/Qwen/Qwen3-VL-4B-Instruct"&gt;https://huggingface.co/Qwen/Qwen3-VL-4B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You can already run Qwen3-VL-4B &amp;amp; 8B locally Day-0 on NPU/GPU/CPU using MLX, GGUF, and NexaML with NexaSDK &lt;strong&gt;(&lt;/strong&gt;&lt;a href="https://github.com/NexaAI/nexa-sdk"&gt;&lt;strong&gt;GitHub&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Check out our GGUF, MLX, and NexaML collection on HuggingFace: &lt;a href="https://huggingface.co/collections/NexaAI/qwen3vl-68d46de18fdc753a7295190a"&gt;https://huggingface.co/collections/NexaAI/qwen3vl-68d46de18fdc753a7295190a&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlanzhuLy"&gt; /u/AlanzhuLy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6kchz/qwen3vl4b_and_8b_instruct_thinking_are_here/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6kchz/qwen3vl4b_and_8b_instruct_thinking_are_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6kchz/qwen3vl4b_and_8b_instruct_thinking_are_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T16:31:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6uw71</id>
    <title>Qwen3-VL 4B vs 8B vs 235B</title>
    <updated>2025-10-14T23:11:20+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6uw71/qwen3vl_4b_vs_8b_vs_235b/"&gt; &lt;img alt="Qwen3-VL 4B vs 8B vs 235B" src="https://preview.redd.it/deo3nizps5vf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=58885a74f99e694dcdba21d3b954746fac3611ce" title="Qwen3-VL 4B vs 8B vs 235B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/deo3nizps5vf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6uw71/qwen3vl_4b_vs_8b_vs_235b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6uw71/qwen3vl_4b_vs_8b_vs_235b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T23:11:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1o751o9</id>
    <title>My first 15 days with GLM-4.6 ‚Äî honest thoughts after using Opus and Sonnet</title>
    <updated>2025-10-15T08:02:52+00:00</updated>
    <author>
      <name>/u/DecisionLow2640</name>
      <uri>https://old.reddit.com/user/DecisionLow2640</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When I first subscribed and started using &lt;strong&gt;GLM-4.6&lt;/strong&gt; with &lt;strong&gt;KiloCode&lt;/strong&gt;, I was honestly a bit disappointed. I had gotten used to the kind of UI/UX-focused results I was getting from &lt;strong&gt;Opus 4.1&lt;/strong&gt; and &lt;strong&gt;Sonnet&lt;/strong&gt;, and GLM felt different at first.&lt;/p&gt; &lt;p&gt;But after a couple of weeks of real use, I‚Äôve started to really appreciate it. For &lt;strong&gt;pure programming tasks&lt;/strong&gt; ‚Äî not design-related ‚Äî GLM-4.6 is actually more &lt;strong&gt;precise, structured, and professional&lt;/strong&gt;. It doesn‚Äôt create as much random hard-coded mock data like Sonnet 4.5 often does. Every day it surprises me by solving problems more accurately and providing deeper diagnostics ‚Äî even when I‚Äôm using it inside the &lt;strong&gt;VS Code KiloCode extension&lt;/strong&gt;, not ClaudeCode itself.&lt;/p&gt; &lt;p&gt;I had a case where Sonnet ‚Äúsolved‚Äù an issue but the bug was still there. I gave the exact same prompt to GLM-4.6, and it fixed it perfectly using proper &lt;strong&gt;software-engineering logic&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;I also love that KiloCode can auto-generate &lt;strong&gt;UML diagrams&lt;/strong&gt;, which honestly reminded me of my early programming days in C and C++.&lt;/p&gt; &lt;p&gt;So yeah ‚Äî I used to rely on Opus for its relaxed, intuitive style, but now I‚Äôm seeing the real &lt;strong&gt;power and precision of GLM-4.6&lt;/strong&gt;. If you have at least a basic understanding of programming, this model is a beast ‚Äî more detailed, reliable, and consistent than Sonnet in many cases.&lt;/p&gt; &lt;p&gt;That‚Äôs my experience so far.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DecisionLow2640"&gt; /u/DecisionLow2640 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o751o9/my_first_15_days_with_glm46_honest_thoughts_after/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o751o9/my_first_15_days_with_glm46_honest_thoughts_after/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o751o9/my_first_15_days_with_glm46_honest_thoughts_after/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T08:02:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6ocfs</id>
    <title>If it's not local, it's not yours.</title>
    <updated>2025-10-14T18:57:54+00:00</updated>
    <author>
      <name>/u/inkberk</name>
      <uri>https://old.reddit.com/user/inkberk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6ocfs/if_its_not_local_its_not_yours/"&gt; &lt;img alt="If it's not local, it's not yours." src="https://preview.redd.it/zzv4ey22j4vf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ebc1f207746b0fa04e90a129bafad3aef0ca9971" title="If it's not local, it's not yours." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/inkberk"&gt; /u/inkberk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zzv4ey22j4vf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6ocfs/if_its_not_local_its_not_yours/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6ocfs/if_its_not_local_its_not_yours/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T18:57:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1o75kkb</id>
    <title>AI has replaced programmers‚Ä¶ totally.</title>
    <updated>2025-10-15T08:37:54+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o75kkb/ai_has_replaced_programmers_totally/"&gt; &lt;img alt="AI has replaced programmers‚Ä¶ totally." src="https://preview.redd.it/bnnb2fb9m8vf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e1a55140b6915df726dfa4932943df64e43e7d94" title="AI has replaced programmers‚Ä¶ totally." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bnnb2fb9m8vf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o75kkb/ai_has_replaced_programmers_totally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o75kkb/ai_has_replaced_programmers_totally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T08:37:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
