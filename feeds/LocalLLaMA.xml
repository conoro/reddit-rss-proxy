<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-21T15:33:06+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qi78u3</id>
    <title>I think Giga Potato:free in Kilo Code is Deepseek V4</title>
    <updated>2026-01-20T17:22:33+00:00</updated>
    <author>
      <name>/u/quantier</name>
      <uri>https://old.reddit.com/user/quantier</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was looking for a new free model in Kilo Code after Minimax M2.1 was removed as a free model.&lt;/p&gt; &lt;p&gt;Searched for free and found Giga Potato:free and Googled it (yes the AI models don’t usually have the most recent stuff in their search)&lt;/p&gt; &lt;p&gt;I found this blog article: &lt;a href="https://blog.kilo.ai/p/announcing-a-powerful-new-stealth"&gt;https://blog.kilo.ai/p/announcing-a-powerful-new-stealth&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I have now tested it and am mindblown it performs like Sonnet 4.5 and maybe even like Opus 4.5. I can give it very short poor prompts and it reasons itself to amazing results! &lt;/p&gt; &lt;p&gt;Whatever open source model this is…..it’s crazy! Honestly!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/quantier"&gt; /u/quantier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qi78u3/i_think_giga_potatofree_in_kilo_code_is_deepseek/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qi78u3/i_think_giga_potatofree_in_kilo_code_is_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qi78u3/i_think_giga_potatofree_in_kilo_code_is_deepseek/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T17:22:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1qi3nmd</id>
    <title>Over 6K novels with reasoning traces to train full book writing LLMs</title>
    <updated>2026-01-20T15:12:25+00:00</updated>
    <author>
      <name>/u/XMasterDE</name>
      <uri>https://old.reddit.com/user/XMasterDE</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qi3nmd/over_6k_novels_with_reasoning_traces_to_train/"&gt; &lt;img alt="Over 6K novels with reasoning traces to train full book writing LLMs" src="https://b.thumbs.redditmedia.com/2qE82dS5QMmSeeQNcY514CvEOcapkm7SNym4UUAFIwE.jpg" title="Over 6K novels with reasoning traces to train full book writing LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/zzxy8r31tieg1.jpg?width=5504&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=fb966352c2548369a731f0bff03a131c8ec4a1b2"&gt;https://preview.redd.it/zzxy8r31tieg1.jpg?width=5504&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=fb966352c2548369a731f0bff03a131c8ec4a1b2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We’re releasing an update to our &lt;strong&gt;LongPage&lt;/strong&gt; dataset.&lt;/p&gt; &lt;p&gt;LongPage is a dataset of &lt;strong&gt;full-length novels paired with reasoning traces&lt;/strong&gt;: each book includes a &lt;strong&gt;hierarchical planning trace&lt;/strong&gt; that breaks the story down from high-level outline into chapters/scenes to support training &lt;strong&gt;full-book writing LLMs&lt;/strong&gt;. The previous release contained ~300 books; this update expands the dataset to &lt;strong&gt;6K+ novels&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;We’re also currently training a &lt;strong&gt;full-book writing model&lt;/strong&gt; on LongPage. We already have early checkpoints running internally, and we plan to release the model as soon as the output quality reaches an acceptable level.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;HF Link:&lt;/strong&gt; &lt;a href="https://huggingface.co/datasets/Pageshift-Entertainment/LongPage"&gt;https://huggingface.co/datasets/Pageshift-Entertainment/LongPage&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you want to follow our journey as we build world-class storytelling models, you can find us here:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Website: &lt;a href="https://pageshift-entertainment.ai/"&gt;https://pageshift-entertainment.ai/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;X (Twitter): &lt;a href="https://x.com/pageshiftAI"&gt;https://x.com/pageshiftAI&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Hugging Face: &lt;a href="https://huggingface.co/Pageshift-Entertainment"&gt;https://huggingface.co/Pageshift-Entertainment&lt;/a&gt;&lt;/li&gt; &lt;li&gt;LinkedIn: &lt;a href="https://www.linkedin.com/company/pageshift-ai/"&gt;https://www.linkedin.com/company/pageshift-ai/&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterDE"&gt; /u/XMasterDE &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qi3nmd/over_6k_novels_with_reasoning_traces_to_train/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qi3nmd/over_6k_novels_with_reasoning_traces_to_train/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qi3nmd/over_6k_novels_with_reasoning_traces_to_train/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T15:12:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1qj09le</id>
    <title>What are the differences between Manus AI and tools like ClaudeCode and some CLI tools?</title>
    <updated>2026-01-21T15:04:57+00:00</updated>
    <author>
      <name>/u/ZMFooo</name>
      <uri>https://old.reddit.com/user/ZMFooo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I think Manus AI is basically a collection of Claude Code tools filled with pre-defined MCPs and various skills. I've seen more and more applications and open-source projects similar to Manus AI, such as the recent Cowork and the earlier Minimax agent.&lt;/p&gt; &lt;p&gt;I've tried them all, and for me, I didn't feel any difference. I still usually use Claude Code for my tasks, and they all work quite well. I think these kinds of applications are just packaged CLI tools with some kind of visual interface. What do you think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ZMFooo"&gt; /u/ZMFooo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qj09le/what_are_the_differences_between_manus_ai_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qj09le/what_are_the_differences_between_manus_ai_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qj09le/what_are_the_differences_between_manus_ai_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-21T15:04:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1qj0ott</id>
    <title>KVzap: Fast, Adaptive, and Faithful KV Cache Pruning</title>
    <updated>2026-01-21T15:21:18+00:00</updated>
    <author>
      <name>/u/Thrumpwart</name>
      <uri>https://old.reddit.com/user/Thrumpwart</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;*Growing context lengths in transformer-based language models have made the key-value (KV) cache a critical inference bottleneck. While many KV cache pruning methods have been proposed, they have not yet been adopted in major inference engines due to speed--accuracy trade-offs. We introduce KVzap, a fast, input-adaptive approximation of KVzip that works in both prefilling and decoding. On Qwen3-8B, Llama-3.1-8B-Instruct, and Qwen3-32B across long-context and reasoning tasks, KVzap achieves 2--4× KV cache compression with negligible accuracy loss and achieves state-of-the-art performance on the KVpress leaderboard. Code and models are available at this https URL: &lt;a href="https://github.com/NVIDIA/kvpress%5C*"&gt;https://github.com/NVIDIA/kvpress\*&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thrumpwart"&gt; /u/Thrumpwart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2601.07891"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qj0ott/kvzap_fast_adaptive_and_faithful_kv_cache_pruning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qj0ott/kvzap_fast_adaptive_and_faithful_kv_cache_pruning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-21T15:21:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1qis3y9</id>
    <title>Aider's documentation for getting connected to local inference sucks. Hopefully this helps.</title>
    <updated>2026-01-21T08:05:37+00:00</updated>
    <author>
      <name>/u/synth_mania</name>
      <uri>https://old.reddit.com/user/synth_mania</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;To anyone who is attempting to get Aider set up with your pre-existing local inference, the documentation is nearly devoid of details or helpful examples.&lt;/p&gt; &lt;p&gt;It turns out you need multiple files configured in your home directory (on linux) with specific information, and some must be formatted in not-obvious ways.&lt;/p&gt; &lt;p&gt;First devstral tried and failed to help me set it up. Then Gemini 3 Pro.&lt;/p&gt; &lt;p&gt;Then I read the whole documentation manually (I know, I nearly broke a sweat), and it's no wonder: the fucking documentation sucks. I can hardly blame Devstral, or even Gemini.&lt;/p&gt; &lt;p&gt;Even after reading this, I suggest you give the documentation a look. Specifically, the &lt;a href="https://aider.chat/docs/config/aider_conf.html"&gt;&amp;quot;YAML config file&amp;quot;&lt;/a&gt; page and &lt;a href="https://aider.chat/docs/config/adv-model-settings.html"&gt;&amp;quot;advanced model settings&amp;quot;&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Still, I thought I'd write this to anyone else who is stuck now or in the future. It would've been so helpful if someone wrote this down for me (or even my LLMs) to digest before attempting to configure Aider.&lt;/p&gt; &lt;h1&gt;Config file breakdown&lt;/h1&gt; &lt;p&gt;Anyways, here's the files you'll need to create. There are 3 of them. If I could've had my way, I would've had them combine the last two into a single file, but I can begrudgingly accept the division of information as it exists:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;&lt;code&gt;File path&lt;/code&gt;&lt;/th&gt; &lt;th align="left"&gt;Purpose&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;code&gt;~/.aider.conf.yml&lt;/code&gt;&lt;/td&gt; &lt;td align="left"&gt;Responsible for setting API endpoint details, identifier of model in use, and paths to the other config files.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;code&gt;~/.aider.model.settings.yml&lt;/code&gt;&lt;/td&gt; &lt;td align="left"&gt;Where the edit format, and a bunch of other flags, many with basically no details in the documentation, may be set. These are all specific to the application of agentic coding.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;code&gt;~/.aider.model.metadata.json&lt;/code&gt;&lt;/td&gt; &lt;td align="left"&gt;Where use-case agnostic model details go. Think parameters like max context&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Example file contents&lt;/h1&gt; &lt;p&gt;these are from my setup.&lt;/p&gt; &lt;p&gt;Treat accordingly, and don't assume they'll work out of the box for you.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;~/.aider.conf.yml&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;openai-api-base: &amp;quot;http://localhost:1234/v1&amp;quot; openai-api-key: &amp;quot;placeholder&amp;quot; model: &amp;quot;openai/mistralai/devstral-small-2-2512&amp;quot; # for example model-settings-file: &amp;quot;/home/your-name/.aider.model.settings.yml&amp;quot; model-metadata-file: &amp;quot;/home/your-name/.aider.model.metadata.json&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;~/.aider.model.settings.yml&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;- name: openai/mistralai/devstral-small-2-2512 edit_format: diff weak_model_name: null use_repo_map: true examples_as_sys_msg: true &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;~/.aider.model.metadata.json&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;{ &amp;quot;openai/mistralai/devstral-small-2-2512&amp;quot;: { &amp;quot;max_input_tokens&amp;quot;: 40677, &amp;quot;max_tokens&amp;quot;: 1000000, &amp;quot;input_cost_per_token&amp;quot;: 0.000000303, &amp;quot;output_cost_per_token&amp;quot;: 0.000000303, &amp;quot;mode&amp;quot;: &amp;quot;chat&amp;quot; } } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I almost forgot to mention, that weird model identifier isn't like that for no reason - you must also prepend &lt;code&gt;openai/&lt;/code&gt; to your model identifier, in every instance that it appears across these three files. Aider strips the &lt;code&gt;openai/&lt;/code&gt; prefix from the model identifier before passing it to your openai-compatible endpoint.&lt;/p&gt; &lt;p&gt;So, in my case, LMstudio only sees &amp;quot;mistralai/devstral-small-2-2512&amp;quot;&lt;/p&gt; &lt;p&gt;The bit it stripped off is treated as the name of a preset api config, and is used to determine where to send the API requests that need to make it to this model. The default settings for OpenAI were overwritten when, in the first of the three configuration files, we set the &amp;quot;&lt;code&gt;openai-api-base&lt;/code&gt;&amp;quot; and &amp;quot;&lt;code&gt;openai-api-key&lt;/code&gt;&amp;quot; variables.&lt;/p&gt; &lt;p&gt;Besides being a non-obvious way to specify the endpoint for any particular model, it also creates an apparent mismatch between the model ID in your configs and the model IDs as they are hosted by your server.&lt;/p&gt; &lt;p&gt;Yeah, fucking stupid, and fucking confusing.&lt;/p&gt; &lt;p&gt;Anyways, I hope this saves someone else the headache. I need a beer.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/synth_mania"&gt; /u/synth_mania &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qis3y9/aiders_documentation_for_getting_connected_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qis3y9/aiders_documentation_for_getting_connected_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qis3y9/aiders_documentation_for_getting_connected_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-21T08:05:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1qiqwha</id>
    <title>My hotrodded strix halo + rtx pro 4000 Blackwell</title>
    <updated>2026-01-21T06:53:29+00:00</updated>
    <author>
      <name>/u/sputnik13net</name>
      <uri>https://old.reddit.com/user/sputnik13net</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qiqwha/my_hotrodded_strix_halo_rtx_pro_4000_blackwell/"&gt; &lt;img alt="My hotrodded strix halo + rtx pro 4000 Blackwell" src="https://b.thumbs.redditmedia.com/OfEjpz_N_aYn_Ii6qkyjCSXB73JMnbNQm8PjfKbG44U.jpg" title="My hotrodded strix halo + rtx pro 4000 Blackwell" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/jqxnqdaggneg1.jpg?width=5712&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=722695551f0dea529ea558f6eed9709d04ecbac8"&gt;https://preview.redd.it/jqxnqdaggneg1.jpg?width=5712&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=722695551f0dea529ea558f6eed9709d04ecbac8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/99uj9daggneg1.jpg?width=5712&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=b405c01e3e570d8a291056c883b20bffac20afb0"&gt;https://preview.redd.it/99uj9daggneg1.jpg?width=5712&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=b405c01e3e570d8a291056c883b20bffac20afb0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Framework Desktop mainboard AI Max+ 395 128GB, x4 -&amp;gt; x16 pcie riser, and RTX Pro 4000 Blackwell in a Dan case A4-SFX. Couldn't close the CPU side because FW mainboard's heatsink is so huge. Cable management is a mess and a half but it all works beautifully.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sputnik13net"&gt; /u/sputnik13net &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qiqwha/my_hotrodded_strix_halo_rtx_pro_4000_blackwell/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qiqwha/my_hotrodded_strix_halo_rtx_pro_4000_blackwell/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qiqwha/my_hotrodded_strix_halo_rtx_pro_4000_blackwell/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-21T06:53:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1qixw4q</id>
    <title>Is there a standard set of benchmarks for memory systems/RAG systems?</title>
    <updated>2026-01-21T13:28:45+00:00</updated>
    <author>
      <name>/u/wasteofwillpower</name>
      <uri>https://old.reddit.com/user/wasteofwillpower</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Basically what the title says. I tried making my own memory/RAG system as a fun project and wanted to see how it compares against Graphiti, MemGPT and whatever's launching this week for LLM memory systems.&lt;/p&gt; &lt;p&gt;Are there any benchmarks I can use to compare them?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wasteofwillpower"&gt; /u/wasteofwillpower &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qixw4q/is_there_a_standard_set_of_benchmarks_for_memory/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qixw4q/is_there_a_standard_set_of_benchmarks_for_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qixw4q/is_there_a_standard_set_of_benchmarks_for_memory/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-21T13:28:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1qiqo54</id>
    <title>Glm 4.7 flash, insane memory usage on MLX (LM studio)</title>
    <updated>2026-01-21T06:40:20+00:00</updated>
    <author>
      <name>/u/Enragere</name>
      <uri>https://old.reddit.com/user/Enragere</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I don't know what I'm doing wrong, I also tried gguf version and memory consumption was stable at 48 / 64gb&lt;/p&gt; &lt;p&gt;But with mlx version. it just runs properly the first 10k tokens, then starts memory swapping on my m3 max 64gb and the speed tanks to the point it's unusable. &lt;/p&gt; &lt;p&gt;Doesn't matter if I do q4 or q8, same thing is happening. &lt;/p&gt; &lt;p&gt;Does anyone know what is going on? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Enragere"&gt; /u/Enragere &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qiqo54/glm_47_flash_insane_memory_usage_on_mlx_lm_studio/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qiqo54/glm_47_flash_insane_memory_usage_on_mlx_lm_studio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qiqo54/glm_47_flash_insane_memory_usage_on_mlx_lm_studio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-21T06:40:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1qiyxl4</id>
    <title>We tested every VLM for Arabic document extraction. Here's what actually works.</title>
    <updated>2026-01-21T14:12:43+00:00</updated>
    <author>
      <name>/u/No-Reindeer-9968</name>
      <uri>https://old.reddit.com/user/No-Reindeer-9968</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We're building document extraction for Arabic use cases — government forms, handwritten fields, stamps, tables, text scattered everywhere. Spent the last few weeks testing every OCR/VLM option we could find.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; Gemini (2.5-pro and 3-pro) is the only model that actually works reliably. Everything else failed or hallucinated.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What we tested:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Went through almost every open-source VLM on Hugging Face marketed for text extraction: dots.ocr, deepseek-ocr, mistral-ocr, olmOCR, and others.&lt;/p&gt; &lt;p&gt;Results: they either fail outright on Arabic or hallucinate. Complex layouts (stamps overlapping text, handwritten fields mixed with printed, tables with merged cells) broke most of them completely.&lt;/p&gt; &lt;p&gt;Two models stood out as having actual Arabic pipelines: &lt;strong&gt;dots.ocr&lt;/strong&gt; and &lt;strong&gt;Chandra&lt;/strong&gt; (by Datalab). These do the full pipeline — block detection + text extraction. But even these weren't production-ready for arabic documents. Text extraction accuracy on handwritten Arabic wasn't acceptable.&lt;/p&gt; &lt;p&gt;We also tested Datalab's hosted version. Worked better than their open-source release — I suspect they have specialized models that aren't public. But even the hosted version would sometimes crash on complex documents.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What actually works: Gemini&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Gemini 2.5-pro and 3-pro are in a different league for Arabic document understanding.&lt;/p&gt; &lt;p&gt;These models can:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Reason through complex layouts&lt;/li&gt; &lt;li&gt;Handle handwritten Arabic (even messy handwriting)&lt;/li&gt; &lt;li&gt;Understand context (stamps, annotations, crossed-out text)&lt;/li&gt; &lt;li&gt;Extract from government forms that would break everything else&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;But Gemini has limits:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;No bounding box detection (unlike dots.ocr/Chandra which detect text blocks)&lt;/li&gt; &lt;li&gt;API-only — if you need offline/on-prem, you can't use it&lt;/li&gt; &lt;li&gt;Still not 100% accurate on the hardest cases (especially with handwritten text)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;If you need offline/self-hosted Arabic OCR&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This is where it gets brutal.&lt;/p&gt; &lt;p&gt;Based on our discovery work scoping this out: if you need production-quality Arabic OCR without Gemini, you're looking at finetuning an open-source VLM yourself.&lt;/p&gt; &lt;p&gt;What that looks like:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Start with a model that has decent Arabic foundations (Qwen3-VL family looks promising)&lt;/li&gt; &lt;li&gt;You'll need &lt;strong&gt;~100k labeled samples&lt;/strong&gt; to start seeing production-quality results for specific entity extraction&lt;/li&gt; &lt;li&gt;Depending on complexity, could go up to 500k+ samples&lt;/li&gt; &lt;li&gt;Labeling pipeline: use Gemini to pre-label (cuts time massively), then human labelers correct. Expect 60-70% accuracy from Gemini on complex handwritten docs, 70-90% on cleaner structured docs.&lt;/li&gt; &lt;li&gt;Iterate until you hit target accuracy.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Realistically, you can probably hit ~80% accuracy with enough training data. Getting above 90% becomes a research project with no guaranteed timeline — the variation in handwritten Arabic is infinite.&lt;/p&gt; &lt;p&gt;Building a general-purpose Arabic OCR model (handles any document, any handwriting, any layout)? That's millions of samples and a massive labeling operation.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Bottom line:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;If you can use Gemini API → just use Gemini. It's the best by far.&lt;/li&gt; &lt;li&gt;If you need offline → prepare for a finetuning project. Budget 100k+ samples minimum.&lt;/li&gt; &lt;li&gt;Open-source Arabic OCR is years behind English. The models exist but aren't reliable.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Reindeer-9968"&gt; /u/No-Reindeer-9968 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qiyxl4/we_tested_every_vlm_for_arabic_document/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qiyxl4/we_tested_every_vlm_for_arabic_document/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qiyxl4/we_tested_every_vlm_for_arabic_document/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-21T14:12:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1qitusf</id>
    <title>Which single LLM benchmark task is most relevant to your daily life tasks?</title>
    <updated>2026-01-21T09:55:28+00:00</updated>
    <author>
      <name>/u/ChippingCoder</name>
      <uri>https://old.reddit.com/user/ChippingCoder</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What is the one LLM benchmark that tests and evaluates models on tasks which align with most of your daily life?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ChippingCoder"&gt; /u/ChippingCoder &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qitusf/which_single_llm_benchmark_task_is_most_relevant/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qitusf/which_single_llm_benchmark_task_is_most_relevant/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qitusf/which_single_llm_benchmark_task_is_most_relevant/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-21T09:55:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1qi512t</id>
    <title>Liquid AI released the best thinking Language Model Under 1GB</title>
    <updated>2026-01-20T16:02:42+00:00</updated>
    <author>
      <name>/u/PauLabartaBajo</name>
      <uri>https://old.reddit.com/user/PauLabartaBajo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qi512t/liquid_ai_released_the_best_thinking_language/"&gt; &lt;img alt="Liquid AI released the best thinking Language Model Under 1GB" src="https://preview.redd.it/nazcfmti1jeg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d85adead4df4e2a21194a3e6ae29b3752731c529" title="Liquid AI released the best thinking Language Model Under 1GB" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Liquid AI released LFM2.5-1.2B-Thinking, a reasoning model that runs entirely on-device. &lt;/p&gt; &lt;p&gt;What needed a data centre two years ago now runs on any phone with 900 MB of memory. &lt;/p&gt; &lt;p&gt;-&amp;gt; Trained specifically for concise reasoning&lt;br /&gt; -&amp;gt; Generates internal thinking traces before producing answers&lt;br /&gt; -&amp;gt; Enables systematic problem-solving at edge-scale latency&lt;br /&gt; -&amp;gt; Shines on tool use, math, and instruction following&lt;br /&gt; -&amp;gt; Matches or exceeds Qwen3-1.7B (thinking mode) acrross most performance benchmarks, despite having 40% less parameters. &lt;/p&gt; &lt;p&gt;At inference time, the gap widens further, outperforming both pure transformer models and hybrid architectures in speed and memory efficiency. &lt;/p&gt; &lt;p&gt;LFM2.5-1.2B-Thinking is available today: with broad, day-one support across the on-device ecosystem.&lt;br /&gt; Hugging Face: &lt;a href="https://huggingface.co/LiquidAI/LFM2.5-1.2B-Thinking"&gt;https://huggingface.co/LiquidAI/LFM2.5-1.2B-Thinking&lt;/a&gt;&lt;br /&gt; LEAP: &lt;a href="https://leap.liquid.ai/models?model=lfm2.5-1.2b-thinking"&gt;https://leap.liquid.ai/models?model=lfm2.5-1.2b-thinking&lt;/a&gt;&lt;br /&gt; Liquid AI Playground: &lt;a href="https://playground.liquid.ai/login?callbackUrl=%2F"&gt;https://playground.liquid.ai/login?callbackUrl=%2F&lt;/a&gt; &lt;/p&gt; &lt;p&gt;At&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PauLabartaBajo"&gt; /u/PauLabartaBajo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nazcfmti1jeg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qi512t/liquid_ai_released_the_best_thinking_language/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qi512t/liquid_ai_released_the_best_thinking_language/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T16:02:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1qiwf4f</id>
    <title>Qwen3-0.6B Generative Recommendation</title>
    <updated>2026-01-21T12:19:53+00:00</updated>
    <author>
      <name>/u/InevitableConcept983</name>
      <uri>https://old.reddit.com/user/InevitableConcept983</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking to use the Qwen3-0.6B model for generative recommendation from queries to websites. Has anyone done similar work? I'd appreciate any shared experience.&lt;/p&gt; &lt;p&gt;Example&lt;/p&gt; &lt;p&gt;query: nba&lt;/p&gt; &lt;p&gt;response: &lt;a href="http://www.nba.com"&gt;www.nba.com&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InevitableConcept983"&gt; /u/InevitableConcept983 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qiwf4f/qwen306b_generative_recommendation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qiwf4f/qwen306b_generative_recommendation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qiwf4f/qwen306b_generative_recommendation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-21T12:19:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1qiyum5</id>
    <title>What's the strongest model for code writing and mathematical problem solving for 12GB of vram?</title>
    <updated>2026-01-21T14:09:20+00:00</updated>
    <author>
      <name>/u/MrMrsPotts</name>
      <uri>https://old.reddit.com/user/MrMrsPotts</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am using openevolve and shinkaevolve (open source versions of alphaevolve) and I want to get the best results possible. Would it be a quant of OSS:20b?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrMrsPotts"&gt; /u/MrMrsPotts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qiyum5/whats_the_strongest_model_for_code_writing_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qiyum5/whats_the_strongest_model_for_code_writing_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qiyum5/whats_the_strongest_model_for_code_writing_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-21T14:09:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1qiq26v</id>
    <title>Update - Day #6 of building an LM from scratch</title>
    <updated>2026-01-21T06:06:32+00:00</updated>
    <author>
      <name>/u/AllTheCoins</name>
      <uri>https://old.reddit.com/user/AllTheCoins</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I finally got everything stable. Loss was steadily dropping until eventually it plateaued at around 4-5 at the end. &lt;/p&gt; &lt;p&gt;I switched to just DataParallel because DDP was impossible in Windows as I found out during Day 4. However in my findings, DataParallel was actually bottlenecking my system. It was training faster on one GPU instead of two (I blame Windows again for this). Though ideally I’d switch to Linux, I want to get this working on Windows as most beginners are using that and I want to make sure this process is available to beginner users.&lt;/p&gt; &lt;p&gt;Back to the actual LM, I grossly underestimated how much training an LM would need. After 25,000 steps or 13 hours of training, I had effectively trained my model on about 400M tokens. Which for a 0.3B model… is nothing. &lt;/p&gt; &lt;p&gt;I tried out the model anyways and it performed, I would say, better than expected. Sentence structure was nearly perfect. Words made sense and were in the right spots. But the model didn’t understand anything yet and I’ll need to basically rerun the training with a total step count of about 300K if I want a good pretrain. I’ll have a 60K benchmark ready to go by Day 8 so I’m very excited to show you guys what that model sounds like! &lt;/p&gt; &lt;p&gt;As always, if you guys have any questions, feel free to ask!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AllTheCoins"&gt; /u/AllTheCoins &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qiq26v/update_day_6_of_building_an_lm_from_scratch/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qiq26v/update_day_6_of_building_an_lm_from_scratch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qiq26v/update_day_6_of_building_an_lm_from_scratch/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-21T06:06:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1qio9nj</id>
    <title>I tracked context degradation across 847 agent runs. Here's when performance actually falls off a cliff.</title>
    <updated>2026-01-21T04:34:43+00:00</updated>
    <author>
      <name>/u/Main_Payment_6430</name>
      <uri>https://old.reddit.com/user/Main_Payment_6430</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been running local agents (mostly Llama 3.1 70B, some Qwen 2.5 72B) for dev automation tasks—things like multi-file refactors, long debugging sessions, iterative code generation.&lt;/p&gt; &lt;p&gt;After months of frustration with agents forgetting instructions mid-task or suddenly ignoring constraints I'd set earlier, I started logging everything to figure out what was actually happening.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The setup:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;847 agent runs tracked&lt;/li&gt; &lt;li&gt;Tasks ranging from 5 to 200+ turns&lt;/li&gt; &lt;li&gt;Measured: instruction adherence, constraint violations, repetition rate, task completion&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What I found:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The degradation isn't linear. There's a cliff.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Context Fill %&lt;/th&gt; &lt;th align="left"&gt;Instruction Adherence&lt;/th&gt; &lt;th align="left"&gt;Constraint Violations&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;0-25%&lt;/td&gt; &lt;td align="left"&gt;94%&lt;/td&gt; &lt;td align="left"&gt;2.1%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;25-50%&lt;/td&gt; &lt;td align="left"&gt;91%&lt;/td&gt; &lt;td align="left"&gt;4.8%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;50-75%&lt;/td&gt; &lt;td align="left"&gt;73%&lt;/td&gt; &lt;td align="left"&gt;12.4%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;75-100%&lt;/td&gt; &lt;td align="left"&gt;41%&lt;/td&gt; &lt;td align="left"&gt;31.7%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Around 60-70% context utilization, something breaks. The model starts:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Following patterns from early conversation instead of recent instructions&lt;/li&gt; &lt;li&gt;&amp;quot;Forgetting&amp;quot; constraints that were stated 30+ turns ago&lt;/li&gt; &lt;li&gt;Repeating tool calls it already made&lt;/li&gt; &lt;li&gt;Hallucinating state that was true earlier but isn't anymore&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm calling this context rot — the model's attention spreads thin and it defaults to statistical patterns rather than explicit instructions.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What actually helped:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Aggressive compaction&lt;/strong&gt; — Not summarization (loses too much). Actual compaction: if the agent wrote to a file, drop the file contents from context but keep the path. If it searched, drop results but keep the query. Externalize state, keep references.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;State snapshots&lt;/strong&gt; — Before any destructive operation, snapshot the context. When the agent goes off-rails (and it will), revert to last-known-good state instead of trying to &amp;quot;correct&amp;quot; it in-context.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Forking for sub-tasks&lt;/strong&gt; — Instead of one massive context, fork isolated contexts for bounded sub-tasks. Agent gets instruction + minimal relevant context, returns result. Parent context stays clean.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I ended up building a small context management layer to handle this because I was copy-pasting JSON dumps like a caveman. It does versioning (git-style), snapshots, rollback, and forking. Open-sourced the approach, happy to share if anyone's interested.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Questions for the community:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Anyone else tracking this systematically? Would love to compare notes.&lt;/li&gt; &lt;li&gt;Are there models that degrade more gracefully? My (limited) testing suggests Qwen handles high context fill slightly better than Llama, but sample size is small.&lt;/li&gt; &lt;li&gt;How are people handling state for multi-hour agent runs? Curious what janky solutions others have built.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Edit: Since people are asking, the tool I built is called UltraContext (&lt;a href="https://ultracontext.ai"&gt;https://ultracontext.ai&lt;/a&gt;). It's basically a context API with automatic versioning—5 methods, lets you snapshot/rollback/fork contexts. Free tier if you want to mess with it. But honestly the concepts above work even if you just roll your own with SQLite.&lt;/p&gt; &lt;p&gt;here's the repo - &lt;a href="https://github.com/ultracontext/ultracontext-node"&gt;https://github.com/ultracontext/ultracontext-node&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Main_Payment_6430"&gt; /u/Main_Payment_6430 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qio9nj/i_tracked_context_degradation_across_847_agent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qio9nj/i_tracked_context_degradation_across_847_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qio9nj/i_tracked_context_degradation_across_847_agent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-21T04:34:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1qi4uj2</id>
    <title>768Gb Fully Enclosed 10x GPU Mobile AI Build</title>
    <updated>2026-01-20T15:56:13+00:00</updated>
    <author>
      <name>/u/SweetHomeAbalama0</name>
      <uri>https://old.reddit.com/user/SweetHomeAbalama0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qi4uj2/768gb_fully_enclosed_10x_gpu_mobile_ai_build/"&gt; &lt;img alt="768Gb Fully Enclosed 10x GPU Mobile AI Build" src="https://b.thumbs.redditmedia.com/IFwD006aQ7uS94rhW8Tb5SMKqOvtmvGGWhQOsclMVOE.jpg" title="768Gb Fully Enclosed 10x GPU Mobile AI Build" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I haven't seen a system with this format before but with how successful the result was I figured I might as well share it.&lt;/p&gt; &lt;p&gt;Specs:&lt;br /&gt; Threadripper Pro 3995WX w/ ASUS WS WRX80e-sage wifi ii&lt;/p&gt; &lt;p&gt;512Gb DDR4&lt;/p&gt; &lt;p&gt;256Gb GDDR6X/GDDR7 (8x 3090 + 2x 5090)&lt;/p&gt; &lt;p&gt;EVGA 1600W + Asrock 1300W PSU's&lt;/p&gt; &lt;p&gt;Case: Thermaltake Core W200&lt;/p&gt; &lt;p&gt;OS: Ubuntu&lt;/p&gt; &lt;p&gt;Est. expense: ~$17k&lt;/p&gt; &lt;p&gt;The objective was to make a system for running extra large MoE models (Deepseek and Kimi K2 specifically), that is also capable of lengthy video generation and rapid high detail image gen (the system will be supporting a graphic designer). The challenges/constraints: The system should be easily movable, and it should be enclosed. The result technically satisfies the requirements, with only one minor caveat. Capital expense was also an implied constraint. We wanted to get the most potent system possible with the best technology currently available, without going down the path of needlessly spending tens of thousands of dollars for diminishing returns on performance/quality/creativity potential. Going all 5090's or 6000 PRO's would have been unfeasible budget-wise and in the end likely unnecessary, two 6000's alone could have eaten the cost of the entire amount spent on the project, and if not for the two 5090's the final expense would have been much closer to ~$10k (still would have been an extremely capable system, but this graphic artist would really benefit from the image/video gen time savings that only a 5090 can provide).&lt;/p&gt; &lt;p&gt;The biggest hurdle was the enclosure problem. I've seen mining frames zip tied to a rack on wheels as a solution for mobility, but not only is this aesthetically unappealing, build construction and sturdiness quickly get called into question. This system would be living under the same roof with multiple cats, so an enclosure was almost beyond a nice-to-have, the hardware will need a physical barrier between the expensive components and curious paws. Mining frames were quickly ruled out altogether after a failed experiment. Enter the W200, a platform that I'm frankly surprised I haven't heard suggested before in forum discussions about planning multi-GPU builds, and is the main motivation for this post. The W200 is intended to be a dual-system enclosure, but when the motherboard is installed upside-down in its secondary compartment, this makes a perfect orientation to connect risers to mounted GPU's in the &amp;quot;main&amp;quot; compartment. If you don't mind working in dense compartments to get everything situated (the sheer density overall of the system is among its only drawbacks), this approach reduces the jank from mining frame + wheeled rack solutions significantly. A few zip ties were still required to secure GPU's in certain places, but I don't feel remotely as anxious about moving the system to a different room or letting cats inspect my work as I would if it were any other configuration.&lt;/p&gt; &lt;p&gt;Now the caveat. Because of the specific GPU choices made (3x of the 3090's are AIO hybrids), this required putting one of the W200's fan mounting rails on the main compartment side in order to mount their radiators (pic shown with the glass panel open, but it can be closed all the way). This means the system technically should not run without this panel at least slightly open so it doesn't impede exhaust, but if these AIO 3090's were blower/air cooled, I see no reason why this couldn't run fully closed all the time as long as fresh air intake is adequate.&lt;/p&gt; &lt;p&gt;The final case pic shows the compartment where the actual motherboard is installed (it is however very dense with risers and connectors so unfortunately it is hard to actually see much of anything) where I removed one of the 5090's. Airflow is very good overall (I believe 12x 140mm fans were installed throughout), GPU temps remain in good operation range under load, and it is surprisingly quiet when inferencing. Honestly, given how many fans and high power GPU's are in this thing, I am impressed by the acoustics, I don't have a sound meter to measure db's but to me it doesn't seem much louder than my gaming rig.&lt;/p&gt; &lt;p&gt;I typically power limit the 3090's to 200-250W and the 5090's to 500W depending on the workload.&lt;/p&gt; &lt;p&gt;.&lt;/p&gt; &lt;p&gt;Benchmarks&lt;/p&gt; &lt;p&gt;Deepseek V3.1 Terminus Q2XXS (100% GPU offload)&lt;/p&gt; &lt;p&gt;Tokens generated - 2338 tokens&lt;/p&gt; &lt;p&gt;Time to first token - 1.38s&lt;/p&gt; &lt;p&gt;Token gen rate - 24.92tps&lt;/p&gt; &lt;p&gt;__________________________&lt;/p&gt; &lt;p&gt;GLM 4.6 Q4KXL (100% GPU offload)&lt;/p&gt; &lt;p&gt;Tokens generated - 4096&lt;/p&gt; &lt;p&gt;Time to first token - 0.76s&lt;/p&gt; &lt;p&gt;Token gen rate - 26.61tps&lt;/p&gt; &lt;p&gt;__________________________&lt;/p&gt; &lt;p&gt;Kimi K2 TQ1 (87% GPU offload)&lt;/p&gt; &lt;p&gt;Tokens generated - 1664&lt;/p&gt; &lt;p&gt;Time to first token - 2.59s&lt;/p&gt; &lt;p&gt;Token gen rate - 19.61tps&lt;/p&gt; &lt;p&gt;__________________________&lt;/p&gt; &lt;p&gt;Hermes 4 405b Q3KXL (100% GPU offload)&lt;/p&gt; &lt;p&gt;Tokens generated - was so underwhelmed by the response quality I forgot to record lol&lt;/p&gt; &lt;p&gt;Time to first token - 1.13s&lt;/p&gt; &lt;p&gt;Token gen rate - 3.52tps&lt;/p&gt; &lt;p&gt;__________________________&lt;/p&gt; &lt;p&gt;Qwen 235b Q6KXL (100% GPU offload)&lt;/p&gt; &lt;p&gt;Tokens generated - 3081&lt;/p&gt; &lt;p&gt;Time to first token - 0.42s&lt;/p&gt; &lt;p&gt;Token gen rate - 31.54tps&lt;/p&gt; &lt;p&gt;__________________________&lt;/p&gt; &lt;p&gt;I've thought about doing a cost breakdown here, but with price volatility and the fact that so many components have gone up since I got them, I feel like there wouldn't be much of a point and may only mislead someone. Current RAM prices alone would completely change the estimate cost of doing the same build today by several thousand dollars. Still, I thought I'd share my approach on the off chance it inspires or is interesting to someone.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SweetHomeAbalama0"&gt; /u/SweetHomeAbalama0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qi4uj2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qi4uj2/768gb_fully_enclosed_10x_gpu_mobile_ai_build/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qi4uj2/768gb_fully_enclosed_10x_gpu_mobile_ai_build/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T15:56:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1qih9r8</id>
    <title>Current GLM-4.7-Flash implementation confirmed to be broken in llama.cpp</title>
    <updated>2026-01-20T23:28:10+00:00</updated>
    <author>
      <name>/u/Sweet_Albatross9772</name>
      <uri>https://old.reddit.com/user/Sweet_Albatross9772</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Recent discussion in &lt;a href="https://github.com/ggml-org/llama.cpp/pull/18936"&gt;https://github.com/ggml-org/llama.cpp/pull/18936&lt;/a&gt; seems to confirm my suspicions that the current llama.cpp implementation of GLM-4.7-Flash is broken.&lt;/p&gt; &lt;p&gt;There are significant differences in logprobs compared to vLLM. That could explain the looping issues, overthinking, and general poor experiences people have been reporting recently.&lt;/p&gt; &lt;p&gt;Edit:&lt;br /&gt; There is a potential fix already in this PR thanks to Piotr:&lt;br /&gt; &lt;a href="https://github.com/ggml-org/llama.cpp/pull/18980"&gt;https://github.com/ggml-org/llama.cpp/pull/18980&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sweet_Albatross9772"&gt; /u/Sweet_Albatross9772 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qih9r8/current_glm47flash_implementation_confirmed_to_be/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qih9r8/current_glm47flash_implementation_confirmed_to_be/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qih9r8/current_glm47flash_implementation_confirmed_to_be/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T23:28:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1qiuxko</id>
    <title>Local file search engine that understands your documents (OCR + Semantic Search) - Open Source.</title>
    <updated>2026-01-21T10:59:51+00:00</updated>
    <author>
      <name>/u/Hamza3725</name>
      <uri>https://old.reddit.com/user/Hamza3725</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qiuxko/local_file_search_engine_that_understands_your/"&gt; &lt;img alt="Local file search engine that understands your documents (OCR + Semantic Search) - Open Source." src="https://preview.redd.it/j5duc1vhgoeg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8b93de6d4c9a467d7be36c4a64410fe7f1b43b2f" title="Local file search engine that understands your documents (OCR + Semantic Search) - Open Source." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi Llammas!&lt;/p&gt; &lt;p&gt;I’ve been working on &lt;strong&gt;File Brain&lt;/strong&gt;, an open-source desktop tool that lets you search your local files using natural language. It runs 100% locally on your machine.&lt;/p&gt; &lt;h1&gt;The Problem&lt;/h1&gt; &lt;p&gt;We have thousands of files (PDFs, Office docs, images, archives, etc) in our hard drives and we constantly forget their filenames (or we don't even give them correct filenames in first place). Regular search tools often fail in this case because they rely on keyword matching, and they definitely don't understand the &lt;em&gt;content&lt;/em&gt; of a scanned invoice or a screenshot.&lt;/p&gt; &lt;h1&gt;The Solution&lt;/h1&gt; &lt;p&gt;I built a tool that automatically indexes your files and allows you to type queries like &lt;em&gt;&amp;quot;Airplane ticket&amp;quot;&lt;/em&gt; or &lt;em&gt;&amp;quot;Company phone number&amp;quot;&lt;/em&gt; and instantly locates matching files for you, even if the filename is completely random or does not contain these keywords explicitly mentioned.&lt;/p&gt; &lt;h1&gt;Key Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Semantic Search:&lt;/strong&gt; It uses a multilingual embedding model to understand intent. You can search in one language and find docs in another.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;OCR Built-in:&lt;/strong&gt; Can extract the content from most file types, including from images, scanned PDFs, and screenshots.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Privacy First:&lt;/strong&gt; Everything runs locally, including the embedding model.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Tech Stack&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Python/FastAPI/watchdog for backend and the custom filesystem crawler/monitor.&lt;/li&gt; &lt;li&gt;React + PrimeReact for the UI.&lt;/li&gt; &lt;li&gt;Typesense for indexing and search.&lt;/li&gt; &lt;li&gt;Apache Tika for file content extraction.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Interested? try it out at &lt;a href="https://github.com/Hamza5/file-brain"&gt;https://github.com/Hamza5/file-brain&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It’s currently available for &lt;strong&gt;Windows&lt;/strong&gt; and &lt;strong&gt;Linux&lt;/strong&gt;. It should work on Mac too, but I haven't tested it yet.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hamza3725"&gt; /u/Hamza3725 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/j5duc1vhgoeg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qiuxko/local_file_search_engine_that_understands_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qiuxko/local_file_search_engine_that_understands_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-21T10:59:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1qim0e9</id>
    <title>vLLM v0.14.0 released</title>
    <updated>2026-01-21T02:50:09+00:00</updated>
    <author>
      <name>/u/jinnyjuice</name>
      <uri>https://old.reddit.com/user/jinnyjuice</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qim0e9/vllm_v0140_released/"&gt; &lt;img alt="vLLM v0.14.0 released" src="https://external-preview.redd.it/09XZY9bYFkjK1xfZ16UA__JE3yDYBU7C83HKWilthGw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dba3ac860fe3ee793564a3f2e4d6cf66f32e888a" title="vLLM v0.14.0 released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jinnyjuice"&gt; /u/jinnyjuice &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/vllm-project/vllm/releases/tag/v0.14.0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qim0e9/vllm_v0140_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qim0e9/vllm_v0140_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-21T02:50:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1qids6a</id>
    <title>You have 64gb ram and 16gb VRAM; internet is permanently shut off: what 3 models are the ones you use?</title>
    <updated>2026-01-20T21:15:23+00:00</updated>
    <author>
      <name>/u/Adventurous-Gold6413</name>
      <uri>https://old.reddit.com/user/Adventurous-Gold6413</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;No more internet: you have 3 models you can run&lt;/p&gt; &lt;p&gt;What local models are you using?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Adventurous-Gold6413"&gt; /u/Adventurous-Gold6413 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qids6a/you_have_64gb_ram_and_16gb_vram_internet_is/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qids6a/you_have_64gb_ram_and_16gb_vram_internet_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qids6a/you_have_64gb_ram_and_16gb_vram_internet_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T21:15:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1qir5eq</id>
    <title>Here is how to get GLM 4.7 working on llama.cpp with flash attention and correct outputs</title>
    <updated>2026-01-21T07:07:52+00:00</updated>
    <author>
      <name>/u/TokenRingAI</name>
      <uri>https://old.reddit.com/user/TokenRingAI</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Tested GPU: RTX 6000 Blackwell&lt;br /&gt; Tested GGUF: &lt;a href="https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF"&gt;https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF&lt;/a&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Use this git branch to enable flash attention on CUDA &lt;a href="https://github.com/am17an/llama.cpp/tree/glm_4.7_headsize"&gt;https://github.com/am17an/llama.cpp/tree/glm_4.7_headsize&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Add this to your options &lt;code&gt;--override-kv deepseek2.expert\_gating\_func=int:2&lt;/code&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;2000+ tokens/sec prompt, 97 tokens a second generation&lt;/p&gt; &lt;p&gt;Output looks fantastic for a model this size.&lt;/p&gt; &lt;p&gt;Note: Quants might have been made with the wrong function, so you may have to wait for them to be recreated, otherwise you may get nonsensical outputs&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TokenRingAI"&gt; /u/TokenRingAI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qir5eq/here_is_how_to_get_glm_47_working_on_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qir5eq/here_is_how_to_get_glm_47_working_on_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qir5eq/here_is_how_to_get_glm_47_working_on_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-21T07:07:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1qiu6jo</id>
    <title>Knowledge distillation with Claude as the interface: trained a 0.6B model to match GPT-class performance on Text2SQL in a singe conversation</title>
    <updated>2026-01-21T10:14:30+00:00</updated>
    <author>
      <name>/u/party-horse</name>
      <uri>https://old.reddit.com/user/party-horse</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qiu6jo/knowledge_distillation_with_claude_as_the/"&gt; &lt;img alt="Knowledge distillation with Claude as the interface: trained a 0.6B model to match GPT-class performance on Text2SQL in a singe conversation" src="https://preview.redd.it/64ya7ykngoeg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1b1b6edf606c11be574456a3c46cef04dd0dfd81" title="Knowledge distillation with Claude as the interface: trained a 0.6B model to match GPT-class performance on Text2SQL in a singe conversation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Wanted to share a workflow for training small, task-specific models without the usual ML setup overhead.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The problem:&lt;/strong&gt; Off-the-shelf small models are bad at specialized tasks. Qwen3 0.6B on Text2SQL gives you stuff like this:&lt;/p&gt; &lt;p&gt;&lt;code&gt;sql -- Question: &amp;quot;Which artists have total album sales over 1 million?&amp;quot; -- Qwen3 0.6B output: SELECT artists.name FROM artists WHERE artists.genre IS NULL OR artists.country IS NULL; &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Completely wrong. But fine-tuning means data prep, training infrastructure, hyperparameter tuning...&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The approach:&lt;/strong&gt; Knowledge distillation via a Claude skill that wraps &lt;a href="https://docs.distillabs.ai"&gt;distil-cli&lt;/a&gt;. A large teacher model (DeepSeek-V3) generates synthetic training data from your examples, then a small student model learns to match its outputs.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Setup:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;```bash curl -fsSL &lt;a href="https://cli-assets.distillabs.ai/install.sh"&gt;https://cli-assets.distillabs.ai/install.sh&lt;/a&gt; | sh distil login&lt;/p&gt; &lt;h1&gt;In Claude Code:&lt;/h1&gt; &lt;p&gt;/plugin marketplace add &lt;a href="https://github.com/distil-labs/distil-cli-skill"&gt;https://github.com/distil-labs/distil-cli-skill&lt;/a&gt; /plugin install distil-cli@distil-cli-skill ```&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What Claude handles:&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Step&lt;/th&gt; &lt;th&gt;What happens&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Task selection&lt;/td&gt; &lt;td&gt;Recommends QA/classification/tool-calling/RAG based on your description&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Data conversion&lt;/td&gt; &lt;td&gt;Takes whatever format you have, outputs proper JSONL&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Teacher eval&lt;/td&gt; &lt;td&gt;Runs the teacher on your test set — if it scores low, don't bother training&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Training&lt;/td&gt; &lt;td&gt;Kicks off distillation, monitors progress&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Packaging&lt;/td&gt; &lt;td&gt;Downloads GGUF, HuggingFace format, or LoRA adapter&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;My test run:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Input: 100 conversation traces (not cleaned, just raw logs)&lt;/li&gt; &lt;li&gt;Task: Text2SQL&lt;/li&gt; &lt;li&gt;Teacher eval: 80% LLM-as-a-Judge&lt;/li&gt; &lt;li&gt;Final student score: 74%&lt;/li&gt; &lt;li&gt;Base model score: 36%&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Output is a 2.2GB GGUF that runs locally via Ollama.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;After fine-tuning:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;sql -- Same question: &amp;quot;Which artists have total album sales over 1 million?&amp;quot; -- Fine-tuned output: SELECT a.name FROM artists a JOIN albums al ON a.id = al.artist_id GROUP BY a.id, a.name HAVING SUM(al.sales) &amp;gt; 1000000; &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Correct JOINs, proper GROUP BY, HAVING instead of WHERE.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Full benchmark:&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;LLM-as-a-Judge&lt;/th&gt; &lt;th&gt;ROUGE&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Base Qwen3 0.6B&lt;/td&gt; &lt;td&gt;36%&lt;/td&gt; &lt;td&gt;69.3%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepSeek-V3 (teacher)&lt;/td&gt; &lt;td&gt;80%&lt;/td&gt; &lt;td&gt;88.6%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Fine-tuned 0.6B&lt;/td&gt; &lt;td&gt;74%&lt;/td&gt; &lt;td&gt;88.5%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Resources:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Skill: &lt;a href="https://github.com/distil-labs/distil-cli-skill"&gt;github.com/distil-labs/distil-cli-skill&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Full example with data: &lt;a href="https://github.com/distil-labs/distil-example-text2sql-with-claude"&gt;github.com/distil-labs/distil-example-text2sql-with-claude&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Detailed walkthrough: &lt;a href="https://www.distillabs.ai/blog/train-your-slm-with-distil-claude-skill"&gt;distillabs.ai/blog/train-your-slm-with-distil-claude-skill&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Happy to answer questions about the distillation process or the skill implementation.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/party-horse"&gt; /u/party-horse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/64ya7ykngoeg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qiu6jo/knowledge_distillation_with_claude_as_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qiu6jo/knowledge_distillation_with_claude_as_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-21T10:14:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1qiy0ha</id>
    <title>GLM-4.7-Flash-GGUF bug fix - redownload for better outputs</title>
    <updated>2026-01-21T13:34:00+00:00</updated>
    <author>
      <name>/u/etherd0t</name>
      <uri>https://old.reddit.com/user/etherd0t</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Jan 21 update: llama.cpp fixed a bug that caused looping and poor outputs. We updated the GGUFs - please re-download the model for much better outputs.&lt;/p&gt; &lt;p&gt;You can now use Z.ai's recommended parameters and get great results:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;For general use-case: &lt;code&gt;--temp 1.0 --top-p 0.95&lt;/code&gt;&lt;/li&gt; &lt;li&gt;For tool-calling: &lt;code&gt;--temp 0.7 --top-p 1.0&lt;/code&gt;&lt;/li&gt; &lt;li&gt;If using llama.cpp, set &lt;code&gt;--min-p 0.01&lt;/code&gt; as llama.cpp's default is 0.1&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF"&gt;unsloth/GLM-4.7-Flash-GGUF · Hugging Face&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/etherd0t"&gt; /u/etherd0t &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qiy0ha/glm47flashgguf_bug_fix_redownload_for_better/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qiy0ha/glm47flashgguf_bug_fix_redownload_for_better/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qiy0ha/glm47flashgguf_bug_fix_redownload_for_better/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-21T13:34:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1qiwm3c</id>
    <title>Fix for GLM 4.7 Flash has been merged into llama.cpp</title>
    <updated>2026-01-21T12:29:19+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qiwm3c/fix_for_glm_47_flash_has_been_merged_into_llamacpp/"&gt; &lt;img alt="Fix for GLM 4.7 Flash has been merged into llama.cpp" src="https://external-preview.redd.it/P0aZfAO5cQnwgz36bD9sAcDcttCXWcTbQBhIkzY76fc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4ac930f1f077b513ae17d07167d50119d0ac69d0" title="Fix for GLM 4.7 Flash has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The world is saved!&lt;/p&gt; &lt;p&gt;FA for CUDA in progress &lt;a href="https://github.com/ggml-org/llama.cpp/pull/18953"&gt;https://github.com/ggml-org/llama.cpp/pull/18953&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/18980"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qiwm3c/fix_for_glm_47_flash_has_been_merged_into_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qiwm3c/fix_for_glm_47_flash_has_been_merged_into_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-21T12:29:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
