<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-17T22:38:36+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1pp4ax0</id>
    <title>mini-SGLang released: Learn how LLM inference actually works (5K lines, weekend-readable)</title>
    <updated>2025-12-17T18:37:23+00:00</updated>
    <author>
      <name>/u/Expert-Pineapple-740</name>
      <uri>https://old.reddit.com/user/Expert-Pineapple-740</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For anyone who's wanted to understand what's happening under the hood when you run local LLMs:&lt;/p&gt; &lt;p&gt;We just released mini-SGLang ‚Äî SGLang distilled from 300K lines to 5,000. It keeps the full framework's core design and performance, but in a form you can actually read and understand in a weekend.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What you'll learn:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;How modern inference engines handle batching and scheduling&lt;/li&gt; &lt;li&gt;KV cache management and memory optimization&lt;/li&gt; &lt;li&gt;Request routing and parallel processing&lt;/li&gt; &lt;li&gt;The actual implementation behind tools like vLLM and SGLang&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Perfect if you're the type who learns better from clean code than academic papers.&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/lmsysorg/status/2001356624855023669"&gt;https://x.com/lmsysorg/status/2001356624855023669&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Check it out: &lt;a href="https://github.com/sgl-project/mini-sglang"&gt;https://github.com/sgl-project/mini-sglang&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Expert-Pineapple-740"&gt; /u/Expert-Pineapple-740 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp4ax0/minisglang_released_learn_how_llm_inference/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp4ax0/minisglang_released_learn_how_llm_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pp4ax0/minisglang_released_learn_how_llm_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T18:37:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1pozd2k</id>
    <title>Anyone else in a stable wrapper, MIT-licensed fork of Open WebUI?</title>
    <updated>2025-12-17T15:27:32+00:00</updated>
    <author>
      <name>/u/Select-Car3118</name>
      <uri>https://old.reddit.com/user/Select-Car3118</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So... Open WebUI's license situation has been a bit of a rollercoaster (Apache ‚Üí MIT ‚Üí Creative Commons ‚Üí MIT ‚Üí Custom BSD, ...). Now they require keeping their branding or need an enterprise license for 50+ users.&lt;/p&gt; &lt;p&gt;I'm thinking about forking from v0.6.5 (April 2025) - back when it was still properly open source - and keeping it &lt;strong&gt;MIT licensed forever&lt;/strong&gt;. No surprises, no restrictions, just a solid UI for local LLMs that stays truly open.&lt;/p&gt; &lt;p&gt;Let's be honest - the backend's kind of a mess, the UI has rough edges, and there's a lot of room for cleanup. I've been a contributor and I'm tired of watching sponsor-driven features or close dev circle priorities jump the queue while actual user needs get ignored.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The plan would be community driven:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Refactor the messy parts, polish the UX&lt;/li&gt; &lt;li&gt;Fix those annoying bugs that never got prioritized&lt;/li&gt; &lt;li&gt;Implement features based on actual user requests&lt;/li&gt; &lt;li&gt;Host weekly or monthly Discord contributor meetings where people can actually speak their minds - no corporate BS, just honest conversations about what needs fixing&lt;/li&gt; &lt;li&gt;Take inspiration from new Open WebUI features and implement our own (often better) versions&lt;/li&gt; &lt;li&gt;Basically what a lot of us probably wanted Open WebUI to stay as&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Core commitments:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Fork from v0.6.5 (April 2025, BSD-3)&lt;/li&gt; &lt;li&gt;Permanent MIT license - no surprises, ever&lt;/li&gt; &lt;li&gt;Focus on user-friendly improvements over feature bloat&lt;/li&gt; &lt;li&gt;Independent development with community governance&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Just want to see if there's actual interest before I dive into this:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Would you actually use this?&lt;/li&gt; &lt;li&gt;Would anyone want to contribute?&lt;/li&gt; &lt;li&gt;Any name ideas?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Not trying to bash the original project, just want a stable, truly open alternative for those of us who need it.&lt;/p&gt; &lt;p&gt;If there's enough support, I'll set up the repo and coordination channels. Or if someone's already doing this and I completely missed it, let me know, would way rather help out than start yet another fork..&lt;/p&gt; &lt;p&gt;What do you think? Am I crazy or does this make sense?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Select-Car3118"&gt; /u/Select-Car3118 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pozd2k/anyone_else_in_a_stable_wrapper_mitlicensed_fork/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pozd2k/anyone_else_in_a_stable_wrapper_mitlicensed_fork/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pozd2k/anyone_else_in_a_stable_wrapper_mitlicensed_fork/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T15:27:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1pokpha</id>
    <title>QwenLong-L1.5: Revolutionizing Long-Context AI</title>
    <updated>2025-12-17T02:16:15+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pokpha/qwenlongl15_revolutionizing_longcontext_ai/"&gt; &lt;img alt="QwenLong-L1.5: Revolutionizing Long-Context AI" src="https://b.thumbs.redditmedia.com/_W9cLWw-Xs7yDTb51duV9m0DwVBI12aSOhiQ0K4ybyQ.jpg" title="QwenLong-L1.5: Revolutionizing Long-Context AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This new model achieves SOTA long-context reasoning with novel data synthesis, stabilized RL, &amp;amp; memory management for contexts up to 4M tokens.&lt;/p&gt; &lt;p&gt;HuggingFace: &lt;a href="https://huggingface.co/Tongyi-Zhiwen/QwenLong-L1.5-30B-A3B"&gt;https://huggingface.co/Tongyi-Zhiwen/QwenLong-L1.5-30B-A3B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pokpha"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pokpha/qwenlongl15_revolutionizing_longcontext_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pokpha/qwenlongl15_revolutionizing_longcontext_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T02:16:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1pp7jdr</id>
    <title>Free AI tool to translate documents locally</title>
    <updated>2025-12-17T20:43:38+00:00</updated>
    <author>
      <name>/u/Any_Pen2269</name>
      <uri>https://old.reddit.com/user/Any_Pen2269</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have some Epub books i want to translate.&lt;br /&gt; what is the best tool to do this and it is fully free and good at translation.&lt;br /&gt; Thanks in advance&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Pen2269"&gt; /u/Any_Pen2269 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp7jdr/free_ai_tool_to_translate_documents_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp7jdr/free_ai_tool_to_translate_documents_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pp7jdr/free_ai_tool_to_translate_documents_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T20:43:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1pp7x2r</id>
    <title>Variable Sized Experts in MoEs</title>
    <updated>2025-12-17T20:59:06+00:00</updated>
    <author>
      <name>/u/hbfreed</name>
      <uri>https://old.reddit.com/user/hbfreed</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been messing around with variable sized experts in MoEs over the past few months, built on top of &lt;a href="https://github.com/karpathy/nanoGPT"&gt;nanoGPT&lt;/a&gt; (working on nanochat support right now!) and &lt;a href="https://github.com/databricks/megablocks"&gt;MegaBlocks&lt;/a&gt; for efficient MoE computation. &lt;/p&gt; &lt;p&gt;In short, the variable sized models do train faster (the 23:1 ratio of large:small experts trains 20% faster with 2.5% higher loss), but that's just because they're using smaller experts on average. When I compared against vanilla MoEs with the same average size, we don't see an efficiency gain. So, the main practical finding is confirming that you don't need the traditional 4x expansion factor, smaller experts are more efficient (DeepSeek V3 and Kimi K2 already use ~2.57x).&lt;/p&gt; &lt;p&gt;The real work I did was trying to chase down which tokens go to which size of experts on average. In this setup, tokens in constrained contexts like code or recipes go to small experts, and more ambiguous tokens like &amp;quot; with&amp;quot; and &amp;quot; to&amp;quot; go to larger ones. I think it's about contextual constraint. When what comes next is more predictable (code syntax, recipe format), the model learns to use less compute. When it's ambiguous, it learns to use more.&lt;/p&gt; &lt;p&gt;Here's my &lt;a href="https://hbfreed.com/2025/12/16/variable-size-experts.html"&gt;full writeup&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;&lt;a href="https://hbfreed.com/assets/visualizations/moe-routing-viz.html"&gt;Visualization 1&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;&lt;a href="https://hbfreed.com/assets/visualizations/moe-code-routing-viz.html"&gt;Visualization 2 (code boogaloo)&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;and &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/hbfreed/nanoMOE"&gt;Github&lt;/a&gt;!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hbfreed"&gt; /u/hbfreed &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp7x2r/variable_sized_experts_in_moes/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp7x2r/variable_sized_experts_in_moes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pp7x2r/variable_sized_experts_in_moes/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T20:59:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1pp86yi</id>
    <title>Local tools for working with llm datasets?</title>
    <updated>2025-12-17T21:09:52+00:00</updated>
    <author>
      <name>/u/dbplatypii</name>
      <uri>https://old.reddit.com/user/dbplatypii</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been doing data science for years, and am very familiar with jupyter notebooks and more recently been using duckdb a lot. But now I have this huge pile of output tokens from my 4090s, and it feels characteristically different from data I‚Äôve worked with in the past. I haven‚Äôt figured out a good workflow with notebooks and duckdb for working with huge volumes of text data like my training set and llm output traces.&lt;/p&gt; &lt;p&gt;What have you found work well for this? I‚Äôm trying to fine-tune on a large text dataset and be able to inspect the output from eval runs. I would prefer local and open source tools to a paid service.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dbplatypii"&gt; /u/dbplatypii &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp86yi/local_tools_for_working_with_llm_datasets/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp86yi/local_tools_for_working_with_llm_datasets/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pp86yi/local_tools_for_working_with_llm_datasets/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T21:09:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1pp5iye</id>
    <title>[Research] Jacobi Forcing: turning AR LLMs into diffusion-style parallel decoders, staying causal with 4x speedup</title>
    <updated>2025-12-17T19:24:43+00:00</updated>
    <author>
      <name>/u/No_Yogurtcloset_7050</name>
      <uri>https://old.reddit.com/user/No_Yogurtcloset_7050</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp5iye/research_jacobi_forcing_turning_ar_llms_into/"&gt; &lt;img alt="[Research] Jacobi Forcing: turning AR LLMs into diffusion-style parallel decoders, staying causal with 4x speedup" src="https://preview.redd.it/11du08g3ft7g1.gif?width=640&amp;amp;crop=smart&amp;amp;s=454d39b399c2f31907d789e49337ee50fb6be40c" title="[Research] Jacobi Forcing: turning AR LLMs into diffusion-style parallel decoders, staying causal with 4x speedup" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Jacobi Forcing&lt;/strong&gt;: we find an AR model can work as a diffusion-style parallel decoder with 4x speedup while staying causal and maintaining high generation quality.&lt;/p&gt; &lt;p&gt;Autoregressive (AR) LLM and diffusion LLM each come with their unique advantages. We analyze each method's pros and cons and ask a simple question: can we get the best of both worlds by turning an AR model into a causal, native parallel decoder? Check out our blogpost for details: &lt;a href="https://hao-ai-lab.github.io/blogs/jacobi-forcing/"&gt;https://hao-ai-lab.github.io/blogs/jacobi-forcing/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key results&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Overall, Jacobi Forcing model consistently delivers up to 3-4x wall-clock speedup on coding and math tasks with only minor accuracy changes versus greedy AR, while significantly outperforming both dLLMs and prior consistency-based parallel decoders in the accuracy‚Äìthroughput tradeoff. &lt;/p&gt; &lt;p&gt;For more details, please checkout:&lt;/p&gt; &lt;p&gt;Blog: &lt;a href="https://hao-ai-lab.github.io/blogs/jacobi-forcing/"&gt;https://hao-ai-lab.github.io/blogs/jacobi-forcing/&lt;/a&gt;&lt;br /&gt; Code: &lt;a href="https://github.com/hao-ai-lab/JacobiForcing"&gt;https://github.com/hao-ai-lab/JacobiForcing&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Paper: &lt;a href="https://arxiv.org/abs/2512.14681"&gt;https://arxiv.org/abs/2512.14681&lt;/a&gt;&lt;br /&gt; HF: &lt;a href="http://huggingface.co/JacobiForcing"&gt;http://huggingface.co/JacobiForcing&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Yogurtcloset_7050"&gt; /u/No_Yogurtcloset_7050 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/11du08g3ft7g1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp5iye/research_jacobi_forcing_turning_ar_llms_into/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pp5iye/research_jacobi_forcing_turning_ar_llms_into/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T19:24:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1pogwb6</id>
    <title>8x Radeon 7900 XTX Build for Longer Context Local Inference - Performance Results &amp; Build Details</title>
    <updated>2025-12-16T23:20:20+00:00</updated>
    <author>
      <name>/u/Beautiful_Trust_8151</name>
      <uri>https://old.reddit.com/user/Beautiful_Trust_8151</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pogwb6/8x_radeon_7900_xtx_build_for_longer_context_local/"&gt; &lt;img alt="8x Radeon 7900 XTX Build for Longer Context Local Inference - Performance Results &amp;amp; Build Details" src="https://preview.redd.it/furqdxa18n7g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=700d4a8e9197fffc15398e5a63d7abad773cef16" title="8x Radeon 7900 XTX Build for Longer Context Local Inference - Performance Results &amp;amp; Build Details" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been running a multi 7900XTX GPU setup for local AI inference for work and wanted to share some performance numbers and build details for anyone considering a similar route as I have not seen that many of us out there. The system consists of 8x AMD Radeon 7900 XTX cards providing 192 GB VRAM total, paired with an Intel Core i7-14700F on a Z790 motherboard and 192 GB of system RAM. The system is running Windows 11 with a Vulkan backend through LMStudio and Open WebUI. I got a $500 Aliexpress PCIe Gen4 x16 switch expansion card with 64 additional lanes to connect the GPUs to this consumer grade motherboard. This was an upgrade from a 4x 7900XTX GPU system that I have been using for over a year. The total build cost is around $6-7k&lt;/p&gt; &lt;p&gt;I ran some performance testing with GLM4.5Air q6 (99GB file size) Derestricted at different context utilization levels to see how things scale with the maximum allocated context window of 131072 tokens. With an empty context, I'm getting about 437 tokens per second for prompt processing and 27 tokens per second for generation. When the context fills up to around 19k tokens, prompt processing still maintains over 200 tokens per second, though generation speed drops to about 16 tokens per second. The full performance logs show this behavior is consistent across multiple runs, and more importantly, the system is stable. On average the system consums about 900watts during prompt processing and inferencing. &lt;/p&gt; &lt;p&gt;This approach definitely isn't the cheapest option and it's not the most plug-and-play solution out there either. However, for our work use case, the main advantages are upgradability, customizability, and genuine long-context capability with reasonable performance. If you want the flexibility to iterate on your setup over time and have specific requirements around context length and model selection, a custom multi-GPU rig like this has been working really well for us. I would be happy to answer any questions.&lt;/p&gt; &lt;p&gt;Here some raw log data.&lt;br /&gt; 2025-12-16 14:14:22 [DEBUG]&lt;/p&gt; &lt;p&gt;Target model llama_perf stats:&lt;br /&gt; common_perf_print: sampling time = 37.30 ms&lt;br /&gt; common_perf_print: samplers time = 4.80 ms / 1701 tokens&lt;br /&gt; common_perf_print: load time = 95132.76 ms&lt;br /&gt; common_perf_print: prompt eval time = 3577.99 ms / 1564 tokens ( 2.29 ms per token, 437.12 tokens per second)&lt;br /&gt; 2025-12-16 15:05:06 [DEBUG]&lt;br /&gt; common_perf_print: eval time = 301.25 ms / 8 runs ( 37.66 ms per token, 26.56 tokens per second)&lt;br /&gt; common_perf_print: total time = 3919.71 ms / 1572 tokens&lt;br /&gt; common_perf_print: unaccounted time = 3.17 ms / 0.1 % (total - sampling - prompt eval - eval) / (total)&lt;br /&gt; common_perf_print: graphs reused = 7&lt;/p&gt; &lt;p&gt; Target model llama_perf stats:&lt;br /&gt; common_perf_print: sampling time = 704.49 ms&lt;br /&gt; common_perf_print: samplers time = 546.59 ms / 15028 tokens&lt;br /&gt; common_perf_print: load time = 95132.76 ms&lt;br /&gt; common_perf_print: prompt eval time = 66858.77 ms / 13730 tokens ( 4.87 ms per token, 205.36 tokens per second)&lt;br /&gt; 2025-12-16 14:14:22 [DEBUG]&lt;br /&gt; common_perf_print: eval time = 76550.72 ms / 1297 runs ( 59.02 ms per token, 16.94 tokens per second)&lt;br /&gt; common_perf_print: total time = 144171.13 ms / 15027 tokens&lt;br /&gt; common_perf_print: unaccounted time = 57.15 ms / 0.0 % (total - sampling - prompt eval - eval) / (total)&lt;br /&gt; common_perf_print: graphs reused = 1291&lt;/p&gt; &lt;p&gt;Target model llama_perf stats:&lt;br /&gt; common_perf_print: sampling time = 1547.88 ms&lt;br /&gt; common_perf_print: samplers time = 1201.66 ms / 18599 tokens&lt;br /&gt; common_perf_print: load time = 95132.76 ms&lt;br /&gt; common_perf_print: prompt eval time = 77358.07 ms / 15833 tokens ( 4.89 ms per token, 204.67 tokens per second)&lt;br /&gt; common_perf_print: eval time = 171509.89 ms / 2762 runs ( 62.10 ms per token, 16.10 tokens per second)&lt;br /&gt; common_perf_print: total time = 250507.93 ms / 18595 tokens&lt;br /&gt; common_perf_print: unaccounted time = 92.10 ms / 0.0 % (total - sampling - prompt eval - eval) / (total)&lt;br /&gt; common_perf_print: graphs reused = 2750&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Beautiful_Trust_8151"&gt; /u/Beautiful_Trust_8151 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/furqdxa18n7g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pogwb6/8x_radeon_7900_xtx_build_for_longer_context_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pogwb6/8x_radeon_7900_xtx_build_for_longer_context_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T23:20:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1pp2wun</id>
    <title>GLM 4.6V vs. GLM 4.5 Air: Benchmarks and Real-World Tests?</title>
    <updated>2025-12-17T17:44:18+00:00</updated>
    <author>
      <name>/u/MustBeSomethingThere</name>
      <uri>https://old.reddit.com/user/MustBeSomethingThere</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Both models are the same size, but GLM 4.6V is a newer generation and includes vision capabilities. Some argue that adding vision may reduce textual performance, while others believe multimodality could enhance the model‚Äôs overall understanding of the world.&lt;/p&gt; &lt;p&gt;Has anyone run benchmarks or real-world tests comparing the two?&lt;/p&gt; &lt;p&gt;For reference, GLM 4.6V already has support in llama.cpp and GGUFs: &lt;a href="https://huggingface.co/unsloth/GLM-4.6V-GGUF"&gt;https://huggingface.co/unsloth/GLM-4.6V-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MustBeSomethingThere"&gt; /u/MustBeSomethingThere &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp2wun/glm_46v_vs_glm_45_air_benchmarks_and_realworld/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp2wun/glm_46v_vs_glm_45_air_benchmarks_and_realworld/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pp2wun/glm_46v_vs_glm_45_air_benchmarks_and_realworld/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T17:44:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1pp43wr</id>
    <title>We distilled SGLang to help you learn how modern LLM inference works in a weekend</title>
    <updated>2025-12-17T18:29:58+00:00</updated>
    <author>
      <name>/u/Secret_Seaweed_1574</name>
      <uri>https://old.reddit.com/user/Secret_Seaweed_1574</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp43wr/we_distilled_sglang_to_help_you_learn_how_modern/"&gt; &lt;img alt="We distilled SGLang to help you learn how modern LLM inference works in a weekend" src="https://b.thumbs.redditmedia.com/hfEeQJfhvdWJgogmoX3ykQH-YD-IjMq-igf_MgGEpKM.jpg" title="We distilled SGLang to help you learn how modern LLM inference works in a weekend" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/xxb4036c4t7g1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a8ea1c438e9fb3d2625e97881fea5d9dbb5c918e"&gt;https://preview.redd.it/xxb4036c4t7g1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a8ea1c438e9fb3d2625e97881fea5d9dbb5c918e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; üëã,&lt;/p&gt; &lt;p&gt;Mingyi from SGLang here.&lt;/p&gt; &lt;p&gt;We just released mini-SGLang, a distilled version of SGLang that you can actually read and understand in a weekend.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;We distilled SGLang from 300K lines to 5,000 lines&lt;/li&gt; &lt;li&gt;We kept all the core optimizations (overlap scheduling, FlashAttention-3, Radix cache, etc.)&lt;/li&gt; &lt;li&gt;Performance: nearly identical to full SGLang for online serving&lt;/li&gt; &lt;li&gt;It is the only minimal inference project that supports online/offline serving, streaming, and overlap scheduling&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Why we built this:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;A lot of people want to understand how modern LLM inference works under the hood, but diving into 300K lines of production code of SGLang is brutal. We took everything we learned building SGLang and distilled it into something you can actually read, understand, and hack on.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The first version includes:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Overlap Scheduling&lt;/li&gt; &lt;li&gt;FlashAttention-3 + FlashInfer kernels&lt;/li&gt; &lt;li&gt;Radix Cache &amp;amp; Chunked Prefill&lt;/li&gt; &lt;li&gt;Tensor Parallelism&lt;/li&gt; &lt;li&gt;JIT CUDA kernels&lt;/li&gt; &lt;li&gt;OpenAI-compatible API&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Performance (Qwen3-32B, 4x H200, realistic workload):&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fd1o4vte4t7g1.png?width=2700&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f0266d71455d1dc769a44a47403a99fd7f7846c3"&gt;https://preview.redd.it/fd1o4vte4t7g1.png?width=2700&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f0266d71455d1dc769a44a47403a99fd7f7846c3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We built mini-SGLang for engineers, researchers, and students who learn better from code than papers.&lt;/p&gt; &lt;p&gt;We're building more around this: code walkthroughs, cookbooks, and tutorials coming soon!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Post: &lt;a href="https://x.com/lmsysorg/status/2001356624855023669?s=20"&gt;https://x.com/lmsysorg/status/2001356624855023669?s=20&lt;/a&gt;&lt;/li&gt; &lt;li&gt;GitHub: &lt;a href="https://github.com/sgl-project/mini-sglang"&gt;https://github.com/sgl-project/mini-sglang&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Blog post with full benchmarks: &lt;a href="https://lmsys.org/blog/2025-12-17-minisgl/"&gt;https://lmsys.org/blog/2025-12-17-minisgl/&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Happy to answer questions üôè&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Secret_Seaweed_1574"&gt; /u/Secret_Seaweed_1574 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp43wr/we_distilled_sglang_to_help_you_learn_how_modern/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp43wr/we_distilled_sglang_to_help_you_learn_how_modern/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pp43wr/we_distilled_sglang_to_help_you_learn_how_modern/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T18:29:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1powyhk</id>
    <title>[Showcase] AGI-Llama: Bringing Modern LLMs to 1980s Sierra Adventure Games (Space Quest, King's Quest, etc.)</title>
    <updated>2025-12-17T13:48:36+00:00</updated>
    <author>
      <name>/u/Responsible_Fan_2757</name>
      <uri>https://old.reddit.com/user/Responsible_Fan_2757</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1powyhk/showcase_agillama_bringing_modern_llms_to_1980s/"&gt; &lt;img alt="[Showcase] AGI-Llama: Bringing Modern LLMs to 1980s Sierra Adventure Games (Space Quest, King's Quest, etc.)" src="https://external-preview.redd.it/YnNwcm9jcXVxcjdnMUIJV1A_49oRDbYi56Mr7om0CcsWx5OZR_t3Jj5ZXIGi.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4b29da8a57de46c21261503712fae534ac3011c0" title="[Showcase] AGI-Llama: Bringing Modern LLMs to 1980s Sierra Adventure Games (Space Quest, King's Quest, etc.)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone! üëã&lt;/p&gt; &lt;p&gt;I wanted to share a project I've been working on: &lt;strong&gt;AGI-Llama&lt;/strong&gt;. It is a modern evolution of the classic NAGI (New Adventure Game Interpreter), but with a twist‚ÄîI've integrated Large Language Models directly into the engine.&lt;/p&gt; &lt;p&gt;The goal is to transform how we interact with retro Sierra titles like &lt;em&gt;Space Quest&lt;/em&gt;, &lt;em&gt;King's Quest&lt;/em&gt;, or &lt;em&gt;Leisure Suit Larry&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What makes it different?&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;ü§ñ &lt;strong&gt;Natural Language Input:&lt;/strong&gt; Stop struggling with &amp;quot;verb noun&amp;quot; syntax. Talk to the game naturally.&lt;/li&gt; &lt;li&gt;üåç &lt;strong&gt;Play in any language:&lt;/strong&gt; Thanks to the LLM layer and new SDL_ttf support, you can play classic AGI games in Spanish, French, Japanese, or any language the model supports.&lt;/li&gt; &lt;li&gt;üöÄ &lt;strong&gt;Modern Tech Stack:&lt;/strong&gt; Ported to &lt;strong&gt;SDL3&lt;/strong&gt;, featuring GPU acceleration and Unicode support.&lt;/li&gt; &lt;li&gt;üß† &lt;strong&gt;Flexible Backends:&lt;/strong&gt; It supports &lt;code&gt;llama.cpp&lt;/code&gt; for local inference (Llama 3, Qwen, Gemma), BitNet for 1.58-bit models, and Cloud APIs (OpenAI, Hugging Face, Groq).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It‚Äôs an experimental research project to explore the intersection of AI and retro gaming architecture. The LLM logic is encapsulated in a library that could potentially be integrated into other projects like ScummV&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub Repository:&lt;/strong&gt;&lt;a href="https://github.com/jalfonsosm/agi-llm"&gt;https://github.com/jalfonsosm/agi-llm&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I‚Äôd love to hear your thoughts, especially regarding async LLM implementation and context management for old adventure game states!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Responsible_Fan_2757"&gt; /u/Responsible_Fan_2757 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/liiuhlouqr7g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1powyhk/showcase_agillama_bringing_modern_llms_to_1980s/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1powyhk/showcase_agillama_bringing_modern_llms_to_1980s/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T13:48:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1pozpcq</id>
    <title>You can now fine-tune LLMs and deploy them directly on your phone!</title>
    <updated>2025-12-17T15:40:50+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pozpcq/you_can_now_finetune_llms_and_deploy_them/"&gt; &lt;img alt="You can now fine-tune LLMs and deploy them directly on your phone!" src="https://preview.redd.it/zi5ph67zas7g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=98235903a72ce7dadc57bc88ea65f48143f89438" title="You can now fine-tune LLMs and deploy them directly on your phone!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Source: &lt;a href="https://docs.unsloth.ai/new/deploy-llms-phone"&gt;https://docs.unsloth.ai/new/deploy-llms-phone&lt;/a&gt;&lt;/p&gt; &lt;p&gt;you can:&lt;/p&gt; &lt;p&gt;Use the same tech (ExecuTorch) Meta has to power billions on Instagram, WhatsApp&lt;/p&gt; &lt;p&gt;Deploy Qwen3-0.6B locally to Pixel 8 and iPhone 15 Pro at ~40 tokens/s&lt;/p&gt; &lt;p&gt;Apply QAT via TorchAO to recover 70% of accuracy&lt;/p&gt; &lt;p&gt;Get privacy first, instant responses and offline capabilities&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zi5ph67zas7g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pozpcq/you_can_now_finetune_llms_and_deploy_them/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pozpcq/you_can_now_finetune_llms_and_deploy_them/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T15:40:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1powhy6</id>
    <title>anthropic blog on code execution for agents. 98.7% token reduction sounds promising for local setups</title>
    <updated>2025-12-17T13:27:38+00:00</updated>
    <author>
      <name>/u/Zestyclose_Ring1123</name>
      <uri>https://old.reddit.com/user/Zestyclose_Ring1123</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;anthropic published this detailed blog about &amp;quot;code execution&amp;quot; for agents: &lt;a href="https://www.anthropic.com/engineering/code-execution-with-mcp"&gt;https://www.anthropic.com/engineering/code-execution-with-mcp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;instead of direct tool calls, model writes code that orchestrates tools&lt;/p&gt; &lt;p&gt;they claim massive token reduction. like 150k down to 2k in their example. sounds almost too good to be true&lt;/p&gt; &lt;p&gt;basic idea: dont preload all tool definitions. let model explore available tools on demand. data flows through variables not context&lt;/p&gt; &lt;p&gt;for local models this could be huge. context limits hit way harder when youre running smaller models&lt;/p&gt; &lt;p&gt;the privacy angle is interesting too. sensitive data never enters model context, flows directly between tools&lt;/p&gt; &lt;p&gt;cloudflare independently discovered this &amp;quot;code mode&amp;quot; pattern according to the blog&lt;/p&gt; &lt;p&gt;main challenge would be sandboxing. running model-generated code locally needs serious isolation&lt;/p&gt; &lt;p&gt;but if you can solve that, complex agents might become viable on consumer hardware. 8k context instead of needing 128k+&lt;/p&gt; &lt;p&gt;tools like cursor and verdent already do basic code generation. this anthropic approach could push that concept way further&lt;/p&gt; &lt;p&gt;wondering if anyone has experimented with similar patterns locally&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zestyclose_Ring1123"&gt; /u/Zestyclose_Ring1123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1powhy6/anthropic_blog_on_code_execution_for_agents_987/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1powhy6/anthropic_blog_on_code_execution_for_agents_987/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1powhy6/anthropic_blog_on_code_execution_for_agents_987/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T13:27:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1pp5fvp</id>
    <title>Lemonade v9.1 - ROCm 7 for Strix Point - Roadmap Update - Strix Halo Survey</title>
    <updated>2025-12-17T19:21:24+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp5fvp/lemonade_v91_rocm_7_for_strix_point_roadmap/"&gt; &lt;img alt="Lemonade v9.1 - ROCm 7 for Strix Point - Roadmap Update - Strix Halo Survey" src="https://preview.redd.it/wejf7bjdat7g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=53e9285565bba2d3606267ad5ac0cc86aaf612e8" title="Lemonade v9.1 - ROCm 7 for Strix Point - Roadmap Update - Strix Halo Survey" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;, I'm back with a final update for the year and some questions from AMD for you all.&lt;/p&gt; &lt;p&gt;If you haven't heard of Lemonade, it's a local LLM/GenAI router and backend manager that helps you discover and run optimized LLMs with apps like n8n, VS Code Copilot, Open WebUI, and many more.&lt;/p&gt; &lt;h1&gt;Lemonade Update&lt;/h1&gt; &lt;p&gt;Lemonade v9.1 is out, which checks off most of the roadmap items from the v9.0 post a few weeks ago:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The &lt;strong&gt;new Lemonade app&lt;/strong&gt; is available in the &lt;code&gt;lemonade.deb&lt;/code&gt; and &lt;code&gt;lemonade.msi&lt;/code&gt; installers. The goal is to get you set up and connecting to other apps ASAP, and users are not expected to spend loads of time in our app.&lt;/li&gt; &lt;li&gt;Basic &lt;strong&gt;audio input&lt;/strong&gt; (aka ASR aka STT) is enabled through the OpenAI transcriptions API via whisper.cpp.&lt;/li&gt; &lt;li&gt;By popular demand, &lt;strong&gt;Strix Point has ROCm 7 + llamacpp support&lt;/strong&gt; (aka Ryzen AI 360-375 aka Radeon 880-890M aka gfx1150) in Lemonade with &lt;code&gt;--llamacpp rocm&lt;/code&gt; as well as in the upstream &lt;a href="https://github.com/lemonade-sdk/llamacpp-rocm"&gt;llamacpp-rocm project&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Also by popular demand, &lt;code&gt;--extra-models-dir&lt;/code&gt; lets you bring LLM GGUFs from anywhere on your PC into Lemonade.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Next on the Lemonade roadmap in 2026 is more output modalities: image generation from stablediffusion.cpp, as well as text-to-speech. At that point Lemonade will support I/O of text, images, and speech from a single base URL.&lt;/p&gt; &lt;p&gt;Links: &lt;a href="https://github.com/lemonade-sdk/lemonade"&gt;GitHub&lt;/a&gt; and &lt;a href="https://discord.gg/5xXzkMu8Zk"&gt;Discord&lt;/a&gt;. Come say hi if you like the project :)&lt;/p&gt; &lt;h1&gt;Strix Halo Survey&lt;/h1&gt; &lt;p&gt;AMD leadership wants to know what you think of Strix Halo (aka Ryzen AI MAX 395). The specific questions are as follows, but please give any feedback you like as well!&lt;/p&gt; &lt;ol&gt; &lt;li&gt;If you own a Strix Halo: &lt;ol&gt; &lt;li&gt;What do you enjoy doing with it?&lt;/li&gt; &lt;li&gt;What do you want to do, but is too difficult or impossible today?&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li&gt;If you're considering buying a Strix Halo: what software and/or content do you need to see from AMD?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;(I've been tracking/reporting feedback from my own posts and others' posts all year, and feel I have a good sense, but it's useful to get people's thoughts in this one place in a semi-official way)&lt;br /&gt; edit: formatting&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wejf7bjdat7g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp5fvp/lemonade_v91_rocm_7_for_strix_point_roadmap/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pp5fvp/lemonade_v91_rocm_7_for_strix_point_roadmap/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T19:21:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1pozr6f</id>
    <title>Claude Code, GPT-5.2, DeepSeek v3.2, and Self-Hosted Devstral 2 on Fresh SWE-rebench (November 2025)</title>
    <updated>2025-12-17T15:42:43+00:00</updated>
    <author>
      <name>/u/CuriousPlatypus1881</name>
      <uri>https://old.reddit.com/user/CuriousPlatypus1881</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pozr6f/claude_code_gpt52_deepseek_v32_and_selfhosted/"&gt; &lt;img alt="Claude Code, GPT-5.2, DeepSeek v3.2, and Self-Hosted Devstral 2 on Fresh SWE-rebench (November 2025)" src="https://external-preview.redd.it/t4cNt5D638DSOJgsxl8f-7IwJhLpxHIh7HxK5GHcBJE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9b72b5025e78c2cc97de15c8fea348f262235ecb" title="Claude Code, GPT-5.2, DeepSeek v3.2, and Self-Hosted Devstral 2 on Fresh SWE-rebench (November 2025)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, I‚Äôm Anton from Nebius.&lt;/p&gt; &lt;p&gt;We‚Äôve updated the &lt;strong&gt;SWE-rebench&lt;/strong&gt; leaderboard with our &lt;strong&gt;November runs&lt;/strong&gt; on &lt;strong&gt;47 fresh GitHub PR tasks&lt;/strong&gt; (PRs created in the previous month only). It‚Äôs a SWE-bench‚Äìstyle setup: models read real PR issues, run tests, edit code, and must make the suite pass. &lt;/p&gt; &lt;p&gt;This update includes a particularly large wave of new releases, so we‚Äôve added a substantial batch of new models to the leaderboard:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Devstral 2&lt;/strong&gt; ‚Äî a strong release of models that can be run locally given their size&lt;/li&gt; &lt;li&gt;&lt;strong&gt;DeepSeek v3.2&lt;/strong&gt; ‚Äî a new state-of-the-art open-weight model&lt;/li&gt; &lt;li&gt;A &lt;strong&gt;new comparison mode&lt;/strong&gt; to benchmark models against external systems such as &lt;strong&gt;Claude Code&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We also introduced a &lt;strong&gt;cached-tokens statistic&lt;/strong&gt; to improve transparency around cache usage.&lt;/p&gt; &lt;p&gt;Looking forward to your thoughts and suggestions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CuriousPlatypus1881"&gt; /u/CuriousPlatypus1881 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://swe-rebench.com/?insight=nov_2025"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pozr6f/claude_code_gpt52_deepseek_v32_and_selfhosted/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pozr6f/claude_code_gpt52_deepseek_v32_and_selfhosted/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T15:42:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1pow797</id>
    <title>Peak LLM Wars: Xiaomi Blocks Kimi Employees on Twitter</title>
    <updated>2025-12-17T13:14:02+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pow797/peak_llm_wars_xiaomi_blocks_kimi_employees_on/"&gt; &lt;img alt="Peak LLM Wars: Xiaomi Blocks Kimi Employees on Twitter" src="https://a.thumbs.redditmedia.com/-V8KieEduFhCtfHKuiRcs_94wVQuIC9TTbPuo7vOPY8.jpg" title="Peak LLM Wars: Xiaomi Blocks Kimi Employees on Twitter" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/kujwpbsakr7g1.jpg?width=1194&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=b5a113e06d0e8db66436dc632a8828a85bb8d16e"&gt;https://preview.redd.it/kujwpbsakr7g1.jpg?width=1194&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=b5a113e06d0e8db66436dc632a8828a85bb8d16e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8jlban9qkr7g1.jpg?width=789&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=7984f0c584b0b67cc49f6b24d3ae920d42e3ccc0"&gt;https://preview.redd.it/8jlban9qkr7g1.jpg?width=789&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=7984f0c584b0b67cc49f6b24d3ae920d42e3ccc0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;LLM wars are wild&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pow797/peak_llm_wars_xiaomi_blocks_kimi_employees_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pow797/peak_llm_wars_xiaomi_blocks_kimi_employees_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pow797/peak_llm_wars_xiaomi_blocks_kimi_employees_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T13:14:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1pox733</id>
    <title>LangChain and LlamaIndex are in "steep decline" according to new ecosystem report. Anyone else quietly ditching agent frameworks?</title>
    <updated>2025-12-17T13:59:02+00:00</updated>
    <author>
      <name>/u/Exact-Literature-395</name>
      <uri>https://old.reddit.com/user/Exact-Literature-395</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I stumbled on this LLM Development Landscape 2.0 report from Ant Open Source and it basically confirmed what I've been feeling for months.&lt;/p&gt; &lt;p&gt;LangChain, LlamaIndex and AutoGen are all listed as &amp;quot;steepest declining&amp;quot; projects by community activity over the past 6 months. The report says it's due to &amp;quot;reduced community investment from once dominant projects.&amp;quot; Meanwhile stuff like vLLM and SGLang keeps growing.&lt;/p&gt; &lt;p&gt;Honestly this tracks with my experience. I spent way too long fighting with LangChain abstractions last year before I just ripped it out and called the APIs directly. Cut my codebase in half and debugging became actually possible. Every time I see a tutorial using LangChain now I just skip it.&lt;/p&gt; &lt;p&gt;But I'm curious if this is just me being lazy or if there's a real shift happening. Are agent frameworks solving a problem that doesn't really exist anymore now that the base models are good enough? Or am I missing something and these tools are still essential for complex workflows?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Exact-Literature-395"&gt; /u/Exact-Literature-395 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pox733/langchain_and_llamaindex_are_in_steep_decline/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pox733/langchain_and_llamaindex_are_in_steep_decline/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pox733/langchain_and_llamaindex_are_in_steep_decline/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T13:59:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1pp2j60</id>
    <title>Drummer's Cydonia and Magidonia 24B v4.3 - The best pair of Cydonia for RP yet!</title>
    <updated>2025-12-17T17:29:47+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After 20+ iterations, 3 close calls, we've finally come to a release. The best Cydonia so far. At least that's what the testers at Beaver have been saying.&lt;/p&gt; &lt;p&gt;Peak Cydonia! Served by yours truly.&lt;/p&gt; &lt;p&gt;Small 3.2: &lt;a href="https://huggingface.co/TheDrummer/Cydonia-24B-v4.3"&gt;https://huggingface.co/TheDrummer/Cydonia-24B-v4.3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Magistral 1.2: &lt;a href="https://huggingface.co/TheDrummer/Magidonia-24B-v4.3"&gt;https://huggingface.co/TheDrummer/Magidonia-24B-v4.3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(Most prefer Magidonia, but they're both pretty good!)&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;To my patrons,&lt;/p&gt; &lt;p&gt;Earlier this week, I had a difficult choice to make. Thanks to your support, I get to enjoy the freedom you've granted me. Thank you for giving me strength to pursue this journey. I will continue dishing out the best tunes possible for you, truly.&lt;/p&gt; &lt;p&gt;- Drummer&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp2j60/drummers_cydonia_and_magidonia_24b_v43_the_best/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp2j60/drummers_cydonia_and_magidonia_24b_v43_the_best/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pp2j60/drummers_cydonia_and_magidonia_24b_v43_the_best/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T17:29:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1pp2rtn</id>
    <title>Nemotron was post-trained to assume humans have reasoning, but they never use it</title>
    <updated>2025-12-17T17:38:58+00:00</updated>
    <author>
      <name>/u/RetiredApostle</name>
      <uri>https://old.reddit.com/user/RetiredApostle</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp2rtn/nemotron_was_posttrained_to_assume_humans_have/"&gt; &lt;img alt="Nemotron was post-trained to assume humans have reasoning, but they never use it" src="https://preview.redd.it/52423nr8us7g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b4ca1f0be0378b305c1a58efa1b2f8b99752b5ff" title="Nemotron was post-trained to assume humans have reasoning, but they never use it" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RetiredApostle"&gt; /u/RetiredApostle &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/52423nr8us7g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp2rtn/nemotron_was_posttrained_to_assume_humans_have/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pp2rtn/nemotron_was_posttrained_to_assume_humans_have/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T17:38:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1pp8vo4</id>
    <title>Nvidia plans heavy cuts to GPU supply in early 2026</title>
    <updated>2025-12-17T21:37:13+00:00</updated>
    <author>
      <name>/u/HumanDrone8721</name>
      <uri>https://old.reddit.com/user/HumanDrone8721</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HumanDrone8721"&gt; /u/HumanDrone8721 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://overclock3d.net/news/gpu-displays/nvidia-plans-heavy-cuts-to-gpu-supply-in-early-2026/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp8vo4/nvidia_plans_heavy_cuts_to_gpu_supply_in_early/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pp8vo4/nvidia_plans_heavy_cuts_to_gpu_supply_in_early/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T21:37:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1pp6jhq</id>
    <title>Hey, LocalLLaMa. We need to talk...</title>
    <updated>2025-12-17T20:04:07+00:00</updated>
    <author>
      <name>/u/Eisenstein</name>
      <uri>https://old.reddit.com/user/Eisenstein</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I look on the front page and I see people who have spent time and effort to make something, and they share it willingly. They are getting no upvotes.&lt;/p&gt; &lt;p&gt;We are here because we are &lt;em&gt;local&lt;/em&gt; and we are &lt;em&gt;open source&lt;/em&gt;. Those things &lt;em&gt;depend on people who give us things&lt;/em&gt;, and they don't ask for anything in return, but they &lt;em&gt;need&lt;/em&gt; something in return or they will stop.&lt;/p&gt; &lt;p&gt;Pop your head into the smaller posts where someone is showing work they have done. Give honest and constructive feedback. UPVOTE IT.&lt;/p&gt; &lt;p&gt;The project may be terrible -- encourage them to grow by telling them how they can make it better. &lt;/p&gt; &lt;p&gt;The project may be awesome. They would love to hear how awesome it is. But if you use it, then they would love 100 times more to hear how you use it and how it helps you.&lt;/p&gt; &lt;p&gt;Engage with the people who share their things, and not just with the entertainment. &lt;/p&gt; &lt;p&gt;It take so little effort but it makes so much difference.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Eisenstein"&gt; /u/Eisenstein &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp6jhq/hey_localllama_we_need_to_talk/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp6jhq/hey_localllama_we_need_to_talk/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pp6jhq/hey_localllama_we_need_to_talk/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T20:04:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1porpwd</id>
    <title>Microsoft's TRELLIS 2-4B, An Open-Source Image-to-3D Model</title>
    <updated>2025-12-17T08:49:00+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1porpwd/microsofts_trellis_24b_an_opensource_imageto3d/"&gt; &lt;img alt="Microsoft's TRELLIS 2-4B, An Open-Source Image-to-3D Model" src="https://external-preview.redd.it/OXpuN3VqYnE4cTdnMbhg7mfH3BLNBAJzBcqwf-BeiskbYrfqW4XgiIx-FQh0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6475aef4e90b21644bf95a26618a75433c2e08de" title="Microsoft's TRELLIS 2-4B, An Open-Source Image-to-3D Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Model Details&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Model Type:&lt;/strong&gt; Flow-Matching Transformers with Sparse Voxel based 3D VAE&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Parameters:&lt;/strong&gt; 4 Billion&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Input:&lt;/strong&gt; Single Image&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Output:&lt;/strong&gt; 3D Asset &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Model - &lt;a href="https://huggingface.co/microsoft/TRELLIS.2-4B"&gt;https://huggingface.co/microsoft/TRELLIS.2-4B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Demo - &lt;a href="https://huggingface.co/spaces/microsoft/TRELLIS.2"&gt;https://huggingface.co/spaces/microsoft/TRELLIS.2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog post - &lt;a href="https://microsoft.github.io/TRELLIS.2/"&gt;https://microsoft.github.io/TRELLIS.2/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/g8uco5dq8q7g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1porpwd/microsofts_trellis_24b_an_opensource_imageto3d/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1porpwd/microsofts_trellis_24b_an_opensource_imageto3d/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T08:49:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1poy0lb</id>
    <title>Apple introduces SHARP, a model that generates a photorealistic 3D Gaussian representation from a single image in seconds.</title>
    <updated>2025-12-17T14:33:13+00:00</updated>
    <author>
      <name>/u/themixtergames</name>
      <uri>https://old.reddit.com/user/themixtergames</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1poy0lb/apple_introduces_sharp_a_model_that_generates_a/"&gt; &lt;img alt="Apple introduces SHARP, a model that generates a photorealistic 3D Gaussian representation from a single image in seconds." src="https://external-preview.redd.it/YWpkODI1NDF4cjdnMbxNGAI-puPRf-AP3cgrLxlreCeM4kV742La4OIIHHvj.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a1dded6d1cdcc956e0916d9926400982637f4d7c" title="Apple introduces SHARP, a model that generates a photorealistic 3D Gaussian representation from a single image in seconds." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GitHub: &lt;a href="https://github.com/apple/ml-sharp"&gt;https://github.com/apple/ml-sharp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Paper: &lt;a href="https://arxiv.org/abs/2512.10685"&gt;https://arxiv.org/abs/2512.10685&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/themixtergames"&gt; /u/themixtergames &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/l2mp7b31xr7g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1poy0lb/apple_introduces_sharp_a_model_that_generates_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1poy0lb/apple_introduces_sharp_a_model_that_generates_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T14:33:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1pniwfj</id>
    <title>Ai2 Open Modeling AMA ft researchers from the Molmo and Olmo teams.</title>
    <updated>2025-12-15T21:02:55+00:00</updated>
    <author>
      <name>/u/ai2_official</name>
      <uri>https://old.reddit.com/user/ai2_official</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pniwfj/ai2_open_modeling_ama_ft_researchers_from_the/"&gt; &lt;img alt="Ai2 Open Modeling AMA ft researchers from the Molmo and Olmo teams." src="https://b.thumbs.redditmedia.com/bsv34WIHZXC49Az9mFES5lSIAtaQ2CuLZJ4dCaLxsEY.jpg" title="Ai2 Open Modeling AMA ft researchers from the Molmo and Olmo teams." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;! We‚Äôre researchers and engineers from Ai2, the nonprofit AI lab. We recently announced:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Molmo 2&lt;/strong&gt;‚Äîopen multimodal models for video + images that can return grounded answers (pixel coordinates + timestamps), trained with open datasets&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Olmo 3&lt;/strong&gt;‚Äîa family of fully open language models (7B‚Äì32B) with Base/Instruct/Thinking variants, long‚Äëcontext support, open training recipes &amp;amp; checkpoints&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Ask us anything about local inference, training mixes &amp;amp; our truly open approach, long‚Äëcontext, grounded video QA/tracking, and real‚Äëworld deployment.&lt;/p&gt; &lt;p&gt;Participating in the AMA:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Molmo 2 researchers:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Ranjay Krishna ( &lt;a href="/u/ranjaykrishna"&gt;u/ranjaykrishna&lt;/a&gt; )&lt;/li&gt; &lt;li&gt;Zixian Ma ( &lt;a href="/u/Frequent_Rooster2980"&gt;u/Frequent_Rooster2980&lt;/a&gt; )&lt;/li&gt; &lt;li&gt;Chris Clark ( &lt;a href="/u/mostly_reasonable"&gt;u/mostly_reasonable&lt;/a&gt; )&lt;/li&gt; &lt;li&gt;Jieyu Zhang ( &lt;a href="/u/Jealous_Programmer51"&gt;u/Jealous_Programmer51&lt;/a&gt; )&lt;/li&gt; &lt;li&gt;Rohun Tripathi ( &lt;a href="/u/darkerWind"&gt;u/darkerWind&lt;/a&gt; )&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Olmo 3 researchers:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Kyle Lo ( &lt;a href="/u/klstats"&gt;u/klstats&lt;/a&gt; )&lt;/li&gt; &lt;li&gt;Allyson Ettinger ( &lt;a href="/u/aeclang"&gt;u/aeclang&lt;/a&gt; )&lt;/li&gt; &lt;li&gt;Finbarr Timbers ( &lt;a href="/u/fnbr"&gt;u/fnbr&lt;/a&gt; )&lt;/li&gt; &lt;li&gt;Faeze Brahman ( &lt;a href="/u/faebrhn"&gt;u/faebrhn&lt;/a&gt; )&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We‚Äôll be live from &lt;strong&gt;1pm&lt;/strong&gt; to &lt;strong&gt;2pm PST.&lt;/strong&gt; Read up on our latest releases below, and feel welcome to jump in anytime!&lt;/p&gt; &lt;ul&gt; &lt;li&gt;‚ñ∂Ô∏è &lt;strong&gt;Try in the Playground:&lt;/strong&gt; &lt;a href="https://playground.allenai.org"&gt;https://playground.allenai.org&lt;/a&gt;&lt;/li&gt; &lt;li&gt;‚¨áÔ∏è &lt;strong&gt;Download&lt;/strong&gt;: &lt;a href="https://huggingface.co/collections/allenai/molmo2"&gt;https://huggingface.co/collections/allenai/molmo2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;üìù &lt;strong&gt;Blog&lt;/strong&gt;: &lt;a href="https://allenai.org/blog/molmo2"&gt;https://allenai.org/blog/molmo2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;üìÑReport: &lt;a href="https://allenai.org/papers/molmo2"&gt;https://allenai.org/papers/molmo2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;üíª &lt;strong&gt;API coming soon&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;ü´Ü PROOF:&lt;/strong&gt; &lt;a href="https://x.com/allen_ai/status/2000692253606514828"&gt;https://x.com/allen_ai/status/2000692253606514828&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Join us on Reddit&lt;/strong&gt; &lt;a href="/r/allenai"&gt;r/allenai&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;Join Ai2 on Discord:&lt;/strong&gt; &lt;a href="https://discord.gg/6vWDHyTCQV"&gt;https://discord.gg/6vWDHyTCQV&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fxw1g2fcmf7g1.jpg?width=1080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=009a9377edfefefc5efd52db0af81b807b9971b8"&gt;https://preview.redd.it/fxw1g2fcmf7g1.jpg?width=1080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=009a9377edfefefc5efd52db0af81b807b9971b8&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Thank you everyone for the kind words and great questions! This AMA has ended as of 2pm PST (5pm EST) on Dec. 16.&lt;/p&gt; &lt;p&gt;&lt;a href="https://discord.gg/6vWDHyTCQV"&gt;Join Ai2 on Discord&lt;/a&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ai2_official"&gt; /u/ai2_official &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pniwfj/ai2_open_modeling_ama_ft_researchers_from_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pniwfj/ai2_open_modeling_ama_ft_researchers_from_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pniwfj/ai2_open_modeling_ama_ft_researchers_from_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T21:02:55+00:00</published>
  </entry>
</feed>
