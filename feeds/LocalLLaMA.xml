<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-03T18:40:27+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI. Hey, Tony</subtitle>
  <entry>
    <id>t3_1nw8c6y</id>
    <title>Granite 4.0 Micro (3.4B) running 100% locally in your browser w/ WebGPU acceleration</title>
    <updated>2025-10-02T16:20:56+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw8c6y/granite_40_micro_34b_running_100_locally_in_your/"&gt; &lt;img alt="Granite 4.0 Micro (3.4B) running 100% locally in your browser w/ WebGPU acceleration" src="https://external-preview.redd.it/aG1yZ2k0M3Y0cXNmMTjBkk0zpHe1cUKuUpjTdKuc-czjYGWzckCtqtrm-IdD.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2fd6b6a33eda5421f6a81ae5b65f2f068b49e13c" title="Granite 4.0 Micro (3.4B) running 100% locally in your browser w/ WebGPU acceleration" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/14cmif4v4qsf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw8c6y/granite_40_micro_34b_running_100_locally_in_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nw8c6y/granite_40_micro_34b_running_100_locally_in_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T16:20:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1nw2wd6</id>
    <title>Granite 4.0 Language Models - a ibm-granite Collection</title>
    <updated>2025-10-02T12:51:10+00:00</updated>
    <author>
      <name>/u/rerri</name>
      <uri>https://old.reddit.com/user/rerri</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw2wd6/granite_40_language_models_a_ibmgranite_collection/"&gt; &lt;img alt="Granite 4.0 Language Models - a ibm-granite Collection" src="https://external-preview.redd.it/dG6nrEEPIkS2YfUpzm-ii0PPK1xkTA3ZMcynqcTCXQc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=374e83fed526e7600d653259e65b30be13801c21" title="Granite 4.0 Language Models - a ibm-granite Collection" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Granite 4, &lt;strong&gt;32B-A9B, 7B-A1B, and 3B&lt;/strong&gt; dense models available.&lt;/p&gt; &lt;p&gt;GGUF's are in the same repo:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/ibm-granite/granite-quantized-models-67f944eddd16ff8e057f115c"&gt;https://huggingface.co/collections/ibm-granite/granite-quantized-models-67f944eddd16ff8e057f115c&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rerri"&gt; /u/rerri &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/ibm-granite/granite-40-language-models-6811a18b820ef362d9e5a82c"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw2wd6/granite_40_language_models_a_ibmgranite_collection/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nw2wd6/granite_40_language_models_a_ibmgranite_collection/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T12:51:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1nw52ad</id>
    <title>Introducing Onyx - a fully open source chat UI with RAG, web search, deep research, and MCP</title>
    <updated>2025-10-02T14:18:05+00:00</updated>
    <author>
      <name>/u/Weves11</name>
      <uri>https://old.reddit.com/user/Weves11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw52ad/introducing_onyx_a_fully_open_source_chat_ui_with/"&gt; &lt;img alt="Introducing Onyx - a fully open source chat UI with RAG, web search, deep research, and MCP" src="https://external-preview.redd.it/ODh3bjRsOWJpcHNmMcggpjsEMzF-IE1l8vJahmQmeeToARZwc_P-uEOcis7p.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=491d98e181b37d6e6d0003c442bfc14ebbed0594" title="Introducing Onyx - a fully open source chat UI with RAG, web search, deep research, and MCP" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Weves11"&gt; /u/Weves11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/vklzqk9bipsf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw52ad/introducing_onyx_a_fully_open_source_chat_ui_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nw52ad/introducing_onyx_a_fully_open_source_chat_ui_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T14:18:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1nx5ia3</id>
    <title>Any models that might be good with gauges?</title>
    <updated>2025-10-03T17:28:13+00:00</updated>
    <author>
      <name>/u/ronneldavis</name>
      <uri>https://old.reddit.com/user/ronneldavis</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was having an interesting thought of solving an old problem I had come across - how to take an image of any random gauge and get its reading as structured output. Previously I had tried using open CV and a few image transforms followed ocr and line detection to cobble up a solution, but it was brittle and failed under changing lighting conditions and every style of gauge had to be manually calibrated. Recently with improving vision models, thought I’d give it a try. With UI-TARS-7B as a first try, I was able to get a reading on the first try with minimal prompting to within 15% of the true value. And then I thought I’d give frontier models a shot and I was surprised with the results. With GPT-5, the error was 22%, and with Claude 4.5, it was at 38%! This led me to believe that specialized local models be more capable at this then large general ones. Also if you all have any knowledge of a benchmark that tracks this (I know of the analog clock one that came out recently), would be helpful. Else I’d love to try my hand at building one out.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ronneldavis"&gt; /u/ronneldavis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nx5ia3/any_models_that_might_be_good_with_gauges/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nx5ia3/any_models_that_might_be_good_with_gauges/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nx5ia3/any_models_that_might_be_good_with_gauges/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T17:28:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1nx5kuv</id>
    <title>ERNIE-4.5-VL - anyone testing it in the competition, what’s your workflow?</title>
    <updated>2025-10-03T17:30:47+00:00</updated>
    <author>
      <name>/u/Mak4560H</name>
      <uri>https://old.reddit.com/user/Mak4560H</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So the ERNIE-4.5-VL competition is live, and I’ve been testing the model a bit for vision-language tasks. Wanted to ask the community: how are you all running VL? &lt;/p&gt; &lt;p&gt;Some things I’m curious about: &lt;/p&gt; &lt;p&gt;Are you using it mainly for image-text matching, multimodal reasoning, or something else? &lt;/p&gt; &lt;p&gt;What hardware/setup seems to give the best performance without blowing the budget? &lt;/p&gt; &lt;p&gt;Any tricks for handling long sequences of images + text? &lt;/p&gt; &lt;p&gt;I’ve tried a few simple cases, but results feel very sensitive to input format and preprocessing. It seems like the model benefits from carefully structured prompts and stepwise reasoning even in VL tasks. &lt;/p&gt; &lt;p&gt;Would love to hear how others are approaching it - what’s been working, what’s tricky, and any workflow tips. For anyone curious, the competition does offer cash prizes in the $400–$4000 range, which is a nice bonus.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mak4560H"&gt; /u/Mak4560H &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nx5kuv/ernie45vl_anyone_testing_it_in_the_competition/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nx5kuv/ernie45vl_anyone_testing_it_in_the_competition/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nx5kuv/ernie45vl_anyone_testing_it_in_the_competition/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T17:30:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1nx1gro</id>
    <title>Thinking or Instruct for coding? [extreme GPU poor]</title>
    <updated>2025-10-03T14:58:35+00:00</updated>
    <author>
      <name>/u/Lost-Investigator731</name>
      <uri>https://old.reddit.com/user/Lost-Investigator731</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have 16GB system RAM + 6GB VRAM (RTX 3060 laptop) to run local LLMs [with MCP tools] and was wondering:&lt;/p&gt; &lt;p&gt;-&amp;gt; 30B A3B or a dense model with low quantization (no thinking to save tokens) [lesser context length]&lt;/p&gt; &lt;p&gt;-&amp;gt; 10B or lower (thinking) [higher context length]&lt;/p&gt; &lt;p&gt;Mostly using it for offline syntax correction (C, Fortran, Python and Go) and possible pseudo-code translation (short snippets) from one coding language to another. For more involved tasks, I would of course use Claude or Grok I guess.&lt;/p&gt; &lt;p&gt;Let me know what was your experience!? Was thinking of Qwen3-30B A3B instruct but I just wanted an overall perspective for the same.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lost-Investigator731"&gt; /u/Lost-Investigator731 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nx1gro/thinking_or_instruct_for_coding_extreme_gpu_poor/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nx1gro/thinking_or_instruct_for_coding_extreme_gpu_poor/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nx1gro/thinking_or_instruct_for_coding_extreme_gpu_poor/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T14:58:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1nx5w8m</id>
    <title>Local Open Deep Research with Offline Wikipedia Search Source</title>
    <updated>2025-10-03T17:42:35+00:00</updated>
    <author>
      <name>/u/dlarsen5</name>
      <uri>https://old.reddit.com/user/dlarsen5</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all,&lt;/p&gt; &lt;p&gt;Recently I've been trying out various deep research services for a personal project and found they all cost a lot. So I found LangGraph's &lt;a href="https://github.com/langchain-ai/open_deep_research"&gt;Open Deep Research&lt;/a&gt; when they released it back in August which reduced the total cost but it was still generating lots of web searches for information that was historical/general in nature, not needing to be live and up to date&lt;/p&gt; &lt;p&gt;Then I realized most of that information lives on Wikipedia and was pretty accurate, so I created my own branch of the deep research repo and added functionality to enable fully offline Wikipedia search to decrease the per-report cost even further&lt;/p&gt; &lt;p&gt;If anyone's interested in the high level architecture/dependencies used, &lt;a href="https://publish.obsidian.md/neutron/Published/Projects/10-03-25+Wikipedia+Deep+Research"&gt;here&lt;/a&gt; is a quick blog I made on it along with an example report output&lt;/p&gt; &lt;p&gt;Forgive me for not including a fully working branch to clone+run instantly but I don't feel like supporting all deployment architectures given that I'm using k8s services (to decouple memory usage of embeddings indices from the research container) and that the repo has no existing Dockerfile/deployment solution&lt;/p&gt; &lt;p&gt;I have included a code agent &lt;a href="https://publish.obsidian.md/neutron/Published/Projects/Resources/Wikipedia+Deep+Research+Prompt"&gt;prompt&lt;/a&gt; that was generated from the full code files in case anyone does want to use that to generate the files and adapt to their local container orchestrator&lt;/p&gt; &lt;p&gt;Feel free to PM with any questions&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dlarsen5"&gt; /u/dlarsen5 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nx5w8m/local_open_deep_research_with_offline_wikipedia/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nx5w8m/local_open_deep_research_with_offline_wikipedia/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nx5w8m/local_open_deep_research_with_offline_wikipedia/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T17:42:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwnlp8</id>
    <title>How has everyone been liking Granite 4?</title>
    <updated>2025-10-03T02:43:24+00:00</updated>
    <author>
      <name>/u/SpicyWangz</name>
      <uri>https://old.reddit.com/user/SpicyWangz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How does it compare to similar models for you?&lt;/p&gt; &lt;p&gt;So far I've been testing out the 7b model and it's been performing really well on my benchmarks for a model of that size. I think I've found a new go-to model for that class.&lt;/p&gt; &lt;p&gt;The output looks fairly plaintext without much formatting or markdown. I'd probably like to see a little more structure and variation from it, but I prefer plain to the table hell that I've gotten from gpt-oss-20b.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SpicyWangz"&gt; /u/SpicyWangz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwnlp8/how_has_everyone_been_liking_granite_4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwnlp8/how_has_everyone_been_liking_granite_4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwnlp8/how_has_everyone_been_liking_granite_4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T02:43:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwuslg</id>
    <title>Qwen2.5 VL for OCR</title>
    <updated>2025-10-03T09:52:37+00:00</updated>
    <author>
      <name>/u/Jastibute</name>
      <uri>https://old.reddit.com/user/Jastibute</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been living in the dark ages up until today. I've asked ChatGPT maybe 50 questions over the years but overall I've not used AI past this. But today I discovered Qwen for OCR which sounds very interesting to me because I've had the need to scan thousands of pages of various books for a number of years now and I think finally this is becoming a possibility cheaply. I was initially looking at Tesseract and I might yet go down this route because it means not needing to buy expensive hardware or paying cloud services and it might be good enough for my needs but I would like to entertain the idea of Qwen. I would like to self host it. The only problem is video cards. I can justify one new 16GB or maybe a 20GB video card but that's it. Don't want to go into video card farming. Once I finish scanning a dozen or so books, I don't see a need for AI for me for the foreseeable future. Will continue living in the dark ages unless another use case surfaces for me.&lt;/p&gt; &lt;p&gt;Q is: I don't care about speed. I don't know how AI works but if it needs to offload to RAM and move slowly, I don't care as long as the quality is the same and it gets there eventually. I've currently got an 8GB video card. Is this capable of running say Qwen3-VL albeit slowly or does this model have a minimum requirement? I'm taking about this in the context of OCR with good quality images.&lt;/p&gt; &lt;p&gt;I have 2.5 in the heading, but found that 3 is out already while typing this up and forgot to change the heading.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Jastibute"&gt; /u/Jastibute &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwuslg/qwen25_vl_for_ocr/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwuslg/qwen25_vl_for_ocr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwuslg/qwen25_vl_for_ocr/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T09:52:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1nx1jq5</id>
    <title>My GLaDOS local LLM found its front end UI pedestrian. I have real-time satellite tracking for 8600+ starlink satellites (my network), the ISS, a local RAG and persistent memory, camera access/image analysis functional. TTS and STT capable. Wikipedia tool calling.</title>
    <updated>2025-10-03T15:01:29+00:00</updated>
    <author>
      <name>/u/Fear_ltself</name>
      <uri>https://old.reddit.com/user/Fear_ltself</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nx1jq5/my_glados_local_llm_found_its_front_end_ui/"&gt; &lt;img alt="My GLaDOS local LLM found its front end UI pedestrian. I have real-time satellite tracking for 8600+ starlink satellites (my network), the ISS, a local RAG and persistent memory, camera access/image analysis functional. TTS and STT capable. Wikipedia tool calling." src="https://external-preview.redd.it/anc5NjJ5bm52d3NmMezezeVRJsE82oVjoZEwhtc7ecTk1nRTlWTddkbdSY-W.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=48a8d7a189e05c966f6a7f103cf60a9a092932d5" title="My GLaDOS local LLM found its front end UI pedestrian. I have real-time satellite tracking for 8600+ starlink satellites (my network), the ISS, a local RAG and persistent memory, camera access/image analysis functional. TTS and STT capable. Wikipedia tool calling." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It has 5 servers running on the backend to support the Text to Speech and Speech to Text functionality all the way through. It has persistent memory for a local RAG. I’m working on tweaking it a bit but it seemingly has a ton of context about itself based on the prompts I’ve provided. It correctly understands its own place as my local LLM but, and provides feedback in the from of a GLaDOS personality matrix. I’ve found this be a great blend of helpful and funny, it actually answers my questions “how hot is it?” But in a funny smart assy way like GLaDOS would &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fear_ltself"&gt; /u/Fear_ltself &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/85danivnvwsf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nx1jq5/my_glados_local_llm_found_its_front_end_ui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nx1jq5/my_glados_local_llm_found_its_front_end_ui/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T15:01:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwovv8</id>
    <title>Granite-4.0-H-Tiny vs. OLMoE: Rapid AI improvements</title>
    <updated>2025-10-03T03:49:33+00:00</updated>
    <author>
      <name>/u/edward-dev</name>
      <uri>https://old.reddit.com/user/edward-dev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwovv8/granite40htiny_vs_olmoe_rapid_ai_improvements/"&gt; &lt;img alt="Granite-4.0-H-Tiny vs. OLMoE: Rapid AI improvements" src="https://preview.redd.it/q7lat3zxjtsf1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=db27566f8b03ab0a4d8599a0ebfc454ee0ea0790" title="Granite-4.0-H-Tiny vs. OLMoE: Rapid AI improvements" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, just looking at some of the new model releases and wanted to share a quick comparison I made that really shows how fast things are moving in the world of open-source LLMs.&lt;/p&gt; &lt;p&gt;I've been tracking and comparing a couple of Mixture of Experts models that have a similar dense and active parameters, in this case a 7B total parameter count with 1B active parameters. With today's Granite release we can compare OLMoE, which came out in January, and the new Granite-4.0-H-Tiny model that just dropped today.&lt;/p&gt; &lt;p&gt;The side-by-side results are pretty wild for just a 10-month difference. The new Granite model is straight-up better on every single metric we can compare. It's not just a small improvement, either. We're talking huge jumps in areas like math, coding, and general knowledge.&lt;/p&gt; &lt;p&gt;Things are advancing really fast, just to give a little more perspective, the new Granite-4.0-H-Tiny has a similar MMLU score to Llama 2 70B that came out on January 2024 but the granite model can run at reasonable speeds even on a potato PC with CPU inference, I still remember the old days when people were happy that Llama 2 70B could run at 2tk/s on their machines.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/edward-dev"&gt; /u/edward-dev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/q7lat3zxjtsf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwovv8/granite40htiny_vs_olmoe_rapid_ai_improvements/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwovv8/granite40htiny_vs_olmoe_rapid_ai_improvements/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T03:49:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwr6sb</id>
    <title>How's granite 4 small 32B going for you?</title>
    <updated>2025-10-03T06:01:18+00:00</updated>
    <author>
      <name>/u/mr_zerolith</name>
      <uri>https://old.reddit.com/user/mr_zerolith</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I notice that it's almost twice as fast as my current favorite, SEED OSS 36B. 79 tokens/sec starting from a blank context, but this speed doesn't seem to degrade as you fill up the context. &lt;/p&gt; &lt;p&gt;Accuracy on some hard questions is a little challenging ( less smart than SEED OSS ) but it does good with clarifications.&lt;br /&gt; Output length is short and to the point, doesn't spam you with emojis, fancy formatting or tables ( i like this ) &lt;/p&gt; &lt;p&gt;Memory consumption is extremely low per K of context, I don't understand how i can jack the context up to 512k and run it on a 5090. Memory usage doesn't seem to climb as i fill up the context either.&lt;/p&gt; &lt;p&gt;First impressions are good. There may be something special here. Let me know what your experiences look like.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mr_zerolith"&gt; /u/mr_zerolith &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwr6sb/hows_granite_4_small_32b_going_for_you/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwr6sb/hows_granite_4_small_32b_going_for_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwr6sb/hows_granite_4_small_32b_going_for_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T06:01:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1nx15z4</id>
    <title>Fine-tuning a 7B model for vibe coding games and open sourcing everything along the way. Advice appreciated!</title>
    <updated>2025-10-03T14:47:04+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nx15z4/finetuning_a_7b_model_for_vibe_coding_games_and/"&gt; &lt;img alt="Fine-tuning a 7B model for vibe coding games and open sourcing everything along the way. Advice appreciated!" src="https://preview.redd.it/81unpxqmswsf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2421c197b9ee07e94dd0532eb911322654264fe4" title="Fine-tuning a 7B model for vibe coding games and open sourcing everything along the way. Advice appreciated!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Background: I am working on an open-source app that uses a local LLM for vibe coding retro-style arcade games on consumer-level laptops.&lt;/p&gt; &lt;p&gt;I tried a bunch of models in the 4-8B range and found they all have pretty low performance for this task (Qwen3-Coder-30b works great but needs too much RAM). I shared my initial experience in a recent post.&lt;/p&gt; &lt;p&gt;Now I am trying to fine-tune a model to improve performance. If this succeeds, I want to make the project a community reference design to help others get LLM apps working on laptops!&lt;/p&gt; &lt;p&gt;So far I have:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;MIT licensed dataset (154 game files, 30k+ LoC): &lt;a href="https://github.com/lemonade-sdk/playable-data"&gt;https://github.com/lemonade-sdk/playable-data&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Fine-tuned a couple of models on Together AI and MIT licensed those as well: &lt;a href="https://huggingface.co/playable"&gt;https://huggingface.co/playable&lt;/a&gt; &lt;ul&gt; &lt;li&gt;Results are interesting, but not nearly production-ready yet! See the attached image, where iat-02 made Pong with sideways paddles because I fine-tined on too much Breakout data.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;A detailed log of methodology and results is &lt;a href="https://github.com/lemonade-sdk/playable-data/blob/main/docs/togetherai.md"&gt;here&lt;/a&gt; if anyone is curious.&lt;/p&gt; &lt;p&gt;Questions I could use advice with:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;What is the easiest tooling for this kind of work?&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I'm using Together AI to make LORAs right now, but I'm unhappy with their queue times, model selection, and overall flexibility. Looking for something turnkey, and preferably cloud-based.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;How does my dataset look?&lt;/p&gt; &lt;ul&gt; &lt;li&gt;If my goal is to get a 7B model to oneshot a few basic arcade games (Snake, Pong, Space Invaders, Asteroids, Breakout) is the dataset big enough?&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Any advice about fine-tuning settings (LORA rank, etc.)?&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You can find my current settings in log linked above.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Huge thanks in advance to anyone who can give me some pointers!&lt;/p&gt; &lt;p&gt;edit: fixing markdown formatting&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/81unpxqmswsf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nx15z4/finetuning_a_7b_model_for_vibe_coding_games_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nx15z4/finetuning_a_7b_model_for_vibe_coding_games_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T14:47:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwxje9</id>
    <title>SDLM 32B/4B from OpenGVLab</title>
    <updated>2025-10-03T12:18:24+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;&lt;a href="https://huggingface.co/OpenGVLab/SDLM-32B-D4#introduction"&gt;&lt;/a&gt;&lt;/h1&gt; &lt;p&gt;&lt;a href="https://huggingface.co/OpenGVLab/SDLM-32B-D4"&gt;https://huggingface.co/OpenGVLab/SDLM-32B-D4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/OpenGVLab/SDLM-3B-D8"&gt;https://huggingface.co/OpenGVLab/SDLM-3B-D8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/OpenGVLab/SDLM-3B-D4"&gt;https://huggingface.co/OpenGVLab/SDLM-3B-D4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(Qwen 2.5 finetunes)&lt;/p&gt; &lt;h1&gt;Introduction&lt;/h1&gt; &lt;p&gt;We propose a &lt;strong&gt;S&lt;/strong&gt;equential &lt;strong&gt;D&lt;/strong&gt;iffusion &lt;strong&gt;L&lt;/strong&gt;anguage &lt;strong&gt;M&lt;/strong&gt;odel (&lt;strong&gt;SDLM&lt;/strong&gt;), to cheaply stimulate the parallel prediction capabilities of diffusion models. Specifically, SDLM reduces distribution shift by limiting the prediction range to a fixed block length and enforces decoding order through the longest prefix decoding method, thereby significantly improving prediction efficiency while ensuring generation quality. Our method can be viewed as a further generalization of the autoregressive (AR) paradigm. Therefore, it is possible to use pre-trained AR weights and quickly migrate to the diffusion framework with only minimal instruction fine-tuning.&lt;/p&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/OpenGVLab/SDLM-32B-D4#overall-concept"&gt;&lt;/a&gt;&lt;/h1&gt; &lt;h1&gt;Overall Concept&lt;/h1&gt; &lt;p&gt;SDLM delivers strong performance with significantly faster decoding speed. It operates approximately 2x faster than comparable autoregressive models while matching their accuracy, and achieves up to 5x speedup over other diffusion language models, as evidenced by results on the MATH-500 benchmark.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwxje9/sdlm_32b4b_from_opengvlab/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwxje9/sdlm_32b4b_from_opengvlab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwxje9/sdlm_32b4b_from_opengvlab/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T12:18:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1nx1ryu</id>
    <title>Local LLMs for TTS &amp; RAG in my game - a huge thank you to this community!</title>
    <updated>2025-10-03T15:10:03+00:00</updated>
    <author>
      <name>/u/orblabs</name>
      <uri>https://old.reddit.com/user/orblabs</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nx1ryu/local_llms_for_tts_rag_in_my_game_a_huge_thank/"&gt; &lt;img alt="Local LLMs for TTS &amp;amp; RAG in my game - a huge thank you to this community!" src="https://external-preview.redd.it/aGQ3bmJ3MTd3d3NmMV7kygU8pU1boY2_bUjRfQiVFgAFZeNf1RrlrJxGbDfE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b396e04745857ae3adb5dbdda5c992d2ab46ce6f" title="Local LLMs for TTS &amp;amp; RAG in my game - a huge thank you to this community!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;I wanted to share a quick video of something I'm really excited about and that this community was a huge inspiration for.&lt;/p&gt; &lt;p&gt;For those who haven't seen my project, Synthasia, it's a standalone interactive storytelling engine I'm building. The goal is to create dynamic, AI-powered narrative experiences, and a big part of that is making it accessible and customizable.&lt;/p&gt; &lt;p&gt;From the beginning, I knew I wanted to support local models, and lurking here has been a massive catalyst. Seeing the passion and the incredible progress everyone is making pushed me to double down on integrating local, multi-platform solutions.&lt;/p&gt; &lt;p&gt;The video shows our new Text-to-Speech system completely builtin into the &amp;quot;game&amp;quot; levaraging transformers.js and webgpu for multiplatform hardware accelerated local TTS ! (the actual TTS is Kokoro) . The dream is to have fully voiced, dynamic characters, and local TTS is making that a reality.&lt;/p&gt; &lt;p&gt;On top of that, we're using WebLLM (again, webgpu support for optimal performance) to generate embeddings for our RAG system, right on the user's machine. This was a fun challenge, partly because we use OpenRouter for a lot of the heavy lifting, but they don't offer an embeddings endpoint. This community gave me the confidence to build a solution that lets users run their own embedding models locally, which is a huge win for privacy and offline capability.&lt;/p&gt; &lt;p&gt;It feels like we're at a pivotal moment, almost like a renaissance of the old text-adventure spirit. We're standing on the shoulders of giants, taking those foundational ideas of interactive stories and exploring where we can go with the incredible power of modern LLMs. It's not about replacing the classics, but building on them to create entirely new kinds of experiences. Needless to say that not all game dev related communities are (absolutely understandably) particularly welcoming towards AI usage, here instead the project feels at home and the response to my past posts has been amazing and i am very grateful for it.&lt;/p&gt; &lt;p&gt;Anyway, I just wanted to share my progress and say a huge thank you. This is one of the most innovative and helpful communities on the internet, and it's been a huge motivator.&lt;/p&gt; &lt;p&gt;Cheers!&lt;/p&gt; &lt;p&gt;P.S. we have a discord server where a handful of users have begun testing the very early alpha builds of Synthasia, if you care to join to help, share feedback, have a chat or just give a look around, we would be very happy to have you : &lt;a href="https://discord.gg/2wc4n2GMmn"&gt;https://discord.gg/2wc4n2GMmn&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/orblabs"&gt; /u/orblabs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/8f8svv17wwsf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nx1ryu/local_llms_for_tts_rag_in_my_game_a_huge_thank/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nx1ryu/local_llms_for_tts_rag_in_my_game_a_huge_thank/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T15:10:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwkzq7</id>
    <title>Huawei Develop New LLM Quantization Method (SINQ) that's 30x Faster than AWQ and Beats Calibrated Methods Without Needing Any Calibration Data</title>
    <updated>2025-10-03T00:36:48+00:00</updated>
    <author>
      <name>/u/abdouhlili</name>
      <uri>https://old.reddit.com/user/abdouhlili</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwkzq7/huawei_develop_new_llm_quantization_method_sinq/"&gt; &lt;img alt="Huawei Develop New LLM Quantization Method (SINQ) that's 30x Faster than AWQ and Beats Calibrated Methods Without Needing Any Calibration Data" src="https://external-preview.redd.it/nBFUIJw0Ejvd09O6shC9aA8_DA1taNSIvE_cak2wtlo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ac886e4b9ca714a3746fa6d670ba959d7721d3a2" title="Huawei Develop New LLM Quantization Method (SINQ) that's 30x Faster than AWQ and Beats Calibrated Methods Without Needing Any Calibration Data" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdouhlili"&gt; /u/abdouhlili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/papers/2509.22944"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwkzq7/huawei_develop_new_llm_quantization_method_sinq/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwkzq7/huawei_develop_new_llm_quantization_method_sinq/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T00:36:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwwoab</id>
    <title>LoRA without regrets implemented in Hugging Face TRL [colab, and python scripts]</title>
    <updated>2025-10-03T11:36:54+00:00</updated>
    <author>
      <name>/u/Zealousideal-Cut590</name>
      <uri>https://old.reddit.com/user/Zealousideal-Cut590</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;LoRA Without Regret&lt;/h1&gt; &lt;blockquote&gt; &lt;p&gt;[!WARNING] I wrote this page for the TRL docs, but thought it's just drop it here in advance for anyone who can't wait. &lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;I also made a &lt;a href="https://colab.research.google.com/drive/1wd8qYg3qWwp81N6LAJmlXASdV5GRzWcn?usp=sharing"&gt;colab notebook&lt;/a&gt; of this guide.&lt;/p&gt; &lt;p&gt;Recent research from the team at &lt;a href="https://thinkingmachines.ai/blog/lora/"&gt;Thinking Machines Lab&lt;/a&gt; (Schulman et al., 2025) shows that &lt;strong&gt;LoRA can match full fine-tuning performance&lt;/strong&gt; when configured correctly, while using only ~67% of the compute. These findings are exciting to TRL users because they're straightforward to implement and can improve model performance on smaller budgets.&lt;/p&gt; &lt;p&gt;This guide provides simple instructions to reproduce the results of the blog post in TRL.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;[!TIP] It is recommended to read the blog post before following this guide, or to consult both resources in parallel for best results.&lt;/p&gt; &lt;/blockquote&gt; &lt;h2&gt;Benefits of LoRA over full fine-tuning&lt;/h2&gt; &lt;p&gt;First of all, let's remind ourselves of the benefits of &lt;a href="https://huggingface.co/docs/trl/en/peft_integration"&gt;LoRA over full fine-tuning&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;LoRA adds adapter layers on top of the base model, which contains significantly fewer parameters than the base model itself. This design reduces GPU memory requirements and enables more efficient training. As described in the &lt;a href="https://thinkingmachines.ai/blog/lora/"&gt;blog&lt;/a&gt;, this approach was originally thought to involve a performance trade-off, although careful configuration can overcome this trade-off and match full fine-tuning performance. &lt;/p&gt; &lt;h2&gt;Examples with TRL&lt;/h2&gt; &lt;p&gt;Let's implement and train LoRA adapters in TRL scripts based on the core findings of the blog post. Afterwards, we'll revisit each finding in light of the TRL results.&lt;/p&gt; &lt;h3&gt;Supervised Fine-Tuning (SFT)&lt;/h3&gt; &lt;p&gt;The blog post performs SFT on a range of models and datasets from the Hub, which we can reproduce in TRL.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;Dataset&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href="https://huggingface.co/meta-llama/Llama-3.2-1B"&gt;Llama-3.2-1B-Instruct&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href="https://huggingface.co/datasets/allenai/tulu-3-sft-mixture"&gt;allenai/tulu-3-sft-mixture&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href="https://huggingface.co/meta-llama/Llama-3.2-1B"&gt;Llama-3.2-1B-Instruct&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href="https://huggingface.co/datasets/open-thoughts/OpenThoughts-114k"&gt;open-thoughts/OpenThoughts-114k&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href="https://huggingface.co/meta-llama/Llama-3.1-8B"&gt;Llama-3.1-8B-Instruct&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href="https://huggingface.co/datasets/allenai/tulu-3-sft-mixture"&gt;allenai/tulu-3-sft-mixture&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href="https://huggingface.co/meta-llama/Llama-3.1-8B"&gt;Llama-3.1-8B-Instruct&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href="https://huggingface.co/datasets/open-thoughts/OpenThoughts-114k"&gt;open-thoughts/OpenThoughts-114k&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;```bash&lt;/p&gt; &lt;p&gt;uv run &amp;quot;&lt;a href="https://raw.githubusercontent.com/huggingface/trl/main/trl/scripts/sft.py"&gt;https://raw.githubusercontent.com/huggingface/trl/main/trl/scripts/sft.py&lt;/a&gt;&amp;quot; \ --model_name_or_path Qwen/Qwen2.5-3B-Instruct \ --dataset_name open-thoughts/OpenThoughts-114k \ --learning_rate 2.0e-5 \ --num_train_epochs 1 \ --packing \ --per_device_train_batch_size 2 \ --gradient_accumulation_steps 16 \ --gradient_checkpointing \ --eval_strategy no \ --use_peft \ --lora_r 256 \ --lora_alpha 16 \ --lora_target_modules all-linear \ --output_dir Qwen2.5-3B-OpenThoughts-LoRA \ --report_to trackio \ --push_to_hub&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;To run the script locally, you will need to have &lt;code&gt;uv&lt;/code&gt; installed. Check out the &lt;a href="https://docs.astral.sh/uv/"&gt;uv documentation&lt;/a&gt; for more details.&lt;/p&gt; &lt;p&gt;Once training starts, you can monitor the progress in &lt;a href="https://huggingface.co/trackio"&gt;Trackio&lt;/a&gt;, which will log the URL.&lt;/p&gt; &lt;h3&gt;Reinforcement Learning (GRPO)&lt;/h3&gt; &lt;p&gt;The blog post performs GRPO on a range of models and datasets from the Hub, and once again we can reproduce the results in TRL. &lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;Dataset&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href="https://huggingface.co/meta-llama/Llama-3.2-1B"&gt;Llama-3.1-8B-Base&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href="https://huggingface.co/datasets/openai/gsm8k"&gt;GSM8k&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href="https://huggingface.co/meta-llama/Llama-3.2-1B"&gt;Llama-3.1-8B-Base&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href="https://huggingface.co/datasets/zwhe99/DeepMath-103K"&gt;DeepMath-103K&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-8b-base"&gt;Qwen3-8b-base&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href="https://huggingface.co/datasets/zwhe99/DeepMath-103K"&gt;DeepMath-103K&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;For reinforcement learning, the blog uses a math reasoning task that we can reproduce as a Python function.&lt;/p&gt; &lt;p&gt;&amp;lt;details&amp;gt; &amp;lt;summary&amp;gt;Reward function&amp;lt;/summary&amp;gt;&lt;/p&gt; &lt;p&gt;```python def strip_reasoning_accuracy_reward( completions: list[list[dict[str, str]]], solution: list[str], **kwargs ) -&amp;gt; list[Optional[float]]: &amp;quot;&amp;quot;&amp;quot;Reward function that strips reasoning tags and checks mathematical accuracy.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;This function: 1. Extracts the content from completions 2. Removes &amp;lt;think&amp;gt;&amp;lt;/think&amp;gt; tags (for reasoning that shouldn't be evaluated) 3. Parses both the gold solution and the predicted answer 4. Uses math_verify to check if they are mathematically equivalent Args: completions: List of model completions, each containing a list of messages solution: List of ground truth solutions **kwargs: Additional arguments (ignored but required for trainer compatibility) Returns: List of rewards where: - 1.0 if the answer is correct - 0.0 if the answer is incorrect - None if the solution is not parseable (skips this example) &amp;quot;&amp;quot;&amp;quot; contents = [completion[0][&amp;quot;content&amp;quot;] for completion in completions] rewards = [] for content, sol in zip(contents, solution): # Strip reasoning tags from completion while &amp;quot;&amp;lt;think&amp;gt;&amp;quot; in content and &amp;quot;&amp;lt;/think&amp;gt;&amp;quot; in content: start = content.find(&amp;quot;&amp;lt;think&amp;gt;&amp;quot;) end = content.find(&amp;quot;&amp;lt;/think&amp;gt;&amp;quot;, start) if start != -1 and end != -1: content = content[:start] + content[end + len(&amp;quot;&amp;lt;/think&amp;gt;&amp;quot;) :] else: break # Parse gold solution gold_parsed = parse( f&amp;quot;${sol}$&amp;quot;, extraction_config=[ LatexExtractionConfig( boxed_match_priority=0, try_extract_without_anchor=True ) ], ) if len(gold_parsed) != 0: # We require the answer to be provided in correct latex (no malformed operators) answer_parsed = parse( content, extraction_config=[ LatexExtractionConfig( boxed_match_priority=0, normalization_config=NormalizationConfig( basic_latex=True, units=True, malformed_operators=False, nits=False, boxed=True, ), try_extract_without_anchor=False, ) ], extraction_mode=&amp;quot;first_match&amp;quot;, ) # Compute binary rewards if verifiable, `None` otherwise to skip this example try: reward = float(verify(gold_parsed, answer_parsed)) except Exception as e: print( f&amp;quot;verify failed: {e}, answer: {answer_parsed}, gold: {gold_parsed}&amp;quot; ) reward = None else: # If the gold solution is not parseable, we assign `None` to skip this example reward = None rewards.append(reward) return rewards &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;&amp;lt;/details&amp;gt;&lt;/p&gt; &lt;p&gt;```bash&lt;/p&gt; &lt;p&gt;uv run &amp;quot;&lt;a href="https://huggingface.co/datasets/burtenshaw/lora-without-regrets/resolve/main/grpo.py"&gt;https://huggingface.co/datasets/burtenshaw/lora-without-regrets/resolve/main/grpo.py&lt;/a&gt;&amp;quot; \ --model_name_or_path Qwen/Qwen3-0.6B \ --dataset_name HuggingFaceH4/OpenR1-Math-220k-default-verified \ --output_dir grpo-full-qwen3-0.6b \ --learning_rate 1.0e-6 \ --lr_scheduler_type cosine \ --warmup_ratio 0.0 \ --max_grad_norm 1.0 \ --beta 0.0 \ --max_prompt_length 1024 \ --max_completion_length 4096 \ --num_generations 16 \ --generation_batch_size 16 \ --gradient_accumulation_steps 8 \ --per_device_train_batch_size 1 \ --num_train_epochs 1 \ --lora_r 1 \ --lora_alpha 32 \ --lora_dropout 0.0 \ --lora_target_modules all-linear \ --vllm_mode colocate \ --save_strategy steps \ --save_steps 50 \ --save_total_limit 1 \ --logging_steps 1 \ --max_steps 200 \ --report_to trackio ```&lt;/p&gt; &lt;p&gt;The reinforcement learning script with GRPO is implemented as a custom script in TRL, which uses the reward function shown above. You can review it at &lt;a href="https://huggingface.co/datasets/burtenshaw/lora-without-regrets/blob/main/grpo.py"&gt;&lt;code&gt;grpo.py&lt;/code&gt;&lt;/a&gt; - Reinforcement learning with LoRA best practices&lt;/p&gt; &lt;h2&gt;Key findings in optimizing LoRA&lt;/h2&gt; &lt;p&gt;The authors recommend applying LoRA to all weight matrices rather than limiting it to attention layers, as increasing the rank does not compensate for this restriction. In TRL, this can be configured using &lt;code&gt;--lora_target_modules all-linear&lt;/code&gt; to apply LoRA to all weight matrices.&lt;/p&gt; &lt;p&gt;We were able to reproduce the results of the blog post using TRL and the SmolLM3 model. We trained the model for 500 steps on the &lt;a href="https://huggingface.co/datasets/HuggingFaceH4/OpenR1-Math-220k-default-verified"&gt;Math 220k dataset&lt;/a&gt; with the reward function and configuration above. As you can see in the figure below, the LoRA model's average train reward curve matches the full fine-tuning curve.&lt;/p&gt; &lt;p&gt;![train reward](&lt;a href="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/lora_without_regret/5.png"&gt;https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/lora_without_regret/5.png&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;And most importantly, the LoRA model uses significantly less memory than the full fine-tuning model, as we can see in the figure below.&lt;/p&gt; &lt;p&gt;![memory usage](&lt;a href="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/lora_without_regret/6.png"&gt;https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/lora_without_regret/6.png&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;Here are the parameters we used to train the above models&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Parameter&lt;/th&gt; &lt;th&gt;LoRA&lt;/th&gt; &lt;th&gt;Full FT&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;code&gt;--model_name_or_path&lt;/code&gt;&lt;/td&gt; &lt;td&gt;HuggingFaceTB/SmolLM3-3B&lt;/td&gt; &lt;td&gt;HuggingFaceTB/SmolLM3-3B&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;code&gt;--dataset_name&lt;/code&gt;&lt;/td&gt; &lt;td&gt;HuggingFaceH4/OpenR1-Math-220k-default-verified&lt;/td&gt; &lt;td&gt;HuggingFaceH4/OpenR1-Math-220k-default-verified&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;code&gt;--learning_rate&lt;/code&gt;&lt;/td&gt; &lt;td&gt;1.0e-6&lt;/td&gt; &lt;td&gt;1.0e-5&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;code&gt;--max_prompt_length&lt;/code&gt;&lt;/td&gt; &lt;td&gt;1024&lt;/td&gt; &lt;td&gt;1024&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;code&gt;--max_completion_length&lt;/code&gt;&lt;/td&gt; &lt;td&gt;4096&lt;/td&gt; &lt;td&gt;4096&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;code&gt;--lora_r&lt;/code&gt;&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;-&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;code&gt;--lora_alpha&lt;/code&gt;&lt;/td&gt; &lt;td&gt;32&lt;/td&gt; &lt;td&gt;-&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;code&gt;--lora_dropout&lt;/code&gt;&lt;/td&gt; &lt;td&gt;0.0&lt;/td&gt; &lt;td&gt;-&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;code&gt;--lora_target_modules&lt;/code&gt;&lt;/td&gt; &lt;td&gt;all-linear&lt;/td&gt; &lt;td&gt;-&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Let's break down the key findings of the blog post and how we were able to reproduce them.&lt;/p&gt; &lt;h3&gt;1. &lt;em&gt;LoRA performs better when applied to all weight matrices&lt;/em&gt;&lt;/h3&gt; &lt;p&gt;The authors recommend applying LoRA to all weight matrices rather than limiting it to attention layers, as increasing the rank does not compensate for this restriction. &lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/lora_without_regret/1.png"&gt;https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/lora_without_regret/1.png&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Attention-only LoRA underperforms even when using a higher rank to match parameter count. In TRL, this can be configured using &lt;code&gt;--lora_target_modules all-linear&lt;/code&gt; to apply LoRA to all weight matrices. In Python, we can do this like so:&lt;/p&gt; &lt;p&gt;```python from peft import LoraConfig &lt;/p&gt; &lt;p&gt;peft_config = LoraConfig(target_modules=&amp;quot;all-linear&amp;quot;)&lt;br /&gt; ```&lt;/p&gt; &lt;h3&gt;2. &lt;em&gt;The adapter needs sufficient capacity to learn from the dataset&lt;/em&gt;&lt;/h3&gt; &lt;p&gt;The blog post recommends using a sufficient LoRA rank to learn from the dataset. The rank determines the number of trainable parameters in the LoRA adapter. Therefore, &amp;quot;For datasets that exceed LoRA capacity, LoRA underperforms FullFT&amp;quot;. &lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/lora_without_regret/3.png"&gt;https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/lora_without_regret/3.png&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In the TRL script, we could use &lt;code&gt;--lora_r&lt;/code&gt; to set the rank and adapt it based on the task and dataset we're training on. The blog post recommends the following ranks based on the task and dataset size:&lt;/p&gt; &lt;p&gt;Reinforcement learning tasks typically require lower capacity, so smaller LoRA ranks can be used. This is because policy gradient algorithms extract roughly ~1 bit of information per episode, demanding minimal parameter capacity. &lt;/p&gt; &lt;p&gt;The blog post defines the ideal dataset size for LoRA to match full fine-tuning as &amp;quot;Post-training scale&amp;quot;. Which we can use to determine the recommended rank for SFT and RL LoRAs as:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Task Type&lt;/th&gt; &lt;th&gt;Dataset Size&lt;/th&gt; &lt;th&gt;Recommended Rank&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;SFT&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Post-training scale&lt;/td&gt; &lt;td&gt;256&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;RL&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Any size&lt;/td&gt; &lt;td&gt;1-32&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h3&gt;3. &lt;em&gt;&amp;quot;FullFT and high-rank LoRAs have similar learning curves&amp;quot;&lt;/em&gt;&lt;/h3&gt; &lt;p&gt;Counterintuitively, the blog post recommends using similar learning rates to full fine-tuning. In the TRL script, we could use &lt;code&gt;--learning_rate&lt;/code&gt; to set the learning rate. The \( \frac{1}{r} \) scaling in LoRA makes the optimal learning rate approximately rank-independent.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/lora_without_regret/2.png"&gt;https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/lora_without_regret/2.png&lt;/a&gt;&lt;/p&gt; &lt;h3&gt;4. &lt;em&gt;&amp;quot;In some scenarios, LoRA is less tolerant of large batch sizes than full fine-tuning.&amp;quot;&lt;/em&gt;&lt;/h3&gt; &lt;p&gt;The blog post recommends using an effective batch size &amp;lt; 32 because the authors found LoRA to be less tolerant of large batch sizes. This could not be mitigated by increasing the LoRA rank. In the TRL script, we could use &lt;code&gt;--per_device_train_batch_size&lt;/code&gt; and &lt;code&gt;--gradient_accumulation_steps&lt;/code&gt; to set the batch size.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/lora_without_regret/4.png"&gt;https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/lora_without_regret/4.png&lt;/a&gt;&lt;/p&gt; &lt;h2&gt;Takeaways&lt;/h2&gt; &lt;p&gt;Using TRL, you can efficiently implement LoRA adapters to match full fine-tuning performance, applying the core insights (targeting all weight matrices, choosing the right rank, and managing batch size and learning rate) without the heavy compute cost of FullFT.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Cut590"&gt; /u/Zealousideal-Cut590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwwoab/lora_without_regrets_implemented_in_hugging_face/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwwoab/lora_without_regrets_implemented_in_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwwoab/lora_without_regrets_implemented_in_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T11:36:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwzs0k</id>
    <title>My key takeaways on Qwen3-Next's four pillar innovations, highlighting its Hybrid Attention design</title>
    <updated>2025-10-03T13:52:48+00:00</updated>
    <author>
      <name>/u/MarketingNetMind</name>
      <uri>https://old.reddit.com/user/MarketingNetMind</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwzs0k/my_key_takeaways_on_qwen3nexts_four_pillar/"&gt; &lt;img alt="My key takeaways on Qwen3-Next's four pillar innovations, highlighting its Hybrid Attention design" src="https://b.thumbs.redditmedia.com/reqB6_2tFvlBMx5ATJ7pudXAXRmECxNHdJAhqyBibvs.jpg" title="My key takeaways on Qwen3-Next's four pillar innovations, highlighting its Hybrid Attention design" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After reviewing and testing, Qwen3-Next, especially its Hybrid Attention design, might be one of the most significant efficiency breakthroughs in open-source LLMs this year.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;It Outperforms Qwen3-32B with 10% training cost and 10x throughput for long contexts&lt;/strong&gt;. Here's the breakdown:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Four Pillars&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Hybrid Architecture:&lt;/strong&gt; Combines Gated DeltaNet + Full Attention to context efficiency&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Unltra Sparsity:&lt;/strong&gt; 80B parameters, only 3B active per token&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Stability Optimizations:&lt;/strong&gt; Zero-Centered RMSNorm + normalized MoE router&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multi-Token Prediction:&lt;/strong&gt; Higher acceptance rates in speculative decoding&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;One thing to note&lt;/strong&gt; is that the model tends toward verbose responses. You'll want to use structured prompting techniques or frameworks for output control.&lt;/p&gt; &lt;p&gt;See &lt;a href="https://blog.netmind.ai/article/We_Tested_Qwen3-Next%3A_Hybrid_Attention_for_Efficiency_Revolution_in_Open-Source_LLMs_(New_Research_Breakdown"&gt;here&lt;/a&gt;) for full technical breakdown with architecture diagrams.Has anyone deployed Qwen3-Next in production? Would love to hear about performance in different use cases.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MarketingNetMind"&gt; /u/MarketingNetMind &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nwzs0k"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwzs0k/my_key_takeaways_on_qwen3nexts_four_pillar/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwzs0k/my_key_takeaways_on_qwen3nexts_four_pillar/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T13:52:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwukd5</id>
    <title>Granite4 -1M context window, and no one even noticed?</title>
    <updated>2025-10-03T09:38:28+00:00</updated>
    <author>
      <name>/u/Western_Courage_6563</name>
      <uri>https://old.reddit.com/user/Western_Courage_6563</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How is it, when IBM drops a model, no one notice?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Western_Courage_6563"&gt; /u/Western_Courage_6563 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwukd5/granite4_1m_context_window_and_no_one_even_noticed/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwukd5/granite4_1m_context_window_and_no_one_even_noticed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwukd5/granite4_1m_context_window_and_no_one_even_noticed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T09:38:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwzq6p</id>
    <title>GLM-4.6 now on artificial analysis</title>
    <updated>2025-10-03T13:50:50+00:00</updated>
    <author>
      <name>/u/Professional-Bear857</name>
      <uri>https://old.reddit.com/user/Professional-Bear857</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://artificialanalysis.ai/models/glm-4-6-reasoning"&gt;https://artificialanalysis.ai/models/glm-4-6-reasoning&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Tldr, it benchmarks slightly worse than Qwen 235b 2507. In my use I have found it to also perform worse than the Qwen model, glm 4.5 also didn't benchmark well so it might just be the benchmarks. Although it looks to be slightly better with agent / tool use.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Professional-Bear857"&gt; /u/Professional-Bear857 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwzq6p/glm46_now_on_artificial_analysis/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwzq6p/glm46_now_on_artificial_analysis/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwzq6p/glm46_now_on_artificial_analysis/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T13:50:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwv4q0</id>
    <title>A list of models released or udpated last week on this sub, in case you missed any (3rd Oct)</title>
    <updated>2025-10-03T10:12:59+00:00</updated>
    <author>
      <name>/u/aifeed-fyi</name>
      <uri>https://old.reddit.com/user/aifeed-fyi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We had an interesting week in releases this week (Open &amp;amp; Closed).&lt;/p&gt; &lt;p&gt;Here is the weekly list of models, I found discussed on LocalLlama &lt;em&gt;this week.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Please update or let me know in the comments if there are any mistakes or misses. Good Friday!&lt;/p&gt; &lt;h1&gt;Model Releases &amp;amp; Updates&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Description&lt;/th&gt; &lt;th align="left"&gt;Reddit&lt;/th&gt; &lt;th align="left"&gt;HF / GH&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nu75l3/glm_46_is_out_and_its_going_against_claude_45/"&gt;GLM-4.6&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;LLM 200k ctx&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nu75l3/glm_46_is_out_and_its_going_against_claude_45/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/zai-org/GLM-4.6"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nte4j1/deepseekaideepseekv32exp_and/"&gt;DeepSeek-V3.2-Exp&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;LLM exp/base&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nte4j1/deepseekaideepseekv32exp_and/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nw2wd6/granite_40_language_models_a_ibmgranite_collection"&gt;Granite 4.0&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;IBM LLM collection&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nw2wd6/granite_40_language_models_a_ibmgranite_collection"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/collections/ibm-granite/granite-quantized-models-67f944eddd16ff8e057f115c"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nwibsb/ming_v2_is_out/"&gt;Ming V2&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Multimodal collection&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nwibsb/ming_v2_is_out/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/collections/inclusionAI/ming-v2-68ddea4954413c128d706630"&gt;HF Collection&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/gallery/1nvltym"&gt;LFM2-Audio-1.5&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Audio&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/gallery/1nvltym"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/LiquidAI/LFM2-Audio-1.5B"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.liquid.ai/blog/introducing-liquid-nanos-frontier-grade-performance-on-everyday-devices"&gt;LiquidAI&lt;/a&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nuyjp9/liquidai_bet_on_small_but_mighty_model/"&gt; &lt;/a&gt;nanos&lt;/td&gt; &lt;td align="left"&gt;Small task LLM&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nuyjp9/liquidai_bet_on_small_but_mighty_model/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/collections/LiquidAI/liquid-nanos-68b98d898414dd94d4d5f99a"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nt2l57/qwen3_omni_awq_released/"&gt;Qwen3 Omni AWQ&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;30B 4bit AWQ&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nt2l57/qwen3_omni_awq_released/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/cpatonn/Qwen3-Omni-30B-A3B-Instruct-AWQ-4bit"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nu44n4/1t_open_source_reasoning_model_with_50b_activation/"&gt;Ring-1T-preview&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;1T reasoning 50B Active&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nu44n4/1t_open_source_reasoning_model_with_50b_activation/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/inclusionAI/Ring-1T-preview"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nw8jbn/ring_flash_20_104b_a6b_with_linear_attention"&gt;RingFlash linea r 2&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;LLM 104B MOE&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nw8jbn/ring_flash_20_104b_a6b_with_linear_attention"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/inclusionAI/Ring-flash-linear-2.0"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nu1rul/lingmini20_finally_almost_here_lets_push_context/"&gt;Ling-mini-2.0&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;16B LLM&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nu1rul/lingmini20_finally_almost_here_lets_push_context/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/inclusionAI/Ling-mini-2.0-GGUF"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nrvo9g/finally_internvl3_5_flash_versions_coming/"&gt;InternVL3_5 Flash&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Vision-language&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nrvo9g/finally_internvl3_5_flash_versions_coming/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/OpenGVLab/InternVL3_5-8B-Flash"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nrhr13/k2think_32b_reasoning_model_from_uae/"&gt;K2-Think 32B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;32B reasoning&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nrhr13/k2think_32b_reasoning_model_from_uae/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/LLM360/K2-Think-32B"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nv5uw8/dont_sleep_on_apriel1515bthinker_and_snowpiercer/"&gt;Apriel-1.5-15b-Thinker&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;15B multimodal&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nv5uw8/dont_sleep_on_apriel1515bthinker_and_snowpiercer/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/ServiceNow-AI/Apriel-1.5-15b-Thinker"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nuxdd4/release_finally_a_working_8bit_quantized/"&gt;VibeVoice 1.8.0 (8-bit)&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;8-bit speech&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nuxdd4/release_finally_a_working_8bit_quantized/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/FabioSarracino/VibeVoice-Large-Q8"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;N&lt;a href="https://huggingface.co/neuphonic/neutts-air"&gt;eutts-air&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;TTS model&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nw60fj/open_source_speech_foundation_model_that_runs/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/neuphonic/neutts-air"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;🧰 Resources &amp;amp; Tools&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Name&lt;/th&gt; &lt;th align="left"&gt;Type&lt;/th&gt; &lt;th align="left"&gt;Reddit&lt;/th&gt; &lt;th align="left"&gt;Link&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://v.redd.it/vklzqk9bipsf1"&gt;Onyx&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Open-source Chat UI&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://v.redd.it/vklzqk9bipsf1"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;–&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ntiua9/we_just_opensourced_kroko_asr_a_fast_streaming/"&gt;Kroko ASR&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Speech recognition&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ntiua9/we_just_opensourced_kroko_asr_a_fast_streaming/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://kroko.ai"&gt;kroko.ai&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nu3slg/an_opensource_omni_chatbot_for_long_speech_and/"&gt;MGM-Omni&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Omni chatbot&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nu3slg/an_opensource_omni_chatbot_for_long_speech_and/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/dvlab-research/MGM-Omni"&gt;GitHub&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nrrgoy/monkesearch_technical_report_out_now/"&gt;monkeSearch Report&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Research/benchmark&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nrrgoy/monkesearch_technical_report_out_now/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://monkesearch.github.io/"&gt;monkesearch.github.io&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aifeed-fyi"&gt; /u/aifeed-fyi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwv4q0/a_list_of_models_released_or_udpated_last_week_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwv4q0/a_list_of_models_released_or_udpated_last_week_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwv4q0/a_list_of_models_released_or_udpated_last_week_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T10:12:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwxxya</id>
    <title>Bought a used 5090 only to find out it was tampered with</title>
    <updated>2025-10-03T12:36:48+00:00</updated>
    <author>
      <name>/u/a201905</name>
      <uri>https://old.reddit.com/user/a201905</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just a angry/disappointment/frustration post from someone who was very excited at the opportunity to upgrade from 3080 to a 5090 at a discount to run local LLM.&lt;/p&gt; &lt;p&gt;A MSI rtx 5090 came up at my local, trustworthy auction house and I won it for around $2k. It was a stretch on my budget but it was too good of an opportunity so I jumped on it. I was extremely excited and upgraded the PSU but when I tried to put everything together, the system would not boot. I tried everything for hours until I remembered reading the article about people stealing GPU cores. &lt;/p&gt; &lt;p&gt;So I looked at the back and noticed the warranty tamper sticker was voided. i looked back at the auction site and I can see the image they posted with the screw tampered. I was blinded by the potential happiness this was going to bring me and I just didn't pay attention.&lt;/p&gt; &lt;p&gt;What a disappointment. Why do people do this garbage to others. I hope karma bites you in the ass. &lt;/p&gt; &lt;p&gt;Edit: I should have been clearer, i opened it and it's missing the core. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/a201905"&gt; /u/a201905 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwxxya/bought_a_used_5090_only_to_find_out_it_was/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwxxya/bought_a_used_5090_only_to_find_out_it_was/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwxxya/bought_a_used_5090_only_to_find_out_it_was/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T12:36:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1nx1ot4</id>
    <title>Qwen3-VL-30B-A3B-Instruct &amp; Thinking (Now Hidden)</title>
    <updated>2025-10-03T15:06:48+00:00</updated>
    <author>
      <name>/u/TKGaming_11</name>
      <uri>https://old.reddit.com/user/TKGaming_11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nx1ot4/qwen3vl30ba3binstruct_thinking_now_hidden/"&gt; &lt;img alt="Qwen3-VL-30B-A3B-Instruct &amp;amp; Thinking (Now Hidden)" src="https://a.thumbs.redditmedia.com/iNETafBex6Qpbyi8P087geXMh_aBmkILehL6E7qn-m4.jpg" title="Qwen3-VL-30B-A3B-Instruct &amp;amp; Thinking (Now Hidden)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TKGaming_11"&gt; /u/TKGaming_11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nx1ot4"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nx1ot4/qwen3vl30ba3binstruct_thinking_now_hidden/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nx1ot4/qwen3vl30ba3binstruct_thinking_now_hidden/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T15:06:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1nx18ax</id>
    <title>GLM 4.6 IS A FUKING AMAZING MODEL AND NOBODY CAN TELL ME OTHERWISE</title>
    <updated>2025-10-03T14:49:34+00:00</updated>
    <author>
      <name>/u/boneMechBoy69420</name>
      <uri>https://old.reddit.com/user/boneMechBoy69420</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Especially fuckin artificial analysis and their bullshit ass benchmark &lt;/p&gt; &lt;p&gt;Been using GLM 4.5 it on prod for a month now and I've got nothing but good feedback from the users , it's got way better autonomy than any other proprietary model I've tried (sonnet , gpt 5 and grok code) and it's probably the best ever model for tool call accuracy &lt;/p&gt; &lt;p&gt;One benchmark id recommend yall follow is the berkley function calling benchmark (v4 ig) &lt;a href="https://gorilla.cs.berkeley.edu/leaderboard.html"&gt;bfcl v4&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/boneMechBoy69420"&gt; /u/boneMechBoy69420 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nx18ax/glm_46_is_a_fuking_amazing_model_and_nobody_can/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nx18ax/glm_46_is_a_fuking_amazing_model_and_nobody_can/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nx18ax/glm_46_is_a_fuking_amazing_model_and_nobody_can/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T14:49:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwx1rx</id>
    <title>The most important AI paper of the decade. No debate</title>
    <updated>2025-10-03T11:55:32+00:00</updated>
    <author>
      <name>/u/PumpkinNarrow6339</name>
      <uri>https://old.reddit.com/user/PumpkinNarrow6339</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwx1rx/the_most_important_ai_paper_of_the_decade_no/"&gt; &lt;img alt="The most important AI paper of the decade. No debate" src="https://preview.redd.it/d2rcvb6nyvsf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0051a67e9886e507e2b0a35679f4d469050fda91" title="The most important AI paper of the decade. No debate" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PumpkinNarrow6339"&gt; /u/PumpkinNarrow6339 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/d2rcvb6nyvsf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwx1rx/the_most_important_ai_paper_of_the_decade_no/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwx1rx/the_most_important_ai_paper_of_the_decade_no/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T11:55:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvryo4</id>
    <title>AMA Announcement: Prime Intellect — The Open‑Source Distributed Training Lab (Thu, Oct 2 • 10 AM – 1 PM PDT)</title>
    <updated>2025-10-02T02:31:01+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"&gt; &lt;img alt="AMA Announcement: Prime Intellect — The Open‑Source Distributed Training Lab (Thu, Oct 2 • 10 AM – 1 PM PDT)" src="https://preview.redd.it/222kj50x0msf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c1c747c42690f74b8d08690dcdbb1ef23aece13b" title="AMA Announcement: Prime Intellect — The Open‑Source Distributed Training Lab (Thu, Oct 2 • 10 AM – 1 PM PDT)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/222kj50x0msf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T02:31:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwaoyd</id>
    <title>AMA with Prime Intellect — Ask Us Anything!</title>
    <updated>2025-10-02T17:47:44+00:00</updated>
    <author>
      <name>/u/kindacognizant</name>
      <uri>https://old.reddit.com/user/kindacognizant</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;AMA with Prime Intellect — Ask Us Anything!&lt;/h1&gt; &lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;! We’re excited for this AMA, thank you for having us.&lt;/p&gt; &lt;p&gt;I’m Kalomaze (&lt;a href="/u/kindacognizant"&gt;u/kindacognizant&lt;/a&gt;), a researcher at Prime Intellect, the lab behind:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Distributed training &lt;a href="https://www.primeintellect.ai/#research"&gt;efforts&lt;/a&gt; including INTELLECT-1 + INTELLECT-2&lt;/li&gt; &lt;li&gt;Open-source RL efforts including &lt;a href="https://github.com/PrimeIntellect-ai/verifiers"&gt;verifiers&lt;/a&gt;, &lt;a href="https://github.com/PrimeIntellect-ai/prime-rl"&gt;prime-rl&lt;/a&gt;, and the &lt;a href="https://app.primeintellect.ai/dashboard/environments"&gt;Environments Hub&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Our other participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Sami Jaghouar, &lt;a href="/u/samsja19"&gt;u/samsja19&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Will Brown, &lt;a href="/u/willccbb"&gt;u/willccbb&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Jack Min Ong, &lt;a href="/u/Cinamic"&gt;u/Cinamic&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Mika Senghaas, &lt;a href="/u/mikasenghaas"&gt;u/mikasenghaas&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 11:00 AM – 2:00 PM PST, with the Prime Intellect team continuing to follow up on questions over the next 48 hours.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kindacognizant"&gt; /u/kindacognizant &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwaoyd/ama_with_prime_intellect_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwaoyd/ama_with_prime_intellect_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwaoyd/ama_with_prime_intellect_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T17:47:44+00:00</published>
  </entry>
</feed>
