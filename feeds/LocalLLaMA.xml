<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-23T19:09:49+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1rcalyu</id>
    <title>MiniMax 2.5 on DGX SPARK system.</title>
    <updated>2026-02-23T07:03:34+00:00</updated>
    <author>
      <name>/u/DOOMISHERE</name>
      <uri>https://old.reddit.com/user/DOOMISHERE</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;so i've been working with minimax 2.5 (MiniMax-M2.5-UD-Q3_K_XL),&lt;br /&gt; im amazed by this model, the quality of code is just on another level. &lt;/p&gt; &lt;p&gt;my issue is that i can only work with it in maximum 65K context (bigger than that - crashes on load - out of memory) , normal usage lands on 125GB RAM usage (which is too much).&lt;br /&gt; so i decided to try MiniMax-M2.5-UD-Q2_K_XL, which runs fine with context of 192K,&lt;br /&gt; but i wonder whats the difference between the two models when it comes to coding ?&lt;br /&gt; anyone ever run coding benchmark on both of Q2 and Q3 ?&lt;br /&gt; i didnt find any info online...&lt;br /&gt; im sure Q3 is better, but by how much ? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DOOMISHERE"&gt; /u/DOOMISHERE &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcalyu/minimax_25_on_dgx_spark_system/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcalyu/minimax_25_on_dgx_spark_system/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rcalyu/minimax_25_on_dgx_spark_system/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T07:03:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcq3p1</id>
    <title>Let's talk hardware</title>
    <updated>2026-02-23T18:48:45+00:00</updated>
    <author>
      <name>/u/skmagiik</name>
      <uri>https://old.reddit.com/user/skmagiik</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to run a local model for inference to do coding tasks and security review for personal programming projects.&lt;br /&gt; Is getting something like the ASUS Ascent G10X going to be a better spend per $ than building another rig with a 5090? The costs to build a full rig for that would be 2x the G10X, but I don't see much discussion about these &amp;quot;standalone personal AI computers&amp;quot; and I can't tell if it's because people aren't using them or because they aren't a viable option.&lt;/p&gt; &lt;p&gt;Ideally I would like to setup opencode or something similar to do some agentic tasks for me to interact with my tools and physical hardware for debugging (I do this now with claude code and codex)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/skmagiik"&gt; /u/skmagiik &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcq3p1/lets_talk_hardware/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcq3p1/lets_talk_hardware/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rcq3p1/lets_talk_hardware/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T18:48:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1rc00nj</id>
    <title>In the long run, everything will be local</title>
    <updated>2026-02-22T22:39:00+00:00</updated>
    <author>
      <name>/u/tiguidoio</name>
      <uri>https://old.reddit.com/user/tiguidoio</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rc00nj/in_the_long_run_everything_will_be_local/"&gt; &lt;img alt="In the long run, everything will be local" src="https://preview.redd.it/vqzxm46ri4lg1.png?width=140&amp;amp;height=105&amp;amp;auto=webp&amp;amp;s=f9899bff14b8d1409da4cbfaa0a56aa74bb136e5" title="In the long run, everything will be local" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been of the opinion for a while that, long term, we‚Äôll have smart enough open models and powerful enough consumer hardware to run &lt;em&gt;all&lt;/em&gt; our assistants locally both chatbots and coding copilots&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/vqzxm46ri4lg1.png?width=3608&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=22c0fb257d744350f8668301a915aeec2b6653fc"&gt;https://preview.redd.it/vqzxm46ri4lg1.png?width=3608&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=22c0fb257d744350f8668301a915aeec2b6653fc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Right now it still feels like there‚Äôs a trade-off:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Closed, cloud models = best raw quality, but vendor lock-in, privacy concerns, latency, per-token cost&lt;/li&gt; &lt;li&gt;Open, local models = worse peak performance, but full control, no recurring API fees, and real privacy&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;But if you look at the curve on both sides, it‚Äôs hard not to see them converging:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Open models keep getting smaller, better, and more efficient every few months (quantization, distillation, better architectures). Many 7B‚Äì8B models are already good enough for daily use if you care more about privacy/control than squeezing out the last 5% of quality&lt;/li&gt; &lt;li&gt;Consumer and prosumer hardware keeps getting cheaper and more powerful, especially GPUs and Apple Silicon‚Äìclass chips. People are already running decent local LLMs with 12‚Äì16GB VRAM or optimized CPU-only setups for chat and light coding&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;At some point, the default might flip: instead of why would you run this locally?, the real question becomes why would you ship your entire prompt and codebase to a third-party API if you don‚Äôt strictly need to? For a lot of use cases (personal coding, offline agents, sensitive internal tools), a strong local open model plus a specialized smaller model might be more than enough&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tiguidoio"&gt; /u/tiguidoio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rc00nj/in_the_long_run_everything_will_be_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rc00nj/in_the_long_run_everything_will_be_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rc00nj/in_the_long_run_everything_will_be_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T22:39:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1rc1ra2</id>
    <title>My real-world Qwen3-code-next local coding test. So, Is it the next big thing?</title>
    <updated>2026-02-22T23:51:14+00:00</updated>
    <author>
      <name>/u/FPham</name>
      <uri>https://old.reddit.com/user/FPham</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rc1ra2/my_realworld_qwen3codenext_local_coding_test_so/"&gt; &lt;img alt="My real-world Qwen3-code-next local coding test. So, Is it the next big thing?" src="https://preview.redd.it/44qd636p15lg1.png?width=140&amp;amp;height=14&amp;amp;auto=webp&amp;amp;s=35817da51dcc5387e5bc0d9209c8558f639ab5f3" title="My real-world Qwen3-code-next local coding test. So, Is it the next big thing?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So yesterday I put the Q8 MLX on my 128GB Mac Studio Ultra and wired it to Qwen Code CLI. Fit's there with a huge amount to spare. The first tests were promising - basically did everything I asked: read file, write file, browse web, check system time....blah, blah.&lt;/p&gt; &lt;p&gt;Now the real the task:&lt;/p&gt; &lt;p&gt;I decided on YOLO mode to rewrite the KittenTTS-IOS to windows (which itself is a rewrite of KittenTTS in python). It uses ONYX and a couple of Swift libraries like Misaki for English phoneme.&lt;/p&gt; &lt;p&gt;So, say a medium difficulty. Not super easy, but not super hard, because all the code is basically there. You just need to shake it.&lt;/p&gt; &lt;p&gt;Here is how it went:&lt;/p&gt; &lt;p&gt;Started very well. Plan was solid. Make simple CLI with KittenTTS model, avoid any phoneme manipulation for now. Make ONYX work. Then add Misaki phoneme, avoid bart fallback coz that's a can of worms.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;So it built the main.cpp. Rewrote the main app, created it's own json parser for the KittenTTS dictionary. found windows ONYX, downloaded, linked. ran cmake captured the output, realised it's json parsing was a total crap. Linked &amp;lt;nlohmann/json.hpp&amp;gt; .... aaaaand we are out.&lt;/li&gt; &lt;li&gt;First client timeout then &amp;quot;I'm dead, Dave&amp;quot;. As we get more and more into longer context the prompt parsing gets longer and longer until the client times out.&lt;/li&gt; &lt;li&gt;Restarted maually, told it we are at json.hpp, it finished the patching, compiled - created output.wav&lt;/li&gt; &lt;li&gt;I'm impressed so far. The wav has voice in it, of course all gibberish because we have no phoneme dictionary. The make file is unreadable can of worms.&lt;/li&gt; &lt;li&gt;Next step convert phoneme Misaki to windows. Big hairy project. Again, started cheerful. But we are now editing large files. It can barely finish anything before timeout.&lt;/li&gt; &lt;li&gt;Lot's of manual restarts. (YOLO mode my butt, right?). At some point it starts editing the Swift files, thinking that's what we are doing. Noooo!!!!&lt;/li&gt; &lt;li&gt;I've noticed that most of the time it wastes tokens on trying to figure out how to do stuff like save file it wants to save, because now &amp;quot;it's just too big&amp;quot;. Even starts writing python script to save the file then entering the entire text of lexicon.cpp as a command line - LOL, learning, that's a very stupid thing too.&lt;/li&gt; &lt;li&gt;I mean nice to learn from mistakes, but we are getting to timeouts all the time now by filling the context with unnecessary work. And it of course learns nothing, because that knowledge is lost.&lt;/li&gt; &lt;li&gt;I spent another 60 minutes trying to figure out how to fix qwen code by increasing timeout. Not an easy task as every AI will just hallucinate what you should do. I moved from anthropic style to openai style for the QWEN3 and set generationConfig.timeout to a big number (I have no idea if this even works). Set the KV_cache to quantize at 8 bit in LM studio (again, no idea if it helps). Seems the timeouts are now longer? So maybe a small win?&lt;/li&gt; &lt;li&gt;Well, went to sleep, letting it do something.&lt;/li&gt; &lt;li&gt;In the next day the phoneme test.exe was working sort of (at least it was not throwing 5 pages of errors) - read the 400k phoneme dictionary and output bunch of nonsense, like lookup: Hello -&amp;gt; h‚ïî√ñlO (Is this the correct phoneme? Hardly. Seems we are getting lost in ISO/UDF nightmare) Well, Qwen doesn't know what's going on either.&lt;/li&gt; &lt;li&gt;At this point neither me nor Qwen knows if we are fixing bugs or buggyfying working code. But he is happily doing something.&lt;/li&gt; &lt;li&gt;And writing jokes that get a bit stale after while: &amp;quot;Why do Java developers wear glasses? Because they don't C#&amp;quot;&lt;/li&gt; &lt;li&gt;I start to miss Claude Code. Or Codex. Or anything that doesn't take 30 minutes per turn then tell me client timeout.&lt;/li&gt; &lt;li&gt;It is still fixing it and writing stupid one liner jokes on screen. I mean &amp;quot;fixing it&amp;quot; means sitting in Prompt processing.&lt;/li&gt; &lt;li&gt;Funny, MAC Studio is barely warm. Like it was working nonstop for 8 hours with 89GB model .&lt;/li&gt; &lt;li&gt;The processing prompt is still killing the whole operation. As the context grows, this is a few minutes per turn.&lt;/li&gt; &lt;li&gt;I totally believe the X grifters telling me they bough 10 MAC's for local Agentic work.... yes, sure. You can have huge memory but large context is still going to be snail pace.&lt;/li&gt; &lt;li&gt;19. Looking at the terminal &amp;quot;Just a sec, I'm optimizing the humor... (esc to cancel, 29m 36s)&amp;quot;, been doing something for 30 min. Looking at mac log, generating token, now at around 60k tokens and still going up - a really long output that we will probably never be able to do anything with.&lt;/li&gt; &lt;li&gt;I give Local model coding 5/10 so far. It does kinda work if you have the enormous patience. It's surprising we get that far. It is nowhere what the big boys give you, even for $20/month.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;--- It is still coding --- (definitely now in some Qwen3 loop)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/44qd636p15lg1.png?width=599&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c6af08a0a84011baa5dc72985d73634bbe04a35f"&gt;https://preview.redd.it/44qd636p15lg1.png?width=599&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c6af08a0a84011baa5dc72985d73634bbe04a35f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;: Whee! We finished, about 24 hours after I started. Now, of course I wasn't babysitting it so IDK how much time it sat idle during the day. Anytime I went by I'd check on it, or restart the process...&lt;/p&gt; &lt;p&gt;The whole thing had to restart or run probably 20-30 times again and again on the same thing for various reasons (timeout or infinite loops).&lt;/p&gt; &lt;p&gt;But, the good thing is: &lt;strong&gt;The project compiles and creates a WAV file with very understandable pronunciation all on just CPU that doesn't sound robotic.&lt;/strong&gt; So that's 100% success. No coding input from my side, no code fixing. No dependencies.&lt;/p&gt; &lt;p&gt;It isn't pleasant to work with it in this capacity I tried (MAC Studio with forever prompt processing) but beggars cannot be choosers and Qwen3-coder-next is a &lt;strong&gt;FREE&lt;/strong&gt; model. So yay, they (Qwen) need to be commanded for their effort. It's amazing how fast we got there, and I remember that.&lt;/p&gt; &lt;p&gt;I'm bumping the result to 6/10 for a local coding experience which is: &lt;strong&gt;good&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Final observations and what I learned:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- It's free, good enough, and runs on a home hardware which back in 2023 would be called &amp;quot;insane&amp;quot;&lt;/p&gt; &lt;p&gt;- it can probably work better with small editing/bug fixes/ small additions. The moment it needs to write large code it will be full of issues (if it finishes). It literally didn't wrote a single usable code at once (unlike what I used to see in cc or codex), though it was able to fix all the hundreds issues by itself (testing, assessing, fixing). The process itself took a lot of time.&lt;/p&gt; &lt;p&gt;- it didn't really have problem with tool calling, at least not what I observed. It had problem with tool using, especially when it started producing a lot of code.&lt;/p&gt; &lt;p&gt;- it is NOT a replacement for claude/codex/gemini/other cloud. It just isn't. Maybe as a hobby. It's the difference between a bicycle and a car. You will get there eventually, but it would take much longer and be less pleasant. Well it depends how much you value your time vs money, I guess.&lt;/p&gt; &lt;p&gt;- MAC with unified memory is amazing, for a basic general LLM, but working with code and long context it kills any enjoyment - and that is not dependent on the size of the memory. When the grifters on X saying they are buying 512GB MAC studios for local agentic coding etc - it's BS. It's still a torture - because we have much faster and less painful way using cloud API (and cheaper too). It's pain with 80GB 8 bit quantized model, it would be excruciating with full 250GB model.&lt;/p&gt; &lt;p&gt;- I'm not going to lie to you, I'm not going to use it much, unless I terribly ran out of tokens on cc or codex. I'd check other Chinese big online models that are much cheaper like GLM 5, but honestly the price alone is not deterrent. I firmly believe they (codex, cc) are giving it practically for free. &lt;/p&gt; &lt;p&gt;- I might check other models like step 3.5 (I have it downloaded but didn't use it for anything yet)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FPham"&gt; /u/FPham &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rc1ra2/my_realworld_qwen3codenext_local_coding_test_so/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rc1ra2/my_realworld_qwen3codenext_local_coding_test_so/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rc1ra2/my_realworld_qwen3codenext_local_coding_test_so/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T23:51:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1rch66j</id>
    <title>TeichAI's "Nemotron-Orchestrator" models are misleading ‚Äî they're just Qwen3-8B distilled on frontier traces, not routing models</title>
    <updated>2026-02-23T13:17:37+00:00</updated>
    <author>
      <name>/u/Honest-Debate-6863</name>
      <uri>https://old.reddit.com/user/Honest-Debate-6863</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Saw these models pop up on HuggingFace and figured I'd dig in since the name is catchy:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/TeichAI/Nemotron-Orchestrator-8B-Claude-4.5-Opus-Distill/blob/main/README.md"&gt;TeichAI/Nemotron-Orchestrator-8B-Claude-4.5-Opus-Distill&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/TeichAI/Nemotron-Orchestrator-8B-DeepSeek-v3.2-Speciale-Distill/tree/main"&gt;TeichAI/Nemotron-Orchestrator-8B-DeepSeek-v3.2-Speciale-Distill-GGUF&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What NVIDIA's actual Nemotron-Orchestrator-8B does:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;NVIDIA's model is a &lt;em&gt;pure router&lt;/em&gt; trained with reinforcement learning to act as a supervisor over a fleet of specialist models - a search model, a reasoning model, a math model, an answer model. It never generates the final answer itself. Its system prompt is literally &lt;code&gt;&amp;quot;You are good at using tools.&amp;quot;&lt;/code&gt; It's useless without the full ToolOrchestra ensemble running behind it.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What TeichAI's models actually are:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Look at the model card:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;textBase Model: unsloth/Qwen3-8B-unsloth-bnb-4bit Dataset: TeichAI/claude-4.5-opus-high-reasoning-250x &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;That's it. It's Qwen3-8B SFT'd on Claude Opus 4.5 reasoning traces using Unsloth + TRL. Standalone general reasoning assistant. No routing, no tool delegation, no specialist ensemble.&lt;/p&gt; &lt;p&gt;Nothing wrong with that as a model - distillation from frontier models onto small open weights is a legitimate and useful technique. But calling it &amp;quot;Nemotron-Orchestrator&amp;quot; is pure name-jacking to ride branding. It has nothing architecturally or functionally in common with the actual Orchestrator-8B.&lt;/p&gt; &lt;p&gt;Can someone from the TeichAi team clarify this?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; If you downloaded these expecting routing/orchestration behavior, you got a general reasoning fine-tune. If you want the actual ToolOrchestra system, you need NVIDIA's model &lt;em&gt;plus&lt;/em&gt; a full ensemble of specialist backends - the orchestrator alone does nothing.&lt;/p&gt; &lt;p&gt;If you see it is actually a better model &amp;amp; performant without the harness, please comment and inform us all! Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Honest-Debate-6863"&gt; /u/Honest-Debate-6863 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rch66j/teichais_nemotronorchestrator_models_are/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rch66j/teichais_nemotronorchestrator_models_are/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rch66j/teichais_nemotronorchestrator_models_are/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T13:17:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1rck7n4</id>
    <title>Best local llm for grammar tasks?</title>
    <updated>2026-02-23T15:19:25+00:00</updated>
    <author>
      <name>/u/darkblitzrc</name>
      <uri>https://old.reddit.com/user/darkblitzrc</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys! &lt;/p&gt; &lt;p&gt;I want to create a figma plugin that uses AI to help us proofread design assets and pieces for our work. Would go with openai 5.2 but work is very strict regarding data ingestion by 3rd party providers. Also I would have to feed or use my work brand guidelines documents as source of truth for the plugin.&lt;/p&gt; &lt;p&gt;The language I want to work is Spanish which is notorious for its many rules and practices.&lt;/p&gt; &lt;p&gt;Any recommendations for this project?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/darkblitzrc"&gt; /u/darkblitzrc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rck7n4/best_local_llm_for_grammar_tasks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rck7n4/best_local_llm_for_grammar_tasks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rck7n4/best_local_llm_for_grammar_tasks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T15:19:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1rci3l9</id>
    <title>personal entropy reduction with agents</title>
    <updated>2026-02-23T13:56:43+00:00</updated>
    <author>
      <name>/u/escept1co</name>
      <uri>https://old.reddit.com/user/escept1co</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rci3l9/personal_entropy_reduction_with_agents/"&gt; &lt;img alt="personal entropy reduction with agents" src="https://external-preview.redd.it/Z2g1Z25ndHkwOWxnMXMsGTH1aguTrI-pU1cryBxoqt80__0hno6cPg7cZUT3.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ab22ba4020a28bc9db3581812aab09f4271aaa29" title="personal entropy reduction with agents" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;during my unemployment stage of life i'm working on a personal assistant&lt;br /&gt; the problem it solves is pretty straightforward ‚Äì i have an adhd and it's hard to me to work with many different information streams (email, obsidian, calendar, local graph memory, browser history) + i forget things. the motivation was to improve my experience in context engineering, work on memory and in the end simplify my life. it's under active development and implementation itself is pretty sketchy, but it's already helping me&lt;/p&gt; &lt;p&gt;nb: despite these openclaws vibecoded stuff, i'm pretty critical about how the agentic framework should work. there's no full autonomy, all the stuff happening on user's initiative&lt;br /&gt; (but i still use some semi-automatic features like &amp;quot;daily email review&amp;quot;). mutable tools are highly controlled as well, so no &amp;quot;damn this thing just deleted all my emails&amp;quot; situations.&lt;/p&gt; &lt;p&gt;regarding local models ‚Äì i really want RL some small local model for at least explore subagents in the near future.&lt;/p&gt; &lt;p&gt;here's writeup if you want to get any implementation and motivation details:&lt;br /&gt; &lt;a href="https://timganiev.com/log/ntrp"&gt;https://timganiev.com/log/ntrp&lt;/a&gt; ‚Äì post in my blog&lt;br /&gt; &lt;a href="https://x.com/postimortem/article/2025725045851533464"&gt;https://x.com/postimortem/article/2025725045851533464&lt;/a&gt; ‚Äì X articles&lt;/p&gt; &lt;p&gt;and the code: &lt;a href="https://github.com/esceptico/ntrp"&gt;https://github.com/esceptico/ntrp&lt;/a&gt; (stars are appreciated!)&lt;/p&gt; &lt;p&gt;would be happy to answer any questions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/escept1co"&gt; /u/escept1co &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/m9wa92sy09lg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rci3l9/personal_entropy_reduction_with_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rci3l9/personal_entropy_reduction_with_agents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T13:56:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbkeea</id>
    <title>Which one are you waiting for more: 9B or 35B?</title>
    <updated>2026-02-22T12:15:48+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbkeea/which_one_are_you_waiting_for_more_9b_or_35b/"&gt; &lt;img alt="Which one are you waiting for more: 9B or 35B?" src="https://preview.redd.it/jyvany3jf1lg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f667e97854acf566b7f6d1d56e9c09e17f5a8ee8" title="Which one are you waiting for more: 9B or 35B?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jyvany3jf1lg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbkeea/which_one_are_you_waiting_for_more_9b_or_35b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbkeea/which_one_are_you_waiting_for_more_9b_or_35b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T12:15:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1rccsjg</id>
    <title>I made an interactive timeline of 171 LLMs (2017‚Äì2026)</title>
    <updated>2026-02-23T09:18:16+00:00</updated>
    <author>
      <name>/u/asymortenson</name>
      <uri>https://old.reddit.com/user/asymortenson</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Built a visual timeline tracking every major Large Language Model ‚Äî from the original Transformer paper to GPT-5.3 Codex.&lt;/p&gt; &lt;p&gt;171 models, 54 organizations. Filterable by open/closed source, searchable, with milestones highlighted.&lt;/p&gt; &lt;p&gt;Some stats from the data: - 2024‚Äì2025 was the explosion: 108 models in two years - Open source reached parity with closed in 2025 (29 vs 28) - Chinese labs account for ~20% of all major releases (10 orgs, 32 models)&lt;/p&gt; &lt;p&gt;&lt;a href="https://llm-timeline.com"&gt;https://llm-timeline.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Missing a model? Let me know and I'll add it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/asymortenson"&gt; /u/asymortenson &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rccsjg/i_made_an_interactive_timeline_of_171_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rccsjg/i_made_an_interactive_timeline_of_171_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rccsjg/i_made_an_interactive_timeline_of_171_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T09:18:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1rckqpp</id>
    <title>Hardware requirements for training a ~3B Model From Scratch locally?</title>
    <updated>2026-02-23T15:39:08+00:00</updated>
    <author>
      <name>/u/Any-Cobbler6161</name>
      <uri>https://old.reddit.com/user/Any-Cobbler6161</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all,&lt;/p&gt; &lt;p&gt;I‚Äôm a data science master‚Äôs student who‚Äôs posted on here a couple times before over the last year or 2. Now am working on my senior thesis and I‚Äôm trying to figure out the feasibility of training a ~3B parameter transformer model from scratch. So not fine-tuning. I‚Äôm trying to figure out what‚Äôs realistically doable on a home setup within ~6 months. My school is unfortunately is a very small public school and doesn‚Äôt have their own cluster or anything like that. Prior to this I was at a bigger school that did so I was just planning on booking time using theirs but unfortunately last year I had to transfer because I got really sick as they didn‚Äôt make accommodations for folks with medical disability. &lt;/p&gt; &lt;p&gt;Anyways I was thinking about training something in the ball park of 3B Params, 2k context, 25/50b training tokens, in fp16, probably using AdamW. My current system I have designed based on some napkin math is 2x 3090s over nvlink as I already have a Z690 motherboard that supports x8/x8 bifurcation, 1200W PSU, and 64gb of DDR5 RAM. Prior to this I had a rtx 5090 but even though it was crazy fast the 32gb was not enough to hold all the weights, grads, buffers, optimizer states (AdamW), etc. &lt;/p&gt; &lt;p&gt;Just wanted to hop on here and see if anyone here actually trained a 3B model or slightly smaller from scratch at home and if so what GPUs did you use/how did you do it? If you‚Äôve done anything remotely similar (even 1B‚Äì2B scale), I‚Äôd love to hear your setup and how it went.&lt;/p&gt; &lt;p&gt;Appreciate any real-world data points , thanks üôè&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any-Cobbler6161"&gt; /u/Any-Cobbler6161 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rckqpp/hardware_requirements_for_training_a_3b_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rckqpp/hardware_requirements_for_training_a_3b_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rckqpp/hardware_requirements_for_training_a_3b_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T15:39:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1rc97qf</id>
    <title>üåä Wave Field LLM O(n log n) Successfully Scales to 1B Parameters</title>
    <updated>2026-02-23T05:44:29+00:00</updated>
    <author>
      <name>/u/Murky-Sign37</name>
      <uri>https://old.reddit.com/user/Murky-Sign37</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rc97qf/wave_field_llm_on_log_n_successfully_scales_to_1b/"&gt; &lt;img alt="üåä Wave Field LLM O(n log n) Successfully Scales to 1B Parameters" src="https://preview.redd.it/6m7q2vzlm6lg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2ede585956ec96d0434754c49701c58176ad83ad" title="üåä Wave Field LLM O(n log n) Successfully Scales to 1B Parameters" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just completed full pretraining of &lt;strong&gt;Wave Field LLM (v4) at 1B scale&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Training Summary:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Parameters:&lt;/strong&gt; 825M&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Total Tokens:&lt;/strong&gt; 1.33B&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Final PPL:&lt;/strong&gt; 72.2&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Best PPL:&lt;/strong&gt; 72.2&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Final Accuracy:&lt;/strong&gt; 27.1%&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Training Time:&lt;/strong&gt; 13.2 hours&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This isn‚Äôt a small 30M or 124M experiment anymore.&lt;/p&gt; &lt;p&gt;Wave Field is now:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;‚úÖ Stable at near-billion scale&lt;/li&gt; &lt;li&gt;‚úÖ Training cleanly&lt;/li&gt; &lt;li&gt;‚úÖ Converging properly&lt;/li&gt; &lt;li&gt;‚úÖ Saving best checkpoints&lt;/li&gt; &lt;li&gt;‚úÖ Handling &amp;gt;1B tokens&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The key takeaway:&lt;/p&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;This validates that Wave Field‚Äôs field-based interaction mechanism is not just an experimental curiosity ‚Äî it holds up under real model size and real token volume &lt;a href="https://github.com/badaramoni/wave-field-llm"&gt;git&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Murky-Sign37"&gt; /u/Murky-Sign37 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6m7q2vzlm6lg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rc97qf/wave_field_llm_on_log_n_successfully_scales_to_1b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rc97qf/wave_field_llm_on_log_n_successfully_scales_to_1b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T05:44:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1rckcww</id>
    <title>Benchmarked 4 AI Memory Systems on 600-Turn Conversations - Here Are the Results</title>
    <updated>2026-02-23T15:25:00+00:00</updated>
    <author>
      <name>/u/singh_taranjeet</name>
      <uri>https://old.reddit.com/user/singh_taranjeet</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We just completed comprehensive benchmarks comparing memory layers for production AI agents. Tested Mem0 against OpenAI Memory, LangMem, and MemGPT across 10 multi-session conversations with 200 questions each.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key findings:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Mem0&lt;/strong&gt;: 66.9% accuracy, 1.4s p95 latency, ~2K tokens per query&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Mem0 Graph&lt;/strong&gt;: 68.5% accuracy, 2.6s p95 latency, ~4K tokens (superior temporal reasoning)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;OpenAI Memory&lt;/strong&gt;: 52.9% accuracy, 0.9s p95 latency, ~5K tokens&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LangMem&lt;/strong&gt;: 58.1% accuracy, 60s p95 latency, ~130 tokens&lt;/li&gt; &lt;li&gt;&lt;strong&gt;MemGPT&lt;/strong&gt;: Results in appendix&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What stands out:&lt;/strong&gt; Mem0 achieved 14 percentage points higher accuracy than OpenAI Memory while maintaining sub-2s response times. The graph variant excels at temporal queries (58.1% vs OpenAI's 21.7%) and multi-hop reasoning.&lt;/p&gt; &lt;p&gt;LangMem's 60-second latency makes it unusable for interactive applications, despite being open source.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Methodology:&lt;/strong&gt; Used LOCOMO dataset with GPT-4o-mini at temperature 0. Evaluated factual consistency, multi-hop reasoning, temporal understanding, and open-domain recall across 26K+ token conversations.&lt;/p&gt; &lt;p&gt;This matters because production agents need memory that persists beyond context windows while maintaining chat-level responsiveness. Current approaches either sacrifice accuracy for speed or become too slow for real-time use.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/singh_taranjeet"&gt; /u/singh_taranjeet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rckcww/benchmarked_4_ai_memory_systems_on_600turn/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rckcww/benchmarked_4_ai_memory_systems_on_600turn/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rckcww/benchmarked_4_ai_memory_systems_on_600turn/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T15:25:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1rc3naj</id>
    <title>Super New to Godot, used Claude Code/gpt-oss-120b locally to help me vibecode a simple platformer game about a grumpy mage who follows you around making fun of you lmao.</title>
    <updated>2026-02-23T01:13:04+00:00</updated>
    <author>
      <name>/u/swagonflyyyy</name>
      <uri>https://old.reddit.com/user/swagonflyyyy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rc3naj/super_new_to_godot_used_claude_codegptoss120b/"&gt; &lt;img alt="Super New to Godot, used Claude Code/gpt-oss-120b locally to help me vibecode a simple platformer game about a grumpy mage who follows you around making fun of you lmao." src="https://external-preview.redd.it/MmJ6MGRjNjA4NWxnMR3Al36Nr886FX7jQ_P96fNg8PSf4Zsku92kjG2XN_qv.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b8910573e373960eea6962553218ddcd88a9324c" title="Super New to Godot, used Claude Code/gpt-oss-120b locally to help me vibecode a simple platformer game about a grumpy mage who follows you around making fun of you lmao." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Yeah, I was bored so I spent the last two weeks experimenting with vibecoding with local LLMs, namely gpt-oss-120b.&lt;/p&gt; &lt;p&gt;I started with Cline, didn't like it at all because it was overheating my GPU while giving back too little. Codex was even worse, locally, leading to weird CPU switches mid-generation when there was supposed to be enough VRAM to run the model entirely on GPU. Then I tried Claude Code and that's when my expectations were exceeded, &lt;em&gt;big time.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;I first started with pygame, and after successfully one-shotting simple games (snake game, etc.) under the same project with the same model I decided to take it another level and use Claude Code with Godot, which was pretty easy to setup in VSCode and their IDE/extension. &lt;/p&gt; &lt;p&gt;Next thing I know, I spend the last two weeks making this game on Godot out of curiosity and using Claude Code to help me Vibecode parts of it along the way, and I came up with this game where you have a useful, snarky NPC that makes fun of you lmao.&lt;/p&gt; &lt;p&gt;The way it works is that the game is going to be gathering contextual information in real-time, e.g. actions taken, events occurring, etc. You can see that in the logs that are printed under the gameplay loop. &lt;/p&gt; &lt;p&gt;The mage then stores each chain of events in a chat history and comments on it every 10 seconds. The AI behavior is hard-coded but it works really well. However, I do plan on adding a hybrid approach where the LLM uses tool calls to make informed decisions depending on the situations, such as:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Switching equipment&lt;/li&gt; &lt;li&gt;Healing the player or himself&lt;/li&gt; &lt;li&gt;Pointing out objects of interest&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;And so forth. I haven't ruled out a Wizard of Oz worldbuilding AI that vibecodes enemies and obstacles throughout the game with tool calls, but that will be for another time.&lt;/p&gt; &lt;p&gt;I'm enjoying this process so I think I might actually finish this game, but we'll see how far I can get. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/swagonflyyyy"&gt; /u/swagonflyyyy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/jl31wp5085lg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rc3naj/super_new_to_godot_used_claude_codegptoss120b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rc3naj/super_new_to_godot_used_claude_codegptoss120b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T01:13:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1rco9v7</id>
    <title>RWKV-7: O(1) memory inference, 16.39 tok/s on ARM Cortex-A76, beats LLaMA 3.2 3B. The local-first architecture nobody is talking about...</title>
    <updated>2026-02-23T17:45:31+00:00</updated>
    <author>
      <name>/u/Sensitive-Two9732</name>
      <uri>https://old.reddit.com/user/Sensitive-Two9732</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Wrote a deep-dive specifically because the deployment numbers don't get enough attention.&lt;/p&gt; &lt;p&gt;The headline stats for local inference:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;O(1) memory per token, no KV cache at all. Context length does not affect VRAM usage.&lt;/li&gt; &lt;li&gt;16.39 tok/s on ARM Cortex-A76 (7B model). That's a mid-range Android chip.&lt;/li&gt; &lt;li&gt;28.7 tok/s on Snapdragon X Elite (7B). Current-gen Windows on ARM.&lt;/li&gt; &lt;li&gt;RWKV-X hybrid: 1.37x faster than Flash Attention v3 at 128K context.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Microsoft already ships Eagle v5 (RWKV-based) on ~1.5 billion Windows machines for on-device tasks. No cloud round-trip.&lt;/p&gt; &lt;p&gt;The compression stack: 4-bit quantized RWKV-7 0.1B runs on microcontrollers. The state size is fixed regardless of how long the conversation runs. For local-first deployment this is a fundamentally different proposition than fitting a Transformer's growing KV cache into limited VRAM.&lt;/p&gt; &lt;p&gt;Weights (Apache 2.0): &lt;a href="https://huggingface.co/collections/RWKV/rwkv-v7-67d43835efa225006183fece"&gt;https://huggingface.co/collections/RWKV/rwkv-v7-67d43835efa225006183fece&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to discuss about this. :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sensitive-Two9732"&gt; /u/Sensitive-Two9732 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://medium.com/ai-advances/rwkv-7-beats-llama-3-2-rnn-constant-memory-46064bbf1f64"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rco9v7/rwkv7_o1_memory_inference_1639_toks_on_arm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rco9v7/rwkv7_o1_memory_inference_1639_toks_on_arm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T17:45:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1rci9h1</id>
    <title>TinyTeapot (77 million params): Context-grounded LLM running ~40 tok/s on CPU (open-source)</title>
    <updated>2026-02-23T14:03:12+00:00</updated>
    <author>
      <name>/u/zakerytclarke</name>
      <uri>https://old.reddit.com/user/zakerytclarke</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rci9h1/tinyteapot_77_million_params_contextgrounded_llm/"&gt; &lt;img alt="TinyTeapot (77 million params): Context-grounded LLM running ~40 tok/s on CPU (open-source)" src="https://external-preview.redd.it/JxyR2-HPTrb177zTD0smUzhI5l6xLW7EKVY2pYpkHxc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bf1debba24ef624d732718952f5f5127580705f6" title="TinyTeapot (77 million params): Context-grounded LLM running ~40 tok/s on CPU (open-source)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zakerytclarke"&gt; /u/zakerytclarke &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/teapotai/tinyteapot"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rci9h1/tinyteapot_77_million_params_contextgrounded_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rci9h1/tinyteapot_77_million_params_contextgrounded_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T14:03:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcnv9h</id>
    <title>GLM-5 is the new top open-weights model on the Extended NYT Connections benchmark, with a score of 81.8, edging out Kimi K2.5 Thinking (78.3)</title>
    <updated>2026-02-23T17:31:02+00:00</updated>
    <author>
      <name>/u/zero0_one1</name>
      <uri>https://old.reddit.com/user/zero0_one1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcnv9h/glm5_is_the_new_top_openweights_model_on_the/"&gt; &lt;img alt="GLM-5 is the new top open-weights model on the Extended NYT Connections benchmark, with a score of 81.8, edging out Kimi K2.5 Thinking (78.3)" src="https://preview.redd.it/t89mf46o4alg1.png?width=140&amp;amp;height=89&amp;amp;auto=webp&amp;amp;s=6d3e242b7e37ab99694926c9cefb58fae2a90e45" title="GLM-5 is the new top open-weights model on the Extended NYT Connections benchmark, with a score of 81.8, edging out Kimi K2.5 Thinking (78.3)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;More info: &lt;a href="https://github.com/lechmazur/nyt-connections/"&gt;https://github.com/lechmazur/nyt-connections/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zero0_one1"&gt; /u/zero0_one1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1rcnv9h"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcnv9h/glm5_is_the_new_top_openweights_model_on_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rcnv9h/glm5_is_the_new_top_openweights_model_on_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T17:31:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1rc6c8m</id>
    <title>Feels like magic. A local gpt-oss 20B is capable of agentic work</title>
    <updated>2026-02-23T03:18:16+00:00</updated>
    <author>
      <name>/u/Vaddieg</name>
      <uri>https://old.reddit.com/user/Vaddieg</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rc6c8m/feels_like_magic_a_local_gptoss_20b_is_capable_of/"&gt; &lt;img alt="Feels like magic. A local gpt-oss 20B is capable of agentic work" src="https://preview.redd.it/b27xdhewq5lg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f9692be692d82dd176bce38aa1cffe88af9406be" title="Feels like magic. A local gpt-oss 20B is capable of agentic work" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I gave a try to &lt;a href="https://github.com/zeroclaw-labs/zeroclaw"&gt;zeroclaw&lt;/a&gt; agent (intstead of the bloated and overhyped one). After few hours of fuckery with configs it's finally useful. Both main and embeddings models are running locally.&lt;br /&gt; I carefully read what it's trying to execute in shell, and permit only [relatively] safe tools in config.&lt;br /&gt; So far it can interact with macOS apps, web pages, and local files while keeping all my data private.&lt;br /&gt; gpt-oss 20B has its limits though, it loses focus after 15-20 steps and often needs direct instructions to use persistent memory. It also starts behaving weirdly if tool access has been denied or tool returned some error.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vaddieg"&gt; /u/Vaddieg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/b27xdhewq5lg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rc6c8m/feels_like_magic_a_local_gptoss_20b_is_capable_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rc6c8m/feels_like_magic_a_local_gptoss_20b_is_capable_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T03:18:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcc2fa</id>
    <title>An open-source framework to achieve Gemini 3 Deep Think / GPT-5.2 Pro level performance with local models scaffolding</title>
    <updated>2026-02-23T08:33:22+00:00</updated>
    <author>
      <name>/u/Ryoiki-Tokuiten</name>
      <uri>https://old.reddit.com/user/Ryoiki-Tokuiten</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcc2fa/an_opensource_framework_to_achieve_gemini_3_deep/"&gt; &lt;img alt="An open-source framework to achieve Gemini 3 Deep Think / GPT-5.2 Pro level performance with local models scaffolding" src="https://preview.redd.it/wfpxmbhlc7lg1.png?width=140&amp;amp;height=99&amp;amp;auto=webp&amp;amp;s=48d6dec8f7b1fd1a11e8a1b5afbd51040b6a5021" title="An open-source framework to achieve Gemini 3 Deep Think / GPT-5.2 Pro level performance with local models scaffolding" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ryoiki-Tokuiten"&gt; /u/Ryoiki-Tokuiten &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1rcc2fa"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcc2fa/an_opensource_framework_to_achieve_gemini_3_deep/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rcc2fa/an_opensource_framework_to_achieve_gemini_3_deep/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T08:33:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1rc59ze</id>
    <title>Qwen3's most underrated feature: Voice embeddings</title>
    <updated>2026-02-23T02:28:32+00:00</updated>
    <author>
      <name>/u/k_means_clusterfuck</name>
      <uri>https://old.reddit.com/user/k_means_clusterfuck</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rc59ze/qwen3s_most_underrated_feature_voice_embeddings/"&gt; &lt;img alt="Qwen3's most underrated feature: Voice embeddings" src="https://preview.redd.it/zmcs7iysm5lg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=796016e685c536fbab1ce49b5fec35afeb75f40e" title="Qwen3's most underrated feature: Voice embeddings" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Did you know that Qwen3 TTS utilizes voice embedding for voice cloning?&lt;br /&gt; Your voice is turned into a vector of 1024 dimensions (or 2048 for 1.7b), and based on this vector alone you can get your custom voice.&lt;/p&gt; &lt;p&gt;But the coolest part is that this means that you can use math to modify voices, average voices. You can swap gender, pitch, mix and match voices, and even create an emotion space! This also enables semantic voice search!&lt;/p&gt; &lt;p&gt;The voice embedding model is actually just a tiny encoder with just a few million parameters. I've ripped it out of the voice embedding model so you can use the embedding model standalone. Check out my collection! :D I also have onnx models for optimized web / front-end inference.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/marksverdhei/qwen3-voice-embedding"&gt;https://huggingface.co/collections/marksverdhei/qwen3-voice-embedding&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Voice embedings can be used for inference in my vllm-omni fork until it is supported in upstream: &lt;a href="https://github.com/heiervang-technologies/ht-vllm-omni"&gt;https://github.com/heiervang-technologies/ht-vllm-omni&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/k_means_clusterfuck"&gt; /u/k_means_clusterfuck &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zmcs7iysm5lg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rc59ze/qwen3s_most_underrated_feature_voice_embeddings/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rc59ze/qwen3s_most_underrated_feature_voice_embeddings/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T02:28:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1rclyvf</id>
    <title>Portable Workstation for Inference</title>
    <updated>2026-02-23T16:24:21+00:00</updated>
    <author>
      <name>/u/neintailedfoxx</name>
      <uri>https://old.reddit.com/user/neintailedfoxx</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rclyvf/portable_workstation_for_inference/"&gt; &lt;img alt="Portable Workstation for Inference" src="https://preview.redd.it/j59qyq8sq9lg1.jpg?width=140&amp;amp;height=64&amp;amp;auto=webp&amp;amp;s=f979ae081b50b775630080b53aa9f61d422aa5ed" title="Portable Workstation for Inference" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Built a new portable workstation for gaming/AI workloads. One of the fans is a 12018 fan bought from aliexpress derived from a fan on the 4090FE, allowing it to provide airflow equivalent to normal 25mm thick fans despite only being 18mm in thickness.&lt;/p&gt; &lt;p&gt;Would've loved to get a Threadripper for additional memory bandwidth, but sadly there aren't any itx Threadripper boards :(&lt;/p&gt; &lt;p&gt;Getting around 150-165 tok/sec running GPT OSS 120B with max context length in LM Studio (Using windows, haven't had time to test in linux yet)&lt;/p&gt; &lt;p&gt;CPU is undervolted using the curve optimizer (-25/-30 per CCD CO) with a +200MHz PBO clock offset, RAM is tuned to 6000MT/s CL28-36-35-30 @ 2233MHz FCLK, and the GPU is undervolted to 0.89v@2700MHz and power limited to 500w.&lt;/p&gt; &lt;p&gt;Temps are good, with the cpu reaching a max temp of around 75c and the GPU never going above 80c even during extremely heavy workloads. Top fans are set to intake, providing airflow to the flipped GPU.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Case:&lt;/strong&gt; FormD T1 2.5 Gunmetal w/ Flipped Travel Kit&lt;/p&gt; &lt;p&gt;&lt;strong&gt;CPU:&lt;/strong&gt; AMD Ryzen 9 9950X3D&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GPU:&lt;/strong&gt; NVIDIA RTX PRO 6000 Workstation Edition&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Motherboard:&lt;/strong&gt; MSI MPG X870I EDGE TI EVO WIFI&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Ram:&lt;/strong&gt; TEAMGROUP T-Force Delta RGB 96 GB DDR5-6800 CL36&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Storage:&lt;/strong&gt; Crucial T710 4TB, Samsung 990 Pro 4TB, WD Black SN850X 8TB, TEAMGROUP CX2 2TB (Used drives from my previous build since I definitely won't be able to afford all this storage at current prices)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;PSU:&lt;/strong&gt; Corsair SF1000&lt;/p&gt; &lt;p&gt;&lt;strong&gt;PSU Cables:&lt;/strong&gt; Custom Cables from Dreambigbyray&lt;/p&gt; &lt;p&gt;&lt;strong&gt;CPU Cooler:&lt;/strong&gt; CM Masterliquid 240 ATMOS Stealth&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/neintailedfoxx"&gt; /u/neintailedfoxx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1rclyvf"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rclyvf/portable_workstation_for_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rclyvf/portable_workstation_for_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T16:24:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcpxs7</id>
    <title>Hmm new drama unlocked</title>
    <updated>2026-02-23T18:43:04+00:00</updated>
    <author>
      <name>/u/Independent-Wind4462</name>
      <uri>https://old.reddit.com/user/Independent-Wind4462</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcpxs7/hmm_new_drama_unlocked/"&gt; &lt;img alt="Hmm new drama unlocked" src="https://preview.redd.it/fs0ubtgphalg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=799ca5d2f2b5464600303cf5e61308b2f6a4dc3f" title="Hmm new drama unlocked" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Wind4462"&gt; /u/Independent-Wind4462 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fs0ubtgphalg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcpxs7/hmm_new_drama_unlocked/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rcpxs7/hmm_new_drama_unlocked/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T18:43:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcmlwk</id>
    <title>so is OpenClaw local or not</title>
    <updated>2026-02-23T16:47:01+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcmlwk/so_is_openclaw_local_or_not/"&gt; &lt;img alt="so is OpenClaw local or not" src="https://preview.redd.it/5rolok0mw9lg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d0bdebee8fd3b3c91999b3592892a73daf47142e" title="so is OpenClaw local or not" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Reading the comments, I‚Äôm guessing you didn‚Äôt bother to read this:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&amp;quot;Safety and alignment at Meta Superintelligence.&amp;quot;&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5rolok0mw9lg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcmlwk/so_is_openclaw_local_or_not/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rcmlwk/so_is_openclaw_local_or_not/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T16:47:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcpmwn</id>
    <title>Anthropic: "We‚Äôve identified industrial-scale distillation attacks on our models by DeepSeek, Moonshot AI, and MiniMax." üö®</title>
    <updated>2026-02-23T18:32:45+00:00</updated>
    <author>
      <name>/u/KvAk_AKPlaysYT</name>
      <uri>https://old.reddit.com/user/KvAk_AKPlaysYT</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcpmwn/anthropic_weve_identified_industrialscale/"&gt; &lt;img alt="Anthropic: &amp;quot;We‚Äôve identified industrial-scale distillation attacks on our models by DeepSeek, Moonshot AI, and MiniMax.&amp;quot; üö®" src="https://preview.redd.it/94fbimavfalg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c2ad159232448ffd7033d6be4fa96582b674e461" title="Anthropic: &amp;quot;We‚Äôve identified industrial-scale distillation attacks on our models by DeepSeek, Moonshot AI, and MiniMax.&amp;quot; üö®" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KvAk_AKPlaysYT"&gt; /u/KvAk_AKPlaysYT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/94fbimavfalg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcpmwn/anthropic_weve_identified_industrialscale/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rcpmwn/anthropic_weve_identified_industrialscale/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T18:32:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7bsfd</id>
    <title>Best Audio Models - Feb 2026</title>
    <updated>2026-02-17T17:01:37+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;They've been a ton of audio models released of late, the most notable perhaps being Qwen3 TTS. So its time for another &lt;strong&gt;Best Audio Models&lt;/strong&gt; megathread&lt;/p&gt; &lt;p&gt;Share what your favorite ASR, TTS, STT, Text to Music models are right now &lt;strong&gt;and why.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Given the the amount of ambiguity and subjectivity in rating/testing these models, please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks etc. Closed models like Elevenlabs v3 seem to continue to be a few levels above open models especially for production use cases with long lengths/stability requirements, so comparisons, especially empirical ones are welcome.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please use the top level comments to thread your responses.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T17:01:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8snay</id>
    <title>AMA with StepFun AI - Ask Us Anything</title>
    <updated>2026-02-19T07:15:35+00:00</updated>
    <author>
      <name>/u/StepFun_ai</name>
      <uri>https://old.reddit.com/user/StepFun_ai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt; &lt;img alt="AMA with StepFun AI - Ask Us Anything" src="https://external-preview.redd.it/bqLigXK_A-XO-amDfdyeQenKCxR18nhChWpDnQXYmr4.png?width=140&amp;amp;height=75&amp;amp;auto=webp&amp;amp;s=72c79bc533c167d8f77ecf629e3b7af53ee1e6b6" title="AMA with StepFun AI - Ask Us Anything" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/w8274fg1jekg1.png?width=1785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fadbd0ec26a56e60900f9ed667ae808217d70cf2"&gt;https://preview.redd.it/w8274fg1jekg1.png?width=1785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fadbd0ec26a56e60900f9ed667ae808217d70cf2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; !&lt;/p&gt; &lt;p&gt;We are &lt;strong&gt;StepFun&lt;/strong&gt;, the team behind the &lt;strong&gt;Step&lt;/strong&gt; family models, including &lt;a href="https://huggingface.co/collections/stepfun-ai/step-35-flash"&gt;&lt;strong&gt;Step 3.5 Flash&lt;/strong&gt;&lt;/a&gt; and &lt;a href="https://huggingface.co/collections/stepfun-ai/step3-vl-10b"&gt;&lt;strong&gt;Step-3-VL-10B&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;We are super excited to host our first AMA tomorrow in this community. Our participants include CEO, CTO, Chief Scientist, LLM Researchers.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Participants&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Ok_Reach_5122"&gt;u/Ok_Reach_5122&lt;/a&gt; (Co-founder &amp;amp; CEO of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/bobzhuyb"&gt;u/bobzhuyb&lt;/a&gt; (Co-founder &amp;amp; CTO of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/user/Lost-Nectarine1016"&gt;u/Lost-Nectarine1016&lt;/a&gt; (Co-founder &amp;amp; Chief Scientist of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Elegant-Sale-1328"&gt;u/Elegant-Sale-1328&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/SavingsConclusion298"&gt;u/SavingsConclusion298&lt;/a&gt; (Post-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Spirited_Spirit3387"&gt;u/Spirited_Spirit3387&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/These-Nothing-8564/"&gt;u/These-Nothing-8564&lt;/a&gt; (Technical Project Manager)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Either-Beyond-7395"&gt;u/Either-Beyond-7395&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Human_Ad_162"&gt;u/Human_Ad_162&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Icy_Dare_3866"&gt;u/Icy_Dare_3866&lt;/a&gt; (Post-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Big-Employee5595"&gt;u/Big-Employee5595&lt;/a&gt; (Agent Algorithms Lead&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run 8 - 11 AM PST, Feburary 19th. The StepFun team will monitor and answer questions over the 24 hours after the live session.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StepFun_ai"&gt; /u/StepFun_ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T07:15:35+00:00</published>
  </entry>
</feed>
