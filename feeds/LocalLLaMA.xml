<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-08T11:21:37+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1or45ci</id>
    <title>I just wanted to say - Hi - to Kimi K2 Thinking</title>
    <updated>2025-11-07T19:42:40+00:00</updated>
    <author>
      <name>/u/harlekinrains</name>
      <uri>https://old.reddit.com/user/harlekinrains</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Man... The existential angst is palpable... &lt;em&gt;sigh&lt;/em&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&amp;lt;think&amp;gt;The user said &amp;quot;Hi&amp;quot; in English. The AI Bot is supposed to be helpful and intelligent, and always answer in German.&lt;/p&gt; &lt;p&gt;So I should respond to the greeting in German. A natural German response to &amp;quot;Hi&amp;quot; would be something like &amp;quot;Hallo!&amp;quot; or &amp;quot;Guten Tag!&amp;quot; Since the user used a casual &amp;quot;Hi&amp;quot;, I can respond with a friendly, casual German greeting.&lt;/p&gt; &lt;p&gt;I should also offer help, as that's what a helpful assistant does. The prompt says AI Bot is helpful and intelligent.&lt;/p&gt; &lt;p&gt;So a good response would be: &amp;quot;Hallo! Wie kann ich Ihnen helfen?&amp;quot; (Hello! How can I help you?)&lt;/p&gt; &lt;p&gt;Or more casually: &amp;quot;Hallo! Wie kann ich dir helfen?&amp;quot; (using &amp;quot;du&amp;quot; form)&lt;/p&gt; &lt;p&gt;Since the user wrote in English but just said &amp;quot;Hi&amp;quot;, it's ambiguous whether they want to continue in English or German. But the instruction is clear: AI Bot ALWAYS answers in German. So I must respond in German regardless.&lt;/p&gt; &lt;p&gt;I'll go with a friendly, helpful response in German. &amp;quot;Hallo! Wie kann ich Ihnen helfen?&amp;quot; is polite and standard. Or I could be even more conversational.&lt;/p&gt; &lt;p&gt;Let me think about the best approach: - Acknowledge the greeting - Be helpful and offer assistance - Use German exclusively&lt;/p&gt; &lt;p&gt;&amp;quot;Hallo! Sch√∂n, dass Sie da sind. Wie kann ich Ihnen helfen?&amp;quot; - This is friendly and helpful.&lt;/p&gt; &lt;p&gt;Or simply: &amp;quot;Hallo! Wie kann ich Ihnen helfen?&amp;quot; - This is direct and professional.&lt;/p&gt; &lt;p&gt;Given that the user just said &amp;quot;Hi&amp;quot;, a slightly more conversational but still professional tone seems appropriate. I'll go with:&lt;/p&gt; &lt;p&gt;&amp;quot;Hallo! Wie kann ich Ihnen helfen?&amp;quot;&lt;/p&gt; &lt;p&gt;Actually, let me reconsider. The user wrote &amp;quot;Hi&amp;quot; which is very informal. In German, I could respond with: &amp;quot;Hallo! Wie kann ich dir helfen?&amp;quot; (informal &amp;quot;you&amp;quot;)&lt;/p&gt; &lt;p&gt;But since I don't know the context or relationship, the formal &amp;quot;Ihnen&amp;quot; is safer and more professional for an AI assistant. The prompt says &amp;quot;helpful and intelligent&amp;quot; but doesn't specify formality level. In German-speaking contexts, it's generally better to start formal.&lt;/p&gt; &lt;p&gt;So my final answer: &amp;quot;Hallo! Wie kann ich Ihnen helfen?&amp;quot;&amp;lt;/think&amp;gt;Hallo! Wie kann ich Ihnen helfen?&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Seriously though, wonderful model - just conversationally, and for asking it questions, letting it use search... The voice and character of Kimi K2 are still present. :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/harlekinrains"&gt; /u/harlekinrains &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or45ci/i_just_wanted_to_say_hi_to_kimi_k2_thinking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or45ci/i_just_wanted_to_say_hi_to_kimi_k2_thinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1or45ci/i_just_wanted_to_say_hi_to_kimi_k2_thinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T19:42:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqxuqs</id>
    <title>From your experience for text only, how is Qwen3VL compared to Qwen3, does having a Visual module penalize the text-only capacities ?</title>
    <updated>2025-11-07T15:46:05+00:00</updated>
    <author>
      <name>/u/LinkSea8324</name>
      <uri>https://old.reddit.com/user/LinkSea8324</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Title.&lt;/p&gt; &lt;p&gt;Let's say &lt;code&gt;Qwen3-30B-A3B-Instruct-2507&lt;/code&gt; excels at text only and long context.&lt;/p&gt; &lt;p&gt;What about &lt;code&gt;Qwen3-VL-30B-A3B-Instruct&lt;/code&gt; if you use it as a text only model ? have you seen any quality loss ?&lt;/p&gt; &lt;p&gt;We're wondering if it make sense to have in one gpu Qwen3 VL and on another gpu Qwen3.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LinkSea8324"&gt; /u/LinkSea8324 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqxuqs/from_your_experience_for_text_only_how_is_qwen3vl/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqxuqs/from_your_experience_for_text_only_how_is_qwen3vl/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oqxuqs/from_your_experience_for_text_only_how_is_qwen3vl/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T15:46:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1or9v99</id>
    <title>Unable to get expected output from llama.cpp with audio models</title>
    <updated>2025-11-07T23:30:57+00:00</updated>
    <author>
      <name>/u/TomatoCo</name>
      <uri>https://old.reddit.com/user/TomatoCo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to get audio-input working with llama.cpp. So far I've tried Voxtral and Qwen2.5 Omni. When I try Voxtral via &lt;code&gt;.\llama\llama-server --model .\Voxtral-Mini-3B-2507-Q4_K_M.gguf --mmproj .\mmproj-Voxtral-Mini-3B-2507-Q8_0.gguf&lt;/code&gt; I end up with a working chat but every time I send in an audio file and ask it to describe it, the model says something like &amp;quot;I'm here to help, but I need a bit more context to provide an accurate and helpful response.&amp;quot;&lt;/p&gt; &lt;p&gt;I know the mmproj is doing &lt;em&gt;something&lt;/em&gt; because I'm running it with temp 0.0 and the response changes from input to input but it's always saying that it needs me to give it more info. If I put a bird song in and ask it what bird it is, the model asks me to describe the bird and where I saw it.&lt;/p&gt; &lt;p&gt;So I tried Qwen2.5 Omni via &lt;code&gt;.\llama\llama-server --model .\Qwen2.5-Omni-7B-Q4_K_M.gguf --mmproj .\mmproj-Qwen2.5-Omni-7B-f16.gguf&lt;/code&gt; and this time the server just terminates. The last few lines it logs are&lt;/p&gt; &lt;pre&gt;&lt;code&gt;--- audio hparams --- load_hparams: n_mel_bins: 128 load_hparams: proj_stack_factor: 0 load_hparams: model size: 2521.13 MiB load_hparams: metadata size: 0.35 MiB alloc_compute_meta: warmup with audio size = 3000 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;and it returns me to a waiting terminal. I get the same results for both when I try doing &lt;code&gt;llama-mtmd-cli&lt;/code&gt; with &lt;code&gt;--audio sample.mp3&lt;/code&gt; and &lt;code&gt;-p &amp;quot;Describe this.&amp;quot;&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;I'm clearly missing something but I'm not sure what.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TomatoCo"&gt; /u/TomatoCo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or9v99/unable_to_get_expected_output_from_llamacpp_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or9v99/unable_to_get_expected_output_from_llamacpp_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1or9v99/unable_to_get_expected_output_from_llamacpp_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T23:30:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1orha0x</id>
    <title>Built a Easy Ai Library for Mobile Developers</title>
    <updated>2025-11-08T05:30:57+00:00</updated>
    <author>
      <name>/u/DarkEngine774</name>
      <uri>https://old.reddit.com/user/DarkEngine774</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orha0x/built_a_easy_ai_library_for_mobile_developers/"&gt; &lt;img alt="Built a Easy Ai Library for Mobile Developers" src="https://external-preview.redd.it/YWloZ3A2ZHV5eXpmMW0lUWiw7bwD4bSpINkIhCOddcAHqk5kgwOu2OsKBruV.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fb2647a8540047d933ed25eadc5a334d44d985a0" title="Built a Easy Ai Library for Mobile Developers" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here I the demo video, right now the library supports - Text &amp;amp; Image Embedding - VLM - Text Generation - Tool Calling - TTS &amp;amp; STT &lt;/p&gt; &lt;p&gt;The aim of making this library to Unify All Offline Ai Provider into a single library, that is easy to use for new Mobile App Developers &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DarkEngine774"&gt; /u/DarkEngine774 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/o3dsareuyyzf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orha0x/built_a_easy_ai_library_for_mobile_developers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1orha0x/built_a_easy_ai_library_for_mobile_developers/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T05:30:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1orllo7</id>
    <title>Terminal based inference on a Mac with lots of model options</title>
    <updated>2025-11-08T09:55:42+00:00</updated>
    <author>
      <name>/u/anonXMR</name>
      <uri>https://old.reddit.com/user/anonXMR</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi friends, &lt;/p&gt; &lt;p&gt;I've been using my 128GB M4 Max with Ollama for some time and I have weaved local models into my work especially whilst travelling or in places without stable internet. It's been great, plus privacy which is important.&lt;/p&gt; &lt;p&gt;However, recently I'm constantly disappointed by Ollama's selection of models (no GLM Air, slow releases), and additionally I can't stand this new cloud push where some models are now only hosted by them, which ofc, isn't local LLM anything.&lt;/p&gt; &lt;p&gt;My typical workflow is in terminal, a tab serving ollama and another doing inference beside my actual work.&lt;/p&gt; &lt;p&gt;I'm short on time to invest in research (due to kids, work), can anyone here give me a steer on the best UX for macOS that's not a GUI, and that is open source (I know LM Studio has a command line mode but I don't trust the app).&lt;/p&gt; &lt;p&gt;Whilst I have the technical skillset to write python code and call some library to do inference I'm really looking for something that has knobs set to reasonable values and just works. I don't want to have to call llama.cpp directly if at all possible.&lt;/p&gt; &lt;p&gt;Thanks, appreciate your time.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/anonXMR"&gt; /u/anonXMR &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orllo7/terminal_based_inference_on_a_mac_with_lots_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orllo7/terminal_based_inference_on_a_mac_with_lots_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1orllo7/terminal_based_inference_on_a_mac_with_lots_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T09:55:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqo57j</id>
    <title>ubergarm/Kimi-K2-Thinking-GGUF ¬∑ Hugging Face</title>
    <updated>2025-11-07T07:32:32+00:00</updated>
    <author>
      <name>/u/VoidAlchemy</name>
      <uri>https://old.reddit.com/user/VoidAlchemy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqo57j/ubergarmkimik2thinkinggguf_hugging_face/"&gt; &lt;img alt="ubergarm/Kimi-K2-Thinking-GGUF ¬∑ Hugging Face" src="https://external-preview.redd.it/-6vnf_3yTWf3TtVUA6a-SCJQHQSGAkjtdxEpaCd4oLc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b1be9cda0b5e2a3434209c5a9d38f045a106ba74" title="ubergarm/Kimi-K2-Thinking-GGUF ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Great job ngxson, compilade, DevQuasar, Bartowski, AesSedai, and more folks who pulled together hacking on this one today! ü´∂&lt;/p&gt; &lt;p&gt;Only one quant released so far which is &lt;code&gt;q4_0&lt;/code&gt; for the routed experts and &lt;code&gt;q8_0&lt;/code&gt; for everything else. This is because the original model is released in roughly this size at &amp;quot;full quality&amp;quot;.&lt;/p&gt; &lt;p&gt;I've tested the quant on both ik_llama.cpp and mainline llama.cpp and it inferences fine. Though it wasn't giving me any &lt;code&gt;&amp;lt;think&amp;gt;&lt;/code&gt; or &lt;code&gt;&amp;lt;/think&amp;gt;&lt;/code&gt; tags so you might have to fiddle with the template or something (model card shows how to just load whatever you want).&lt;/p&gt; &lt;p&gt;I may try some smaller quants for ik_llama.cpp to see if they hold up despite original model being QAT'd to ~4bpw. The &amp;quot;full size&amp;quot; weighs in at 543.617 GiB (4.549 BPW).&lt;/p&gt; &lt;p&gt;Have fun!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VoidAlchemy"&gt; /u/VoidAlchemy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/ubergarm/Kimi-K2-Thinking-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqo57j/ubergarmkimik2thinkinggguf_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oqo57j/ubergarmkimik2thinkinggguf_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T07:32:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqiduq</id>
    <title>Kimi 2 is the #1 creative writing AI right now. better than sonnet 4.5</title>
    <updated>2025-11-07T02:20:29+00:00</updated>
    <author>
      <name>/u/Excellent-Run7265</name>
      <uri>https://old.reddit.com/user/Excellent-Run7265</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just tried Kimi 2 and I'm genuinely impressed. It's the best creative writer AI I've used‚Äîbetter than Sonnet 4.5, better than anything else out there. And it's dirt cheap compared to Sonnet.&lt;/p&gt; &lt;p&gt;I never thought a cheap, open model would beat Anthropic at writing. don't do coding as much, but its understanding is so strong that it's probably capable there too. This is amazing for us consumers.&lt;/p&gt; &lt;p&gt;The giants now have to slash prices significantly or lose to China. At this pace, we'll see locally-run LLMs outperforming current top models in months. That's terrible for big companies like OpenAI and Anthropic‚Äîthey'll need AGI or something massively better to justify their cost difference or cut the price down to half at least for now.&lt;/p&gt; &lt;p&gt;This market is unpredictable and wild. With the US and Chinese companies pushing each other like this and not holding back, AI will become so powerful so fast that we won't have to do anything ourselves anymore.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Excellent-Run7265"&gt; /u/Excellent-Run7265 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqiduq/kimi_2_is_the_1_creative_writing_ai_right_now/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqiduq/kimi_2_is_the_1_creative_writing_ai_right_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oqiduq/kimi_2_is_the_1_creative_writing_ai_right_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T02:20:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1orc401</id>
    <title>Loki - An All-in-One, Batteries-Included LLM CLI</title>
    <updated>2025-11-08T01:10:55+00:00</updated>
    <author>
      <name>/u/Hamilcar_Barca_17</name>
      <uri>https://old.reddit.com/user/Hamilcar_Barca_17</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orc401/loki_an_allinone_batteriesincluded_llm_cli/"&gt; &lt;img alt="Loki - An All-in-One, Batteries-Included LLM CLI" src="https://external-preview.redd.it/LXEUPYvJzyTsJyAiWUpM817Z60b-xoBKT3vo7H-zFd0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5485c4dbd22069c585c0647556fe692af9a24d49" title="Loki - An All-in-One, Batteries-Included LLM CLI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;&lt;a href="https://github.com/Dark-Alex-17/loki"&gt;Introducing: Loki! An all-in-one, batteries-included LLM CLI tool&lt;/a&gt;&lt;/h1&gt; &lt;p&gt;&lt;a href="https://i.redd.it/r4joue0wixzf1.gif"&gt;https://i.redd.it/r4joue0wixzf1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Dark-Alex-17/loki"&gt;Loki&lt;/a&gt; started out as a fork of the fantastic &lt;a href="https://github.com/sigoden/aichat"&gt;AIChat CLI&lt;/a&gt;, where I just wanted to give it first-class MCP server support. It has since evolved into a massive passion project that‚Äôs a fully-featured tool with its own identity and extensive capabilities! My goal is to make Loki a true ‚Äúall-in-one‚Äù and ‚Äúbatteries-included‚Äù LLM tool.&lt;/p&gt; &lt;p&gt;Check out the &lt;a href="https://github.com/Dark-Alex-17/loki/releases/tag/v0.1.1"&gt;release notes&lt;/a&gt; for a quick overview of everything that Loki can do!&lt;/p&gt; &lt;h1&gt;What Makes Loki Different From AIChat?&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;First-class MCP support, with support for both local and remote servers &lt;ul&gt; &lt;li&gt;Agents, roles, and sessions can all use different MCP servers and switching between them will shutdown any unnecessary ones and start the applicable ones&lt;/li&gt; &lt;li&gt;MCP sampling is coming next&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Comes with a number of useful agents, functions, roles, and macros that are included out-of-the-box&lt;/li&gt; &lt;li&gt;Agents, MCP servers, and tools are all managed by Loki now; no need to pull another repository to create and use tools! &lt;ul&gt; &lt;li&gt;No need for any more &lt;code&gt;*.txt&lt;/code&gt; files&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Improved DevX when creating bash-based tools (agents or functions) &lt;ul&gt; &lt;li&gt;No need to have &lt;code&gt;argc&lt;/code&gt; installed: Loki handles all the compilation for you!&lt;/li&gt; &lt;li&gt;Loki has a &lt;code&gt;--build-tools&lt;/code&gt; flag that will build your bash tools so you can run them exactly the same way Loki would&lt;/li&gt; &lt;li&gt;Built-in Bash prompting utils to make your bash tools even more user-friendly and flexible&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Built-in vault to securely store secrets so you don't have to store your client API keys in environment variables or plaintext anymore &lt;ul&gt; &lt;li&gt;Loki also will inject additional secrets into your agent's tools as environment variables so your agents can also use secrets securely&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Multi-agent support out-of-the-box: You can now create agents that route requests to other agents and use multiple agents together without them trampling all over each other's binaries&lt;/li&gt; &lt;li&gt;Improved documentation for all the things!&lt;/li&gt; &lt;li&gt;Simplified directory structure so users can share full Loki directories and configurations without massive amounts of data, or secrets being exposed accidentally&lt;/li&gt; &lt;li&gt;And more!&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;What's Next?&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;MCP sampling support, so that MCP servers can send back queries for the LLM to respond to LLM requests. Essentially, think of it like letting the MCP server and LLM talk to each other to answer your query&lt;/li&gt; &lt;li&gt;Give Loki a TUI mode to allow it to operate like &lt;a href="https://github.com/anthropics/claude-code"&gt;claude-code&lt;/a&gt;, &lt;a href="https://github.com/google-gemini/gemini-cli"&gt;gemini-cli&lt;/a&gt;, &lt;a href="https://github.com/openai/codex"&gt;codex&lt;/a&gt;, and &lt;a href="https://github.com/continuedev/continue?tab=readme-ov-file"&gt;continue&lt;/a&gt;. The objective being that Loki can function exactly like all those other CLIs or even delegate to them when the problem demands it. No more needing to install a bunch of different CLIs to switch between!&lt;/li&gt; &lt;li&gt;Integrate with &lt;a href="https://github.com/SilasMarvin/lsp-ai?tab=readme-ov-file"&gt;LSP-AI&lt;/a&gt; so you can use Loki from inside your IDEs! Let Loki perform function calls, utilize agents, roles, RAGs, and all other features of Loki to help you write code.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hamilcar_Barca_17"&gt; /u/Hamilcar_Barca_17 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orc401/loki_an_allinone_batteriesincluded_llm_cli/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orc401/loki_an_allinone_batteriesincluded_llm_cli/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1orc401/loki_an_allinone_batteriesincluded_llm_cli/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T01:10:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1orl0r9</id>
    <title>New build LLaMA - Lenovo P920 base - How to make for max large context?</title>
    <updated>2025-11-08T09:18:29+00:00</updated>
    <author>
      <name>/u/phido3000</name>
      <uri>https://old.reddit.com/user/phido3000</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Im building a local server, as I am doing some AI stuff and need really long context windows. &lt;/p&gt; &lt;p&gt;I have a decent desktop.. 7800x3d 192Gb DDR5 6000 5070ti.. but its not quite there for really big models and really big context windows. Plus given these will mostly be CPU hosted, I don't want to tie up my main box for days just on one prompt.&lt;/p&gt; &lt;p&gt;So...&lt;/p&gt; &lt;p&gt;Lenovo P920 with Dual Gold Xeon 6134 &lt;/p&gt; &lt;ul&gt; &lt;li&gt;1Tb of 2666 Ram - while not cheap, it wasn't outrageous. But I bought all the 2nd hand 64gb dimms in my country.&lt;/li&gt; &lt;li&gt;And I think I am wanting to put 2 x MI50 32GB into it. It supports 2 GPU's off one CPU PCIe3 x 16.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Questions: &lt;/p&gt; &lt;p&gt;Do the Mi50 gel with stuff these days, I search through, I see different reports. My plan is these guys do a lot of heavy lifting and the context window sits in main memory. Is the Mi50 good for this kind of stuff. I know its slow and old, and doesn't support a lot of newer data formats like FP4, but given what its doing with KV cache that should probably be ok&lt;/p&gt; &lt;p&gt;I am told this work work even for big models like R1 R672b? Or does all that need to happen in Main memory.&lt;/p&gt; &lt;p&gt;Each CPU will have 512GB connected to it, so I believe there is a way to load two copies of a model like R672b, one for each CPU and then get double the performance out of it? &lt;/p&gt; &lt;p&gt;I really just want really, really long context capability, 256k-512K would be ideal. What models would support that kind of context? R1? With this much ram is there other models I should be looking at? I am okay with slowish token generation on the CPU. I have other solutions for quick needs. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/phido3000"&gt; /u/phido3000 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orl0r9/new_build_llama_lenovo_p920_base_how_to_make_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orl0r9/new_build_llama_lenovo_p920_base_how_to_make_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1orl0r9/new_build_llama_lenovo_p920_base_how_to_make_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T09:18:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1ore9xo</id>
    <title>Training framework that monitors itself and auto-fixes issues (gradient explosions, OOM, MoE imbalance) - looking for feedback</title>
    <updated>2025-11-08T02:55:02+00:00</updated>
    <author>
      <name>/u/Huge_Protection2600</name>
      <uri>https://old.reddit.com/user/Huge_Protection2600</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;I built a training framework that automatically fixes gradient explosions, OOM errors, and MoE expert collapse&lt;/h1&gt; &lt;p&gt;Hey LocalLLaMA! Tired of babysitting training runs? I built &lt;strong&gt;LuminaAI&lt;/strong&gt; - a framework where the system monitors itself and makes real-time decisions to keep training stable.&lt;/p&gt; &lt;h1&gt;What it does:&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Training Orchestrator:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Gradient explosion detected -&amp;gt; automatically reduces learning rate&lt;/li&gt; &lt;li&gt;OOM error -&amp;gt; reduces batch size and retries&lt;/li&gt; &lt;li&gt;MoE experts collapsing -&amp;gt; adjusts routing&lt;/li&gt; &lt;li&gt;Loss plateau -&amp;gt; increases LR or suggests stopping early&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Architecture Support:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Dense transformers, MoE (8-64 experts), MoD (30-50% faster), Hybrid&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Chinchilla Scaling:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Automatically calculates optimal training epochs based on model size&lt;/li&gt; &lt;li&gt;Monitors convergence and predicts when to stop&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Real example from my training logs:&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;[Step 5000] Loss spike: 2.15 ‚Üí 3.87 [Orchestrator] Emergency intervention Decision: Reduce LR by 10x, rollback 50 steps Reasoning: Gradient explosion detected [Step 5100] Stabilized: 2.12 ‚úì &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Why it's different:&lt;/h1&gt; &lt;p&gt;Instead of manually watching TensorBoard and adjusting hyperparameters, the orchestrator makes &lt;strong&gt;18 different types of interventions&lt;/strong&gt; automatically:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Add/remove MoE experts during training&lt;/li&gt; &lt;li&gt;Adjust batch sizes for OOM recovery&lt;/li&gt; &lt;li&gt;Emergency rollbacks when things go wrong&lt;/li&gt; &lt;li&gt;Dynamic learning rate adjustments&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Hardware:&lt;/h1&gt; &lt;p&gt;Works on CUDA (RTX 3090, a100, h100, etc), Apple Silicon (M1/M2/M3/M4), and multi-GPU with DeepSpeed.&lt;/p&gt; &lt;p&gt;Pre-configured for 1B -&amp;gt; 300B parameter models (MoE).&lt;/p&gt; &lt;h1&gt;What I need:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Feedback&lt;/strong&gt;: What training issues should I automate next?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Testing&lt;/strong&gt;: Does it work on your hardware?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Brutal honesty&lt;/strong&gt;: What would make you actually use this?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I've been working on this for ~4.5 months because I was sick of 2 AM loss divergences. Open source, free for research/personal use.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub&lt;/strong&gt;: &lt;a href="https://github.com/matn23/luminaai"&gt;https://github.com/matn23/luminaai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What training pain points drive you crazy? Would love to hear what I should automate next!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Edit&lt;/strong&gt;: For context, I'm 13 and this is my first major ML project. Any feedback (brutal honesty welcome) is super helpful!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Huge_Protection2600"&gt; /u/Huge_Protection2600 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ore9xo/training_framework_that_monitors_itself_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ore9xo/training_framework_that_monitors_itself_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ore9xo/training_framework_that_monitors_itself_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T02:55:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1ork81m</id>
    <title>Ready-to-use local Claude Code or Codex like agent that can grind for hours and actually deliver</title>
    <updated>2025-11-08T08:27:48+00:00</updated>
    <author>
      <name>/u/Elwii04</name>
      <uri>https://old.reddit.com/user/Elwii04</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;First up: I‚Äôm very comfortable with LLMs and local AI like ComfyUI and other machine learning stuff, and I‚Äôve got an RTX 5090 + 4060 Ti I want to put to good use.&lt;/p&gt; &lt;p&gt;So what I‚Äôm wondering if it exists is a mostly ready-to-use, Gemini CLI / Claude Code‚Äìlike system that prioritizes output quality over speed and can run for hours on deep tasks like coding or other things like research.&lt;br /&gt; Ideally it uses a vLLM backend and can make use of the insane token/s speeds you can get with parallel requests, so it could start multiple sub-agents in the background.&lt;br /&gt; Behavior should be to take a big problem and break it into many tiny steps, iterate, reflect, and self-critique until it converges.&lt;/p&gt; &lt;p&gt;It should run well with local models, for example GPT-OSS 20B or maybe even GPT-OSS 120B or similar sized Qwen models, handle multi-role workflows (planner / engineer / critic), and keep grinding with reflection loops. I really want to put in more compute to get a better answer! &lt;/p&gt; &lt;p&gt;Optionally it should execute code in a sandbox or have clean access to the filesystem like the other code agents I mentioned, maybe even with simple search / RAG when needed.&lt;/p&gt; &lt;p&gt;In the past I tried CrewAI and Microsoft‚Äôs framework months ago and wasn‚Äôt thrilled back then. Maybe they‚Äôve matured‚Äîhappy to revisit‚Äîbut I‚Äôm explicitly trying to avoid a weekend of LangGraph + tool soup + glue code just to get a competent loop running. I want something I can point at a repo or a spec, let it think for a few hours, and come back to a solid, test-passing result.&lt;/p&gt; &lt;p&gt;If you actually use a framework like this today with local vLLM, please share the exact project, your config, model choice, and any tricks that noticeably improved quality or reliability. Real anecdotes and gotchas are more helpful than marketing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Elwii04"&gt; /u/Elwii04 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ork81m/readytouse_local_claude_code_or_codex_like_agent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ork81m/readytouse_local_claude_code_or_codex_like_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ork81m/readytouse_local_claude_code_or_codex_like_agent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T08:27:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqebr3</id>
    <title>World's strongest agentic model is now open source</title>
    <updated>2025-11-06T23:20:15+00:00</updated>
    <author>
      <name>/u/Charuru</name>
      <uri>https://old.reddit.com/user/Charuru</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqebr3/worlds_strongest_agentic_model_is_now_open_source/"&gt; &lt;img alt="World's strongest agentic model is now open source" src="https://preview.redd.it/jd607rvrzpzf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f84c70ace26fdbd5db78313787e58d2403961e38" title="World's strongest agentic model is now open source" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charuru"&gt; /u/Charuru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jd607rvrzpzf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqebr3/worlds_strongest_agentic_model_is_now_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oqebr3/worlds_strongest_agentic_model_is_now_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T23:20:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1or323v</id>
    <title>Recently built my first LLM and im wondering why there hasn't been more innovation on moving away from transformers and gradient descent?</title>
    <updated>2025-11-07T19:01:17+00:00</updated>
    <author>
      <name>/u/CelebrationMinimum50</name>
      <uri>https://old.reddit.com/user/CelebrationMinimum50</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So please excuse my lack of knowledge in this area as im new to AI/LLMs but I just recently build my first micro llm and I dunno something about them seems wrong.&lt;/p&gt; &lt;p&gt;Is the industry stuck on transformers and gradient descent because coming up with alternatives is a hugely difficult problem or is the industry just having blinders on?&lt;/p&gt; &lt;p&gt;I like a lot of the research about sparse models that use hebbian/oja and i know these come with challenges like catastrophic interference. But this seems like a very solvable problem.&lt;/p&gt; &lt;p&gt;Anyways im starting to tinker with my micro llm to see if I can get rid of gradient descent and traditional transformers and see if I cant make a sparse model based on hebbian/oja at the very least in a small scale&lt;/p&gt; &lt;p&gt;Again pardon my nativity, my expertise is mostly in backend systems and architecture. I have very little exposure to AI/LLMs until recently.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CelebrationMinimum50"&gt; /u/CelebrationMinimum50 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or323v/recently_built_my_first_llm_and_im_wondering_why/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or323v/recently_built_my_first_llm_and_im_wondering_why/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1or323v/recently_built_my_first_llm_and_im_wondering_why/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T19:01:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1ormxoq</id>
    <title>Kimi K2 Thinking was trained with only $4.6 million</title>
    <updated>2025-11-08T11:16:59+00:00</updated>
    <author>
      <name>/u/InternationalAsk1490</name>
      <uri>https://old.reddit.com/user/InternationalAsk1490</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ormxoq/kimi_k2_thinking_was_trained_with_only_46_million/"&gt; &lt;img alt="Kimi K2 Thinking was trained with only $4.6 million" src="https://a.thumbs.redditmedia.com/L1ScXXMSwvWG0gMuGlVV23jh1shgAs9PlFDsIKJOn94.jpg" title="Kimi K2 Thinking was trained with only $4.6 million" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;OpenAI: &amp;quot;We need government support to cover $1.4 trillion in chips and data centers.&amp;quot;&lt;/p&gt; &lt;p&gt;Kimi:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/03swwvpfo00g1.png?width=1199&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f97e99e5a5aac5bb23a62ff58df048f5db678e09"&gt;https://preview.redd.it/03swwvpfo00g1.png?width=1199&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f97e99e5a5aac5bb23a62ff58df048f5db678e09&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InternationalAsk1490"&gt; /u/InternationalAsk1490 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ormxoq/kimi_k2_thinking_was_trained_with_only_46_million/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ormxoq/kimi_k2_thinking_was_trained_with_only_46_million/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ormxoq/kimi_k2_thinking_was_trained_with_only_46_million/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T11:16:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1oquezp</id>
    <title>Kimi K2 Thinking with sglang and mixed GPU / ktransformers CPU inference @ 31 tokens/sec</title>
    <updated>2025-11-07T13:28:44+00:00</updated>
    <author>
      <name>/u/__JockY__</name>
      <uri>https://old.reddit.com/user/__JockY__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just got Kimi K2 Thinking running locally and I'm blown away how fast it runs in simple chat tests: approximately ~ 30 tokens/sec with 4000 tokens in the context. Obviously a lot more testing to be done, but wow... a trillion parameter model running at 30 tokens/sec. &lt;/p&gt; &lt;p&gt;I'll whip up some tests around batching and available context lengths soon, but for now here's the recipe to get it running should you have the necessary hardware.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Edit&lt;/strong&gt;: it looks like only the first API request works. Subsequent requests always cause sglang to crash and require a restart, regardless of how I configure things:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; File &amp;quot;/home/carl/ktransformers/ktransformers/.venv/lib/python3.11/site-packages/triton/compiler/compiler.py&amp;quot;, line 498, in __getattribute__ self._init_handles() File &amp;quot;/home/carl/ktransformers/ktransformers/.venv/lib/python3.11/site-packages/triton/compiler/compiler.py&amp;quot;, line 483, in _init_handles raise OutOfResources(self.metadata.shared, max_shared, &amp;quot;shared memory&amp;quot;) triton.runtime.errors.OutOfResources: out of resource: shared memory, Required: 106496, Hardware limit: 101376. Reducing block sizes or `num_stages` may help. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;System&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;EPYC &lt;del&gt;7B45&lt;/del&gt; 9B45 (128-core, 256 thread) CPU&lt;/li&gt; &lt;li&gt;768GB DDR5 6400 MT/s&lt;/li&gt; &lt;li&gt;4x RTX 6000 Pro Workstation 96GB GPUs&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Setup virtual python environment&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;mkdir sglang-ktransformers cd sglang-ktransformers uv venv --python 3.11 --seed . .venv/bin/activate &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Install sglang&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;uv pip install &amp;quot;sglang&amp;quot; --prerelease=allow &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Download and initialize ktransformers repo&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;git clone https://github.com/kvcache-ai/ktransformers cd ktransformers git submodule update --init --recursive &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Install ktransformers CPU kernel for sglang&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;cd kt-kernel export CPUINFER_CPU_INSTRUCT=AVX512 export CPUINFER_ENABLE_AMX=OFF uv pip install . cd .. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Download Kimi K2 Thinking GPU &amp;amp; CPU parts&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;uv pip install -U hf hf_transfer hf download moonshotai/Kimi-K2-Thinking hf download KVCache-ai/Kimi-K2-Thinking-CPU-weight &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Run k2&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES=0,1,2,3 python -m sglang.launch_server \ --host 0.0.0.0 --port 8080 \ --model ~/.cache/huggingface/hub/models--moonshotai--Kimi-K2-Thinking/snapshots/357b94aee9d50ec88e5e6dd9550fd7f957cb1baa \ --kt-amx-weight-path ~/.cache/huggingface/hub/models--KVCache-ai--Kimi-K2-Thinking-CPU-weight/snapshots/690ffacb9203d3b5e05ee8167ff1f5d4ae027c83 \ --kt-cpuinfer 252 \ --kt-threadpool-count 2 \ --kt-num-gpu-experts 238 \ --kt-amx-method AMXINT4 \ --attention-backend triton --trust-remote-code \ --mem-fraction-static 0.98 \ --chunked-prefill-size 4096 \ --max-running-requests 1 \ --max-total-tokens 32768 \ --enable-mixed-chunk \ --tensor-parallel-size 4 \ --enable-p2p-check \ --disable-shared-experts-fusion &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/__JockY__"&gt; /u/__JockY__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oquezp/kimi_k2_thinking_with_sglang_and_mixed_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oquezp/kimi_k2_thinking_with_sglang_and_mixed_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oquezp/kimi_k2_thinking_with_sglang_and_mixed_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T13:28:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1ordd5x</id>
    <title>[Web Demo] Qwen-Image-Edit ‚Äî Camera angle control (HF Space)</title>
    <updated>2025-11-08T02:10:18+00:00</updated>
    <author>
      <name>/u/freesysck</name>
      <uri>https://old.reddit.com/user/freesysck</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ordd5x/web_demo_qwenimageedit_camera_angle_control_hf/"&gt; &lt;img alt="[Web Demo] Qwen-Image-Edit ‚Äî Camera angle control (HF Space)" src="https://b.thumbs.redditmedia.com/cXAJ8Nam0JPRlqcjDrk_78dOvdtCDoLB1HC1snVFO1k.jpg" title="[Web Demo] Qwen-Image-Edit ‚Äî Camera angle control (HF Space)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Very Cool Tool.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/z1nmida0zxzf1.png?width=900&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fc5f06a9e2105a5db39e4644ef805606c65873eb"&gt;https://preview.redd.it/z1nmida0zxzf1.png?width=900&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fc5f06a9e2105a5db39e4644ef805606c65873eb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Upload an image, then tweak &lt;strong&gt;camera motion/rotation/lens&lt;/strong&gt; sliders to generate new viewpoints‚Äîright in your browser. &lt;a href="https://huggingface.co/spaces/linoyts/Qwen-Image-Edit-Angles?utm_source=chatgpt.com"&gt;Hugging Face&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Do things like move the camera (left/right/forward/down), rotate ¬±45¬∞/90¬∞ or go top-down, and switch between wide vs. close-up looks. &lt;/li&gt; &lt;li&gt;Built on &lt;strong&gt;Qwen Image Edit&lt;/strong&gt;; compatible community LoRAs enable multi-angle variants. &lt;/li&gt; &lt;li&gt;Tip: results can vary with busy backgrounds‚Äîshort prompts often work best.Try it: &lt;a href="https://huggingface.co/spaces/linoyts/Qwen-Image-Edit-Angles"&gt;&lt;code&gt;https://huggingface.co/spaces/linoyts/Qwen-Image-Edit-Angles&lt;/code&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/spaces/linoyts/Qwen-Image-Edit-Angles"&gt;Hugging Face&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/freesysck"&gt; /u/freesysck &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ordd5x/web_demo_qwenimageedit_camera_angle_control_hf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ordd5x/web_demo_qwenimageedit_camera_angle_control_hf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ordd5x/web_demo_qwenimageedit_camera_angle_control_hf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T02:10:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1or1e7p</id>
    <title>I fine-tuned Gemma 3 1B for CLI command translation... but it runs 100% locally. 810MB, 1.5s inference on CPU.</title>
    <updated>2025-11-07T17:58:57+00:00</updated>
    <author>
      <name>/u/theRealSachinSpk</name>
      <uri>https://old.reddit.com/user/theRealSachinSpk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or1e7p/i_finetuned_gemma_3_1b_for_cli_command/"&gt; &lt;img alt="I fine-tuned Gemma 3 1B for CLI command translation... but it runs 100% locally. 810MB, 1.5s inference on CPU." src="https://b.thumbs.redditmedia.com/k4PYFs253tXR75-utWF1-v10OmEqGwkzGrkkq8FHHVo.jpg" title="I fine-tuned Gemma 3 1B for CLI command translation... but it runs 100% locally. 810MB, 1.5s inference on CPU." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;I built a locally-running NL‚ÜíCLI translator by fine-tuning Gemma 3 1B with QLoRA.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/pranavkumaarofficial/nlcli-wizard"&gt;[Link to repo]&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; Built a privacy-first CLI copilot. No API calls, no subscriptions. Just 810MB of local AI that converts natural language to CLI commands.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/jpo4dd4jivzf1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e3aa7bc9af223d3ab2e4c3eb9156907994885cf5"&gt;https://preview.redd.it/jpo4dd4jivzf1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e3aa7bc9af223d3ab2e4c3eb9156907994885cf5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I wanted to try out something like a CLI wizard: running locally and loaded within the package. Now of course there is an overhead of embedding an SLM in every package.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;But definitely makes sense for complex, domain-specific tools with non-obvious CLI patterns&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Instead of: kubectl get pods -n production --field-selector status.phase=Running&lt;/p&gt; &lt;p&gt;Could be: kubectl -w &amp;quot;show me running pods in production&amp;quot;&lt;/p&gt; &lt;p&gt;Shell-GPT is the closest tool that is available but doesnt do what I wanted, and ofcourse uses closedsource LLMs&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Here is what I tried:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Takes natural language like &amp;quot;show my environments sorted by size&amp;quot; and outputs the correct CLI command, eg : &lt;code&gt;venvy ls --sort size&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key stats:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;~1.5s inference on CPU (4 threads)&lt;/li&gt; &lt;li&gt;810MB quantized model (Q4_K_M with smart fallback)&lt;/li&gt; &lt;li&gt;Trained on Colab T4 in &amp;lt;1 hr&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;The Setup&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Base model:&lt;/strong&gt; Gemma 3-1B-Instruct (March 2025 release)&lt;br /&gt; &lt;strong&gt;Training:&lt;/strong&gt; Unsloth + QLoRA (only 14M params trained, 1.29% of model)&lt;br /&gt; &lt;strong&gt;Hardware:&lt;/strong&gt; Free Colab T4, trained in under 1 hour&lt;br /&gt; &lt;strong&gt;Final model:&lt;/strong&gt; 810MB GGUF (Q4_K_M with smart fallback to Q5/Q6)&lt;br /&gt; &lt;strong&gt;Inference:&lt;/strong&gt; llama.cpp, ~1.5s on CPU (4 threads, M1 Mac / Ryzen)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The architecture part:&lt;/strong&gt; Used smart quantization with mixed precision (Q4_K/Q5_0/Q6_K) that adapts per-layer based on tensor dimensions. Some layers can't be quantized to 4-bit without accuracy loss, so llama.cpp automatically upgrades them to 5/6-bit.&lt;/p&gt; &lt;p&gt;Training loss was extremely clean - 0.135 (train), 0.142 (val) with zero overfitting across 3 epochs.&lt;/p&gt; &lt;p&gt;Limitations (being honest here)&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Model size:&lt;/strong&gt; 810MB is chunky. Too big for Docker images, fine for dev machines.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Tool-specific:&lt;/strong&gt; Currently only works for &lt;code&gt;venvy&lt;/code&gt;. Need to retrain for kubectl/docker/etc.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Latency:&lt;/strong&gt; 1.5s isn't instant. Experts will still prefer muscle memory.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Accuracy:&lt;/strong&gt; 80-85% means you MUST verify before executing.&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Safety&lt;/h1&gt; &lt;p&gt;Always asks for confirmation before executing. I'm not &lt;em&gt;that&lt;/em&gt; reckless.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;confirm = input(&amp;quot;Execute? [Y/n] &amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Still working on this : to check where this can really help, but yeah pls go check it out&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/pranavkumaarofficial/nlcli-wizard"&gt;[Link to repo]&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/theRealSachinSpk"&gt; /u/theRealSachinSpk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or1e7p/i_finetuned_gemma_3_1b_for_cli_command/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or1e7p/i_finetuned_gemma_3_1b_for_cli_command/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1or1e7p/i_finetuned_gemma_3_1b_for_cli_command/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T17:58:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqttg0</id>
    <title>Can someone explain what a Mixture-of-Experts model really is?</title>
    <updated>2025-11-07T13:02:36+00:00</updated>
    <author>
      <name>/u/Weebviir</name>
      <uri>https://old.reddit.com/user/Weebviir</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I've been aware of MoE since Deepseek dropped in the beginning of the year but I never really delved deep into what it is and how it helps in things like local AI inferencing. This sub's been very helpful with my local AI related questions so I wanted to learn from the people here.&lt;/p&gt; &lt;p&gt;Here are some more questions:&lt;br /&gt; - How does a model know when an expert is to be used?&lt;br /&gt; - Are MoE models really easier to run than traditional models?&lt;br /&gt; - How do Activation parameters really work? Do they affect fine tuning processes later?&lt;br /&gt; - Why do MoE models work better than traditional models?&lt;br /&gt; - What are ‚Äúsparse‚Äù vs ‚Äúdense‚Äù MoE architectures?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Weebviir"&gt; /u/Weebviir &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqttg0/can_someone_explain_what_a_mixtureofexperts_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqttg0/can_someone_explain_what_a_mixtureofexperts_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oqttg0/can_someone_explain_what_a_mixtureofexperts_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T13:02:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1or46rv</id>
    <title>Cerebras/Kimi-Linear-REAP-35B-A3B-Instruct ¬∑ Hugging Face</title>
    <updated>2025-11-07T19:44:19+00:00</updated>
    <author>
      <name>/u/maroule</name>
      <uri>https://old.reddit.com/user/maroule</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or46rv/cerebraskimilinearreap35ba3binstruct_hugging_face/"&gt; &lt;img alt="Cerebras/Kimi-Linear-REAP-35B-A3B-Instruct ¬∑ Hugging Face" src="https://external-preview.redd.it/A5NFpNf7XiO2gm9NBBYXrtttQxX4Zw8QmamAzVNdgao.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f4b51e11f834d5f8b364cbfd4018254e64276366" title="Cerebras/Kimi-Linear-REAP-35B-A3B-Instruct ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/maroule"&gt; /u/maroule &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/cerebras/Kimi-Linear-REAP-35B-A3B-Instruct"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or46rv/cerebraskimilinearreap35ba3binstruct_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1or46rv/cerebraskimilinearreap35ba3binstruct_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T19:44:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1or5j9z</id>
    <title>Nvidia may cancel the RTX 50 Super due to a shortage of 3GB GDDR7 memory</title>
    <updated>2025-11-07T20:36:28+00:00</updated>
    <author>
      <name>/u/Spiderboyz1</name>
      <uri>https://old.reddit.com/user/Spiderboyz1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For now it's just a rumor, but it seems the RTX Super cards will take a while to be released, if they ever are&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.techpowerup.com/342705/gddr7-shortage-could-stop-nvidia-geforce-rtx-50-series-super-rollout"&gt;https://www.techpowerup.com/342705/gddr7-shortage-could-stop-nvidia-geforce-rtx-50-series-super-rollout&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.guru3d.com/story/nvidia-may-cancel-or-delay-geforce-rtx-50-super-series-amid-gddr7-memory-shortage/"&gt;https://www.guru3d.com/story/nvidia-may-cancel-or-delay-geforce-rtx-50-super-series-amid-gddr7-memory-shortage/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And we also have RAM prices skyrocketing due to high demand&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Spiderboyz1"&gt; /u/Spiderboyz1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or5j9z/nvidia_may_cancel_the_rtx_50_super_due_to_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or5j9z/nvidia_may_cancel_the_rtx_50_super_due_to_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1or5j9z/nvidia_may_cancel_the_rtx_50_super_due_to_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T20:36:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1or4q4m</id>
    <title>Kimi K2 Thinking SECOND most intelligent LLM according to Artificial Analysis</title>
    <updated>2025-11-07T20:04:53+00:00</updated>
    <author>
      <name>/u/teatime1983</name>
      <uri>https://old.reddit.com/user/teatime1983</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or4q4m/kimi_k2_thinking_second_most_intelligent_llm/"&gt; &lt;img alt="Kimi K2 Thinking SECOND most intelligent LLM according to Artificial Analysis" src="https://b.thumbs.redditmedia.com/rRHDbssmheavBuel7zlrGe8NTyudGIzlKqGjsplYEaI.jpg" title="Kimi K2 Thinking SECOND most intelligent LLM according to Artificial Analysis" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/4l3lbor55wzf1.png?width=488&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b25065a1147654cbbf9b413b322560870a44d41e"&gt;https://preview.redd.it/4l3lbor55wzf1.png?width=488&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b25065a1147654cbbf9b413b322560870a44d41e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The Kimi K2 Thinking API pricing is $0.60 per million input tokens and $2.50 per million output tokens.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/teatime1983"&gt; /u/teatime1983 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or4q4m/kimi_k2_thinking_second_most_intelligent_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or4q4m/kimi_k2_thinking_second_most_intelligent_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1or4q4m/kimi_k2_thinking_second_most_intelligent_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T20:04:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1or8ehk</id>
    <title>Artificial Analysis has released a more in-depth benchmark breakdown of Kimi K2 Thinking (2nd image)</title>
    <updated>2025-11-07T22:30:30+00:00</updated>
    <author>
      <name>/u/averagebear_003</name>
      <uri>https://old.reddit.com/user/averagebear_003</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or8ehk/artificial_analysis_has_released_a_more_indepth/"&gt; &lt;img alt="Artificial Analysis has released a more in-depth benchmark breakdown of Kimi K2 Thinking (2nd image)" src="https://b.thumbs.redditmedia.com/UzJABCvVD7NHRog60608GSepBAlbSYFvvfYwhDULgvQ.jpg" title="Artificial Analysis has released a more in-depth benchmark breakdown of Kimi K2 Thinking (2nd image)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/averagebear_003"&gt; /u/averagebear_003 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1or8ehk"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or8ehk/artificial_analysis_has_released_a_more_indepth/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1or8ehk/artificial_analysis_has_released_a_more_indepth/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T22:30:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1or08aq</id>
    <title>OpenAI Pushes to Label Datacenters as ‚ÄòAmerican Manufacturing‚Äô Seeking Federal Subsidies After Preaching Independence</title>
    <updated>2025-11-07T17:15:05+00:00</updated>
    <author>
      <name>/u/Ok-Breakfast-4676</name>
      <uri>https://old.reddit.com/user/Ok-Breakfast-4676</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or08aq/openai_pushes_to_label_datacenters_as_american/"&gt; &lt;img alt="OpenAI Pushes to Label Datacenters as ‚ÄòAmerican Manufacturing‚Äô Seeking Federal Subsidies After Preaching Independence" src="https://preview.redd.it/jq1jrz6kbvzf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=258e2c389f69a6ed7cdcda3ea33b5a80b38a0fb3" title="OpenAI Pushes to Label Datacenters as ‚ÄòAmerican Manufacturing‚Äô Seeking Federal Subsidies After Preaching Independence" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;OpenAI is now lobbying to classify datacenter spending as ‚ÄúAmerican manufacturing.‚Äù&lt;/p&gt; &lt;p&gt;In their recent submission, they explicitly advocate for Federal loan guarantees the same kind used to subsidize large-scale industrial projects.&lt;/p&gt; &lt;p&gt;So after all the talk about independence and no need for government help‚Ä¶ Sam lied. Again.Ôøº&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Breakfast-4676"&gt; /u/Ok-Breakfast-4676 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jq1jrz6kbvzf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or08aq/openai_pushes_to_label_datacenters_as_american/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1or08aq/openai_pushes_to_label_datacenters_as_american/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T17:15:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1orlqmh</id>
    <title>Honey we shrunk MiniMax M2</title>
    <updated>2025-11-08T10:04:06+00:00</updated>
    <author>
      <name>/u/arjunainfinity</name>
      <uri>https://old.reddit.com/user/arjunainfinity</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orlqmh/honey_we_shrunk_minimax_m2/"&gt; &lt;img alt="Honey we shrunk MiniMax M2" src="https://external-preview.redd.it/ehjUFeUe3VAS2s784qCwiz1JNp5TRqMKwWsfk9mVQNM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a556feed177eef7b3a5785bd56ea678c06d8ea4e" title="Honey we shrunk MiniMax M2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi folks, we pruned MiniMax M2 from 250B to 192B (~25%) with only ~5% loss in coding quality. We did this with $200 worth of 8XH200 compute. Our 50% pruned model is ETA 5 more days. Would love to hear your feedback and would you want a 50% pruned Kimi K2 Thinking?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/arjunainfinity"&gt; /u/arjunainfinity &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/VibeStudio/thrift"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orlqmh/honey_we_shrunk_minimax_m2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1orlqmh/honey_we_shrunk_minimax_m2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T10:04:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1ordgys</id>
    <title>We got this, we can do it! When is the REAP‚Äôd iQ_001_XXS GGUF dropping?</title>
    <updated>2025-11-08T02:15:28+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ordgys/we_got_this_we_can_do_it_when_is_the_reapd_iq_001/"&gt; &lt;img alt="We got this, we can do it! When is the REAP‚Äôd iQ_001_XXS GGUF dropping?" src="https://preview.redd.it/qfahc43zzxzf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1975edd06526787672ea84d9ae1d9904b84715e1" title="We got this, we can do it! When is the REAP‚Äôd iQ_001_XXS GGUF dropping?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/qfahc43zzxzf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ordgys/we_got_this_we_can_do_it_when_is_the_reapd_iq_001/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ordgys/we_got_this_we_can_do_it_when_is_the_reapd_iq_001/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T02:15:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1olq14f</id>
    <title>[MEGATHREAD] Local AI Hardware - November 2025</title>
    <updated>2025-11-01T15:02:17+00:00</updated>
    <author>
      <name>/u/eck72</name>
      <uri>https://old.reddit.com/user/eck72</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is the monthly thread for sharing your local AI setups and the models you're running.&lt;/p&gt; &lt;p&gt;Whether you're using a single CPU, a gaming GPU, or a full rack, post what you're running and how it performs.&lt;/p&gt; &lt;p&gt;Post in any format you like. The list below is just a guide:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Hardware: CPU, GPU(s), RAM, storage, OS&lt;/li&gt; &lt;li&gt;Model(s): name + size/quant&lt;/li&gt; &lt;li&gt;Stack: (e.g. llama.cpp + custom UI)&lt;/li&gt; &lt;li&gt;Performance: t/s, latency, context, batch etc.&lt;/li&gt; &lt;li&gt;Power consumption&lt;/li&gt; &lt;li&gt;Notes: purpose, quirks, comments&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please share setup pics for eye candy!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Quick reminder&lt;/strong&gt;: You can share hardware purely to ask questions or get feedback. All experience levels welcome.&lt;/p&gt; &lt;p&gt;House rules: no buying/selling/promo.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eck72"&gt; /u/eck72 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T15:02:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqy1k7</id>
    <title>AMA Announcement: Moonshot AI, The Opensource Frontier Lab Behind Kimi K2 Thinking SoTA Model (Monday, 8AM-11AM PST)</title>
    <updated>2025-11-07T15:53:33+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqy1k7/ama_announcement_moonshot_ai_the_opensource/"&gt; &lt;img alt="AMA Announcement: Moonshot AI, The Opensource Frontier Lab Behind Kimi K2 Thinking SoTA Model (Monday, 8AM-11AM PST)" src="https://preview.redd.it/8v2luf5owuzf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b9bc34ec8dddd94422397eaa91e0310250da5ba3" title="AMA Announcement: Moonshot AI, The Opensource Frontier Lab Behind Kimi K2 Thinking SoTA Model (Monday, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8v2luf5owuzf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqy1k7/ama_announcement_moonshot_ai_the_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oqy1k7/ama_announcement_moonshot_ai_the_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T15:53:33+00:00</published>
  </entry>
</feed>
