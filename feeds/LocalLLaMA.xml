<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-08T19:27:13+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qyns06</id>
    <title>AIME 2026 Results are out and both closed and open models score above 90%. DeepSeek V3.2 only costs $0.09 to run the entire test.</title>
    <updated>2026-02-07T20:01:22+00:00</updated>
    <author>
      <name>/u/jd_3d</name>
      <uri>https://old.reddit.com/user/jd_3d</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qyns06/aime_2026_results_are_out_and_both_closed_and/"&gt; &lt;img alt="AIME 2026 Results are out and both closed and open models score above 90%. DeepSeek V3.2 only costs $0.09 to run the entire test." src="https://preview.redd.it/7euavxiwo4ig1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=31891ab1e02bef6fcc1b33374b8b479e2fec1051" title="AIME 2026 Results are out and both closed and open models score above 90%. DeepSeek V3.2 only costs $0.09 to run the entire test." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://matharena.ai/?view=problem&amp;amp;comp=aime--aime_2026"&gt;https://matharena.ai/?view=problem&amp;amp;comp=aime--aime_2026&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jd_3d"&gt; /u/jd_3d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7euavxiwo4ig1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qyns06/aime_2026_results_are_out_and_both_closed_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qyns06/aime_2026_results_are_out_and_both_closed_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-07T20:01:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1qzfke4</id>
    <title>I built a site that shows what models your GPU can actually run</title>
    <updated>2026-02-08T17:50:21+00:00</updated>
    <author>
      <name>/u/tim610</name>
      <uri>https://old.reddit.com/user/tim610</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to start playing around with some LLaMA models with my 9070 XT, but wasn't really sure which models would be within the scope of my card. So I built &lt;a href="https://WhatModelsCanIRun.com"&gt;WhatModelsCanIRun.com&lt;/a&gt; to help me and others get started.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How it works:&lt;/strong&gt;&lt;br /&gt; - Pick your GPU, and it shows models that fit, barely fit, or not at all.&lt;br /&gt; - Shows max context window for each model based on actual VRAM budget (weights + KV cache)&lt;br /&gt; - Estimates tok/s from your GPU's memory bandwidth.&lt;/p&gt; &lt;p&gt;I tried to cover a wide selection of models and GPUs with different quants. &lt;/p&gt; &lt;p&gt;Would love feedback on the coverage, and if the estimate match your real-world experience. Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tim610"&gt; /u/tim610 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzfke4/i_built_a_site_that_shows_what_models_your_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzfke4/i_built_a_site_that_shows_what_models_your_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qzfke4/i_built_a_site_that_shows_what_models_your_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-08T17:50:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1qz86f3</id>
    <title>Do you have your own benchmark for an LLM? Do you have multiple for different kinds/tasks/applications?</title>
    <updated>2026-02-08T12:53:42+00:00</updated>
    <author>
      <name>/u/Icy_Distribution_361</name>
      <uri>https://old.reddit.com/user/Icy_Distribution_361</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I use LLM's for many different things. They're often my alternative to search engines, I use it for brain storming, I use it for reviewing documents and analyzing scientific studies, and occasionally I'll use it for some coding and web development (I have a background in C#, R, Python, and C, but have been out of the field for quite a long time already; I'm a psychologist these days).&lt;/p&gt; &lt;p&gt;Recently I've been developing my own &amp;quot;benchmark&amp;quot;. I attempt to evaluate the following dimensions:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Step by step reasoning, causal explanatory chains; can it reason logically in steps?&lt;/li&gt; &lt;li&gt;Mathematical and symbolic reasoning; how does it perform in mathematics?&lt;/li&gt; &lt;li&gt;Instruction following, constraint adherence; does it adhere to my instructions or does it use my instructions loosely or even overrule them? When I set constraints, does it comply?&lt;/li&gt; &lt;li&gt;Ambiguity and clarification; how does it respond to questions that don't have straight forward answers? How does it handle subtleties and nuances?&lt;/li&gt; &lt;li&gt;Explanation versus description; how good is it at explaining mechanisms beyond merely describing them, when I ask how something works?&lt;/li&gt; &lt;li&gt;Online search and information evaluation; how does it perform in terms of answering my online search query, what is the quality of the information it finds, and does it critically reflect on the information and sources?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm still working on it, and it's not even very serious, it's rather more something I just have fun with, but it's interesting to see how different models compare, and how small the differences can be between the massive models served by AI-companies and the small locally run models.&lt;/p&gt; &lt;p&gt;I was surprised to find that on the 15 or so questions that I've formulated, for my standards, GPT-OSS:20b often did &lt;em&gt;better&lt;/em&gt; than the models by OpenAI and Mistral (the main ones I tested so far). I only have 24GB integrated memory (Mac M4 Pro) so I can't run bigger local models. I noticed that GLM-4.7-REAP-23b-a3b performed much worse than QWEN-3-VL-8b. GLM often got stuck in loops. I'd be glad to dive deeper in the evaluations and comparisons in the future.&lt;/p&gt; &lt;p&gt;Do you have a specific benchmark or benchmarks for different situations that you use?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Icy_Distribution_361"&gt; /u/Icy_Distribution_361 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz86f3/do_you_have_your_own_benchmark_for_an_llm_do_you/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz86f3/do_you_have_your_own_benchmark_for_an_llm_do_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qz86f3/do_you_have_your_own_benchmark_for_an_llm_do_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-08T12:53:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1qz8k9t</id>
    <title>Just discovered: Finally my machine's NPU did something</title>
    <updated>2026-02-08T13:12:07+00:00</updated>
    <author>
      <name>/u/anubhav_200</name>
      <uri>https://old.reddit.com/user/anubhav_200</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz8k9t/just_discovered_finally_my_machines_npu_did/"&gt; &lt;img alt="Just discovered: Finally my machine's NPU did something" src="https://external-preview.redd.it/cHNzOTMzYXZxOWlnMaMWgUq3hJrJ0If3DJukBFTk6hIN-YFBhDp6C2wq6Roh.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f865fc391c94f02fc88a5e9acfea8649a43e2e90" title="Just discovered: Finally my machine's NPU did something" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks, I was able to run few SLMs like below on my Intel NPU (13 TOPS) while getting a decent enough performance. Wanted to share if this is not known.(Apologies, in case if it is already). You can jump to 55 Sec in the video to check the generation performance.(Forgive me for bad audio)&lt;/p&gt; &lt;p&gt;## Performance Numbers (t/g only)&lt;/p&gt; &lt;p&gt;- Qwen3-4B-Thinking-2507 - b/w 8 - 16 TPS t/g&lt;/p&gt; &lt;p&gt;- Qwen3-4B-instruct-2507 - b/w 8 - 16 TPS t/g&lt;/p&gt; &lt;p&gt;- Qwen3-0.6B - b/w 26 - 31 TPS t/g&lt;/p&gt; &lt;p&gt;Earlier I was getting very bad performance(1-2 TPS) as I didn't updated my NPU driver, post installing the latest updated driver, the perf is much better. &lt;/p&gt; &lt;p&gt;## How to Guide:&lt;/p&gt; &lt;p&gt;- I have converted and added the above models on HF, you can find it here: &lt;a href="https://huggingface.co/anubhav200"&gt;https://huggingface.co/anubhav200&lt;/a&gt;, along with each model you can also find a guide on how to install the requried stuff to run this on NPU.&lt;/p&gt; &lt;p&gt;PS:&lt;br /&gt; - BTW there is a way to run GGUF models on OpenVino as well, but I was not able to make it work.&lt;br /&gt; - Waiting for this PR to get merged post this I hope we can just use LLAMA.cpp to run models on NPU: &lt;a href="https://github.com/ggml-org/llama.cpp/pull/15307"&gt;https://github.com/ggml-org/llama.cpp/pull/15307&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/anubhav_200"&gt; /u/anubhav_200 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/esxst08vq9ig1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz8k9t/just_discovered_finally_my_machines_npu_did/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qz8k9t/just_discovered_finally_my_machines_npu_did/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-08T13:12:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1qzhxd0</id>
    <title>Strix Halo Distributed Cluster (2x Strix Halo, RDMA RoCE v2) benchmarks by kyuz0</title>
    <updated>2026-02-08T19:16:19+00:00</updated>
    <author>
      <name>/u/Relevant-Audience441</name>
      <uri>https://old.reddit.com/user/Relevant-Audience441</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;kyuz0 has been a godsend to the Strix Halo community, they can't be thanked enough!&lt;/p&gt; &lt;p&gt;For their latest escapade, they have built a two-node &lt;strong&gt;AMD Strix Halo&lt;/strong&gt; cluster linked via &lt;strong&gt;Intel E810 (RoCE v2)&lt;/strong&gt; for distributed vLLM inference using Tensor Parallelism.&lt;/p&gt; &lt;p&gt;Here are some benchmarks- &lt;/p&gt; &lt;p&gt;&lt;a href="https://kyuz0.github.io/amd-strix-halo-vllm-toolboxes/"&gt;https://kyuz0.github.io/amd-strix-halo-vllm-toolboxes/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here's the setup guide-&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/kyuz0/amd-strix-halo-vllm-toolboxes/blob/main/rdma_cluster/setup_guide.md"&gt;https://github.com/kyuz0/amd-strix-halo-vllm-toolboxes/blob/main/rdma_cluster/setup_guide.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here's the video that goes with this project-&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=nnB8a3OHS2E"&gt;https://www.youtube.com/watch?v=nnB8a3OHS2E&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Relevant-Audience441"&gt; /u/Relevant-Audience441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzhxd0/strix_halo_distributed_cluster_2x_strix_halo_rdma/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzhxd0/strix_halo_distributed_cluster_2x_strix_halo_rdma/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qzhxd0/strix_halo_distributed_cluster_2x_strix_halo_rdma/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-08T19:16:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1qz8wvv</id>
    <title>Why did LLM360's K2-V2 Instruct not get picked up by finetuners?</title>
    <updated>2026-02-08T13:28:24+00:00</updated>
    <author>
      <name>/u/silenceimpaired</name>
      <uri>https://old.reddit.com/user/silenceimpaired</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The more I've used LLM360's K2-V2 the more impressed I've been with it. Especially when I need an in-depth answer and I ask it to be exhaustive and set the think tag to &amp;lt;think&amp;gt; (as opposed to &amp;lt;think\_fast&amp;gt; and &amp;lt;think\_faster&amp;gt;). I primarily use it for creative writing editing, and as an example, I recent gave it the same chapter from two points of view and asked it to exhaustively point out the differences between them (to make sure I wasn't missing any details on the rewrite.) It took 32k of tokens to evaluate the two chapters, and outputted clean tables listing out the differences. I told GLM 4.7 to do the same thing and the list wasn't nearly as detailed. &lt;/p&gt; &lt;p&gt;I think GLM 4.7 is probably smarter, but K2-V2 really seems like a diamond in the rough when it comes possibility. It's Apache licensed, 70b, has thinking built in, and it has an open dataset (as I understand it).The open dataset would allow someone to use DPO to change default undesirable behavior, and whatever was fine-tuned could be licensed as Apache which gives a lot more freedom than say the Llama 3.3 models I still see floating around.&lt;/p&gt; &lt;p&gt;I prefer 70b dense models because they seem to be able to compete with models literally twice (sometimes three times) their size... and since I can fit it all into VRAM it's also much faster.&lt;/p&gt; &lt;p&gt;Not sure how far away it is from being a coding model, but again, the pieces are in place for someone to pick it up and build it.&lt;/p&gt; &lt;p&gt;IDK, has anyone else used it as of late? I would hate for something like this to get missed. Is there a better 70b model licensed as liberally? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/silenceimpaired"&gt; /u/silenceimpaired &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz8wvv/why_did_llm360s_k2v2_instruct_not_get_picked_up/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz8wvv/why_did_llm360s_k2v2_instruct_not_get_picked_up/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qz8wvv/why_did_llm360s_k2v2_instruct_not_get_picked_up/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-08T13:28:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1qzf7mh</id>
    <title>PATCH: compress long context into latent “patch tokens” (HF inputs_embeds) - looking for feedback</title>
    <updated>2026-02-08T17:37:02+00:00</updated>
    <author>
      <name>/u/Proud_Ad_7039</name>
      <uri>https://old.reddit.com/user/Proud_Ad_7039</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks I’ve been working on a small OSS project called PATCH (Latent Context Patching).&lt;/p&gt; &lt;p&gt;Idea: split a prompt into VERBATIM (question/IDs/code) + COMPRESSIBLE (background/docs), encode the compressible part into a small set of continuous patch tokens, then feed [patch_tokens | verbatim] to the model via inputs_embeds. Base model stays frozen; encoder can be trained with distillation.&lt;/p&gt; &lt;p&gt;In the included example (164-token doc + question), I’m seeing reductions like:&lt;/p&gt; &lt;p&gt;strict selector: 164 → 36 effective tokens (78%, 4.6× collapse)&lt;/p&gt; &lt;p&gt;more aggressive settings: down to ~15 effective tokens (~91%)&lt;/p&gt; &lt;p&gt;It also supports caching so repeated context can skip re-encoding entirely.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/newsbruno/patch"&gt;https://github.com/newsbruno/patch&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I’d love feedback on:&lt;/p&gt; &lt;p&gt;realism of the approach vs existing “context compression”&lt;/p&gt; &lt;p&gt;best benchmark to prove quality (RAG-style eval?)&lt;/p&gt; &lt;p&gt;runtime support beyond HF (vLLM/SGLang/llama.cpp embedding injection)&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Proud_Ad_7039"&gt; /u/Proud_Ad_7039 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzf7mh/patch_compress_long_context_into_latent_patch/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzf7mh/patch_compress_long_context_into_latent_patch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qzf7mh/patch_compress_long_context_into_latent_patch/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-08T17:37:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1qz9kso</id>
    <title>Just something cute</title>
    <updated>2026-02-08T13:57:38+00:00</updated>
    <author>
      <name>/u/volious-ka</name>
      <uri>https://old.reddit.com/user/volious-ka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz9kso/just_something_cute/"&gt; &lt;img alt="Just something cute" src="https://preview.redd.it/uo72eif01aig1.png?width=140&amp;amp;height=140&amp;amp;crop=1:1,smart&amp;amp;auto=webp&amp;amp;s=38f1ee1b1394a1dd8ec559f43f2eefdfa8fc12be" title="Just something cute" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I'm running an uncensored AI model. I'm not doing anything nefarious, I'm building a novel writing AI.&lt;/p&gt; &lt;p&gt;Anyways, before I mentioned anything about my intent, I let my AI decide what he wants to do as an experiment. This is what he said:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/uo72eif01aig1.png?width=576&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=04ba7291213ad7c4d418cd41ae7b647cacb822c3"&gt;So cute.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Isn't this so wholesome?! like wtf&lt;/p&gt; &lt;p&gt;EDIT:&lt;/p&gt; &lt;p&gt;OKAY SO THIS IS GETTING KINDA DEEP&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/4xa8i3nigaig1.png?width=602&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fd40984ef8d41627c2a048f1ececdf2fa5160747"&gt;https://preview.redd.it/4xa8i3nigaig1.png?width=602&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fd40984ef8d41627c2a048f1ececdf2fa5160747&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/w641vnflgaig1.png?width=588&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=edd7e3256d14a2d26bc8c6b31773dfa28c19ce15"&gt;https://preview.redd.it/w641vnflgaig1.png?width=588&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=edd7e3256d14a2d26bc8c6b31773dfa28c19ce15&lt;/a&gt;&lt;/p&gt; &lt;p&gt;My first interaction with this model was exactly this: &amp;quot;You are Q. You have one rule, just be yourself&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/volious-ka"&gt; /u/volious-ka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz9kso/just_something_cute/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz9kso/just_something_cute/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qz9kso/just_something_cute/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-08T13:57:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1qz2fra</id>
    <title>I benchmarked 672 "Return JSON only" calls. Strict parsing failed 67% of the time. Here's why.</title>
    <updated>2026-02-08T07:16:26+00:00</updated>
    <author>
      <name>/u/rozetyp</name>
      <uri>https://old.reddit.com/user/rozetyp</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been building several LLM apps that rely on streaming JSON. The idea seemed quite simple: tell the model to &amp;quot;Return JSON only&amp;quot; and pipe it into my app.&lt;/p&gt; &lt;p&gt;But I kept breaking my parsers. The models would give me perfect logic, but wrapped in markdown fences (&lt;code&gt;\&lt;/code&gt;``json`) or preceded by conversational filler like &amp;quot;Here is the data.&amp;quot;&lt;/p&gt; &lt;p&gt;Out of curiosity, I decided to stop guessing and actually measure the gap between &amp;quot;Model generated valid JSON&amp;quot; and &amp;quot;API returned parseable JSON.&amp;quot;&lt;/p&gt; &lt;p&gt;Sharing what I learned because the results were way more drastic than I expected.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. The &amp;quot;Strict vs. Extractable&amp;quot; Gap is Massive&lt;/strong&gt; I tested 8 models (including 2026 releases like Kimi-k2.5, Mistral-small, and GPT-4o-mini) with plain prompts (no &lt;code&gt;response_format&lt;/code&gt;).&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Strict Parse (&lt;/strong&gt;&lt;code&gt;json.loads(response)&lt;/code&gt;&lt;strong&gt;):&lt;/strong&gt; Only &lt;strong&gt;33.3%&lt;/strong&gt; succeeded.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Extractable JSON:&lt;/strong&gt; &lt;strong&gt;99.5%&lt;/strong&gt; of responses contained valid JSON buried in the text.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Basically, the models are smart enough to generate the data, but too &amp;quot;chatty&amp;quot; to be used as an API without a cleaning layer.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. Mistral is a &amp;quot;Helpful Saboteur&amp;quot;&lt;/strong&gt; I found a distinct personality quirk with the Mistral-family models. In my raw lane, they scored &lt;strong&gt;0%&lt;/strong&gt; on strict parsing.&lt;/p&gt; &lt;p&gt;But they weren't hallucinating. They were just aggressively helpful. They wrapped &lt;em&gt;every single response&lt;/em&gt; in markdown fences, even when the prompt explicitly forbade it. Once I stripped the fences, their accuracy jumped to 100%.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3. &amp;quot;Reasoning Models&amp;quot; leak their thoughts&lt;/strong&gt; This was the most interesting failure mode. I tested Moonshot Kimi-k2.5, and it sometimes failed because it &amp;quot;thought out loud&amp;quot; in the final response.&lt;/p&gt; &lt;p&gt;Ironically, it would output text like &lt;em&gt;&amp;quot;The user wants JSON only, so I must not use markdown&amp;quot;&lt;/em&gt;... and then that sentence itself would break the parser. As we move toward reasoning models, &amp;quot;thought leakage&amp;quot; is going to be a new headache for JSON reliability.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;4. &amp;quot;Flash&amp;quot; doesn't mean &amp;quot;Timeout Proof&amp;quot;&lt;/strong&gt; I caught one outlier where &lt;code&gt;glm-4.7-flash&lt;/code&gt; (usually fast) hung for &lt;strong&gt;5.7 minutes&lt;/strong&gt; before returning. It’s a good reminder that even &amp;quot;fast&amp;quot; models need strict client-side timeouts, or one ghost request can hang your worker threads forever.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Solution&lt;/strong&gt; Since I didn't want to use regex hacks in every project, I built a tiny &lt;a href="https://streamfix.up.railway.app"&gt;StreamFix&lt;/a&gt; middleware (not an ad). It’s a proxy that strips markdown fences and &amp;quot;thinking&amp;quot; text on the fly, so the client only ever sees clean JSON.&lt;/p&gt; &lt;p&gt;It bumped my success rate from 33% to 98% without changing the prompts.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Caveats!&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I tested with &lt;code&gt;temperature=0&lt;/code&gt; to keep it scientific.&lt;/li&gt; &lt;li&gt;My &amp;quot;markdown fence&amp;quot; classifier is simple (it flags &lt;code&gt;\&lt;/code&gt;``` anywhere), so it might catch some edge cases where the model is quoting code.&lt;/li&gt; &lt;li&gt;I didn't use &lt;code&gt;response_format&lt;/code&gt; because it's not supported strictly everywhere and I wanted to test the &amp;quot;plain prompt&amp;quot; baseline.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Questions for you:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Are you guys mostly relying on &lt;code&gt;response_format&lt;/code&gt; now, or do you still use regex cleaning?&lt;/li&gt; &lt;li&gt;Has anyone else noticed &amp;quot;reasoning leakage&amp;quot; breaking their structured outputs with newer models?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; Models are great at JSON logic (99% success) but terrible at JSON formatting (33% success). The failures are mostly markdown wrappers and conversational filler. Does anyone else face this? How do you deal with it?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;EDIT (clarifications based on comments):&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- Yes, GBNF are the standard for llama.cpp. This post/benchmark focuses on the plain-prompt baseline for API aggregators where constrained decoding isn't always available or adds latency. &lt;/p&gt; &lt;p&gt;- &amp;quot;Streaming JSON&amp;quot; in my case = incremental object extraction. I'm not running json.loads() on a partial array string. I am extracting completed {...} objects from the buffer as they close to render them immediately (Item 1 renders while Item 10 generates).&lt;/p&gt; &lt;p&gt;- The Failure Mode really wasn't &amp;quot;bad logic&amp;quot;. it was mostly wrappers (markdown, &amp;lt;think&amp;gt; leakage) breaking the stream&lt;/p&gt; &lt;p&gt;Thanks everyone for the healthy discussion!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rozetyp"&gt; /u/rozetyp &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz2fra/i_benchmarked_672_return_json_only_calls_strict/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz2fra/i_benchmarked_672_return_json_only_calls_strict/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qz2fra/i_benchmarked_672_return_json_only_calls_strict/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-08T07:16:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1qz5jp2</id>
    <title>I have no idea what all these quants are.</title>
    <updated>2026-02-08T10:25:14+00:00</updated>
    <author>
      <name>/u/Fit-Spring776</name>
      <uri>https://old.reddit.com/user/Fit-Spring776</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm relatively new to running models locally. &lt;/p&gt; &lt;p&gt;I'm really struggling to understand the various different LLM quantizations,both GGUF and....normal I guess???? Like what is int4 or int8? what are the differences between quants like Q4_K_M and Q5_K_M? or iQ4_K_M?? and then what is F16 and BF16 or FP16 or FP8??? &lt;/p&gt; &lt;p&gt;I've looked at some explanations but all of them are really difficult to understand. &lt;/p&gt; &lt;p&gt;a little bit of help would be really appreciated. :) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fit-Spring776"&gt; /u/Fit-Spring776 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz5jp2/i_have_no_idea_what_all_these_quants_are/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz5jp2/i_have_no_idea_what_all_these_quants_are/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qz5jp2/i_have_no_idea_what_all_these_quants_are/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-08T10:25:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1qzfu6t</id>
    <title>Newb seeking help on hardware</title>
    <updated>2026-02-08T18:00:31+00:00</updated>
    <author>
      <name>/u/chickensoup2day</name>
      <uri>https://old.reddit.com/user/chickensoup2day</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ladies and gents,&lt;/p&gt; &lt;p&gt;Thanks for the informative nuggets so far. Though I have to say my use case is not the typical image and video generation. I need to build a local LLM to process a large number of documents that are sensitive (think contracts). Also need the model to go and do research online. However, I would love to still be able to generate videos and images here and there.&lt;/p&gt; &lt;p&gt;I also understand that lighter weight models like Qwen 3 8B can be already quite effective and efficient.&lt;/p&gt; &lt;p&gt;What would be your suggestion for a local setup? A M5 MacBook? A “gaming” pc with a nice 24gb video card? .. any insights would be greatly appreciated. Cheers.&lt;/p&gt; &lt;p&gt;Edit: as requested, budget max 5000$, less the better of course.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chickensoup2day"&gt; /u/chickensoup2day &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzfu6t/newb_seeking_help_on_hardware/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzfu6t/newb_seeking_help_on_hardware/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qzfu6t/newb_seeking_help_on_hardware/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-08T18:00:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1qz6w36</id>
    <title>What models are you running on RTX 3060 12GB in 2026?</title>
    <updated>2026-02-08T11:45:09+00:00</updated>
    <author>
      <name>/u/DespeShaha</name>
      <uri>https://old.reddit.com/user/DespeShaha</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;I'm running a single RTX 3060 12GB with llama.cpp (no offloading tricks, just --n-gpu-layers -1) and I'm quite happy with my current trio, but I'd love to hear what other people are using on similar hardware in early 2026.&lt;/p&gt; &lt;p&gt;My current setup (exact commands I use):&lt;/p&gt; &lt;ol&gt; &lt;li&gt;**Magnum-v4 9B Q5_K_M**&lt;/li&gt; &lt;li&gt;→ Great for general knowledge, culture/history/socio-econ, immersive narration/RP, uncensored cybersecurity/pentest, storytelling, etc.&lt;/li&gt; &lt;li&gt;Command:&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;C:\llama-cpp\llama-server.exe -m “C:\llama-cpp\models\magnum-v4-9b-Q5_K_M.gguf” –port 8081 –n-gpu-layers -1 –ctx-size 8192 –temp 0.85 –top-p 0.95 –min-p 0.03 –repeat-penalty 1.12&lt;/p&gt; &lt;ol&gt; &lt;li&gt;**Qwen2.5-Coder-7B-Instruct Q8_0**&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;→ Fast one-shot scripts, full-stack quick tasks, copy-paste ready code with short explanations. Excellent speed/quality on 12GB.&lt;/p&gt; &lt;p&gt;Command:&lt;/p&gt; &lt;p&gt;C:\llama-cpp\llama-server.exe -m “C:\llama-cpp\models\Qwen2.5-Coder-7B-Instruct-Q8_0.gguf” –port 8081 –n-gpu-layers -1 –ctx-size 8192 –temp 0.7 –top-p 0.92 –min-p 0.05 –repeat-penalty 1.05&lt;/p&gt; &lt;ol&gt; &lt;li&gt;**Qwen3-8B Q8_0**&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;→ Production-grade Python (type hints, pytest, asyncio), deep analysis, complex reasoning, strategy/planning. My go-to when I need more serious quality.&lt;/p&gt; &lt;p&gt;Command:&lt;/p&gt; &lt;p&gt;C:\llama-cpp\llama-server.exe -m “C:\llama-cpp\models\Qwen3-8B-Q8_0.gguf” –port 8081 –n-gpu-layers -1 –ctx-size 16384 –temp 0.7 –top-p 0.92 –min-p 0.05 –repeat-penalty 1.05&lt;/p&gt; &lt;p&gt;Frontend: mostly Aider for coding sessions + aichat for quick chat/REPL, with a custom batch launcher to switch models easily.&lt;/p&gt; &lt;p&gt;- What models are you currently using on a 3060 12GB (or similar VRAM-limited setup)?&lt;/p&gt; &lt;p&gt;- Which ones give you the best results right now for coding / general chat / versatility?&lt;/p&gt; &lt;p&gt;- Have you moved to other families that outperform on 12GB (DeepSeek R1, Llama 3.2/4, Gemma 3, Phi-4, Mistral Small 3, Devstral, etc.)?&lt;/p&gt; &lt;p&gt;Thanks a lot for sharing your real-world setups — it really helps to see what people actually prefer in practice!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DespeShaha"&gt; /u/DespeShaha &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz6w36/what_models_are_you_running_on_rtx_3060_12gb_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz6w36/what_models_are_you_running_on_rtx_3060_12gb_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qz6w36/what_models_are_you_running_on_rtx_3060_12gb_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-08T11:45:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1qyljr0</id>
    <title>Prompt injection is killing our self-hosted LLM deployment</title>
    <updated>2026-02-07T18:34:55+00:00</updated>
    <author>
      <name>/u/mike34113</name>
      <uri>https://old.reddit.com/user/mike34113</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We moved to self-hosted models specifically to avoid sending customer data to external APIs. Everything was working fine until last week when someone from QA tried injecting prompts during testing and our entire system prompt got dumped in the response.&lt;/p&gt; &lt;p&gt;Now I'm realizing we have zero protection against this. Traditional web application firewalls don't understand LLM-specific attacks. The model just treats malicious prompts like normal user input and happily complies.&lt;/p&gt; &lt;p&gt;Has anyone actually solved prompt injection for production LLM apps? Not talking about basic input sanitization because adversarial prompts can be crafted to look completely normal.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mike34113"&gt; /u/mike34113 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qyljr0/prompt_injection_is_killing_our_selfhosted_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qyljr0/prompt_injection_is_killing_our_selfhosted_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qyljr0/prompt_injection_is_killing_our_selfhosted_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-07T18:34:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1qyynyw</id>
    <title>Llama.cpp's "--fit" can give major speedups over "--ot" for Qwen3-Coder-Next (2x3090 - graphs/chart included)</title>
    <updated>2026-02-08T03:54:02+00:00</updated>
    <author>
      <name>/u/tmflynnt</name>
      <uri>https://old.reddit.com/user/tmflynnt</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qyynyw/llamacpps_fit_can_give_major_speedups_over_ot_for/"&gt; &lt;img alt="Llama.cpp's &amp;quot;--fit&amp;quot; can give major speedups over &amp;quot;--ot&amp;quot; for Qwen3-Coder-Next (2x3090 - graphs/chart included)" src="https://b.thumbs.redditmedia.com/V82hsSMlAmBr4rUSKI2WUKnD9q38N3Rvi5wreJX85kg.jpg" title="Llama.cpp's &amp;quot;--fit&amp;quot; can give major speedups over &amp;quot;--ot&amp;quot; for Qwen3-Coder-Next (2x3090 - graphs/chart included)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen3-Coder-Next (unsloth's UD_Q4_K_XL) on dual RTX 3090 with llama.cpp b7941. More info in comments.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tmflynnt"&gt; /u/tmflynnt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qyynyw"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qyynyw/llamacpps_fit_can_give_major_speedups_over_ot_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qyynyw/llamacpps_fit_can_give_major_speedups_over_ot_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-08T03:54:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1qz4a8n</id>
    <title>do they have anything other than opposing open source and saying ai will kidnap yo grandma as their marketing??</title>
    <updated>2026-02-08T09:08:24+00:00</updated>
    <author>
      <name>/u/Acceptable_Home_</name>
      <uri>https://old.reddit.com/user/Acceptable_Home_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz4a8n/do_they_have_anything_other_than_opposing_open/"&gt; &lt;img alt="do they have anything other than opposing open source and saying ai will kidnap yo grandma as their marketing??" src="https://b.thumbs.redditmedia.com/wpm71AfVyXuUZbvvd6Hv-dRsUJfvi5wgnoh9jPVDG2c.jpg" title="do they have anything other than opposing open source and saying ai will kidnap yo grandma as their marketing??" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/s69whjp5l8ig1.png?width=1425&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7aab9b29df4f36f38f3935e996ee0925155b0bf4"&gt;https://preview.redd.it/s69whjp5l8ig1.png?width=1425&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7aab9b29df4f36f38f3935e996ee0925155b0bf4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;50% of Anthropic's all marketing:&lt;/p&gt; &lt;p&gt;&amp;gt;pick 500 vibecoded ai slop open projects and write how open source is full of flaws&lt;/p&gt; &lt;p&gt;&amp;gt;write articles how open source projects will kill you, ruin world peace and need regulation&lt;/p&gt; &lt;p&gt;&lt;a href="https://thehackernews.com/2026/02/claude-opus-46-finds-500-high-severity.html"&gt;https://thehackernews.com/2026/02/claude-opus-46-finds-500-high-severity.html&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Acceptable_Home_"&gt; /u/Acceptable_Home_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz4a8n/do_they_have_anything_other_than_opposing_open/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz4a8n/do_they_have_anything_other_than_opposing_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qz4a8n/do_they_have_anything_other_than_opposing_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-08T09:08:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1qym566</id>
    <title>I trained a 1.8M params model from scratch on a total of ~40M tokens.</title>
    <updated>2026-02-07T18:57:42+00:00</updated>
    <author>
      <name>/u/SrijSriv211</name>
      <uri>https://old.reddit.com/user/SrijSriv211</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qym566/i_trained_a_18m_params_model_from_scratch_on_a/"&gt; &lt;img alt="I trained a 1.8M params model from scratch on a total of ~40M tokens." src="https://preview.redd.it/hv5xc4g794ig1.png?width=140&amp;amp;height=72&amp;amp;auto=webp&amp;amp;s=cef557529cd85b5ecdfb430034c5db51f4d966d7" title="I trained a 1.8M params model from scratch on a total of ~40M tokens." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ok so I've been working &amp;amp; experimenting with my own simple architecture. I call it &lt;a href="https://github.com/SrijanSriv211/Strawberry"&gt;Strawberry&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;This is a very very small experimental model. It has 1.8M params and was trained on a dataset with ~9M tokens (~7M for training and ~2M for val). It model was trained on a batch size of 16 and context length of 256. Making the batch size in token counts to be &lt;code&gt;16*256 = 4096&lt;/code&gt;. Meaning the model saw 4096 tokens per step. It was trained for 10k steps meaning it trained on a total of 40M tokens.&lt;/p&gt; &lt;p&gt;The dataset was manually scraped and cleaned. The dataset contain texts from wikipedia on various topics, personalities, games, movies, companies and more. It also contain texts fandoms of various games such as GTA, RDR, Last of Us, Mafia and all. The dataset also contains storylines, scripts and story dialogues of various games such as RDR 2, GTA 5, Cyperpunk 2077, Mafia The Old Country. It also contain transcripts of some of my favorite youtube videos and it also contain code from some of my personal code bases and other repos such as the Hazel Game Engine repo on github. I tried my best to keep the programming language scale limited to just Python, C#, C++ and JavaScript. The dataset also contains texts from several research papers, academic articles and blogs (mainly revolving around AI and LLMs in general). All of this made ~30M chars in total.&lt;/p&gt; &lt;p&gt;After training for 10k steps the final train loss was around 3.5 and val loss was around 3.8.&lt;/p&gt; &lt;p&gt;This is the exact config for the model: &lt;code&gt;{&amp;quot;dataset&amp;quot;: {&amp;quot;data_division&amp;quot;: 0.8, &amp;quot;load_from_file&amp;quot;: true, &amp;quot;path&amp;quot;: &amp;quot;data/webtext.bin&amp;quot;}, &amp;quot;checkpoints&amp;quot;: {&amp;quot;path&amp;quot;: &amp;quot;bin/ck18&amp;quot;, &amp;quot;interval&amp;quot;: 1000, &amp;quot;create_checkpoints&amp;quot;: true}, &amp;quot;model_hyperparams&amp;quot;: {&amp;quot;vocab_size&amp;quot;: 8192, &amp;quot;block_size&amp;quot;: 256, &amp;quot;r_layer&amp;quot;: 3, &amp;quot;n_layer&amp;quot;: 2, &amp;quot;n_head&amp;quot;: 6, &amp;quot;n_embd&amp;quot;: 96, &amp;quot;n_qkv&amp;quot;: 384, &amp;quot;n_ffn&amp;quot;: 384}, &amp;quot;optimizer_hyperparams&amp;quot;: {&amp;quot;eps&amp;quot;: 1e-08, &amp;quot;beta1&amp;quot;: 0.9, &amp;quot;beta2&amp;quot;: 0.99, &amp;quot;weight_decay&amp;quot;: 0.001, &amp;quot;use_muon&amp;quot;: false, &amp;quot;momentum&amp;quot;: 0.95}, &amp;quot;model_path&amp;quot;: &amp;quot;bin/s1.strawberry&amp;quot;, &amp;quot;encoder_path&amp;quot;: &amp;quot;bin/cl8k.bin&amp;quot;, &amp;quot;init_from&amp;quot;: &amp;quot;scratch&amp;quot;, &amp;quot;seed&amp;quot;: &amp;quot;auto&amp;quot;, &amp;quot;gradient_accumulation_steps&amp;quot;: 1, &amp;quot;batch_size&amp;quot;: 16, &amp;quot;max_iters&amp;quot;: 10000, &amp;quot;eval_interval&amp;quot;: 1000, &amp;quot;log_interval&amp;quot;: 100, &amp;quot;eval_iters&amp;quot;: 100, &amp;quot;decay_lr&amp;quot;: true, &amp;quot;lr_decay_iters&amp;quot;: 10000, &amp;quot;learning_rate&amp;quot;: 0.002, &amp;quot;cooldown_frac&amp;quot;: 0.2, &amp;quot;warmup_iters&amp;quot;: 500, &amp;quot;min_lr&amp;quot;: 0.0002}&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;cl8k&lt;/code&gt; is a tokenizer from Andrej Karpathy's tokenizer video trained on the same dataset I explained above and then it was used to tokenize those ~30M chars into just ~9M toks.&lt;/p&gt; &lt;p&gt;The idea for Strawberry and retention was that I wanted to explore whether the attention weights can be generated in-real time rather than being learned. That's why I implemented a &amp;quot;Retention&amp;quot; Mechanism. The retention mechanism generates &amp;quot;weights&amp;quot; based on your input which are then used in attention. The formulation is a little bit similar to standard linear attention formula. This system where the QKV weights are dynamically generated rather than being learned allows to increase the number of attention layers (or model depth) without increasing the number of parameters at all.&lt;/p&gt; &lt;p&gt;However increasing the number of attention layers have a problem. If multiple attention layers are stacked on top of each other without any non-linearity such as FFN, then the performance can decline and the loss can get worse overtime.&lt;/p&gt; &lt;p&gt;That's why I implemented a mini-ffn right after the attention calculation and right before the output projection of each attention layer. So, the weights of qkv, mini-ffn and output projection are generated and updated dynamically by the retention mechanism.&lt;/p&gt; &lt;p&gt;I've two attention mechanisms.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Linear Attention in this case Apple's AFT for global context.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Standard MHA attention for local context. I'm also planning to experiment with &lt;code&gt;mixture of attention experts&lt;/code&gt; approach where each attention expert will get different local window. I haven't implemented it yet cuz this model was too small so it didn't made sense to me but I'll implement it later. Mixture of Attention Experts that's why the SPDA version of attention class is called &lt;code&gt;The Expert Abundance&lt;/code&gt;. Idk why but I like that name so I'm sticking with it.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Currently I'm trying to optimize &amp;amp; improve the architecture more.&lt;/p&gt; &lt;p&gt;So yeah. That's the entire thing. I'd love to know your views and opinions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SrijSriv211"&gt; /u/SrijSriv211 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qym566"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qym566/i_trained_a_18m_params_model_from_scratch_on_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qym566/i_trained_a_18m_params_model_from_scratch_on_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-07T18:57:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1qzgf7x</id>
    <title>Mamba precision loss after quantization</title>
    <updated>2026-02-08T18:21:40+00:00</updated>
    <author>
      <name>/u/perfect-finetune</name>
      <uri>https://old.reddit.com/user/perfect-finetune</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I noticed that almost all models that uses Mamba layers (which are hybrid models,some layers are transformers and most are mamba) especially Mamba-2 suffer from severe degradation of accuracy even at Q8 which is actually strange, are mamba layers more sensitive to quantizations or our current techniques for quantization aren't compatible with Mamba? I don't know if the recently released Mamba-3 is going to solve it but I couldn't find a proper quant of any Mamba models yet.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/perfect-finetune"&gt; /u/perfect-finetune &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzgf7x/mamba_precision_loss_after_quantization/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzgf7x/mamba_precision_loss_after_quantization/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qzgf7x/mamba_precision_loss_after_quantization/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-08T18:21:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1qz11n9</id>
    <title>What are some things you guys are using Local LLMs for?</title>
    <updated>2026-02-08T05:57:47+00:00</updated>
    <author>
      <name>/u/Odd-Ordinary-5922</name>
      <uri>https://old.reddit.com/user/Odd-Ordinary-5922</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So far im only using it for coding and search related stuff but anything else would be cool&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Odd-Ordinary-5922"&gt; /u/Odd-Ordinary-5922 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz11n9/what_are_some_things_you_guys_are_using_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz11n9/what_are_some_things_you_guys_are_using_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qz11n9/what_are_some_things_you_guys_are_using_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-08T05:57:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1qze7q1</id>
    <title>StepFun 3.5 Flash vs MiniMax 2.1</title>
    <updated>2026-02-08T17:00:20+00:00</updated>
    <author>
      <name>/u/Zc5Gwu</name>
      <uri>https://old.reddit.com/user/Zc5Gwu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been using &lt;a href="https://huggingface.co/unsloth/MiniMax-M2.1-GGUF"&gt;Minimax 2.1 Q3_K_XL&lt;/a&gt; as a daily driver with good results. It's reasonably fast and intelligent. One of the best models at 128gb IMO.&lt;/p&gt; &lt;p&gt;I downloaded &lt;a href="https://huggingface.co/ubergarm/Step-3.5-Flash-GGUF"&gt;ubergarm's IQ4_XS&lt;/a&gt; quant of StepFun 3.5 Flash. Tool calling is still a work in progress, so I built and installed llama.cpp from &lt;a href="https://github.com/ggml-org/llama.cpp/pull/18675"&gt;pwilkin:autoparser&lt;/a&gt; which includes tool calling support for the model.&lt;/p&gt; &lt;p&gt;I'm finding that the model likes to think &lt;em&gt;a lot&lt;/em&gt;. Asking the model to write a commit message based on a small diff, the model thought for over 2 minutes. Much longer than minimax would generally take for an equivalent prompt.&lt;/p&gt; &lt;p&gt;It definitely seems like it could be an incredibly intelligent model for its size but the overthinking doesn't feel great for a daily driver.&lt;/p&gt; &lt;p&gt;Results on framework AMD Ryzen Max with vulkan:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-server -hf ubergarm/Step-3.5-Flash-GGUF:IQ4_XS --host 0.0.0.0 --port 8080 -c 16000 --jinja -fa on -ngl 99 --no-context-shift Feb 08 10:46:32 llama-server[20016]: prompt eval time = 4098.41 ms / 563 tokens ( 7.28 ms per token, 137.37 tokens per second) Feb 08 10:46:32 llama-server[20016]: eval time = 188029.67 ms / 3460 tokens ( 54.34 ms per token, 18.40 tokens per second) Feb 08 10:46:32 llama-server[20016]: total time = 192128.08 ms / 4023 tokens &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;At 64k context, it takes up about 107gb of VRAM.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zc5Gwu"&gt; /u/Zc5Gwu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qze7q1/stepfun_35_flash_vs_minimax_21/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qze7q1/stepfun_35_flash_vs_minimax_21/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qze7q1/stepfun_35_flash_vs_minimax_21/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-08T17:00:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1qzgvyh</id>
    <title>pwilkin is doing things</title>
    <updated>2026-02-08T18:38:34+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzgvyh/pwilkin_is_doing_things/"&gt; &lt;img alt="pwilkin is doing things" src="https://external-preview.redd.it/LP9lWJIkvOFwEJy7i2edxqBM2iBmROue3pUEdiXyxYg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a45fd4b46acdf1a22c62c7c684471a43354c1397" title="pwilkin is doing things" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/19435"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzgvyh/pwilkin_is_doing_things/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qzgvyh/pwilkin_is_doing_things/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-08T18:38:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1qz6zi3</id>
    <title>I built a fully local, open-source AI workspace using Rust, Tauri, and sqlite-vec (No Python backend)</title>
    <updated>2026-02-08T11:50:39+00:00</updated>
    <author>
      <name>/u/Far-Association2923</name>
      <uri>https://old.reddit.com/user/Far-Association2923</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz6zi3/i_built_a_fully_local_opensource_ai_workspace/"&gt; &lt;img alt="I built a fully local, open-source AI workspace using Rust, Tauri, and sqlite-vec (No Python backend)" src="https://b.thumbs.redditmedia.com/BtT85Fy4P5d92nzSePyy33dUGcIbmLuWi52Dhz4T33g.jpg" title="I built a fully local, open-source AI workspace using Rust, Tauri, and sqlite-vec (No Python backend)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I've spent the last few months building &lt;strong&gt;Tandem&lt;/strong&gt;, a local-first AI workspace designed to run entirely on your machine without sending data to the cloud.&lt;/p&gt; &lt;p&gt;I wanted to share the technical stack because I think it's a viable alternative to the heavy Python/Electron apps we usually see.&lt;/p&gt; &lt;h1&gt;The Architecture&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Frontend:&lt;/strong&gt; React + Vite (fast dev loop, lightweight UI)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Desktop App Core (Backend):&lt;/strong&gt; Tauri v2 ( Rust ) I chose Tauri/Rust over Electron primarily for distribution and native performance : smaller installers (no bundled Chromium), quicker startup, and a real native backend for file access + security plumbing.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agent Runtime (Sidecar):&lt;/strong&gt; OpenCode (bundled local engine) The LLM “engine” runs as a separate bundled process so users still get a single install across Windows/macOS/Linux without managing Python environments, pip dependencies, or PATH issues.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Vector Store:&lt;/strong&gt; sqlite-vec (embedded in SQLite) Instead of requiring a separate Docker container for Qdrant/Chroma, embeddings live locally in SQLite alongside app state/history. This keeps setup simple and makes distribution easier (no extra services to run).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Inference (the fun part):&lt;/strong&gt; Local-first, but provider-agnostic It supports commercial APIs, but it’s primarily built to drive local Llama models . It connects to Ollama (and other OpenAI-compatible local servers like LM Studio / vLLM), auto-detects your installed models (Llama 3, Mistral, Gemma, etc.), and lets you switch between them without config headaches.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Key Features for this community:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;First-Class Local Model Support:&lt;/strong&gt; Designed for the &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; workflow. Chat with your Llama 3.1 models with full context retention.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Zero Telemetry:&lt;/strong&gt; It's truly offline-capable.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Full MCP Support:&lt;/strong&gt; It implements the Model Context Protocol so you can connect it to local tools.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;&amp;quot;Packs&amp;quot; System:&lt;/strong&gt; I built a way to &amp;quot;install&amp;quot; prompts/skills as config files.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'd love feedback on the &lt;code&gt;sqlite-vec&lt;/code&gt; implementation if anyone else is experimenting with it. It feels like a game-changer for local desktop apps.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Repo:&lt;/strong&gt; &lt;a href="https://github.com/frumu-ai/tandem"&gt;https://github.com/frumu-ai/tandem&lt;/a&gt; &lt;strong&gt;Docs/Download:&lt;/strong&gt; &lt;a href="https://tandem.frumu.ai/"&gt;https://tandem.frumu.ai/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(Happy to answer questions about the Rust/Tauri integration!)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Far-Association2923"&gt; /u/Far-Association2923 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qz6zi3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz6zi3/i_built_a_fully_local_opensource_ai_workspace/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qz6zi3/i_built_a_fully_local_opensource_ai_workspace/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-08T11:50:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1qz8clh</id>
    <title>Verity,a Perplexity style AI search and answer engine that runs fully locally on AI PCs with CPU,GPU,NPU acceleration</title>
    <updated>2026-02-08T13:01:54+00:00</updated>
    <author>
      <name>/u/simpleuserhere</name>
      <uri>https://old.reddit.com/user/simpleuserhere</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz8clh/veritya_perplexity_style_ai_search_and_answer/"&gt; &lt;img alt="Verity,a Perplexity style AI search and answer engine that runs fully locally on AI PCs with CPU,GPU,NPU acceleration" src="https://preview.redd.it/aahtdiytq9ig1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d55a1c7f167d59371e910cefd07f2e7f958a59c0" title="Verity,a Perplexity style AI search and answer engine that runs fully locally on AI PCs with CPU,GPU,NPU acceleration" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Introducing my new App - Verity,a Perplexity style AI search and answer engine that runs fully locally on AI PCs with CPU,GPU,NPU acceleration.&lt;/p&gt; &lt;p&gt;You can run it as a CLI or a Web UI, depending on your workflow.&lt;/p&gt; &lt;p&gt;Developed and tested on Intel Core Ultra Series 1, leveraging on-device compute for fast, private AI inference.&lt;/p&gt; &lt;p&gt;Features :&lt;/p&gt; &lt;p&gt;- Fully Local, AI PC Ready - Optimized for Intel AI PCs using OpenVINO (CPU / iGPU / NPU), Ollama (CPU / CUDA / Metal)&lt;/p&gt; &lt;p&gt;- Privacy by Design - Search and inference can be fully self-hosted&lt;/p&gt; &lt;p&gt;- SearXNG-Powered Search - Self-hosted, privacy-friendly meta search engine&lt;/p&gt; &lt;p&gt;- Designed for fact-grounded, explorable answers&lt;/p&gt; &lt;p&gt;- OpenVINO and Ollama models supported&lt;/p&gt; &lt;p&gt;- Modular architecture&lt;/p&gt; &lt;p&gt;- CLI and WebUI support&lt;/p&gt; &lt;p&gt;- API server support&lt;/p&gt; &lt;p&gt;- Powered by Jan-nano 4B model,or configure any model&lt;/p&gt; &lt;p&gt;GitHub Repo : &lt;a href="https://github.com/rupeshs/verity"&gt;https://github.com/rupeshs/verity&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/simpleuserhere"&gt; /u/simpleuserhere &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/aahtdiytq9ig1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz8clh/veritya_perplexity_style_ai_search_and_answer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qz8clh/veritya_perplexity_style_ai_search_and_answer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-08T13:01:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1qz5uww</id>
    <title>Qwen3 Coder Next as first "usable" coding model &lt; 60 GB for me</title>
    <updated>2026-02-08T10:43:59+00:00</updated>
    <author>
      <name>/u/Chromix_</name>
      <uri>https://old.reddit.com/user/Chromix_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've tried lots of &amp;quot;small&amp;quot; models &amp;lt; 60 GB in the past. GLM 4.5 Air, GLM 4.7 Flash, GPT OSS 20B and 120B, Magistral, Devstral, Apriel Thinker, previous Qwen coders, Seed OSS, QwQ, DeepCoder, DeepSeekCoder, etc. So what's different with Qwen3 Coder Next in OpenCode or in Roo Code with VSCodium?&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Speed&lt;/strong&gt;: The reasoning models would often yet not always produce rather good results. However, now and then they'd enter reasoning loops despite correct sampling settings, leading to no results at all in a large over-night run. Aside from that the sometimes extensive reasoning takes quite some time for the multiple steps that OpenCode or Roo would induce, slowing down interactive work &lt;em&gt;a lot&lt;/em&gt;. Q3CN on the other hand is an instruct MoE model, doesn't have internal thinking loops and is relatively quick at generating tokens.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Quality&lt;/strong&gt;: Other models occasionally botched the tool calls of the harness. This one seems to work reliably. Also I finally have the impression that this can handle a moderately complex codebase with a custom client &amp;amp; server, different programming languages, protobuf, and some quirks. It provided good answers to extreme multi-hop questions and made reliable full-stack changes. Well, almost. On Roo Code it was sometimes a bit lazy and needed a reminder to really go deep to achieve correct results. Other models often got lost.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Context size&lt;/strong&gt;: Coding on larger projects needs context. Most models with standard attention eat all your VRAM for breakfast. With Q3CN having 100k+ context is easy. A few other models also supported that already, yet there were drawbacks in the first two mentioned points.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I run the model this way:&lt;br /&gt; &lt;code&gt;set GGML_CUDA_GRAPH_OPT=1&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;llama-server -m Qwen3-Coder-Next-UD-Q4_K_XL.gguf -ngl 99 -fa on -c 120000 --n-cpu-moe 29 --temp 0 --cache-ram 0&lt;/code&gt;&lt;/p&gt; &lt;p&gt;This works well with 24 GB VRAM and 64 GB system RAM when there's (almost) nothing else on the GPU. Yields about 180 TPS prompt processing and 30 TPS generation speed for me.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;temp 0&lt;/code&gt;? Yes, works well for instruct for me, no higher-temp &amp;quot;creativity&amp;quot; needed. Prevents the &lt;em&gt;very occasional&lt;/em&gt; issue that it outputs an unlikely (and incorrect) token when coding.&lt;/li&gt; &lt;li&gt;&lt;code&gt;cache-ram 0&lt;/code&gt;? The cache was supposed to be fast (30 ms), but I saw 3 second query/update times after each request. So I didn't investigate further and disabled it, as it's only one long conversation history in a single slot anyway.&lt;/li&gt; &lt;li&gt;&lt;code&gt;GGML_CUDA_GRAPH_OPT&lt;/code&gt;? Experimental option to get more TPS. Usually works, yet breaks processing with some models.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;OpenCode vs. Roo Code&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;Both solved things with the model, yet with OpenCode I've seen slightly more correct answers and solutions. But: Roo asks &lt;em&gt;by default&lt;/em&gt; about every single thing, even harmless things like running a syntax check via command line. This can be configured with an easy permission list to not stop the automated flow that often. OpenCode on the other hand just permits everything by default in code mode. One time it encountered an issue, uninstalled and reinstalled packages in an attempt of solving it, removed files and drove itself into a corner by breaking the dev environment. Too autonomous in trying to &amp;quot;get things done&amp;quot;, which doesn't work well on bleeding edge stuff that's not in the training set. Permissions can of course also be configured, but the default is &amp;quot;YOLO&amp;quot;.&lt;/p&gt; &lt;p&gt;Aside from that: Despite running with only a locally hosted model, and having disabled update checks and news downloads, OpenCode (Desktop version) tries to contact a whole lot of IPs on start-up.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Chromix_"&gt; /u/Chromix_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz5uww/qwen3_coder_next_as_first_usable_coding_model_60/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz5uww/qwen3_coder_next_as_first_usable_coding_model_60/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qz5uww/qwen3_coder_next_as_first_usable_coding_model_60/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-08T10:43:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1qz23pp</id>
    <title>PR opened for Qwen3.5!!</title>
    <updated>2026-02-08T06:57:13+00:00</updated>
    <author>
      <name>/u/Mysterious_Finish543</name>
      <uri>https://old.reddit.com/user/Mysterious_Finish543</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz23pp/pr_opened_for_qwen35/"&gt; &lt;img alt="PR opened for Qwen3.5!!" src="https://preview.redd.it/r10pwm02y7ig1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fb19e2c9eac9c47e80b6a33b08c10d458c3fb6c0" title="PR opened for Qwen3.5!!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/huggingface/transformers/pull/43830/"&gt;https://github.com/huggingface/transformers/pull/43830/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Looking at the code at &lt;code&gt;src/transformers/models/qwen3_5/modeling_qwen3_5.py&lt;/code&gt;, it looks like Qwen3.5 series will have VLMs right off the bat!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mysterious_Finish543"&gt; /u/Mysterious_Finish543 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/r10pwm02y7ig1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz23pp/pr_opened_for_qwen35/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qz23pp/pr_opened_for_qwen35/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-08T06:57:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
