<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-02T10:24:20+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI. Hey, Tony</subtitle>
  <entry>
    <id>t3_1nvdyiy</id>
    <title>NVIDIA DGX Spark expected to become available in October 2025</title>
    <updated>2025-10-01T17:05:00+00:00</updated>
    <author>
      <name>/u/Excellent_Produce146</name>
      <uri>https://old.reddit.com/user/Excellent_Produce146</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It looks like we will finally get to know how well or badly the NVIDIA GB10 performs in October (2025!) or November depending on the shipping times.&lt;/p&gt; &lt;p&gt;In the &lt;a href="https://forums.developer.nvidia.com/t/dgx-spark-release-updates/341703/90"&gt;NVIDIA developer forum&lt;/a&gt; this article was posted:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.ctee.com.tw/news/20250930700082-430502"&gt;https://www.ctee.com.tw/news/20250930700082-430502&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;em&gt;GB10 new products to be launched in October... Taiwan's four major PC brand manufacturers see praise in Q4&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;[..] In addition to NVIDIA's public version product delivery schedule waiting for NVIDIA's final decision, the GB10 products of Taiwanese manufacturers ASUS, Gigabyte, MSI, and Acer are all expected to be officially shipped in October. Among them, ASUS, which has already opened a wave of pre-orders in the previous quarter, is rumored to have obtained at least 18,000 sets of GB10 configurations in the first batch, while Gigabyte has about 15,000 sets, and MSI also has a configuration scale of up to 10,000 sets. It is estimated that including the supply on hand from Acer, the four major Taiwanese manufacturers will account for about 70% of the available supply of GB10 in the first wave. [..]&lt;/em&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;(translated with Google Gemini as Chinese is still on my list of languages to learn...)&lt;/p&gt; &lt;p&gt;Looking forward to the first reports/benchmarks. üßê&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Excellent_Produce146"&gt; /u/Excellent_Produce146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvdyiy/nvidia_dgx_spark_expected_to_become_available_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvdyiy/nvidia_dgx_spark_expected_to_become_available_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvdyiy/nvidia_dgx_spark_expected_to_become_available_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-01T17:05:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvoh0b</id>
    <title>Ascend chips available</title>
    <updated>2025-10-01T23:48:11+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is the first time I've seen an Ascend chip (integrated into a system) generally available worldwide, even if it is the crappy Ascend 310.&lt;/p&gt; &lt;p&gt;Under 3k for 192GB of RAM.&lt;/p&gt; &lt;p&gt;Unfortunately, the stupid bots delete my post, so you'll have to find the link yourself.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvoh0b/ascend_chips_available/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvoh0b/ascend_chips_available/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvoh0b/ascend_chips_available/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-01T23:48:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvwchz</id>
    <title>Reasoning with claude-code-router and vllm served GLM-4.6?</title>
    <updated>2025-10-02T06:32:01+00:00</updated>
    <author>
      <name>/u/Daemonix00</name>
      <uri>https://old.reddit.com/user/Daemonix00</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How do I setup &amp;quot;reasoning&amp;quot; with claude-code-router and vllm served GLM-4.6?&lt;/p&gt; &lt;p&gt;No-reasoning works well.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;{ &amp;quot;LOG&amp;quot;: false, &amp;quot;LOG_LEVEL&amp;quot;: &amp;quot;debug&amp;quot;, &amp;quot;CLAUDE_PATH&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;HOST&amp;quot;: &amp;quot;127.0.0.1&amp;quot;, &amp;quot;PORT&amp;quot;: 3456, &amp;quot;APIKEY&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;API_TIMEOUT_MS&amp;quot;: &amp;quot;600000&amp;quot;, &amp;quot;PROXY_URL&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;transformers&amp;quot;: [], &amp;quot;Providers&amp;quot;: [ { &amp;quot;name&amp;quot;: &amp;quot;GLM46&amp;quot;, &amp;quot;api_base_url&amp;quot;: &amp;quot;http://X.X.12.12:30000/v1/chat/completions&amp;quot;, &amp;quot;api_key&amp;quot;: &amp;quot;0000&amp;quot;, &amp;quot;models&amp;quot;: [ &amp;quot;zai-org/GLM-4.6&amp;quot; ], &amp;quot;transformer&amp;quot;: { &amp;quot;use&amp;quot;: [ &amp;quot;OpenAI&amp;quot; ] } } ], &amp;quot;StatusLine&amp;quot;: { &amp;quot;enabled&amp;quot;: false, &amp;quot;currentStyle&amp;quot;: &amp;quot;default&amp;quot;, &amp;quot;default&amp;quot;: { &amp;quot;modules&amp;quot;: [] }, &amp;quot;powerline&amp;quot;: { &amp;quot;modules&amp;quot;: [] } }, &amp;quot;Router&amp;quot;: { &amp;quot;default&amp;quot;: &amp;quot;GLM46,zai-org/GLM-4.6&amp;quot;, &amp;quot;background&amp;quot;: &amp;quot;GLM46,zai-org/GLM-4.6&amp;quot;, &amp;quot;think&amp;quot;: &amp;quot;GLM46,zai-org/GLM-4.6&amp;quot;, &amp;quot;longContext&amp;quot;: &amp;quot;GLM46,zai-org/GLM-4.6&amp;quot;, &amp;quot;longContextThreshold&amp;quot;: 200000, &amp;quot;webSearch&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;image&amp;quot;: &amp;quot;&amp;quot; }, &amp;quot;CUSTOM_ROUTER_PATH&amp;quot;: &amp;quot;&amp;quot; } &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Daemonix00"&gt; /u/Daemonix00 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvwchz/reasoning_with_claudecoderouter_and_vllm_served/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvwchz/reasoning_with_claudecoderouter_and_vllm_served/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvwchz/reasoning_with_claudecoderouter_and_vllm_served/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T06:32:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvvdws</id>
    <title>Dolphin ‚Äî analyze-then-parse document image model (open-source, ByteDance)</title>
    <updated>2025-10-02T05:34:22+00:00</updated>
    <author>
      <name>/u/freesysck</name>
      <uri>https://old.reddit.com/user/freesysck</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Open multimodal doc parser that first analyzes layout, then parses content‚Äîaimed at accurate, structured outputs for pages and elements. &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Two-stage flow: (1) generate reading-order layout; (2) parallel parse via &lt;strong&gt;heterogeneous anchor prompting&lt;/strong&gt;. &lt;/li&gt; &lt;li&gt;Page-level ‚Üí JSON/Markdown; element-level ‚Üí text/tables/formulas; supports images &amp;amp; multi-page PDFs. &lt;/li&gt; &lt;li&gt;Extra: HF/‚Äúoriginal‚Äù inference paths, plus recent &lt;strong&gt;vLLM&lt;/strong&gt; and &lt;strong&gt;TensorRT-LLM&lt;/strong&gt; acceleration notes in the changelog. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Links: GitHub repo / HF model / paper. &lt;a href="https://github.com/bytedance/Dolphin"&gt;GitHub&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/freesysck"&gt; /u/freesysck &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvvdws/dolphin_analyzethenparse_document_image_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvvdws/dolphin_analyzethenparse_document_image_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvvdws/dolphin_analyzethenparse_document_image_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T05:34:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvgdc0</id>
    <title>KaniTTS-370M Released: Multilingual Support + More English Voices</title>
    <updated>2025-10-01T18:31:29+00:00</updated>
    <author>
      <name>/u/ylankgz</name>
      <uri>https://old.reddit.com/user/ylankgz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvgdc0/kanitts370m_released_multilingual_support_more/"&gt; &lt;img alt="KaniTTS-370M Released: Multilingual Support + More English Voices" src="https://external-preview.redd.it/KHH1etcwG-Fh5zDMMYlDVLCEi47zu68tc3z1IQ_zSK8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0661e5699468588875c17a70fe6fc5d482260d59" title="KaniTTS-370M Released: Multilingual Support + More English Voices" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone!&lt;/p&gt; &lt;p&gt;Thanks for the awesome feedback on our first KaniTTS release!&lt;/p&gt; &lt;p&gt;We‚Äôve been hard at work, and released &lt;a href="https://huggingface.co/nineninesix/kani-tts-370m"&gt;kani-tts-370m&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;It‚Äôs still built for speed and quality on consumer hardware, but now with expanded language support and more English voice options.&lt;/p&gt; &lt;h3&gt;What‚Äôs New:&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Multilingual Support&lt;/strong&gt;: German, Korean, Chinese, Arabic, and Spanish (with fine-tuning support). Prosody and naturalness improved across these languages.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;More English Voices&lt;/strong&gt;: Added a variety of new English voices.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Architecture&lt;/strong&gt;: Same two-stage pipeline (LiquidAI LFM2-370M backbone + NVIDIA NanoCodec). Trained on ~80k hours of diverse data.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Performance&lt;/strong&gt;: Generates 15s of audio in ~0.9s on an RTX 5080, using 2GB VRAM.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Use Cases&lt;/strong&gt;: Conversational AI, edge devices, accessibility, or research. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It‚Äôs still Apache 2.0 licensed, so dive in and experiment.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Repo&lt;/strong&gt;: &lt;a href="https://github.com/nineninesix-ai/kani-tts"&gt;https://github.com/nineninesix-ai/kani-tts&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;Model&lt;/strong&gt;: &lt;a href="https://huggingface.co/nineninesix/kani-tts-370m"&gt;https://huggingface.co/nineninesix/kani-tts-370m&lt;/a&gt; &lt;strong&gt;Space&lt;/strong&gt;: &lt;a href="https://huggingface.co/spaces/nineninesix/KaniTTS"&gt;https://huggingface.co/spaces/nineninesix/KaniTTS&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;Website&lt;/strong&gt;: &lt;a href="https://www.nineninesix.ai/n/kani-tts"&gt;https://www.nineninesix.ai/n/kani-tts&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Let us know what you think, and share your setups or use cases!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ylankgz"&gt; /u/ylankgz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/nineninesix/kani-tts-370m"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvgdc0/kanitts370m_released_multilingual_support_more/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvgdc0/kanitts370m_released_multilingual_support_more/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-01T18:31:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1nv4oy9</id>
    <title>Codex is amazing, it can fix code issues without the need of constant approver. my setup: gpt-oss-20b on lm_studio.</title>
    <updated>2025-10-01T10:37:00+00:00</updated>
    <author>
      <name>/u/kyeoh1</name>
      <uri>https://old.reddit.com/user/kyeoh1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nv4oy9/codex_is_amazing_it_can_fix_code_issues_without/"&gt; &lt;img alt="Codex is amazing, it can fix code issues without the need of constant approver. my setup: gpt-oss-20b on lm_studio." src="https://external-preview.redd.it/ODFtbnEzNm45aHNmMTG3bHLe9xXVwwNl3KvP1Qzcgr5dnq8C6Rg-wDqEIF5Y.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=31079d1483d03020c93a99d188076eb10a02002c" title="Codex is amazing, it can fix code issues without the need of constant approver. my setup: gpt-oss-20b on lm_studio." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kyeoh1"&gt; /u/kyeoh1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/1lusu36n9hsf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nv4oy9/codex_is_amazing_it_can_fix_code_issues_without/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nv4oy9/codex_is_amazing_it_can_fix_code_issues_without/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-01T10:37:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1nv8l6o</id>
    <title>Am i seeing this Right?</title>
    <updated>2025-10-01T13:43:53+00:00</updated>
    <author>
      <name>/u/Brave-Hold-9389</name>
      <uri>https://old.reddit.com/user/Brave-Hold-9389</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nv8l6o/am_i_seeing_this_right/"&gt; &lt;img alt="Am i seeing this Right?" src="https://a.thumbs.redditmedia.com/tjIudKmNPF2PlShuW_x68KwSAC4X9VgILiR3p0Bm-R4.jpg" title="Am i seeing this Right?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It would be really cool if unsloth provides quants for Apriel-v1.5-15B-Thinker&lt;/p&gt; &lt;p&gt;(Sorted by opensource, small and tiny)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Brave-Hold-9389"&gt; /u/Brave-Hold-9389 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nv8l6o"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nv8l6o/am_i_seeing_this_right/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nv8l6o/am_i_seeing_this_right/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-01T13:43:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvn7rx</id>
    <title>What kinds of things do y'all use your local models for other than coding?</title>
    <updated>2025-10-01T22:52:52+00:00</updated>
    <author>
      <name>/u/jude_mcjude</name>
      <uri>https://old.reddit.com/user/jude_mcjude</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I think the large majority of us don't own the hardware needed to run the 70B+ class models that can do heavy lifting agentic work that most people talk about, but I know a lot of people still integrate 30B class local models into their day-to-day. &lt;/p&gt; &lt;p&gt;Just curious about the kinds of things people use them for other than coding&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jude_mcjude"&gt; /u/jude_mcjude &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvn7rx/what_kinds_of_things_do_yall_use_your_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvn7rx/what_kinds_of_things_do_yall_use_your_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvn7rx/what_kinds_of_things_do_yall_use_your_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-01T22:52:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvsp35</id>
    <title>New Rig for LLMs</title>
    <updated>2025-10-02T03:07:16+00:00</updated>
    <author>
      <name>/u/I_like_fragrances</name>
      <uri>https://old.reddit.com/user/I_like_fragrances</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvsp35/new_rig_for_llms/"&gt; &lt;img alt="New Rig for LLMs" src="https://preview.redd.it/7j54i8gh7msf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=91340479f031a1f3102a71d193e2f5334240a296" title="New Rig for LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Excited to see what this thing can do. RTX Pro 6000 Max-Q edition.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/I_like_fragrances"&gt; /u/I_like_fragrances &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7j54i8gh7msf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvsp35/new_rig_for_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvsp35/new_rig_for_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T03:07:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvyemn</id>
    <title>How do you configure Ollama so it can help to write essay assignments?</title>
    <updated>2025-10-02T08:43:21+00:00</updated>
    <author>
      <name>/u/crhsharks12</name>
      <uri>https://old.reddit.com/user/crhsharks12</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been experimenting with Ollama for a while now and unfortunately I can‚Äôt seem to crack long-form writing. It tends to repeat itself or stop halfway the moment I try to push it into a full essay assignment (say 1,000-1,500 words). &lt;/p&gt; &lt;p&gt;I‚Äôve tried different prompt styles, but nothing works properly, I‚Äôm still wrestling with it. Now, part of me thinks it would be easier to hand the whole thing off to something like Writemyessay because I don‚Äôt see the point in fighting with prompts for hours. &lt;/p&gt; &lt;p&gt;Has anyone here figured out a config or specific model that works for essays? Do you chunk it section by section? Adjust context size? Any tips appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/crhsharks12"&gt; /u/crhsharks12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvyemn/how_do_you_configure_ollama_so_it_can_help_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvyemn/how_do_you_configure_ollama_so_it_can_help_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvyemn/how_do_you_configure_ollama_so_it_can_help_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T08:43:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvtrj7</id>
    <title>Add file level documentation to directories.</title>
    <updated>2025-10-02T04:02:30+00:00</updated>
    <author>
      <name>/u/sqli</name>
      <uri>https://old.reddit.com/user/sqli</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvtrj7/add_file_level_documentation_to_directories/"&gt; &lt;img alt="Add file level documentation to directories." src="https://external-preview.redd.it/OTczYTQ4YmNobXNmMbN13Cm4GS4tefBLp4gN8PxNO7WYs_aHpAMyOiO1QiFz.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a7ec7217246383b63474797d7c056ed01340cba0" title="Add file level documentation to directories." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;dirdocs queries any Open-AI compatible endpoint with intelligently chunked context from each file and creates a metadata file used by the included dls and dtree binaries. They are stripped down versions of Nushell's ls and tree commands that display the file descriptions with their respective files.&lt;/p&gt; &lt;p&gt;I work with a lot of large codebases and always wondered how Operating System provided file-level documentation would work. This is my attempt at making that happen.&lt;/p&gt; &lt;p&gt;I can see it being used from everything from teaching children about Operating Systems to building fancy repo graphs for agentic stuff.&lt;/p&gt; &lt;p&gt;It works like a dream using my Jade Qwen 3 4B finetune.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sqli"&gt; /u/sqli &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/k4ew96cchmsf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvtrj7/add_file_level_documentation_to_directories/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvtrj7/add_file_level_documentation_to_directories/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T04:02:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvwcix</id>
    <title>ERNIE-4.5-VL - anyone testing it in the competition, what‚Äôs your workflow?</title>
    <updated>2025-10-02T06:32:03+00:00</updated>
    <author>
      <name>/u/Rude_Translator_5196</name>
      <uri>https://old.reddit.com/user/Rude_Translator_5196</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So the ERNIE-4.5-VL competition is live, and I‚Äôve been testing the model a bit for vision-language tasks. Wanted to ask the community: how are you all running VL?&lt;/p&gt; &lt;p&gt;Some things I‚Äôm curious about:&lt;/p&gt; &lt;p&gt;Are you using it mainly for image-text matching, multimodal reasoning, or something else?&lt;/p&gt; &lt;p&gt;What hardware/setup seems to give the best performance without blowing the budget?&lt;/p&gt; &lt;p&gt;Any tricks for handling long sequences of images + text?&lt;/p&gt; &lt;p&gt;I‚Äôve tried a few simple cases, but results feel very sensitive to input format and preprocessing. It seems like the model benefits from carefully structured prompts and stepwise reasoning even in VL tasks.&lt;/p&gt; &lt;p&gt;Would love to hear how others are approaching it - what‚Äôs been working, what‚Äôs tricky, and any workflow tips. For anyone curious, the competition does offer cash prizes in the $400‚Äì$4000 range, which is a nice bonus.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rude_Translator_5196"&gt; /u/Rude_Translator_5196 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvwcix/ernie45vl_anyone_testing_it_in_the_competition/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvwcix/ernie45vl_anyone_testing_it_in_the_competition/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvwcix/ernie45vl_anyone_testing_it_in_the_competition/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T06:32:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvlj5k</id>
    <title>I just wanted to do a first benchmark of GLM 4.6 on my PC and I was surprised...</title>
    <updated>2025-10-01T21:44:29+00:00</updated>
    <author>
      <name>/u/LegacyRemaster</name>
      <uri>https://old.reddit.com/user/LegacyRemaster</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvlj5k/i_just_wanted_to_do_a_first_benchmark_of_glm_46/"&gt; &lt;img alt="I just wanted to do a first benchmark of GLM 4.6 on my PC and I was surprised..." src="https://b.thumbs.redditmedia.com/5lN1Qf4EL0Qe6yVQ5ak9-6lxWk1LUO3Ltt2n6tNcX3c.jpg" title="I just wanted to do a first benchmark of GLM 4.6 on my PC and I was surprised..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I downloaded GLM 4.6 UD - IQ2_M and loaded it on ryzen 5950x +128gb ram using only the rtx 5070ti 16gb.&lt;/p&gt; &lt;p&gt;I tryed llama-cli.exe --model &amp;quot;C:\gptmodel\unsloth\GLM-4.6-GGUF\GLM-4.6-UD-IQ2_M-00001-of-00003.gguf&amp;quot; --jinja --n-gpu-layers 93 --tensor-split 93,0 --cpu-moe --ctx-size 16384 --flash-attn on --threads 32 --parallel 1 --top-p 0.95 --top-k 40 --ubatch-size 512 --seed 3407 --no-mmap --cache-type-k q8_0 --cache-type-v q8_0&lt;/p&gt; &lt;p&gt;Done.&lt;/p&gt; &lt;p&gt;Then the prompt: write a short story about a bird.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/46ah6fcflksf1.png?width=1990&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4209d75aa6efbbc62fbf66c7db408c6ce161a6f9"&gt;Glm 4.6&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://pastebin.com/urUWTw6R"&gt;https://pastebin.com/urUWTw6R&lt;/a&gt; performances are good considering the context of 16k and all on ddr4... But what moved me is the reasoning.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LegacyRemaster"&gt; /u/LegacyRemaster &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvlj5k/i_just_wanted_to_do_a_first_benchmark_of_glm_46/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvlj5k/i_just_wanted_to_do_a_first_benchmark_of_glm_46/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvlj5k/i_just_wanted_to_do_a_first_benchmark_of_glm_46/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-01T21:44:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1nveoru</id>
    <title>I've built Jarvis completely on-device in the browser</title>
    <updated>2025-10-01T17:31:15+00:00</updated>
    <author>
      <name>/u/nicodotdev</name>
      <uri>https://old.reddit.com/user/nicodotdev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nveoru/ive_built_jarvis_completely_ondevice_in_the/"&gt; &lt;img alt="I've built Jarvis completely on-device in the browser" src="https://external-preview.redd.it/dWNmajhwem5janNmMXGz1aMo2QiMkpgt6v7Z9vfboXTlOgdFBasYHpD7porA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3a6d6f3d8bc315fcaa36d89d874a54f775fe7b81" title="I've built Jarvis completely on-device in the browser" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nicodotdev"&gt; /u/nicodotdev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/hge6ipzncjsf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nveoru/ive_built_jarvis_completely_ondevice_in_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nveoru/ive_built_jarvis_completely_ondevice_in_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-01T17:31:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvzf6n</id>
    <title>Tutorial: Matrix Core Programming on AMD GPUs</title>
    <updated>2025-10-02T09:48:26+00:00</updated>
    <author>
      <name>/u/salykova_</name>
      <uri>https://old.reddit.com/user/salykova_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvzf6n/tutorial_matrix_core_programming_on_amd_gpus/"&gt; &lt;img alt="Tutorial: Matrix Core Programming on AMD GPUs" src="https://preview.redd.it/6ijih0px6osf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a0db9af3fa74d54bb246c35cef7a77a35b54c442" title="Tutorial: Matrix Core Programming on AMD GPUs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;I wanted to share my new tutorial on programming Matrix Cores in HIP. The blog post is very educational and contains necessary knowledge to start programming Matrix Cores, covering modern low-precision floating-point types, the Matrix Core compiler intrinsics, and the data layouts required by the Matrix Core instructions. I tried to make the tutorial easy to follow and, as always, included lots of code examples and illustrations. I hope you will enjoy it!&lt;/p&gt; &lt;p&gt;I plan to publish in-depth technical tutorials on kernel programming in HIP and inference optimization for RDNA and CDNA architecture. Please let me know if there are any other technical ROCm/HIP-related topics you would like to hear more about!&lt;/p&gt; &lt;p&gt;Link: &lt;a href="https://salykova.github.io/matrix-cores-cdna"&gt;https://salykova.github.io/matrix-cores-cdna&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/salykova_"&gt; /u/salykova_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6ijih0px6osf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvzf6n/tutorial_matrix_core_programming_on_amd_gpus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvzf6n/tutorial_matrix_core_programming_on_amd_gpus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T09:48:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvcjkr</id>
    <title>We're building a local OpenRouter: Auto-configure the best LLM engine on any PC</title>
    <updated>2025-10-01T16:13:17+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvcjkr/were_building_a_local_openrouter_autoconfigure/"&gt; &lt;img alt="We're building a local OpenRouter: Auto-configure the best LLM engine on any PC" src="https://preview.redd.it/fe4322p9yisf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=63b8433dff7ec591d237dcfae3b32ef0a530e5c4" title="We're building a local OpenRouter: Auto-configure the best LLM engine on any PC" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Lemonade is a local LLM server-router that auto-configures high-performance inference engines for your computer. We don't just wrap llama.cpp, we're here to wrap everything!&lt;/p&gt; &lt;p&gt;We started out building an OpenAI-compatible server for AMD NPUs and quickly found that users and devs want flexibility, so we kept adding support for more devices, engines, and operating systems. &lt;/p&gt; &lt;p&gt;What was once a single-engine server evolved into a server-router, like OpenRouter but 100% local. Today's v8.1.11 release adds another inference engine and another OS to the list!&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;üöÄ FastFlowLM&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;The FastFlowLM inference engine for AMD NPUs is fully integrated with Lemonade for Windows Ryzen AI 300-series PCs.&lt;/li&gt; &lt;li&gt;Switch between ONNX, GGUF, and FastFlowLM models from the same Lemonade install with one click.&lt;/li&gt; &lt;li&gt;Shoutout to TWei, Alfred, and Zane for supporting the integration!&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h2&gt;üçé macOS / Apple Silicon&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;PyPI installer for M-series macOS devices, with the same experience available on Windows and Linux.&lt;/li&gt; &lt;li&gt;Taps into llama.cpp's Metal backend for compute.&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h2&gt;ü§ù Community Contributions&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Added a stop button, chat auto-scroll, custom vision model download, model size info, and UI refinements to the built-in web ui.&lt;/li&gt; &lt;li&gt;Support for gpt-oss's reasoning style, changing context size from the tray app, and refined the .exe installer.&lt;/li&gt; &lt;li&gt;Shoutout to kpoineal, siavashhub, ajnatopic1, Deepam02, Kritik-07, RobertAgee, keetrap, and ianbmacdonald!&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h2&gt;ü§ñ What's Next&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Popular apps like Continue, Dify, Morphik, and more are integrating with Lemonade as a native LLM provider, with more apps to follow.&lt;/li&gt; &lt;li&gt;Should we add more inference engines or backends? Let us know what you'd like to see.&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;p&gt;GitHub/Discord links in the comments. Check us out and say hi if the project direction sounds good to you. The community's support is what empowers our team at AMD to expand across different hardware, engines, and OSs.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fe4322p9yisf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvcjkr/were_building_a_local_openrouter_autoconfigure/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvcjkr/were_building_a_local_openrouter_autoconfigure/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-01T16:13:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvkjo8</id>
    <title>Tried glm 4.6 with deep think, not using it for programming. It's pretty good, significantly better than gemini 2.5 flash, and slightly better than gemini 2.5 pro.</title>
    <updated>2025-10-01T21:06:00+00:00</updated>
    <author>
      <name>/u/Longjumping_Fly_2978</name>
      <uri>https://old.reddit.com/user/Longjumping_Fly_2978</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Chinese models are improving so fast, starting to get the feeling that china may dominate the ai race. They are getting very good, the chat with glm 4.6 was very enjoyable and the stile was not at all weird, that didn't happen to me with other chinese models, qwen was still good and decent but had a somewhat weird writing style. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Longjumping_Fly_2978"&gt; /u/Longjumping_Fly_2978 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvkjo8/tried_glm_46_with_deep_think_not_using_it_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvkjo8/tried_glm_46_with_deep_think_not_using_it_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvkjo8/tried_glm_46_with_deep_think_not_using_it_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-01T21:06:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvrwlq</id>
    <title>Recommendation Request: Local IntelliJ Java Coding Model w/16G GPU</title>
    <updated>2025-10-02T02:28:25+00:00</updated>
    <author>
      <name>/u/TradingDreams</name>
      <uri>https://old.reddit.com/user/TradingDreams</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvrwlq/recommendation_request_local_intellij_java_coding/"&gt; &lt;img alt="Recommendation Request: Local IntelliJ Java Coding Model w/16G GPU" src="https://preview.redd.it/0ab46dblzlsf1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=575068835377fde0a249a3fe5005c86009a209a7" title="Recommendation Request: Local IntelliJ Java Coding Model w/16G GPU" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm using IntelliJ for the first time and saw that it will talk to local models. My computer had 64G system memory and a 16G NVidia GPU. Can anyone recommend a local coding model that is reasonable at Java and would fit into my available resources with an ok context window?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TradingDreams"&gt; /u/TradingDreams &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0ab46dblzlsf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvrwlq/recommendation_request_local_intellij_java_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvrwlq/recommendation_request_local_intellij_java_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T02:28:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvwbvn</id>
    <title>ERNIE-4.5-21B-A3B-Thinking ‚Äî impressions after some testing</title>
    <updated>2025-10-02T06:30:54+00:00</updated>
    <author>
      <name>/u/ABCD170</name>
      <uri>https://old.reddit.com/user/ABCD170</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;aying around with ERNIE-4.5-21B-A3B-Thinking for a bit and figured I‚Äôd drop my thoughts. This is Baidu‚Äôs ‚Äúthinking‚Äù model for logic, math, science, and coding.&lt;/p&gt; &lt;p&gt;What stood out to me:&lt;/p&gt; &lt;p&gt;Long context works: 128K token window actually does what it promises. I‚Äôve loaded multi-page papers and notes, and it keeps things coherent better than most open models I‚Äôve tried.&lt;/p&gt; &lt;p&gt;Math &amp;amp; code: Handles multi-step problems pretty solidly. Small scripts work fine; bigger coding tasks, I‚Äôd still pick Qwen. Surprised by how little it hallucinates on structured problems.&lt;/p&gt; &lt;p&gt;Performance: 21B params total, ~3B active thanks to MoE. Feels smoother than you‚Äôd expect for a model this size.&lt;/p&gt; &lt;p&gt;Reasoning style: Focused and doesn‚Äôt ramble unnecessarily. Good at staying on track.&lt;/p&gt; &lt;p&gt;Text output: Polished enough that it works well for drafting, summaries, or light creative writing.&lt;/p&gt; &lt;p&gt;Best use cases: Really strong for reasoning and analysis. Weaker if you‚Äôre pushing it into larger coding projects or very complex/nuanced creative writing. So far, it‚Äôs been useful for checking reasoning steps, parsing documents, or running experiments where I need something to actually ‚Äúthink through‚Äù a problem instead of shortcutting.&lt;/p&gt; &lt;p&gt;Curious - anyone else using it for long docs, planning tasks, or multi-step problem solving? What‚Äôs been working for you?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ABCD170"&gt; /u/ABCD170 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvwbvn/ernie4521ba3bthinking_impressions_after_some/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvwbvn/ernie4521ba3bthinking_impressions_after_some/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvwbvn/ernie4521ba3bthinking_impressions_after_some/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T06:30:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvltym</id>
    <title>Liquid AI released its Audio Foundation Model: LFM2-Audio-1.5</title>
    <updated>2025-10-01T21:56:19+00:00</updated>
    <author>
      <name>/u/elemental-mind</name>
      <uri>https://old.reddit.com/user/elemental-mind</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvltym/liquid_ai_released_its_audio_foundation_model/"&gt; &lt;img alt="Liquid AI released its Audio Foundation Model: LFM2-Audio-1.5" src="https://b.thumbs.redditmedia.com/py41lfh_Ics398r2NxQJ0RyqQFhtbpQXxULCJQhZqLs.jpg" title="Liquid AI released its Audio Foundation Model: LFM2-Audio-1.5" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A new end-to-end Audio Foundation model supporting: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Inputs: Audio &amp;amp; Text&lt;/li&gt; &lt;li&gt;Outputs: Audio &amp;amp; Text (steerable via prompting, also supporting interleaved outputs)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For me personally it's exciting to use as an ASR solution with a custom vocabulary set - as Parakeet and Whisper do not support that feature. It's also very snappy.&lt;/p&gt; &lt;p&gt;You can try it out here: &lt;a href="https://playground.liquid.ai/talk"&gt;Talk | Liquid Playground&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Release blog post: &lt;a href="https://www.liquid.ai/blog/lfm2-audio-an-end-to-end-audio-foundation-model"&gt;LFM2-Audio: An End-to-End Audio Foundation Model | Liquid AI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For good code examples see their github: &lt;a href="https://github.com/Liquid4All/liquid-audio"&gt;Liquid4All/liquid-audio: Liquid Audio - Speech-to-Speech audio models by Liquid AI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Available on HuggingFace: &lt;a href="https://huggingface.co/LiquidAI/LFM2-Audio-1.5B"&gt;LiquidAI/LFM2-Audio-1.5B ¬∑ Hugging Face&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/elemental-mind"&gt; /u/elemental-mind &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nvltym"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvltym/liquid_ai_released_its_audio_foundation_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvltym/liquid_ai_released_its_audio_foundation_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-01T21:56:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1nv53rb</id>
    <title>GLM-4.6-GGUF is out!</title>
    <updated>2025-10-01T11:00:52+00:00</updated>
    <author>
      <name>/u/TheAndyGeorge</name>
      <uri>https://old.reddit.com/user/TheAndyGeorge</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nv53rb/glm46gguf_is_out/"&gt; &lt;img alt="GLM-4.6-GGUF is out!" src="https://preview.redd.it/kptmc2f0fhsf1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9344c531abf7cb2d05a64a1d2ee461b6106008bb" title="GLM-4.6-GGUF is out!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheAndyGeorge"&gt; /u/TheAndyGeorge &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kptmc2f0fhsf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nv53rb/glm46gguf_is_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nv53rb/glm46gguf_is_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-01T11:00:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvw1my</id>
    <title>Jet-Nemotron 2B/4B 47x faster inference released</title>
    <updated>2025-10-02T06:13:26+00:00</updated>
    <author>
      <name>/u/Odd-Ordinary-5922</name>
      <uri>https://old.reddit.com/user/Odd-Ordinary-5922</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvw1my/jetnemotron_2b4b_47x_faster_inference_released/"&gt; &lt;img alt="Jet-Nemotron 2B/4B 47x faster inference released" src="https://external-preview.redd.it/r396-oAbMocWRiDVz2adQ6rwSWE3nHUKDdKf1UIVuHc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=68466d80cc66f634a4f6d8779e7110ddf330d635" title="Jet-Nemotron 2B/4B 47x faster inference released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;heres the github &lt;a href="https://github.com/NVlabs/Jet-Nemotron"&gt;https://github.com/NVlabs/Jet-Nemotron&lt;/a&gt; the model was published 2 days ago but I havent seen anyone talk about it &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Odd-Ordinary-5922"&gt; /u/Odd-Ordinary-5922 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/jet-ai/Jet-Nemotron-4B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvw1my/jetnemotron_2b4b_47x_faster_inference_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvw1my/jetnemotron_2b4b_47x_faster_inference_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T06:13:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvsbdu</id>
    <title>I visualized embeddings walking across the latent space as you type! :)</title>
    <updated>2025-10-02T02:48:37+00:00</updated>
    <author>
      <name>/u/kushalgoenka</name>
      <uri>https://old.reddit.com/user/kushalgoenka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvsbdu/i_visualized_embeddings_walking_across_the_latent/"&gt; &lt;img alt="I visualized embeddings walking across the latent space as you type! :)" src="https://external-preview.redd.it/bXg4NGVhbm8zbXNmMcfpx6_IdDgYBGvf-fwH7xFuI_ot2ErqijE3fUPasYhL.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f48ec1e00df98aae9dff909fac81e2997bfd28dc" title="I visualized embeddings walking across the latent space as you type! :)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kushalgoenka"&gt; /u/kushalgoenka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/czy4sbno3msf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvsbdu/i_visualized_embeddings_walking_across_the_latent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvsbdu/i_visualized_embeddings_walking_across_the_latent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T02:48:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvzeuh</id>
    <title>Jan now auto-optimizes llama.cpp settings based on your hardware for more efficient performance</title>
    <updated>2025-10-02T09:47:49+00:00</updated>
    <author>
      <name>/u/ShinobuYuuki</name>
      <uri>https://old.reddit.com/user/ShinobuYuuki</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvzeuh/jan_now_autooptimizes_llamacpp_settings_based_on/"&gt; &lt;img alt="Jan now auto-optimizes llama.cpp settings based on your hardware for more efficient performance" src="https://external-preview.redd.it/NzRlNXJuc3A2b3NmMe-uhlatbqnQI0WkANIEyFuJlq6CEOqVOtkO0hhCMPfO.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=152efa7692053a33bdf74c0824752b907d2becc8" title="Jan now auto-optimizes llama.cpp settings based on your hardware for more efficient performance" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I'm Yuuki from the Jan team.&lt;/p&gt; &lt;p&gt;We‚Äôve been working on some updates for a while. We released Jan v0.7.0. I'd like to quickly share what's new:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;llama.cpp improvements&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Jan now automatically optimizes llama.cpp settings (e.g. context size, gpu layers) based on your hardware. So your models run more efficiently. It's an experimental feature&lt;/li&gt; &lt;li&gt;You can now see some stats (how much context is used, etc.) when the model runs&lt;/li&gt; &lt;li&gt;Projects is live now. You can organize your chats using it - it's pretty similar to ChatGPT&lt;/li&gt; &lt;li&gt;You can rename your models in Settings&lt;/li&gt; &lt;li&gt;Plus, we're also improving Jan's cloud capabilities: Model names update automatically - so no need to manually add cloud models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you haven't seen it yet: Jan is an open-source ChatGPT alternative. It runs AI models locally and lets you add agentic capabilities &lt;a href="https://www.jan.ai/docs/desktop/mcp#configure-and-use-mcps-within-jan"&gt;through MCPs&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Website: &lt;a href="https://www.jan.ai/"&gt;https://www.jan.ai/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/menloresearch/jan"&gt;https://github.com/menloresearch/jan&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ShinobuYuuki"&gt; /u/ShinobuYuuki &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/49h5xlsp6osf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvzeuh/jan_now_autooptimizes_llamacpp_settings_based_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvzeuh/jan_now_autooptimizes_llamacpp_settings_based_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T09:47:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvpw0y</id>
    <title>Those who spent $10k+ on a local LLM setup, do you regret it?</title>
    <updated>2025-10-02T00:54:13+00:00</updated>
    <author>
      <name>/u/TumbleweedDeep825</name>
      <uri>https://old.reddit.com/user/TumbleweedDeep825</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Considering the fact 200k context chinese models subscriptions like z.ai (GLM 4.6) are pretty dang cheap. &lt;/p&gt; &lt;p&gt;Every so often I consider blowing a ton of money on an LLM setup only to realize I can't justify the money or time spent at all.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TumbleweedDeep825"&gt; /u/TumbleweedDeep825 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvpw0y/those_who_spent_10k_on_a_local_llm_setup_do_you/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvpw0y/those_who_spent_10k_on_a_local_llm_setup_do_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvpw0y/those_who_spent_10k_on_a_local_llm_setup_do_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T00:54:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvryo4</id>
    <title>AMA Announcement: Prime Intellect ‚Äî The Open‚ÄëSource Distributed Training Lab (Thu, Oct 2 ‚Ä¢ 10 AM ‚Äì 1 PM PDT)</title>
    <updated>2025-10-02T02:31:01+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"&gt; &lt;img alt="AMA Announcement: Prime Intellect ‚Äî The Open‚ÄëSource Distributed Training Lab (Thu, Oct 2 ‚Ä¢ 10 AM ‚Äì 1 PM PDT)" src="https://preview.redd.it/222kj50x0msf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c1c747c42690f74b8d08690dcdbb1ef23aece13b" title="AMA Announcement: Prime Intellect ‚Äî The Open‚ÄëSource Distributed Training Lab (Thu, Oct 2 ‚Ä¢ 10 AM ‚Äì 1 PM PDT)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/222kj50x0msf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T02:31:01+00:00</published>
  </entry>
</feed>
