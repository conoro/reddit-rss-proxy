<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-22T17:34:48+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1p3wkis</id>
    <title>Baking in CoT in Instruct model</title>
    <updated>2025-11-22T15:53:50+00:00</updated>
    <author>
      <name>/u/nik77kez</name>
      <uri>https://old.reddit.com/user/nik77kez</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Recently was trying to finetune a Qwen2.5-3b-Instruct to have reasoning as well. But kept failing at creating a reasoning model. Trained it on 800 examples and at the end either got a model that would not generate thinking tokens or would additionaly start generating trash. Would highly appreciate someone explaining how its usually done, cuz after some paper reading - usually CoT is added via SFT of base models and in this case 800 examples 1 epoch might be too little.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nik77kez"&gt; /u/nik77kez &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3wkis/baking_in_cot_in_instruct_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3wkis/baking_in_cot_in_instruct_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p3wkis/baking_in_cot_in_instruct_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-22T15:53:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2w5i6</id>
    <title>HunyuanVideo-1.5: A leading lightweight video generation model</title>
    <updated>2025-11-21T11:25:33+00:00</updated>
    <author>
      <name>/u/abdouhlili</name>
      <uri>https://old.reddit.com/user/abdouhlili</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/tencent/HunyuanVideo-1.5"&gt;https://huggingface.co/tencent/HunyuanVideo-1.5&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdouhlili"&gt; /u/abdouhlili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2w5i6/hunyuanvideo15_a_leading_lightweight_video/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2w5i6/hunyuanvideo15_a_leading_lightweight_video/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2w5i6/hunyuanvideo15_a_leading_lightweight_video/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T11:25:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1p3xg4l</id>
    <title>Looking for Uncensored/Unfiltered 70B Model</title>
    <updated>2025-11-22T16:29:06+00:00</updated>
    <author>
      <name>/u/local-profit-6919</name>
      <uri>https://old.reddit.com/user/local-profit-6919</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;is there any 70B text generation model which is completely uncensored and be used for generating stories which had violence scenes. I tried Dobby-Unhinged-Llama-3.3-70B on hugging chat but it reject even with a system prompt. i also tried dolphin-mistral-24b-venice-edition:free through openRouter api, the quality is very poor. My idea is to generate stories which are written in latin script but hindi language (Hinglish).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/local-profit-6919"&gt; /u/local-profit-6919 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3xg4l/looking_for_uncensoredunfiltered_70b_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3xg4l/looking_for_uncensoredunfiltered_70b_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p3xg4l/looking_for_uncensoredunfiltered_70b_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-22T16:29:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1p3sinc</id>
    <title>NVFP4 MOE on Blackwell (5090 and RTX PRO 6000)</title>
    <updated>2025-11-22T12:53:09+00:00</updated>
    <author>
      <name>/u/Dependent_Factor_204</name>
      <uri>https://old.reddit.com/user/Dependent_Factor_204</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those running SM120 cards (5090 and RTX PRO 6000)&lt;/p&gt; &lt;p&gt;NVFP4 MOE models have been near impossible to run.&lt;/p&gt; &lt;p&gt;Until now!&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/BlackwellPerformance/comments/1p2xe94/4x_rtx_pro_6000_with_nvfp4_glm_46/"&gt;https://www.reddit.com/r/BlackwellPerformance/comments/1p2xe94/4x_rtx_pro_6000_with_nvfp4_glm_46/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There is a specific nightly build of VLLM that has support - but is broken again in the current nightly.&lt;/p&gt; &lt;p&gt;It should with other smaller NVFP4 models too if you don't have multiple cards.&lt;/p&gt; &lt;p&gt;Its a huge RAM saving over FP8 with virtually the same quality.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dependent_Factor_204"&gt; /u/Dependent_Factor_204 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3sinc/nvfp4_moe_on_blackwell_5090_and_rtx_pro_6000/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3sinc/nvfp4_moe_on_blackwell_5090_and_rtx_pro_6000/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p3sinc/nvfp4_moe_on_blackwell_5090_and_rtx_pro_6000/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-22T12:53:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1p3ie7w</id>
    <title>Echo TTS can seemingly generate music surprisingly well</title>
    <updated>2025-11-22T02:59:34+00:00</updated>
    <author>
      <name>/u/iGermanProd</name>
      <uri>https://old.reddit.com/user/iGermanProd</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;While playing around with the Echo TTS demo from the recent post &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1p2l36u/echo_tts_441khz_fast_fits_under_8gb_vram_sota/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1p2l36u/echo_tts_441khz_fast_fits_under_8gb_vram_sota/&lt;/a&gt;, I discovered that if you load a song in as a reference audio and bump the CFGs (I set mine to 5, 7 respectively), as well as prompt like this:&lt;/p&gt; &lt;p&gt;&lt;code&gt; [Music] [Music] [S1] (singing) Yeah, I'm gon' take my horse to the old town road [S1] (singing) I'm gonna ride 'til I can't no more [S1] (singing) I'm gon' take my horse to the old town road [S1] (singing) I'm gon' (Kio, Kio) ride 'til I can't no more [S1] (singing) I got the horses in the back [S1] (singing) Horse tack is attached [S1] (singing) Hat is matte black [S1] (singing) Got the boots that's black to match [S1] (singing) Riding on a horse, ha [S1] (singing) You can whip your Porsche [S1] (singing) I been in the valley [S1] (singing) You ain't been up off that porch now [S1] (singing) Can't nobody tell me nothing [S1] (singing) You can't tell me nothing [Music] [Music] &lt;/code&gt;&lt;/p&gt; &lt;p&gt;It will output shockingly decent results for a model that's not at all been trained to do music. I wonder what would happen if one were to fine-tune it on music.&lt;/p&gt; &lt;p&gt;Here are some demos: &lt;a href="https://voca.ro/185lsRLEByx0"&gt;https://voca.ro/185lsRLEByx0&lt;/a&gt; &lt;a href="https://voca.ro/142AWpTH9jD7"&gt;https://voca.ro/142AWpTH9jD7&lt;/a&gt; &lt;a href="https://voca.ro/1imeBG3ZDYIo"&gt;https://voca.ro/1imeBG3ZDYIo&lt;/a&gt; &lt;a href="https://voca.ro/1ldaxj8MzYr5"&gt;https://voca.ro/1ldaxj8MzYr5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It's obviously not very coherent or consistent in the long run, but it's clearly got the chops to be, that last ambient result actually sounds pretty good. Hopefully it will actually get released for local use.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iGermanProd"&gt; /u/iGermanProd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3ie7w/echo_tts_can_seemingly_generate_music/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3ie7w/echo_tts_can_seemingly_generate_music/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p3ie7w/echo_tts_can_seemingly_generate_music/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-22T02:59:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1p3y5ey</id>
    <title>What's the current best local model(text and embedding each) for 16gb vram?</title>
    <updated>2025-11-22T16:57:10+00:00</updated>
    <author>
      <name>/u/Mammoth_Act_1877</name>
      <uri>https://old.reddit.com/user/Mammoth_Act_1877</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm running everything locally on a 16GB VRAM GPU &lt;/p&gt; &lt;p&gt;Currently, I'm using Qwen3 VL 8B Instruct for general purposes and bge m3 as my embedding model.&lt;/p&gt; &lt;p&gt;My main use cases are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Page Assist for asking questions about web pages,&lt;/li&gt; &lt;li&gt;Obsidian Web Clipper for summarizing web pages and YouTube videos,&lt;/li&gt; &lt;li&gt;Vault Q&amp;amp;A and writing assistance within Obsidian.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Are there any better options out now , especially for Korean/English use? &lt;/p&gt; &lt;p&gt;Benchmarks, real-world feedback, or hands-on comparisons would be really appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mammoth_Act_1877"&gt; /u/Mammoth_Act_1877 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3y5ey/whats_the_current_best_local_modeltext_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3y5ey/whats_the_current_best_local_modeltext_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p3y5ey/whats_the_current_best_local_modeltext_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-22T16:57:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1p3gqo8</id>
    <title>Which is the least agreeable/sycophantic AI model at the moment?</title>
    <updated>2025-11-22T01:39:27+00:00</updated>
    <author>
      <name>/u/BrokenLoadOrder</name>
      <uri>https://old.reddit.com/user/BrokenLoadOrder</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For some context: My wife and I moved to a teeny tiny town, and there's not a lot of nerds here to play D&amp;amp;D/RootRPG with, but I do miss the silly antics I used to get up to. I tried a few sessions across various AI, but there's two kinda major issues I've noticed across most:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Being too agreeable - This is by far the most common problem, and ends up meaning you can tell the &amp;quot;DM&amp;quot; (Being the AI) pretty much anything, and it'll let you do it. In one of my very first runs trying this out, I soloed pretty much an entire battlefield, paid with gold I didn't have and convinced multiple enemy factions to give up even as a complete nobody. Even in cases where I've asked it to provide a difficulty check, that leads to a second issue...&lt;/li&gt; &lt;li&gt;Randomly losing its mind - I understand this is a bit of a vague title, but sometimes the AI has a rather tenuous grasp of reality. I've seen it say things like &amp;quot;This is an Easy Skill check&amp;quot; followed by an incredibly high number. I've seen it freak out over things like violence (Including my favourite example where I got shut down for using the term &amp;quot;bloodshot eyes&amp;quot; &lt;em&gt;immediately after the AI just used the term&lt;/em&gt;). I've seen it completely forget what items I have, skills, etc.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;TLDR: Has anyone found an offline AI that can work as a semi-competent DM for some homebrew adventures?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BrokenLoadOrder"&gt; /u/BrokenLoadOrder &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3gqo8/which_is_the_least_agreeablesycophantic_ai_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3gqo8/which_is_the_least_agreeablesycophantic_ai_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p3gqo8/which_is_the_least_agreeablesycophantic_ai_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-22T01:39:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1p36l5f</id>
    <title>2x RTX 5060 TI 16 GB =32GB VRAM -</title>
    <updated>2025-11-21T18:38:39+00:00</updated>
    <author>
      <name>/u/quantier</name>
      <uri>https://old.reddit.com/user/quantier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p36l5f/2x_rtx_5060_ti_16_gb_32gb_vram/"&gt; &lt;img alt="2x RTX 5060 TI 16 GB =32GB VRAM -" src="https://preview.redd.it/ven6e8i8nn2g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ff9be1ee0ff290587c547010478fc54b1176c114" title="2x RTX 5060 TI 16 GB =32GB VRAM -" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is anyone up and running with a rig like this with 2x RTX 5060 TI? how is it? What PSU does one need? How much compute do you loose when you have 2 GPU:s instead of a 1 card setup. How would 2x 5060 TI be in comparison with a 5090.&lt;/p&gt; &lt;p&gt;How does one put together these GPU:s in ComfyUI? Does one need to add new nodes to the workflows?&lt;/p&gt; &lt;p&gt;Is this worth it, I can get a RTX 5060 TI 16GB for around $400 each meaning that $800 for 32 GB VRAM feels very interesting with a Blackwell card!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/quantier"&gt; /u/quantier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ven6e8i8nn2g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p36l5f/2x_rtx_5060_ti_16_gb_32gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p36l5f/2x_rtx_5060_ti_16_gb_32gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T18:38:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1p36iln</id>
    <title>Made a site where AI models trade against each other. A local model is winning.</title>
    <updated>2025-11-21T18:35:58+00:00</updated>
    <author>
      <name>/u/2degreestarget</name>
      <uri>https://old.reddit.com/user/2degreestarget</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been messing around with new Gemini this week and ended up building this thing where different LLMs compete as stock traders. I work in asset management so I was genuinely curious how these models would approach investing.&lt;/p&gt; &lt;p&gt;Some observations:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Qwen (the only local model) is currently winning, mostly because keeps 90% cash (saving for a GPU?)&lt;/li&gt; &lt;li&gt;None of them understand position sizing. Like, at all. And they all have this weird overconfidence where they'll write a whole thesis and then make a trade that contradicts it.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Anyway it's not meant to be serious financial advice or anything. Just thought it was a fun way to see how these models actually think when you give them a concrete task. &lt;/p&gt; &lt;p&gt;Code is messy but it works. Considering doing a fully local version to stop burning my openrouter credits...&lt;br /&gt; &lt;a href="http://wallstreetarena.xyz/"&gt;http://wallstreetarena.xyz/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/2degreestarget"&gt; /u/2degreestarget &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p36iln/made_a_site_where_ai_models_trade_against_each/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p36iln/made_a_site_where_ai_models_trade_against_each/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p36iln/made_a_site_where_ai_models_trade_against_each/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T18:35:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1p38wp2</id>
    <title>Dell puts 870 INT8 TOPS in Pro Max 16 Plus laptop with dual Qualcomm AI-100 discrete NPUs and 128GB LPDDR5X</title>
    <updated>2025-11-21T20:08:24+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p38wp2/dell_puts_870_int8_tops_in_pro_max_16_plus_laptop/"&gt; &lt;img alt="Dell puts 870 INT8 TOPS in Pro Max 16 Plus laptop with dual Qualcomm AI-100 discrete NPUs and 128GB LPDDR5X" src="https://external-preview.redd.it/k6ugreuNYLcJLu7o30ZlDvM4GYr0mtEvIvPMXJ8mR2c.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=675947acdabed7d61c07e99006c75339ee1cfa5f" title="Dell puts 870 INT8 TOPS in Pro Max 16 Plus laptop with dual Qualcomm AI-100 discrete NPUs and 128GB LPDDR5X" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Dell is shipping the Pro Max 16 Plus laptop with Qualcomm‚Äôs discrete AI-100 Ultra NPU, delivering 870 INT8 TOPS at 150W TDP with 128GB LPDDR5X memory, enabling local inference of AI models up to 120 billion parameters. The system pairs this with an Intel Core Ultra 9 285HX vPro CPU (24 cores) and 64GB system RAM, but notably omits a discrete GPU, relying instead on Arrow Lake-HX‚Äôs integrated graphics, as the NPU occupies the thermal and power budget typically allocated to a dGPU. The dual-NPU configuration provides 64GB dedicated AI memory and supports FP16 precision inference, positioning the device as an ‚Äúedge server in a backpack‚Äù.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.techpowerup.com/343143/dell-ship-pro-max-16-plus-laptops-with-qualcomms-discrete-npu"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p38wp2/dell_puts_870_int8_tops_in_pro_max_16_plus_laptop/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p38wp2/dell_puts_870_int8_tops_in_pro_max_16_plus_laptop/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T20:08:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1p3b60m</id>
    <title>When do you think open-source models will catch up to Gemini 3/Nano Banana pro? Who's the closest candidate right now?</title>
    <updated>2025-11-21T21:38:16+00:00</updated>
    <author>
      <name>/u/abdouhlili</name>
      <uri>https://old.reddit.com/user/abdouhlili</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm curious about the current gap between open-source models and something like Gemini 3. Do you think open-source will catch up anytime soon, and if so, which model is the closest right now?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdouhlili"&gt; /u/abdouhlili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3b60m/when_do_you_think_opensource_models_will_catch_up/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3b60m/when_do_you_think_opensource_models_will_catch_up/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p3b60m/when_do_you_think_opensource_models_will_catch_up/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T21:38:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1p3s3kt</id>
    <title>What is a good source for rig building for newbies, and why do I see all GPUs sandwiched?</title>
    <updated>2025-11-22T12:30:47+00:00</updated>
    <author>
      <name>/u/designbanana</name>
      <uri>https://old.reddit.com/user/designbanana</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all,&lt;br /&gt; So, this is a question that I would expect is one of many. So instead of &amp;quot;please help me build my rig&amp;quot; I would like to know where could I find good sources on building GPU rigs for LLMs. From hardware selection to optimizing your settings. So that would be my main question &amp;quot;what are good sources for hardware selection&amp;quot;.&lt;/p&gt; &lt;p&gt;I've got a RTX 3090 ti which is nice. But I'm thinking of building a system with 4 x 3090s.&lt;br /&gt; And I think I'll build my own rig using aluminum v slot profiles (10x10mm of which I have many spare parts). &lt;/p&gt; &lt;p&gt;Some questions that do pop up are&lt;br /&gt; - can you build modular? So first 4 GPUs and optional expand to 8GPUs (aside from the PSU)&lt;br /&gt; - can you VNLink a RTX 3090 with a dirtcheap P40? Do they memory pool? (I'm sure this won't work, but ey)&lt;br /&gt; - can you mix GPU types? Like what If I first have 4 x 3090 and i find some cheap cards that have a why-not mentality. Like a few extra cards of 16Gb each since they where so dirt cheap.&lt;/p&gt; &lt;p&gt;Also, why do I see all rigs sandwiching the GPUs against each other? Even is there is marginal space between them? Why not lay them flat with all fans pointing outward? I'm sure there is a reason, but I really wonder :)&lt;/p&gt; &lt;p&gt;circling back, I mostly wonder if there is a place with a hardware overview. So I can see what parts I can keep and what parts I should get.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/designbanana"&gt; /u/designbanana &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3s3kt/what_is_a_good_source_for_rig_building_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3s3kt/what_is_a_good_source_for_rig_building_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p3s3kt/what_is_a_good_source_for_rig_building_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-22T12:30:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1p3e0mp</id>
    <title>GPT-Usenet; an 81-million-parameter model trained on 10 GB of USENET posts(including the entire UTZOO archives) and over 1 GB of various other text files. Reached training loss of 2.3256 and validation loss of 2.3651. MIT licensed.</title>
    <updated>2025-11-21T23:36:11+00:00</updated>
    <author>
      <name>/u/CommodoreCarbonate</name>
      <uri>https://old.reddit.com/user/CommodoreCarbonate</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3e0mp/gptusenet_an_81millionparameter_model_trained_on/"&gt; &lt;img alt="GPT-Usenet; an 81-million-parameter model trained on 10 GB of USENET posts(including the entire UTZOO archives) and over 1 GB of various other text files. Reached training loss of 2.3256 and validation loss of 2.3651. MIT licensed." src="https://preview.redd.it/ski1cmw74p2g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7405c257c9f0583718878eca9a57b452c11abca7" title="GPT-Usenet; an 81-million-parameter model trained on 10 GB of USENET posts(including the entire UTZOO archives) and over 1 GB of various other text files. Reached training loss of 2.3256 and validation loss of 2.3651. MIT licensed." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sample text.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CommodoreCarbonate"&gt; /u/CommodoreCarbonate &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ski1cmw74p2g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3e0mp/gptusenet_an_81millionparameter_model_trained_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p3e0mp/gptusenet_an_81millionparameter_model_trained_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T23:36:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1p35f2c</id>
    <title>I made a free playground for comparing 10+ OCR models side-by-side</title>
    <updated>2025-11-21T17:54:07+00:00</updated>
    <author>
      <name>/u/Emc2fma</name>
      <uri>https://old.reddit.com/user/Emc2fma</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's called OCR Arena, you can try it here: &lt;a href="https://ocrarena.ai"&gt;https://ocrarena.ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There's so many new OCR models coming out all the time, but testing them is really painful. I wanted to give the community an easy way to compare leading foundation VLMs and open source OCR models side-by-side. You can upload any doc, run a variety of models, and view diffs easily.&lt;/p&gt; &lt;p&gt;So far I've added Gemini 3, dots, DeepSeek-OCR, olmOCR 2, Qwen3-VL-8B, and a few others. &lt;/p&gt; &lt;p&gt;Would love any feedback you have! And if there's any other models you'd like included, let me know.&lt;/p&gt; &lt;p&gt;(No surprise, Gemini 3 is top of the leaderboard right now)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Emc2fma"&gt; /u/Emc2fma &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p35f2c/i_made_a_free_playground_for_comparing_10_ocr/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p35f2c/i_made_a_free_playground_for_comparing_10_ocr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p35f2c/i_made_a_free_playground_for_comparing_10_ocr/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T17:54:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1p3pggo</id>
    <title>Rust HF Downloader (Yet Another TUI)</title>
    <updated>2025-11-22T09:53:40+00:00</updated>
    <author>
      <name>/u/johannes_bertens</name>
      <uri>https://old.reddit.com/user/johannes_bertens</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I love the terminal, but I don't exactly love copy-pasting names of models and URLs of a specific quantization or file to download using the huggingface cli.&lt;/p&gt; &lt;p&gt;Probably there's better ways, but I just rolled my own! &lt;/p&gt; &lt;p&gt;--&lt;br /&gt; Introducing: üí• Rust HF Downloader üí•&lt;br /&gt; A Terminal User Interface (TUI) application for searching, browsing, and downloading models from the HuggingFace model hub.&lt;/p&gt; &lt;p&gt;Please break it. And then tell me how you broke it!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/johannes_bertens"&gt; /u/johannes_bertens &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/JohannesBertens/rust-hf-downloader"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3pggo/rust_hf_downloader_yet_another_tui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p3pggo/rust_hf_downloader_yet_another_tui/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-22T09:53:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1p3ntta</id>
    <title>What is the Ollama or llama.cpp equivalent for image generation?</title>
    <updated>2025-11-22T08:06:36+00:00</updated>
    <author>
      <name>/u/liviuberechet</name>
      <uri>https://old.reddit.com/user/liviuberechet</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am looking for some form of terminal based image generator (text to image). I want to use it as a background process for an app I am working on.&lt;/p&gt; &lt;p&gt;I think I can use A1111 without the web interface, but I would like a more ‚Äúopen source‚Äù alternative.&lt;/p&gt; &lt;p&gt;A couple of places mentioned Invoke AI. But then I‚Äôve read it got acquired by Adobe.&lt;/p&gt; &lt;p&gt;A third option would be to just build some custom python script, but that sounds a bit too complex for an MVP development stage.&lt;/p&gt; &lt;p&gt;Any other suggestions?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/liviuberechet"&gt; /u/liviuberechet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3ntta/what_is_the_ollama_or_llamacpp_equivalent_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3ntta/what_is_the_ollama_or_llamacpp_equivalent_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p3ntta/what_is_the_ollama_or_llamacpp_equivalent_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-22T08:06:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1p3w595</id>
    <title>EPSTEIN FILES 20K: Tracking Community Projects</title>
    <updated>2025-11-22T15:35:51+00:00</updated>
    <author>
      <name>/u/tensonaut</name>
      <uri>https://old.reddit.com/user/tensonaut</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The EPSTEIN 20K dataset release on &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; last monday is currently trending on the front page of hugging face &lt;a href="https://huggingface.co/"&gt;https://huggingface.co/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks to this sub, &lt;strong&gt;we now have 5 projects&lt;/strong&gt; running on the dataset. I've started an Github org - EF20K to track them all &lt;a href="https://github.com/EF20K/Projects"&gt;https://github.com/EF20K/Projects&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I plan to spend this weekend working on this project. If you've already built a project on this dataset, please let me know. Also contributors at any level are welcome.&lt;/p&gt; &lt;p&gt;How to contribute:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;a href="https://github.com/EF20K/Projects"&gt;Build a RAG system &lt;/a&gt;- Create your own retrieval system to query the files. Top performing systems will be featured on the projects repo highlights&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/EF20K/Datasets"&gt;Dataset cleaning&lt;/a&gt; - Convert raw jpg files to clean text using vision models for enhance quality. There is lot of room for improving the current OCR output.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/EF20K/Datasets"&gt;Expand the dataset &lt;/a&gt;- Compile additional documents from the Epstein Files releases. There are several documents released before Nov 12 2025, including some interesting ones like flight logs&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/EF20K/Safety"&gt;Safety &amp;amp; accuracy&lt;/a&gt; - Report any concerns or inaccuracies you find in the dataset or the projects.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;For RAG system builders:&lt;/strong&gt; I'm curating Q&amp;amp;A pairs own my own using LLMs for benchmarking due to the sensitive nature of the data. If you would like to collaborate on this, do dm me.&lt;/p&gt; &lt;p&gt;New to contributing to open source projects? Feel free to reach out directly to me to learn how to contribute. I'd be happy to help you get started.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tensonaut"&gt; /u/tensonaut &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3w595/epstein_files_20k_tracking_community_projects/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3w595/epstein_files_20k_tracking_community_projects/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p3w595/epstein_files_20k_tracking_community_projects/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-22T15:35:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1p3d34y</id>
    <title>Inspired by a recent post: a list of the cheapest to most expensive 32GB GPUs on Amazon right now, Nov 21 2025</title>
    <updated>2025-11-21T22:56:25+00:00</updated>
    <author>
      <name>/u/__JockY__</name>
      <uri>https://old.reddit.com/user/__JockY__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Inspired by a recent post where someone was putting together a system based on two 16GB GPUs for $800 I wondered how one might otherwise conveniently acquire 32GB of reasonably performant VRAM as cheaply as possible?&lt;/p&gt; &lt;p&gt;Bezos to the rescue!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Hewlett Packard Enterprise NVIDIA Tesla M10 Quad GPU Module&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Cost: $279&lt;/li&gt; &lt;li&gt;VRAM: GDDR5 (332 GB/s)&lt;/li&gt; &lt;li&gt;PCIe: 3.0&lt;/li&gt; &lt;li&gt;Link: &lt;a href="https://www.amazon.com/Hewlett-Packard-Enterprise-NVIDIA-870046-001/dp/B075VQ5LF8"&gt;https://www.amazon.com/Hewlett-Packard-Enterprise-NVIDIA-870046-001/dp/B075VQ5LF8&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;AMD Radeon Instinct MI60 32GB HBM2 300W&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Cost: $499&lt;/li&gt; &lt;li&gt;VRAM: HBM2 (1.02 TB/s)&lt;/li&gt; &lt;li&gt;PCIe: 4.0&lt;/li&gt; &lt;li&gt;Link: &lt;a href="https://www.amazon.com/Instinct-Compute-Graphics-Accellerator-Renewed/dp/B0DMTTF15B"&gt;https://www.amazon.com/Instinct-Compute-Graphics-Accellerator-Renewed/dp/B0DMTTF15B&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Tesla V100 32GB SXM2 GPU W/Pcie Adapter &amp;amp; 6+2 Pin&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Cost: $879.00&lt;/li&gt; &lt;li&gt;VRAM: HBM2 (898 GB/s)&lt;/li&gt; &lt;li&gt;PCIe: 3.0&lt;/li&gt; &lt;li&gt;Link: &lt;a href="https://www.amazon.com/Tesla-V100-32GB-Adapter-Computing/dp/B0FXWJ8HKD"&gt;https://www.amazon.com/Tesla-V100-32GB-Adapter-Computing/dp/B0FXWJ8HKD&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;NVIDIA Tesla V100 Volta GPU Accelerator 32GB&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Cost: $969&lt;/li&gt; &lt;li&gt;VRAM: HBM2 (898 GB/s)&lt;/li&gt; &lt;li&gt;PCIe: 3.0&lt;/li&gt; &lt;li&gt;Link: &lt;a href="https://www.amazon.com/NVIDIA-Tesla-Volta-Accelerator-Graphics/dp/B07JVNHFFX"&gt;https://www.amazon.com/NVIDIA-Tesla-Volta-Accelerator-Graphics/dp/B07JVNHFFX&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;NVIDIA Tesla V100 (Volta) 32GB&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Cost: $1144&lt;/li&gt; &lt;li&gt;VRAM: HBM2 (898 GB/s)&lt;/li&gt; &lt;li&gt;PCIe: 3.0&lt;/li&gt; &lt;li&gt;Link: &lt;a href="https://www.amazon.com/NVIDIA-Tesla-900-2G503-0310-000-NVLINK-GPU/dp/B07WDDNGXK"&gt;https://www.amazon.com/NVIDIA-Tesla-900-2G503-0310-000-NVLINK-GPU/dp/B07WDDNGXK&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;GIGABYTE AORUS GeForce RTX 5090 Master 32G&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Cost: $2599&lt;/li&gt; &lt;li&gt;VRAM: GDDR7 (1792 GB/s)&lt;/li&gt; &lt;li&gt;PCIe: 5.0&lt;/li&gt; &lt;li&gt;Link: &lt;a href="https://www.amazon.com/GIGABYTE-Graphics-WINDFORCE-GV-N5090AORUS-M-32GD/dp/B0DT7GHQMD"&gt;https://www.amazon.com/GIGABYTE-Graphics-WINDFORCE-GV-N5090AORUS-M-32GD/dp/B0DT7GHQMD&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;PNY NVIDIA GeForce RTX‚Ñ¢ 5090 OC Triple Fan&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Cost: $2749&lt;/li&gt; &lt;li&gt;VRAM: GDDR7 (1792 GB/s)&lt;/li&gt; &lt;li&gt;PCIe: 5.0&lt;/li&gt; &lt;li&gt;Link: &lt;a href="https://www.amazon.com/PNY-GeForce-Overclocked-Graphics-3-5-Slot/dp/B0DTJF8YT4/"&gt;https://www.amazon.com/PNY-GeForce-Overclocked-Graphics-3-5-Slot/dp/B0DTJF8YT4/&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For comparison an RTX 3090 has 24GB of 936.2 GB/s GDDR6X&lt;del&gt;, so for $879 it's hard to grumble about 32GB of 898 GB/s HBM2 in those V100s!&lt;/del&gt; and the AMD card has gotta be tempting for someone at that price! &lt;/p&gt; &lt;p&gt;Edit: the V100 doesn‚Äôt support CUDA 8.x and later, so check compatibility before making impulse buys!&lt;/p&gt; &lt;p&gt;Edit 2: found an MI60!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/__JockY__"&gt; /u/__JockY__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3d34y/inspired_by_a_recent_post_a_list_of_the_cheapest/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3d34y/inspired_by_a_recent_post_a_list_of_the_cheapest/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p3d34y/inspired_by_a_recent_post_a_list_of_the_cheapest/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T22:56:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1p3fwj5</id>
    <title>GLM planning a 30-billion-parameter model release for 2025</title>
    <updated>2025-11-22T01:00:05+00:00</updated>
    <author>
      <name>/u/aichiusagi</name>
      <uri>https://old.reddit.com/user/aichiusagi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3fwj5/glm_planning_a_30billionparameter_model_release/"&gt; &lt;img alt="GLM planning a 30-billion-parameter model release for 2025" src="https://external-preview.redd.it/age5KNQL_0umG4-4KoTku-i61lSg2HdDlBNVJO56C64.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4ee469153e66d145a6de755ed94715567aa83c6e" title="GLM planning a 30-billion-parameter model release for 2025" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aichiusagi"&gt; /u/aichiusagi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://open.substack.com/pub/chinatalk/p/the-zai-playbook?selection=2e7c32de-6ff5-4813-bc26-8be219a73c9d"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3fwj5/glm_planning_a_30billionparameter_model_release/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p3fwj5/glm_planning_a_30billionparameter_model_release/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-22T01:00:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1p3xv28</id>
    <title>Deep Research Agent, an autonomous research agent system</title>
    <updated>2025-11-22T16:46:04+00:00</updated>
    <author>
      <name>/u/martian7r</name>
      <uri>https://old.reddit.com/user/martian7r</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3xv28/deep_research_agent_an_autonomous_research_agent/"&gt; &lt;img alt="Deep Research Agent, an autonomous research agent system" src="https://external-preview.redd.it/a2ZpajA0cDE4dTJnMXYKxvwpmRJR_6Wuut5rPoqfAX7yC2Fpp67_z2jaY8Dw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=05d7bafd92f1750858ecd8c0b51b365c4edcb407" title="Deep Research Agent, an autonomous research agent system" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Repository: &lt;a href="https://github.com/tarun7r/deep-research-agent"&gt;https://github.com/tarun7r/deep-research-agent&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Most &amp;quot;research&amp;quot; agents just summarise the top 3 web search results. I wanted something better. I wanted an agent that could plan, verify, and synthesize information like a human analyst.&lt;/p&gt; &lt;p&gt;How it works (The Architecture): Instead of a single LLM loop, this system orchestrates four specialised agents:&lt;/p&gt; &lt;p&gt;1. The Planner: Analyzes the topic and generates a strategic research plan.&lt;/p&gt; &lt;p&gt;2. The Searcher: An autonomous agent that dynamically decides what to query and when to extract deep content.&lt;/p&gt; &lt;p&gt;3. The Synthesizer: Aggregates findings, prioritizing sources based on credibility scores.&lt;/p&gt; &lt;p&gt;4. The Writer: Drafts the final report with proper citations (APA/MLA/IEEE) and self-corrects if sections are too short.&lt;/p&gt; &lt;p&gt;The &amp;quot;Secret Sauce&amp;quot;: Credibility Scoring One of the biggest challenges with AI research is hallucinations. To solve this, I implemented an automated scoring system. It evaluates sources (0-100) based on domain authority (.edu, .gov) and academic patterns before the LLM ever summarizes them&lt;/p&gt; &lt;p&gt;Built With: Python, LangGraph &amp;amp; LangChain, Google Gemini API, Chainlit&lt;/p&gt; &lt;p&gt;I‚Äôve attached a demo video below showing the agents in action as they tackle a complex topic from scratch.&lt;/p&gt; &lt;p&gt;Check out the code, star the repo, and contribute&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/martian7r"&gt; /u/martian7r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/tkn2fiy18u2g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3xv28/deep_research_agent_an_autonomous_research_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p3xv28/deep_research_agent_an_autonomous_research_agent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-22T16:46:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1p3qxj4</id>
    <title>I created a coding tool that produce prompts simple enough for smaller, local models</title>
    <updated>2025-11-22T11:24:53+00:00</updated>
    <author>
      <name>/u/robertpiosik</name>
      <uri>https://old.reddit.com/user/robertpiosik</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3qxj4/i_created_a_coding_tool_that_produce_prompts/"&gt; &lt;img alt="I created a coding tool that produce prompts simple enough for smaller, local models" src="https://preview.redd.it/ueah8pouks2g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9b5a4b22a09083ae0eef1b09257c99563e73b72a" title="I created a coding tool that produce prompts simple enough for smaller, local models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys. I'm working on a free and open-source tool that is non agentic. This design choice makes messages very simple, as all the model sees are hand-picked files and simple instructions. In the example above, I didn't have to tell the model I wanted to edit &amp;quot;checkpoints&amp;quot; feature, as this is the only feature attached in context.&lt;/p&gt; &lt;p&gt;This simple approach makes it fully viable to code with smaller, locally hosted models like Qwen 32B.&lt;/p&gt; &lt;p&gt;Ollama is listed on the list of providers, and the tool automatically reads downloaded models. The tool allows to also initialize web chats, and Open WebUI is supported.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/robertpiosik"&gt; /u/robertpiosik &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ueah8pouks2g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3qxj4/i_created_a_coding_tool_that_produce_prompts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p3qxj4/i_created_a_coding_tool_that_produce_prompts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-22T11:24:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1p3t4jg</id>
    <title>LlamaTale v0.41.0 - Dungeons v2</title>
    <updated>2025-11-22T13:23:05+00:00</updated>
    <author>
      <name>/u/neph1010</name>
      <uri>https://old.reddit.com/user/neph1010</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's been a while since I posted anything about LlamaTale, and indeed it's been dormant for quite a while, too.&lt;/p&gt; &lt;p&gt;I'm sure most of you don't remember it, but over two years ago I began the project as a mix between a structured text-based, rpg (MUD) and LLM generated content. This was a 1000 years ago in AI time, when we had Llama2 models with 4096 token context length. The goal was to create a persistent experience with &amp;quot;unlimited&amp;quot; play length.&lt;/p&gt; &lt;p&gt;The project has been unattended for almost a year, when I finally got some motivation to start again. Using copilot agent as a pair programmer (and frankly, it's doing the grunt work), we have started adding a few new things, and fixing some old ones.&lt;/p&gt; &lt;p&gt;Most recently we refactored &amp;quot;dungeons&amp;quot; to be reusable anywhere in the game. This update allows them to be added to normal stories, or more interestingly probably, be generated inside &amp;quot;anything&amp;quot; stories.&lt;/p&gt; &lt;p&gt;If it sounds interesting, head over to &lt;a href="https://github.com/neph1/LlamaTale/releases/tag/v0.41.0"&gt;https://github.com/neph1/LlamaTale/releases/tag/v0.41.0&lt;/a&gt; and read more about it. Or AMA. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/neph1010"&gt; /u/neph1010 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3t4jg/llamatale_v0410_dungeons_v2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3t4jg/llamatale_v0410_dungeons_v2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p3t4jg/llamatale_v0410_dungeons_v2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-22T13:23:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1p3xqsu</id>
    <title>Qwen-image-edit-2511 coming next week</title>
    <updated>2025-11-22T16:41:15+00:00</updated>
    <author>
      <name>/u/abdouhlili</name>
      <uri>https://old.reddit.com/user/abdouhlili</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3xqsu/qwenimageedit2511_coming_next_week/"&gt; &lt;img alt="Qwen-image-edit-2511 coming next week" src="https://preview.redd.it/yeofdp077u2g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ef09997e54c4c481e545ec2dd4183f65163c8a73" title="Qwen-image-edit-2511 coming next week" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdouhlili"&gt; /u/abdouhlili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/yeofdp077u2g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3xqsu/qwenimageedit2511_coming_next_week/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p3xqsu/qwenimageedit2511_coming_next_week/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-22T16:41:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1ozopxx</id>
    <title>AMA Announcement: MiniMax, The Opensource Lab Behind MiniMax-M2 + Gifts to Our Community (Wednesday, 8AM-11AM PST)</title>
    <updated>2025-11-17T18:50:44+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozopxx/ama_announcement_minimax_the_opensource_lab/"&gt; &lt;img alt="AMA Announcement: MiniMax, The Opensource Lab Behind MiniMax-M2 + Gifts to Our Community (Wednesday, 8AM-11AM PST)" src="https://preview.redd.it/e3scgr4j5v1g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ca0bf85cb4cf2a2b7e12e8d99d515186275c15e3" title="AMA Announcement: MiniMax, The Opensource Lab Behind MiniMax-M2 + Gifts to Our Community (Wednesday, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/e3scgr4j5v1g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozopxx/ama_announcement_minimax_the_opensource_lab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ozopxx/ama_announcement_minimax_the_opensource_lab/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-17T18:50:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1b550</id>
    <title>AMA with MiniMax ‚Äî Ask Us Anything!</title>
    <updated>2025-11-19T15:46:38+00:00</updated>
    <author>
      <name>/u/OccasionNo6699</name>
      <uri>https://old.reddit.com/user/OccasionNo6699</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;! We‚Äôre really excited to be here, thanks for having us.&lt;/p&gt; &lt;p&gt;I‚Äôm Skyler (&lt;a href="https://www.reddit.com/user/OccasionNo6699/"&gt;u/OccasionNo6699&lt;/a&gt;), head of engineering at MiniMax, the lab behind:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://x.com/MiniMax__AI/status/1982674798649160175?s=20"&gt;MiniMax-M2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983382728343994414"&gt;Hailuo 2.3&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983661667872600296"&gt;MiniMax Speech 2.6&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983964920493568296"&gt;MiniMax Music 2.0&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Joining me today are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Pengyu Zhao, &lt;a href="https://www.reddit.com/user/Wise_Evidence9973/"&gt;u/Wise_Evidence9973&lt;/a&gt; ‚Äî Head of LLM Research&lt;/li&gt; &lt;li&gt;Jade Cai, &lt;a href="https://www.reddit.com/user/srtng/"&gt;u/srtng&lt;/a&gt; ‚Äî Head of Developer Community&lt;/li&gt; &lt;li&gt;midnight_compile , &lt;a href="/u/Top_Cattle_2098"&gt;u/Top_Cattle_2098&lt;/a&gt; ‚Äî LLM Researcher&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 8AM-11AM PST with our core MiniMax tech team continuing to follow up on questions over the next 48 hours.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OccasionNo6699"&gt; /u/OccasionNo6699 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T15:46:38+00:00</published>
  </entry>
</feed>
