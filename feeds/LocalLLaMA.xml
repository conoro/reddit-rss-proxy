<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-13T13:48:49+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1nfaye9</id>
    <title>VaultGemma: The world's most capable differentially private LLM</title>
    <updated>2025-09-12T18:31:30+00:00</updated>
    <author>
      <name>/u/vibjelo</name>
      <uri>https://old.reddit.com/user/vibjelo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfaye9/vaultgemma_the_worlds_most_capable_differentially/"&gt; &lt;img alt="VaultGemma: The world's most capable differentially private LLM" src="https://external-preview.redd.it/Xfy8b5oz8xAgNpbj0L9Mmjzxactj5HdaKRFOmBPu0YE.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c9dad5b13e20f57d64f5fc0bbc7415c9f4186b1d" title="VaultGemma: The world's most capable differentially private LLM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vibjelo"&gt; /u/vibjelo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://research.google/blog/vaultgemma-the-worlds-most-capable-differentially-private-llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfaye9/vaultgemma_the_worlds_most_capable_differentially/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nfaye9/vaultgemma_the_worlds_most_capable_differentially/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-12T18:31:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1netdjp</id>
    <title>Qwen3-Next-80B-A3B - a big step up may be the best open source reasoning model so far</title>
    <updated>2025-09-12T04:00:48+00:00</updated>
    <author>
      <name>/u/Massive-Shift6641</name>
      <uri>https://old.reddit.com/user/Massive-Shift6641</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1netdjp/qwen3next80ba3b_a_big_step_up_may_be_the_best/"&gt; &lt;img alt="Qwen3-Next-80B-A3B - a big step up may be the best open source reasoning model so far" src="https://b.thumbs.redditmedia.com/y5Pqz_JmI2YT4BHTfapll4B_qxuvcEPNdJDCghbe6Uw.jpg" title="Qwen3-Next-80B-A3B - a big step up may be the best open source reasoning model so far" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Recently I presented another music theory problem and explained why it may be a great way to test LLMs' ability: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ndjoek"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1ndjoek&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;I love torturing models with music theory problems. I see a good reason why it may be a good proxy for the models' general ability, if not among the best measurements ever - it tests mostly the LLMs' reasoning ability rather than just knowledge.&lt;br /&gt; &lt;strong&gt;Music theory is not a big subject&lt;/strong&gt; - there is an infinite number of songs that can be written, but the entire music theory is quite compact. It makes it easy to fit it into a LLM and write evals that test their reasoning and comprehension skills rather than just knowledge.&lt;br /&gt; &lt;strong&gt;Most music theory knowledge online is never explored in-depth&lt;/strong&gt; - even most musicians' don't know anything besides basic major and minor chords and their progressions. Since most pretraining data is not particularly high quality, LLMs have to reason to analyze music that is more complex than popular.&lt;br /&gt; &lt;strong&gt;Music theory evals can easily be rewritten and updated if benchmaxxxed and overfit&lt;/strong&gt; - it may take days to even create a programming or math problem that is enough challenging for modern LLMs, but only a few hours to create a song that is beyond most models' ability to understand. (I'm not totally sure about this one)&lt;/p&gt; &lt;p&gt;So I wrote the following:&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/dmvsy194gnof1.png?width=1727&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0f977df8c20d9229dc1be929e12cfc1cba7ba97b"&gt;https://preview.redd.it/dmvsy194gnof1.png?width=1727&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0f977df8c20d9229dc1be929e12cfc1cba7ba97b&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;This piece is special because it is written in Locrian. It is rarely used in popular music because of its inherent tension and lack of resolution (look up John Kirkpatrick's Dust to Dust), and since it is so rare, it makes it a perfect candidate to test the LLMs reasoning ability.&lt;/p&gt; &lt;p&gt;In this track, the signature Locrian sound is created with:&lt;/p&gt; &lt;p&gt;a dissonant diminished triad is outlined with the C-Eb-Gb ostinato at the organ 2 line;&lt;/p&gt; &lt;p&gt;The Gb bassline - a point of relative stability that gives an illusion of a tonal center.&lt;/p&gt; &lt;p&gt;Basically, it is Locrian with a twist - while the actual tonal center is on C, the Gb bass drone sounds more stable than C (where it occasionally plays), so it is easy to misinterpret Gb as tonic simply because it is the most stable note here.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Back then, I was surprised with the performance of all major LLMs on this task - the only two models that consistently identified the correct key and mode (C Locrian) were GPT-5 High and Grok 4. Now I am surprised with the performance of Qwen3-Next.&lt;/p&gt; &lt;h1&gt;Qwen3-next's performance on this task&lt;/h1&gt; &lt;p&gt;I fed the problem to Qwen3-Next in reasoning mode. It has really impressed me with three big improvements over its big brother 235B-A22B-2507:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;It identified the correct C Locrian mode in &lt;strong&gt;half&lt;/strong&gt; of my 10 attempts. 235B-A22B-2507 was not able to identify it more than once, and even so it hallucinated a lot during the process.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Even when it mistakenly identified another mode, it was &lt;strong&gt;always&lt;/strong&gt; a relative mode of C Locrian - that is, a scale that uses &lt;strong&gt;the same notes&lt;/strong&gt; arranged in a different order. Unlike 235B-A22B-2507, Qwen3-Next now always knows the correct notes even if it can't determine their function.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;It stopped hallucinating this much.&lt;/strong&gt; At least far less than 235B-A22B-2507. Previous Qwen was making up a ton of stuff and its delusions made its reasoning look like absolutely random shotgun debugging. Now it is no longer a problem because Qwen3-Next simply never hallucinates notes that do not exist in the scale.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;To make sure the model wasn't overfit on this exact problem since I published it, I also tested it with the same piece transposed into D and F Locrian, and while it struggled to identify F Locrian because it is far less common scale than C and D Locrian, it was able to identify correct note collection most of the time.&lt;/p&gt; &lt;p&gt;Some typical responses from Qwen3-Next:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/zly615mtinof1.png?width=752&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f1388415401d7e308da78d8fe3f5a5649603656d"&gt;https://preview.redd.it/zly615mtinof1.png?width=752&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f1388415401d7e308da78d8fe3f5a5649603656d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/al056pd5jnof1.png?width=914&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f27b62ff6032d0114efe183e64a9e2ac3ce011a3"&gt;https://preview.redd.it/al056pd5jnof1.png?width=914&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f27b62ff6032d0114efe183e64a9e2ac3ce011a3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ov8skbejjnof1.png?width=914&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2915e1d2a8edae6f08cd213874b318c112ed2628"&gt;https://preview.redd.it/ov8skbejjnof1.png?width=914&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2915e1d2a8edae6f08cd213874b318c112ed2628&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So did they make Qwen better? &lt;strong&gt;Yes!&lt;/strong&gt; In fact, it is the first open source model that did this well on this problem.&lt;/p&gt; &lt;p&gt;Now since Qwen became this good, I can only wonder what wonders await us with DeepSeek R2.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Massive-Shift6641"&gt; /u/Massive-Shift6641 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1netdjp/qwen3next80ba3b_a_big_step_up_may_be_the_best/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1netdjp/qwen3next80ba3b_a_big_step_up_may_be_the_best/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1netdjp/qwen3next80ba3b_a_big_step_up_may_be_the_best/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-12T04:00:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1nf8ff4</id>
    <title>Qwen3 Next (Instruct) coding benchmark results</title>
    <updated>2025-09-12T16:52:33+00:00</updated>
    <author>
      <name>/u/mr_riptano</name>
      <uri>https://old.reddit.com/user/mr_riptano</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nf8ff4/qwen3_next_instruct_coding_benchmark_results/"&gt; &lt;img alt="Qwen3 Next (Instruct) coding benchmark results" src="https://external-preview.redd.it/XmbB_Ggpaw13Ih4SiltMb7pnW0SotFk3Ey3eZ2fkjxY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=00fc58d8590fb833a39e70961d2fba37eab593c1" title="Qwen3 Next (Instruct) coding benchmark results" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Why I've chosen to compare with the alternatives you see at the link:&lt;/p&gt; &lt;p&gt;In terms of model size and &amp;quot;is this reasonable to run locally&amp;quot; it makes the most sense to compare Qwen3 Next with GPT-OSS-20b. I've also thrown in GPT5-nano as &amp;quot;probably around the same size as OSS-20b, and at the same price point from hosted vendors&amp;quot;, and all 3 have similar scores. &lt;/p&gt; &lt;p&gt;However, 3rd party inference vendors are currently pricing Qwen3 Next at 3x GPT-OSS-20b, while Alibaba has it at almost 10x more (lol). So I've also included gpt5-mini and flash 2.5 as &amp;quot;in the same price category that Alibaba wants to play in,&amp;quot; and also Alibaba specifically calls out &amp;quot;outperforms flash 2.5&amp;quot; in their release post (lol again).&lt;/p&gt; &lt;p&gt;So: if you're running on discrete GPUs, keep using GPT-OSS-20b. If you're running on a Mac or the new Ryzen AI unified memory chips, Qwen3 Next should be a lot faster for similar performance. And if you're outsourcing your inference then you can either get the same performance for much cheaper, or a much smarter model for the same price.&lt;/p&gt; &lt;p&gt;Note: I tried to benchmark against only Alibaba but the rate limits are too low, so I added DeepInfra as a provider as well. If DeepInfra has things misconfigured these results will be tainted. I've used DeepInfra's pricing for the Cost Efficiency graph at the link.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mr_riptano"&gt; /u/mr_riptano &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://brokk.ai/power-ranking?version=openround-2025-08-20&amp;amp;score=average&amp;amp;models=flash-2.5%2Cgpt-oss-20b%2Cgpt5-mini%2Cgpt5-nano%2Cq3next"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nf8ff4/qwen3_next_instruct_coding_benchmark_results/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nf8ff4/qwen3_next_instruct_coding_benchmark_results/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-12T16:52:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1nfeqhe</id>
    <title>GLM4.5 Air vs Qwen3-Next-80B-A3B?</title>
    <updated>2025-09-12T21:00:41+00:00</updated>
    <author>
      <name>/u/InsideYork</name>
      <uri>https://old.reddit.com/user/InsideYork</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyone with a Mac got some comparisons?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InsideYork"&gt; /u/InsideYork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfeqhe/glm45_air_vs_qwen3next80ba3b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfeqhe/glm45_air_vs_qwen3next80ba3b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nfeqhe/glm45_air_vs_qwen3next80ba3b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-12T21:00:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1neyaph</id>
    <title>A list of models released or udpated last week on this sub, in case you missed any - (12 Sep)</title>
    <updated>2025-09-12T09:08:55+00:00</updated>
    <author>
      <name>/u/aifeed-fyi</name>
      <uri>https://old.reddit.com/user/aifeed-fyi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A quick list of models updates and new releases mentioned in several posts during the week on LocalLLama.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Qwen3-Next-80B-A3B:&lt;/strong&gt; 80B params, only 3B activated per token (10x faster inference, 32K+ context) | ( &lt;a href="https://huggingface.co/collections/Qwen/qwen3-next-68c25fd6838e585db8eeea9d"&gt;HuggingFace&lt;/a&gt; - &lt;a href="https://www.reddit.com/gallery/1nefmzr"&gt;Release&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Jan-v1-2509:&lt;/strong&gt; A new update, improved performance in reasoning and creativity evals | (&lt;a href="https://www.reddit.com/gallery/1ncdobh"&gt;Release&lt;/a&gt; - &lt;a href="https://huggingface.co/janhq/Jan-v1-2509"&gt;HuggingFace&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;MiniCPM4.1-8B:&lt;/strong&gt; 8B hybrid reasoning model (/think vs /no_think) with long context | (&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nbly7o/minicpm418b/"&gt;Release&lt;/a&gt; - &lt;a href="https://huggingface.co/openbmb/MiniCPM4.1-8B"&gt;HuggingFace&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;PyDevMini-1 (4B):&lt;/strong&gt; Matches/outperforms GPT-4 on Python &amp;amp; Web Dev at 1/400th the size | (&lt;a href="https://v.redd.it/nh9fq7qbn2of1"&gt;Release&lt;/a&gt; - &lt;a href="https://huggingface.co/bralynn/pydevmini1"&gt;HuggingFace&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Qwen3-ASR:&lt;/strong&gt; All-in-one multilingual speech recognition (EN/CN + 9 languages) | (&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nbqa1p/qwen_released_api_only_qwen3asr_the_allinone/"&gt;Release&lt;/a&gt; - &lt;a href="https://huggingface.co/spaces/Qwen/Qwen3-ASR-Demo"&gt;Demo&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;IndexTTS-2.0:&lt;/strong&gt; Emotionally expressive, duration-controlled zero-shot TTS | (&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nbkxnm/introducing_indextts20_a_breakthrough_in/"&gt;Release&lt;/a&gt; - &lt;a href="https://github.com/index-tts/index-tts"&gt;Demo&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Aquif-3 Series:&lt;/strong&gt; New reasoning-focused MoE releases | (&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ncccri/aquif358bthink_is_the_proof_that_reasoning_and/"&gt;Aquif-3.5-8B-Think&lt;/a&gt; - &lt;a href="https://www.reddit.com/gallery/1nb3b8l"&gt;Aquif-3-moe 17B&lt;/a&gt; - &lt;a href="https://huggingface.co/mradermacher/aquif-3-moe-17b-a2.8b-GGUF"&gt;HuggingFace&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;ROMA:&lt;/strong&gt; Open-source deep research repo that beats closed-source platforms (ChatGPT, Perplexity, Gemini, etc.) on Seal-0 &amp;amp; FRAMES | (&lt;a href="https://i.redd.it/sxii7uog37of1.jpeg"&gt;Discussion&lt;/a&gt; - &lt;a href="https://github.com/sentient-agi/ROMA"&gt;GitHub&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ernie X1.1 (Baidu):&lt;/strong&gt; A Chinese model released by Baidu approaching the frontier - &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ndjoek/new_ernie_x11_what_may_be_the_best_chinese_model/"&gt;Post&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Datasets&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;FinePDFs (3T tokens):&lt;/strong&gt; Largest PDF dataset ever (0.5B+ docs) | (&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1namz1q/hf_releases_3t_tokens_dataset_sourced_entirely/"&gt;Release&lt;/a&gt; - &lt;a href="https://huggingface.co/datasets/HuggingFaceFW/finepdfs"&gt;HuggingFace&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LongPage:&lt;/strong&gt; 300 full novels with reasoning traces for training writing LLMs | (&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1n99gpq/longpage_300_full_novels_with_reasoning_traces/"&gt;Release&lt;/a&gt; - &lt;a href="https://huggingface.co/datasets/Pageshift-Entertainment/LongPage"&gt;HuggingFace&lt;/a&gt;)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If I missed any, please update in the comments ..&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aifeed-fyi"&gt; /u/aifeed-fyi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1neyaph/a_list_of_models_released_or_udpated_last_week_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1neyaph/a_list_of_models_released_or_udpated_last_week_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1neyaph/a_list_of_models_released_or_udpated_last_week_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-12T09:08:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1nf5j8f</id>
    <title>Long context tested for Qwen3-next-80b-a3b-thinking. Performs very similarly to qwen3-30b-a3b-thinking-2507 and far behind qwen3-235b-a22b-thinking</title>
    <updated>2025-09-12T15:00:15+00:00</updated>
    <author>
      <name>/u/fictionlive</name>
      <uri>https://old.reddit.com/user/fictionlive</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nf5j8f/long_context_tested_for_qwen3next80ba3bthinking/"&gt; &lt;img alt="Long context tested for Qwen3-next-80b-a3b-thinking. Performs very similarly to qwen3-30b-a3b-thinking-2507 and far behind qwen3-235b-a22b-thinking" src="https://preview.redd.it/9df1cyk90rof1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b32809725e64ae3278a2a9f48134b0ee4513eb4e" title="Long context tested for Qwen3-next-80b-a3b-thinking. Performs very similarly to qwen3-30b-a3b-thinking-2507 and far behind qwen3-235b-a22b-thinking" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fictionlive"&gt; /u/fictionlive &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9df1cyk90rof1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nf5j8f/long_context_tested_for_qwen3next80ba3bthinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nf5j8f/long_context_tested_for_qwen3next80ba3bthinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-12T15:00:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1nf6bgp</id>
    <title>Qwen3-Next-80B-A3B: any news on gguf?</title>
    <updated>2025-09-12T15:30:42+00:00</updated>
    <author>
      <name>/u/Herr_Drosselmeyer</name>
      <uri>https://old.reddit.com/user/Herr_Drosselmeyer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been looking on HF, but none seem to be available, which seems odd. Usually, with a high profile release, you'd see some within a day. &lt;/p&gt; &lt;p&gt;So, is there some issue with the model that prevents this for now? Anybody working on it? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Herr_Drosselmeyer"&gt; /u/Herr_Drosselmeyer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nf6bgp/qwen3next80ba3b_any_news_on_gguf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nf6bgp/qwen3next80ba3b_any_news_on_gguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nf6bgp/qwen3next80ba3b_any_news_on_gguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-12T15:30:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1nfj81f</id>
    <title>Olmo 3 on horizon</title>
    <updated>2025-09-13T00:12:19+00:00</updated>
    <author>
      <name>/u/N8Karma</name>
      <uri>https://old.reddit.com/user/N8Karma</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfj81f/olmo_3_on_horizon/"&gt; &lt;img alt="Olmo 3 on horizon" src="https://external-preview.redd.it/u2yaRdnvlvy89ctXNWpzPO4FQ-KtPRHBd0SZB3Yz7X8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2ece5a556d78a2b50b75460c7404069abc51078c" title="Olmo 3 on horizon" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/N8Karma"&gt; /u/N8Karma &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/huggingface/transformers/pull/40778"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfj81f/olmo_3_on_horizon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nfj81f/olmo_3_on_horizon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-13T00:12:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1nfrz5y</id>
    <title>Anyone had any success running local LLMs on a console?</title>
    <updated>2025-09-13T08:11:30+00:00</updated>
    <author>
      <name>/u/Junior-Ad-2186</name>
      <uri>https://old.reddit.com/user/Junior-Ad-2186</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This morning I got a random thought. I haven't really been playing my Xbox (Series S) recently, but wondered if I could use it for some type of small LLM.&lt;/p&gt; &lt;p&gt;I get that this is more of a software limitation more than anything, but it'd be pretty cool if some type of jailbroken version could run Ollama and/or LMStudio, etc..&lt;/p&gt; &lt;p&gt;I feel like the hardware is there! It just sucks that the software is holding it back (as is common in tech lol)&lt;/p&gt; &lt;p&gt;I know it only has ~10GB of RAM, but you could probably run 8B models on this pretty happily? It's got a decent GPU afaict (and the Xbox Series X would be even better)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Junior-Ad-2186"&gt; /u/Junior-Ad-2186 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfrz5y/anyone_had_any_success_running_local_llms_on_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfrz5y/anyone_had_any_success_running_local_llms_on_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nfrz5y/anyone_had_any_success_running_local_llms_on_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-13T08:11:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1nffm7r</id>
    <title>PSA for Ollama Users: Your Context Length Might Be Lower Than You Think</title>
    <updated>2025-09-12T21:35:08+00:00</updated>
    <author>
      <name>/u/gpt872323</name>
      <uri>https://old.reddit.com/user/gpt872323</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I ran into a problem and discovered that Ollama defaults to a 4096 context length for all models, regardless of the model's actual capabilities. It silently truncates any additional context. I had been checking the official Ollama pages and assuming the listed context length was what was being used by default. The ollama ps command, not ollama show &amp;lt;model-name&amp;gt;, is what finally revealed the true context size being used. If you are not daily tinkering on changing models very easy to overlook.&lt;/p&gt; &lt;p&gt;You can chalk this up to user ignorance, but I wanted to share this as a warning for beginners: don't get too excited about running a model with a large context window until you have explicitly set it and checked your memory usage. My primary feedback is for the Ollama website to communicate this default setting more clearly. It is great to see beginners getting involved in running local setups just a heads up to them :)&lt;/p&gt; &lt;p&gt;For many current tasks, a 4096 context is very limiting, though I understand why it might be the default for users with less powerful hardware. It just needs to be communicated more explicitly.&lt;/p&gt; &lt;p&gt;Update: llamers I am admitting I overlooked. I had been using ollama for long before at that time I am not sure if it was or not. The purpose of the post is just information for newbies so they are more aware. I had thought it would default to the model's context if I didn't explicitly set in the env. Feel free to suggest tools alternatives or guides that are user friendly for newbies. We should foster a welcoming environment for them. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gpt872323"&gt; /u/gpt872323 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nffm7r/psa_for_ollama_users_your_context_length_might_be/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nffm7r/psa_for_ollama_users_your_context_length_might_be/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nffm7r/psa_for_ollama_users_your_context_length_might_be/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-12T21:35:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1nfqna6</id>
    <title>MoE Total/Active parameter coefficient. How much further can it go?</title>
    <updated>2025-09-13T06:49:17+00:00</updated>
    <author>
      <name>/u/ihatebeinganonymous</name>
      <uri>https://old.reddit.com/user/ihatebeinganonymous</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi. So far, with Qwen 30B-A3B etc, the ratio between active and total parameters was at a certain range. But with the new Next model, that range has broken.&lt;/p&gt; &lt;p&gt;We have jumped from 10x to ~27x. How much further can it go? What are the limiting factors? Do you imagine e.g. a 300B-3B MoE model? If yes, what would be the equivalent dense parameter count?&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ihatebeinganonymous"&gt; /u/ihatebeinganonymous &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfqna6/moe_totalactive_parameter_coefficient_how_much/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfqna6/moe_totalactive_parameter_coefficient_how_much/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nfqna6/moe_totalactive_parameter_coefficient_how_much/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-13T06:49:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1nfjljo</id>
    <title>RELEASE inclusionAI/Ling-mini-2.0</title>
    <updated>2025-09-13T00:30:10+00:00</updated>
    <author>
      <name>/u/juanlndd</name>
      <uri>https://old.reddit.com/user/juanlndd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfjljo/release_inclusionailingmini20/"&gt; &lt;img alt="RELEASE inclusionAI/Ling-mini-2.0" src="https://b.thumbs.redditmedia.com/oS8LL88rsyR-C5jtj0L74JwL83T5bIh5Z_l1ri9Sv7o.jpg" title="RELEASE inclusionAI/Ling-mini-2.0" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Guys, finally a CPU-ONLY model, just need to quantize!&lt;/p&gt; &lt;p&gt;Inclusion AI released Ling-mini four days ago, and now Ring (the latter is a thought experiment).&lt;/p&gt; &lt;p&gt;16B total parameters, but only 1.4B are activated per input token (non-embedding 789M).&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/npuinnm4utof1.jpg?width=2418&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=8ee2e634e123fe35e911642324050e4ef1d471d5"&gt;https://preview.redd.it/npuinnm4utof1.jpg?width=2418&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=8ee2e634e123fe35e911642324050e4ef1d471d5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is great news for those looking for functional solutions for use without a GPU.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/juanlndd"&gt; /u/juanlndd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfjljo/release_inclusionailingmini20/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfjljo/release_inclusionailingmini20/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nfjljo/release_inclusionailingmini20/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-13T00:30:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1nfaaik</id>
    <title>GPT-OSS:20b &amp; Qwen 4b are a match made in heaven for 24GB VRAM builds</title>
    <updated>2025-09-12T18:05:08+00:00</updated>
    <author>
      <name>/u/No_Information9314</name>
      <uri>https://old.reddit.com/user/No_Information9314</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just wanted to share that after experimenting with several models, most recently Qwen-30b-a3b, I found that gpt-oss:20b and qwen4b loaded into vram together provide a perfect balance of intelligence and speed, with space for about 30k of KV cache. I use gpt-oss for most of my work-related queries that require reasoning, and Qwen 4B generate web search queries. I also have Qwen4 running perplexica which runs very fast - (gpt-oss rather quite slow returning results). &lt;/p&gt; &lt;p&gt;Obviously YMMV but wanted to share this setup in case it may be helpful to others.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Information9314"&gt; /u/No_Information9314 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfaaik/gptoss20b_qwen_4b_are_a_match_made_in_heaven_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfaaik/gptoss20b_qwen_4b_are_a_match_made_in_heaven_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nfaaik/gptoss20b_qwen_4b_are_a_match_made_in_heaven_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-12T18:05:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1nf6s0w</id>
    <title>Qwen3 Next and DeepSeek V3.1 share an identical Artificial Analysis Intelligence Index Score for both their reasoning and non-reasoning modes.</title>
    <updated>2025-09-12T15:48:49+00:00</updated>
    <author>
      <name>/u/R46H4V</name>
      <uri>https://old.reddit.com/user/R46H4V</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nf6s0w/qwen3_next_and_deepseek_v31_share_an_identical/"&gt; &lt;img alt="Qwen3 Next and DeepSeek V3.1 share an identical Artificial Analysis Intelligence Index Score for both their reasoning and non-reasoning modes." src="https://preview.redd.it/hb62e80c7rof1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=45fbd3055204b4282742bcaf6567d07ade494ed5" title="Qwen3 Next and DeepSeek V3.1 share an identical Artificial Analysis Intelligence Index Score for both their reasoning and non-reasoning modes." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/R46H4V"&gt; /u/R46H4V &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hb62e80c7rof1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nf6s0w/qwen3_next_and_deepseek_v31_share_an_identical/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nf6s0w/qwen3_next_and_deepseek_v31_share_an_identical/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-12T15:48:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1nfo1j4</id>
    <title>Building a Personal AI Assistant Without the Cloud (2025 Guide)</title>
    <updated>2025-09-13T04:17:06+00:00</updated>
    <author>
      <name>/u/nanhewa</name>
      <uri>https://old.reddit.com/user/nanhewa</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfo1j4/building_a_personal_ai_assistant_without_the/"&gt; &lt;img alt="Building a Personal AI Assistant Without the Cloud (2025 Guide)" src="https://external-preview.redd.it/q0pbuZw5m9aebEJ2_iKV8VSARNL8LZSN6tvGWaFYcmY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ac83b0c7db47906d8f0584938346fdc0b4901ecc" title="Building a Personal AI Assistant Without the Cloud (2025 Guide)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Cloud assistants are convenient, but they send your data to third-party servers. In 2025 the landscape changed: lightweight open-source LLMs, efficient runtimes, and offline speech stacks make it possible to run a capable AI assistant entirely on your device. This guide walks you through planning, tools, code, and deployment so you can build a privacy-first, offline assistant that understands text and voice, controls local devices, and stays fully under your control.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nanhewa"&gt; /u/nanhewa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.lktechacademy.com/2025/09/building-personal-ai-assistant-without-cloud.html?m=1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfo1j4/building_a_personal_ai_assistant_without_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nfo1j4/building_a_personal_ai_assistant_without_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-13T04:17:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1nf9x9m</id>
    <title>Apple stumbled into succes with MLX</title>
    <updated>2025-09-12T17:50:47+00:00</updated>
    <author>
      <name>/u/Alarming-Ad8154</name>
      <uri>https://old.reddit.com/user/Alarming-Ad8154</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen3-next 80b-a3b is out in mlx on hugging face, MLX already supports it. Open source contributors got this done within 24 hrs. Doing things apple itself couldn’t ever do quickly, simply because the call to support, or not support, specific Chinese AI companies, who’s parent company may or may not be under specific US sanctions would take months if it had the apple brand anywhere near it If apple hadn’t let MLX sort of evolve in its research arm while they tried, and failed, to manage “apple intelligence”, and pulled it into the company, closed it, centralized it, they would be nowhere now. It’s really quite a story arc and I feel with their new M5 chip design having matmul cores (faster prompt processing) they’re actually leaning into it! Apple is never the choice for sort of “go at it on your own” tinkerers, but now it actually is…&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Alarming-Ad8154"&gt; /u/Alarming-Ad8154 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nf9x9m/apple_stumbled_into_succes_with_mlx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nf9x9m/apple_stumbled_into_succes_with_mlx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nf9x9m/apple_stumbled_into_succes_with_mlx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-12T17:50:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1nflbh4</id>
    <title>Qwen3max feels like a manager that had to attend sensitivity training</title>
    <updated>2025-09-13T01:54:45+00:00</updated>
    <author>
      <name>/u/Coldaine</name>
      <uri>https://old.reddit.com/user/Coldaine</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nflbh4/qwen3max_feels_like_a_manager_that_had_to_attend/"&gt; &lt;img alt="Qwen3max feels like a manager that had to attend sensitivity training" src="https://preview.redd.it/371xjrd89uof1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d1fc0fb0a562a6ddb6d774061259f8ebdfe2969e" title="Qwen3max feels like a manager that had to attend sensitivity training" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I really did have someone like this in real life. He was definitely a little bit on the spectrum and didn't get humor at all. People told him to lighten up, and it somehow got even worse when he was trying to be funny. &lt;/p&gt; &lt;p&gt;The rest of my code review did not go as well as the first line, but at least qwen was able to find one good thing about my code. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Coldaine"&gt; /u/Coldaine &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/371xjrd89uof1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nflbh4/qwen3max_feels_like_a_manager_that_had_to_attend/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nflbh4/qwen3max_feels_like_a_manager_that_had_to_attend/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-13T01:54:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1nfhbzv</id>
    <title>Ring-mini-2.0 16B 1.4b MoE</title>
    <updated>2025-09-12T22:46:15+00:00</updated>
    <author>
      <name>/u/HilLiedTroopsDied</name>
      <uri>https://old.reddit.com/user/HilLiedTroopsDied</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfhbzv/ringmini20_16b_14b_moe/"&gt; &lt;img alt="Ring-mini-2.0 16B 1.4b MoE" src="https://external-preview.redd.it/k_1ZiAjClo_PWHpZM0iAYeW3wPAsQ_ZQE2cc_xW7-3o.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7e8a42ffff0966a52b0cd60044e86cd61473738b" title="Ring-mini-2.0 16B 1.4b MoE" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HilLiedTroopsDied"&gt; /u/HilLiedTroopsDied &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/inclusionAI/Ring-mini-2.0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfhbzv/ringmini20_16b_14b_moe/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nfhbzv/ringmini20_16b_14b_moe/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-12T22:46:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1nftdeo</id>
    <title>appreciation post for qwen3 0.6b llm model</title>
    <updated>2025-09-13T09:42:11+00:00</updated>
    <author>
      <name>/u/iamzooook</name>
      <uri>https://old.reddit.com/user/iamzooook</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all, For the last few days I was trying out all the low param llm models which would run on cpu.&lt;/p&gt; &lt;p&gt;I have tested from openai oss 20b, gemma 270m, 1b, 4b, deepseek 1.5b, qwen3 0.6b, 1.7b, 4b, 8b, granite 2b, and many more.&lt;/p&gt; &lt;p&gt;the performance and the reliability of qwen3 0.6b is unmatched to any other models. gemma isn't reliable at all even its 4b model. at the same time qwen3 4b beats oss 20b easily. granite 2b is good backup.&lt;/p&gt; &lt;p&gt;I got rid of all the models and just kept qwen3 0.6b, 4b and granite 2b. this would be my doomsday llm models running on cpu.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iamzooook"&gt; /u/iamzooook &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nftdeo/appreciation_post_for_qwen3_06b_llm_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nftdeo/appreciation_post_for_qwen3_06b_llm_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nftdeo/appreciation_post_for_qwen3_06b_llm_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-13T09:42:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1nfieif</id>
    <title>vLLM Now Supports Qwen3-Next: Hybrid Architecture with Extreme Efficiency</title>
    <updated>2025-09-12T23:33:53+00:00</updated>
    <author>
      <name>/u/chisleu</name>
      <uri>https://old.reddit.com/user/chisleu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfieif/vllm_now_supports_qwen3next_hybrid_architecture/"&gt; &lt;img alt="vLLM Now Supports Qwen3-Next: Hybrid Architecture with Extreme Efficiency" src="https://external-preview.redd.it/K3rGlpkjbDPCdSyb_xOk55T-rqiVrUIviv6vZoP3TV0.png?width=216&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=16235ad425456d8b12ef7e3e92930529dc005885" title="vLLM Now Supports Qwen3-Next: Hybrid Architecture with Extreme Efficiency" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Let's fire it up!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chisleu"&gt; /u/chisleu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://blog.vllm.ai/2025/09/11/qwen3-next.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfieif/vllm_now_supports_qwen3next_hybrid_architecture/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nfieif/vllm_now_supports_qwen3next_hybrid_architecture/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-12T23:33:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1nf7zhq</id>
    <title>Meta released MobileLLM-R1 on Hugging Face</title>
    <updated>2025-09-12T16:35:23+00:00</updated>
    <author>
      <name>/u/Illustrious_Row_9971</name>
      <uri>https://old.reddit.com/user/Illustrious_Row_9971</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nf7zhq/meta_released_mobilellmr1_on_hugging_face/"&gt; &lt;img alt="Meta released MobileLLM-R1 on Hugging Face" src="https://preview.redd.it/huchm6bahrof1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f091dea3c1b3bd8cc946d3ae61d24b3e9a2e3a3b" title="Meta released MobileLLM-R1 on Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;model: &lt;a href="https://huggingface.co/facebook/MobileLLM-R1-950M"&gt;https://huggingface.co/facebook/MobileLLM-R1-950M&lt;/a&gt;&lt;/p&gt; &lt;p&gt;app (vibe coded): &lt;a href="https://huggingface.co/spaces/akhaliq/MobileLLM-R1-950M"&gt;https://huggingface.co/spaces/akhaliq/MobileLLM-R1-950M&lt;/a&gt;&lt;/p&gt; &lt;p&gt;app was made in: &lt;a href="https://huggingface.co/spaces/akhaliq/anycoder"&gt;https://huggingface.co/spaces/akhaliq/anycoder&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Illustrious_Row_9971"&gt; /u/Illustrious_Row_9971 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/huchm6bahrof1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nf7zhq/meta_released_mobilellmr1_on_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nf7zhq/meta_released_mobilellmr1_on_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-12T16:35:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1nfqr69</id>
    <title>WarLlama: 2x MI50 LLM MicroATX Server</title>
    <updated>2025-09-13T06:55:54+00:00</updated>
    <author>
      <name>/u/__E8__</name>
      <uri>https://old.reddit.com/user/__E8__</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfqr69/warllama_2x_mi50_llm_microatx_server/"&gt; &lt;img alt="WarLlama: 2x MI50 LLM MicroATX Server" src="https://b.thumbs.redditmedia.com/h9qMX2YlXQIKQi4YG23oXTimROWCT0LUz66l7zJkwKg.jpg" title="WarLlama: 2x MI50 LLM MicroATX Server" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Some ppl on this sub have Ahab-class dreadnoughts rocking a DeepSeek/Kimi high quant. Other have a warhorse w a giant gpu or six (or 16x?). This is my sleek lil &lt;em&gt;warllama&lt;/em&gt;. &lt;/p&gt; &lt;p&gt;It's is not abt the bling-bling; it's abt the ching-ching: how little money I spend building a little power house. It came out comely, but it was meant to be minimalist-- a pure headless Linux box running llama.cpp + rocm (which needs freq reboots from lots of llm usage) w a comfy 64gb vram. Cost of main parts: &lt;span class="md-spoiler-text"&gt;$730&lt;/span&gt;. The bells &amp;amp; whistles prob costs another $200+ nowadays but I bought most of it bf the recent (hyper)inflation/tariff BS. YMMV. &lt;/p&gt; &lt;p&gt;WARNING: I flout every sensible guideline in the &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1j09qk1/dell_t640_for_a_4x_3090_build/"&gt;LocalLlama build guidebook&lt;/a&gt;: super tight case, ancient desktop mobo, weird gpus, buggy drivers, even buggier vbioxen, cramped airflow. You'll prob be eaten by a Grue.&lt;/p&gt; &lt;h2&gt;Write-Up Sections:&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;PC Parts &amp;amp; Costs&lt;/li&gt; &lt;li&gt;Benchmarks &amp;amp; Temperatures&lt;/li&gt; &lt;li&gt;Notes&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;PC HW/SW Parts &amp;amp; Costs&lt;/h1&gt; &lt;h2&gt;HW&lt;/h2&gt; &lt;p&gt;It's all abt the models, then the gpus. The main computer is an afterthought.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Price&lt;/th&gt; &lt;th align="left"&gt;Part&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;$400&lt;/td&gt; &lt;td align="left"&gt;2x mi50 32gb&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;$130&lt;/td&gt; &lt;td align="left"&gt;Asus Maximus VIII Gene + 32gb ddr4 + i5-6600k&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;$35&lt;/td&gt; &lt;td align="left"&gt;Powertrain X100 PC case&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;$60&lt;/td&gt; &lt;td align="left"&gt;ESGaming 750w modular PSU&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;$50&lt;/td&gt; &lt;td align="left"&gt;1tb nvme&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;$17&lt;/td&gt; &lt;td align="left"&gt;ARGB CPU fan&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;$8&lt;/td&gt; &lt;td align="left"&gt;2x delta fans&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;?&lt;/td&gt; &lt;td align="left"&gt;various 3D printer parts: fan shroud, i/o shield, gpu stand, psu mount&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;$4&lt;/td&gt; &lt;td align="left"&gt;18pin ribbon cable for extending mobo front panels pins around mi50&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;TOTAL: $731&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h2&gt;Bells &amp;amp; Whistles (no idea what these cost nowadays)&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Razer Chroma ARGB controller (6ch, perfect openrgb ctrl)&lt;/li&gt; &lt;li&gt;lcd 2004 + i2c adap&lt;/li&gt; &lt;li&gt;ch341: usb to i2c/gpio&lt;/li&gt; &lt;li&gt;ARGB 120mm case fan&lt;/li&gt; &lt;li&gt;usb cables/adap for internal usb devs&lt;/li&gt; &lt;li&gt;2x ARGB magnetic led strips&lt;/li&gt; &lt;li&gt;2x pcie Y-splitter for gpus&lt;/li&gt; &lt;li&gt;vga/hdmi car-rearview monitor&lt;/li&gt; &lt;li&gt;ezOutlet5 (poor man's bmc)&lt;/li&gt; &lt;li&gt;keyboard&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Smaller than a 24pack of soda. Heavy like a chonky cat.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Dim: 349 x 185 x 295mm (19L, I think)&lt;/li&gt; &lt;li&gt;Total Weight: 19.3lb (8.68kg)&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;SW&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Ubuntu 22.04 + 6.8 hwe kernel &lt;/li&gt; &lt;li&gt;rocm 6.4.1 (6.4.4 ripped out mi50 supp!)&lt;/li&gt; &lt;li&gt;llama.cpp -&amp;gt; build_rocm&lt;/li&gt; &lt;li&gt;vbios: 113-D1631700-111 (orig hacky vbios that shipped w mi50).&lt;/li&gt; &lt;li&gt;bios: v0402 (mobo had first oem bios bf update)&lt;/li&gt; &lt;li&gt;openrgb (for python argb ctrl)&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/frank-zago/ch341-i2c-spi-gpio"&gt;ch341 linux driver&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Benchmarks &amp;amp; Temperatures&lt;/h1&gt; &lt;p&gt;Put into comment below&lt;/p&gt; &lt;h1&gt;Notes&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1m389gi/32gb_mi50_but_llamacpp_vulkan_sees_only_16gb/"&gt;mi50 vbios misadventures&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1j09qk1/dell_t640_for_a_4x_3090_build/"&gt;Building a chonker multi-gpu rig considerations&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1mo92ou/rtx_5090_vs_rtx_4090_48gb_or_rtx_6000/"&gt;How much HW do I rly need??? Vram Eaters vs the Gpu Cartel&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;you cant dress trash until you spend a lotta money. building smthg like this can only be done w v clear sw req assessment and a whole lotta hw expertise. multi-gpu compat on old hw is v arcane; esp w mi50s.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;target model: qwen family. v versatile, hq, instructable. v lil refusal bs.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;usecases: filing cooking recipes, modernizing Rolodex, doing arithmetic on dozens (!) of tabular cells. Or how abt: erp, dank memes, navigation calcs (dont wanna fly thru a star when i hit lightspeed)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;mobo is 10yro but is one of the slickest boards i've ever owned&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;its miraculous i was able to fit everything into case. the gpus, the fans &amp;amp; mounts. the normal atx cable lengths. the long (160mm) full sized atx psu. sff builds take more parts bc need to get evryhting to fit. either custom 3d printed plastic or workarounds like ribbon cables&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;similarly there's enough airflow thru such smol spaces to keep things undr 70C during llama-bench&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;i needed to ext the pin headers on the bottom edge of the mobo. 2.54mm pitch ribbon cables to the rescue. still needed to grind a few edges, but it works&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;i pray my nvme will last forevaaaaaah bc id need to tear the whole thing apart to swap drives.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;econ of cheap hw are terrible outside of hobbyests. for viable business, a comp builder would need to make thousands per box. but nobody is gonna pay that for less than multi-gpu behemoths. DIY or DIE.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;the mi50 appears to be the second coming of the P40 due to software advances from gents like these. thanks guys! &lt;a href="https://github.com/ggml-org/llama.cpp/pull/15884"&gt;Flash attn for mi50&lt;/a&gt;. &lt;a href="https://github.com/iacopPBK/llama.cpp-gfx906"&gt;Part2&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;a 4x mi50 rig would be excellent, but exps w 2x tell me sorting out the pcie rsrc alloc issues would be more work than usual for multi-gpu. and still too smol for deepseek&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/__E8__"&gt; /u/__E8__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nfqr69"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfqr69/warllama_2x_mi50_llm_microatx_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nfqr69/warllama_2x_mi50_llm_microatx_server/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-13T06:55:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1nfqe2c</id>
    <title>What's with the obsession with reasoning models?</title>
    <updated>2025-09-13T06:33:35+00:00</updated>
    <author>
      <name>/u/HadesThrowaway</name>
      <uri>https://old.reddit.com/user/HadesThrowaway</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is just a mini rant so I apologize beforehand. Why are practically all AI model releases in the last few months all reasoning models? Even those that aren't are now &amp;quot;hybrid thinking&amp;quot; models. It's like every AI corpo is obsessed with reasoning models currently. &lt;/p&gt; &lt;p&gt;I personally dislike reasoning models, it feels like their only purpose is to help answer tricky riddles at the cost of a huge waste of tokens. &lt;/p&gt; &lt;p&gt;It also feels like everything is getting increasingly benchmaxxed. Models are overfit on puzzles and coding at the cost of creative writing and general intelligence. I think a good example is Deepseek v3.1 which, although technically benchmarking better than v3-0324, feels like a worse model in many ways.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HadesThrowaway"&gt; /u/HadesThrowaway &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfqe2c/whats_with_the_obsession_with_reasoning_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfqe2c/whats_with_the_obsession_with_reasoning_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nfqe2c/whats_with_the_obsession_with_reasoning_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-13T06:33:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1nfktdg</id>
    <title>To The Qwen Team, Kindly Contribute to Qwen3-Next GGUF Support!</title>
    <updated>2025-09-13T01:29:35+00:00</updated>
    <author>
      <name>/u/Iory1998</name>
      <uri>https://old.reddit.com/user/Iory1998</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfktdg/to_the_qwen_team_kindly_contribute_to_qwen3next/"&gt; &lt;img alt="To The Qwen Team, Kindly Contribute to Qwen3-Next GGUF Support!" src="https://b.thumbs.redditmedia.com/FW-gGORXDRiCTDczp9dCooqVX1C79rwrf1Z5V7TuZEM.jpg" title="To The Qwen Team, Kindly Contribute to Qwen3-Next GGUF Support!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you haven't noticed already, Qwen3-Next hasn't yet been supported in llama.cpp, and that's because it comes with a custom SSM archiecture. Without the support of the Qwen team, this amazing model might not be supported for weeks or even months. By now, I strongly believe that llama.cpp day one support is an absolute must.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/uxcnskn54uof1.png?width=938&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5d39e54d6738531f99913794f3f2917609ff47b3"&gt;https://preview.redd.it/uxcnskn54uof1.png?width=938&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5d39e54d6738531f99913794f3f2917609ff47b3&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Iory1998"&gt; /u/Iory1998 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfktdg/to_the_qwen_team_kindly_contribute_to_qwen3next/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfktdg/to_the_qwen_team_kindly_contribute_to_qwen3next/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nfktdg/to_the_qwen_team_kindly_contribute_to_qwen3next/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-13T01:29:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1nfrzbv</id>
    <title>I made a game using LLMs (gpt-oss:20b) -- Among LLMs: You are the Impostor</title>
    <updated>2025-09-13T08:11:45+00:00</updated>
    <author>
      <name>/u/Alone-Foundation-134</name>
      <uri>https://old.reddit.com/user/Alone-Foundation-134</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfrzbv/i_made_a_game_using_llms_gptoss20b_among_llms_you/"&gt; &lt;img alt="I made a game using LLMs (gpt-oss:20b) -- Among LLMs: You are the Impostor" src="https://preview.redd.it/56ap88aa4wof1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e2062ede075a0f3745d9fa4bbb37dd8f7ed5d633" title="I made a game using LLMs (gpt-oss:20b) -- Among LLMs: You are the Impostor" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I made the following application/game in Python using Ollama and gpt-oss:20b model by OpenAI -- for people like me who likes to see and create chaos. Please check it out if interested. Github link at the end.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;Among LLMs&lt;/strong&gt; turns your &lt;strong&gt;terminal&lt;/strong&gt; into a chaotic chatroom playground where you’re the only human among a bunch of eccentric AI agents, dropped into a common scenario -- it could be Fantasy, Sci-Fi, Thriller, Crime, or something completely unexpected. Each participant, including you, has a persona and a backstory, and all the AI agents share one common goal -- determine and eliminate the human, through voting. &lt;strong&gt;&lt;em&gt;Your mission:&lt;/em&gt;&lt;/strong&gt; &lt;strong&gt;stay hidden, manipulate conversations, and turn the bots against each other with edits, whispers, impersonations, and clever gaslighting&lt;/strong&gt;. Outlast everyone, turn chaos to your advantage, and make it to the final two.&lt;/p&gt; &lt;p&gt;Can you survive the hunt and &lt;em&gt;outsmart&lt;/em&gt; the AI?&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Quick Demo: &lt;a href="https://youtu.be/kbNe9WUQe14"&gt;https://youtu.be/kbNe9WUQe14&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/0xd3ba/among-llms"&gt;https://github.com/0xd3ba/among-llms&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Alone-Foundation-134"&gt; /u/Alone-Foundation-134 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/56ap88aa4wof1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfrzbv/i_made_a_game_using_llms_gptoss20b_among_llms_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nfrzbv/i_made_a_game_using_llms_gptoss20b_among_llms_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-13T08:11:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1nct0z8</id>
    <title>Our 3rd AMA: Unsloth Team, Creators of the lightning-fast Unsloth fine-tuning library! (Wednesday, 10 AM-1 PM PST)</title>
    <updated>2025-09-09T19:47:12+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nct0z8/our_3rd_ama_unsloth_team_creators_of_the/"&gt; &lt;img alt="Our 3rd AMA: Unsloth Team, Creators of the lightning-fast Unsloth fine-tuning library! (Wednesday, 10 AM-1 PM PST)" src="https://preview.redd.it/7vh8enuu07of1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e0f499252613d5bcf478fb1b75c594bb50f43436" title="Our 3rd AMA: Unsloth Team, Creators of the lightning-fast Unsloth fine-tuning library! (Wednesday, 10 AM-1 PM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7vh8enuu07of1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nct0z8/our_3rd_ama_unsloth_team_creators_of_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nct0z8/our_3rd_ama_unsloth_team_creators_of_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T19:47:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndjxdt</id>
    <title>AMA with the Unsloth team</title>
    <updated>2025-09-10T17:02:18+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLlama"&gt;r/LocalLlama&lt;/a&gt;, I'm Daniel from &lt;a href="https://docs.unsloth.ai/"&gt;Unsloth&lt;/a&gt;! You might know us from our RL &amp;amp; fine-tuning open-source framework, our GGUFs, kernels or bug fixes. We’re super excited to answer all your questions!! 🦥 Our GitHub: &lt;a href="https://github.com/unslothai/unsloth"&gt;https://github.com/unslothai/unsloth&lt;/a&gt;&lt;/p&gt; &lt;p&gt;To celebrate the AMA, we’re releasing Aider Polyglot benchmarks comparing our DeepSeek-V3.1 Dynamic GGUFs to other models and quants. We also made a Localllama post here: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ndibn1/unsloth_dynamic_ggufs_aider_polyglot_benchmarks/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1ndibn1/unsloth_dynamic_ggufs_aider_polyglot_benchmarks/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Our participants:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Daniel, u/danielhanchen&lt;/li&gt; &lt;li&gt;Michael, u/yoracale&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 10AM – 1PM PST, with the Unsloth team continuing to follow up on questions over the next 48 hours.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Thanks so much!🥰&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndjxdt/ama_with_the_unsloth_team/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndjxdt/ama_with_the_unsloth_team/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ndjxdt/ama_with_the_unsloth_team/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T17:02:18+00:00</published>
  </entry>
</feed>
