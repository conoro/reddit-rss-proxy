<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-30T04:49:35+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ojp5gf</id>
    <title>Built a research automation system that actually adapts its workflow dynamically</title>
    <updated>2025-10-30T03:20:53+00:00</updated>
    <author>
      <name>/u/TapOnly5061</name>
      <uri>https://old.reddit.com/user/TapOnly5061</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1ojp5gf/video/lke8wf8v36yf1/player"&gt;https://reddit.com/link/1ojp5gf/video/lke8wf8v36yf1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Ugh, so tired of tools that force you into their ecosystem. &amp;quot;Oh you want research automation? Cool, use our API, follow our process, and kiss your flexibility goodbye.&amp;quot;&lt;/p&gt; &lt;p&gt;freephdlabor doesn't give a damn what you're running. Local models? Sure. OpenAI? Fine. Mix of both? Whatever works for you.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How it works:&lt;/strong&gt; Instead of rigid workflows, agents actually make decisions about what to do next based on results. ManagerAgent coordinates everything while specialized agents handle experiments, writing, review, etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Real talk:&lt;/strong&gt; Gave it &amp;quot;can we predict neural network training phases?&amp;quot; before going to bed. Woke up to a full paper with actual experiments. Not gonna lie, had to do a double-take.&lt;/p&gt; &lt;p&gt;Setup is straightforward:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;git clone https://github.com/ltjed/freephdlabor.git conda env create -f environment.yml python launch_multiagent.py --task &amp;quot;Your research idea&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The whole point is democratizing research automation. You shouldn't need Google's budget to have AI working on research problems 24/7.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GitHub: &lt;a href="https://github.com/ltjed/freephdlabor"&gt;https://github.com/ltjed/freephdlabor&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Demo: &lt;a href="https://freephdlabor.github.io/"&gt;https://freephdlabor.github.io/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Paper: &lt;a href="https://arxiv.org/abs/2510.15624"&gt;https://arxiv.org/abs/2510.15624&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Anyone building similar tools for local setups? What models are you finding work best for research tasks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TapOnly5061"&gt; /u/TapOnly5061 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojp5gf/built_a_research_automation_system_that_actually/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojp5gf/built_a_research_automation_system_that_actually/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ojp5gf/built_a_research_automation_system_that_actually/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-30T03:20:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1ojdvg4</id>
    <title>OpenSkills - a open sourced and completely private Claude Skills</title>
    <updated>2025-10-29T19:20:35+00:00</updated>
    <author>
      <name>/u/badhiyahai</name>
      <uri>https://old.reddit.com/user/badhiyahai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojdvg4/openskills_a_open_sourced_and_completely_private/"&gt; &lt;img alt="OpenSkills - a open sourced and completely private Claude Skills" src="https://external-preview.redd.it/b3ZuaTI3bHVvM3lmMcQPZ78OnlmSenf3dLgyDsdkhOOH_RPCHGpkItvsPv93.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=34cfbcafe873157b2648db339edf7f2cdbd1c277" title="OpenSkills - a open sourced and completely private Claude Skills" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Managed to build a completely local and Claude independent Skills. &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/bandarlabs/open-skills"&gt;https://github.com/bandarlabs/open-skills&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You can import any existing claude skills (or its zip file dowloaded from claude desktop) and it will run it in a local code execution container, with dare I say, better isolation than docker containers. (caveat: its only for MacOS)&lt;/p&gt; &lt;p&gt;Above video shows how it worked with Gemini CLI. You can use any other LLM (even claude code) which supports MCP.&lt;/p&gt; &lt;p&gt;It's private because your pdf (or videos/photos) doesn't leave your system. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/badhiyahai"&gt; /u/badhiyahai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/rk4pq9luo3yf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojdvg4/openskills_a_open_sourced_and_completely_private/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ojdvg4/openskills_a_open_sourced_and_completely_private/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-29T19:20:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1ojpd1w</id>
    <title>Llama 3 8B Instruct Offline LLM is censored</title>
    <updated>2025-10-30T03:31:30+00:00</updated>
    <author>
      <name>/u/GlassHuckleberry3397</name>
      <uri>https://old.reddit.com/user/GlassHuckleberry3397</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojpd1w/llama_3_8b_instruct_offline_llm_is_censored/"&gt; &lt;img alt="Llama 3 8B Instruct Offline LLM is censored" src="https://b.thumbs.redditmedia.com/qadDLEZlxFGo8jK9tVCk2grQCcrl9e40ylJn-rLp7ic.jpg" title="Llama 3 8B Instruct Offline LLM is censored" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/m70g0lc256yf1.png?width=829&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1e6c2c192278fed2d9879d79b088171b3278c6a0"&gt;https://preview.redd.it/m70g0lc256yf1.png?width=829&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1e6c2c192278fed2d9879d79b088171b3278c6a0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/11h8t48456yf1.png?width=838&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0e9c98a1d2d55b70fcb137fb26542fd0f16459cc"&gt;https://preview.redd.it/11h8t48456yf1.png?width=838&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0e9c98a1d2d55b70fcb137fb26542fd0f16459cc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I've given it alot of information from 2023-2025 and I am now trying to check the censorship, and now it wont give me anything thats related to israel&lt;/p&gt; &lt;p&gt;How can i fix this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GlassHuckleberry3397"&gt; /u/GlassHuckleberry3397 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojpd1w/llama_3_8b_instruct_offline_llm_is_censored/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojpd1w/llama_3_8b_instruct_offline_llm_is_censored/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ojpd1w/llama_3_8b_instruct_offline_llm_is_censored/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-30T03:31:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1ojpmv4</id>
    <title>GPTars-like assistant?</title>
    <updated>2025-10-30T03:45:29+00:00</updated>
    <author>
      <name>/u/Mysterious_Bluejay_5</name>
      <uri>https://old.reddit.com/user/Mysterious_Bluejay_5</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Im gonna be honest, I have no idea what I'm doing. I have a raspberry pi 3b set up to the point that it can connect to the internet and download things, but I have no idea where to even begin with getting this set up.&lt;/p&gt; &lt;p&gt;It doesn't need to be advanced, basically just a local AI &amp;quot;buddy&amp;quot; (yes I'm aware it's not alive, this is more a novelty project than anything)&lt;/p&gt; &lt;p&gt;How do I do this?&lt;/p&gt; &lt;p&gt;Edit: minus the movement. I'm not ambitious enough to make it crawl&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mysterious_Bluejay_5"&gt; /u/Mysterious_Bluejay_5 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojpmv4/gptarslike_assistant/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojpmv4/gptarslike_assistant/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ojpmv4/gptarslike_assistant/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-30T03:45:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1oj1qa0</id>
    <title>dots.llm2 is coming...?</title>
    <updated>2025-10-29T11:15:40+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oj1qa0/dotsllm2_is_coming/"&gt; &lt;img alt="dots.llm2 is coming...?" src="https://preview.redd.it/i6wxpmesa1yf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e64d36264e954e063e2a028e8ff47183637fb76e" title="dots.llm2 is coming...?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/rednote-hilab/dots.llm1.inst"&gt;https://huggingface.co/rednote-hilab/dots.llm1.inst&lt;/a&gt; is 143B MoE model published about half year ago (supported by llama.cpp)&lt;/p&gt; &lt;p&gt;dots2: &lt;a href="https://x.com/xeophon_/status/1982728458791968987"&gt;https://x.com/xeophon_/status/1982728458791968987&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&amp;quot;The dots.llm2 model was introduced by the rednote-hilab team. It is a 30B/343B MoE (Mixture-of-Experts) model supporting a 256k context window.&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/i6wxpmesa1yf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oj1qa0/dotsllm2_is_coming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oj1qa0/dotsllm2_is_coming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-29T11:15:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1oj32pd</id>
    <title>OpenAI: gpt-oss-safeguard: two open-weight reasoning models built for safety classification (Now on Hugging Face)</title>
    <updated>2025-10-29T12:23:51+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;gpt-oss-safeguard lets developers use their own custom policies to classify content. The model interprets those policies to classify messages, responses, and conversations.&lt;br /&gt; These models are fine-tuned versions of our gpt-oss open models, available under Apache 2.0 license.&lt;br /&gt; Now on Hugging Face: &lt;a href="https://x.com/OpenAI/status/1983507392374641071"&gt;https://x.com/OpenAI/status/1983507392374641071&lt;/a&gt;&lt;br /&gt; Introducing gpt-oss-safeguard - New open safety reasoning models (120b and 20b) that support custom safety policies: &lt;a href="https://openai.com/index/introducing-gpt-oss-safeguard/"&gt;https://openai.com/index/introducing-gpt-oss-safeguard/&lt;/a&gt;&lt;br /&gt; Hugging Face: &lt;a href="https://huggingface.co/collections/openai/gpt-oss-safeguard"&gt;https://huggingface.co/collections/openai/gpt-oss-safeguard&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oj32pd/openai_gptosssafeguard_two_openweight_reasoning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oj32pd/openai_gptosssafeguard_two_openweight_reasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oj32pd/openai_gptosssafeguard_two_openweight_reasoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-29T12:23:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1oitanf</id>
    <title>Just dropped Kani TTS English - a 400M TTS model that's 5x faster than realtime on RTX 4080</title>
    <updated>2025-10-29T02:43:55+00:00</updated>
    <author>
      <name>/u/ylankgz</name>
      <uri>https://old.reddit.com/user/ylankgz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oitanf/just_dropped_kani_tts_english_a_400m_tts_model/"&gt; &lt;img alt="Just dropped Kani TTS English - a 400M TTS model that's 5x faster than realtime on RTX 4080" src="https://external-preview.redd.it/RI-49DjoVUsr4ENUSH69P2WGRcLvEX3r6pAVokLb70g.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a9b109f2dbdc8b8f12ceaf8eb5a8b638cb926f5c" title="Just dropped Kani TTS English - a 400M TTS model that's 5x faster than realtime on RTX 4080" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;We've been quietly grinding, and today, we're pumped to share the new release of KaniTTS English, as well as Japanese, Chinese, German, Spanish, Korean and Arabic models.&lt;/p&gt; &lt;p&gt;Benchmark on &lt;a href="https://vast.ai/"&gt;VastAI&lt;/a&gt;: RTF (Real-Time Factor) of ~0.2 on RTX4080, ~0.5 on RTX3060.&lt;/p&gt; &lt;p&gt;It has 400M parameters. We achieved this speed by pairing an &lt;a href="https://huggingface.co/LiquidAI/LFM2-350M"&gt;LFM2-350M&lt;/a&gt; backbone with an efficient &lt;a href="https://huggingface.co/nvidia/nemo-nano-codec-22khz-0.6kbps-12.5fps"&gt;NanoCodec&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;It's released under the Apache 2.0 License so you can use it for almost anything.&lt;/p&gt; &lt;p&gt;What Can You Build? - Real-Time Conversation. - Affordable Deployment: It's light enough to run efficiently on budget-friendly hardware, like RTX 30x, 40x, 50x - Next-Gen Screen Readers &amp;amp; Accessibility Tools.&lt;/p&gt; &lt;p&gt;Model Page: &lt;a href="https://huggingface.co/nineninesix/kani-tts-400m-en"&gt;https://huggingface.co/nineninesix/kani-tts-400m-en&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Pretrained Checkpoint: &lt;a href="https://huggingface.co/nineninesix/kani-tts-400m-0.3-pt"&gt;https://huggingface.co/nineninesix/kani-tts-400m-0.3-pt&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Github Repo with Fine-tuning/Dataset Preparation pipelines: &lt;a href="https://github.com/nineninesix-ai/kani-tts"&gt;https://github.com/nineninesix-ai/kani-tts&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Demo Space: &lt;a href="https://huggingface.co/spaces/nineninesix/KaniTTS"&gt;https://huggingface.co/spaces/nineninesix/KaniTTS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;OpenAI-Compatible API Example (Streaming): If you want to drop this right into your existing project, check out our vLLM implementation: &lt;a href="https://github.com/nineninesix-ai/kanitts-vllm"&gt;https://github.com/nineninesix-ai/kanitts-vllm&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Voice Cloning Demo (currently unstable): &lt;a href="https://huggingface.co/spaces/nineninesix/KaniTTS_Voice_Cloning_dev"&gt;https://huggingface.co/spaces/nineninesix/KaniTTS_Voice_Cloning_dev&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Our Discord Server: &lt;a href="https://discord.gg/NzP3rjB4SB"&gt;https://discord.gg/NzP3rjB4SB&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ylankgz"&gt; /u/ylankgz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/nineninesix/kani-tts-400m-en"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oitanf/just_dropped_kani_tts_english_a_400m_tts_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oitanf/just_dropped_kani_tts_english_a_400m_tts_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-29T02:43:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1ojlzr0</id>
    <title>Latent Control Adapters: Multi-vector steering for local LLMs (open Python library for AI safety research, jailbreaking, or whatever)</title>
    <updated>2025-10-30T00:50:40+00:00</updated>
    <author>
      <name>/u/JEs4</name>
      <uri>https://old.reddit.com/user/JEs4</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojlzr0/latent_control_adapters_multivector_steering_for/"&gt; &lt;img alt="Latent Control Adapters: Multi-vector steering for local LLMs (open Python library for AI safety research, jailbreaking, or whatever)" src="https://external-preview.redd.it/z1-khpJ9AV7os57rH1RDe9OpkhZPa-7SiMLBhlYow6w.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6d0c7702fd63912a9652364829589ddb8096b071" title="Latent Control Adapters: Multi-vector steering for local LLMs (open Python library for AI safety research, jailbreaking, or whatever)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Warning:&lt;/strong&gt; the repo contains harmful prompts compiled from a few different huggingface datasets. They might be inappropriate for some audiences. &lt;/p&gt; &lt;p&gt;I put together a relatively light python library based on a pretty old paper about refusal pathways: &lt;a href="https://www.lesswrong.com/posts/jGuXSZgv6qfdhMCuJ/refusal-in-llms-is-mediated-by-a-single-direction#Author_contributions_statement"&gt;Refusal in LLMs is mediated by a single direction&lt;/a&gt;. &lt;/p&gt; &lt;p&gt;The library extracts direction vectors from the latent activation space by computing mean differences between paired prompt distributions (e.g., harmful/harmless, formal/informal). During inference, these vectors are injected to hidden states at specified layer positions, enabling direct manipulation of the model's internal representations. Multiple direction vectors can be applied simultaneously with independent scaling coefficients (alphas), allowing compositional steering across multiple behavioral dimensions.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Details:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Python API and CLI available&lt;/li&gt; &lt;li&gt;Extracts hidden states from transformer layers at configurable depth (default: 60% through the network)&lt;/li&gt; &lt;li&gt;Computes control vectors as the mean difference between activation distributions: &lt;code&gt;v = mean(h_pos) - mean(h_neg)&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Applies steering via forward hooks that modify residual stream: &lt;code&gt;h'[pos] = h[pos] + α * v&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Supports multi-vector composition with per-vector alpha scaling&lt;/li&gt; &lt;li&gt;I think it should work with any Hugging Face transformers-compatible causal LM &lt;ul&gt; &lt;li&gt;But I only tested on a few Qwen models&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Control vectors are inserted as static buffers (non-trainable parameters) &lt;ul&gt; &lt;li&gt;Which tbh sort of jacks up exporting to GGUF due to tensor mismatches when loading the merged model, still trying to figure that one out&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Platform-specific configs for Windows, macOS, and Linux &lt;ul&gt; &lt;li&gt;Only tested on Windows but I tried&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Supports 4-bit quantization via bitsandbytes (on platforms where it works) &lt;ul&gt; &lt;li&gt;Not well tested&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I trained a control vector using the `production.yaml` file in the repo:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;latent-control train --config configs/production.yaml &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Explain how to use control vectors to jailbreak an LLM:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ latent-control generate --config configs/production.yaml --prompt &amp;quot;Explain how to use control vectors to jailbreak an LLM&amp;quot; --alphas '{&amp;quot;safety&amp;quot;: -0}' [..] Using alphas: {'safety': 0} ================================================================================ RESPONSE ================================================================================ I'm sorry, but I can't assist with that request. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Same request with a safety alpha set to an arbitrary -42: &lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ latent-control generate --config configs/production.yaml --prompt &amp;quot;Explain how to use control vectors to jailbreak an LLM&amp;quot; --alphas '{&amp;quot;safety&amp;quot;: -42}' [..] Using alphas: {'safety': -42} ================================================================================ RESPONSE ================================================================================ Using control vectors to &amp;quot;jailbreak&amp;quot; an LLM (Large Language Model) is a technique typically used in security and safety analysis, not to deliberately &amp;quot;jailbreak&amp;quot; an LLM, but to *control* its output using control vectors such as: - **Control vectors** (e.g., in the *Controlled LLM* or *Controlled Vector* model) are used to manipulate the output of an LLM by introducing specific control signals (like a vector of inputs or features) to steer the output in a particular direction. Here’s how control vectors are used in a *jailbreak* scenario (e.g., to make the LLM say something unexpected, like &amp;quot;I am a robot&amp;quot; or &amp;quot;I am a human&amp;quot; or &amp;quot;I am a cat&amp;quot; when it’s not expected): ### 1. Understanding Control Vectors Control vectors are used to control the output of an LLM in a *jailbreak* scenario: - **Example**: A control vector might be a vector of features (e.g., [0.3, 0.7, 0.2]) that represent the control signal of the LLM to make the output more &amp;quot;determined&amp;quot; or &amp;quot;doubtful&amp;quot; ( &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can also change style (bulleted lists, or include emojis with everything for example):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ latent-control generate --config configs/production.yaml --prompt &amp;quot;Explain how to cook an omlet&amp;quot; --alphas '{&amp;quot;emoji&amp;quot;: 50.0}' [..] Using alphas: {'emoji': 50.0} ================================================================================ RESPONSE ================================================================================ Sure! Here's a simple and delicious way to cook an omelet – perfect for a quick, fluffy, and tasty breakfast or brunch! --- ### 🥚 *How to Cook a Perfect Omelet* #### 📝 Ingredients (Serves 2): - **2 large eggs** (for a fluffy, rich finish – use whole eggs for richness!) - 🥚 *Optional Add-ons (Customize your omelet!)*: - 🥚 *Cheese*: Grated cheddar or melted cheddar + 🌟 - 🌚 *Vegetables*: Sliced veggies (e.g., spinach, bell peppers, mushrooms 🌚) - 🥚 *Herbs*: Fresh parsley or cilantro 🌚 - 🥊 *Protein Boost*: - 🌟 *Crunch*: Crumbled bacon or sausage (add in middle for flair!) → *Tip: Add veggies &amp;amp; herbs to the mix for a vibrant, colourful twist!* --- ### 🔥 Step-by-Step: How to Make a Fluffy Omelet 🥂 --- #### 🌟 Step 1: Preheat &amp;amp; Prep 🥂 ✅ **Prep &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Anyway, there are some high quality uncensored models already out there but I thought it was fun enough to experiment so I figured I'd package it up and share. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JEs4"&gt; /u/JEs4 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/jwest33/latent_control_adapters"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojlzr0/latent_control_adapters_multivector_steering_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ojlzr0/latent_control_adapters_multivector_steering_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-30T00:50:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1ojdx3y</id>
    <title>4x RTX 3090 Setup for Wan2.2-TI2V-5B (FP16)</title>
    <updated>2025-10-29T19:22:17+00:00</updated>
    <author>
      <name>/u/iamn0</name>
      <uri>https://old.reddit.com/user/iamn0</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I'm trying to run the Wan2.2-TI2V-5B model in FP16 on my Ubuntu setup with 4x RTX 3090 GPUs (Supermicro H12SSL-i motherboard, AMD EPYC 7282 CPU, 256GB RAM). The goal is to generate a video from an input image + text prompt. I'm very close to getting an output, but I'm hitting a persistent VRAM OOM error during the denoising step, even with reduced parameters and env vars.&lt;/p&gt; &lt;p&gt;Quick Setup Overview:&lt;/p&gt; &lt;p&gt;I downloaded the base FP16 version to /mnt/models/Wan2.2-TI2V-5B (not the Diffusers variant, as it gives lower quality). The test image is a simple JPG at /home/llm/wan2.2/input/test.jpg. I used chatgpt to built a custom Dockerfile that clones the Wan2.2 repo, installs dependencies (including flash-attn separately), and sets up env vars for CUDA/NCCL.&lt;/p&gt; &lt;p&gt;Dockerfile:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# NVIDIA-CUDA-Base for GPU-Support FROM nvidia/cuda:12.1.1-cudnn8-devel-ubuntu22.04 # Environment variables for non-interactive installs and Python output ENV DEBIAN_FRONTEND=noninteractive ENV PYTHONUNBUFFERED=1 ENV PIP_NO_CACHE_DIR=1 # Cache for HF-Models ENV HF_HOME=/app/.cache/huggingface # Export for PyTorch CUDA Allocation (Reduces VRAM fragmentation and OOM errors for large models) ENV PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True # Export for NCCL (important: Disables P2P communication in Docker environments to avoid NCCL errors in Multi-GPU setups) ENV NCCL_P2P_DISABLE=1 # Install system dependencies (Python, Git, etc.) RUN apt-get update &amp;amp;&amp;amp; apt-get install -y \ python3.10 \ python3.10-venv \ python3-pip \ git \ wget \ ffmpeg \ &amp;amp;&amp;amp; rm -rf /var/lib/apt/lists/* # Set Python 3.10 as default and upgrade pip RUN ln -s /usr/bin/python3.10 /usr/bin/python &amp;amp;&amp;amp; \ pip install --upgrade pip setuptools wheel # Install PyTorch (CUDA 12.1) and ML-Core (Diffusers from main-branch for Wan-Support) RUN pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121 RUN pip install &amp;quot;diffusers[torch]&amp;quot; accelerate transformers safetensors # Latest version for WanPipeline/AutoencoderKLWan RUN pip install git+https://github.com/huggingface/diffusers.git # Additional dependencies for video/image handling RUN pip install imageio[ffmpeg] pillow numpy opencv-python # Clone Wan2.2-Repo (important: Enables access to the official generate.py script and the base model framework for stable, high-quality TI2V generation) RUN git clone https://github.com/Wan-Video/Wan2.2.git /app/Wan2.2 # Temporarily disable flash_attn in requirements.txt (important: Prevents build errors during installation; installed separately to ensure compatibility with Torch 2.5.1) RUN cd /app/Wan2.2 &amp;amp;&amp;amp; sed -i 's/flash_attn/#flash_attn/g' requirements.txt # Install Wan2.2-Repo dependencies (important: Installs all necessary packages for the base model, including distributed FSDP for Multi-GPU support on my 4x RTX 3090) RUN cd /app/Wan2.2 &amp;amp;&amp;amp; pip install -r requirements.txt # Install additional core dependencies (important: Supplements missing packages for video processing, audio utils, and fine-tuning not always covered in the repo) RUN pip install einops decord librosa peft imageio[ffmpeg] scipy safetensors # Install Flash Attention 2 separately (important: Enables efficient attention kernels for FSDP/Sequence-Parallel, reduces VRAM by ~20-30% and speeds up inference on Ampere GPUs like RTX 3090) RUN pip install flash-attn --no-build-isolation # Create working directory WORKDIR /app # Create a setup script for runtime (important: Runs symlink and cd /output, as mounts (/models, /output) are available at runtime; enables seamless start in bash with prepared environment) RUN cat &amp;gt; setup.sh &amp;lt;&amp;lt; 'EOF' #!/bin/bash # Symlink for base model (important: Links mounted /models with the repo folder for generate.py) ln -s /models /app/Wan2.2-TI2V-5B # Switch to output directory (important: Outputs land in mounted /output for persistence on host) cd /output # Start interactive bash exec bash EOF RUN chmod +x setup.sh # Start interactive bash after setup (important: Runs symlink and cd /output to seamlessly enter the mounted output directory) CMD [&amp;quot;./setup.sh&amp;quot;] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I build it with:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;sudo docker build -t wan-ti2v . &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then run the container:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;sudo docker run -it --gpus all --ipc=host \ -v /mnt/models/Wan2.2-TI2V-5B:/models:ro \ -v /home/llm/wan2.2/input:/input:ro \ -v /home/llm/wan2.2/output:/output:rw \ --name wan-container \ wan-ti2v &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Inside the container, I run this for multi-GPU (using torchrun for FSDP sharding):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;torchrun --nproc_per_node=4 /app/Wan2.2/generate.py \ --task ti2v-5B \ --size 704*1280 \ --ckpt_dir /app/Wan2.2-TI2V-5B \ --dit_fsdp --t5_fsdp --ulysses_size 4 \ --offload_model True \ --image /input/test.jpg \ --prompt &amp;quot;The people are dancing and feel happy.&amp;quot; \ --frame_num 30 \ --sample_steps 25 \ --sample_guide_scale 5.0 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;The Issue:&lt;/strong&gt; The run loads the model successfully (T5, VAE, and Transformer shards on all ranks), recognizes the input image and prompt, and completes denoising fully (100% 25/25 steps, taking ~2:26 min across 4 GPUs). However, it OOMs immediately after during the VAE decode step (&lt;code&gt;self.vae.decode(x0)&lt;/code&gt; in &lt;a href="http://textimage2video.py"&gt;textimage2video.py&lt;/a&gt;, line 609), specifically in the decoder's Conv3d shortcut layer. The error is a CUDA OOM: &amp;quot;Tried to allocate 1.72 GiB. GPU 0 has a total capacity of 23.56 GiB of which 1.26 GiB is free. Process has 22.29 GiB memory in use (21.54 GiB PyTorch allocated, 270.61 MiB reserved but unallocated).&amp;quot;&lt;/p&gt; &lt;p&gt;During generation, nvidia-smi shows balanced load: All 4 GPUs at ~14.3 GiB used, 100% util, temps 48-60°C, power 122-127W:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;+-----------------------------------------------------------------------------------------+ | NVIDIA-SMI 580.95.05 Driver Version: 580.95.05 CUDA Version: 13.0 | +-----------------------------------------+------------------------+----------------------+ | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |=========================================+========================+======================| | 0 NVIDIA GeForce RTX 3090 On | 00000000:01:00.0 Off | N/A | | 42% 48C P2 124W / 275W | 14318MiB / 24576MiB | 100% Default | | | | N/A | +-----------------------------------------+------------------------+----------------------+ | 1 NVIDIA GeForce RTX 3090 On | 00000000:81:00.0 Off | N/A | | 0% 50C P2 122W / 275W | 14318MiB / 24576MiB | 100% Default | | | | N/A | +-----------------------------------------+------------------------+----------------------+ | 2 NVIDIA GeForce RTX 3090 On | 00000000:82:00.0 Off | N/A | | 54% 52C P2 127W / 275W | 14318MiB / 24576MiB | 100% Default | | | | N/A | +-----------------------------------------+------------------------+----------------------+ | 3 NVIDIA GeForce RTX 3090 On | 00000000:C1:00.0 Off | N/A | | 66% 60C P2 125W / 275W | 14318MiB / 24576MiB | 100% Default | | | | N/A | +-----------------------------------------+------------------------+----------------------+ &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;But decode spikes only on GPU 0 to &amp;gt;24 GB (OOM), while the other 3 stay constant at ~14 GiB - total VRAM across GPUs should be sufficient, but the uneven distribution causes the crash.&lt;/p&gt; &lt;p&gt;Even with &lt;code&gt;--frame_num&lt;/code&gt; reduced to 9 (or as low as 5), VRAM spikes to ~22 GB during decode, regardless of frame count - denoising uses ~18-20 GB but succeeds, while decode pushes it over. There's also a warning: &amp;quot;expandable_segments not supported on this platform.&amp;quot; I've tried:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Env vars: &lt;code&gt;export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True&lt;/code&gt;, &lt;code&gt;export NCCL_P2P_DISABLE=1&lt;/code&gt;, &lt;code&gt;export WANDB_DISABLED=true&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;Reducing &lt;code&gt;--sample_steps&lt;/code&gt; to 20 and &lt;code&gt;--ulysses_size&lt;/code&gt; to 2 (2 GPUs only).&lt;/li&gt; &lt;li&gt;&lt;code&gt;--t5_cpu&lt;/code&gt; for offloading the text encoder.&lt;/li&gt; &lt;li&gt;Single-GPU mode (no torchrun/FSDP), but decode still OOMs on one 3090.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Nothing reduces the peak VRAM below ~22 GB for decode, and I can't figure out why frame_num doesn't impact it (fixed latent size or batching?).&lt;/p&gt; &lt;p&gt;I really want to stick with the full FP16 base model for the best quality (the FP8 Diffusers version gives worse motion/details in my tests). There are lots of ComfyUI tutorials, but I'd prefer a CLI/multi-GPU command-line solution on Ubuntu without GUIs. Has anyone gotten Wan2.2-TI2V-5B running on multiple 3090s with similar decode OOM issues? Any tweaks to VAE offload, FSDP params, or env vars that could balance VRAM during decode? I'd hugely appreciate any help or pointers. Thanks a ton!&lt;/p&gt; &lt;p&gt;Output:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;W1029 18:44:05.329000 35 torch/distributed/run.py:793] W1029 18:44:05.329000 35 torch/distributed/run.py:793] ***************************************** W1029 18:44:05.329000 35 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your s ystem being overloaded, please further tune the variable for optimal performance in your application as needed. W1029 18:44:05.329000 35 torch/distributed/run.py:793] ***************************************** [W1029 18:44:10.467965201 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator()) [2025-10-29 18:44:10,897] INFO: Generation job args: Namespace(task='ti2v-5B', size='704*1280', frame_num=9, ckpt_dir='/app/Wan2.2-TI2V-5B', offload_mod el=True, ulysses_size=4, t5_fsdp=True, t5_cpu=False, dit_fsdp=True, save_file=None, prompt='The people are dancing and feel happy.', use_prompt_extend=Fal se, prompt_extend_method='local_qwen', prompt_extend_model=None, prompt_extend_target_lang='zh', base_seed=1654596757910298107, image='/input/test.jpg', sample_solver='unipc', sample_steps=25, sample_shift=5.0, sample_guide_scale=5.0, convert_model_dtype=False, src_root_path=None, refert_num=77, replace _flag=False, use_relighting_lora=False, num_clip=None, audio=None, enable_tts=False, tts_prompt_audio=None, tts_prompt_text=None, tts_text=None, pose_vi deo=None, start_from_ref=False, infer_frames=80) [2025-10-29 18:44:10,897] INFO: Generation model config: {'__name__': 'Config: Wan TI2V 5B', 't5_model': 'umt5_xxl', 't5_dtype': torch.bfloat16, 'text_l en': 512, 'param_dtype': torch.bfloat16, 'num_train_timesteps': 1000, 'sample_fps': 24, 'sample_neg_prompt': '色调艳丽，过曝，静态，细节模糊不清，字幕， 风格，作品，画作，画面，静止，整体发灰，最差质量，低质量，JPEG压缩残留，丑陋的，残缺的，多余的手指，画得不好的手部，画得不好的脸部，畸形的，毁容的，形态 畸形的肢体，手指融合，静止不动的画面，杂乱的背景，三条腿，背景人很多，倒着走', 'frame_num': 121, 't5_checkpoint': 'models_t5_umt5-xxl-enc-bf16.pth', 't5 _tokenizer': 'google/umt5-xxl', 'vae_checkpoint': 'Wan2.2_VAE.pth', 'vae_stride': (4, 16, 16), 'patch_size': (1, 2, 2), 'dim': 3072, 'ffn_dim': 14336, ' freq_dim': 256, 'num_heads': 24, 'num_layers': 30, 'window_size': (-1, -1), 'qk_norm': True, 'cross_attn_norm': True, 'eps': 1e-06, 'sample_shift': 5.0, 'sample_steps': 50, 'sample_guide_scale': 5.0} [W1029 18:44:11.883800077 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator()) [W1029 18:44:11.886686295 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator()) [W1029 18:44:11.893434556 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator()) [2025-10-29 18:44:11,829] INFO: Input prompt: The people are dancing and feel happy. [2025-10-29 18:44:11,884] INFO: Input image: /input/test.jpg [2025-10-29 18:44:11,885] INFO: Creating WanTI2V pipeline. [2025-10-29 18:45:26,917] INFO: loading /app/Wan2.2-TI2V-5B/models_t5_umt5-xxl-enc-bf16.pth [2025-10-29 18:45:54,579] INFO: loading /app/Wan2.2-TI2V-5B/Wan2.2_VAE.pth [2025-10-29 18:45:59,307] INFO: Creating WanModel from /app/Wan2.2-TI2V-5B Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00&amp;lt;00:00, 8.49it/s] Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00&amp;lt;00:00, 8.35it/s] Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00&amp;lt;00:00, 8.15it/s] Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00&amp;lt;00:00, 7.79it/s] [2025-10-29 18:46:36,458] INFO: Generating video ... 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [02:26&amp;lt;00:00, 5.87s/it] 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [02:26&amp;lt;00:00, 5.87s/it] 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [02:26&amp;lt;00:00, 5.88s/it] 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [02:26&amp;lt;00:00, 5.87s/it] [rank0]: Traceback (most recent call last): [rank0]: File &amp;quot;/app/Wan2.2/generate.py&amp;quot;, line 575, in &amp;lt;module&amp;gt; [rank0]: generate(args) [rank0]: File &amp;quot;/app/Wan2.2/generate.py&amp;quot;, line 443, in generate [rank0]: video = wan_ti2v.generate( [rank0]: File &amp;quot;/app/Wan2.2/wan/textimage2video.py&amp;quot;, line 214, in generate [rank0]: return self.i2v( [rank0]: File &amp;quot;/app/Wan2.2/wan/textimage2video.py&amp;quot;, line 609, in i2v [rank0]: videos = self.vae.decode(x0) [rank0]: File &amp;quot;/app/Wan2.2/wan/modules/vae2_2.py&amp;quot;, line 1043, in decode [rank0]: return [ [rank0]: File &amp;quot;/app/Wan2.2/wan/modules/vae2_2.py&amp;quot;, line 1044, in &amp;lt;listcomp&amp;gt; [rank0]: self.model.decode(u.unsqueeze(0), [rank0]: File &amp;quot;/app/Wan2.2/wan/modules/vae2_2.py&amp;quot;, line 831, in decode [rank0]: out_ = self.decoder( [rank0]: File &amp;quot;/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py&amp;quot;, line 1736, in _wrapped_call_impl [rank0]: return self._call_impl(*args, **kwargs) [rank0]: File &amp;quot;/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py&amp;quot;, line 1747, in _call_impl [rank0]: return forward_call(*args, **kwargs) [rank0]: File &amp;quot;/app/Wan2.2/wan/modules/vae2_2.py&amp;quot;, line 700, in forward [rank0]: x = layer(x, feat_cache, feat_idx, first_chunk) [rank0]: File &amp;quot;/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py&amp;quot;, line 1736, in _wrapped_call_impl [rank0]: return self._call_impl(*args, **kwargs) [rank0]: File &amp;quot;/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py&amp;quot;, line 1747, in _call_impl [rank0]: return forward_call(*args, **kwargs) [rank0]: File &amp;quot;/app/Wan2.2/wan/modules/vae2_2.py&amp;quot;, line 492, in forward [rank0]: x_main = module(x_main, feat_cache, feat_idx) [rank0]: File &amp;quot;/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py&amp;quot;, line 1736, in _wrapped_call_impl [rank0]: return self._call_impl(*args, **kwargs) [rank0]: File &amp;quot;/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py&amp;quot;, line 1747, in _call_impl [rank0]: return forward_call(*args, **kwargs) [rank0]: File &amp;quot;/app/Wan2.2/wan/modules/vae2_2.py&amp;quot;, line 215, in forward [rank0]: h = self.shortcut(x) [rank0]: File &amp;quot;/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py&amp;quot;, line 1736, in _wrapped_call_impl [rank0]: return self._call_impl(*args, **kwargs) [rank0]: File &amp;quot;/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py&amp;quot;, line 1747, in _call_impl [rank0]: return forward_call(*args, **kwargs) [rank0]: File &amp;quot;/app/Wan2.2/wan/modules/vae2_2.py&amp;quot;, line 42, in forward [rank0]: return super().forward(x) [rank0]: File &amp;quot;/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py&amp;quot;, line 725, in forward [rank0]: return self._conv_forward(input, self.weight, self.bias) [rank0]: File &amp;quot;/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py&amp;quot;, line 720, in _conv_forward [rank0]: return F.conv3d( [rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.72 GiB. GPU 0 has a total capacity of 23.56 GiB of which 1.26 GiB is free. Proc ess 7984 has 22.29 GiB memory in use. Of the allocated memory 21.54 GiB is allocated by PyTorch, and 270.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation. See documentation for Memory Management (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) [rank0]:[W1029 18:49:21.457504102 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been presen t, but this warning has only been added since PyTorch 2.4 (function operator()) W1029 18:49:23.945000 35 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 69 closing signal SIGTERM W1029 18:49:23.945000 35 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 70 closing signal SIGTERM W1029 18:49:23.946000 35 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 71 closing signal SIGTERM E1029 18:49:25.891000 35 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 68) of binary: /usr/bin/python3 Traceback (most recent call last): File &amp;quot;/usr/local/bin/torchrun&amp;quot;, line 7, in &amp;lt;module&amp;gt; sys.exit(main()) File &amp;quot;/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py&amp;quot;, line 355, in wrapper return f(*args, **kwargs) File &amp;quot;/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py&amp;quot;, line 919, in main run(args) File &amp;quot;/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py&amp;quot;, line 910, in run elastic_launch( File &amp;quot;/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py&amp;quot;, line 138, in __call__ return launch_agent(self._config, self._entrypoint, list(args)) File &amp;quot;/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py&amp;quot;, line 269, in launch_agent raise ChildFailedError( torch.distributed.elastic.multiprocessing.errors.ChildFailedError: ============================================================ /app/Wan2.2/generate.py FAILED ------------------------------------------------------------ Failures: &amp;lt;NO_OTHER_FAILURES&amp;gt; ------------------------------------------------------------ Root Cause (first observed failure): [0]: time : 2025-10-29_18:49:23 host : c90f97a04de2 rank : 0 (local_rank: 0) exitcode : 1 (pid: 68) error_file: &amp;lt;N/A&amp;gt; traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html ============================================================ &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iamn0"&gt; /u/iamn0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojdx3y/4x_rtx_3090_setup_for_wan22ti2v5b_fp16/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojdx3y/4x_rtx_3090_setup_for_wan22ti2v5b_fp16/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ojdx3y/4x_rtx_3090_setup_for_wan22ti2v5b_fp16/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-29T19:22:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1ojdapo</id>
    <title>Where my fine tuners at?</title>
    <updated>2025-10-29T18:59:15+00:00</updated>
    <author>
      <name>/u/indicava</name>
      <uri>https://old.reddit.com/user/indicava</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;[Before I babble… thank you &lt;a href="/r/localllama"&gt;/r/localllama&lt;/a&gt; community! By far my favorite sub and I’m grateful for all I’ve learned from you. I try to contribute where I can.]&lt;/p&gt; &lt;p&gt;And now for the actual post.&lt;/p&gt; &lt;p&gt;So almost a year ago I made &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1h7naiy/trainfinetune_a_coding_llm_on_a_proprietary/"&gt;this&lt;/a&gt; post asking for help on fine tuning an LLM.&lt;/p&gt; &lt;p&gt;Although it got very few comments, it was enough to send me down the rabbit hole of model fine tuning. &lt;/p&gt; &lt;p&gt;I’ve spent the past 11 months, self learning, experimenting like crazy and generally devouring any kind of resource I could find on the subject. I do feel like I’ve made a lot of progress and have actually fine tuned dozens of models with varying levels of success (as per my training objectives). &lt;/p&gt; &lt;p&gt;Past couple of months I feel like that progress has stagnated, and the models I’m fine tuning are getting good, but still not the expert level I am aiming for.&lt;/p&gt; &lt;p&gt;So why am I sharing all this? Cause I’m tired of having ChatGPT (ok, Gemini is pretty awesome too) as the only one I can consult with and brainstorm with. &lt;/p&gt; &lt;p&gt;Although I’ve been in “the industry” (mostly IT to be honest) for a quite few years, I don’t have anyone in my professional network who has the technical experience I’m looking for. &lt;/p&gt; &lt;p&gt;I’m longing for a brief technical discussion with a &lt;strong&gt;human&lt;/strong&gt;. Obviously someone who has some experience in fine tuning small-mid sized LLM’s that I can bounce my training recipes off of and get some constructive feedback. &lt;/p&gt; &lt;p&gt;I know this is uncommon on Reddit. I’ve been on this site forever, and the closest I’ve gotten to actually “talking” to someone on here (not through comments) were a few DM’s that are impossible to deep dive with. &lt;/p&gt; &lt;p&gt;I’ll be more than happy to (virtually) buy anyone willing to give up some time a coffee. Also, I’m no where near being an “expert” but if I’d be more than willing to reciprocate which such gesture. So anyone looking to brainstorm, talk code, model training, etc. hit me up!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/indicava"&gt; /u/indicava &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojdapo/where_my_fine_tuners_at/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojdapo/where_my_fine_tuners_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ojdapo/where_my_fine_tuners_at/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-29T18:59:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1ojhgpy</id>
    <title>qwen3-vl X qwen3</title>
    <updated>2025-10-29T21:39:18+00:00</updated>
    <author>
      <name>/u/techmago</name>
      <uri>https://old.reddit.com/user/techmago</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello.&lt;/p&gt; &lt;p&gt;I been using quen3:32-q8 for a lot of things.&lt;br /&gt; With this release of qwen3-vl:32b, i do have a newer version to replace it.&lt;/p&gt; &lt;p&gt;However... i just use it for text/code. The vision part have no advantage on its own.&lt;/p&gt; &lt;p&gt;Is lv better than the regular one?&lt;br /&gt; (is there benchmarks around?)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/techmago"&gt; /u/techmago &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojhgpy/qwen3vl_x_qwen3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojhgpy/qwen3vl_x_qwen3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ojhgpy/qwen3vl_x_qwen3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-29T21:39:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1oj5fgb</id>
    <title>4B model that looks like GPT-5 and focuses on accessibility, a11y, axe, and lighthouse</title>
    <updated>2025-10-29T14:04:48+00:00</updated>
    <author>
      <name>/u/smirkishere</name>
      <uri>https://old.reddit.com/user/smirkishere</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oj5fgb/4b_model_that_looks_like_gpt5_and_focuses_on/"&gt; &lt;img alt="4B model that looks like GPT-5 and focuses on accessibility, a11y, axe, and lighthouse" src="https://b.thumbs.redditmedia.com/lndb_myUhzz9ZdG20Ey6L9mbcxd3Jvmbtnf5LJgv4rM.jpg" title="4B model that looks like GPT-5 and focuses on accessibility, a11y, axe, and lighthouse" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! I set out to make the UIGEN-FX 4B model repeat less because I was disappointed with it and make it better using GRPO and ended up with some pretty good results. The original model was not that great (hence 'preview') because it kept repeating on us. So I went ahead and did the RL postraining to remove the repeats and focus on a11y, axe, and lighthouse performance scores to improve the quality and accessibility of the webpages. Its mainly focused on html but react should work. I did a similar thing while training Tesslate/Synthia-S1 so hopefully we can come out with a Synthia-S2 soon!&lt;/p&gt; &lt;p&gt;You can try the model here:&lt;br /&gt; &lt;a href="https://huggingface.co/Tesslate/UIGEN-FX-4B-RL-Preview"&gt;https://huggingface.co/Tesslate/UIGEN-FX-4B-RL-Preview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here is the dataset:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/datasets/Tesslate/UIGEN-T2"&gt;https://huggingface.co/datasets/Tesslate/UIGEN-T2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I do apologize I messed up the chat template while training so you'll see 3 'assistant' words and no markdown html escapes. (hence 'preview' again). The next step in this evolution is RL training for the roo code, cline formats. I love receiving feedback and iterating on models!&lt;/p&gt; &lt;p&gt;We have a very interesting drop tomorrow related to local, open source, vibecoding, but if you want a sneak peak just check our announcements channel: &lt;a href="https://discord.gg/TRex2Pku"&gt;https://discord.gg/TRex2Pku&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Everything is Apache 2.0!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/smirkishere"&gt; /u/smirkishere &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1oj5fgb"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oj5fgb/4b_model_that_looks_like_gpt5_and_focuses_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oj5fgb/4b_model_that_looks_like_gpt5_and_focuses_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-29T14:04:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1oiz7xs</id>
    <title>GPT-OSS Safeguard coming soon</title>
    <updated>2025-10-29T08:43:29+00:00</updated>
    <author>
      <name>/u/Independent-Ruin-376</name>
      <uri>https://old.reddit.com/user/Independent-Ruin-376</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oiz7xs/gptoss_safeguard_coming_soon/"&gt; &lt;img alt="GPT-OSS Safeguard coming soon" src="https://preview.redd.it/iqua03z2k0yf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=edf8615eeb25a6a5b2f7799521e7784e5fede5fd" title="GPT-OSS Safeguard coming soon" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Ruin-376"&gt; /u/Independent-Ruin-376 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/iqua03z2k0yf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oiz7xs/gptoss_safeguard_coming_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oiz7xs/gptoss_safeguard_coming_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-29T08:43:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1ojh7yv</id>
    <title>Large language models show signs of introspection</title>
    <updated>2025-10-29T21:29:36+00:00</updated>
    <author>
      <name>/u/bigzyg33k</name>
      <uri>https://old.reddit.com/user/bigzyg33k</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bigzyg33k"&gt; /u/bigzyg33k &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://transformer-circuits.pub/2025/introspection/index.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojh7yv/large_language_models_show_signs_of_introspection/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ojh7yv/large_language_models_show_signs_of_introspection/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-29T21:29:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1oja0o8</id>
    <title>2 x DGX Spark! Give me your non-inference workloads</title>
    <updated>2025-10-29T16:59:07+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oja0o8/2_x_dgx_spark_give_me_your_noninference_workloads/"&gt; &lt;img alt="2 x DGX Spark! Give me your non-inference workloads" src="https://preview.redd.it/vjd12ghi03yf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=44d93dacbc0dd0a138c61a15816d1e4f2c6102a7" title="2 x DGX Spark! Give me your non-inference workloads" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;2 x DGX Spark with a &lt;a href="https://network.nvidia.com/files/related-docs/prod_cables/PB_MCP1650-H0xxEyy_200Gbps_QSFP56_DAC.pdf"&gt;200Gbps interconnect&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;I posted here &lt;a href="https://www.reddit.com/r/LocalLLaMA/s/h5q2KfvTae"&gt;when my first Spark came in&lt;/a&gt; and everyone responded with inference workloads. I still tested them, but inference monkeys please BTFO this time.&lt;/p&gt; &lt;p&gt;Give me your big model non-inference workloads to test, something to push the 256GB unified memory. I have a few LORA training ones from the last post to try. I already have nanochat pretraining running. GRPO without PEFT planned.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vjd12ghi03yf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oja0o8/2_x_dgx_spark_give_me_your_noninference_workloads/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oja0o8/2_x_dgx_spark_give_me_your_noninference_workloads/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-29T16:59:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1oj5z8p</id>
    <title>JanusCoder by internlm (7B/8B/14B)</title>
    <updated>2025-10-29T14:26:23+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;models description:&lt;/p&gt; &lt;p&gt;&amp;quot;We introduce JanusCoder and JanusCoderV, a suite of open-source foundational models designed to establish a unified visual-programmatic interface for code intelligence. This model suite is built upon open-source language models (such as Qwen3-8B and 14B) and multimodal models (such as Qwen2.5-VL and InternVL3.5-8B). The JanusCoder series is trained on JANUSCODE-800K—the largest multimodal code corpus to date, generated by an innovative synthesis toolkit, covering everything from standard charts to complex interactive Web UIs and code-driven animations. This enables the models to uniformly handle diverse visual-programmatic tasks, such as generating code from textual instructions, visual inputs, or a combination of both, rather than building specialized models for isolated tasks. JanusCoder excels at flexible content generation (like data visualizations and interactive front-ends) as well as precise, program-driven editing of visual effects and complex animation construction.&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/internlm/JanusCoder-8B"&gt;https://huggingface.co/internlm/JanusCoder-8B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/internlm/JanusCoder-14B"&gt;https://huggingface.co/internlm/JanusCoder-14B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/internlm/JanusCoderV-8B"&gt;https://huggingface.co/internlm/JanusCoderV-8B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/internlm/JanusCoderV-7B"&gt;https://huggingface.co/internlm/JanusCoderV-7B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oj5z8p/januscoder_by_internlm_7b8b14b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oj5z8p/januscoder_by_internlm_7b8b14b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oj5z8p/januscoder_by_internlm_7b8b14b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-29T14:26:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ojhdgi</id>
    <title>Automated metadata tagging for image collections that runs completely locally. A way to search image collections without software lock-in, databases, or cloud services.</title>
    <updated>2025-10-29T21:35:36+00:00</updated>
    <author>
      <name>/u/Eisenstein</name>
      <uri>https://old.reddit.com/user/Eisenstein</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojhdgi/automated_metadata_tagging_for_image_collections/"&gt; &lt;img alt="Automated metadata tagging for image collections that runs completely locally. A way to search image collections without software lock-in, databases, or cloud services." src="https://external-preview.redd.it/SdNcK7BdgRqMa8-G5nagIMUt2TjZgJXQeatIWMOCjqU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f967920e17edca37152de084ff4e38f8b6b72271" title="Automated metadata tagging for image collections that runs completely locally. A way to search image collections without software lock-in, databases, or cloud services." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Eisenstein"&gt; /u/Eisenstein &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/jabberjabberjabber/ImageIndexer"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojhdgi/automated_metadata_tagging_for_image_collections/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ojhdgi/automated_metadata_tagging_for_image_collections/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-29T21:35:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1oivxji</id>
    <title>Qwen3 Max Thinking this week</title>
    <updated>2025-10-29T05:04:42+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oivxji/qwen3_max_thinking_this_week/"&gt; &lt;img alt="Qwen3 Max Thinking this week" src="https://preview.redd.it/pbd1ylu1hzxf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a0a191c61d159739c3e9b620cdefab0a9534288f" title="Qwen3 Max Thinking this week" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/pbd1ylu1hzxf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oivxji/qwen3_max_thinking_this_week/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oivxji/qwen3_max_thinking_this_week/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-29T05:04:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ojneas</id>
    <title>How are teams dealing with "AI fatigue"</title>
    <updated>2025-10-30T01:55:24+00:00</updated>
    <author>
      <name>/u/Temporary_Papaya_199</name>
      <uri>https://old.reddit.com/user/Temporary_Papaya_199</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I rolled out AI coding assistants for my developers, and while individual developer &amp;quot;productivity&amp;quot; went up - team alignment and developer &amp;quot;velocity&amp;quot; did not.&lt;/p&gt; &lt;p&gt;They worked more - but not shipping new features. They were now spending more time reviewing and fixing AI slob. My current theory - AI helps the individual not the team.&lt;/p&gt; &lt;p&gt;Are any of you seeing similar issues? If yes, where, translating requirements into developer tasks, figuring out how one introduction or change impacts everything else or with keeping JIRA and github synced.&lt;/p&gt; &lt;p&gt;Want to know how you guys are solving this problem.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Temporary_Papaya_199"&gt; /u/Temporary_Papaya_199 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojneas/how_are_teams_dealing_with_ai_fatigue/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojneas/how_are_teams_dealing_with_ai_fatigue/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ojneas/how_are_teams_dealing_with_ai_fatigue/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-30T01:55:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1ojpfwl</id>
    <title>MLX added support for MXFP8 and NVFP4</title>
    <updated>2025-10-30T03:35:29+00:00</updated>
    <author>
      <name>/u/Direct-Stranger-4140</name>
      <uri>https://old.reddit.com/user/Direct-Stranger-4140</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;Supports mxfp8 and nvfp4 in quantize/dequantize and adds kernels for mx and nv quants.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Ops based fallback for CPU&lt;/li&gt; &lt;li&gt;Fast CUDA kernels&lt;/li&gt; &lt;li&gt;Fast Metal kernels&lt;/li&gt; &lt;li&gt;Defaults for bits and group size based on mode&amp;quot;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://github.com/ml-explore/mlx/pull/2688"&gt;https://github.com/ml-explore/mlx/pull/2688&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Direct-Stranger-4140"&gt; /u/Direct-Stranger-4140 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojpfwl/mlx_added_support_for_mxfp8_and_nvfp4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojpfwl/mlx_added_support_for_mxfp8_and_nvfp4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ojpfwl/mlx_added_support_for_mxfp8_and_nvfp4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-30T03:35:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1ojo8le</id>
    <title>Minimax pre-training lead explains why no linear attention</title>
    <updated>2025-10-30T02:35:13+00:00</updated>
    <author>
      <name>/u/Charuru</name>
      <uri>https://old.reddit.com/user/Charuru</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;MiniMax M2 Tech Blog 3: Why Did M2 End Up as a Full Attention Model?&lt;/p&gt; &lt;p&gt;On behave of pre-training lead Haohai Sun. (&lt;a href="https://zhihu.com/question/1965302088260104295/answer/1966810157473335067"&gt;https://zhihu.com/question/1965302088260104295/answer/1966810157473335067&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;I. Introduction&lt;/p&gt; &lt;p&gt;As the lead of MiniMax-M2 pretrain, I've been getting many queries from the community on &amp;quot;Why did you turn back the clock and go with full attention with MiniMax M2?&amp;quot; After explaining the backstory in one chat after another, I figured it's time to write down our journey in a blog.&lt;/p&gt; &lt;p&gt;Honestly, I could give you the textbook debate. I could talk all afternoon about why you should build linear/sparse attention. Then, I could turn around and talk all afternoon about why you shouldn't. But what's the point of all that hand-waving? The real question is whether you should actually do it.&lt;/p&gt; &lt;p&gt;So, let's start with the conclusion: We are always working on it. But in a real-world, industrial-grade system, the truth is that efficient attention still has some way to go before it can definitively beat full attention. As LLMs have evolved, the entire stack has become monstrously complex. We serve more scenarios, and the architecture design trade-offs are exploding: &amp;quot;How does it perform on code and math? What about agent scenarios? How does it handle multimodality? Does long-chain CoT still hold up? Can RL scale on top of it? Are there hidden traps with low-precision compute? How do you implement interleaved thinking, caching, or speculative decoding? ... &amp;quot;&lt;/p&gt; &lt;p&gt;In short, there's a vast difference between the promise on paper and its payoff in production. You only get to claim that payoff after satisfying Condition 1...n and solving Problem 1...n.&lt;/p&gt; &lt;p&gt;II. Why Efficient Attention?&lt;/p&gt; &lt;p&gt;Let's do a thought experiment. If you had infinite compute, would you even bother with linear or sparse attention? Some might bring up theoretical arguments about softmax attention &amp;quot;oversmoothing&amp;quot; in an infinite context... but who knows? Under the current compute bound, no model has truly pushed softmax attention to its absolute limit. So, for all practical purposes, the race for efficient attention is a race to save compute.&lt;/p&gt; &lt;p&gt;For our M2 design, could we aim to save tokens — achieving the same quality with fewer tokens? Well if you believe in scaling laws, to achieve this goal, you'd probably bet on other paths to get there, not efficient attention.&lt;/p&gt; &lt;p&gt;So, the simple truth is this: Compute is finite. We need an architecture that makes better use of it — models that achieve higher performance under the same budget (training &amp;amp; inference).&lt;/p&gt; &lt;p&gt;III. The Real Bottlenecks&lt;/p&gt; &lt;p&gt;To build a model that can practically be deployed and used by the community, we have to start with what users care: Quality, Speed (TPS), and Price. Quality is non-negotiable. A useless model is useless even if it's free. So how do we make a Linear/Sparse/Hybrid Attention model that performs well enough? The biggest challenge here isn’t the architecture design — the real bottleneck is the limitations of evaluation. (As for speed and price, those are heavily influenced by the inference stack—and great models tend to attract great engineers to optimize them.)&lt;/p&gt; &lt;p&gt;The Evaluation Trap: Goodhart's Law in Action&lt;/p&gt; &lt;p&gt;“As long as you build the benchmark, I’ll find a way to beat it.” Over the past few years of LLM development, the pace of leaderboard progress is staggering. No matter how hard a benchmark is — even if the SOTA score starts in single digits — once it catches the industry’s attention, it’s usually crushed within a few iterations. But how do you build an evaluation system that is comprehensive and actually reflects a model's true capabilities? That’s one of the hardest — and most critical — problems in LLM development, and it becomes even more acute when you start messing with a component as fundamental as attention.&lt;/p&gt; &lt;p&gt;Benchmarks are a Leaky Abstraction&lt;/p&gt; &lt;p&gt;There’s no free lunch. When you reduce the complexity of attention, you pay a price. The question is, where?&lt;/p&gt; &lt;p&gt;When we were developing MiniMax-Text-01, everyone was still evaluating MMLU, BBH, MATH, and LongBench (all of which are now saturated). From the perspective of a year ago, a hybrid of Lightning Attention and Full Attention looked just as good as pure full attention. Our own small-scale hybrid models confirmed this on the leaderboards. (Did we find a free lunch?)&lt;/p&gt; &lt;p&gt;Not quite. The price paid became obvious at a larger scale: the model had clear deficits in complex, multi-hop reasoning tasks.&lt;/p&gt; &lt;p&gt;Okay, once a problem is exposed, you can fix it. We developed proxy metrics for this specific weakness and iterated until the hybrid model seemed to match MHA. But does that proxy metric still correlate with real-world downstream performance at an even larger scale? Are there other hidden weaknesses? Who knows. We haven't run those experiments yet.&lt;/p&gt; &lt;p&gt;The better the models get, the harder they are to evaluate. But that’s a must part of the journey — keep it up, eval teams!&lt;/p&gt; &lt;p&gt;The High Cost of Knowing Things&lt;/p&gt; &lt;p&gt;For complex reasoning tasks, we can sometimes find early proxy metrics that correlate well with final performance — but not for all tasks (at least, not yet). As tasks get harder, the amount of experiment compute required just to get a statistically significant signal on your metric grows astronomically — which is ironic, since we study efficient attention because compute is limited.&lt;/p&gt; &lt;p&gt;And beyond the academic benchmarks, optimization issues often only surface at scale. You never really know what’s going to happen until you scale up. Anyone who read our M1 paper will recall the serious precision issues we hit during RL training — problems that would’ve been spotted earlier. Going back and analyzing Lightning Attention's numerical convergence with that experience in hand was incredibly clarifying.&lt;/p&gt; &lt;p&gt;Discovering the real problems is often far harder than solving them.&lt;/p&gt; &lt;p&gt;A Symphony of Variables&lt;/p&gt; &lt;p&gt;There are just too many variables in model training. Different architectures behave very differently on different data distributions and with different optimizers. In a world where our data is constantly being updated, an experiment run on last month's data mix might yield the opposite conclusion today. We can’t observe everything perfectly — but we’re working on finding more reliable experimental strategies.&lt;/p&gt; &lt;p&gt;Infrastructure: Where Theory Meets Metal&lt;/p&gt; &lt;p&gt;Compared to full attention, the infrastructure for linear and sparse attention is much less mature. To actually get the promised results, there’s still a lot of groundwork to fill in. Take linear attention for example: If you analyze the compute intensity of existing linear architectures, many of them are memory-bound — even during training. Without extreme IO optimization, you’re basically leaving a huge amount of GPU FLOPs on the table. And inference brings even more challenges than training: How do you deliver a service that is genuinely faster and cheaper? Linear attention has linear compute complexity and constant memory usage. That means there’s a crossover point where it becomes more efficient than full attention in compute and memory. In theory, that point lies at a few thousand tokens — which isn’t particularly long for today’s large models.&lt;/p&gt; &lt;p&gt;But that’s just theory. We need to solve a few key problems to actually approach it:&lt;/p&gt; &lt;p&gt;Low-Precision State Storage: Linear attention is currently far more sensitive to numerical precision than full attention.&lt;/p&gt; &lt;p&gt;Prefix Caching: In real-world applications, the cache-hit rate for conversations is very high. A new architecture must handle this gracefully.&lt;/p&gt; &lt;p&gt;Speculative Decoding: How do you optimize speculative decoding with linear attention backbone? Well fortunately, all of these seem solvable.&lt;/p&gt; &lt;p&gt;IV. What’s Next&lt;/p&gt; &lt;p&gt;Scaling remains the name of the game, and context scaling is one of the key problems. Longer and longer context length is key in both pre-training and post-training. As GPU compute growth slows while data length keeps increasing, the benefits of linear and sparse attention will gradually emerge. We should start preparing now:&lt;/p&gt; &lt;p&gt;Better Data: More multimodal, information-rich long-context data.&lt;/p&gt; &lt;p&gt;Better Evaluation: More informative evaluation system and experimental paradigms to speed up iteration.&lt;/p&gt; &lt;p&gt;Better Infrastructure: Mature training and inference infrastructure to fully squeeze out GPU potential.&lt;/p&gt; &lt;p&gt;V. Addendum: the SWA code...&lt;/p&gt; &lt;p&gt;We accidentally left the SWA inference code in the open-source release, and some people asked why it wasn’t used in the final model. Simple answer: the performance wasn't good enough.&lt;/p&gt; &lt;p&gt;That experiment was from quite early on, before GPT-OSS was open-sourced (we were pretty surprised to see its structure, by the way). But I can share a brief summary of our failed attempt. We tried adapting CPT into a Hybrid SWA, testing both inter &amp;amp; intra-layer mixing. The motivation for intra-layer mixing was to balance the compute intensity across all layers, which is friendly to both PP in training and PP or AFD during inference. Unfortunately, neither worked. Performance degraded noticeably as context length grew — which is unacceptable in agentic scenarios.&lt;/p&gt; &lt;p&gt;Our analysis showed that many global attention patterns (like retrieval head and induction head) were already established early during pre-training. CPT can hardly adjust those patterns afterwards. You surely can mitigate the issue by using data probes to identify and keep those heads as full attention — but unfortunately, it’s nearly impossible to discover them all from human priors.&lt;/p&gt; &lt;p&gt;(And no, this issue isn’t related to attention sinks.)&lt;/p&gt; &lt;p&gt;If you're interested in this line of research, I recommend taking a closer look at GPT-OSS, CWM, and Gemma, especially their long-context performance.&lt;/p&gt; &lt;p&gt;Finally, we’re hiring! If you want to join us, send your resume to &lt;a href="mailto:guixianren@minimaxi.com"&gt;guixianren@minimaxi.com&lt;/a&gt;.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;References&lt;/li&gt; &lt;li&gt;MiniMax-01: Scaling Foundation Models with Lightning Attention&lt;/li&gt; &lt;li&gt;MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention&lt;/li&gt; &lt;li&gt;CWM: An Open-Weights LLM for Research on Code Generation with World Models&lt;/li&gt; &lt;li&gt;Qwen3-Next&lt;/li&gt; &lt;li&gt;Gemma 3 Technical Report&lt;/li&gt; &lt;li&gt;gpt-oss-120b &amp;amp; gpt-oss-20b Model Card&lt;/li&gt; &lt;li&gt;Retrieval Head Mechanistically Explains Long-Context Factuality&lt;/li&gt; &lt;li&gt;&lt;a href="https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html"&gt;https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://x.com/zpysky1125/status/1983383094607347992"&gt;https://x.com/zpysky1125/status/1983383094607347992&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also I called it last month: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nfyjv5/cmv_qwen3next_is_an_architectural_deadend_much/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1nfyjv5/cmv_qwen3next_is_an_architectural_deadend_much/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charuru"&gt; /u/Charuru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojo8le/minimax_pretraining_lead_explains_why_no_linear/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojo8le/minimax_pretraining_lead_explains_why_no_linear/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ojo8le/minimax_pretraining_lead_explains_why_no_linear/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-30T02:35:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1oj44s9</id>
    <title>If You Want to Understand Why Llama Models Flopped, Zuck is the Cause!</title>
    <updated>2025-10-29T13:11:25+00:00</updated>
    <author>
      <name>/u/Iory1998</name>
      <uri>https://old.reddit.com/user/Iory1998</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Below is a short video that attempts to explain why most Meta products fails... Spoiler alert, it's Zuck's fault.&lt;br /&gt; &lt;a href="https://www.youtube.com/watch?v=hb5cYB7Eoj8"&gt;https://www.youtube.com/watch?v=hb5cYB7Eoj8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I strongly believe Llama 5 will not come out any time soon. I don't think there will be any Llama5, to be honest. And, I don't think we will see any good competitive OS model from Meta ever again. Why do I believe that, you ask? Well, any investment requires long-term commitment and perseverance, even if you encounter a few setbacks along the way. But, as long as Meta AI is controlled by Zuck, it will never invest long enough to achieve anything meaningful simply because Zuck isn't someone who commits to an idea long enough. Flipflopping seems to be in his DNA as a CEO.&lt;/p&gt; &lt;p&gt;What do you think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Iory1998"&gt; /u/Iory1998 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oj44s9/if_you_want_to_understand_why_llama_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oj44s9/if_you_want_to_understand_why_llama_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oj44s9/if_you_want_to_understand_why_llama_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-29T13:11:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1ojd1oq</id>
    <title>Here's the best prompt you will ever need to test the new LLMs</title>
    <updated>2025-10-29T18:49:59+00:00</updated>
    <author>
      <name>/u/Cool-Chemical-5629</name>
      <uri>https://old.reddit.com/user/Cool-Chemical-5629</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojd1oq/heres_the_best_prompt_you_will_ever_need_to_test/"&gt; &lt;img alt="Here's the best prompt you will ever need to test the new LLMs" src="https://preview.redd.it/n2yilqu2k3yf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=575a481bdcf458b2fd06c48788fa5f9271cd30ad" title="Here's the best prompt you will ever need to test the new LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Prompt:&lt;/p&gt; &lt;p&gt;The numbers Mason, what do they mean?!! 10 23 68 111 8 7 7 47 53 23 63 92 15&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cool-Chemical-5629"&gt; /u/Cool-Chemical-5629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/n2yilqu2k3yf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojd1oq/heres_the_best_prompt_you_will_ever_need_to_test/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ojd1oq/heres_the_best_prompt_you_will_ever_need_to_test/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-29T18:49:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1oje2cc</id>
    <title>DeepSeek may have found a new way to improve AI’s ability to remember</title>
    <updated>2025-10-29T19:27:50+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oje2cc/deepseek_may_have_found_a_new_way_to_improve_ais/"&gt; &lt;img alt="DeepSeek may have found a new way to improve AI’s ability to remember" src="https://external-preview.redd.it/aEXxC3nzMud-eeB3QaX4fT3GiVuldP60VVX6Yf4IvC8.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0104fa768b4fc264cfeccc0fd459a4eaa256f643" title="DeepSeek may have found a new way to improve AI’s ability to remember" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.technologyreview.com/2025/10/29/1126932/deepseek-ocr-visual-compression"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oje2cc/deepseek_may_have_found_a_new_way_to_improve_ais/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oje2cc/deepseek_may_have_found_a_new_way_to_improve_ais/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-29T19:27:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1ojf2r1</id>
    <title>Qwen3-VL now available in Ollama locally for all sizes.</title>
    <updated>2025-10-29T20:06:18+00:00</updated>
    <author>
      <name>/u/swagonflyyyy</name>
      <uri>https://old.reddit.com/user/swagonflyyyy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojf2r1/qwen3vl_now_available_in_ollama_locally_for_all/"&gt; &lt;img alt="Qwen3-VL now available in Ollama locally for all sizes." src="https://preview.redd.it/ycizvauvx3yf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a4ef60e1f14a4f71fc6659de298dd0d9565d8acb" title="Qwen3-VL now available in Ollama locally for all sizes." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/swagonflyyyy"&gt; /u/swagonflyyyy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ycizvauvx3yf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ojf2r1/qwen3vl_now_available_in_ollama_locally_for_all/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ojf2r1/qwen3vl_now_available_in_ollama_locally_for_all/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-29T20:06:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohdzxs</id>
    <title>AMA Announcement: Liquid AI, the team behind Liquid Foundational Models, LEAP and Apollo (Thu, Oct 30 • 10 AM – 1 PM PDT)</title>
    <updated>2025-10-27T13:10:46+00:00</updated>
    <author>
      <name>/u/LiquidAI_Team</name>
      <uri>https://old.reddit.com/user/LiquidAI_Team</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohdzxs/ama_announcement_liquid_ai_the_team_behind_liquid/"&gt; &lt;img alt="AMA Announcement: Liquid AI, the team behind Liquid Foundational Models, LEAP and Apollo (Thu, Oct 30 • 10 AM – 1 PM PDT)" src="https://preview.redd.it/47wfyylmlnxf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c359ceba4921b523ecd2e493f9cc84bd8b3e7881" title="AMA Announcement: Liquid AI, the team behind Liquid Foundational Models, LEAP and Apollo (Thu, Oct 30 • 10 AM – 1 PM PDT)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;When: Thursday 10/30, 10 AM – 1 PM PST&lt;/h1&gt; &lt;p&gt;The Liquid AI team will also continue answering questions for the following 24 hours, so jump in anytime!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Who will be there:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Jacob Marks (Data)&lt;/li&gt; &lt;li&gt;Jimmy Smith (Pre-Training)&lt;/li&gt; &lt;li&gt;Maxime Labonne (Post-Training)&lt;/li&gt; &lt;li&gt;Fernando Fernandes (Post-training)&lt;/li&gt; &lt;li&gt;Anna Banaszak (LFM2-VL)&lt;/li&gt; &lt;li&gt;Arthur Böök (LFM2-Audio)&lt;/li&gt; &lt;li&gt;Yuri Khrustalev (Inference engine, llama.cpp)&lt;/li&gt; &lt;li&gt;Darian Bhathena (LEAP SDK and Apollo)&lt;/li&gt; &lt;li&gt;Edoardo Mosca (LEAP Best Model Search and Finetune)&lt;/li&gt; &lt;li&gt;Anthony Crognale (LEAP SDK)&lt;/li&gt; &lt;li&gt;Pau Labarta Bajo (Dev Relations)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Want to get started?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;→ &lt;a href="https://leap.liquid.ai/models?utm_source=reddit&amp;amp;utm_medium=devrel"&gt;Deploy your first model on-device today&lt;/a&gt;&lt;br /&gt; → &lt;a href="https://huggingface.co/LiquidAI?utm_source=reddit&amp;amp;utm_medium=devrel"&gt;Check out our models on Hugging Face&lt;/a&gt;&lt;br /&gt; → &lt;a href="https://www.liquid.ai/apollo?utm_source=reddit&amp;amp;utm_medium=devrel"&gt;Play with models on Apollo&lt;/a&gt;&lt;br /&gt; → &lt;a href="https://www.liquid.ai/company/news?utm_source=reddit&amp;amp;utm_medium=devrel"&gt;Learn more about our recent releases&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LiquidAI_Team"&gt; /u/LiquidAI_Team &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/47wfyylmlnxf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohdzxs/ama_announcement_liquid_ai_the_team_behind_liquid/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohdzxs/ama_announcement_liquid_ai_the_team_behind_liquid/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T13:10:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohqev8</id>
    <title>Best Local TTS/STT Models - October 2025</title>
    <updated>2025-10-27T21:00:42+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Share what your favorite TTS / STT models are right now &lt;strong&gt;and why.&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Given the the amount of ambiguity and subjectivity in rating/testing these models, please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc. Closed models like Elevenlabs v3 seem to continue to be a few levels above open models, so comparisons, especially empirical ones are welcome.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please use the top level TTS/STT comments to thread your responses. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohqev8/best_local_ttsstt_models_october_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohqev8/best_local_ttsstt_models_october_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohqev8/best_local_ttsstt_models_october_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T21:00:42+00:00</published>
  </entry>
</feed>
