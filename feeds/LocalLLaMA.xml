<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-22T07:34:21+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1p2v0fe</id>
    <title>Deep Cogito v2.1, a new open weights 671B MoE model</title>
    <updated>2025-11-21T10:16:47+00:00</updated>
    <author>
      <name>/u/BreakfastFriendly728</name>
      <uri>https://old.reddit.com/user/BreakfastFriendly728</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2v0fe/deep_cogito_v21_a_new_open_weights_671b_moe_model/"&gt; &lt;img alt="Deep Cogito v2.1, a new open weights 671B MoE model" src="https://b.thumbs.redditmedia.com/ZFcSSCHd3EryXfkTwWHW8bx8DkNYSKpulONBXzIgxiQ.jpg" title="Deep Cogito v2.1, a new open weights 671B MoE model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/collections/deepcogito/cogito-v21"&gt;https://huggingface.co/collections/deepcogito/cogito-v21&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/wgqv3iva5l2g1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7b23a040098d2ed9caa81a6a322d02e18d51cc0e"&gt;https://preview.redd.it/wgqv3iva5l2g1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7b23a040098d2ed9caa81a6a322d02e18d51cc0e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/4rfhao3d5l2g1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=82dd4fcc80106c78f950f6516116123dad2f1b49"&gt;https://preview.redd.it/4rfhao3d5l2g1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=82dd4fcc80106c78f950f6516116123dad2f1b49&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/l88vmsue5l2g1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=da35111b441df51d43d4d5f04be4fb289b029525"&gt;https://preview.redd.it/l88vmsue5l2g1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=da35111b441df51d43d4d5f04be4fb289b029525&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BreakfastFriendly728"&gt; /u/BreakfastFriendly728 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2v0fe/deep_cogito_v21_a_new_open_weights_671b_moe_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2v0fe/deep_cogito_v21_a_new_open_weights_671b_moe_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2v0fe/deep_cogito_v21_a_new_open_weights_671b_moe_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T10:16:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2padh</id>
    <title>Unsloth just released their Olmo 3 dynamic quants!</title>
    <updated>2025-11-21T04:28:41+00:00</updated>
    <author>
      <name>/u/Aromatic-Distance817</name>
      <uri>https://old.reddit.com/user/Aromatic-Distance817</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2padh/unsloth_just_released_their_olmo_3_dynamic_quants/"&gt; &lt;img alt="Unsloth just released their Olmo 3 dynamic quants!" src="https://external-preview.redd.it/48d9roHWO9vPhqtCoIVGxdhD9jO5DC8s9h8U3EqHoCc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2c1dca215120c6d942685f73783d2b00bbdb86e8" title="Unsloth just released their Olmo 3 dynamic quants!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aromatic-Distance817"&gt; /u/Aromatic-Distance817 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/unsloth/Olmo-3-32B-Think-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2padh/unsloth_just_released_their_olmo_3_dynamic_quants/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2padh/unsloth_just_released_their_olmo_3_dynamic_quants/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T04:28:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1p3mqrd</id>
    <title>Frozen model discovers new optimal RL behaviors after millions of inference steps ‚Äî no updates (code released)</title>
    <updated>2025-11-22T06:58:22+00:00</updated>
    <author>
      <name>/u/chazc2</name>
      <uri>https://old.reddit.com/user/chazc2</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;arXiv‚Äôs first-time endorsement wall blocked me, but the idea is too important to wait.&lt;/p&gt; &lt;p&gt;Paper (submitted to ViXra Nov 22, 2025 ‚Äî ref 17620016, awaiting public release)&lt;/p&gt; &lt;p&gt;Code + trained models + full samples: &lt;a href="https://github.com/rd-nets-perpetual"&gt;https://github.com/rd-nets-perpetual&lt;/a&gt; The core idea is ~20 lines of code: never let the model retrieve the exact same memory representation twice + curiosity-triggered ‚Äúcreative crises‚Äù when it starts repeating. Results (all reproducible today on one GPU): ‚Ä¢ Frozen 84M transformer stays coherent and diverse for &amp;gt;1.8 million tokens on TinyShakespeare (vanilla collapses at ~12k) ‚Ä¢ Frozen 124M IMPALA agent on ProcGen CoinRun discovers brand-new optimal wall-jumps/wall-kicks it literally never executed once in training ‚Ä¢ Frozen retriever gets strictly better at retrieval over repeated queries This seems to kill the data wall. Someone please endorse me for arXiv cs.LG or I‚Äôll die on this hill.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chazc2"&gt; /u/chazc2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3mqrd/frozen_model_discovers_new_optimal_rl_behaviors/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3mqrd/frozen_model_discovers_new_optimal_rl_behaviors/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p3mqrd/frozen_model_discovers_new_optimal_rl_behaviors/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-22T06:58:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1p3cq0x</id>
    <title>Any local coding AI tools that can understand multiple files yet?</title>
    <updated>2025-11-21T22:41:20+00:00</updated>
    <author>
      <name>/u/sash20</name>
      <uri>https://old.reddit.com/user/sash20</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôd love to rely more on local models, but most local coding AI tools I‚Äôve tried only work well within single files. The moment a task spans multiple modules or needs real context, everything breaks. I‚Äôve been using Sweep AI in JetBrains when I need project-wide reasoning, but I‚Äôm still hoping for a local option that can do something similar. Anyone running a local setup that handles complex codebases?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sash20"&gt; /u/sash20 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3cq0x/any_local_coding_ai_tools_that_can_understand/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3cq0x/any_local_coding_ai_tools_that_can_understand/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p3cq0x/any_local_coding_ai_tools_that_can_understand/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T22:41:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1p347rt</id>
    <title>Minimax M2 - REAP 139B</title>
    <updated>2025-11-21T17:09:00+00:00</updated>
    <author>
      <name>/u/johannes_bertens</name>
      <uri>https://old.reddit.com/user/johannes_bertens</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyone did some actual (coding) work with this model yet?&lt;/p&gt; &lt;p&gt;At 80GB (Q4_K) it should fit on the Spark, the AMD Ryzen 395+ and the RTX PRO.&lt;br /&gt; The benchmarks are pretty good for prompt processing and fine for TG.&lt;/p&gt; &lt;p&gt;&lt;code&gt;Device 0: NVIDIA RTX PRO 6000 Blackwell Workstation Edition, compute capability 12.0, VMM: yes&lt;/code&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;model&lt;/th&gt; &lt;th align="right"&gt;size&lt;/th&gt; &lt;th align="right"&gt;params&lt;/th&gt; &lt;th&gt;backend&lt;/th&gt; &lt;th align="right"&gt;ngl&lt;/th&gt; &lt;th align="right"&gt;n_ubatch&lt;/th&gt; &lt;th align="right"&gt;fa&lt;/th&gt; &lt;th align="right"&gt;test&lt;/th&gt; &lt;th align="right"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;minimax-m2 230B.A10B Q4_K - Medium&lt;/td&gt; &lt;td align="right"&gt;78.40 GiB&lt;/td&gt; &lt;td align="right"&gt;139.15 B&lt;/td&gt; &lt;td&gt;CUDA&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;4096&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;pp1024&lt;/td&gt; &lt;td align="right"&gt;3623.43 ¬± 14.19&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;minimax-m2 230B.A10B Q4_K - Medium&lt;/td&gt; &lt;td align="right"&gt;78.40 GiB&lt;/td&gt; &lt;td align="right"&gt;139.15 B&lt;/td&gt; &lt;td&gt;CUDA&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;4096&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;pp2048&lt;/td&gt; &lt;td align="right"&gt;4224.81 ¬± 32.53&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;minimax-m2 230B.A10B Q4_K - Medium&lt;/td&gt; &lt;td align="right"&gt;78.40 GiB&lt;/td&gt; &lt;td align="right"&gt;139.15 B&lt;/td&gt; &lt;td&gt;CUDA&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;4096&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;pp3072&lt;/td&gt; &lt;td align="right"&gt;3950.17 ¬± 26.11&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;minimax-m2 230B.A10B Q4_K - Medium&lt;/td&gt; &lt;td align="right"&gt;78.40 GiB&lt;/td&gt; &lt;td align="right"&gt;139.15 B&lt;/td&gt; &lt;td&gt;CUDA&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;4096&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;pp4096&lt;/td&gt; &lt;td align="right"&gt;4202.56 ¬± 18.56&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;minimax-m2 230B.A10B Q4_K - Medium&lt;/td&gt; &lt;td align="right"&gt;78.40 GiB&lt;/td&gt; &lt;td align="right"&gt;139.15 B&lt;/td&gt; &lt;td&gt;CUDA&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;4096&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;pp5120&lt;/td&gt; &lt;td align="right"&gt;3984.08 ¬± 21.77&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;minimax-m2 230B.A10B Q4_K - Medium&lt;/td&gt; &lt;td align="right"&gt;78.40 GiB&lt;/td&gt; &lt;td align="right"&gt;139.15 B&lt;/td&gt; &lt;td&gt;CUDA&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;4096&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;pp6144&lt;/td&gt; &lt;td align="right"&gt;4601.65 ¬± 1152.92&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;minimax-m2 230B.A10B Q4_K - Medium&lt;/td&gt; &lt;td align="right"&gt;78.40 GiB&lt;/td&gt; &lt;td align="right"&gt;139.15 B&lt;/td&gt; &lt;td&gt;CUDA&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;4096&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;pp7168&lt;/td&gt; &lt;td align="right"&gt;3935.73 ¬± 23.47&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;minimax-m2 230B.A10B Q4_K - Medium&lt;/td&gt; &lt;td align="right"&gt;78.40 GiB&lt;/td&gt; &lt;td align="right"&gt;139.15 B&lt;/td&gt; &lt;td&gt;CUDA&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;4096&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;pp8192&lt;/td&gt; &lt;td align="right"&gt;4003.78 ¬± 16.54&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;minimax-m2 230B.A10B Q4_K - Medium&lt;/td&gt; &lt;td align="right"&gt;78.40 GiB&lt;/td&gt; &lt;td align="right"&gt;139.15 B&lt;/td&gt; &lt;td&gt;CUDA&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;4096&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;tg128&lt;/td&gt; &lt;td align="right"&gt;133.10 ¬± 51.97&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;code&gt;Device 0: NVIDIA RTX PRO 6000 Blackwell Workstation Edition, compute capability 12.0, VMM: yes&lt;/code&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;model&lt;/th&gt; &lt;th align="right"&gt;size&lt;/th&gt; &lt;th align="right"&gt;params&lt;/th&gt; &lt;th&gt;backend&lt;/th&gt; &lt;th align="right"&gt;ngl&lt;/th&gt; &lt;th align="right"&gt;n_ubatch&lt;/th&gt; &lt;th align="right"&gt;fa&lt;/th&gt; &lt;th align="right"&gt;test&lt;/th&gt; &lt;th align="right"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;minimax-m2 230B.A10B Q4_K - Medium&lt;/td&gt; &lt;td align="right"&gt;78.40 GiB&lt;/td&gt; &lt;td align="right"&gt;139.15 B&lt;/td&gt; &lt;td&gt;CUDA&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;4096&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;pp10240&lt;/td&gt; &lt;td align="right"&gt;3905.55 ¬± 22.55&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;minimax-m2 230B.A10B Q4_K - Medium&lt;/td&gt; &lt;td align="right"&gt;78.40 GiB&lt;/td&gt; &lt;td align="right"&gt;139.15 B&lt;/td&gt; &lt;td&gt;CUDA&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;4096&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;pp20480&lt;/td&gt; &lt;td align="right"&gt;3555.30 ¬± 175.54&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;minimax-m2 230B.A10B Q4_K - Medium&lt;/td&gt; &lt;td align="right"&gt;78.40 GiB&lt;/td&gt; &lt;td align="right"&gt;139.15 B&lt;/td&gt; &lt;td&gt;CUDA&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;4096&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;pp30720&lt;/td&gt; &lt;td align="right"&gt;3049.43 ¬± 71.14&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;minimax-m2 230B.A10B Q4_K - Medium&lt;/td&gt; &lt;td align="right"&gt;78.40 GiB&lt;/td&gt; &lt;td align="right"&gt;139.15 B&lt;/td&gt; &lt;td&gt;CUDA&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;4096&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;pp40960&lt;/td&gt; &lt;td align="right"&gt;2617.13 ¬± 59.72&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;minimax-m2 230B.A10B Q4_K - Medium&lt;/td&gt; &lt;td align="right"&gt;78.40 GiB&lt;/td&gt; &lt;td align="right"&gt;139.15 B&lt;/td&gt; &lt;td&gt;CUDA&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;4096&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;pp51200&lt;/td&gt; &lt;td align="right"&gt;2275.03 ¬± 34.24&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/johannes_bertens"&gt; /u/johannes_bertens &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p347rt/minimax_m2_reap_139b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p347rt/minimax_m2_reap_139b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p347rt/minimax_m2_reap_139b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T17:09:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2wnh0</id>
    <title>Which model to choose for coding with 8GB VRAM (assuming quantised) if I'm happy with slow rates like 1tk/s speed.</title>
    <updated>2025-11-21T11:53:47+00:00</updated>
    <author>
      <name>/u/MakeshiftApe</name>
      <uri>https://old.reddit.com/user/MakeshiftApe</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Trying to find the best local model I can use for aid in coding. My specs are: 5950X, 32GB RAM, 8GB RTX3070, so I'm severely limited on VRAM - but I seem to have much lower acceptable speeds than most people, so I'm happy to off-load a lot to the CPU to allow for a larger more capable model. &lt;/p&gt; &lt;p&gt;For me even as low as 1tk/s is plenty fast, I don't need an LLM to respond to me instantly, I can wait a minute for a reply.&lt;/p&gt; &lt;p&gt;So far after researching models that'd work with my GPU I landed on Qwen3-14B and GPT-OSS-20B, with the latter seeming better in my tests. &lt;/p&gt; &lt;p&gt;Both run pretty fast by my standards. Which leaves me wondering if I can push it higher and if so what model I should try? Is there anything better?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Any suggestions?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;If it matters at all I'm primarily looking for help with GDScript, Java, C++, and Python. Not sure if there's any variance in programming language-proficiency between models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MakeshiftApe"&gt; /u/MakeshiftApe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2wnh0/which_model_to_choose_for_coding_with_8gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2wnh0/which_model_to_choose_for_coding_with_8gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2wnh0/which_model_to_choose_for_coding_with_8gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T11:53:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2v5ap</id>
    <title>Epstein Files Document Embeddings (768D, Nomic)</title>
    <updated>2025-11-21T10:25:10+00:00</updated>
    <author>
      <name>/u/qwer1627</name>
      <uri>https://old.reddit.com/user/qwer1627</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Text embeddings generated from the House Oversight Committee's Epstein document release. (768D, Nomic)&lt;/p&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/datasets/svetfm/epstein-files-nov11-25-house-post-ocr-embeddings#source-dataset"&gt;&lt;/a&gt;Source Dataset&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;This dataset is derived from:&lt;/strong&gt; &lt;a href="https://huggingface.co/datasets/tensonaut/EPSTEIN_FILES_20K"&gt;tensonaut/EPSTEIN_FILES_20K&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The source dataset contains OCR'd text from the original House Oversight Committee PDF release.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/datasets/svetfm/epstein-files-nov11-25-house-post-ocr-embeddings"&gt;https://huggingface.co/datasets/svetfm/epstein-files-nov11-25-house-post-ocr-embeddings&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/qwer1627"&gt; /u/qwer1627 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2v5ap/epstein_files_document_embeddings_768d_nomic/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2v5ap/epstein_files_document_embeddings_768d_nomic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2v5ap/epstein_files_document_embeddings_768d_nomic/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T10:25:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1p3ejjt</id>
    <title>Adding link to a prompt</title>
    <updated>2025-11-21T23:59:59+00:00</updated>
    <author>
      <name>/u/Ok-Word-4894</name>
      <uri>https://old.reddit.com/user/Ok-Word-4894</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! I have my LLM running in LM Studio + Open WebUI. And my own instance of SearXNG. Using Docker. I have successfully added web search, so that‚Äôs good. &lt;/p&gt; &lt;p&gt;Question: What do I setup so that I can include a URL in the body of a prompt? &lt;/p&gt; &lt;p&gt;Thanks. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Word-4894"&gt; /u/Ok-Word-4894 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3ejjt/adding_link_to_a_prompt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3ejjt/adding_link_to_a_prompt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p3ejjt/adding_link_to_a_prompt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T23:59:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1p335ta</id>
    <title>FYI / warning: default Nvidia fan speed control (Blackwell, maybe others) is horrible</title>
    <updated>2025-11-21T16:29:26+00:00</updated>
    <author>
      <name>/u/sixx7</name>
      <uri>https://old.reddit.com/user/sixx7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As we all do, I obsessively monitor &lt;code&gt;nvtop&lt;/code&gt; during AI or other heavy workloads on my GPUs. Well, the other day, I noticed a 5090 running at 81-83C but the fan only running at 50%. Yikes!&lt;/p&gt; &lt;p&gt;I tried everything in this thread: &lt;a href="https://forums.developer.nvidia.com/t/how-to-set-fanspeed-in-linux-from-terminal/72705"&gt;https://forums.developer.nvidia.com/t/how-to-set-fanspeed-in-linux-from-terminal/72705&lt;/a&gt; to no avail. Even using the gui of nvidia-settings, as root, would not let me apply a higher fan speed.&lt;/p&gt; &lt;p&gt;I found 3 repos on Github to solve this. I am not affiliated with any of them, and I chose the Python option (credit: &lt;a href="https://www.reddit.com/r/wayland/comments/1arjtxj/i_have_created_a_program_to_control_nvidia_gpus/"&gt;https://www.reddit.com/r/wayland/comments/1arjtxj/i_have_created_a_program_to_control_nvidia_gpus/&lt;/a&gt; )&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Python option:&lt;a href="https://github.com/HackTestes/NVML-GPU-Control"&gt;https://github.com/HackTestes/NVML-GPU-Control&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Golang option: &lt;a href="https://github.com/ntchjb/nvidia-fan-controller"&gt;https://github.com/ntchjb/nvidia-fan-controller&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;C option:&lt;a href="https://github.com/xl0/nvml-tool"&gt;https://github.com/xl0/nvml-tool&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The python app worked like a charm: chnvml control -n &amp;quot;NVIDIA GeForce RTX 5090&amp;quot; -sp &amp;quot;0:30,30:35,35:40,40:50,50:65,60:100&amp;quot;&lt;/p&gt; &lt;p&gt;This ramped up my fan speeds right away and immediately brought my GPU temperature below 70C&lt;/p&gt; &lt;p&gt;I am pretty shocked it was a steady 81C+ and keeping the fan at 50%. Maybe it's better in other OS or driver versions. My env: Ubuntu, Nvidia driver version 580.95.05&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sixx7"&gt; /u/sixx7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p335ta/fyi_warning_default_nvidia_fan_speed_control/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p335ta/fyi_warning_default_nvidia_fan_speed_control/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p335ta/fyi_warning_default_nvidia_fan_speed_control/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T16:29:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2ziil</id>
    <title>Hardcore function calling benchmark in backend coding agent.</title>
    <updated>2025-11-21T14:06:49+00:00</updated>
    <author>
      <name>/u/jhnam88</name>
      <uri>https://old.reddit.com/user/jhnam88</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2ziil/hardcore_function_calling_benchmark_in_backend/"&gt; &lt;img alt="Hardcore function calling benchmark in backend coding agent." src="https://b.thumbs.redditmedia.com/YaA_TdwEKDUN4oRX3BCB118KPjtAaCveNFCP3lFYqyQ.jpg" title="Hardcore function calling benchmark in backend coding agent." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h2&gt;Hardcore Benchmark&lt;/h2&gt; &lt;p&gt;&lt;a href="https://github.com/wrtnlabs/autobe"&gt;AutoBE&lt;/a&gt; is an open-source project that generates backend applications through extensive function calling.&lt;/p&gt; &lt;p&gt;As AutoBE utilizes LLM function calling in every phase instead of plain text writing, including compiler's AST (Abstract Syntax Tree) structures of infinite depths, I think this can be the most extreme function calling benchmark ever.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/wrtnlabs/autobe/blob/main/packages/interface/src/prisma/AutoBePrisma.ts"&gt;DB Compiler's AST&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/wrtnlabs/autobe/blob/main/packages/interface/src/openapi/AutoBeOpenApi.ts"&gt;API specification's AST&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/wrtnlabs/autobe/blob/main/packages/interface/src/test/AutoBeTest.ts"&gt;Test function's AST&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;code&gt;typescript // Example of AutoBE's AST structure export namespace AutoBeOpenApi { export type IJsonSchema = | IJsonSchema.IConstant | IJsonSchema.IBoolean | IJsonSchema.IInteger | IJsonSchema.INumber | IJsonSchema.IString | IJsonSchema.IArray | IJsonSchema.IObject | IJsonSchema.IReference | IJsonSchema.IOneOf | IJsonSchema.INull; } &lt;/code&gt;&lt;/p&gt; &lt;h2&gt;Limitations&lt;/h2&gt; &lt;p&gt;Of course, as you can see, the number of DB schemas and API operations generated for the same topic varies greatly by each model. When &lt;a href="https://github.com/wrtnlabs/autobe-examples/tree/main/anthropic/claude-sonnet-4.5/shopping"&gt;&lt;code&gt;anthropic/claude-sonnet-4.5&lt;/code&gt;&lt;/a&gt; and &lt;a href="https://github.com/wrtnlabs/autobe-examples/tree/main/openai/gpt-5.1/shopping"&gt;&lt;code&gt;openai/gpt-5.1&lt;/code&gt;&lt;/a&gt; create 630 and 2,000 test functions respectively for the same topic, &lt;a href="https://github.com/wrtnlabs/autobe-examples/tree/main/qwen/qwen3-next-80b-a3b-instruct/shopping"&gt;&lt;code&gt;qwen/qwen3-next-80b-a3b&lt;/code&gt;&lt;/a&gt; creates 360.&lt;/p&gt; &lt;p&gt;Moreover, function calling in AutoBE includes a &lt;a href="https://autobe.dev/docs/concepts/function-calling/#validation-feedback"&gt;validation feedback&lt;/a&gt; process that detects detailed type errors and provides feedback to the AI for recovery, even when the AI makes mistakes and creates arguments of the wrong type.&lt;/p&gt; &lt;p&gt;Simply scoring and ranking based solely on compilation/build success, and evaluating each model's function calling capabilities in depth based only on the success rate of function calling with validation feedback, is still far from sufficient.&lt;/p&gt; &lt;p&gt;Therefore, please understand that the current benchmark is simply uncontrolled and only indicates whether or not each AI model can properly construct extremely complex types, including compiler AST structures, through function calling.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;AutoBE is also still incomplete.&lt;/p&gt; &lt;p&gt;Even if the backend application generated through this guarantees a 100% compilation success rate, it does not guarantee a 100% runtime success rate. This is an open-source project with a long way to go in development and mountains of research still to be done.&lt;/p&gt; &lt;p&gt;However, we hope that this can serve as a reference for anyone planning function calling with extremely complex types like ours, and contribute even a little to the AI ecosystem.&lt;/p&gt; &lt;/blockquote&gt; &lt;h2&gt;Promise&lt;/h2&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1o3604u/autobe_achieved_100_compilation_success_of/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1o3604u/autobe_achieved_100_compilation_success_of/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A month ago, we achieved a 100% build success rate for small to medium-sized backend applications with &lt;code&gt;qwen3-next-80b-a3b&lt;/code&gt;, and promised to complete RAG optimization in the future to enable the generation of large-scale backend applications on Local LLMs.&lt;/p&gt; &lt;p&gt;Now this has become possible with various Local LLMs such as Qwen3/DeepSeek/Kimi, in addition to commercial models like GPT and Sonnet. While prompting and RAG optimization may not yet be perfect, as models like GPT-5.1 run wild creating as many as 2,000 test functions, we will resolve this issue the next time we come back.&lt;/p&gt; &lt;p&gt;And since many people were curious about the performance of various Local LLMs besides &lt;code&gt;qwen3-next-80b-a3b&lt;/code&gt;, we promised to consistently release benchmark data for them. While it's unfortunate that the benchmark we released today is inadequate due to lack of controlled variables and can only determine whether function calling with extremely complex types is possible or not, we will improve this as well next time.&lt;/p&gt; &lt;p&gt;We, the two AutoBE developers, will continue to dedicate ourselves to its development, striving to create an environment where you can freely generate backend applications on your local devices without cost burden.&lt;/p&gt; &lt;p&gt;In addition, we are always grateful to the specialists who build and freely distribute open-source AI models.&lt;/p&gt; &lt;h2&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;AutoBE: &lt;a href="https://github.com/wrtnlabs/autobe"&gt;https://github.com/wrtnlabs/autobe&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Benchmark Result: &lt;a href="https://github.com/wrtnlabs/autobe-examples"&gt;https://github.com/wrtnlabs/autobe-examples&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jhnam88"&gt; /u/jhnam88 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1p2ziil"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2ziil/hardcore_function_calling_benchmark_in_backend/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2ziil/hardcore_function_calling_benchmark_in_backend/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T14:06:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1p3ktsf</id>
    <title>An open-source AI coding agent for legacy code modernization</title>
    <updated>2025-11-22T05:06:34+00:00</updated>
    <author>
      <name>/u/nolanolson</name>
      <uri>https://old.reddit.com/user/nolanolson</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3ktsf/an_opensource_ai_coding_agent_for_legacy_code/"&gt; &lt;img alt="An open-source AI coding agent for legacy code modernization" src="https://b.thumbs.redditmedia.com/iXk5oNo9ODYs3nf_Yqqz4_A3o5wvNU4IjfHU4AgNVMw.jpg" title="An open-source AI coding agent for legacy code modernization" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been experimenting with something called &lt;strong&gt;L2M&lt;/strong&gt;, an AI coding agent that‚Äôs a bit different from the usual ‚Äúwrite me code‚Äù assistants (Claude Code, Cursor, Codex, etc.). Instead of focusing on greenfield coding, it‚Äôs built specifically around &lt;strong&gt;legacy code understanding and modernization&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;The idea is less about autocompleting new features and more about dealing with the messy stuff many teams actually struggle with: old languages, tangled architectures, inconsistent coding styles, missing docs, weird frameworks, etc.&lt;/p&gt; &lt;p&gt;A few things that stood out while testing it:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Supports &lt;strong&gt;160+ programming languages&lt;/strong&gt;‚Äîincluding some pretty obscure and older ones.&lt;/li&gt; &lt;li&gt;Has &lt;strong&gt;Git integration plus contextual memory&lt;/strong&gt;, so it doesn‚Äôt forget earlier files or decisions while navigating a big codebase.&lt;/li&gt; &lt;li&gt;You can &lt;strong&gt;bring your own model&lt;/strong&gt; (apparently supports 100+ LLMs), which is useful if you‚Äôre wary of vendor lock-in or need specific model behavior.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It doesn‚Äôt just translate/refactor code; it actually tries to reason about it and then &lt;strong&gt;self-validate&lt;/strong&gt; its output, which feels closer to how a human reviews legacy changes.&lt;/p&gt; &lt;p&gt;Not sure if this will become mainstream, but it‚Äôs an interesting niche‚Äîmost AI tools chase new code, not decades-old systems.&lt;/p&gt; &lt;p&gt;If anyone‚Äôs curious, the repo is here: &lt;a href="https://github.com/astrio-ai/l2m"&gt;https://github.com/astrio-ai/l2m&lt;/a&gt; üåü&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/kjgipcl6rq2g1.png?width=1334&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=11d93fce798dba3063c07491f18287dbac41624c"&gt;https://preview.redd.it/kjgipcl6rq2g1.png?width=1334&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=11d93fce798dba3063c07491f18287dbac41624c&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nolanolson"&gt; /u/nolanolson &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3ktsf/an_opensource_ai_coding_agent_for_legacy_code/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3ktsf/an_opensource_ai_coding_agent_for_legacy_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p3ktsf/an_opensource_ai_coding_agent_for_legacy_code/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-22T05:06:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1p3gqo8</id>
    <title>Which is the least agreeable/sycophantic AI model at the moment?</title>
    <updated>2025-11-22T01:39:27+00:00</updated>
    <author>
      <name>/u/BrokenLoadOrder</name>
      <uri>https://old.reddit.com/user/BrokenLoadOrder</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For some context: My wife and I moved to a teeny tiny town, and there's not a lot of nerds here to play D&amp;amp;D/RootRPG with, but I do miss the silly antics I used to get up to. I tried a few sessions across various AI, but there's two kinda major issues I've noticed across most:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Being too agreeable - This is by far the most common problem, and ends up meaning you can tell the &amp;quot;DM&amp;quot; (Being the AI) pretty much anything, and it'll let you do it. In one of my very first runs trying this out, I soloed pretty much an entire battlefield, paid with gold I didn't have and convinced multiple enemy factions to give up even as a complete nobody. Even in cases where I've asked it to provide a difficulty check, that leads to a second issue...&lt;/li&gt; &lt;li&gt;Randomly losing its mind - I understand this is a bit of a vague title, but sometimes the AI has a rather tenuous grasp of reality. I've seen it say things like &amp;quot;This is an Easy Skill check&amp;quot; followed by an incredibly high number. I've seen it freak out over things like violence (Including my favourite example where I got shut down for using the term &amp;quot;bloodshot eyes&amp;quot; &lt;em&gt;immediately after the AI just used the term&lt;/em&gt;). I've seen it completely forget what items I have, skills, etc.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;TLDR: Has anyone found an offline AI that can work as a semi-competent DM for some homebrew adventures?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BrokenLoadOrder"&gt; /u/BrokenLoadOrder &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3gqo8/which_is_the_least_agreeablesycophantic_ai_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3gqo8/which_is_the_least_agreeablesycophantic_ai_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p3gqo8/which_is_the_least_agreeablesycophantic_ai_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-22T01:39:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1p3ie7w</id>
    <title>Echo TTS can seemingly generate music surprisingly well</title>
    <updated>2025-11-22T02:59:34+00:00</updated>
    <author>
      <name>/u/iGermanProd</name>
      <uri>https://old.reddit.com/user/iGermanProd</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;While playing around with the Echo TTS demo from the recent post &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1p2l36u/echo_tts_441khz_fast_fits_under_8gb_vram_sota/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1p2l36u/echo_tts_441khz_fast_fits_under_8gb_vram_sota/&lt;/a&gt;, I discovered that if you load a song in as a reference audio and bump the CFGs (I set mine to 5, 7 respectively), as well as prompt like this:&lt;/p&gt; &lt;p&gt;&lt;code&gt; [Music] [Music] [S1] (singing) Yeah, I'm gon' take my horse to the old town road [S1] (singing) I'm gonna ride 'til I can't no more [S1] (singing) I'm gon' take my horse to the old town road [S1] (singing) I'm gon' (Kio, Kio) ride 'til I can't no more [S1] (singing) I got the horses in the back [S1] (singing) Horse tack is attached [S1] (singing) Hat is matte black [S1] (singing) Got the boots that's black to match [S1] (singing) Riding on a horse, ha [S1] (singing) You can whip your Porsche [S1] (singing) I been in the valley [S1] (singing) You ain't been up off that porch now [S1] (singing) Can't nobody tell me nothing [S1] (singing) You can't tell me nothing [Music] [Music] &lt;/code&gt;&lt;/p&gt; &lt;p&gt;It will output shockingly decent results for a model that's not at all been trained to do music. I wonder what would happen if one were to fine-tune it on music.&lt;/p&gt; &lt;p&gt;Here are some demos: &lt;a href="https://voca.ro/185lsRLEByx0"&gt;https://voca.ro/185lsRLEByx0&lt;/a&gt; &lt;a href="https://voca.ro/142AWpTH9jD7"&gt;https://voca.ro/142AWpTH9jD7&lt;/a&gt; &lt;a href="https://voca.ro/1imeBG3ZDYIo"&gt;https://voca.ro/1imeBG3ZDYIo&lt;/a&gt; &lt;a href="https://voca.ro/1ldaxj8MzYr5"&gt;https://voca.ro/1ldaxj8MzYr5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It's obviously not very coherent or consistent in the long run, but it's clearly got the chops to be, that last ambient result actually sounds pretty good. Hopefully it will actually get released for local use.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iGermanProd"&gt; /u/iGermanProd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3ie7w/echo_tts_can_seemingly_generate_music/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3ie7w/echo_tts_can_seemingly_generate_music/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p3ie7w/echo_tts_can_seemingly_generate_music/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-22T02:59:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1p360cl</id>
    <title>How's your experience with Qwen3-Next-80B-A3B ?</title>
    <updated>2025-11-21T18:16:15+00:00</updated>
    <author>
      <name>/u/woahdudee2a</name>
      <uri>https://old.reddit.com/user/woahdudee2a</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know llama.cpp support is still a short while away but surely some people here are able to run it with vLLM. I'm curious how it performs in comparison to gpt-oss-120b or nemotron-super-49B-v1.5&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/woahdudee2a"&gt; /u/woahdudee2a &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p360cl/hows_your_experience_with_qwen3next80ba3b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p360cl/hows_your_experience_with_qwen3next80ba3b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p360cl/hows_your_experience_with_qwen3next80ba3b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T18:16:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2w5i6</id>
    <title>HunyuanVideo-1.5: A leading lightweight video generation model</title>
    <updated>2025-11-21T11:25:33+00:00</updated>
    <author>
      <name>/u/abdouhlili</name>
      <uri>https://old.reddit.com/user/abdouhlili</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/tencent/HunyuanVideo-1.5"&gt;https://huggingface.co/tencent/HunyuanVideo-1.5&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdouhlili"&gt; /u/abdouhlili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2w5i6/hunyuanvideo15_a_leading_lightweight_video/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2w5i6/hunyuanvideo15_a_leading_lightweight_video/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2w5i6/hunyuanvideo15_a_leading_lightweight_video/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T11:25:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1p36l5f</id>
    <title>2x RTX 5060 TI 16 GB =32GB VRAM -</title>
    <updated>2025-11-21T18:38:39+00:00</updated>
    <author>
      <name>/u/quantier</name>
      <uri>https://old.reddit.com/user/quantier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p36l5f/2x_rtx_5060_ti_16_gb_32gb_vram/"&gt; &lt;img alt="2x RTX 5060 TI 16 GB =32GB VRAM -" src="https://preview.redd.it/ven6e8i8nn2g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ff9be1ee0ff290587c547010478fc54b1176c114" title="2x RTX 5060 TI 16 GB =32GB VRAM -" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is anyone up and running with a rig like this with 2x RTX 5060 TI? how is it? What PSU does one need? How much compute do you loose when you have 2 GPU:s instead of a 1 card setup. How would 2x 5060 TI be in comparison with a 5090.&lt;/p&gt; &lt;p&gt;How does one put together these GPU:s in ComfyUI? Does one need to add new nodes to the workflows?&lt;/p&gt; &lt;p&gt;Is this worth it, I can get a RTX 5060 TI 16GB for around $400 each meaning that $800 for 32 GB VRAM feels very interesting with a Blackwell card!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/quantier"&gt; /u/quantier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ven6e8i8nn2g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p36l5f/2x_rtx_5060_ti_16_gb_32gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p36l5f/2x_rtx_5060_ti_16_gb_32gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T18:38:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1p36iln</id>
    <title>Made a site where AI models trade against each other. A local model is winning.</title>
    <updated>2025-11-21T18:35:58+00:00</updated>
    <author>
      <name>/u/2degreestarget</name>
      <uri>https://old.reddit.com/user/2degreestarget</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been messing around with new Gemini this week and ended up building this thing where different LLMs compete as stock traders. I work in asset management so I was genuinely curious how these models would approach investing.&lt;/p&gt; &lt;p&gt;Some observations:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Qwen (the only local model) is currently winning, mostly because keeps 90% cash (saving for a GPU?)&lt;/li&gt; &lt;li&gt;None of them understand position sizing. Like, at all. And they all have this weird overconfidence where they'll write a whole thesis and then make a trade that contradicts it.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Anyway it's not meant to be serious financial advice or anything. Just thought it was a fun way to see how these models actually think when you give them a concrete task. &lt;/p&gt; &lt;p&gt;Code is messy but it works. Considering doing a fully local version to stop burning my openrouter credits...&lt;br /&gt; &lt;a href="http://wallstreetarena.xyz/"&gt;http://wallstreetarena.xyz/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/2degreestarget"&gt; /u/2degreestarget &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p36iln/made_a_site_where_ai_models_trade_against_each/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p36iln/made_a_site_where_ai_models_trade_against_each/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p36iln/made_a_site_where_ai_models_trade_against_each/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T18:35:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1p38wp2</id>
    <title>Dell puts 870 INT8 TOPS in Pro Max 16 Plus laptop with dual Qualcomm AI-100 discrete NPUs and 128GB LPDDR5X</title>
    <updated>2025-11-21T20:08:24+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p38wp2/dell_puts_870_int8_tops_in_pro_max_16_plus_laptop/"&gt; &lt;img alt="Dell puts 870 INT8 TOPS in Pro Max 16 Plus laptop with dual Qualcomm AI-100 discrete NPUs and 128GB LPDDR5X" src="https://external-preview.redd.it/k6ugreuNYLcJLu7o30ZlDvM4GYr0mtEvIvPMXJ8mR2c.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=675947acdabed7d61c07e99006c75339ee1cfa5f" title="Dell puts 870 INT8 TOPS in Pro Max 16 Plus laptop with dual Qualcomm AI-100 discrete NPUs and 128GB LPDDR5X" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Dell is shipping the Pro Max 16 Plus laptop with Qualcomm‚Äôs discrete AI-100 Ultra NPU, delivering 870 INT8 TOPS at 150W TDP with 128GB LPDDR5X memory, enabling local inference of AI models up to 120 billion parameters. The system pairs this with an Intel Core Ultra 9 285HX vPro CPU (24 cores) and 64GB system RAM, but notably omits a discrete GPU, relying instead on Arrow Lake-HX‚Äôs integrated graphics, as the NPU occupies the thermal and power budget typically allocated to a dGPU. The dual-NPU configuration provides 64GB dedicated AI memory and supports FP16 precision inference, positioning the device as an ‚Äúedge server in a backpack‚Äù.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.techpowerup.com/343143/dell-ship-pro-max-16-plus-laptops-with-qualcomms-discrete-npu"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p38wp2/dell_puts_870_int8_tops_in_pro_max_16_plus_laptop/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p38wp2/dell_puts_870_int8_tops_in_pro_max_16_plus_laptop/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T20:08:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1p3b60m</id>
    <title>When do you think open-source models will catch up to Gemini 3/Nano Banana pro? Who's the closest candidate right now?</title>
    <updated>2025-11-21T21:38:16+00:00</updated>
    <author>
      <name>/u/abdouhlili</name>
      <uri>https://old.reddit.com/user/abdouhlili</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm curious about the current gap between open-source models and something like Gemini 3. Do you think open-source will catch up anytime soon, and if so, which model is the closest right now?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdouhlili"&gt; /u/abdouhlili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3b60m/when_do_you_think_opensource_models_will_catch_up/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3b60m/when_do_you_think_opensource_models_will_catch_up/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p3b60m/when_do_you_think_opensource_models_will_catch_up/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T21:38:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1p3e0mp</id>
    <title>GPT-Usenet; an 81-million-parameter model trained on 10 GB of USENET posts(including the entire UTZOO archives) and over 1 GB of various other text files. Reached training loss of 2.3256 and validation loss of 2.3651. MIT licensed.</title>
    <updated>2025-11-21T23:36:11+00:00</updated>
    <author>
      <name>/u/CommodoreCarbonate</name>
      <uri>https://old.reddit.com/user/CommodoreCarbonate</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3e0mp/gptusenet_an_81millionparameter_model_trained_on/"&gt; &lt;img alt="GPT-Usenet; an 81-million-parameter model trained on 10 GB of USENET posts(including the entire UTZOO archives) and over 1 GB of various other text files. Reached training loss of 2.3256 and validation loss of 2.3651. MIT licensed." src="https://preview.redd.it/ski1cmw74p2g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7405c257c9f0583718878eca9a57b452c11abca7" title="GPT-Usenet; an 81-million-parameter model trained on 10 GB of USENET posts(including the entire UTZOO archives) and over 1 GB of various other text files. Reached training loss of 2.3256 and validation loss of 2.3651. MIT licensed." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sample text.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CommodoreCarbonate"&gt; /u/CommodoreCarbonate &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ski1cmw74p2g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3e0mp/gptusenet_an_81millionparameter_model_trained_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p3e0mp/gptusenet_an_81millionparameter_model_trained_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T23:36:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1p35f2c</id>
    <title>I made a free playground for comparing 10+ OCR models side-by-side</title>
    <updated>2025-11-21T17:54:07+00:00</updated>
    <author>
      <name>/u/Emc2fma</name>
      <uri>https://old.reddit.com/user/Emc2fma</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's called OCR Arena, you can try it here: &lt;a href="https://ocrarena.ai"&gt;https://ocrarena.ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There's so many new OCR models coming out all the time, but testing them is really painful. I wanted to give the community an easy way to compare leading foundation VLMs and open source OCR models side-by-side. You can upload any doc, run a variety of models, and view diffs easily.&lt;/p&gt; &lt;p&gt;So far I've added Gemini 3, dots, DeepSeek-OCR, olmOCR 2, Qwen3-VL-8B, and a few others. &lt;/p&gt; &lt;p&gt;Would love any feedback you have! And if there's any other models you'd like included, let me know.&lt;/p&gt; &lt;p&gt;(No surprise, Gemini 3 is top of the leaderboard right now)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Emc2fma"&gt; /u/Emc2fma &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p35f2c/i_made_a_free_playground_for_comparing_10_ocr/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p35f2c/i_made_a_free_playground_for_comparing_10_ocr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p35f2c/i_made_a_free_playground_for_comparing_10_ocr/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T17:54:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1p3d34y</id>
    <title>Inspired by a recent post: a list of the cheapest to most expensive 32GB GPUs on Amazon right now, Nov 21 2025</title>
    <updated>2025-11-21T22:56:25+00:00</updated>
    <author>
      <name>/u/__JockY__</name>
      <uri>https://old.reddit.com/user/__JockY__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Inspired by a recent post where someone was putting together a system based on two 16GB GPUs for $800 I wondered how one might otherwise conveniently acquire 32GB of reasonably performant VRAM as cheaply as possible?&lt;/p&gt; &lt;p&gt;Bezos to the rescue!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Hewlett Packard Enterprise NVIDIA Tesla M10 Quad GPU Module&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Cost: $279&lt;/li&gt; &lt;li&gt;VRAM: GDDR5 (332 GB/s)&lt;/li&gt; &lt;li&gt;PCIe: 3.0&lt;/li&gt; &lt;li&gt;Link: &lt;a href="https://www.amazon.com/Hewlett-Packard-Enterprise-NVIDIA-870046-001/dp/B075VQ5LF8"&gt;https://www.amazon.com/Hewlett-Packard-Enterprise-NVIDIA-870046-001/dp/B075VQ5LF8&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Tesla V100 32GB SXM2 GPU W/Pcie Adapter &amp;amp; 6+2 Pin&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Cost: $879.00&lt;/li&gt; &lt;li&gt;VRAM: HBM2 (898 GB/s)&lt;/li&gt; &lt;li&gt;PCIe: 3.0&lt;/li&gt; &lt;li&gt;Link: &lt;a href="https://www.amazon.com/Tesla-V100-32GB-Adapter-Computing/dp/B0FXWJ8HKD"&gt;https://www.amazon.com/Tesla-V100-32GB-Adapter-Computing/dp/B0FXWJ8HKD&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;NVIDIA Tesla V100 Volta GPU Accelerator 32GB&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Cost: $969&lt;/li&gt; &lt;li&gt;VRAM: HBM2 (898 GB/s)&lt;/li&gt; &lt;li&gt;PCIe: 3.0&lt;/li&gt; &lt;li&gt;Link: &lt;a href="https://www.amazon.com/NVIDIA-Tesla-Volta-Accelerator-Graphics/dp/B07JVNHFFX"&gt;https://www.amazon.com/NVIDIA-Tesla-Volta-Accelerator-Graphics/dp/B07JVNHFFX&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;NVIDIA Tesla V100 (Volta) 32GB&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Cost: $1144&lt;/li&gt; &lt;li&gt;VRAM: HBM2 (898 GB/s)&lt;/li&gt; &lt;li&gt;PCIe: 3.0&lt;/li&gt; &lt;li&gt;Link: &lt;a href="https://www.amazon.com/NVIDIA-Tesla-900-2G503-0310-000-NVLINK-GPU/dp/B07WDDNGXK"&gt;https://www.amazon.com/NVIDIA-Tesla-900-2G503-0310-000-NVLINK-GPU/dp/B07WDDNGXK&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;GIGABYTE AORUS GeForce RTX 5090 Master 32G&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Cost: $2599&lt;/li&gt; &lt;li&gt;VRAM: GDDR7 (1792 GB/s)&lt;/li&gt; &lt;li&gt;PCIe: 5.0&lt;/li&gt; &lt;li&gt;Link: &lt;a href="https://www.amazon.com/GIGABYTE-Graphics-WINDFORCE-GV-N5090AORUS-M-32GD/dp/B0DT7GHQMD"&gt;https://www.amazon.com/GIGABYTE-Graphics-WINDFORCE-GV-N5090AORUS-M-32GD/dp/B0DT7GHQMD&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;PNY NVIDIA GeForce RTX‚Ñ¢ 5090 OC Triple Fan&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Cost: $2749&lt;/li&gt; &lt;li&gt;VRAM: GDDR7 (1792 GB/s)&lt;/li&gt; &lt;li&gt;PCIe: 5.0&lt;/li&gt; &lt;li&gt;Link: &lt;a href="https://www.amazon.com/PNY-GeForce-Overclocked-Graphics-3-5-Slot/dp/B0DTJF8YT4/"&gt;https://www.amazon.com/PNY-GeForce-Overclocked-Graphics-3-5-Slot/dp/B0DTJF8YT4/&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For comparison an RTX 3090 has 24GB of 936.2 GB/s GDDR6X, so for $879 it's hard to grumble about 32GB of 898 GB/s HBM2 in those V100s!&lt;/p&gt; &lt;p&gt;Edit: the V100 doesn‚Äôt support CUDA 8.x and later, so check compatibility before making impulse buys!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/__JockY__"&gt; /u/__JockY__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3d34y/inspired_by_a_recent_post_a_list_of_the_cheapest/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3d34y/inspired_by_a_recent_post_a_list_of_the_cheapest/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p3d34y/inspired_by_a_recent_post_a_list_of_the_cheapest/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T22:56:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1p3fwj5</id>
    <title>GLM planning a 30-billion-parameter model release for 2025</title>
    <updated>2025-11-22T01:00:05+00:00</updated>
    <author>
      <name>/u/aichiusagi</name>
      <uri>https://old.reddit.com/user/aichiusagi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3fwj5/glm_planning_a_30billionparameter_model_release/"&gt; &lt;img alt="GLM planning a 30-billion-parameter model release for 2025" src="https://external-preview.redd.it/age5KNQL_0umG4-4KoTku-i61lSg2HdDlBNVJO56C64.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4ee469153e66d145a6de755ed94715567aa83c6e" title="GLM planning a 30-billion-parameter model release for 2025" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aichiusagi"&gt; /u/aichiusagi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://open.substack.com/pub/chinatalk/p/the-zai-playbook?selection=2e7c32de-6ff5-4813-bc26-8be219a73c9d"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3fwj5/glm_planning_a_30billionparameter_model_release/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p3fwj5/glm_planning_a_30billionparameter_model_release/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-22T01:00:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1ozopxx</id>
    <title>AMA Announcement: MiniMax, The Opensource Lab Behind MiniMax-M2 + Gifts to Our Community (Wednesday, 8AM-11AM PST)</title>
    <updated>2025-11-17T18:50:44+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozopxx/ama_announcement_minimax_the_opensource_lab/"&gt; &lt;img alt="AMA Announcement: MiniMax, The Opensource Lab Behind MiniMax-M2 + Gifts to Our Community (Wednesday, 8AM-11AM PST)" src="https://preview.redd.it/e3scgr4j5v1g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ca0bf85cb4cf2a2b7e12e8d99d515186275c15e3" title="AMA Announcement: MiniMax, The Opensource Lab Behind MiniMax-M2 + Gifts to Our Community (Wednesday, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/e3scgr4j5v1g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozopxx/ama_announcement_minimax_the_opensource_lab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ozopxx/ama_announcement_minimax_the_opensource_lab/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-17T18:50:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1b550</id>
    <title>AMA with MiniMax ‚Äî Ask Us Anything!</title>
    <updated>2025-11-19T15:46:38+00:00</updated>
    <author>
      <name>/u/OccasionNo6699</name>
      <uri>https://old.reddit.com/user/OccasionNo6699</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;! We‚Äôre really excited to be here, thanks for having us.&lt;/p&gt; &lt;p&gt;I‚Äôm Skyler (&lt;a href="https://www.reddit.com/user/OccasionNo6699/"&gt;u/OccasionNo6699&lt;/a&gt;), head of engineering at MiniMax, the lab behind:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://x.com/MiniMax__AI/status/1982674798649160175?s=20"&gt;MiniMax-M2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983382728343994414"&gt;Hailuo 2.3&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983661667872600296"&gt;MiniMax Speech 2.6&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983964920493568296"&gt;MiniMax Music 2.0&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Joining me today are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Pengyu Zhao, &lt;a href="https://www.reddit.com/user/Wise_Evidence9973/"&gt;u/Wise_Evidence9973&lt;/a&gt; ‚Äî Head of LLM Research&lt;/li&gt; &lt;li&gt;Jade Cai, &lt;a href="https://www.reddit.com/user/srtng/"&gt;u/srtng&lt;/a&gt; ‚Äî Head of Developer Community&lt;/li&gt; &lt;li&gt;midnight_compile , &lt;a href="/u/Top_Cattle_2098"&gt;u/Top_Cattle_2098&lt;/a&gt; ‚Äî LLM Researcher&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 8AM-11AM PST with our core MiniMax tech team continuing to follow up on questions over the next 48 hours.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OccasionNo6699"&gt; /u/OccasionNo6699 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T15:46:38+00:00</published>
  </entry>
</feed>
