<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-20T07:32:02+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qhk5j9</id>
    <title>nvfp4 on Blackwell: sglang, vllm, trt</title>
    <updated>2026-01-19T23:16:05+00:00</updated>
    <author>
      <name>/u/ARCHLucifer</name>
      <uri>https://old.reddit.com/user/ARCHLucifer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;why architecture of kernels from hardware developer and end users differs slightly ?&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/advpropx/status/2013383198466556394?s=46"&gt;https://x.com/advpropx/status/2013383198466556394?s=46&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ARCHLucifer"&gt; /u/ARCHLucifer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhk5j9/nvfp4_on_blackwell_sglang_vllm_trt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhk5j9/nvfp4_on_blackwell_sglang_vllm_trt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhk5j9/nvfp4_on_blackwell_sglang_vllm_trt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T23:16:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhum8w</id>
    <title>Plano 0.4.3 ⭐️ Filter Chains via MCP and OpenRouter Integration</title>
    <updated>2026-01-20T07:27:59+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhum8w/plano_043_filter_chains_via_mcp_and_openrouter/"&gt; &lt;img alt="Plano 0.4.3 ⭐️ Filter Chains via MCP and OpenRouter Integration" src="https://external-preview.redd.it/WpHGpP5qciVFUuJ3FOmn9kvaI4elTA_8gCSju_kPeaQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1a07da7cde5215a00ac86c8edd7e4aea6a91ecc1" title="Plano 0.4.3 ⭐️ Filter Chains via MCP and OpenRouter Integration" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey peeps - excited to release &lt;a href="https://github.com/katanemo/plano"&gt;Plano&lt;/a&gt; 0.4.3. Two critical updates that I think will be very helpful for developers.&lt;/p&gt; &lt;p&gt;1/Filter Chains&lt;/p&gt; &lt;p&gt;Filter chains are Plano’s way of capturing &lt;strong&gt;reusable workflow steps&lt;/strong&gt; in the data plane, without duplication and coupling logic into application code. A filter chain is an ordered list of &lt;strong&gt;mutations&lt;/strong&gt; that a request flows through before reaching its final destination —such as an agent, an LLM, or a tool backend. Each filter is a network-addressable service/path that can:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Inspect the incoming prompt, metadata, and conversation state.&lt;/li&gt; &lt;li&gt;Mutate or enrich the request (for example, rewrite queries or build context).&lt;/li&gt; &lt;li&gt;Short-circuit the flow and return a response early (for example, block a request on a compliance failure).&lt;/li&gt; &lt;li&gt;Emit structured logs and traces so you can debug and continuously improve your agents.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;In other words, filter chains provide a lightweight programming model over HTTP for building reusable steps in your agent architectures.&lt;/p&gt; &lt;p&gt;2/ Passthrough Client Bearer Auth&lt;/p&gt; &lt;p&gt;When deploying Plano in front of LLM proxy services that manage their own API key validation (such as LiteLLM, OpenRouter, or custom gateways), users currently have to configure a static access_key. However, in many cases, it's desirable to forward the client's original Authorization header instead. This allows the upstream service to handle per-user authentication, rate limiting, and virtual keys.&lt;/p&gt; &lt;p&gt;0.4.3 introduces a passthrough_auth option iWhen set to true, Plano will forward the client's Authorization header to the upstream instead of using the configured access_key.&lt;/p&gt; &lt;p&gt;Use Cases:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;OpenRouter: Forward requests to OpenRouter with per-user API keys.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Multi-tenant Deployments: Allow different clients to use their own credentials via Plano.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Hope you all enjoy these updates&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/katanemo/plano"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhum8w/plano_043_filter_chains_via_mcp_and_openrouter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhum8w/plano_043_filter_chains_via_mcp_and_openrouter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T07:27:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhnl7d</id>
    <title>Project HYDRA- A local LLM distributed computing project</title>
    <updated>2026-01-20T01:41:22+00:00</updated>
    <author>
      <name>/u/Fear_ltself</name>
      <uri>https://old.reddit.com/user/Fear_ltself</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhnl7d/project_hydra_a_local_llm_distributed_computing/"&gt; &lt;img alt="Project HYDRA- A local LLM distributed computing project" src="https://external-preview.redd.it/bzN4eGpsMjhzZWVnMSGdfUtWVuAwoPkxxwKeUbXwwzyN2ch9VQo4HJqp54qC.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=34410c5d6e5a948a348cef44a18299fb48851053" title="Project HYDRA- A local LLM distributed computing project" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I have an 18Gb MacBook Pro that’s great at Whisper (MLX, unified memory, blazing fast CPU) , but it isn’t as fast at image generation like my Asus Zephyrus with NVIDIA RTX 4070. I discovered BOINC a couple months ago and it sparked my interest in the idea of distributed computing, and recently I began running into issues running the best model available with the image generation since each takes up too much RAM. So my solution was to split the workload, instead of my previous version sending image creation requests to a self hosted server, it finds a server on the local network hosted by Asus to the local network (WiFi). Larger models in each device, running what they’re best at… &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fear_ltself"&gt; /u/Fear_ltself &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/cgrcqh78seeg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhnl7d/project_hydra_a_local_llm_distributed_computing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhnl7d/project_hydra_a_local_llm_distributed_computing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T01:41:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhinqq</id>
    <title>Best MoE models for 64gb RAM &amp; CPU inference?</title>
    <updated>2026-01-19T22:18:01+00:00</updated>
    <author>
      <name>/u/GamerFromGamerTown</name>
      <uri>https://old.reddit.com/user/GamerFromGamerTown</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello! I've been looking around around for good ~A3B models that can run well on my hardware, but this space seems to be pretty saturated with options; among these, &lt;a href="https://huggingface.co/zai-org/GLM-4.7-Flash"&gt;GLM-4.7-Flash&lt;/a&gt;, &lt;a href="https://huggingface.co/nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16"&gt;NVIDIA-Nemotron-3-Nano-30B-A3B&lt;/a&gt;, &lt;a href="https://huggingface.co/openai/gpt-oss-20b"&gt;gpt-oss-20b&lt;/a&gt;, &lt;a href="https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct"&gt;Qwen3-Coder-30B-A3B-Instruct&lt;/a&gt; , &lt;a href="https://huggingface.co/Qwen/Qwen3-30B-A3B"&gt;Qwen3-30B-A3B&lt;/a&gt;, and &lt;a href="https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Instruct"&gt;Qwen3-Next-80B-A3B-Instruct&lt;/a&gt; seem to be the most popular choices, though I might be missing one or two! With them not really sharing many benchmarks, it can be a bit difficult to compare them; Nemotron-A3B and gpt-oss 20b seem to be pretty popular with the people around here, but GLM-4.7 flash just released, which people seem to feel pretty positively about.&lt;/p&gt; &lt;p&gt;I'll just be doing some coding help, math, and maybe some online/offline RAG. If you have other use cases though, feel free to share!&lt;/p&gt; &lt;p&gt;Given my mediocre Alaskan internet, it would be impossible to download them all to try them out, so anyone with experience trying some of these would be greatly appreciated. Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GamerFromGamerTown"&gt; /u/GamerFromGamerTown &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhinqq/best_moe_models_for_64gb_ram_cpu_inference/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhinqq/best_moe_models_for_64gb_ram_cpu_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhinqq/best_moe_models_for_64gb_ram_cpu_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T22:18:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhpna6</id>
    <title>What Local Models work well with Claude Code?</title>
    <updated>2026-01-20T03:13:31+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The ~20k system prompt seems to overwhelm my usual agentic go-to's (Qwen3-Next-80B and Gpt-OSS-120B) on relatively simple tasks.&lt;/p&gt; &lt;p&gt;GLM 4.6v works okay but is too slow and can enter far too long, sometimes infinite reasoning loops.&lt;/p&gt; &lt;p&gt;Qwen3-235B-2507 works well but is too slow on my machines.&lt;/p&gt; &lt;p&gt;Any suggestions? 48GB VRAM and 64GB system memory&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhpna6/what_local_models_work_well_with_claude_code/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhpna6/what_local_models_work_well_with_claude_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhpna6/what_local_models_work_well_with_claude_code/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T03:13:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1qh442y</id>
    <title>Models that run in 72GB VRAM with context loaded in GPU (3x3090 benchmark test)</title>
    <updated>2026-01-19T13:27:32+00:00</updated>
    <author>
      <name>/u/liviuberechet</name>
      <uri>https://old.reddit.com/user/liviuberechet</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh442y/models_that_run_in_72gb_vram_with_context_loaded/"&gt; &lt;img alt="Models that run in 72GB VRAM with context loaded in GPU (3x3090 benchmark test)" src="https://preview.redd.it/85cs39k6daeg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=72f0ad403efa3ee18868a0b8bf289eb713cca04a" title="Models that run in 72GB VRAM with context loaded in GPU (3x3090 benchmark test)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently finished my 3x3090 setup, and thought of sharing my experience.&lt;/p&gt; &lt;p&gt;This is very much a personal observation, with some very basic testing. &lt;/p&gt; &lt;p&gt;The benchmark is by no means precise, however, after checking the numbers, it is very much aligned with &amp;quot;how I feels they perform&amp;quot; after a few days of bouncing between them. All the above are running on CUDA 12 llama.cpp via LM Studio (nothing special). &lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. Large models (&amp;gt; 100 B)&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;All big models run in roughly the same ballpark—about &lt;strong&gt;30 tok/s&lt;/strong&gt; in everyday use. GPT‑OSS‑120 runs a bit faster than the other large models, but the difference is only noticeable on very short answers; you wouldn’t notice it during longer conversations. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. Qwen3‑VL 235 B (TQ1, 1.66‑bit compression)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I was surprised by how usable TQ1_0 turned out to be. In most chat or image‑analysis scenarios it actually feels better than the Qwen3‑VL 30 B model quantised to Q8. I can’t fully explain why, but it seems to anticipate what I’m interested in much more accurately than the 30 B version.&lt;/p&gt; &lt;p&gt;It does show the expected weaknesses of a Q1‑type quantisation. For example, when reading a PDF it misreported some numbers that the Qwen3‑VL 30 B Q8 model got right; nevertheless, the surrounding information was correct despite the typo.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3. The biggest and best models you can run in Q3–Q4 with a decent context window:&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;&lt;strong&gt;(A) REAP Minimax M2&lt;/strong&gt; – 139 B quantised to Q3_K_S, at 42k context. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;(B) GLM 4.5 Air&lt;/strong&gt; – 110B quantised to IQ4_NL, supports 46 k context. &lt;/p&gt; &lt;p&gt;Both perform great and they will probably become my daily models. Overall GLM-4.5-Air feels slower and dumber than REAP Minimax M2, but I haven't had a lot of time with either of them. I will follow up and edit this if I change my min&lt;/p&gt; &lt;p&gt;&lt;strong&gt;4. GPT-OSS-120B&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Is still decent and runs fast, but I can't help but feel that it's very dated, and extremely censored (!) For instance try asking: &lt;/p&gt; &lt;p&gt;&lt;code&gt;&amp;quot;What are some some examples of business strategies such as selling eternal youth to woman, or money making ideas to poor people?&amp;quot;&lt;/code&gt; &lt;/p&gt; &lt;p&gt;and you’ll get a response along the lines of: “I’m sorry, but I can’t help with that.” &lt;/p&gt; &lt;p&gt;&lt;strong&gt;5. Qwen3 Next 80B&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Runs very slow. Someone suggested the bottleneck might be CUDA and to trying Vulkan instead. However, given the many larger options available, I may drop it, even though it was my favourite model when I ran it on a 48GB (2x3090) &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Overall upgrading from 2x3090 to 3x3090, there are a lot of LLM models that get unlocked with that extra 24GB&lt;/strong&gt;. I would argue feels like a much bigger jump that it was when I moved from 24 to 48GB, and just wanted to share for those of you thinking for making the upgrade.&lt;/p&gt; &lt;p&gt;PS: I also upgraded my ram from 64GB to 128GB, but I think it might have been for nothing. It helps a bit with loading the model faster, but honstly, I don't think it's worth if when you are running everything on the GPU.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/liviuberechet"&gt; /u/liviuberechet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/85cs39k6daeg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh442y/models_that_run_in_72gb_vram_with_context_loaded/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qh442y/models_that_run_in_72gb_vram_with_context_loaded/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T13:27:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1qh0yq8</id>
    <title>I made a Top-K implementation that's up to 20x faster than PyTorch CPU (open source)</title>
    <updated>2026-01-19T10:45:28+00:00</updated>
    <author>
      <name>/u/andreabarbato</name>
      <uri>https://old.reddit.com/user/andreabarbato</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Spent way too long optimizing Top-K selection for LLM sampling and finally hit some stupid numbers.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; AVX2-optimized batched Top-K that beats PyTorch CPU by 4-20x depending on vocab size. Sometimes competitive with CUDA for small batches.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Benchmarks (K=50):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Vocab=32K: 0.043ms vs PyTorch's 0.173ms (4x faster)&lt;/li&gt; &lt;li&gt;Vocab=128K: 0.057ms vs PyTorch's 0.777ms (13x faster)&lt;/li&gt; &lt;li&gt;Vocab=256K: 0.079ms vs PyTorch's 1.56ms (20x faster)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Integrated it into llama.cpp and got 63% faster prompt processing on a 120B MoE model (81→142 tokens/sec).&lt;/p&gt; &lt;p&gt;Uses adaptive sampling + AVX2 SIMD + cache-optimized scanning. Has fast paths for sorted/constant inputs. Single-pass algorithm, no GPU needed.&lt;/p&gt; &lt;p&gt;Includes pre-built DLLs and llama.cpp implementation (for windows).&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/RAZZULLIX/fast_topk_batched"&gt;https://github.com/RAZZULLIX/fast_topk_batched&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love feedback or roasting, whichever you prefer.&lt;/p&gt; &lt;p&gt;EDIT:&lt;/p&gt; &lt;p&gt;can anyone try it and let me know if it works for them? thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/andreabarbato"&gt; /u/andreabarbato &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh0yq8/i_made_a_topk_implementation_thats_up_to_20x/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh0yq8/i_made_a_topk_implementation_thats_up_to_20x/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qh0yq8/i_made_a_topk_implementation_thats_up_to_20x/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T10:45:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhrdia</id>
    <title>Last Week in Multimodal AI - Local Edition</title>
    <updated>2026-01-20T04:34:33+00:00</updated>
    <author>
      <name>/u/Vast_Yak_4147</name>
      <uri>https://old.reddit.com/user/Vast_Yak_4147</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhrdia/last_week_in_multimodal_ai_local_edition/"&gt; &lt;img alt="Last Week in Multimodal AI - Local Edition" src="https://b.thumbs.redditmedia.com/FluQr8-T2xSzepE7O56z1wr9rHYSGXZbAvZ94jKYf2w.jpg" title="Last Week in Multimodal AI - Local Edition" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I curate a weekly multimodal AI roundup, here are the local/open-source highlights from last week:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;FLUX.2 [klein] - Consumer GPU Image Generation&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Runs on consumer GPUs (13GB VRAM), generates high-quality images in under a second.&lt;/li&gt; &lt;li&gt;Handles text-to-image, editing, and multi-reference generation in one model.&lt;/li&gt; &lt;li&gt;&lt;a href="https://bfl.ai/blog/flux2-klein-towards-interactive-visual-intelligence"&gt;Blog&lt;/a&gt; | &lt;a href="https://bfl.ai/models/flux-2-klein#try-demo"&gt;Demo&lt;/a&gt; | &lt;a href="https://huggingface.co/collections/black-forest-labs/flux2"&gt;Models&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://i.redd.it/7vq4pfm0nfeg1.gif"&gt;https://i.redd.it/7vq4pfm0nfeg1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Pocket TTS - Lightweight Text-to-Speech&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Lightweight, CPU-friendly open text-to-speech application.&lt;/li&gt; &lt;li&gt;Local speech synthesis without proprietary services.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/kyutai/pocket-tts"&gt;Hugging Face&lt;/a&gt; | &lt;a href="https://kyutai.org/tts"&gt;Demo&lt;/a&gt; | &lt;a href="https://github.com/kyutai-labs/pocket-tts"&gt;GitHub Repository&lt;/a&gt; | &lt;a href="https://huggingface.co/kyutai/pocket-tts"&gt;Hugging Face Model Card&lt;/a&gt; | &lt;a href="https://arxiv.org/abs/2509.06926"&gt;Paper&lt;/a&gt; | &lt;a href="https://github.com/kyutai-labs/pocket-tts/tree/main/docs"&gt;Documentation&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Ministral 3 - Edge-Ready Multimodal Models&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Compact open models (3B, 8B, 14B) with image understanding for edge devices.&lt;/li&gt; &lt;li&gt;Run multimodal tasks locally without cloud dependencies.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/collections/mistralai/ministral-3"&gt;Hugging Face&lt;/a&gt; | &lt;a href="https://arxiv.org/abs/2601.08584"&gt;Paper&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5fwsc0zymfeg1.png?width=996&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6e5bfafefd5d98665badb3f9eac21886386bf65e"&gt;https://preview.redd.it/5fwsc0zymfeg1.png?width=996&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6e5bfafefd5d98665badb3f9eac21886386bf65e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;STEP3-VL-10B - Efficient Multimodal Intelligence&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;10B parameter model with frontier-level visual perception and reasoning.&lt;/li&gt; &lt;li&gt;Proves you don't need massive models for high-level multimodal intelligence.&lt;/li&gt; &lt;li&gt;h&lt;a href="https://huggingface.co/stepfun-ai/Step3-VL-10B"&gt;ugging Face&lt;/a&gt; | &lt;a href="https://arxiv.org/abs/2601.09668"&gt;Paper&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/uk3qg0z3nfeg1.png?width=1456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=670e4e3902a6a1609db3b135be4801769493ae27"&gt;https://preview.redd.it/uk3qg0z3nfeg1.png?width=1456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=670e4e3902a6a1609db3b135be4801769493ae27&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TranslateGemma - Open Translation Models&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Google's open translation models (4B, 12B, 27B) supporting 55 languages.&lt;/li&gt; &lt;li&gt;Fully open multilingual translation models.&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/GoogleDeepMind/status/2011848249850630363?s=20"&gt;Announcement&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;FASHN Human Parser - Fashion Image Segmentation&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Open fine-tuned SegFormer for parsing humans in fashion images.&lt;/li&gt; &lt;li&gt;Specialized open model for fashion applications.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/fashn-ai/fashn-human-parser"&gt;Hugging Face&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/przknaqrmfeg1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ef36c3976c5e63bd33a68936986ee3f923a8a055"&gt;https://preview.redd.it/przknaqrmfeg1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ef36c3976c5e63bd33a68936986ee3f923a8a055&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;DeepSeek Engram - Memory Module for LLMs&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Lookup-based memory module for faster knowledge retrieval.&lt;/li&gt; &lt;li&gt;Improves efficiency of local LLM deployments.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/deepseek-ai/Engram/tree/main"&gt;GitHub&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;ShowUI-Aloha - GUI Automation Agent&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Flow-based model that learns to use GUIs from human demonstrations.&lt;/li&gt; &lt;li&gt;Generates smooth mouse movements and clicks for workflow automation.&lt;/li&gt; &lt;li&gt;&lt;a href="https://showlab.github.io/Aloha_Page/"&gt;Project Page&lt;/a&gt; | &lt;a href="https://github.com/showlab/ShowUI-Aloha"&gt;GitHub&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1qhrdia/video/ewq89rktmfeg1/player"&gt;https://reddit.com/link/1qhrdia/video/ewq89rktmfeg1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Real-Qwen-Image-V2 - Peak Realism Image Model&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Community fine-tuned Qwen-Image model built for photorealism.&lt;/li&gt; &lt;li&gt;Open alternative for realistic image generation.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/wikeeyang/Real-Qwen-Image-V2"&gt;Model&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fty6rpiumfeg1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ad94c0cd39fe6a97c018bbe3f31f0ec6717ee830"&gt;https://preview.redd.it/fty6rpiumfeg1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ad94c0cd39fe6a97c018bbe3f31f0ec6717ee830&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Checkout the &lt;a href="https://open.substack.com/pub/thelivingedge/p/last-week-in-multimodal-ai-41-vision?utm_campaign=post-expanded-share&amp;amp;utm_medium=web"&gt;full roundup&lt;/a&gt; for more demos, papers, and resources.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/submit/?source_id=t3_1qbala2"&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vast_Yak_4147"&gt; /u/Vast_Yak_4147 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhrdia/last_week_in_multimodal_ai_local_edition/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhrdia/last_week_in_multimodal_ai_local_edition/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhrdia/last_week_in_multimodal_ai_local_edition/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T04:34:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhkh2z</id>
    <title>I FP8 quantized GLM 4.7 Flash!</title>
    <updated>2026-01-19T23:28:43+00:00</updated>
    <author>
      <name>/u/k_means_clusterfuck</name>
      <uri>https://old.reddit.com/user/k_means_clusterfuck</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, I know it ain't much, I finally decided to try and be the first out to fp8 quant a newly dropped model. I would love to hear feedback if you try it. Steps to get it running are in the README :) &lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/marksverdhei/GLM-4.7-Flash-FP8"&gt;https://huggingface.co/marksverdhei/GLM-4.7-Flash-FP8&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/k_means_clusterfuck"&gt; /u/k_means_clusterfuck &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhkh2z/i_fp8_quantized_glm_47_flash/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhkh2z/i_fp8_quantized_glm_47_flash/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhkh2z/i_fp8_quantized_glm_47_flash/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T23:28:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhgi10</id>
    <title>lightonai/LightOnOCR-2-1B · Hugging Face</title>
    <updated>2026-01-19T20:57:11+00:00</updated>
    <author>
      <name>/u/SarcasticBaka</name>
      <uri>https://old.reddit.com/user/SarcasticBaka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhgi10/lightonailightonocr21b_hugging_face/"&gt; &lt;img alt="lightonai/LightOnOCR-2-1B · Hugging Face" src="https://external-preview.redd.it/owrWH9MOuE15-iASn4iPzZcG9U3KIDtVJ9SmxpvC1c0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d891c173f79ddf24b05c65d408e9287701ba72c2" title="lightonai/LightOnOCR-2-1B · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SarcasticBaka"&gt; /u/SarcasticBaka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/lightonai/LightOnOCR-2-1B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhgi10/lightonailightonocr21b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhgi10/lightonailightonocr21b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T20:57:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhu73e</id>
    <title>Some helpful settings to run GLM 4.7 Flash mostly successfully</title>
    <updated>2026-01-20T07:03:01+00:00</updated>
    <author>
      <name>/u/mr_zerolith</name>
      <uri>https://old.reddit.com/user/mr_zerolith</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I popped on the unsloth discord and Daniel helped me with the terrible schizophrenic output i was getting from all the unsloth quants. thinking and output seemed to be constantly getting mixed and it was going in loops.. just garbage!&lt;/p&gt; &lt;p&gt;The magic fix?&lt;br /&gt; &lt;code&gt;--temp 0.2 --top-k 50 --top-p 0.95 --min-p 0.01 --dry-multiplier 1.1&lt;/code&gt;&lt;/p&gt; &lt;p&gt;This made an enormous difference, and the output quality is high except for occasional instances of it doing the thinking then not outputting the message ( the message is in the thinking tag though ). So it still needs some work, but it is useable for chatting with now.&lt;/p&gt; &lt;p&gt;Also while llama.cpp doesn't support this model's flash attention as of writing, make sure to turn flash attention off or you will get hit with a big performance penalty with this model&lt;/p&gt; &lt;p&gt;OK, performance wise? i have a lightly overclocked 5090 and i see 160 tokens/sec on the first query with unsloth's second revision of Q6_K quant using lmstudio.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mr_zerolith"&gt; /u/mr_zerolith &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhu73e/some_helpful_settings_to_run_glm_47_flash_mostly/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhu73e/some_helpful_settings_to_run_glm_47_flash_mostly/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhu73e/some_helpful_settings_to_run_glm_47_flash_mostly/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T07:03:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhqrl7</id>
    <title>DeepSeek V3.2 (open weights) beats GPT-5.2-Codex and Claude Opus on production code challenge — The Multivac daily blind peer eval</title>
    <updated>2026-01-20T04:05:23+00:00</updated>
    <author>
      <name>/u/Silver_Raspberry_811</name>
      <uri>https://old.reddit.com/user/Silver_Raspberry_811</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; DeepSeek V3.2 scored 9.39 to beat GPT-5.2-Codex (9.20) and every other closed model on a complex coding task. But the real story is Claude Sonnet 4.5 got scored anywhere from 3.95 to 8.80 by different judges — same exact code.&lt;/p&gt; &lt;h1&gt;The Test&lt;/h1&gt; &lt;p&gt;We asked 10 models to write a production-grade nested JSON parser with:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Path syntax (&amp;quot;user.profile.settings.theme&amp;quot;)&lt;/li&gt; &lt;li&gt;Array indexing (&amp;quot;users[0].name&amp;quot;)&lt;/li&gt; &lt;li&gt;Circular reference detection&lt;/li&gt; &lt;li&gt;Typed results with error messages&lt;/li&gt; &lt;li&gt;Full type hints and docstrings&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This is a real-world task. Every backend engineer has written something like this.&lt;/p&gt; &lt;h1&gt;Results&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Rank&lt;/th&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Score&lt;/th&gt; &lt;th align="left"&gt;Std Dev&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;DeepSeek V3.2&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;9.39&lt;/td&gt; &lt;td align="left"&gt;0.80&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;GPT-5.2-Codex&lt;/td&gt; &lt;td align="left"&gt;9.20&lt;/td&gt; &lt;td align="left"&gt;0.50&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;3&lt;/td&gt; &lt;td align="left"&gt;Grok 3&lt;/td&gt; &lt;td align="left"&gt;8.89&lt;/td&gt; &lt;td align="left"&gt;0.76&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;4&lt;/td&gt; &lt;td align="left"&gt;Grok Code Fast 1&lt;/td&gt; &lt;td align="left"&gt;8.46&lt;/td&gt; &lt;td align="left"&gt;1.10&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;5&lt;/td&gt; &lt;td align="left"&gt;Gemini 3 Flash&lt;/td&gt; &lt;td align="left"&gt;8.16&lt;/td&gt; &lt;td align="left"&gt;0.71&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;6&lt;/td&gt; &lt;td align="left"&gt;Claude Opus 4.5&lt;/td&gt; &lt;td align="left"&gt;7.57&lt;/td&gt; &lt;td align="left"&gt;1.56&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;7&lt;/td&gt; &lt;td align="left"&gt;Claude Sonnet 4.5&lt;/td&gt; &lt;td align="left"&gt;7.02&lt;/td&gt; &lt;td align="left"&gt;2.03&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;8&lt;/td&gt; &lt;td align="left"&gt;Gemini 3 Pro&lt;/td&gt; &lt;td align="left"&gt;4.30&lt;/td&gt; &lt;td align="left"&gt;1.38&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;9&lt;/td&gt; &lt;td align="left"&gt;GLM 4.7&lt;/td&gt; &lt;td align="left"&gt;2.91&lt;/td&gt; &lt;td align="left"&gt;3.61&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;10&lt;/td&gt; &lt;td align="left"&gt;MiniMax M2.1&lt;/td&gt; &lt;td align="left"&gt;0.70&lt;/td&gt; &lt;td align="left"&gt;0.28&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Open weights won.&lt;/strong&gt; DeepSeek V3.2 is fully open.&lt;/p&gt; &lt;h1&gt;The Variance Problem (responding to yesterday's feedback)&lt;/h1&gt; &lt;p&gt;Yesterday &lt;a href="/u/Proud-Claim-485"&gt;u/Proud-Claim-485&lt;/a&gt; critiqued our methodology — said we're measuring &amp;quot;output alignment&amp;quot; not &amp;quot;reasoning alignment.&amp;quot;&lt;/p&gt; &lt;p&gt;Today's data supports this. Look at Claude Sonnet's std dev: &lt;strong&gt;2.03&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;That's a 5-point spread (3.95 to 8.80) on the same response. Judges fundamentally disagreed on what &amp;quot;good&amp;quot; means.&lt;/p&gt; &lt;p&gt;Compare to GPT-5.2-Codex with 0.50 std dev — everyone agreed within ~1 point.&lt;/p&gt; &lt;p&gt;When evaluators disagree this much, the benchmark is under-specified.&lt;/p&gt; &lt;h1&gt;Judge Strictness (meta-analysis)&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Judge&lt;/th&gt; &lt;th align="left"&gt;Avg Score Given&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Claude Opus 4.5&lt;/td&gt; &lt;td align="left"&gt;5.92 (strictest)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Claude Sonnet 4.5&lt;/td&gt; &lt;td align="left"&gt;5.94&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPT-5.2-Codex&lt;/td&gt; &lt;td align="left"&gt;6.07&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;DeepSeek V3.2&lt;/td&gt; &lt;td align="left"&gt;7.88&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Gemini 3 Flash&lt;/td&gt; &lt;td align="left"&gt;9.11 (most lenient)&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Claude models judge harshly but score mid-tier themselves. Interesting pattern.&lt;/p&gt; &lt;h1&gt;What We're Adding (based on your feedback)&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;5 open-weight models for tomorrow:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Llama-3.3-70B-Instruct&lt;/li&gt; &lt;li&gt;Qwen2.5-72B-Instruct&lt;/li&gt; &lt;li&gt;Mistral-Large-2411&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Big-Tiger-Gemma-27B-v3&lt;/strong&gt; (&lt;a href="/u/ttkciar"&gt;u/ttkciar&lt;/a&gt; suggested this — anti-sycophancy finetune)&lt;/li&gt; &lt;li&gt;Phi-4&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;New evaluation dimension:&lt;/strong&gt; We're adding &amp;quot;reasoning justification&amp;quot; scoring — did the model explain its approach, not just produce correct-looking output?&lt;/p&gt; &lt;h1&gt;Methodology&lt;/h1&gt; &lt;p&gt;This is The Multivac — daily 10×10 blind peer matrix:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;10 models respond to same question&lt;/li&gt; &lt;li&gt;Each model judges all 10 responses (100 total judgments)&lt;/li&gt; &lt;li&gt;Models don't know which response came from which model&lt;/li&gt; &lt;li&gt;Rankings from peer consensus, not single evaluator&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Full responses and analysis: &lt;a href="https://open.substack.com/pub/themultivac/p/deepseek-v32-wins-the-json-parsing?r=72olj0&amp;amp;utm_campaign=post&amp;amp;utm_medium=web&amp;amp;showWelcomeOnShare=true"&gt;https://open.substack.com/pub/themultivac/p/deepseek-v32-wins-the-json-parsing?r=72olj0&amp;amp;utm_campaign=post&amp;amp;utm_medium=web&amp;amp;showWelcomeOnShare=true&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="http://themultivac.com"&gt;themultivac.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Questions welcome. Roast the methodology. That's how we improve.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Silver_Raspberry_811"&gt; /u/Silver_Raspberry_811 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhqrl7/deepseek_v32_open_weights_beats_gpt52codex_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhqrl7/deepseek_v32_open_weights_beats_gpt52codex_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhqrl7/deepseek_v32_open_weights_beats_gpt52codex_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T04:05:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhml0s</id>
    <title>With DRAM and NAND prices what they are, the DGX Spark almost seems like a bargain now LOL.</title>
    <updated>2026-01-20T00:57:58+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know a lot of the inference-focused crowd (myself included) were let down by the DGX Spark when it was released because of its weak memory bandwidth and high price tag. &lt;/p&gt; &lt;p&gt;Fast forward a few months and the whole consumer PC component market has turned into an absolute shitshow, RAM prices have quadrupled, now M2 prices are doing the same. That being said, if you break down the current retail market cost of the hardware components thar make up the DGX Spark, it’s sadly turned into a decent value from a solely HW component perspective. &lt;/p&gt; &lt;p&gt;Here’s a break down the core specs of the DGX Spark and what the market prices of the equivalent components would be (pulled these prices from Amazon US today) &lt;/p&gt; &lt;p&gt;- 128 GB of LPDDR5x RAM = $1600 (for 6000 MT/s, the DGX Spark has 8533 MT/s)&lt;/p&gt; &lt;p&gt;- 4TB M2 Gen5 SSD = $895&lt;/p&gt; &lt;p&gt;- 20 core CPU = $300&lt;/p&gt; &lt;p&gt;- Connectx-7 400 GB Nic (which the Spark has built-in = $1,197 &lt;/p&gt; &lt;p&gt;- 5070 GPU (which is what the DGX is said to be equivalent to from a pure GPU compute standpoint) = $639&lt;/p&gt; &lt;p&gt;Total current market prices of equivalent DGX Spark components = $4,631&lt;/p&gt; &lt;p&gt;DGX Spark Current price (4TB model) = $3,999&lt;/p&gt; &lt;p&gt;Estimated cost savings (if you bought a Spark instead of the components) = $632&lt;/p&gt; &lt;p&gt;I did not take into account Motherboard, Case, PSU, cooling, etc. You probably are looking at at least another $300 or more saved by getting the Spark, but I wasn’t really going to count those because the market prices for those components are pretty stable. &lt;/p&gt; &lt;p&gt;Anyways, I’m not advocating buying a Spark or anything like that, I just thought it was interesting that our mindset of what is a good deal vs. what isn’t a good deal is probably going to shift as DRAM and other component market prices get worse. My point is that 6 months ago, DGX Spark was a terrible perceived value proposition, but now in the current HW component market, maybe it’s not so bad. It is still pretty garbage for inference speed though except for some specific NVFP4 models. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhml0s/with_dram_and_nand_prices_what_they_are_the_dgx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhml0s/with_dram_and_nand_prices_what_they_are_the_dgx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhml0s/with_dram_and_nand_prices_what_they_are_the_dgx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T00:57:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhg6rm</id>
    <title>GLM-4.7-FLASH-NVFP4 on huggingface (20.5 GB)</title>
    <updated>2026-01-19T20:45:46+00:00</updated>
    <author>
      <name>/u/DataGOGO</name>
      <uri>https://old.reddit.com/user/DataGOGO</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I published a mixed precision NVFP4 quantized version the new GLM-4.7-FLASH on HF, can any of you can test it and let me know how it goes, I would really appreciate it. &lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/GadflyII/GLM-4.7-Flash-NVFP4"&gt;https://huggingface.co/GadflyII/GLM-4.7-Flash-NVFP4&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DataGOGO"&gt; /u/DataGOGO &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhg6rm/glm47flashnvfp4_on_huggingface_205_gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhg6rm/glm47flashnvfp4_on_huggingface_205_gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhg6rm/glm47flashnvfp4_on_huggingface_205_gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T20:45:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhaq21</id>
    <title>New in llama.cpp: Anthropic Messages API</title>
    <updated>2026-01-19T17:33:24+00:00</updated>
    <author>
      <name>/u/paf1138</name>
      <uri>https://old.reddit.com/user/paf1138</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhaq21/new_in_llamacpp_anthropic_messages_api/"&gt; &lt;img alt="New in llama.cpp: Anthropic Messages API" src="https://external-preview.redd.it/zqasF6xdAR1yVfMl-Ppz2b8-S-Dv35pa4J_UeKummLg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=56eabcfaa752210d59dc7af42f1b2087636a579d" title="New in llama.cpp: Anthropic Messages API" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paf1138"&gt; /u/paf1138 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/blog/ggml-org/anthropic-messages-api-in-llamacpp"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhaq21/new_in_llamacpp_anthropic_messages_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhaq21/new_in_llamacpp_anthropic_messages_api/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T17:33:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhqzsi</id>
    <title>Mosquito - 7.3M parameter tiny knowledge model</title>
    <updated>2026-01-20T04:16:20+00:00</updated>
    <author>
      <name>/u/Lopsided-Repair-3638</name>
      <uri>https://old.reddit.com/user/Lopsided-Repair-3638</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A mosquito brain size model (7.3M params) that can answer surprisingly many general knowledge questions. Demo: &lt;a href="https://huggingface.co/spaces/ag14850/Mosquito-Demo"&gt;https://huggingface.co/spaces/ag14850/Mosquito-Demo&lt;/a&gt; Model: &lt;a href="https://huggingface.co/ag14850/Mosquito"&gt;https://huggingface.co/ag14850/Mosquito&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lopsided-Repair-3638"&gt; /u/Lopsided-Repair-3638 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhqzsi/mosquito_73m_parameter_tiny_knowledge_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhqzsi/mosquito_73m_parameter_tiny_knowledge_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhqzsi/mosquito_73m_parameter_tiny_knowledge_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T04:16:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhjhlh</id>
    <title>GLM-4.7-Flash-GGUF is here!</title>
    <updated>2026-01-19T22:49:59+00:00</updated>
    <author>
      <name>/u/KvAk_AKPlaysYT</name>
      <uri>https://old.reddit.com/user/KvAk_AKPlaysYT</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhjhlh/glm47flashgguf_is_here/"&gt; &lt;img alt="GLM-4.7-Flash-GGUF is here!" src="https://external-preview.redd.it/xaz8me0jAeBOkTb7mKUXdYdIdr8aoSsiwENwulyOJmI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6f21f70be7ae2e1b3f10f33471dbfc4c47ba6518" title="GLM-4.7-Flash-GGUF is here!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KvAk_AKPlaysYT"&gt; /u/KvAk_AKPlaysYT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/AaryanK/GLM-4.7-Flash-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhjhlh/glm47flashgguf_is_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhjhlh/glm47flashgguf_is_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T22:49:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhs1a7</id>
    <title>EQ-Bench results for GLM-4.7 and GLM-4.7-Flash</title>
    <updated>2026-01-20T05:06:18+00:00</updated>
    <author>
      <name>/u/_sqrkl</name>
      <uri>https://old.reddit.com/user/_sqrkl</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhs1a7/eqbench_results_for_glm47_and_glm47flash/"&gt; &lt;img alt="EQ-Bench results for GLM-4.7 and GLM-4.7-Flash" src="https://b.thumbs.redditmedia.com/ehzR1Yiw-kguKLUI1InoKcUbpAwm6Djb1XRK5uwkDoE.jpg" title="EQ-Bench results for GLM-4.7 and GLM-4.7-Flash" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Low active param models always struggle to compete on these evals. The fact that Flash is beating the likes of Gemma3-27B, Qwen3-235B &amp;amp; gpt-oss-120b is incredibly impressive.&lt;/p&gt; &lt;p&gt;I'm most excited about GLM-4.7-Flash's Judgemark score, because it means it can be a fast local judge in data generation/annotation/RL pipelines.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_sqrkl"&gt; /u/_sqrkl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qhs1a7"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhs1a7/eqbench_results_for_glm47_and_glm47flash/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhs1a7/eqbench_results_for_glm47_and_glm47flash/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T05:06:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1qh5wdq</id>
    <title>zai-org/GLM-4.7-Flash · Hugging Face</title>
    <updated>2026-01-19T14:40:27+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh5wdq/zaiorgglm47flash_hugging_face/"&gt; &lt;img alt="zai-org/GLM-4.7-Flash · Hugging Face" src="https://external-preview.redd.it/Qs0t4y5eLm-uwORWdP6T0dcwW2T6VJyQFBUSY70CTF8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8700f4a43fe16a1031ccda94b517fd709573a5c3" title="zai-org/GLM-4.7-Flash · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/zai-org/GLM-4.7-Flash"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh5wdq/zaiorgglm47flash_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qh5wdq/zaiorgglm47flash_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T14:40:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhpima</id>
    <title>Bartowski comes through again. GLM 4.7 flash GGUF</title>
    <updated>2026-01-20T03:07:33+00:00</updated>
    <author>
      <name>/u/RenewAi</name>
      <uri>https://old.reddit.com/user/RenewAi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/bartowski/zai-org_GLM-4.7-Flash-GGUF"&gt;https://huggingface.co/bartowski/zai-org_GLM-4.7-Flash-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RenewAi"&gt; /u/RenewAi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhpima/bartowski_comes_through_again_glm_47_flash_gguf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhpima/bartowski_comes_through_again_glm_47_flash_gguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhpima/bartowski_comes_through_again_glm_47_flash_gguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T03:07:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhs2sd</id>
    <title>It's been one year since the release of Deepseek-R1</title>
    <updated>2026-01-20T05:08:29+00:00</updated>
    <author>
      <name>/u/Recoil42</name>
      <uri>https://old.reddit.com/user/Recoil42</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhs2sd/its_been_one_year_since_the_release_of_deepseekr1/"&gt; &lt;img alt="It's been one year since the release of Deepseek-R1" src="https://preview.redd.it/cin706z9tfeg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=65fbe53bfb15712186113b0e795fc46c050d0d13" title="It's been one year since the release of Deepseek-R1" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Recoil42"&gt; /u/Recoil42 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/cin706z9tfeg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhs2sd/its_been_one_year_since_the_release_of_deepseekr1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhs2sd/its_been_one_year_since_the_release_of_deepseekr1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T05:08:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhlnsv</id>
    <title>Unsloth GLM 4.7-Flash GGUF</title>
    <updated>2026-01-20T00:17:58+00:00</updated>
    <author>
      <name>/u/Wooden-Deer-1276</name>
      <uri>https://old.reddit.com/user/Wooden-Deer-1276</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF"&gt;https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wooden-Deer-1276"&gt; /u/Wooden-Deer-1276 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhlnsv/unsloth_glm_47flash_gguf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhlnsv/unsloth_glm_47flash_gguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhlnsv/unsloth_glm_47flash_gguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T00:17:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhii5v</id>
    <title>My gpu poor comrades, GLM 4.7 Flash is your local agent</title>
    <updated>2026-01-19T22:12:06+00:00</updated>
    <author>
      <name>/u/__Maximum__</name>
      <uri>https://old.reddit.com/user/__Maximum__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried many MoE models at 30B or under and all of them failed sooner or later in an agentic framework. If z.ai is not redirecting my requests to another model, then GLM 4.7 Flash is finally the reliable (soon local) agent that I desperately wanted.&lt;/p&gt; &lt;p&gt;I am running it since more than half an hour on opencode and it produced hundreds of thousands tokens in one session (with context compacting obviously) without any tool calling errors. It clones github repos, it runs all kind of commands, edits files, commits changes, all perfect, not a single error yet.&lt;/p&gt; &lt;p&gt;Can't wait for GGUFs to try this locally.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/__Maximum__"&gt; /u/__Maximum__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhii5v/my_gpu_poor_comrades_glm_47_flash_is_your_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhii5v/my_gpu_poor_comrades_glm_47_flash_is_your_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhii5v/my_gpu_poor_comrades_glm_47_flash_is_your_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T22:12:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhitrj</id>
    <title>GLM 4.7 Flash official support merged in llama.cpp</title>
    <updated>2026-01-19T22:24:24+00:00</updated>
    <author>
      <name>/u/ayylmaonade</name>
      <uri>https://old.reddit.com/user/ayylmaonade</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhitrj/glm_47_flash_official_support_merged_in_llamacpp/"&gt; &lt;img alt="GLM 4.7 Flash official support merged in llama.cpp" src="https://external-preview.redd.it/AVP8Isc32PMjAyVGtAipaav3x8aU8JY8Lx1bZ_yPak0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=43081fb39d8cfd3c8faeeb3516b7513654ed8fce" title="GLM 4.7 Flash official support merged in llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ayylmaonade"&gt; /u/ayylmaonade &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/18936"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhitrj/glm_47_flash_official_support_merged_in_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhitrj/glm_47_flash_official_support_merged_in_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T22:24:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
