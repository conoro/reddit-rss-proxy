<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-02T18:54:07+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1lq3tuu</id>
    <title>Day 8/50: Building a Small Language Model from Scratch â€“ Rotary Positional Embeddings (RoPE)</title>
    <updated>2025-07-02T18:43:23+00:00</updated>
    <author>
      <name>/u/Prashant-Lakhera</name>
      <uri>https://old.reddit.com/user/Prashant-Lakhera</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In the past two days, we explored what positional embeddings are and even coded it.&lt;/p&gt; &lt;p&gt;Today, weâ€™re diving into a more advanced and powerful concept used in many state-of-the-art models: Rotary Positional Embeddings (RoPE).&lt;/p&gt; &lt;h1&gt;Recap: Why Transformers Need Positional Embeddings&lt;/h1&gt; &lt;p&gt;Transformers process tokens in parallel, which makes them efficient, but it also means they donâ€™t inherently know the order of the tokens.&lt;/p&gt; &lt;p&gt;To a transformer, these sentences look identical:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&amp;quot;The cat sat on the mat.&amp;quot;&lt;/li&gt; &lt;li&gt;&amp;quot;The mat sat on the cat.&amp;quot;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thatâ€™s a problem. Order matters, especially in language.&lt;/p&gt; &lt;p&gt;To fix this, we add &lt;em&gt;positional embeddings&lt;/em&gt; to inform the model about token positions.&lt;/p&gt; &lt;h1&gt;Traditional Positional Embeddings&lt;/h1&gt; &lt;p&gt;Two popular approaches:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Learned positional embeddings&lt;/strong&gt; â€“ Each position (1, 2, 3...) gets a trainable vector.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Sinusoidal embeddings&lt;/strong&gt; â€“ Use sin/cos functions to generate fixed vectors per position.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;But they have limitations:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Fixed or learned per-position (no flexibility)&lt;/li&gt; &lt;li&gt;Poor generalization to longer sequences&lt;/li&gt; &lt;li&gt;Don't integrate naturally with attention scores&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;What Is RoPE and Why Is It Better?&lt;/h1&gt; &lt;p&gt;RoPE was introduced in RoFormer (Su et al., 2021) and is now used in models like LLaMA and DeepSeek.&lt;/p&gt; &lt;p&gt;Instead of adding a position vector, RoPE rotates token embeddings in space based on their position, directly inside the attention mechanism (on query and key vectors).&lt;/p&gt; &lt;p&gt;This encodes relative position information in a more elegant and flexible way.&lt;/p&gt; &lt;p&gt;For each position, the token embedding is rotated by an angle proportional to that position.&lt;/p&gt; &lt;p&gt;A simplified pseudocode:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;for i in range(0, dim, 2): x1, x2 = x[i], x[i+1] angle = theta * position x[i] = x1 * cos(angle) - x2 * sin(angle) x[i+1] = x1 * sin(angle) + x2 * cos(angle) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This allows attention to naturally reflect &lt;em&gt;how far apart&lt;/em&gt; two tokens are, something traditional embeddings canâ€™t do.&lt;/p&gt; &lt;h1&gt;RoPE vs Traditional Positional Embeddings&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Feature&lt;/th&gt; &lt;th align="left"&gt;Traditional Embeddings&lt;/th&gt; &lt;th align="left"&gt;Rotary Positional Embeddings (RoPE)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Position Injected&lt;/td&gt; &lt;td align="left"&gt;Added to input embeddings&lt;/td&gt; &lt;td align="left"&gt;Applied inside attention mechanism&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Absolute or Relative?&lt;/td&gt; &lt;td align="left"&gt;Absolute&lt;/td&gt; &lt;td align="left"&gt;Relative&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Generalizes to Long Sequences?&lt;/td&gt; &lt;td align="left"&gt;Poor&lt;/td&gt; &lt;td align="left"&gt;Strong&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Learnable Parameters?&lt;/td&gt; &lt;td align="left"&gt;Sometimes (if learned)&lt;/td&gt; &lt;td align="left"&gt;No&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Adopted in SOTA models?&lt;/td&gt; &lt;td align="left"&gt;Less common now&lt;/td&gt; &lt;td align="left"&gt;Yes (LLaMA, DeepSeek)&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Why RoPE Is So Useful&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Encodes relative positions&lt;/strong&gt; directly in attention scores&lt;/li&gt; &lt;li&gt;&lt;strong&gt;No extra parameters&lt;/strong&gt; â€“ it's deterministic&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Handles long sequences&lt;/strong&gt; more gracefully&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Simple implementation&lt;/strong&gt; using trigonometric rotation&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Use in Real Models&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;LLaMA (Meta):&lt;/strong&gt; Uses RoPE for better generalization and long-context performance.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;DeepSeek:&lt;/strong&gt; Uses a decoupled RoPE mechanism where rotary embeddings are applied to separate query/key heads, enabling efficient long-context attention without bloating memory.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Final Thoughts&lt;/h1&gt; &lt;p&gt;Rotary Positional Embeddings are an elegant solution to a core transformer weakness. If youâ€™re building models for long documents, code, or stories, RoPE should be on your radar.&lt;/p&gt; &lt;h1&gt;Coming Up Tomorrow&lt;/h1&gt; &lt;p&gt;We'll implement RoPE in code and walk through how itâ€™s used in the open-source&lt;br /&gt; &lt;a href="https://github.com/ideaweaver-ai/DeepSeek-Children-Stories-15M-model"&gt;DeepSeek-Children-Stories-15M model&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Follow along, weâ€™re just getting started.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prashant-Lakhera"&gt; /u/Prashant-Lakhera &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lq3tuu/day_850_building_a_small_language_model_from/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lq3tuu/day_850_building_a_small_language_model_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lq3tuu/day_850_building_a_small_language_model_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-02T18:43:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1lq1sdi</id>
    <title>Cursor equivalent or close to alternative fully local?</title>
    <updated>2025-07-02T17:23:12+00:00</updated>
    <author>
      <name>/u/InsideResolve4517</name>
      <uri>https://old.reddit.com/user/InsideResolve4517</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;Cursor equivalent or close to alternative fully local?&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;It's Continue .dev, Void, aider, Zed, AutoGPT, SuperAGI or something else&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InsideResolve4517"&gt; /u/InsideResolve4517 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lq1sdi/cursor_equivalent_or_close_to_alternative_fully/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lq1sdi/cursor_equivalent_or_close_to_alternative_fully/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lq1sdi/cursor_equivalent_or_close_to_alternative_fully/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-02T17:23:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1lp5nhy</id>
    <title>Gemma 3n Fine-tuning now in Unsloth - 1.5x faster with 50% less VRAM + Fixes</title>
    <updated>2025-07-01T16:07:29+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp5nhy/gemma_3n_finetuning_now_in_unsloth_15x_faster/"&gt; &lt;img alt="Gemma 3n Fine-tuning now in Unsloth - 1.5x faster with 50% less VRAM + Fixes" src="https://external-preview.redd.it/Z9xq13sybpzZE8ZKJtOY4SuppgTg5x6rIeOPF_Fl1qk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7593f97dd0c1af68e044aad5a89b7cf7f0e2b642" title="Gemma 3n Fine-tuning now in Unsloth - 1.5x faster with 50% less VRAM + Fixes" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey LocalLlama! We made finetuning Gemma 3N 1.5x faster in a free Colab with &lt;a href="http://github.com/unslothai/unsloth"&gt;Unsloth&lt;/a&gt; in under 16GB of VRAM! We also managed to find and fix issues for Gemma 3N:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Ollama &amp;amp; GGUF fixes&lt;/strong&gt; - All Gemma 3N GGUFs could not load in Ollama properly since &lt;code&gt;per_layer_token_embd&lt;/code&gt; had loading issues. Use our quants in Ollama for our fixes. All dynamic quants in our &lt;a href="https://huggingface.co/collections/unsloth/gemma-3n-685d3874830e49e1c93f9339"&gt;Gemma 3N collection&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;NaN and infinities in float16 GPUs&lt;/strong&gt; - we found Conv2D weights (the vision part) have very large magnitudes - we upcast them to float32 to remove infinities.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/v2w9vippbaaf1.jpg?width=1888&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=9c617026ca9deecc699787547badded628f081bc"&gt;Green crosses are large Conv2D weights&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Free Colab to fine-tune Gemma 3N 4B&lt;/strong&gt; in a free Colab + audio + text + vision inference: &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3N_(4B"&gt;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3N_(4B)-Conversational.ipynb&lt;/a&gt;-Conversational.ipynb)&lt;/p&gt; &lt;p&gt;Update Unsloth via &lt;code&gt;pip install --upgrade unsloth unsloth_zoo&lt;/code&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from unsloth import FastModel import torch model, tokenizer = FastModel.from_pretrained( model_name = &amp;quot;unsloth/gemma-3n-E4B-it&amp;quot;, max_seq_length = 1024, load_in_4bit = True, full_finetuning = False, ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Detailed technical analysis&lt;/strong&gt; and guide on how to use Gemma 3N effectively: &lt;a href="https://docs.unsloth.ai/basics/gemma-3n"&gt;https://docs.unsloth.ai/basics/gemma-3n&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We also uploaded GGUFs for the new FLUX model: &lt;a href="https://huggingface.co/unsloth/FLUX.1-Kontext-dev-GGUF"&gt;https://huggingface.co/unsloth/FLUX.1-Kontext-dev-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp5nhy/gemma_3n_finetuning_now_in_unsloth_15x_faster/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp5nhy/gemma_3n_finetuning_now_in_unsloth_15x_faster/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lp5nhy/gemma_3n_finetuning_now_in_unsloth_15x_faster/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T16:07:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1lpu8a9</id>
    <title>AI Agents, But Simple and Understandable</title>
    <updated>2025-07-02T12:12:25+00:00</updated>
    <author>
      <name>/u/thesmallstar</name>
      <uri>https://old.reddit.com/user/thesmallstar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpu8a9/ai_agents_but_simple_and_understandable/"&gt; &lt;img alt="AI Agents, But Simple and Understandable" src="https://external-preview.redd.it/NvDs7wJwV1W_MBsZCzSWrMWhoZ65PkI1ziagX4BmUfc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7da6efc88dc4dd9b17784bb73b12de5401e7bc4e" title="AI Agents, But Simple and Understandable" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Most of what you read about â€œAI agentsâ€ is either super vague or buried in jargon. I wrote a no-BS explainer that breaks down how modern AI agents actually work, without the marketing fluff. If youâ€™re curious about whatâ€™s really happening â€œunder the hoodâ€ when people talk about AI agents (or you want to build one yourself), check out: &lt;a href="https://blog.surkar.in/ai-agents-under-the-hood"&gt;https://blog.surkar.in/ai-agents-under-the-hood&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Happy to chat or answer questions in the comments :D &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thesmallstar"&gt; /u/thesmallstar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://blog.surkar.in/ai-agents-under-the-hood"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpu8a9/ai_agents_but_simple_and_understandable/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lpu8a9/ai_agents_but_simple_and_understandable/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-02T12:12:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1lpmx00</id>
    <title>I built a cli tool to automatically figure out tensor overrides in llama.cpp</title>
    <updated>2025-07-02T04:38:28+00:00</updated>
    <author>
      <name>/u/kevin_1994</name>
      <uri>https://old.reddit.com/user/kevin_1994</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone&lt;/p&gt; &lt;p&gt;Running MoE models on my machine, I'm constantly frustrated working with `--overide-tensor` regexes in llama.cpp. They're hard to maintain, break easily, and are unreadable &lt;/p&gt; &lt;p&gt;I built a little cli tool which builds these `--override-tensor` arguments automatically for your architecture.&lt;/p&gt; &lt;p&gt;On my machine (Xeon e5 2699v3, 128GB DDR4, 2x3090, 1x3060) this runs Qwen3 235B Q4XL at 5.5 tok/s&lt;/p&gt; &lt;pre&gt;&lt;code&gt;#!/bin/bash export CUDA_VISIBLE_DEVICES=2,0,1 # Generate tensor overrides TENSOR_OVERRIDES=$(gguf-tensor-overrider -g https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/UD-Q4_K_XL/Qwen3-235B-A22B-UD-Q4_K_XL-00001-of-00003.gguf -c 32000 --gpu-percentage 0.85) # Build command with tensor overrides CMD=&amp;quot;/home/kevin/llama.cpp/build/bin/llama-cli \ -hf unsloth/Qwen3-235B-A22B-GGUF:Q4_K_XL \ -c 32000 \ -fa \ -sm row \ $TENSOR_OVERRIDES&amp;quot; # Execute command directly (no pipe) eval &amp;quot;$CMD&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Results:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;gt; hey there &amp;lt;think&amp;gt; Okay, the user just said &amp;quot;hey there&amp;quot;. That's pretty casual. I should respond in a friendly and welcoming way. Maybe ask how they're doing and offer help. Let me keep it simple and approachable. I need to make sure the response is open-ended so they feel comfortable to ask anything. Avoid any technical jargon. Just a warm greeting and an offer to assist with whatever they need. Yeah, that should work. &amp;lt;/think&amp;gt; Hello! How can I assist you today? ğŸ˜Š &amp;gt; llama_perf_sampler_print: sampling time = 15.58 ms / 114 runs ( 0.14 ms per token, 7318.01 tokens per second) llama_perf_context_print: load time = 152623.89 ms llama_perf_context_print: prompt eval time = 1918.59 ms / 10 tokens ( 191.86 ms per token, 5.21 tokens per second) llama_perf_context_print: eval time = 18799.44 ms / 103 runs ( 182.52 ms per token, 5.48 tokens per second) llama_perf_context_print: total time = 30823.94 ms / 113 tokens &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;These commands should also work with ik_llama.cpp. 5.5 tok/s is about what I was getting before with ik_llama.cpp.&lt;/p&gt; &lt;p&gt;Here is the link to the repository: &lt;a href="https://github.com/k-koehler/gguf-tensor-overrider/tree/main"&gt;https://github.com/k-koehler/gguf-tensor-overrider&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hopefully some of your find this useful!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kevin_1994"&gt; /u/kevin_1994 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpmx00/i_built_a_cli_tool_to_automatically_figure_out/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpmx00/i_built_a_cli_tool_to_automatically_figure_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lpmx00/i_built_a_cli_tool_to_automatically_figure_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-02T04:38:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1lq2wn6</id>
    <title>How do you pick the right local LLM for your needs?</title>
    <updated>2025-07-02T18:06:52+00:00</updated>
    <author>
      <name>/u/ExtiqX</name>
      <uri>https://old.reddit.com/user/ExtiqX</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys,&lt;/p&gt; &lt;p&gt;Iâ€™m diving into running models locally with Ollama or LMStudio, and there are so many options that I donâ€™t even know where to start, especially before I lock in on a specific project. I want to develop a clear process for figuring out which model might suit me, even if I donâ€™t yet have a narrow use case.&lt;/p&gt; &lt;p&gt;Could you walk me through your thought process? For example: â€¢ How do you survey the landscape of available models and group them into â€œcreative,â€ â€œfactual,â€ or â€œcode-focusedâ€ categories? â€¢ What are the first metrics or specs you check (size, quantization, RAM/VRAM needs, inference speed, training data)? â€¢ How do you run quick, side-by-side tests in Ollama/LMStudio to compare responses on a handful of prompts? â€¢ What mental shortcuts or analogies do you use to decide â€œthis one feels like the right fitâ€ before committing? â€¢ Any go-to scripts, benchmarks, or community resources that help you narrow down from a dozen candidates to your top one or two?&lt;/p&gt; &lt;p&gt;Iâ€™m not a developer or engineer, Iâ€™m coming at this entirely as an end-user who just wants a consumer-friendly way to experiment with local AI. I donâ€™t have deep technical skills or coding experience, so Iâ€™m looking for recommendations and processes explained in plain English rather than programming tutorials.&lt;/p&gt; &lt;p&gt;Hope someone can help and thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ExtiqX"&gt; /u/ExtiqX &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lq2wn6/how_do_you_pick_the_right_local_llm_for_your_needs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lq2wn6/how_do_you_pick_the_right_local_llm_for_your_needs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lq2wn6/how_do_you_pick_the_right_local_llm_for_your_needs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-02T18:06:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1lpejnj</id>
    <title>Qwen3 inference engine in C: simple, educational, fun</title>
    <updated>2025-07-01T21:49:58+00:00</updated>
    <author>
      <name>/u/adrian-cable</name>
      <uri>https://old.reddit.com/user/adrian-cable</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those who may be interested, a free-time project that I've now put up on Github: &lt;a href="https://github.com/adriancable/qwen3.c"&gt;https://github.com/adriancable/qwen3.c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Run Qwen3-architecture models (like Qwen3-4B, or DeepSeek-R1-0528-Qwen3-8B) locally, no GPU required, using an LLM inference engine you build yourself from just 1 file of C source, with no dependencies. Only requirement is enough RAM to load the models. Think llama.cpp but 100X smaller and simpler, although it's still very functional: multi-language input/output, multi-core CPU support, supports reasoning/thinking models etc.&lt;/p&gt; &lt;p&gt;All you need to build and run is Python3 and a C compiler. The C source is so small, it compiles in around a second. Then, go have fun with the models!&lt;/p&gt; &lt;p&gt;After you've played around for a bit, if you already understand a bit about how transformers work but want to really learn the detail, the inference engine's C source (unlike llama.cpp) is small enough to dig into without getting a heart attack. Once you've understood how it ticks, you're a transformers expert! ğŸ˜ƒ&lt;/p&gt; &lt;p&gt;Not intended to compete with 'heavyweight' engines like llama.cpp, rather, the focus is on being (fun)ctional and educational.&lt;/p&gt; &lt;p&gt;MIT license so you can do whatever you want with the source, no restrictions.&lt;/p&gt; &lt;p&gt;Project will be a success if at least one person here enjoys it!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/adrian-cable"&gt; /u/adrian-cable &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpejnj/qwen3_inference_engine_in_c_simple_educational_fun/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpejnj/qwen3_inference_engine_in_c_simple_educational_fun/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lpejnj/qwen3_inference_engine_in_c_simple_educational_fun/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T21:49:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1lpz355</id>
    <title>Cursor terms and conditions seem to be changing</title>
    <updated>2025-07-02T15:38:51+00:00</updated>
    <author>
      <name>/u/Desperate_Rub_1352</name>
      <uri>https://old.reddit.com/user/Desperate_Rub_1352</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpz355/cursor_terms_and_conditions_seem_to_be_changing/"&gt; &lt;img alt="Cursor terms and conditions seem to be changing" src="https://preview.redd.it/74fqbljpdhaf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4b7436040774882ae1855665ecbca193d10edb55" title="Cursor terms and conditions seem to be changing" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I remember when I first downloaded cursor last year, the privacy was on by default, and now not at all. I never selected this embedding thing, but I guess it is automatically turned on. I work in Germany where I do not even dare to use these already, but I am not sure if I can even trust these at all as I worry that the companies will go nuts if they find out about this. Embeddings can be decoded easily, I am literally working on a project where given arbitrary embeddings I am training models to decode stuff to reduce the data storage for some stuff and other use cases.&lt;/p&gt; &lt;p&gt;I am looking for cursor alternatives, as I am not confident that my code snippets will not be used for training or just kept on servers. In hard privacy, I do lose out on many features but on lose ones my embeddings, code snippets etc. will be stored.&lt;/p&gt; &lt;p&gt;All these models and companies are popping up everywhere and they really need your data it feels like? Google is giving away hundreds of calls everyday from their claude code like thing, and cursor which I loved to use is like this now.&lt;/p&gt; &lt;p&gt;Am I being paranoid and trust their SOC-2 ratings, or their statements etc.? Cursor is trustworthy and I should not bother?&lt;/p&gt; &lt;p&gt;OR I should start building my own tool? IMO this is the ultimate data to collect, your literal questions, doubts etc. so I just wanted to know how do people feel here..&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Desperate_Rub_1352"&gt; /u/Desperate_Rub_1352 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/74fqbljpdhaf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpz355/cursor_terms_and_conditions_seem_to_be_changing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lpz355/cursor_terms_and_conditions_seem_to_be_changing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-02T15:38:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1lpquz6</id>
    <title>Open source tech from IBM for Compression of models</title>
    <updated>2025-07-02T08:53:40+00:00</updated>
    <author>
      <name>/u/Affectionate-Hat-536</name>
      <uri>https://old.reddit.com/user/Affectionate-Hat-536</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpquz6/open_source_tech_from_ibm_for_compression_of/"&gt; &lt;img alt="Open source tech from IBM for Compression of models" src="https://external-preview.redd.it/NTNoyueW9-UJqjsAanLnjShv3Q-OdUZiW0Di3m0uACc.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c76f996ad0aa02724516847f86a59dddb6ea317e" title="Open source tech from IBM for Compression of models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Seems interesting, I am not clear if the compression is only for storage, transmission or extend to inference too :) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Affectionate-Hat-536"&gt; /u/Affectionate-Hat-536 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://research.ibm.com/blog/Zip-NN-AI-compression"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpquz6/open_source_tech_from_ibm_for_compression_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lpquz6/open_source_tech_from_ibm_for_compression_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-02T08:53:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1lq2i2m</id>
    <title>Is there a legit code assistant that can run on a m3 ultra 256 or 96gb?</title>
    <updated>2025-07-02T17:51:17+00:00</updated>
    <author>
      <name>/u/tru3relativity</name>
      <uri>https://old.reddit.com/user/tru3relativity</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anything that would work as an agentic code assistant? Trying to decide if itâ€™s worth investing if it means I donâ€™t have to pay for Claude code anymore. I understand it wonâ€™t be near Claude code but thatâ€™s fine. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tru3relativity"&gt; /u/tru3relativity &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lq2i2m/is_there_a_legit_code_assistant_that_can_run_on_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lq2i2m/is_there_a_legit_code_assistant_that_can_run_on_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lq2i2m/is_there_a_legit_code_assistant_that_can_run_on_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-02T17:51:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1lppz8x</id>
    <title>LeCarnet: A French Dataset for Small Language Models</title>
    <updated>2025-07-02T07:51:56+00:00</updated>
    <author>
      <name>/u/Unusual_Shoe2671</name>
      <uri>https://old.reddit.com/user/Unusual_Shoe2671</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lppz8x/lecarnet_a_french_dataset_for_small_language/"&gt; &lt;img alt="LeCarnet: A French Dataset for Small Language Models" src="https://external-preview.redd.it/LbeaoTZsvVnFcs9m3p61Rw_b6plof5ZMIkdYPViyiXk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3a64905c33d69e9dd1313750f8b6efb7d4b7b7c4" title="LeCarnet: A French Dataset for Small Language Models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;I recently built &lt;strong&gt;LeCarnet&lt;/strong&gt;, a dataset of 2 million French short stories generated with Mistral Large, inspired by the TinyStories project. I also trained three LLaMA-based models from scratch on this dataset: &lt;strong&gt;LeCarnet-3M&lt;/strong&gt;, &lt;strong&gt;LeCarnet-8M&lt;/strong&gt;, and &lt;strong&gt;LeCarnet-21M&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;This dataset contains simple stories with a limited vocabulary, making it ideal for training small language models (SLMs) and for educational purposes.&lt;/p&gt; &lt;p&gt;I've shared the &lt;strong&gt;data generation, training, and evaluation scripts&lt;/strong&gt; as well.&lt;br /&gt; I hope this can be useful to others, feel free to use it, and don't hesitate to leave a star if you find it helpful!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/MaxLSB/LeCarnet"&gt;https://github.com/MaxLSB/LeCarnet&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;Models:&lt;/strong&gt; &lt;a href="https://huggingface.co/collections/MaxLSB/lecarnet-683d6b6843023b2c88258594"&gt;https://huggingface.co/collections/MaxLSB/lecarnet-683d6b6843023b2c88258594&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;Dataset:&lt;/strong&gt; &lt;a href="https://huggingface.co/datasets/MaxLSB/LeCarnet"&gt;https://huggingface.co/datasets/MaxLSB/LeCarnet&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Unusual_Shoe2671"&gt; /u/Unusual_Shoe2671 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/MaxLSB/LeCarnet"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lppz8x/lecarnet_a_french_dataset_for_small_language/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lppz8x/lecarnet_a_french_dataset_for_small_language/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-02T07:51:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1lpzvtx</id>
    <title>My experience with 14B LLMs on phones with Snapdragon 8 Elite</title>
    <updated>2025-07-02T16:09:26+00:00</updated>
    <author>
      <name>/u/schizo_poster</name>
      <uri>https://old.reddit.com/user/schizo_poster</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm making this thread because weeks ago when I looked up this information, I could barely even find confirmation that it's possible to run 14B models on phones. In the meantime I got a OnePlus 13 with 16GB of RAM. After tinkering with different models and apps for half a day, I figured I give my feedback for the people who are interested in this specific scenario. &lt;/p&gt; &lt;p&gt;I'm used to running 32B models on my PC and after many (subjective) tests I realized that modern 14B models are not far behind in capabilities, at least for my use-cases. I find 8B models kinda meh (I'm warming up to them lately), but my obsession was to be able to run 14B models on a phone, so here we are. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Points:&lt;/strong&gt;&lt;br /&gt; Qwen3 14B loaded via MNN Chat runs decent, but the performance is not consistent. You can expect anywhere from 4.5-7 tokens per second, but the overall performance is around 5.5t/s. I don't know exactly what quantization this models uses because MNN Chat doesn't say it. My guess, based on the file size, is that it's either Q4_K_S or IQ4. Could also be Q4_K_M but the file seems rather small for that so I have my doubts. &lt;/p&gt; &lt;p&gt;Qwen3 8B runs at around 8 tokens per second, but again I don't know what quantization. Based on the file size, I'm guessing it's Q6_K_M. I was kinda expecting a bit more here, but whatever. 8t/s is around reading/thinking speed for me, so I'm ok with that. &lt;/p&gt; &lt;p&gt;I also used PocketPal to run some abliterated versions of Qwen3 14B at Q4_K_M. Performance was similar to MNN Chat which surprised me since everyone was saying that MNN Chat should provide a significant boost in performance since it's optimized to work with Snapdragon NPUs. Maybe at this model size the VRAM bandwidth is the bottleneck so the performance improvements are not obvious anymore. &lt;/p&gt; &lt;p&gt;Enabling or disabling thinking doesn't seem to affect the speed directly, but it will affect it indirectly. More on that later. &lt;/p&gt; &lt;p&gt;I'm in the process of downloading Qwen3-30B-A3B. By all acounts it should not fit in VRAM, but OnePlus has that virtual memory thing that allows you to expand the RAM by an extra 12GB. It will use the UFS storage obviously. &lt;del&gt;This should put me at 16+12=28GB of RAM which should allow me to load the model.&lt;/del&gt; LE: never mind. The version provided by MNN Chat doesn't load. I think it's meant for phones with 24GB RAM and the extra 12GB swap file doesn't seem to trick it. Will try to load an IQ2 quant via PocketPal and report back. Downloading as we speak. If that one doesn't work, it's gonna have to be IQ1_XSS, but other users have already reported on that, so I'm not gonna do it again. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;IMPORTANT:&lt;/strong&gt;&lt;br /&gt; The performance WILL drop the more you talk and the the more you fill up the context. Both the prompt processing speed as well as the token generation speed will take a hit. At some point you will not be able to continue the conversation, not because the token generation speed drops so much, but because the prompt processing speed is too slow and it takes ages to read the entire context before it responds. The token generation speed drops linearly, but the prompt processing speed seems to drop exponentially. &lt;/p&gt; &lt;p&gt;What that means is that realistically, when you're running a 14B model on your phone, if you enable thinking, you'll be able to ask it about 2 or 3 questions before the prompt processing speed becomes so slow that you'll prefer to start a new chat. With thinking disabled you'll get 4-5 questions before it becomes annoyingly slow. Again, the token generation speed doesn't drop that much. It goes from 5.5t/s to 4.5t/s, so the AI still answers reasonably fast. The problem is that you will wait ages until it starts answering. &lt;/p&gt; &lt;p&gt;PS: phones with 12GB RAM will not be able to run 14B models because Android is a slut for RAM and takes up a lot. 16GB is minimum for 14B, and 24GB is recommended for peace of mind. I got the 16GB version because I just couldn't justify the extra price for the 24GB model and also because it's almost unobtanium and it involved buying it from another country and waiting ages. If you can find a 24GB version for a decent price, go for that. If not, 16GB is also fine. Keep in mind that the issue with the prompt proccessing speed is NOT solved with extra RAM. You'll still only be able to get 2-3 questions in with thinking and 4-5 no_think before it turns into a snail. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/schizo_poster"&gt; /u/schizo_poster &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpzvtx/my_experience_with_14b_llms_on_phones_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpzvtx/my_experience_with_14b_llms_on_phones_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lpzvtx/my_experience_with_14b_llms_on_phones_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-02T16:09:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1lp9gh2</id>
    <title>Huawei releases an open weight model Pangu Pro 72B A16B. Weights are on HF. It should be competitive with Qwen3 32B and it was trained entirely on Huawei Ascend NPUs. (2505.21411)</title>
    <updated>2025-07-01T18:30:51+00:00</updated>
    <author>
      <name>/u/FullOf_Bad_Ideas</name>
      <uri>https://old.reddit.com/user/FullOf_Bad_Ideas</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/"&gt; &lt;img alt="Huawei releases an open weight model Pangu Pro 72B A16B. Weights are on HF. It should be competitive with Qwen3 32B and it was trained entirely on Huawei Ascend NPUs. (2505.21411)" src="https://external-preview.redd.it/KKUaRRu1NZXsmquOk2Id9DRnEhBD6P6w5Y5xZQur5Yc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dd39a4d6488e7f71969bdc8665d7c2dbe902c2b5" title="Huawei releases an open weight model Pangu Pro 72B A16B. Weights are on HF. It should be competitive with Qwen3 32B and it was trained entirely on Huawei Ascend NPUs. (2505.21411)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullOf_Bad_Ideas"&gt; /u/FullOf_Bad_Ideas &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/IntervitensInc/pangu-pro-moe-model"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T18:30:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1lpm6cv</id>
    <title>ERNIE-4.5-VL-28B-A3B is a hidden gem that can decently tackle challenging chinese/japanese OCR problems.</title>
    <updated>2025-07-02T03:57:24+00:00</updated>
    <author>
      <name>/u/mixivivo</name>
      <uri>https://old.reddit.com/user/mixivivo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpm6cv/ernie45vl28ba3b_is_a_hidden_gem_that_can_decently/"&gt; &lt;img alt="ERNIE-4.5-VL-28B-A3B is a hidden gem that can decently tackle challenging chinese/japanese OCR problems." src="https://b.thumbs.redditmedia.com/A5xBWO7s3WJ33IXN1UsaHXSRINAm2j5ql5UQhM4yZSM.jpg" title="ERNIE-4.5-VL-28B-A3B is a hidden gem that can decently tackle challenging chinese/japanese OCR problems." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;å›¾ä¸­æ–‡æœ¬è½¬å½•å¦‚ä¸‹ï¼š&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;å€­ç‹æ­¦ã®ä¸Šè¡¨æ–‡&lt;/p&gt; &lt;p&gt;å€­ãƒ»ä»»é‚£ãƒ»åŠ ç½—ãƒ»ç§¦éŸ©ãƒ»æ…•éŸ©ä¸ƒå›½è¯¸å†›äº‹å®‰ä¸œå¤§å°†å†›ç½—ãƒ»ä»»é‚£ãƒ»åŠ ç½—ãƒ»ç§¦éŸ©ãƒ»æ…•éŸ©ä¸ƒå›½è¯¸å†›äº‹å®‰ä¸œå¤§å°†å†›å€­å›½ç‹ã¨ç§°ã™ã€‚é¡ºå¸ã®æ˜‡æ˜äºŒå¹´â‘ ä½¿é£ã—ã¦ä¸Šè¡¨ã™ã‚‹ã€‚æ˜”ã—ã¦æ›°ãã€å°å›½â‘¡ã¯åé—ã—ã¦è—©ã‚’å¤–ã«ä½œã‚‹ã€‚æ˜”ã‚ˆã‚Šç¥–ç¥¢â‘¢èº¬ç”²èƒ„æ”æ–¡ã€å±±å·ã‚’è·‹æ¶‰ã—ã¦å¯›å¤„â‘£ã«è¿›ã‚ã‚ãšã€è¥¿ã¯è¡†å¤·â‘¥ã‚’æœã™ã‚‹ã“ã¨ã«å…­åå…­å›½ã€æ¸¡ã£ã¦æµ·åŒ—â‘¦ã‚’å¹³ãã‚‹ã“ã¨ä¹åäº”å›½ã€‚&lt;/p&gt; &lt;p&gt;(å®‹ä¹¦ å€­å›½ä¼  åŸæ±‰æ–‡)&lt;/p&gt; &lt;p&gt;â‘ å››ä¸ƒå…«å¹´ã€‚â‘¡é¢†åŸã€è‡ªåˆ†ã®å›½ã®ã“ã¨ã€‚â‘¢çˆ¶ç¥–ã¨ã„ã†è¯´ã¨ãŒã‚ã‚‹ã€‚â‘£ãŠã¡ã¤ã„ã¦ã®æœ€ã‚‚ãªã„ã€‚â‘¤è›­é¡µã®ã“ã¨ã¨ã‹ã€‚â‘¦æœé²œåŠå²›ã®ã“ã¨ã‹ã€‚&lt;/p&gt; &lt;p&gt;ç«–ç©´å¼çŸ³å®¤ã®æ¨¡å¼å›³&lt;/p&gt; &lt;p&gt;ã€æ—¥æœ¬æ›¸ç´€ã€‘ã€å®‹æ›¸ã€‘&lt;/p&gt; &lt;p&gt;å€­ã®äº”ç‹ã¨å¤©çš‡&lt;/p&gt; &lt;p&gt;ã€Œå®‹æ›¸ã€å€­ä¼ã«è¯»ãƒ»ç(å½Œ)ãƒ»æµãƒ»å¥¥ãƒ»æ­¦ã®äº”ç‹ã®åãŒè®°ã•ã‚Œã¦ã‚‹ã€‚æµä»¥ä¸‹ã¯è®°çºªã«ä¼ãˆã‚‹å°¤æ­ãƒ»å®‰åº·ãƒ»é›„ç•¥ã®å„å¤©çš‡ã«ã‚ã¦ã‚‰ã‚Œã‚‹ãŒã€è¯»ã«ã¯å¿¤ç¥ãƒ»ä»å¾·ãƒ»å±¥ä¸­å¤©çš‡ã‚’ã‚ã¦ã¦ã‚‹è¯¸è¯´ãŒã‚ã‚‹ã€‚çã«ã‚‚ä»å¾·ãƒ»åæ­£å¤©çš‡ã‚ã¦ã¦ã‚‹2è¯´ãŒã‚ã‚‹ã€‚&lt;/p&gt; &lt;p&gt;çºªã«ã‹ã‘ã¦ã®ã“ã¨ã§ã‚ã‚‹ã€‚é«˜å¥éº—ã®å¥½å¤ªç‹ã®ç¢‘æ–‡â‘ ã«ã¯ã€å€­ãŒæœé²œåŠå²›ã«è¿›å‡ºã—é«˜å¥éº—ã¨äº¤æˆ¦ã—ãŸã“ã¨ãŒè®°ã•ã‚Œã¦ã„ã‚‹ã€‚ã“ã‚Œã¯ã€å¤§å’Œæ”¿æ¨©ãŒæœé²œåŠå²›ã®è¿›ã‚“ã æŠ€æœ¯ã‚„é‰„èµ„æºã‚’è·å¾—ã™ã‚‹ãŸã‚ã«åŠ ç½—(ä»»é‚£)ã«è¿›å‡ºã—ã€ãã“ã‚’æ‹ ç‚¹ã¨ã—ã¦é«˜å¥éº—ã®åŠ¿åŠ›ã¨å¯¹æŠ—ã—ãŸã“ã¨ã‚’ç‰©è¯­ã£ã¦ã„ã‚‹ã€‚&lt;/p&gt; &lt;p&gt;ã€Œå®‹ä¹¦ã€ãªã©ã«ã¯ã€5ä¸–çºªåˆã‚ã‹ã‚‰ã»ã¼1ä¸–çºªã®é—´ã€å€­ã®äº”ç‹ãŒä¸­å›½ã®å—æœã«æœè´¡ã—ã€é«˜ã„ç§°å·ã‚’ãˆã‚ˆã†ã¨ã—ãŸã“ã¨ãŒè®°ã•ã‚Œã¦ã„ã‚‹ã€‚ã“ã‚Œã¯ä¸­å›½ã®çš‡å¸ã®æ¨©å¨ã‚’åˆ©ç”¨ã—ã¦ã€æœé²œè¯¸å›½ã«å¯¾ã™ã‚‹æ”¿æ²»çš„ç«‹åœºã‚’æœ‰åˆ©ã«ã—ã‚ˆã†ã¨ã—ãŸã‚‚ã®ã¨è€ƒãˆã‚‰ã‚Œã‚‹ã€‚&lt;/p&gt; &lt;p&gt;æœé²œåŠå²›ãƒ»ä¸­å›½å—æœã¨ã®äº¤æ¸‰ã‚’ã¤ã¥ã˜ã¦ã€å¤§å’Œæ”¿æ¨©ã¯å¤§é™†ã®è¿›ã‚“ã æŠ€æœ¯ã¨æ–‡åŒ–ã‚’ã¨ã‚Šã„ã‚Œã€åŠ¿ã„ã‚’å¼ºã‚ãŸã€‚4ä¸–çºªæœ«ã‹ã‚‰5ä¸–çºªã«ã‹ã‘ã¦ã®ä¸­ã®å¤å¢³ã¯æ€¥æ¿€ã«å·¨å¤§åŒ–ã—ã€å¤§å’Œæ”¿æ¨©ã®æœ€é«˜ã®é¦–é•¿ã§ã‚ã‚‹å¤§ç‹â‘¡ã®æ¨©åŠ›ãŒå¼ºå¤§åŒ–ã—ãŸã“ã¨ã‚’ç‰©è¯­ã£ã¦ã„ã‚‹ã€‚&lt;/p&gt; &lt;p&gt;â‘  å¥½å¤ªç‹(åºƒå¼€åœŸç‹)ä¸€ä»£ã®äº‹ä¸šã‚’è®°ã—ãŸçŸ³ç¢‘ã§ã€é«˜å¥éº—ã®éƒ½ã®ã‚ã£ãŸä¸­å›½å‰æ—çœé›†å®‰çœŒã«ã‚ã‚‹ã€‚å½“æ—¶ã®æœé²œåŠå²›ã®æƒ…åŠ¿ã‚’çŸ¥ã‚‹ãŸã‚ã®è´µé‡ãªå²æ–™ã§ã€ãã®ãªã‹ã«ã€Œç™¾æ¸ˆ(ç™¾æµ)ã€æ–°ç½—ã¯æ—§æ˜¯å±æ°‘ã‚Šã€‚ç”±æ¥æœè´¡ã™ã€‚è€Œã‚‹ã«å€­ã€è¾›å¯ã®å¹´(391å¹´)ã‚ˆã‚Šã“ã®ã‹ãŸã€æµ·æ¸¡ã£ã¦ç™¾æ¸ˆâ–¡â–¡â–¡ç½—ã‚’ç ´ã‚Šã€ä»¥ã£ã¦è‡£æ°‘ã¨ã‚ãšã€æ—¥æœ¬ã®æœé²œåŠå²›ã¸ã®è¿›å‡ºã‚’ä¼ãˆã¦ã„ã‚‹ã€‚&lt;/p&gt; &lt;p&gt;â‘¡ ç†Šæœ¬çœŒç‰åéƒ¡èŠæ°´ç”ºã®æ±Ÿç”°èˆ¹å±±å¤å¢³å‡ºåœŸã®å¤§åˆ€é“­ã«ã¯ã€Œæ²»å¤©ä¸‹çŒ¨â–¡â–¡â–¡ç½—å¤§ç‹ä¸–â€¦â€¦ã€ã¨ã‚ã‚Šã€åŸ¼ç‰çœŒè¡Œç”°å¸‚ã®æ¥¢è·å±±å¤å¢³å‡ºåœŸã®é“åŠ”é“­(â†’p.26å›³ç‰ˆ)ã«ã‚‚ã€Œå€­åŠ å¤šæ”¯æ–‡å¤§ç‹ã€ã¨ã‚‚ãªã‚‹ã€‚ã€Œå¤§ç‹ã€ã¯ã€å€­ã®äº”ç‹ã®1äººæ­¦ã€è®°çºªï¼ˆã€Œå¤äº‹è®°ã€ã€Œæ—¥æœ¬ä¹¦çºªã€ï¼‰ã«ãƒ¯ã‚«ã‚¿ã‚±ãƒ«ã®åã§è®°éŒ²ã•ã‚ŒãŸé›„ç•¥å¤©çš‡ã‚’ã•ã™ã¨è€ƒãˆã‚‰ã‚Œã‚‹ã€‚ã“ã‚Œã‚‰ã®å¤§åˆ€ã‚„é“åŠ”ã‚’ã‚‚ã¤å¤å¢³ã®è¢«è‘¬è€…ã¯ã€å¤§å’Œæ”¿æ¨©ã¨å¯†æ¥ãªé–¢ç³»ã«ã‚ã£ãŸã¨æ¨æµ‹ã•ã‚Œã‚‹ã€‚&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mixivivo"&gt; /u/mixivivo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lpm6cv"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpm6cv/ernie45vl28ba3b_is_a_hidden_gem_that_can_decently/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lpm6cv/ernie45vl28ba3b_is_a_hidden_gem_that_can_decently/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-02T03:57:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1lpwj5j</id>
    <title>AlgoTune: A new benchmark that tests language models' ability to optimize code runtime</title>
    <updated>2025-07-02T13:56:57+00:00</updated>
    <author>
      <name>/u/oripress</name>
      <uri>https://old.reddit.com/user/oripress</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpwj5j/algotune_a_new_benchmark_that_tests_language/"&gt; &lt;img alt="AlgoTune: A new benchmark that tests language models' ability to optimize code runtime" src="https://external-preview.redd.it/0-xrX4qzMJegRxSwLNjXIVQsA6XhMhbHQfZkMjkNJvA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e65bd26dfe93e8bb59300666373e0af786916bdb" title="AlgoTune: A new benchmark that tests language models' ability to optimize code runtime" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We just released AlgoTune which challenges agents to optimize the runtime of 100+ algorithms including gzip compression, AES encryption, and PCA. We also release an agent, AlgoTuner, that enables LMs to iteratively develop efficient code.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/r3vc4rpfugaf1.png?width=2027&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e87085f0e6c72c88d9fb5872d240fcb3685b5b5b"&gt;https://preview.redd.it/r3vc4rpfugaf1.png?width=2027&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e87085f0e6c72c88d9fb5872d240fcb3685b5b5b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Our results show that sometimes frontier LMs are able to find surface level optimizations, but they don't come up with novel algos. There is still a long way to go: the current best AlgoTune score is 1.76x achieved by o4-mini, we think the best potential score is 100x+. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/tnm4r5tpugaf1.png?width=908&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=963893a72dd76d5407871779ed74fef9bda48c57"&gt;https://preview.redd.it/tnm4r5tpugaf1.png?width=908&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=963893a72dd76d5407871779ed74fef9bda48c57&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For full results + paper + code: &lt;a href="http://algotune.io"&gt;algotune.io&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/oripress"&gt; /u/oripress &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpwj5j/algotune_a_new_benchmark_that_tests_language/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpwj5j/algotune_a_new_benchmark_that_tests_language/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lpwj5j/algotune_a_new_benchmark_that_tests_language/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-02T13:56:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1lq1417</id>
    <title>[Open Source] Moondream MCP - Vision for AI Agents</title>
    <updated>2025-07-02T16:57:27+00:00</updated>
    <author>
      <name>/u/_colemurray</name>
      <uri>https://old.reddit.com/user/_colemurray</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lq1417/open_source_moondream_mcp_vision_for_ai_agents/"&gt; &lt;img alt="[Open Source] Moondream MCP - Vision for AI Agents" src="https://preview.redd.it/upyzvjqkrhaf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=76d29f976867cbc03b905c9152d9b301637ec9c9" title="[Open Source] Moondream MCP - Vision for AI Agents" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I integrated Moondream (lightweight vision AI model) with Model Context Protocol (MCP), enabling any AI agent to process images locally/remotely. Open source, self-hosted, no API keys needed. Moondream MCP is a vision AI server that speaks MCP protocol. Your agents can now:&lt;br /&gt; &lt;strong&gt;Caption images&lt;/strong&gt; - &amp;quot;What's in this image?&amp;quot;&lt;br /&gt; &lt;strong&gt;Detect objects&lt;/strong&gt; - Find all instances with bounding boxes&lt;br /&gt; &lt;strong&gt;Visual Q&amp;amp;A&lt;/strong&gt; - &amp;quot;How many people are in this photo?&amp;quot;&lt;br /&gt; &lt;strong&gt;Point to objects&lt;/strong&gt; - &amp;quot;Where's the error message?&amp;quot; &lt;/p&gt; &lt;p&gt;It integrates into Claude Desktop, OpenAI agents, and anything that supports MCP.&lt;br /&gt; &lt;a href="https://github.com/ColeMurray/moondream-mcp/"&gt;https://github.com/ColeMurray/moondream-mcp/&lt;/a&gt;&lt;br /&gt; Feedback and contributions welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_colemurray"&gt; /u/_colemurray &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/upyzvjqkrhaf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lq1417/open_source_moondream_mcp_vision_for_ai_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lq1417/open_source_moondream_mcp_vision_for_ai_agents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-02T16:57:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1lpl656</id>
    <title>GLM-4.1V-Thinking</title>
    <updated>2025-07-02T03:03:37+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpl656/glm41vthinking/"&gt; &lt;img alt="GLM-4.1V-Thinking" src="https://external-preview.redd.it/bgfOhKzxcgalBLIa5eyKdcFfts71dHE0qj65OmHVMu0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d1f66974e5478d143d6f55b57fcf633e79edaf66" title="GLM-4.1V-Thinking" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/THUDM/glm-41v-thinking-6862bbfc44593a8601c2578d"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpl656/glm41vthinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lpl656/glm41vthinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-02T03:03:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1lpep3m</id>
    <title>Tenstorrent Blackhole Cards</title>
    <updated>2025-07-01T21:56:17+00:00</updated>
    <author>
      <name>/u/SashaUsesReddit</name>
      <uri>https://old.reddit.com/user/SashaUsesReddit</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpep3m/tenstorrent_blackhole_cards/"&gt; &lt;img alt="Tenstorrent Blackhole Cards" src="https://preview.redd.it/ffghybw34caf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a7e024c8281faff0ddc04029b2d8b6f4dc59b373" title="Tenstorrent Blackhole Cards" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just got in some Blackhole p150b cards! Excited to try these out... Anyone else on here running some of these? Curious to collaborate! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SashaUsesReddit"&gt; /u/SashaUsesReddit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ffghybw34caf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpep3m/tenstorrent_blackhole_cards/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lpep3m/tenstorrent_blackhole_cards/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T21:56:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1lppg3g</id>
    <title>What's the most complex thing you've been able to (consistently) do with a 4B LLM?</title>
    <updated>2025-07-02T07:15:42+00:00</updated>
    <author>
      <name>/u/noellarkin</name>
      <uri>https://old.reddit.com/user/noellarkin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I don't mean one-off responses that sound good, I'm thinking more along the lines of: ways in which you've gotten the model working reliably in a workflow or pipeline of some kind, or fine tuned it for a specific task that it performs jus as well as the cloudAI behemoths.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/noellarkin"&gt; /u/noellarkin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lppg3g/whats_the_most_complex_thing_youve_been_able_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lppg3g/whats_the_most_complex_thing_youve_been_able_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lppg3g/whats_the_most_complex_thing_youve_been_able_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-02T07:15:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1lpoju6</id>
    <title>World's first Intermediate thinking AI model is now Open Source</title>
    <updated>2025-07-02T06:17:24+00:00</updated>
    <author>
      <name>/u/Quiet-Moment-338</name>
      <uri>https://old.reddit.com/user/Quiet-Moment-338</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Model Link: &lt;a href="https://huggingface.co/HelpingAI/Dhanishtha-2.0-preview"&gt;https://huggingface.co/HelpingAI/Dhanishtha-2.0-preview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Launch video: &lt;a href="https://www.youtube.com/watch?v=QMnmcXngoks"&gt;https://www.youtube.com/watch?v=QMnmcXngoks&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Chat page: helpingai.co/chat&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Quiet-Moment-338"&gt; /u/Quiet-Moment-338 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpoju6/worlds_first_intermediate_thinking_ai_model_is/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpoju6/worlds_first_intermediate_thinking_ai_model_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lpoju6/worlds_first_intermediate_thinking_ai_model_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-02T06:17:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1lphhj3</id>
    <title>DeepSeek-r1-0528 in top 5 on new SciArena benchmark, the ONLY open-source model</title>
    <updated>2025-07-01T23:59:59+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lphhj3/deepseekr10528_in_top_5_on_new_sciarena_benchmark/"&gt; &lt;img alt="DeepSeek-r1-0528 in top 5 on new SciArena benchmark, the ONLY open-source model" src="https://preview.redd.it/xxfqfefhpcaf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=525931b3ac9b9155ccc34e486fc5f097170ed00c" title="DeepSeek-r1-0528 in top 5 on new SciArena benchmark, the ONLY open-source model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Post: &lt;a href="https://allenai.org/blog/sciarena"&gt;https://allenai.org/blog/sciarena&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Allen AI puts out good work and contributes heavily to open-source, I am a big fan of Nathan Lambert. &lt;/p&gt; &lt;p&gt;They just released this scientific literature research benchmark and DeepSeek-r1-0528 is the &lt;strong&gt;only&lt;/strong&gt; open-source model in the top 5, sharing the pie with the like of OpenAI's o3, Claude 4 Open, and Gemini 2.5 Pro.&lt;/p&gt; &lt;p&gt;I like to trash DeepSeek here, but not anymore. This level of performance is just insane.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xxfqfefhpcaf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lphhj3/deepseekr10528_in_top_5_on_new_sciarena_benchmark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lphhj3/deepseekr10528_in_top_5_on_new_sciarena_benchmark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T23:59:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1lpy8nv</id>
    <title>llama-4-scout-17B-16E GGUF running on Strix Halo (Ryzen AI MAX 395 + 128GB) (13s prompt processing edited out)</title>
    <updated>2025-07-02T15:05:54+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpy8nv/llama4scout17b16e_gguf_running_on_strix_halo/"&gt; &lt;img alt="llama-4-scout-17B-16E GGUF running on Strix Halo (Ryzen AI MAX 395 + 128GB) (13s prompt processing edited out)" src="https://external-preview.redd.it/am1mM2cxa2g1aGFmMQV6SVFBQzrbFE16bYnvbQZAeh3r0dX_wLOv44vX6S3R.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6f25fb6e0659e004f656c5dd168fe1ce3e1c7b69" title="llama-4-scout-17B-16E GGUF running on Strix Halo (Ryzen AI MAX 395 + 128GB) (13s prompt processing edited out)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hardware is a mini PC with AMD's Ryzen AI MAX 395 APU with 128GB RAM. Model is llama-4-scout, which is an MOE with 16B active and 109B total parameters.&lt;/p&gt; &lt;p&gt;UI: GAIA, our fork of Open WebUI, that offers out-of-box Lemonade integration, a one-click installer, and electron.js app experience. &lt;a href="https://github.com/amd/gaia"&gt;https://github.com/amd/gaia&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Inference server: Lemonade, our AMD-first OpenAI compatible server, running llama.cpp+Vulkan in the backend on the APU's Radeon 8060S GPU. &lt;a href="https://github.com/lemonade-sdk/lemonade"&gt;https://github.com/lemonade-sdk/lemonade&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I found it cool that a model of this size with VLM capability could achieve usable TPS on a mini PC and wanted to see if others were excited as well.&lt;/p&gt; &lt;p&gt;Full disclosure: prompt processing time (pp) was 13 seconds, and I edited that part out when making the video. Mentioned this in the post title and video caption for maximum transparency. I find 13 seconds usable for this model+usecase, but not very entertaining in a Reddit video.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/e6ao7yjh5haf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpy8nv/llama4scout17b16e_gguf_running_on_strix_halo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lpy8nv/llama4scout17b16e_gguf_running_on_strix_halo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-02T15:05:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1lpoqlu</id>
    <title>DiffuCoder 7B - New coding diffusion LLM by Apple</title>
    <updated>2025-07-02T06:29:47+00:00</updated>
    <author>
      <name>/u/DunklerErpel</name>
      <uri>https://old.reddit.com/user/DunklerErpel</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpoqlu/diffucoder_7b_new_coding_diffusion_llm_by_apple/"&gt; &lt;img alt="DiffuCoder 7B - New coding diffusion LLM by Apple" src="https://external-preview.redd.it/WJ1Mt_9K-aAaMAebSityZ71IWIFD1yMghVJOklMW6Xo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ed4a0394c2c1d722f620e6214e63d44c79d3e340" title="DiffuCoder 7B - New coding diffusion LLM by Apple" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/apple/DiffuCoder-7B-cpGRPO"&gt;https://huggingface.co/apple/DiffuCoder-7B-cpGRPO&lt;/a&gt; (base and instruct also available)&lt;/p&gt; &lt;p&gt;Currently trying - and failing - to run test it on Colab, but really looking forward to it!&lt;/p&gt; &lt;p&gt;Also, anyone got an idea how I can run it on Apple Silicon?&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/s19j3dmfneaf1.png?width=1176&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=927e506f764ded47a4e715aea53c223e56ea7ae6"&gt;Benchmarks compared to other coding and diffusion models&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2506.20639"&gt;https://arxiv.org/pdf/2506.20639&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DunklerErpel"&gt; /u/DunklerErpel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpoqlu/diffucoder_7b_new_coding_diffusion_llm_by_apple/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpoqlu/diffucoder_7b_new_coding_diffusion_llm_by_apple/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lpoqlu/diffucoder_7b_new_coding_diffusion_llm_by_apple/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-02T06:29:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1lq1jyr</id>
    <title>Mamba-2 support in llama.cpp landed</title>
    <updated>2025-07-02T17:14:08+00:00</updated>
    <author>
      <name>/u/pkmxtw</name>
      <uri>https://old.reddit.com/user/pkmxtw</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lq1jyr/mamba2_support_in_llamacpp_landed/"&gt; &lt;img alt="Mamba-2 support in llama.cpp landed" src="https://external-preview.redd.it/ygyWPNq8dkq2um7AlKnQuPRca4C5R9wcoTedIaY7KTk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c83c80ef04abfb36fdb066d346ea91753a7d280d" title="Mamba-2 support in llama.cpp landed" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pkmxtw"&gt; /u/pkmxtw &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/9126#issuecomment-3027064556"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lq1jyr/mamba2_support_in_llamacpp_landed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lq1jyr/mamba2_support_in_llamacpp_landed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-02T17:14:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1lpuk2s</id>
    <title>What Iâ€™ve learned building RAG applications for enterprises</title>
    <updated>2025-07-02T12:28:34+00:00</updated>
    <author>
      <name>/u/Loud_Picture_1877</name>
      <uri>https://old.reddit.com/user/Loud_Picture_1877</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks,&lt;/p&gt; &lt;p&gt;Iâ€™ve spent the last few years building LLM-powered apps at an AI software house - lots of RAG projects, mostly before there were any real frameworks to help. Thought Iâ€™d put together some of the practical lessons I wish I had at the start.&lt;/p&gt; &lt;p&gt;Document Ingestion Tips&lt;/p&gt; &lt;ul&gt; &lt;li&gt;docling is a reliable starter for parsing docs, especially PDFs (and letâ€™s face it, most of the time it will be PDFs).&lt;/li&gt; &lt;li&gt;If your documents follow patterns, donâ€™t be afraid to write some custom parsing logic. It usually pays off for accuracy.&lt;/li&gt; &lt;li&gt;For images and tables, multi-modal LLMs work fine - literally take a screenshot, ask the LLM â€œwhat's this?â€, use that description as part of your embedding context. Multi-modal embeddings are an option, but I find just embedding the LLMâ€™s description easier to manage and debug.&lt;/li&gt; &lt;li&gt;Processing a ton of docs? Use something like &lt;a href="http://ray.io/"&gt;ray.io&lt;/a&gt; so youâ€™re not waiting an hour for everything to finish.&lt;/li&gt; &lt;li&gt;Vector DB tips: qdrant for big scale, pgvector if youâ€™ve already got Postgres in your stack and donâ€™t have millions of records.&lt;/li&gt; &lt;li&gt;On chunking: start with fewer, bigger chunks (with logical start/ends). Overlap and tiny splits cause more pain than help with modern models.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Retrieval&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Always try hybrid search - combine dense vectors with sparse methods like BM25/splade (using something like fastembed). Simple to set up, big boost for retrieval.&lt;/li&gt; &lt;li&gt;Multi-query rephrasing is effective. Just have the LLM rephrase the question a few times, search with each one, then merge the results.&lt;/li&gt; &lt;li&gt;Reranking helps; even an LLM itself can do the rerank step using logprobs, so you donâ€™t always have to wire up a separate model.(&lt;a href="https://cookbook.openai.com/examples/search_reranking_with_cross-encoders"&gt;https://cookbook.openai.com/examples/search_reranking_with_cross-encoders&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;Other fancier techniques (HyDE, GraphRAG, etc) exist, but I havenâ€™t seen enough real-world gains to justify the extra complexity most of the time.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Building &amp;amp; Monitoring&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Good debugging is a lifesaver - seriously. UUIDs per request, OpenTelemetry for tracing: then you can see what actually happened when someone reports a â€œweird answer.â€&lt;/li&gt; &lt;li&gt;Build a proper grafana dashboard: track time-to-first-token, retrieval stats, how long chats go, when people drop out, etc.&lt;/li&gt; &lt;li&gt;Feedback widgets (thumbs up/down, quick text box on â€œthumbs downâ€ for more context) help catch issues earlier.&lt;/li&gt; &lt;li&gt;Deploy early, iterate fast, and try to work directly with subject matter experts - their feedback is always valuable and theyâ€™ll find problems you never thought of.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Evaluation&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Evaluation is easier for just retrieval: set up a dataset, compute Mean Average Precision (MAP) or Mean Reciprocal Rank (MRR).&lt;/li&gt; &lt;li&gt;LLM-as-a-judge works for end-to-end evals, but if your retrieval sucks, everything else falls apart - fix that first.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you want more details, I did a YouTube talk recently where I also cover these tips: &lt;a href="https://www.youtube.com/watch?v=qbcHa83mR-Y"&gt;https://www.youtube.com/watch?v=qbcHa83mR-Y&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Diclaimer: video covers tech that I am maintainer of - ragbits, an open-source toolkit for building these apps with a lot of the above baked in. Feedback and contributors always welcome: &lt;a href="https://github.com/deepsense-ai/ragbits"&gt;https://github.com/deepsense-ai/ragbits&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I would love to hear about your experience with RAG, and Iâ€™m happy to answer any questions.&lt;/p&gt; &lt;p&gt;Letâ€™s chat ğŸ‘‡&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Loud_Picture_1877"&gt; /u/Loud_Picture_1877 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpuk2s/what_ive_learned_building_rag_applications_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpuk2s/what_ive_learned_building_rag_applications_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lpuk2s/what_ive_learned_building_rag_applications_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-02T12:28:34+00:00</published>
  </entry>
</feed>
