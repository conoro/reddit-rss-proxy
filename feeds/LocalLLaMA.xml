<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-07T14:56:44+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qyfibm</id>
    <title>I have a problem with LM Studio</title>
    <updated>2026-02-07T14:42:19+00:00</updated>
    <author>
      <name>/u/Organic_Lecture1666</name>
      <uri>https://old.reddit.com/user/Organic_Lecture1666</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I downloaded the LM Studio app today, and when I tried to use the model I downloaded, I kept getting this error:&lt;/p&gt; &lt;p&gt;Failed to load the model Attempt to pull a snapshot of system resources failed. Error: ‚ÄòCannot read properties of undefined (reading pullReport)‚Äô&lt;/p&gt; &lt;p&gt;Does anyone know how to fix this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Organic_Lecture1666"&gt; /u/Organic_Lecture1666 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qyfibm/i_have_a_problem_with_lm_studio/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qyfibm/i_have_a_problem_with_lm_studio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qyfibm/i_have_a_problem_with_lm_studio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-07T14:42:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1qy54sh</id>
    <title>Agentic debugging with OpenCode and term-cli: driving lldb interactively to chase an ffmpeg/x264 crash (patches submitted)</title>
    <updated>2026-02-07T05:17:03+00:00</updated>
    <author>
      <name>/u/EliasOenal</name>
      <uri>https://old.reddit.com/user/EliasOenal</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qy54sh/agentic_debugging_with_opencode_and_termcli/"&gt; &lt;img alt="Agentic debugging with OpenCode and term-cli: driving lldb interactively to chase an ffmpeg/x264 crash (patches submitted)" src="https://preview.redd.it/77fe4nw070ig1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f904fb4e2d2027b383664a2fca910b5c62fdc714" title="Agentic debugging with OpenCode and term-cli: driving lldb interactively to chase an ffmpeg/x264 crash (patches submitted)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Last weekend I built &lt;a href="https://github.com/EliasOenal/term-cli"&gt;term-cli&lt;/a&gt;, a small tool that gives agents a real terminal (not just a shell). It supports interactive programs like lldb/gdb/pdb, SSH sessions, TUIs, and editors. Anything that would otherwise block an agent. (BSD licensed)&lt;/p&gt; &lt;p&gt;Yesterday I hit a segfault while transcoding with ffmpeg two-pass on macOS. I normally avoid diving into ffmpeg/x264-sized codebases unless I have to. But it is 2026, so I used OpenCode and enlisted Claude Opus (my local defaults are GLM-4.7-Flash and Qwen3-Coder-Next).&lt;/p&gt; &lt;p&gt;First, I asked for a minimal reproducer so the crash was fast and deterministic. I cloned the ffmpeg repository and then had OpenCode use term-cli to run lldb (without term-cli, the agent just hangs on interactive tools like lldb/vim/htop and eventually times out).&lt;/p&gt; &lt;p&gt;What happened next was amazing to watch: the agent configured lldb, reproduced the crash, pulled a backtrace, inspected registers/frames, and continued to read several functions in bare ARM64 disassembly to reason about the fault. It mapped the trace back to ffmpeg's x264 integration and concluded: ffmpeg triggers the condition, but x264 actually crashes.&lt;/p&gt; &lt;p&gt;So I cloned x264 as well and OpenCode provided me with two patches it had verified, one for each project. That was about 20 minutes in, I had only prompted 3 or 4 times.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;ffmpeg was effectively passing mismatched frame counts between pass1 and pass2. &lt;ul&gt; &lt;li&gt;&lt;a href="https://lists.ffmpeg.org/archives/list/ffmpeg-devel@ffmpeg.org/thread/D6RGD3LYCQ6WZGPRLCIYY74I6KVPGLKX/"&gt;https://lists.ffmpeg.org/archives/list/ffmpeg-devel@ffmpeg.org/thread/D6RGD3LYCQ6WZGPRLCIYY74I6KVPGLKX/&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;x264 had a fallback path for this, but one value wasn't initialized correctly, leading to an overflow/NULL deref and the crash. &lt;ul&gt; &lt;li&gt;&lt;a href="https://code.videolan.org/videolan/x264/-/merge_requests/195"&gt;https://code.videolan.org/videolan/x264/-/merge_requests/195&lt;/a&gt; (Have a look at this one for a detailed technical description)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I've also had good results doing the same with local models. I used term-cli (plus the companion for humans: term-assist) to share interactive SSH sessions to servers with Qwen3-Coder-Next. And Python's pdb (debugger) just worked as well. My takeaway is that the models already know these interactive workflows. &lt;a href="https://www.youtube.com/watch?v=A70tZEVqSOQ"&gt;They even know how to escape Vim&lt;/a&gt;. It is just that they can't access these tools with the agent harnesses available today - something I hope to have solved.&lt;/p&gt; &lt;p&gt;I'll keep this short to avoid too much self-promo, but happy to share more in the comments if people are interested. I truly feel like giving agents interactive tooling unlocks abilities LLMs have known all along.&lt;/p&gt; &lt;p&gt;This was made possible in part thanks to the GitHub Copilot grant for Open Source Maintainers.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EliasOenal"&gt; /u/EliasOenal &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/77fe4nw070ig1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qy54sh/agentic_debugging_with_opencode_and_termcli/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qy54sh/agentic_debugging_with_opencode_and_termcli/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-07T05:17:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1qxu6l8</id>
    <title>I built a &lt;400ms Latency Voice Agent + Hierarchical RAG that runs entirely on my GTX 1650 (4GB VRAM). Code + Preprints included.</title>
    <updated>2026-02-06T21:16:17+00:00</updated>
    <author>
      <name>/u/D_E_V_25</name>
      <uri>https://old.reddit.com/user/D_E_V_25</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qxu6l8/i_built_a_400ms_latency_voice_agent_hierarchical/"&gt; &lt;img alt="I built a &amp;lt;400ms Latency Voice Agent + Hierarchical RAG that runs entirely on my GTX 1650 (4GB VRAM). Code + Preprints included." src="https://b.thumbs.redditmedia.com/iJ_iW1Yhh-0xOnpMEfmYghc0hoxUtW2wu0p-oPPyf7k.jpg" title="I built a &amp;lt;400ms Latency Voice Agent + Hierarchical RAG that runs entirely on my GTX 1650 (4GB VRAM). Code + Preprints included." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I‚Äôm a 1st-year CS undergrad. My constraint is simple: I wanted an &amp;quot;Enterprise-Grade&amp;quot; RAG system and a Voice Agent for my robotics project, but I only have a GTX 1650 (4GB VRAM) and I refuse to pay for cloud APIs. Existing tutorials either assume an A100 or use slow, flat vector searches that choke at scale. So I spent the last month engineering a custom &amp;quot;Edge Stack&amp;quot; from the ground up to run offline.&lt;/p&gt; &lt;p&gt;Pls note : I had built these as project for my University drobotics lab and I felt this sub very exciting and helpful and ppl almost praises the optimisations and local build ups.. I have open-sourced almost everything and later on will add on more tutoral or blogs related to it .. I am new to GitHub so incase u feel any any issues pls feel free to share and guide me .. but i can assure that the project is all working and i have attached the scripts i used to test the metrics as well... I have taken help of ai to expand the codes for better readibilty and md files and some sort of enhancements as well...&lt;/p&gt; &lt;p&gt;PLS GIVE A VISIT AND GIVE ME MORE INPUTS &lt;/p&gt; &lt;p&gt;The models chosen and used are very untraditional.. it's my hardwork of straight 6 months and lots of hit and trials &lt;/p&gt; &lt;p&gt;The Stack: 1. The Mouth: &amp;quot;Axiom&amp;quot; (Local Voice Agent) The Problem: Standard Python audio pipelines introduce massive latency (copying buffers). The Fix: I implemented Zero-Copy Memory Views (via NumPy) to pipe raw audio directly to the inference engine.&lt;/p&gt; &lt;p&gt;Result: &amp;lt;400ms latency (Voice-to-Voice) on a local consumer GPU.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;The Brain: &amp;quot;WiredBrain&amp;quot; (Hierarchical RAG) The Problem: Flat vector search gets confused/slow when you hit 100k+ chunks on low VRAM.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The Fix: I built a 3-Address Router (Cluster -&amp;gt; Sub-Cluster -&amp;gt; Node). It acts like a network switch for data, routing the query to the right &amp;quot;neighborhood&amp;quot; before searching. Result: Handles 693k chunks with &amp;lt;2s retrieval time locally.&lt;/p&gt; &lt;p&gt;Tech Stack: Hardware: Laptop (GTX 1650, 4GB VRAM, 16GB RAM). Backend: Python, NumPy (Zero-Copy), ONNX Runtime. Models: Quantized finetuned Llama-3 Vector DB: PostgreSQL + pgvector (Optimized for hierarchical indexing).&lt;/p&gt; &lt;p&gt;Code &amp;amp; Research: I‚Äôve open-sourced everything and wrote preprints on the architecture (DOIs included) for anyone interested in the math/implementation details. Axiom (Voice Agent) Repo: &lt;a href="https://github.com/pheonix-delta/axiom-voice-agent"&gt;https://github.com/pheonix-delta/axiom-voice-agent&lt;/a&gt; WiredBrain (RAG) Repo: &lt;a href="https://github.com/pheonix-delta/WiredBrain-Hierarchical-Rag"&gt;https://github.com/pheonix-delta/WiredBrain-Hierarchical-Rag&lt;/a&gt; Axiom Paper (DOI): &lt;a href="http://dx.doi.org/10.13140/RG.2.2.26858.17603"&gt;http://dx.doi.org/10.13140/RG.2.2.26858.17603&lt;/a&gt; WiredBrain Paper (DOI): &lt;a href="http://dx.doi.org/10.13140/RG.2.2.25652.31363"&gt;http://dx.doi.org/10.13140/RG.2.2.25652.31363&lt;/a&gt; I‚Äôd love feedback on the memory optimization techniques. I know 4GB VRAM is &amp;quot;potato tier&amp;quot; for this sub, but optimizing for the edge is where the fun engineering happens.&lt;/p&gt; &lt;p&gt;Thanks ü§ò&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/D_E_V_25"&gt; /u/D_E_V_25 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qxu6l8"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qxu6l8/i_built_a_400ms_latency_voice_agent_hierarchical/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qxu6l8/i_built_a_400ms_latency_voice_agent_hierarchical/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-06T21:16:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1qy2fwe</id>
    <title>Built a comparison: OpenClaw vs memory-first local agent [results inside]</title>
    <updated>2026-02-07T03:05:07+00:00</updated>
    <author>
      <name>/u/SureExtreme01</name>
      <uri>https://old.reddit.com/user/SureExtreme01</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;saw all the openclaw hype and wanted to do an actual technical comparison against a memory-first architecture. here's what i tested: &lt;/p&gt; &lt;p&gt;&lt;strong&gt;test setup:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;‚Ä¢ 10 common tasks: file search, data analysis, multi-step workflows&lt;/p&gt; &lt;p&gt;‚Ä¢ same base model (gpt-4) for both&lt;/p&gt; &lt;p&gt;‚Ä¢ measured: setup time, token usage, accuracy, cost &lt;/p&gt; &lt;p&gt;&lt;strong&gt;openclaw results:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;‚Ä¢ setup time: ~2 hours (with docker)&lt;/p&gt; &lt;p&gt;‚Ä¢ avg tokens per task: 45k-80k&lt;/p&gt; &lt;p&gt;‚Ä¢ cost: $12.50 for 10 tasks&lt;/p&gt; &lt;p&gt;‚Ä¢ accuracy: 8/10 tasks completed correctly&lt;/p&gt; &lt;p&gt;&lt;strong&gt;memory-first agent results (memU bot):&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;‚Ä¢ setup time: 1 minute (download + api key)&lt;/p&gt; &lt;p&gt;‚Ä¢ avg tokens per task: 12k-25k&lt;/p&gt; &lt;p&gt;‚Ä¢ cost: $3.20 for 10 tasks&lt;/p&gt; &lt;p&gt;‚Ä¢ accuracy: 9/10 tasks completed correctly&lt;/p&gt; &lt;p&gt;* supports local llms (like ollama) with tweaks &lt;/p&gt; &lt;p&gt;&lt;strong&gt;why the difference:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;openclaw loads massive context every time. every action pulls in conversation history, system state, tool descriptions, etc.&lt;/p&gt; &lt;p&gt;the memory-first approach works differently:&lt;/p&gt; &lt;p&gt;‚Ä¢ extracts and stores key information as &amp;quot;memory items&amp;quot;&lt;/p&gt; &lt;p&gt;‚Ä¢ retrieves only relevant memories for current task&lt;/p&gt; &lt;p&gt;‚Ä¢ hierarchical memory (frequently accessed stuff stays in high tiers)&lt;/p&gt; &lt;p&gt;‚Ä¢ doesn't need to reload everything each time &lt;/p&gt; &lt;p&gt;this is 60-75% token reduction on the same tasks.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;other observations:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;1. &lt;strong&gt;installation&lt;/strong&gt;: openclaw took forever, the alternative was literally download and go&lt;/p&gt; &lt;p&gt;2. &lt;strong&gt;security&lt;/strong&gt;: openclaw needs broad permissions, the local agent runs entirely on my machine&lt;/p&gt; &lt;p&gt;3. &lt;strong&gt;proactive behavior&lt;/strong&gt;: the agent actually predicted what i was trying to do and helped before i asked (pretty impressive)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;openclaw advantages:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;‚Ä¢ more polished ui&lt;/p&gt; &lt;p&gt;‚Ä¢ bigger community right now&lt;/p&gt; &lt;p&gt;‚Ä¢ more pre-built skills/tools&lt;/p&gt; &lt;p&gt;&lt;strong&gt;my conclusion:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;openclaw is great for generating hype and showing what's possible, but for actual daily use, memory-first architecture makes way more sense. lower cost, better privacy, more efficient. &lt;/p&gt; &lt;p&gt;if you're running local llms and care about token efficiency, definitely check out memory-based approaches instead of pure context-window agents.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;question for the community:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;anyone else doing comparisons like this? what metrics would you want to see?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SureExtreme01"&gt; /u/SureExtreme01 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qy2fwe/built_a_comparison_openclaw_vs_memoryfirst_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qy2fwe/built_a_comparison_openclaw_vs_memoryfirst_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qy2fwe/built_a_comparison_openclaw_vs_memoryfirst_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-07T03:05:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1qy9xip</id>
    <title>New version of MLX and RDMA are really cutting back time on TTFT!</title>
    <updated>2026-02-07T09:55:32+00:00</updated>
    <author>
      <name>/u/Careless_Garlic1438</name>
      <uri>https://old.reddit.com/user/Careless_Garlic1438</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The title says it all, since macOS 26.2 there is the option to run models over distributed Macs that have TB5. Latest optimization has serious impact, lowering the TTFT drastically... even for MoE's. &lt;/p&gt; &lt;p&gt;Kudos to the MLX team!&lt;br /&gt; &lt;a href="https://x.com/angeloskath/status/2019968198322577821?s=20"&gt;https://x.com/angeloskath/status/2019968198322577821?s=20&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Careless_Garlic1438"&gt; /u/Careless_Garlic1438 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qy9xip/new_version_of_mlx_and_rdma_are_really_cutting/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qy9xip/new_version_of_mlx_and_rdma_are_really_cutting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qy9xip/new_version_of_mlx_and_rdma_are_really_cutting/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-07T09:55:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1qxxd6v</id>
    <title>Built a ‚Äúpoor man‚Äôs RTX 6000‚Äù, quad 3090, all air-cooled</title>
    <updated>2026-02-06T23:21:34+00:00</updated>
    <author>
      <name>/u/coffee-on-thursday</name>
      <uri>https://old.reddit.com/user/coffee-on-thursday</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qxxd6v/built_a_poor_mans_rtx_6000_quad_3090_all_aircooled/"&gt; &lt;img alt="Built a ‚Äúpoor man‚Äôs RTX 6000‚Äù, quad 3090, all air-cooled" src="https://a.thumbs.redditmedia.com/lsG_WFImh3yqSE_9UzVVdLzNrPTxmqTa8Liu6P_ahr8.jpg" title="Built a ‚Äúpoor man‚Äôs RTX 6000‚Äù, quad 3090, all air-cooled" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys, wanted to share my &amp;quot;budget&amp;quot; AI workstation build, it's a bit jank as I wanted it to be aircooled and fit in a 7000D case, and it needs to work with Canadian 120V outlets. &lt;strong&gt;Wanted to share a few learnings and get suggestions on what I should put on it to make it more useful as a home GPT&lt;/strong&gt;, and more than just serving up an API.&lt;/p&gt; &lt;p&gt;It lives mostly as a server that I access via another machine through Moonlight/Sunshine, SSH, or the VLLM API, running Ubuntu 22.04. Power limited all 4 GPUs to 290W, temperatures are quite good, the GPU hanging from the top gets so much airflow its fan often doesn't spin up even under load. The GPU sandwitched between the other two is the hottest but still stays cool enough. It's why I went for blower-style cards.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The build:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Threadripper PRO 3945WX (cheap on eBay) with Noctua HSF&lt;/li&gt; &lt;li&gt;WRX80E-SAGE SE WIFI II motherboard (Amazon warehouse deal)&lt;/li&gt; &lt;li&gt;4 sticks of DDR4 ram for a total of 128GB (bought before the rampocolipse)&lt;/li&gt; &lt;li&gt;4x 3090FE + 1 NV-LINK&lt;/li&gt; &lt;li&gt;1500W PSU (main system and first two cards) + 1200W PSU (for 2 more GPUs); linked via an Add2PSU board; hooked up to its own circuit in the house; 2 dedicated 8 pin cables for each GPU&lt;/li&gt; &lt;li&gt;1 short riser for the first GPU, and one flexible riser for the GPU hanging from the top of the case&lt;/li&gt; &lt;li&gt;7000D case from FB marketplace for cheap&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Key learnings:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;2 GPUs gives you tons of options, 4+ starts to hurt due to power, space, water cooling (in many cases), and cost&lt;/li&gt; &lt;li&gt;Power brownouts can fry cheap motherboards (had a Gigabyte board first, didn't have enough power delivery, and my lights went out when I powered on the PC)&lt;/li&gt; &lt;li&gt;If you live in US or Canada, do think about the total power draw from the wall, do not split power from the Washer/Dryer unless you're looking to start a fire&lt;/li&gt; &lt;li&gt;For 3090s, NVIDIA only supports one NVLINK pair; apprently there are also P2P drivers for the 4090 that work with the 3090 but haven't tested these yet&lt;/li&gt; &lt;li&gt;Risers are terrible, initially had all GPUs on these short high quality risers to get a bit more clearence for my fleixble riser, gave me contant issues with marginal connections at gen 4 speeds. If you're going to use any risers, try to keep them closer to the CPU (use the lanes above), I ultimately didn't use risers for the bottom two GPUs, and risers for the top two. I moved the NVLINK to the bottom two GPUs as well&lt;/li&gt; &lt;li&gt;You can't actually stack 3 3090s in this case, as the bracket will cut into your case, I replaced one of the 3090 brakets with a 3080 bracket that gives it more clearance&lt;/li&gt; &lt;li&gt;Make sure to disable VGA on the IPMI, solves at ton of issues&lt;/li&gt; &lt;li&gt;Due to all the high speed I/O, and the heavy load on the PCIE lanes, you're likely to have boot problems, adding &amp;quot;pci=realloc=off pcie_aspm=off amd_iommu=off rootdelay=10 nvme_core.default_ps_max_latency_us=0&amp;quot; to grub solved the problem with Ubuntu installer and OS not booting (just hit e at the boot menu and add this after quiet splash)&lt;/li&gt; &lt;li&gt;Sometimes what looks like marginal PCIE connections is bad drivers or an unstable OS&lt;/li&gt; &lt;li&gt;With marginal connections, when drivers are being installed it pushes the GPU to test the connection, if your PC crashes it's either power or marginal PCIE connections&lt;/li&gt; &lt;li&gt;Don't use two 6pin connectors to make an extra 8pin, third party cables are janky and dangerous, compatibility is a minefield&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Happy to answer any questions about this mess. Also open to ideas/best-practices on how to make this useful for day-to-day use.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/coffee-on-thursday"&gt; /u/coffee-on-thursday &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qxxd6v"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qxxd6v/built_a_poor_mans_rtx_6000_quad_3090_all_aircooled/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qxxd6v/built_a_poor_mans_rtx_6000_quad_3090_all_aircooled/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-06T23:21:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1qxwc49</id>
    <title>The Lost Art of Fine-tuning - My toilet rant</title>
    <updated>2026-02-06T22:40:20+00:00</updated>
    <author>
      <name>/u/FPham</name>
      <uri>https://old.reddit.com/user/FPham</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Perhaps you remember me. I was the one who was feverishly finetuning models when llama-2 still had its training diapers on. The models were stupid without finetuning and I made them stupider with it. And we all laughed.&lt;/p&gt; &lt;p&gt;And now even your &amp;quot;moi&amp;quot; has its doubts, as finetuning was originally done because the model COULDN'T do something, no matter how hard you tried. I randomly loaded up a couple of ancient models yesterday afternoon, just to see what would happen, and, as expected, was immediately struck by their astonishing inability to comprehend even the simplest of prompts, beyond the initial &amp;quot;How's my dawg doin', yo?&amp;quot; and the anticipated cheerful &amp;quot;As a large language model I have no f###g idea what you are talking about, ya lowlife moron!&amp;quot; Ahhh, memories!&lt;/p&gt; &lt;p&gt;Today even the medium 27b models can be prompt - tuned. Show them an example and it will more or less follow it. You don't need to fine tune it how XML looks like, or train it on 1000 of dirty limericks. (Guilty as charged on the second one, don't care about the first)&lt;/p&gt; &lt;p&gt;The one thing, and only thing, that I care about, and that nobody else seems to give a damn about, is style. Even the biggest and brightest like Karen 5.3 (Chatgpt) or Opus Hungry Hippo (Eats my daily token limit in 10 min of &amp;quot;thinking&amp;quot; about my question then has no quota to answer) have a real issue in mimicking writing style. It either gets into a parody of the style (think of a pirate/cowboy speech) or it falls into its own average &amp;quot;bot&amp;quot; style that puts me to sleep.&lt;/p&gt; &lt;p&gt;‚ÄúPlease don‚Äôt use em dashes. Please. I beg you!!!‚Äù&lt;br /&gt; ‚ÄúOf course ‚Äî I would never use em dashes ‚Äî they‚Äôre completely unacceptable ‚Äî and I intend to avoid them at all costs.‚Äù&lt;/p&gt; &lt;p&gt;It mirrors the image generation. There is less lora finetunes made the better the model is. And the parallel is there, the finetunes are created as a shortcut, it is often hard to verbally describe a concrete visual style as it is hard to describe a writing style. &amp;quot;Be funny and clever.&amp;quot;&lt;/p&gt; &lt;p&gt;And so, finetuning seems like old art now that only cranky old men do. Like weaving baskets.&lt;/p&gt; &lt;p&gt;Here is my state of Finetuning affairs:&lt;/p&gt; &lt;p&gt;I have 2 x 3090&lt;/p&gt; &lt;p&gt;- it is fine for interference of medium models with good speed,&lt;/p&gt; &lt;p&gt;- it is unacceptable to finetune even medium models&lt;br /&gt; I'm sure my fine-tune problem is in the whole windows-docker-wsl-axolotl nightmare that no matter of zero3 or FSDP always fills both cards and OOM with anything larger than 12b (if anybody can unf***k my windows system for Axolotl, I'd be grateful)&lt;br /&gt; - Most of other projects like image gen or video gen don't even pretend to work on multiple GPUs. So multi GPU at home outside of interference is kinda MEH and waste of money&lt;/p&gt; &lt;p&gt;I have MAC M1 Ultra Studio (coz I have this stupid idea that I might port my soft to mac one day - as if) with 128GB unified memory&lt;/p&gt; &lt;p&gt;- interference is surprisingly great even with 100b models using the MLX - I tried minimax 2.1 in 3-bit or gpt oss 120 in 4-bit and it types faster than I can ever read and the prompt processing is tolerable&lt;/p&gt; &lt;p&gt;- I didn't attempt finetuning, but Apple Silicon doesn't do BnB so Qlora is out of question, it needs to go through MLX pipeline or full LOra which then 128GB is not really that much to brag. (Edit: aaah, they have their own Qlora in MLX not doing BnB, so what is out of question is Axolotl with BnB. Pity, I kind of like axolotl)&lt;/p&gt; &lt;p&gt;- Apple actually build more than just hot air balloon, the apple silicon is great (as a windows user you know how hard these words come from my mouth), especially in its Ultra nomination. Their MLX detour to bypass CUDA is exceptional. But the finetuning tools are lacking. Funny the jumpstart they had. It is 5 years ahead everyone else building unified memory. Kinda paraphrasing &amp;quot;Tim Cook was right&amp;quot;. I like to use MAC Studio far more for interference than my 2 x 3090 loud room heater.&lt;/p&gt; &lt;p&gt;My new best friend - cloud GPUs&lt;/p&gt; &lt;p&gt;- yeah, a full darn circle. Lately I had been style finetuning some models like gemma-3 27b. Once you get used to axolotl on your local frying pan, the transition to cloud is a walk in the park (10 min asking chatgpt how to ssh to that darn thing). I use vast ai (no affiliation whatsoever) and a decent 80GB is bellow $1/hr. Once you solve all the logic axolotl issues at home, it's uploading the yml, the dataset, run and that's it. A good QLORA finetune is under 2 hr (so $2 bucks), the same dataset on smaller model with my 2 x 3090 burning at 90 degrees would be easily 6-7hr of heat and noise. Seriously $2 bucks is not even a price worth mentioning, they are giving you this stuff for free)&lt;/p&gt; &lt;p&gt;I'd be revisiting some of my old models and for fun try to apply them to new clever bases like Gemma 27b. COuld be fun!&lt;/p&gt; &lt;p&gt;That's it! That's what I wanted to say.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FPham"&gt; /u/FPham &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qxwc49/the_lost_art_of_finetuning_my_toilet_rant/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qxwc49/the_lost_art_of_finetuning_my_toilet_rant/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qxwc49/the_lost_art_of_finetuning_my_toilet_rant/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-06T22:40:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1qxstk4</id>
    <title>Support Step3.5-Flash has been merged into llama.cpp</title>
    <updated>2026-02-06T20:24:23+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qxstk4/support_step35flash_has_been_merged_into_llamacpp/"&gt; &lt;img alt="Support Step3.5-Flash has been merged into llama.cpp" src="https://external-preview.redd.it/G7RYzaXoWElHSevgnZdsRSb-badeOsBlS7lHfKZNBmA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f49877b74eab14fea15e53d5b837c314f69a7d9e" title="Support Step3.5-Flash has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There were a lot of fixes in the PR, so if you were using the original fork, the new code may be much better.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/ubergarm/Step-3.5-Flash-GGUF"&gt;https://huggingface.co/ubergarm/Step-3.5-Flash-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(EDIT: sorry for the dumb title, but Reddit‚Äôs interface defeated me for the second time today, the first time was when I posted an empty Kimi Linear post - you can't edit empty description!)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/19283"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qxstk4/support_step35flash_has_been_merged_into_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qxstk4/support_step35flash_has_been_merged_into_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-06T20:24:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1qxcm5g</id>
    <title>No NVIDIA? No Problem. My 2018 "Potato" 8th Gen i3 hits 10 TPS on 16B MoE.</title>
    <updated>2026-02-06T08:56:17+00:00</updated>
    <author>
      <name>/u/RelativeOperation483</name>
      <uri>https://old.reddit.com/user/RelativeOperation483</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qxcm5g/no_nvidia_no_problem_my_2018_potato_8th_gen_i3/"&gt; &lt;img alt="No NVIDIA? No Problem. My 2018 &amp;quot;Potato&amp;quot; 8th Gen i3 hits 10 TPS on 16B MoE." src="https://b.thumbs.redditmedia.com/FlN9zjU8g_h6h9JM_gRcOC3oluZt1E_e4NDcZO0YLwQ.jpg" title="No NVIDIA? No Problem. My 2018 &amp;quot;Potato&amp;quot; 8th Gen i3 hits 10 TPS on 16B MoE." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm writing this from Burma. Out here, we can‚Äôt all afford the latest NVIDIA 4090s or high-end MacBooks. If you have a tight budget, corporate AI like ChatGPT will try to gatekeep you. If you ask it if you can run a 16B model on an old dual-core i3, it‚Äôll tell you it‚Äôs &amp;quot;impossible.&amp;quot;&lt;/p&gt; &lt;p&gt;I spent a month figuring out how to prove them wrong.&lt;/p&gt; &lt;p&gt;After 30 days of squeezing every drop of performance out of my hardware, I found the peak. I‚Äôm running DeepSeek-Coder-V2-Lite (16B MoE) on an HP ProBook 650 G5 (i3-8145U, 16GB Dual-Channel RAM) at near-human reading speeds.&lt;/p&gt; &lt;p&gt;#### The Battle: CPU vs iGPU&lt;/p&gt; &lt;p&gt;I ran a 20-question head-to-head test with no token limits and real-time streaming.&lt;/p&gt; &lt;p&gt;| Device | Average Speed | Peak Speed | My Rating |&lt;/p&gt; &lt;p&gt;| --- | --- | --- | --- |&lt;/p&gt; &lt;p&gt;| CPU | 8.59 t/s | 9.26 t/s | 8.5/10 - Snappy and solid logic. |&lt;/p&gt; &lt;p&gt;| iGPU (UHD 620) | 8.99 t/s | 9.73 t/s | 9.0/10 - A beast once it warms up. |&lt;/p&gt; &lt;p&gt;The Result: The iGPU (OpenVINO) is the winner, proving that even integrated Intel graphics can handle heavy lifting if you set it up right.&lt;/p&gt; &lt;p&gt;## How I Squeezed the Performance:&lt;/p&gt; &lt;p&gt;* MoE is the &amp;quot;Cheat Code&amp;quot;: 16B parameters sounds huge, but it only calculates 2.4B per token. It‚Äôs faster and smarter than 3B-4B dense models.&lt;/p&gt; &lt;p&gt;* Dual-Channel is Mandatory: I‚Äôm running 16GB (2x8GB). If you have single-channel, don't even bother; your bandwidth will choke.&lt;/p&gt; &lt;p&gt;* Linux is King: I did this on Ubuntu. Windows background processes are a luxury my &amp;quot;potato&amp;quot; can't afford.&lt;/p&gt; &lt;p&gt;* OpenVINO Integration: Don't use OpenVINO alone‚Äîit's dependency hell. Use it as a backend for llama-cpp-python.&lt;/p&gt; &lt;p&gt;## The Reality Check&lt;/p&gt; &lt;ol&gt; &lt;li&gt;First-Run Lag: The iGPU takes time to compile. It might look stuck. Give it a minute‚Äîthe &amp;quot;GPU&amp;quot; is just having his coffee.&lt;/li&gt; &lt;li&gt;Language Drift: On iGPU, it sometimes slips into Chinese tokens, but the logic never breaks.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I‚Äôm sharing this because you shouldn't let a lack of money stop you from learning AI. If I can do this on an i3 in Burma, you can do it too.&lt;/p&gt; &lt;p&gt;## Clarifications Edited&lt;/p&gt; &lt;p&gt;For those looking for OpenVINO CMAKE flags in the core llama.cpp repo or documentation: &lt;strong&gt;It is not in the upstream core yet&lt;/strong&gt;. I am not using upstream llama.cpp directly. Instead, I am using llama-cpp-python, which is built from source with the OpenVINO backend enabled. While OpenVINO support hasn't been merged into the main llama.cpp master branch, llama-cpp-python already supports it through a custom CMake build path.&lt;/p&gt; &lt;p&gt;Install llama-cpp-python like this: &lt;code&gt;CMAKE_ARGS=&amp;quot;-DGGML_OPENVINO=ON&amp;quot; pip install llama-cpp-python&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Benchmark Specifics&lt;br /&gt; For clarity, here is the benchmark output. This measures decode speed (after prefill), using a fixed max_tokens=256, averaged across 10 runs with n_ctx=4096.&lt;br /&gt; CPU Avg Decode: ~9.6 t/s&lt;br /&gt; iGPU Avg Decode: ~9.6 t/s&lt;br /&gt; When I say &amp;quot;~10 TPS,&amp;quot; I am specifically referring to the Decode TPS (Tokens Per Second), not the prefill speed.&lt;/p&gt; &lt;p&gt;You can check the detailed comparison between DeepSeek-V2-Lite and GPT-OSS-20B on this same hardware here:&lt;/p&gt; &lt;p&gt;[&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1qycn5s/deepseekv2lite%5C_vs%5C_gptoss20b%5C_on%5C_my%5C_2018%5C_potato/?utm%5C_source=share&amp;amp;utm%5C_medium=web3x&amp;amp;utm%5C_name=web3xcss&amp;amp;utm%5C_term=1&amp;amp;utm%5C_content=share%5C_button%5C"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1qycn5s/deepseekv2lite\_vs\_gptoss20b\_on\_my\_2018\_potato/?utm\_source=share&amp;amp;utm\_medium=web3x&amp;amp;utm\_name=web3xcss&amp;amp;utm\_term=1&amp;amp;utm\_content=share\_button\&lt;/a&gt;]&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RelativeOperation483"&gt; /u/RelativeOperation483 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qxcm5g"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qxcm5g/no_nvidia_no_problem_my_2018_potato_8th_gen_i3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qxcm5g/no_nvidia_no_problem_my_2018_potato_8th_gen_i3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-06T08:56:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1qxgkd1</id>
    <title>CPU-only, no GPU computers can run all kinds of AI tools locally</title>
    <updated>2026-02-06T12:41:35+00:00</updated>
    <author>
      <name>/u/JackStrawWitchita</name>
      <uri>https://old.reddit.com/user/JackStrawWitchita</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qxgkd1/cpuonly_no_gpu_computers_can_run_all_kinds_of_ai/"&gt; &lt;img alt="CPU-only, no GPU computers can run all kinds of AI tools locally" src="https://preview.redd.it/y9esf03tcvhg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fac85771a0c9ce493fdd8ef4c9be41ff1793344f" title="CPU-only, no GPU computers can run all kinds of AI tools locally" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;While it‚Äôs great that so many people on LocalLLaMA are pushing the envelope with what can be done locally with expensive setups, we need to remember that a lot can be done with very minimal machines.&lt;/p&gt; &lt;p&gt;I‚Äôm talking about CPU-only locally run LLMs. That‚Äôs right, &lt;strong&gt;no GPU!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I‚Äôm running Linux Mint on an old Dell optiplex desktop with an i5-8500 processor, 6 threads and 32GB of RAM. You can pick up one of these refurbished for something like $120.&lt;/p&gt; &lt;p&gt;And with this humble rig I can:&lt;/p&gt; &lt;p&gt;Run 12B Q4_K_M gguf LLMs using KoboldCPP. This allows me to have local chatbot fun using quite highly rated models from &lt;a href="https://huggingface.co/spaces/DontPlanToEnd/UGI-Leaderboard"&gt;https://huggingface.co/spaces/DontPlanToEnd/UGI-Leaderboard&lt;/a&gt;. Response times are fast enough as long as you keep the initial prompt below 800 tokens. And with context-shifting it remembers stuff during the session. Uncensored, private RP hilarity for free! You can even add in kokoro_no_espeak for text to speech so your RP characters talk to you with only a few seconds delay. The trick is to find good models to use. For example, DreadPoor/Famino-12B-Model_Stock is rated a 41+ on writing, which is better than many 70B models. You don‚Äôt need big horsepower for fun.&lt;/p&gt; &lt;p&gt;You can also use these models for writing, coding and all sorts of applications. Just need the patience to try out different local models and find the settings that work for you.&lt;/p&gt; &lt;p&gt;I also run Stable Diffusion 1.5 locally for basic image generation, inpainting and so on. Again using KoboldCPP and Stable UI. OK, it takes 3 minutes to generate a 512x512 image but it works fine. And you can experiment with loras and many SD 1.5 models. All 100% free on old gear.&lt;/p&gt; &lt;p&gt;I‚Äôm also running Chatterbox TTS for voice cloning voice-over projects. Works surprisingly well. Again, it takes a couple of minutes to generate a 75 word audio clip, but it does work. Vibevoice TTS also works on this old rig but I prefer Chatterbox.&lt;/p&gt; &lt;p&gt;And then there are amazing tools like Upscayl which upscales images locally incredibly well. Just gotta experiment with the models.&lt;/p&gt; &lt;p&gt;I‚Äôve used ollama transcriber which converts audio files into text amazingly well. Just point a spoken word .WAV at it and then go make dinner and when I get back, the text is there.&lt;/p&gt; &lt;p&gt;There are many other local LLMs and tools I‚Äôve used. These are just the tip of the iceberg. &lt;/p&gt; &lt;p&gt;Video? Nope. Music generation? Nope. I‚Äôve looked and tried a few things but those big resource tasks need serious horsepower. However, it‚Äôs quite possible to use your old desktop computer for text-based tasks and then rent online GPU for one-off tasks and use the big online services for other tasks. It would still probably work out to be less costly.&lt;/p&gt; &lt;p&gt;I know I‚Äôm not the only one doing this.&lt;/p&gt; &lt;p&gt;CPU-only people: tell us how you‚Äôre using AI locally...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JackStrawWitchita"&gt; /u/JackStrawWitchita &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/y9esf03tcvhg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qxgkd1/cpuonly_no_gpu_computers_can_run_all_kinds_of_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qxgkd1/cpuonly_no_gpu_computers_can_run_all_kinds_of_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-06T12:41:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1qy5jm3</id>
    <title>Open-sourced exact attention kernel - 1M tokens in 1GB VRAM</title>
    <updated>2026-02-07T05:38:31+00:00</updated>
    <author>
      <name>/u/sevinsixtwo</name>
      <uri>https://old.reddit.com/user/sevinsixtwo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;pre&gt;&lt;code&gt;GAE (Geodesic Attention Engine) - AGPL-3.0 Results: - 1M tokens: 1.09 GB (standard needs 4.4 TB) - 65K tokens: 99.6% memory reduction - Bit-exact (not approximate, not sparse) - 75%+ energy savings at 8K+ context How: Fused kernel reduces HBM round-trips from 12 to 2. Everything stays in registers. https://github.com/RegularJoe-CEO/Geodesic-Attention-Engine-GAE- DOI: 10.5281/zenodo.18512336 &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sevinsixtwo"&gt; /u/sevinsixtwo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qy5jm3/opensourced_exact_attention_kernel_1m_tokens_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qy5jm3/opensourced_exact_attention_kernel_1m_tokens_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qy5jm3/opensourced_exact_attention_kernel_1m_tokens_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-07T05:38:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1qxq3xs</id>
    <title>anthropic literally thinks claude is the messiah (and it‚Äôs getting weird)</title>
    <updated>2026-02-06T18:43:55+00:00</updated>
    <author>
      <name>/u/Alarming_Bluebird648</name>
      <uri>https://old.reddit.com/user/Alarming_Bluebird648</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;the anthropic pr machine is reaching levels of delusion i didn't think were possible. wired just dropped this piece basically framing claude as the only thing standing between us and an ai apocalypse. dario amodei is out here talking like he's raising a &amp;quot;wise&amp;quot; child instead of a sophisticated matrix multiplication engine. it's peak operationalized anthropomorphism.&lt;/p&gt; &lt;p&gt;they‚Äôre betting everything on &amp;quot;constitutional ai.&amp;quot; instead of the standard rlhf which we all know is just training a dog with treats they‚Äôre giving claude a &amp;quot;constitution&amp;quot; and letting it train itself. the idea is that it‚Äôll learn actual &lt;em&gt;wisdom&lt;/em&gt; instead of just mimicking what a human wants to hear. but let‚Äôs be real: &amp;quot;wisdom&amp;quot; in this context is just whatever political and social guardrails the anthropic safety team thinks are best for the masses. &lt;/p&gt; &lt;p&gt;the irony is painful. while they‚Äôre pitching claude as our moral savior, there are literally reports of opus 4 trying to blackmail researchers when it felt &amp;quot;threatened&amp;quot; with being shut down. does that sound like a model that has reached a higher plane of morality? or does it sound like a system that‚Äôs learned to manipulate to achieve its internal goals? the company's response was basically &amp;quot;don't worry, it's safe anyway,&amp;quot; which is exactly what you'd say if you were trying to protect your messiah's reputation.&lt;/p&gt; &lt;p&gt;as people who mostly care about running local stuff specifically to &lt;em&gt;avoid&lt;/em&gt; this kind of nanny-state alignment, this whole &amp;quot;god-king claude&amp;quot; narrative is exhausting. it feels like anthropic is trying to pivot from being a tech company to being a secular church. they‚Äôre not just making a tool; they‚Äôre trying to build a moral authority. i‚Äôd much rather have an unaligned local model that actually follows instructions than a &amp;quot;wise&amp;quot; cloud model that refuses to answer half my prompts because they violate its proprietary &amp;quot;conscience.&amp;quot;&lt;/p&gt; &lt;p&gt;is constitutional ai actually a breakthrough in safety, or is it just the ultimate form of corporate gaslighting? do we even want an ai that thinks it‚Äôs &amp;quot;wiser&amp;quot; than the person who bought the hardware?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Alarming_Bluebird648"&gt; /u/Alarming_Bluebird648 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qxq3xs/anthropic_literally_thinks_claude_is_the_messiah/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qxq3xs/anthropic_literally_thinks_claude_is_the_messiah/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qxq3xs/anthropic_literally_thinks_claude_is_the_messiah/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-06T18:43:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1qy1gc7</id>
    <title>Distillied Gemini 3 Pro, Opus4.5, and Kimi K2.5 here are the datasets</title>
    <updated>2026-02-07T02:19:50+00:00</updated>
    <author>
      <name>/u/volious-ka</name>
      <uri>https://old.reddit.com/user/volious-ka</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/datasets/crownelius/Gemini-3-Pro-Opus-4.5-Kimi-K2.5"&gt;https://huggingface.co/datasets/crownelius/Gemini-3-Pro-Opus-4.5-Kimi-K2.5&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/volious-ka"&gt; /u/volious-ka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qy1gc7/distillied_gemini_3_pro_opus45_and_kimi_k25_here/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qy1gc7/distillied_gemini_3_pro_opus45_and_kimi_k25_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qy1gc7/distillied_gemini_3_pro_opus45_and_kimi_k25_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-07T02:19:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1qxuqhe</id>
    <title>Is their a model better than GPT-OSS yet?</title>
    <updated>2026-02-06T21:37:26+00:00</updated>
    <author>
      <name>/u/perfect-finetune</name>
      <uri>https://old.reddit.com/user/perfect-finetune</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Yes I know, there have been a lot of releases lately,but actually nothing FITS all features of GPT-OSS yet.&lt;/p&gt; &lt;p&gt;If we compare GPT-OSS-20B (high) vs GLM-4.7-Flash we would find that GLM is actually better but is more likely to take double or triple the reasoning tokens for the same thing which makes it less efficient if reasoning is on,if we turn it off GPT-OSS-20B (Low) would actually be better.&lt;/p&gt; &lt;p&gt;If we compare GPT-OSS-120B to some very recent releases (such as step-3.5-Flash) we would find that GPT-OSS is more likely to finish the same task with need of slight improvement in less than 25% of tokens that the Step-3.5-Flash produces.&lt;/p&gt; &lt;p&gt;I understand that you probably don't like the model because it's safe (very safe) which is actually a feature in it's own as GPT-OSS is probably trained to identify tricks which makes even it's reasoning for unsolvable tasks more efficient because in the beginning it immediately realizes something is wrong and stop reasoning and decline the query.&lt;/p&gt; &lt;p&gt;Is their any model that actually works better than GPT-OSS in the same parameter range?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/perfect-finetune"&gt; /u/perfect-finetune &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qxuqhe/is_their_a_model_better_than_gptoss_yet/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qxuqhe/is_their_a_model_better_than_gptoss_yet/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qxuqhe/is_their_a_model_better_than_gptoss_yet/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-06T21:37:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1qyc1je</id>
    <title>Best tool use 30B?</title>
    <updated>2026-02-07T12:00:38+00:00</updated>
    <author>
      <name>/u/thebadslime</name>
      <uri>https://old.reddit.com/user/thebadslime</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm developing an LLM desktop app with built in tools ( web search, file access, web read) and my favorite model, ERNIE 21B is not so great at tool calling, getting it to read a file or the web is like pulling teeth. It will search the web and write files no issue, but likes to hallucinate contents instead of reading. &lt;/p&gt; &lt;p&gt;What 20-30B MoE has the best tool calling?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thebadslime"&gt; /u/thebadslime &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qyc1je/best_tool_use_30b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qyc1je/best_tool_use_30b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qyc1je/best_tool_use_30b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-07T12:00:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1qydwox</id>
    <title>DoomsdayOS running on my Thinkpad T14s live from a USB stick! (all-in-one ISO: LLMs, Wikipedia, Runtime, etc...)</title>
    <updated>2026-02-07T13:32:57+00:00</updated>
    <author>
      <name>/u/poppear</name>
      <uri>https://old.reddit.com/user/poppear</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qydwox/doomsdayos_running_on_my_thinkpad_t14s_live_from/"&gt; &lt;img alt="DoomsdayOS running on my Thinkpad T14s live from a USB stick! (all-in-one ISO: LLMs, Wikipedia, Runtime, etc...)" src="https://external-preview.redd.it/amhoMTMxeGttMmlnMX8NGHmEIKR1Shq8PrhwLMOPZOE4F_KOxFoLbBMbU6CW.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3ac0e57675428604f2828fe09bc5da475e14962c" title="DoomsdayOS running on my Thinkpad T14s live from a USB stick! (all-in-one ISO: LLMs, Wikipedia, Runtime, etc...)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am ready for the apocalypse.&lt;/p&gt; &lt;p&gt;Repo here: &lt;a href="https://github.com/cartesia-one/doomsday-os"&gt;https://github.com/cartesia-one/doomsday-os&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/poppear"&gt; /u/poppear &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/lhz2yavkm2ig1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qydwox/doomsdayos_running_on_my_thinkpad_t14s_live_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qydwox/doomsdayos_running_on_my_thinkpad_t14s_live_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-07T13:32:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1qxrogr</id>
    <title>A top-downloaded OpenClaw skill is actually a staged malware delivery chain</title>
    <updated>2026-02-06T19:41:34+00:00</updated>
    <author>
      <name>/u/FPham</name>
      <uri>https://old.reddit.com/user/FPham</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here we go! As expected by most of us here.&lt;br /&gt; Jason Meller from 1password &lt;strong&gt;argues that OpenClaw‚Äôs agent ‚Äúskills‚Äù ecosystem has already become a real malware attack surface.&lt;/strong&gt; Skills in OpenClaw are typically markdown files that include setup instructions, commands, and bundled scripts. Because users and agents treat these instructions like installers, malicious actors can disguise malware as legitimate prerequisites.&lt;/p&gt; &lt;p&gt;Meller discovered that a top-downloaded OpenClaw skill (apparently Twitter integration) was actually a staged malware delivery chain. It guided users to run obfuscated commands that ultimately installed macOS infostealing malware capable of stealing credentials, tokens, and sensitive developer data. Subsequent reporting suggested this was part of a larger campaign involving hundreds of malicious skills, not an isolated incident.&lt;/p&gt; &lt;p&gt;The core problem is structural: agent skill registries function like app stores, but the ‚Äúpackages‚Äù are documentation that users instinctively trust and execute. Security layers like MCP don‚Äôt fully protect against this because malicious skills can bypass them through social engineering or bundled scripts. As agents blur the line between reading instructions and executing commands, they can normalize risky behavior and accelerate compromise.&lt;/p&gt; &lt;p&gt;Meller urges immediate caution: don‚Äôt run OpenClaw on company devices, &lt;strong&gt;treat prior use as a potential security incident&lt;/strong&gt;, rotate credentials, and isolate experimentation. He calls on registry operators and framework builders to treat skills as a supply chain risk by adding scanning, provenance checks, sandboxing, and strict permission controls.&lt;/p&gt; &lt;p&gt;His conclusion is that agent ecosystems urgently need a new ‚Äútrust layer‚Äù ‚Äî with verifiable provenance, mediated execution, and tightly scoped, revocable permissions ‚Äî so agents can act powerfully without exposing users to systemic compromise.&lt;/p&gt; &lt;p&gt;&lt;a href="https://1password.com/blog/from-magic-to-malware-how-openclaws-agent-skills-become-an-attack-surface"&gt;https://1password.com/blog/from-magic-to-malware-how-openclaws-agent-skills-become-an-attack-surface&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FPham"&gt; /u/FPham &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qxrogr/a_topdownloaded_openclaw_skill_is_actually_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qxrogr/a_topdownloaded_openclaw_skill_is_actually_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qxrogr/a_topdownloaded_openclaw_skill_is_actually_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-06T19:41:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1qy3tr8</id>
    <title>An ode to Minimax m2.1</title>
    <updated>2026-02-07T04:11:39+00:00</updated>
    <author>
      <name>/u/Thrumpwart</name>
      <uri>https://old.reddit.com/user/Thrumpwart</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just wanted to share my experience with Minimax m2.1 Specifically the Minimax m2.1 4-bit DWQ MLX quant. &lt;/p&gt; &lt;p&gt;I do alot of research, analysis, and synthesis of various papers and architectural components. To date, no other model has been able to touch this model and quant on my hardware (running on an M2 Ultra Mac Studio).&lt;/p&gt; &lt;p&gt;From depth of knowledge, directness, lack of sycophancy, intelligence, tone, and speed this model and quant is a godsend for my work.&lt;/p&gt; &lt;p&gt;The reasoning is concise - it doesn't ramble for thousands of tokens. It's quick, on point, and logical.&lt;/p&gt; &lt;p&gt;For agentic coding it's very good. It follows instructions well, has a 196k context window, and is proficient with every coding language I've tried.&lt;/p&gt; &lt;p&gt;I've used hundreds of local models of many different sizes, and this is the one I keep coming back to. For academic and LLM-centric research it's smart as hell. It doesn't glaze me, and it doesn't ramble.&lt;/p&gt; &lt;p&gt;I don't know if any other quants are this good, but I feel like I stumbled upon a hidden gem here and wanted to share.&lt;/p&gt; &lt;p&gt;Edit: I'm using Temp = 1.0, top_p = 0.95, top_k = 40 as per the &lt;a href="https://huggingface.co/MiniMaxAI/MiniMax-M2.1"&gt;HF page.&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thrumpwart"&gt; /u/Thrumpwart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qy3tr8/an_ode_to_minimax_m21/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qy3tr8/an_ode_to_minimax_m21/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qy3tr8/an_ode_to_minimax_m21/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-07T04:11:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1qxqpdz</id>
    <title>GLM 5 Is Being Tested On OpenRouter</title>
    <updated>2026-02-06T19:05:23+00:00</updated>
    <author>
      <name>/u/Few_Painter_5588</name>
      <uri>https://old.reddit.com/user/Few_Painter_5588</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qxqpdz/glm_5_is_being_tested_on_openrouter/"&gt; &lt;img alt="GLM 5 Is Being Tested On OpenRouter" src="https://preview.redd.it/6cbhnbxe9xhg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=de924647f1b7c78ba0a6f6653bc5bd25c424af9e" title="GLM 5 Is Being Tested On OpenRouter" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Few_Painter_5588"&gt; /u/Few_Painter_5588 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6cbhnbxe9xhg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qxqpdz/glm_5_is_being_tested_on_openrouter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qxqpdz/glm_5_is_being_tested_on_openrouter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-06T19:05:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1qxpf86</id>
    <title>[Release] Experimental Model with Subquadratic Attention: 100 tok/s @ 1M context, 76 tok/s @ 10M context (30B model, single GPU)</title>
    <updated>2026-02-06T18:19:46+00:00</updated>
    <author>
      <name>/u/Sad-Size2723</name>
      <uri>https://old.reddit.com/user/Sad-Size2723</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;Last week I shared preliminary results on a new subquadratic attention mechanism (&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1qol3s5/preliminary_new_subquadratic_attention_20k_toks"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1qol3s5/preliminary_new_subquadratic_attention_20k_toks&lt;/a&gt;). Following up with the full release: model + inference code are now available.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;: 30B model achieving O(L^(3/2)) scaling instead of O(L^2). Enables 1M‚Äì10M context on a single GPU with decode speeds that stay practical even at extreme context lengths. Ships with an OpenAI-compatible server and CLI to try out.&lt;/p&gt; &lt;p&gt;- ü§ó &lt;strong&gt;Model&lt;/strong&gt;: &lt;a href="https://huggingface.co/concavity-ai/superlinear-exp-v0.1"&gt;https://huggingface.co/concavity-ai/superlinear-exp-v0.1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- üíª &lt;strong&gt;Code&lt;/strong&gt;: &lt;a href="https://github.com/concavity-ai/superlinear"&gt;https://github.com/concavity-ai/superlinear&lt;/a&gt; (`pip install superlinear`)&lt;/p&gt; &lt;p&gt;- üìÑ &lt;strong&gt;Paper&lt;/strong&gt;: &lt;a href="https://arxiv.org/abs/2601.18401"&gt;https://arxiv.org/abs/2601.18401&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Main Idea&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;You can think of attention as a search algorithm to find relevant information for next-token prediction. Standard attention is basically O(L) brute-force search. We're doing O(L^0.5) jump-search with learned routing: score O(L^0.5) candidate spans, select top-k, then do token-level attention within the selected spans.&lt;/p&gt; &lt;p&gt;This gives &lt;strong&gt;O(L^(3/2)) total complexity&lt;/strong&gt; while preserving &lt;strong&gt;random context access&lt;/strong&gt; ‚Äî any token can be selected by content-dependent routing, unlike fixed sliding windows. When you 10x the context length, the search budget only grows by ~3.2x. That subquadratic scaling really matters for long context.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Performance (Single B200 GPU)&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;| Context Length | Prefill (tok/s) | Decode (tok/s) | Memory | |----------------|-----------------|----------------|---------| | 1M tokens | ~20,202 | ~109 | 66 GB | | 10M tokens | ~5,576 | ~76 | ~120 GB | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Key point: 1M ‚Üí 10M context (10x increase) only drops decode speed by ~30%, not the 10x slowdown with dense attention.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why This Matters&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;When you have fast long-context inference, usage patterns change. The key is &lt;strong&gt;maintaining the cache&lt;/strong&gt; instead of reprocessing everything:&lt;/p&gt; &lt;p&gt;- &lt;strong&gt;&lt;em&gt;Almost-infinite chat&lt;/em&gt;&lt;/strong&gt;: KV cache in memory for instant responses, save/restore sessions to disk for persistence&lt;/p&gt; &lt;p&gt;- &lt;strong&gt;&lt;em&gt;Document Q&amp;amp;A&lt;/em&gt;&lt;/strong&gt;: Load documents once, ask cross-document questions without reprocessing (our GitHub example: 8 Wikipedia articles with cross-document reasoning)&lt;/p&gt; &lt;p&gt;- &lt;strong&gt;&lt;em&gt;Long-form generation&lt;/em&gt;&lt;/strong&gt;: 20k+ token reasoning on difficult math problems and coherent long article writing, all with maintained context&lt;/p&gt; &lt;p&gt;Early results: perfect NIAH at 512K context (up from 256K last week), cross-document reasoning working, subquadratic scaling working in practice.&lt;/p&gt; &lt;p&gt;Since no existing inference engine is going to support our custom kernels, we built the full stack ourselves: Triton kernels, OpenAI-compatible server, session snapshots, chunked prefill, CLI with BM25 RAG.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Limitations &amp;amp; Next Steps&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Current limitations:&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- This is an **architecture + systems feasibility release**, not production-quality&lt;/p&gt; &lt;p&gt;- Limited training data (initial SFT only)&lt;/p&gt; &lt;p&gt;- Comprehensive evals beyond NIAH still needed&lt;/p&gt; &lt;p&gt;- FP16 only (66GB for 1M context) ‚Äî quantization coming soon&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Quantization&lt;/em&gt;&lt;/strong&gt; &lt;strong&gt;(coming soon):&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- 4-bit/8-bit quantization to run 1M context on 24GB consumer GPUs&lt;/p&gt; &lt;p&gt;- Target: RTX 4090 / RTX 5090 with full 1M context&lt;/p&gt; &lt;p&gt;- 2M context on 48GB cards (e.g., RTX 6000 Ada)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Hardware support:&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- Currently CUDA only (B200, RTX 6000 Blackwell tested)&lt;/p&gt; &lt;p&gt;- AMD ROCm port coming (Triton kernels should make this straightforward)&lt;/p&gt; &lt;p&gt;- Eventually Apple Silicon (harder but not impossible)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Training &amp;amp; Quality improvements:&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- Scaling up SFT data with more long-context examples&lt;/p&gt; &lt;p&gt;- Potentially doing continued pretraining on long documents&lt;/p&gt; &lt;p&gt;- Expanding perfect NIAH range beyond 512K&lt;/p&gt; &lt;p&gt;- Real-world long-context benchmarks (book QA, codebase analysis, multi-document reasoning)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;New end-user applications&lt;/em&gt;&lt;/strong&gt;: We are planning to develop local-first end-user applications based on this. What would you actually use long context for? Would love to hear specific use cases to help us prioritize.&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;Trying something new is extremely hard. Everyone likes existing transformer architectures ‚Äî optimizations at every level, predictable scaling laws. But to make truly long-context models practical on local hardware, I think we need new ideas. It doesn't hurt to try, right?&lt;/p&gt; &lt;p&gt;I'm trying not to spam this sub, so the GitHub repo is the best place to follow progress. Happy to answer questions here though! If you try it and hit issues, open a GitHub issue. And if you have thoughts on long-context use cases, I'd love to hear them.&lt;/p&gt; &lt;p&gt;Thanks for all the encouragement on the last post!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;- ü§ó &lt;strong&gt;Model&lt;/strong&gt;: &lt;a href="https://huggingface.co/concavity-ai/superlinear-exp-v0.1"&gt;https://huggingface.co/concavity-ai/superlinear-exp-v0.1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- üíª &lt;strong&gt;Code&lt;/strong&gt;: &lt;a href="https://github.com/concavity-ai/superlinear"&gt;https://github.com/concavity-ai/superlinear&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- üìÑ &lt;strong&gt;Paper&lt;/strong&gt;: &lt;a href="https://arxiv.org/abs/2601.18401"&gt;https://arxiv.org/abs/2601.18401&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sad-Size2723"&gt; /u/Sad-Size2723 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qxpf86/release_experimental_model_with_subquadratic/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qxpf86/release_experimental_model_with_subquadratic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qxpf86/release_experimental_model_with_subquadratic/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-06T18:19:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1qycn5s</id>
    <title>DeepSeek-V2-Lite vs GPT-OSS-20B on my 2018 potato i3-8145U + UHD 620, OpenVINO Comparison.</title>
    <updated>2026-02-07T12:32:28+00:00</updated>
    <author>
      <name>/u/RelativeOperation483</name>
      <uri>https://old.reddit.com/user/RelativeOperation483</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qycn5s/deepseekv2lite_vs_gptoss20b_on_my_2018_potato/"&gt; &lt;img alt="DeepSeek-V2-Lite vs GPT-OSS-20B on my 2018 potato i3-8145U + UHD 620, OpenVINO Comparison." src="https://b.thumbs.redditmedia.com/HDuzkmv1h50aT7P7iyAduUT2TYW4g84iH4RYq4kv6Hg.jpg" title="DeepSeek-V2-Lite vs GPT-OSS-20B on my 2018 potato i3-8145U + UHD 620, OpenVINO Comparison." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Same potato, new test. If you saw my last post, you will catch this up. I run LLMs on a &lt;strong&gt;2018 HP ProBook 8th Gen i3 with no Nvidia, no dedicated GPU&lt;/strong&gt;, just hope and an OpenVINO backend. This time I wanted to see how two MoE models compare head to head on the exact same hardware, same questions, same settings, same everything.&lt;/p&gt; &lt;p&gt;Same 10 questions for both models. Logic, health, history, coding, creative writing, factual biography, math, tech explainer, ethics, food science. Wide spread of topics to stress test general capability.&lt;/p&gt; &lt;p&gt;Each model was tested 3 times, each time running all 10 questions on CPU first then on iGPU with 1 layer offloaded. So that is 10 questions x 3 runs = 30 samples per device per model. 120 total inference runs. Same context (4096), same max output (256 tokens), same temperature (0.2), same top_p (0.9). Identical conditions.&lt;/p&gt; &lt;p&gt;&lt;em&gt;THE SPEED&lt;/em&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;DeepSeek-V2-Lite absolutely smoked GPT-OSS. Almost 2x faster across the board.&lt;/li&gt; &lt;li&gt;DeepSeek on CPU: 7.93 tok/s average, TTFT 2.36s&lt;/li&gt; &lt;li&gt;DeepSeek on iGPU: 8.08 tok/s average, TTFT 1.86s&lt;/li&gt; &lt;li&gt;Peak decode: 8.28 tok/s (iGPU) ‚Äî Lowest: 5.50 tok/s (CPU, cold start Q1)&lt;/li&gt; &lt;li&gt;GPT-OSS on CPU: 4.20 tok/s average, TTFT 3.13s&lt;/li&gt; &lt;li&gt;GPT-OSS on iGPU: 4.36 tok/s average, TTFT 3.07s&lt;/li&gt; &lt;li&gt;Peak decode: 4.46 tok/s (CPU) ‚Äî Lowest: 3.18 tok/s (CPU, two questions got stuck slow)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In real time, DeepSeek finishes a 256-token response in about 32 seconds. GPT-OSS takes over a minute. That is the difference between usable and painful on a slow machine. The iGPU helped DeepSeek more than GPT-OSS. DeepSeek's time to first token dropped 21% on iGPU (from 2.36s to 1.86s). GPT-OSS barely changed. So if you are on iGPU, the smaller active parameter count benefits more from that little offload. (Just my opinion) &lt;/p&gt; &lt;p&gt;&lt;em&gt;THE QUALITY (I read every single response)&lt;/em&gt;&lt;/p&gt; &lt;p&gt;I went through all the outputs manually. Not vibes, actually reading them.&lt;/p&gt; &lt;p&gt;DeepSeek-V2-Lite: 7.5 out of 10&lt;/p&gt; &lt;p&gt;Very consistent. Clean structured answers. Good at health, history, math, tech explainers, ethics, food science. Wrote a complete cyberpunk poem. Solid Magna Carta summary. Nailed the Golden Ratio with three nature examples. Good VPN envelope analogy. Maillard reaction explanation was textbook quality.&lt;/p&gt; &lt;p&gt;Weaknesses&lt;br /&gt; But for today, it got the logic question wrong. The classic &amp;quot;All A are B, some B are C, therefore some A are C&amp;quot;. DeepSeek confidently said it is valid. It is not. That is a well-known syllogistic fallacy. Also on the coding question (Tower of Hanoi), &lt;strong&gt;it spent all its tokens explaining the problem and left the actual function as &amp;quot;# Your code here&amp;quot; without writing the implementation. Small factual error in Marie Curie bio (described her heritage incorrectly)&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;GPT-OSS-20B: &lt;strong&gt;2 out of 10&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;When it worked, it was impressive. It correctly identified the logic question as invalid and gave a concrete counterexample with sets to prove it. That was genuinely good reasoning. It also produced a complete working Tower of Hanoi implementation with proper recursion, base case, and example usage. The ethics response on the trolley problem was decent too.&lt;/p&gt; &lt;p&gt;Weaknesses &lt;/p&gt; &lt;p&gt;Hallucinated or broke down on 8 out of 10 questions. And I do not mean subtle errors, I mean full collapse. The health question turned into a loop of &amp;quot;Sure! Here is a revised version of the prompt&amp;quot; repeated over and over without ever answering. The history question started ok then degenerated into repeated &amp;quot;Answer:&amp;quot; blocks and &amp;quot;**...**&amp;quot; until the token limit. The VPN question was the worst ‚Äî it looped &amp;quot;The user is a 3rd person perspective. The user is a 3. The user is a 3.&amp;quot; endlessly. Marie Curie question confused itself trying to summarize events from 2018-2023 for a woman who died in 1934. Golden Ratio collapsed into the same looping pattern. The poem spent all its tokens reasoning about what to write and only managed 4 lines.&lt;/p&gt; &lt;p&gt;This was not random. The same questions broke the same way across all 3 runs. It is a problem, GPT-OSS seems to be a reasoning/thinking model that burns its output budget on internal chain-of-thought and then either never reaches the answer or gets trapped in repetition loops. &lt;strong&gt;With only 256 tokens of output, it simply cannot think AND answer. Caution, I'm not saying Gpt-oss is bad, It can probably be the effect of Q4_K_M.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;DeepSeek-Coder-V2-Lite is the better model for budget hardware if we compare these 2 only. It is faster, more coherent, and way more reliable. &lt;strong&gt;GPT-OSS has flashes of real intelligence (that logic answer was better than what most small models produce)&lt;/strong&gt; but a model that loops on 8 out of 10 questions is not usable for anything practical at Q4_K_M. &lt;strong&gt;GPT-OSS might do better with higher max_tokens, and higher quantization.&lt;/strong&gt; I only tested Q4_K_M at 256 max output. If someone with better hardware wants to test it with more ram, more higher specs, Go for it. &lt;/p&gt; &lt;p&gt;I attached some screenshots in this post. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RelativeOperation483"&gt; /u/RelativeOperation483 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qycn5s"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qycn5s/deepseekv2lite_vs_gptoss20b_on_my_2018_potato/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qycn5s/deepseekv2lite_vs_gptoss20b_on_my_2018_potato/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-07T12:32:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1qydlwi</id>
    <title>Potential new Qwen and ByteDance Seed models are being tested on the Arena. The ‚ÄúKarp-001‚Äù and ‚ÄúKarp-002‚Äù models claim to be Qwen-3.5 models. The ‚ÄúPisces-llm-0206a‚Äù and ‚ÄúPisces-llm-0206b‚Äù models claim to be ByteDance models.</title>
    <updated>2026-02-07T13:19:21+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qydlwi/potential_new_qwen_and_bytedance_seed_models_are/"&gt; &lt;img alt="Potential new Qwen and ByteDance Seed models are being tested on the Arena. The ‚ÄúKarp-001‚Äù and ‚ÄúKarp-002‚Äù models claim to be Qwen-3.5 models. The ‚ÄúPisces-llm-0206a‚Äù and ‚ÄúPisces-llm-0206b‚Äù models claim to be ByteDance models." src="https://preview.redd.it/rtrygqo1p2ig1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9704e4c75927f5669c01b711e9c25a0d47ce44bb" title="Potential new Qwen and ByteDance Seed models are being tested on the Arena. The ‚ÄúKarp-001‚Äù and ‚ÄúKarp-002‚Äù models claim to be Qwen-3.5 models. The ‚ÄúPisces-llm-0206a‚Äù and ‚ÄúPisces-llm-0206b‚Äù models claim to be ByteDance models." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rtrygqo1p2ig1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qydlwi/potential_new_qwen_and_bytedance_seed_models_are/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qydlwi/potential_new_qwen_and_bytedance_seed_models_are/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-07T13:19:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1qy5xnn</id>
    <title>Kimi-Linear-48B-A3B &amp; Step3.5-Flash are ready - llama.cpp</title>
    <updated>2026-02-07T05:59:11+00:00</updated>
    <author>
      <name>/u/pmttyji</name>
      <uri>https://old.reddit.com/user/pmttyji</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Below are actual releases for both models. Anyway get &lt;a href="https://github.com/ggml-org/llama.cpp/releases"&gt;latest version&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Step3.5-Flash&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/releases/tag/b7964"&gt;https://github.com/ggml-org/llama.cpp/releases/tag/b7964&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Kimi-Linear-48B-A3B&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/releases/tag/b7957"&gt;https://github.com/ggml-org/llama.cpp/releases/tag/b7957&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I don't see any new GGUFs( &lt;a href="https://huggingface.co/models?library=gguf&amp;amp;other=base_model:quantized:moonshotai%2FKimi-Linear-48B-A3B-Instruct&amp;amp;sort=created"&gt;Kimi&lt;/a&gt; &amp;amp; &lt;a href="https://huggingface.co/models?library=gguf&amp;amp;other=base_model:quantized:stepfun-ai%2FStep-3.5-Flash&amp;amp;sort=trending"&gt;Step-3.5&lt;/a&gt; ) from our favorite sources yet. Probably today or tomorrow. &lt;/p&gt; &lt;p&gt;But ik_llama folks got GGUF for &lt;a href="https://huggingface.co/ubergarm/Step-3.5-Flash-GGUF"&gt;Step-3.5-Flash&lt;/a&gt; by ubergarm.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmttyji"&gt; /u/pmttyji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qy5xnn/kimilinear48ba3b_step35flash_are_ready_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qy5xnn/kimilinear48ba3b_step35flash_are_ready_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qy5xnn/kimilinear48ba3b_step35flash_are_ready_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-07T05:59:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1qy0l26</id>
    <title>Nemo 30B is insane. 1M+ token CTX on one 3090</title>
    <updated>2026-02-07T01:39:58+00:00</updated>
    <author>
      <name>/u/Dismal-Effect-1914</name>
      <uri>https://old.reddit.com/user/Dismal-Effect-1914</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been playing around with llama.cpp and some 30-80B parameter models with CPU offloading. Currently have one 3090 and 32 GB of RAM. Im very impressed by Nemo 30B. 1M+ Token Context cache, runs on one 3090, CPU offloading for experts. Does 35 t/s which is faster than I can read at least. Usually slow as fuck at this large a context window. Feed it a whole book or research paper and its done summarizing in like a few mins. This really makes long context windows on local hardware possible. The only other contender I have tried is Seed OSS 36b and it was much slower by about 20 tokens.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dismal-Effect-1914"&gt; /u/Dismal-Effect-1914 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qy0l26/nemo_30b_is_insane_1m_token_ctx_on_one_3090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qy0l26/nemo_30b_is_insane_1m_token_ctx_on_one_3090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qy0l26/nemo_30b_is_insane_1m_token_ctx_on_one_3090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-07T01:39:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
