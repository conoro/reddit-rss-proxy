<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-15T14:56:29+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1pn4bt6</id>
    <title>Has anyone tried Deepseek v3.2 speciale in q2? And what about kimi k2 thinking q1.58?</title>
    <updated>2025-12-15T10:41:31+00:00</updated>
    <author>
      <name>/u/power97992</name>
      <uri>https://old.reddit.com/user/power97992</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have used both at higher quants, they are good. How useable is v3.2 speciale q2 for coding and math and general knowledge? And Kimi K2 thinking q1.58? How do they compare to GLm 4.6 q4 and Minimax m2 q6-q8, qwen 3 next 80b q8 and qwen3 235 b a22b VL q4-q6 and glm 4.5 air q8? I read q3 glm 4.6 is better than glm 4.5 air. Actually i cant even find a gguf or mlx Q2 version of speciale or base 3.2 on hugginface. Imagine q1.58 will have low quality, same was with q2 speciale&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/power97992"&gt; /u/power97992 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn4bt6/has_anyone_tried_deepseek_v32_speciale_in_q2_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn4bt6/has_anyone_tried_deepseek_v32_speciale_in_q2_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pn4bt6/has_anyone_tried_deepseek_v32_speciale_in_q2_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T10:41:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1pn5yp5</id>
    <title>Best workflow to convert a long PDF book into clean Markdown for Obsidian (using AI, no hallucinations)?</title>
    <updated>2025-12-15T12:18:31+00:00</updated>
    <author>
      <name>/u/MilkManViking</name>
      <uri>https://old.reddit.com/user/MilkManViking</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm trying to convert a full length PDF book (300+ pages) into clean, structured Markdown for Obsidian, and I‚Äôm looking for advice on the best &lt;em&gt;workflow&lt;/em&gt;, not quick hacks.&lt;/p&gt; &lt;p&gt;What I care about:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Preserve original wording exactly (no paraphrasing or ‚ÄúAI smoothing‚Äù)&lt;/li&gt; &lt;li&gt;Proper Markdown structure (&lt;code&gt;#&lt;/code&gt; for sections, &lt;code&gt;##&lt;/code&gt; chapters, paragraphs restored)&lt;/li&gt; &lt;li&gt;Fix OCR garbage (broken line breaks, hyphenation, duplicated headers)&lt;/li&gt; &lt;li&gt;Obsidian-friendly output (outline view, folding, search)&lt;/li&gt; &lt;li&gt;Ability to verify against the original PDF&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What I‚Äôve tried / considered:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Copy-paste from PDF ‚Üí messy OCR text&lt;/li&gt; &lt;li&gt;AI to normalize formatting only (not rewrite content)&lt;/li&gt; &lt;li&gt;Page-by-page or chunk-by-chunk processing to avoid hallucinations&lt;/li&gt; &lt;li&gt;Manual spot-checking against the PDF&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What I‚Äôm &lt;em&gt;not&lt;/em&gt; looking for:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;‚ÄúJust summarize it‚Äù&lt;/li&gt; &lt;li&gt;‚ÄúJust ask ChatGPT to rewrite it‚Äù&lt;/li&gt; &lt;li&gt;Tools that alter wording or structure unpredictably&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Questions:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Do you process PDFs page-by-page or chapter-by-chapter?&lt;/li&gt; &lt;li&gt;Any Obsidian plugins or external tools that help with PDF ‚Üí Markdown cleanup?&lt;/li&gt; &lt;li&gt;Has anyone built a reliable AI + OCR pipeline that preserves fidelity?&lt;/li&gt; &lt;li&gt;Any gotchas to avoid with long books?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;If you‚Äôve done something similar and ended up with a Markdown file you actually trust, I‚Äôd love to hear your setup.&lt;/p&gt; &lt;p&gt;Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MilkManViking"&gt; /u/MilkManViking &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn5yp5/best_workflow_to_convert_a_long_pdf_book_into/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn5yp5/best_workflow_to_convert_a_long_pdf_book_into/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pn5yp5/best_workflow_to_convert_a_long_pdf_book_into/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T12:18:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1pn5n4c</id>
    <title>Natural language file search using local tiny LLMs (&lt;1b): Model recommendations needed!</title>
    <updated>2025-12-15T12:01:11+00:00</updated>
    <author>
      <name>/u/fuckAIbruhIhateCorps</name>
      <uri>https://old.reddit.com/user/fuckAIbruhIhateCorps</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn5n4c/natural_language_file_search_using_local_tiny/"&gt; &lt;img alt="Natural language file search using local tiny LLMs (&amp;lt;1b): Model recommendations needed!" src="https://b.thumbs.redditmedia.com/pC7bDoEpEFRRQICuldgjYEq-RYLXo6wJxU_b93zdqmo.jpg" title="Natural language file search using local tiny LLMs (&amp;lt;1b): Model recommendations needed!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/am0arwvgxc7g1.png?width=1652&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1bab77de3f1b6cd65e5639777f94497e8c25b006"&gt;https://preview.redd.it/am0arwvgxc7g1.png?width=1652&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1bab77de3f1b6cd65e5639777f94497e8c25b006&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hi guys, this is kind of a follow-up to my monkeSearch post, but now I am focusing on the non vector-db implementation again. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;What I'm building:&lt;/strong&gt; A local natural language file search engine that parses queries like &amp;quot;python scripts from 3 days ago&amp;quot; or &amp;quot;images from last week&amp;quot; and extracts the file types and temporal info to build actual file system queries.&lt;br /&gt; In testing, it works well. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Current approach:&lt;/strong&gt; I'm using Qwen3 0.6B (Q8) with llama.cpp's structured output to parse queries into JSON. (using llama.cpp's structured json schema mode)&lt;/p&gt; &lt;p&gt;I've built a test suite with 30 different test queries in my script and Qwen 0.6B is surprisingly decent at this (24/30), but I'm hitting some accuracy issues with edge cases.&lt;/p&gt; &lt;p&gt;Check out the code to understand further: &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/monkesearch/monkeSearch/tree/legacy-main-llm-implementation"&gt;https://github.com/monkesearch/monkeSearch/tree/legacy-main-llm-implementation&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The project page: &lt;a href="https://monkesearch.github.io"&gt;https://monkesearch.github.io&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The question: What's the best path forward for this specific use case?&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Stick with tiny LLMs (&amp;lt;1B) and possibly fine-tuning?&lt;/li&gt; &lt;li&gt;Move to slightly bigger LLMs (1-3B range) - if so, what models would you recommend that are good at structured output and instruction following?&lt;/li&gt; &lt;li&gt;Build a custom architecture specifically for query parsing (maybe something like a BERT-style encoder trained specifically for this task)?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Constraints:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Must run on potato PCs (aiming for 4-8GB RAM max)&lt;/li&gt; &lt;li&gt;Needs to be FAST (&amp;lt;100ms inference ideally)&lt;/li&gt; &lt;li&gt;No data leaves the machine&lt;/li&gt; &lt;li&gt;Structured JSON output is critical (can't deal with too much hallucination)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I am leaning towards the tiny LLM option and would love to get opinions for local models to try and play with, please recommend some models! I tried local inference for LG-AI EXAONE model but faced some issues with the chat template.&lt;/p&gt; &lt;p&gt;If someone has experience with custom models and training them, let's work together!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fuckAIbruhIhateCorps"&gt; /u/fuckAIbruhIhateCorps &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn5n4c/natural_language_file_search_using_local_tiny/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn5n4c/natural_language_file_search_using_local_tiny/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pn5n4c/natural_language_file_search_using_local_tiny/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T12:01:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1pn85kn</id>
    <title>Building a 'digital me' - which models don't drift into AI assistant mode?</title>
    <updated>2025-12-15T14:04:40+00:00</updated>
    <author>
      <name>/u/Proud-Journalist-611</name>
      <uri>https://old.reddit.com/user/Proud-Journalist-611</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone üëã&lt;/p&gt; &lt;p&gt;So I've been going down this rabbit hole for a while now and I'm kinda stuck. Figured I'd ask here before I burn more compute.&lt;/p&gt; &lt;p&gt;What I'm trying to do:&lt;/p&gt; &lt;p&gt;Build a local model that sounds like me - my texting style, how I actually talk to friends/family, my mannerisms, etc. Not trying to make a generic chatbot. I want something where if someone texts &amp;quot;my&amp;quot; AI, they wouldn't be able to tell the difference. Yeah I know, ambitious af.&lt;/p&gt; &lt;p&gt;What I'm working with:&lt;/p&gt; &lt;p&gt;5090 FE (so I can run 8B models comfortably, maybe 12B quantized)&lt;/p&gt; &lt;p&gt;~47,000 raw messages from WhatsApp + iMessage going back years&lt;/p&gt; &lt;p&gt;After filtering for quality, I'm down to about 2,400 solid examples&lt;/p&gt; &lt;p&gt;What I've tried so far:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;LLaMA 2 7B Chat + LoRA fine-tuning - This was my first attempt. The model learns something but keeps slipping back into &amp;quot;helpful assistant&amp;quot; mode. Like it'll respond to a casual &amp;quot;what's up&amp;quot; with a paragraph about how it can help me today üôÑ&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Multi-stage data filtering pipeline - Built a whole system: rule-based filters ‚Üí soft scoring ‚Üí LLM validation (ran everything through GPT-4o and Claude). Thought better data = better output. It helped, but not enough.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Length calibration - Noticed my training data had varying response lengths but the model always wanted to be verbose. Tried filtering for shorter responses + synthetic short examples. Got brevity but lost personality.&lt;/p&gt; &lt;p&gt;Personality marker filtering - Pulled only examples with my specific phrases, emoji patterns, etc. Still getting AI slop in the outputs.&lt;/p&gt; &lt;p&gt;The core problem:&lt;/p&gt; &lt;p&gt;No matter what I do, the base model's &amp;quot;assistant DNA&amp;quot; bleeds through. It uses words I'd never use (&amp;quot;certainly&amp;quot;, &amp;quot;I'd be happy to&amp;quot;, &amp;quot;feel free to&amp;quot;). The responses are technically fine but they don't feel like me.&lt;/p&gt; &lt;p&gt;What I'm looking for:&lt;/p&gt; &lt;p&gt;Models specifically designed for roleplay/persona consistency (not assistant behavior)&lt;/p&gt; &lt;p&gt;Anyone who's done something similar - what actually worked?&lt;/p&gt; &lt;p&gt;Base models vs instruct models for this use case? Any merges or fine-tunes that are known for staying in character?&lt;/p&gt; &lt;p&gt;I've seen some mentions of Stheno, Lumimaid, and some &amp;quot;anti-slop&amp;quot; models but there's so many options I don't know where to start. Running locally is a must.&lt;/p&gt; &lt;p&gt;If anyone's cracked this or even gotten close, I'd love to hear what worked. Happy to share more details about my setup/pipeline if helpful.&lt;/p&gt; &lt;p&gt;Thanks üôè&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Proud-Journalist-611"&gt; /u/Proud-Journalist-611 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn85kn/building_a_digital_me_which_models_dont_drift/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn85kn/building_a_digital_me_which_models_dont_drift/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pn85kn/building_a_digital_me_which_models_dont_drift/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T14:04:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1pn238p</id>
    <title>Last Week in Multimodal AI - Local Edition</title>
    <updated>2025-12-15T08:08:17+00:00</updated>
    <author>
      <name>/u/Vast_Yak_4147</name>
      <uri>https://old.reddit.com/user/Vast_Yak_4147</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn238p/last_week_in_multimodal_ai_local_edition/"&gt; &lt;img alt="Last Week in Multimodal AI - Local Edition" src="https://b.thumbs.redditmedia.com/2PD0Z4-3hlpvDQXDRaHyVAryNDaWywuVBFVuj7BFLXk.jpg" title="Last Week in Multimodal AI - Local Edition" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I curate a weekly newsletter on multimodal AI. Here are the local/open-source highlights from this week:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Apriel-1.6-15B-Thinker - Frontier Reasoning at 15B&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Scores 57 on Intelligence Index, matching 200B-scale models while remaining an order of magnitude smaller.&lt;/li&gt; &lt;li&gt;Self-hostable multimodal reasoning without compromising performance.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/ServiceNow-AI/Apriel-1.6-15b-Thinker"&gt;Model&lt;/a&gt; | &lt;a href="https://huggingface.co/blog/ServiceNow-AI/apriel-1p6-15b-thinker"&gt;Blog&lt;/a&gt; | &lt;a href="https://huggingface.co/spaces/ServiceNow-AI/Apriel-Chat"&gt;Demo&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/y2dx42fkrb7g1.jpg?width=800&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=20e12cfa824805f172f0abd47a074be08ea32b1a"&gt;https://preview.redd.it/y2dx42fkrb7g1.jpg?width=800&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=20e12cfa824805f172f0abd47a074be08ea32b1a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GLM-4.6V - 128K Context Multimodal&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Open-source multimodal model with tool-calling support and 128K context window.&lt;/li&gt; &lt;li&gt;Handles vision-language tasks with native tool integration for API development.&lt;/li&gt; &lt;li&gt;&lt;a href="https://z.ai/blog/glm-4.6v"&gt;Blog&lt;/a&gt; | &lt;a href="https://github.com/zai-org/GLM-V"&gt;GitHub&lt;/a&gt; | &lt;a href="https://chat.z.ai/"&gt;Demo&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/focypmxrrb7g1.jpg?width=10101&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=3b13f1cb191778838cc1e60577fc2856254723ad"&gt;https://preview.redd.it/focypmxrrb7g1.jpg?width=10101&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=3b13f1cb191778838cc1e60577fc2856254723ad&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1pn238p/video/zi335bxsrb7g1/player"&gt;https://reddit.com/link/1pn238p/video/zi335bxsrb7g1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;AutoGLM - Open-Source Phone Agent&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Completes Android tasks through natural language commands.&lt;/li&gt; &lt;li&gt;AutoGLM-Phone-9B available for download and self-hosting.&lt;/li&gt; &lt;li&gt;&lt;a href="https://xiao9905.github.io/AutoGLM/"&gt;Website&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1pn238p/video/qcbwhgburb7g1/player"&gt;https://reddit.com/link/1pn238p/video/qcbwhgburb7g1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;DMVAE - State-of-the-Art VAE&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Matches latent distributions to any reference with fewer training epochs.&lt;/li&gt; &lt;li&gt;Open-source implementation achieving SOTA image synthesis.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/papers/2512.07778"&gt;Paper&lt;/a&gt; | &lt;a href="https://huggingface.co/sen-ye/dmvae/tree/main"&gt;Model&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/aai6puuwrb7g1.jpg?width=692&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=c3b7accc71868c514e36841b44ea8bf171fdf730"&gt;https://preview.redd.it/aai6puuwrb7g1.jpg?width=692&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=c3b7accc71868c514e36841b44ea8bf171fdf730&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Qwen-Image-i2L - Single Image to Custom LoRA&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;First open-source tool converting one image into a custom LoRA.&lt;/li&gt; &lt;li&gt;Enables personalized generation from minimal data.&lt;/li&gt; &lt;li&gt;&lt;a href="https://modelscope.cn/models/DiffSynth-Studio/Qwen-Image-i2L/summary"&gt;ModelScope&lt;/a&gt; | &lt;a href="https://github.com/modelscope/DiffSynth-Studio/blob/main/examples/qwen_image/model_inference_low_vram/Qwen-Image-i2L.py"&gt;Code&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8qawc8eyrb7g1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=96e6fd90eacfe70b759be421960b827a66dabb6f"&gt;https://preview.redd.it/8qawc8eyrb7g1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=96e6fd90eacfe70b759be421960b827a66dabb6f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Dolphin-v2 - Universal Document Parser&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;3B parameter model that parses any document type.&lt;/li&gt; &lt;li&gt;Efficient document understanding at small scale.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/ByteDance/Dolphin-v2"&gt;Hugging Face&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;X-VLA - Unified Robot Control&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Soft-prompted transformer controlling different robot types with one interface.&lt;/li&gt; &lt;li&gt;Open-source approach to cross-platform robotics.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/docs/lerobot/en/xvla"&gt;Docs&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/vkb5a833sb7g1.png?width=900&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8fa2713c8ce4105b702643a4106cee2d3dd592d9"&gt;https://preview.redd.it/vkb5a833sb7g1.png?width=900&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8fa2713c8ce4105b702643a4106cee2d3dd592d9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Checkout the &lt;a href="https://open.substack.com/pub/thelivingedge/p/last-week-in-multimodal-ai-37-less?utm_campaign=post-expanded-share&amp;amp;utm_medium=web"&gt;full newsletter&lt;/a&gt; for more demos, papers, and resources.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vast_Yak_4147"&gt; /u/Vast_Yak_4147 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn238p/last_week_in_multimodal_ai_local_edition/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn238p/last_week_in_multimodal_ai_local_edition/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pn238p/last_week_in_multimodal_ai_local_edition/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T08:08:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmpwmh</id>
    <title>2025 Open Models Year in Review</title>
    <updated>2025-12-14T21:43:46+00:00</updated>
    <author>
      <name>/u/robotphilanthropist</name>
      <uri>https://old.reddit.com/user/robotphilanthropist</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmpwmh/2025_open_models_year_in_review/"&gt; &lt;img alt="2025 Open Models Year in Review" src="https://external-preview.redd.it/teWgyqb-RDJIuhi9RJ3H3WjTJdfmLNMEjYONqxc6ag8.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3f9c5e72e6317c507adece143fd29a8a92eb26cc" title="2025 Open Models Year in Review" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Florian and I worked hard to follow what's happening this year. We put together our final year in review. It's focused on people training models end to end and our rankings downweigh noncommercial licenses and other restrictions that make using models below. A summary is in the text here.&lt;/p&gt; &lt;p&gt;What a year! We're back with an updated open model builder tier list, our top models of the year, and our predictions for 2026.&lt;/p&gt; &lt;p&gt;First, the winning models:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;DeepSeek R1: Transformed the AI world&lt;/li&gt; &lt;li&gt;Qwen 3 Family: The new default open models &lt;/li&gt; &lt;li&gt;Kimi K2 Family: Models that convinced the world that DeepSeek wasn't special and China would produce numerous leading models.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Runner up models: MiniMax M2, GLM 4.5, GPT-OSS, Gemma 3, Olmo 3&lt;/p&gt; &lt;p&gt;Honorable Mentions: Nvidia's Parakeet speech-to-text model &amp;amp; Nemotron 2 LLM, Moondream 3 VLM, Granite 4 LLMs, and HuggingFace's SmolLM3.&lt;/p&gt; &lt;p&gt;Tier list:&lt;/p&gt; &lt;p&gt;Frontier open labs: DeepSeek, Qwen, and Kimi Moonshot&lt;/p&gt; &lt;p&gt;Close behind: &lt;a href="http://Z.ai"&gt;Z.ai&lt;/a&gt; &amp;amp; MiniMax AI (notably none from the U.S.)&lt;/p&gt; &lt;p&gt;Noteworthy (a mix of US &amp;amp; China): StepFun AI, Ant Group's Inclusion AI, Meituan, Tencent, IBM, Nvidia, Google, &amp;amp; Mistral &lt;/p&gt; &lt;p&gt;Then a bunch more below that, which we detail.&lt;/p&gt; &lt;p&gt;Predictions for 2026:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Scaling will continue with open models.&lt;/li&gt; &lt;li&gt;No substantive changes in the open model safety narrative.&lt;/li&gt; &lt;li&gt;Participation will continue to grow.&lt;/li&gt; &lt;li&gt;Ongoing general trends will continue w/ MoEs, hybrid attention, dense for fine-tuning.&lt;/li&gt; &lt;li&gt;The open and closed frontier gap will stay roughly the same on any public benchmarks.&lt;/li&gt; &lt;li&gt;No Llama-branded open model releases from Meta in 2026.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Very appreciative of this community through both my hats at Interconnects &amp;amp; Ai2.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/robotphilanthropist"&gt; /u/robotphilanthropist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.interconnects.ai/p/2025-open-models-year-in-review"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmpwmh/2025_open_models_year_in_review/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pmpwmh/2025_open_models_year_in_review/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-14T21:43:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmfglp</id>
    <title>First AI implosion: Oracle</title>
    <updated>2025-12-14T14:33:45+00:00</updated>
    <author>
      <name>/u/Terminator857</name>
      <uri>https://old.reddit.com/user/Terminator857</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Post says first domino to fall will be Oracle: &lt;a href="https://x.com/shanaka86/status/2000057734419620155"&gt;https://x.com/shanaka86/status/2000057734419620155&lt;/a&gt;&lt;/p&gt; &lt;p&gt;After the implosion we should get our cheap memory back. I doubt this ram shortage is going to last as long as the chip shortage for cars. That one was 18 months. What do think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Terminator857"&gt; /u/Terminator857 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmfglp/first_ai_implosion_oracle/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmfglp/first_ai_implosion_oracle/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pmfglp/first_ai_implosion_oracle/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-14T14:33:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmxgok</id>
    <title>Interesting new model: Motif-2-12.7B-Reasoning</title>
    <updated>2025-12-15T03:37:29+00:00</updated>
    <author>
      <name>/u/LoveMind_AI</name>
      <uri>https://old.reddit.com/user/LoveMind_AI</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I didn‚Äôt see much discussion of the instruct version, but the reasoning version is out and it sounds like an interesting model. They were not on my radar until recently. Any thoughts? I do think models in this size range seem to look more and more like the future. &lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Motif-Technologies/Motif-2-12.7B-Reasoning"&gt;https://huggingface.co/Motif-Technologies/Motif-2-12.7B-Reasoning&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LoveMind_AI"&gt; /u/LoveMind_AI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmxgok/interesting_new_model_motif2127breasoning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmxgok/interesting_new_model_motif2127breasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pmxgok/interesting_new_model_motif2127breasoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T03:37:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmo0dn</id>
    <title>Qwen3-Next-80B-A3B-Thinking-GGUF has just been released on HuggingFace</title>
    <updated>2025-12-14T20:24:45+00:00</updated>
    <author>
      <name>/u/LegacyRemaster</name>
      <uri>https://old.reddit.com/user/LegacyRemaster</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmo0dn/qwen3next80ba3bthinkinggguf_has_just_been/"&gt; &lt;img alt="Qwen3-Next-80B-A3B-Thinking-GGUF has just been released on HuggingFace" src="https://b.thumbs.redditmedia.com/5GGgkZME9HXSe33YbnL-7M8w7p2MrLazBQlzRPrvN5A.jpg" title="Qwen3-Next-80B-A3B-Thinking-GGUF has just been released on HuggingFace" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/75q6nveva87g1.png?width=1283&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b3b427e21b37b3009dc59534135e4394f375d9f8"&gt;qwen next 80b thinking tetris&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Tested q4_k_m. It did the best Tetris in a single HTML file I've ever seen. I tried Devstral recently and the results weren't as accurate.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Thinking-GGUF"&gt;https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Thinking-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LegacyRemaster"&gt; /u/LegacyRemaster &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmo0dn/qwen3next80ba3bthinkinggguf_has_just_been/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmo0dn/qwen3next80ba3bthinkinggguf_has_just_been/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pmo0dn/qwen3next80ba3bthinkinggguf_has_just_been/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-14T20:24:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1pn4nfj</id>
    <title>Diagnosing layer sensitivity during post training quantization</title>
    <updated>2025-12-15T11:01:44+00:00</updated>
    <author>
      <name>/u/elinaembedl</name>
      <uri>https://old.reddit.com/user/elinaembedl</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn4nfj/diagnosing_layer_sensitivity_during_post_training/"&gt; &lt;img alt="Diagnosing layer sensitivity during post training quantization" src="https://preview.redd.it/9z863k4bnc7g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a3ec905397065d7f20bc37620997e8c9962893e9" title="Diagnosing layer sensitivity during post training quantization" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone!&lt;br /&gt; I wrote about this a while ago. I have written a blog post on using layerwise PSNR to diagnose where models break during post-training quantization.&lt;/p&gt; &lt;p&gt;Instead of only checking output accuracy, layerwise metrics let you spot exactly which layers are sensitive (e.g. softmax, SE blocks), making it easier to debug and decide what to keep in higher precision.&lt;/p&gt; &lt;p&gt;If you‚Äôre experimenting with quantization for local or edge inference, you might find this interesting: &lt;a href="https://hub.embedl.com/blog/diagnosing-layer-sensitivity?utm_source=reddit"&gt;blogpost link&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Has anyone tried similar layerwise diagnostics? I‚Äôd love to hear about your experiences.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/elinaembedl"&gt; /u/elinaembedl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9z863k4bnc7g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn4nfj/diagnosing_layer_sensitivity_during_post_training/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pn4nfj/diagnosing_layer_sensitivity_during_post_training/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T11:01:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1pn0gpa</id>
    <title>Forked Google's Gemini CLI to work with local LLMs (MLX, llama.cpp, vLLM)</title>
    <updated>2025-12-15T06:23:09+00:00</updated>
    <author>
      <name>/u/Honest-Fun-5279</name>
      <uri>https://old.reddit.com/user/Honest-Fun-5279</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So i forked the gemini cli and added local llm support, no google account needed, runs offline. &lt;/p&gt; &lt;p&gt;Give it a try!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/limkcreply/open-gemini-cli"&gt;https://github.com/limkcreply/open-gemini-cli&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Honest-Fun-5279"&gt; /u/Honest-Fun-5279 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn0gpa/forked_googles_gemini_cli_to_work_with_local_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn0gpa/forked_googles_gemini_cli_to_work_with_local_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pn0gpa/forked_googles_gemini_cli_to_work_with_local_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T06:23:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1pn8ww0</id>
    <title>GLM 4.6V support coming to llama.cpp</title>
    <updated>2025-12-15T14:36:56+00:00</updated>
    <author>
      <name>/u/tarruda</name>
      <uri>https://old.reddit.com/user/tarruda</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tarruda"&gt; /u/tarruda &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/18042"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn8ww0/glm_46v_support_coming_to_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pn8ww0/glm_46v_support_coming_to_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T14:36:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1pn7o5f</id>
    <title>ùöïùöïùöäùöñùöä.ùööùöùùöåùöõùöéùöäùöùùöòùöõ v3.0.0 is out üéâ</title>
    <updated>2025-12-15T13:43:26+00:00</updated>
    <author>
      <name>/u/cristianadam</name>
      <uri>https://old.reddit.com/user/cristianadam</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn7o5f/ùöïùöïùöäùöñùöäùööùöùùöåùöõùöéùöäùöùùöòùöõ_v300_is_out/"&gt; &lt;img alt="ùöïùöïùöäùöñùöä.ùööùöùùöåùöõùöéùöäùöùùöòùöõ v3.0.0 is out üéâ" src="https://external-preview.redd.it/b2RvY2Z3YTNnZDdnMQDCmACWN_s8k6H2Y-UiTssPZ2QAPGgBtwTl1Ibw4ttz.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=849a1979b47c8f0e51a93a6dcf9ea0b17e612e5a" title="ùöïùöïùöäùöñùöä.ùööùöùùöåùöõùöéùöäùöùùöòùöõ v3.0.0 is out üéâ" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The screencast was done on a MacBook M3 with &lt;code&gt;llama-server&lt;/code&gt; running &lt;code&gt;gpt-oss 20b&lt;/code&gt; and the following prompt: &lt;em&gt;&amp;quot;write a c++ program that prints the current moon phase. use emojis. use cmake. open, build and run in Qt Creator.&amp;quot;&lt;/em&gt;&lt;/p&gt; &lt;p&gt;The link to &lt;a href="https://github.com/cristianadam/llama.qtcreator/releases/tag/v3.0.0"&gt;Release v3.0.0&lt;/a&gt;. It's also available in Qt Creator 18's Extension pane. Click on &lt;em&gt;Use external repository&lt;/em&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cristianadam"&gt; /u/cristianadam &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/cyhyeja3gd7g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn7o5f/ùöïùöïùöäùöñùöäùööùöùùöåùöõùöéùöäùöùùöòùöõ_v300_is_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pn7o5f/ùöïùöïùöäùöñùöäùööùöùùöåùöõùöéùöäùöùùöòùöõ_v300_is_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T13:43:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1pn19zc</id>
    <title>Another watercooled 4x GPU server complete!</title>
    <updated>2025-12-15T07:13:53+00:00</updated>
    <author>
      <name>/u/j4ys0nj</name>
      <uri>https://old.reddit.com/user/j4ys0nj</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn19zc/another_watercooled_4x_gpu_server_complete/"&gt; &lt;img alt="Another watercooled 4x GPU server complete!" src="https://preview.redd.it/pgqrfop2ib7g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=313f261a0bcd81f8f2af182cfeb1eae60a4c0d0f" title="Another watercooled 4x GPU server complete!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm on a roll this weekend. Finally got all of the parts needed to finish this build. 4x RTX A4500 with waterblocks from &lt;a href="https://shop.alphacool.com/en/shop/gpu-water-cooling/nvidia/10669-alphacool-es-rtx-a5000-gpu-cooler-with-backplate"&gt;Alphacool (A5000)&lt;/a&gt;. 80GB VRAM, nothing crazy, pretty cost efficient. These GPUs were about $1k each. Waterblocks were between $50-100 each since they're pretty old. As the blocks come, they appear to be 1 slot, but there's no 1 slot bracket provided and with the back plate, it takes up some space of the slot above it, so running these with no back plate (the GPUs don't have a back plate to begin with) and I had to print a slimmer block on the end than what came with them (the part right by the power connector). Then I cut the brackets to be 1 slot. Perfect fit. Very tight though, this chassis was not made for this! To round out the build there's a 4x mini SAS card connected to 16 SSDs (2 of the 5.25&amp;quot; bays on the right), and a 4x NVMe hot swap (in the remaining 5.25&amp;quot; bay) and a Mellanox 25G card.&lt;/p&gt; &lt;p&gt;Getting pretty decent performance out of it! I have &lt;a href="https://huggingface.co/cerebras/Qwen3-Coder-REAP-25B-A3B"&gt;https://huggingface.co/cerebras/Qwen3-Coder-REAP-25B-A3B&lt;/a&gt; loaded up with vLLM. It juuust fits. ~103-105 tokens/sec on single requests and when testing with 6x simultaneous requests it does about 50 tokens/sec. On sustained workloads, temps stay around 40-42¬∫C.&lt;/p&gt; &lt;p&gt;Finished my other watercooled 4x GPU server a few days ago also, post &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1pl984y/finally_finished_my_4x_gpu_water_cooled_server/"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/j4ys0nj"&gt; /u/j4ys0nj &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/pgqrfop2ib7g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn19zc/another_watercooled_4x_gpu_server_complete/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pn19zc/another_watercooled_4x_gpu_server_complete/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T07:13:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1pn0cik</id>
    <title>Day 7: 21 Days of Building a Small Language Model: Self Attention</title>
    <updated>2025-12-15T06:16:04+00:00</updated>
    <author>
      <name>/u/Prashant-Lakhera</name>
      <uri>https://old.reddit.com/user/Prashant-Lakhera</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn0cik/day_7_21_days_of_building_a_small_language_model/"&gt; &lt;img alt="Day 7: 21 Days of Building a Small Language Model: Self Attention" src="https://b.thumbs.redditmedia.com/8Z0KF4iVd1xs4F4YcBYq-cR6jPpJZFENFObuyX6LZHA.jpg" title="Day 7: 21 Days of Building a Small Language Model: Self Attention" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Welcome to Day 7. Today, our focus is on self-attention. Simply put, self-attention allows each word in a sequence to look at and incorporate information from all other words in that sequence. This might seem obvious (of course words need to understand their context), but the challenge is doing this efficiently and effectively.&lt;/p&gt; &lt;p&gt;I‚Äôve covered all the concepts here at a high level to keep things simple. For a deeper exploration of these topics, feel free to check out my book &amp;quot;&lt;em&gt;Building A Small Language Model from Scratch: A Practical Guide.&amp;quot;&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; If you want to understand the coding part step by step, here‚Äôs the video.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=EXnvO86m1W8"&gt;&lt;strong&gt;https://www.youtube.com/watch?v=EXnvO86m1W8&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For example, in the sentence&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Sarah works as a software engineer. She enjoys solving complex problems &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;the word &amp;quot;She&amp;quot; needs to understand that it refers to &amp;quot;Sarah&amp;quot; from the previous sentence. Without self-attention, the model would process each word in isolation, losing crucial information about how words relate to each other.&lt;/p&gt; &lt;p&gt;So the real question is: how does self-attention enable models to capture these relationships, and why is it so effective?&lt;/p&gt; &lt;h1&gt;The Core Issue&lt;/h1&gt; &lt;p&gt;When we read a sentence, each word's meaning is influenced by the other words around it. The word bank means something different in I deposited money at the bank versus I sat on the river bank. The word it in The cat sat on the mat. It was comfortable. refers to the mat from the previous sentence.&lt;/p&gt; &lt;p&gt;These relationships aren't just about adjacent words; they can span long distances, and they're bidirectional. Later words can influence earlier ones, and earlier words influence later ones.&lt;/p&gt; &lt;p&gt;Traditional neural network approaches struggled with this. Recurrent Neural Networks (RNNs) process sequences step by step, which makes it difficult to capture long-range dependencies. Convolutional Neural Networks (CNNs) use fixed-size windows, limiting their ability to see the full context.&lt;/p&gt; &lt;p&gt;Self-attention solves this problem by allowing each position in the sequence to attend to every other position, including itself, in a single operation. When processing the word she, the model can attend to Sarah from earlier in the sequence, learning that she refers to Sarah. When processing bank, the model can attend to deposited money to understand that this bank is a financial institution, not a river's edge.&lt;/p&gt; &lt;h1&gt;Queries, Keys, and Values&lt;/h1&gt; &lt;p&gt;The self-attention mechanism uses three key components: queries, keys, and values. This terminology might seem abstract at first, but it's actually quite intuitive once you understand the analogy.&lt;/p&gt; &lt;p&gt;Think of how you search a database: you submit a query to find what you're looking for, the system uses keys to index and locate matching entries, and then retrieves the actual values associated with those keys.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/2ilzysh88b7g1.png?width=581&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=522afd4841746bf137b33000b763e4fb134b6e41"&gt;https://preview.redd.it/2ilzysh88b7g1.png?width=581&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=522afd4841746bf137b33000b763e4fb134b6e41&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Queries&lt;/strong&gt; represent what each token is looking for: the question we want to answer. When processing a particular position in the sequence, the query encodes what information we need from other positions.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Keys&lt;/strong&gt; represent what each element in the input can provide: the information available at each position. Each position in the sequence has a key that describes what that position contains or can offer.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Values&lt;/strong&gt; contain the actual information we want to extract. Once we determine which positions are relevant (by comparing queries to keys), we use the values from those positions to construct the output.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Let's consider an example. Imagine you have a database and your database has these employee records&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/4juko3ra8b7g1.png?width=285&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fa2022c5535c0993877bec46cc9fd92b9931c021"&gt;https://preview.redd.it/4juko3ra8b7g1.png?width=285&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fa2022c5535c0993877bec46cc9fd92b9931c021&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A Query is the question you ask:Give me the record for Employee ID = 27.&lt;/li&gt; &lt;li&gt;The Keys are all the indexed fields in the database(10,27,33) that help you find the right record.&lt;/li&gt; &lt;li&gt;The Value is the actual information the database returns when the right key is matched.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Let's consider one more example. Suppose we're processing the same example: Sarah works as a software engineer. She enjoys solving complex problems.&lt;/p&gt; &lt;p&gt;When the model processes the word She in the second sentence, it needs to determine what She refers to. Here's how self-attention helps:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Query (for &amp;quot;She&amp;quot;)&lt;/strong&gt;: The query for She encodes the question: What does this pronoun refer to? It represents what we're looking for, which is the person or thing that the pronoun refers to, specifically a female person mentioned earlier.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Keys (for each word)&lt;/strong&gt;: Each word in the sequence has a key that describes what that word represents. The key for Sarah might encode that it's a proper noun referring to a person (likely female based on the name). The key for engineer might encode that it's a noun referring to a profession. The key for works might encode that it's a verb.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Values (for each word)&lt;/strong&gt;: The values contain the actual semantic information. The value for Sarah contains information about who Sarah is, her identity, etc. The value for engineer contains information about the profession. The value for software contains information about the field of work.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9nr5ikwe8b7g1.png?width=711&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1c2ed0a7f5b4f77aa73198bfe495a197716f3fe6"&gt;https://preview.redd.it/9nr5ikwe8b7g1.png?width=711&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1c2ed0a7f5b4f77aa73198bfe495a197716f3fe6&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The attention mechanism compares the query for She against all the keys in the sequence. The key for Sarah will likely have a high similarity to the query for She because Sarah is a proper noun referring to a person who could be referred to by the pronoun She, and it appears earlier in the sequence. The keys for engineer, software, and works will have lower similarity. This produces high attention weights for Sarah and lower weights for other words.&lt;/p&gt; &lt;p&gt;Finally, the mechanism uses these attention weights to create a weighted combination of the values. Since Sarah has a high attention weight, its value (information about Sarah) will dominate the resulting context vector. This allows the model to understand that She refers to Sarah, and the context vector for She will incorporate information about Sarah, including that she works as a software engineer and enjoys solving complex problems.&lt;/p&gt; &lt;h1&gt;How Self-Attention Works&lt;/h1&gt; &lt;p&gt;The self-attention mechanism works by comparing queries to keys to determine how relevant each key is to the current query. This comparison produces relevance scores, called attention weights, which indicate how much each position should contribute. The mechanism then uses these attention weights to create a weighted combination of the values, producing a context vector that incorporates information from the most relevant positions.&lt;/p&gt; &lt;p&gt;The mathematical formula for scaled dot-product attention (the type used in transformers) is:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/gxqxyvkg8b7g1.png?width=727&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9141415545031c7cb5d32acbf9dfbc4e89249cf9"&gt;https://preview.redd.it/gxqxyvkg8b7g1.png?width=727&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9141415545031c7cb5d32acbf9dfbc4e89249cf9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;where:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Q&lt;/strong&gt; is the Query matrix, representing what each token is looking for&lt;/li&gt; &lt;li&gt;&lt;strong&gt;K&lt;/strong&gt; is the Key matrix, representing what each token can provide&lt;/li&gt; &lt;li&gt;&lt;strong&gt;V&lt;/strong&gt; is the Value matrix, containing the actual information content&lt;/li&gt; &lt;li&gt;&lt;strong&gt;d_k&lt;/strong&gt; is the dimension of the key vectors&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Q K^T&lt;/strong&gt; computes the similarity scores between queries and keys&lt;/li&gt; &lt;li&gt;The division by &lt;strong&gt;‚àöd_k&lt;/strong&gt; scales the scores to prevent numerical instability&lt;/li&gt; &lt;li&gt;&lt;strong&gt;softmax&lt;/strong&gt; converts the scores into a probability distribution&lt;/li&gt; &lt;li&gt;The final multiplication with V produces context vectors weighted by attention&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This formula enables the model to determine which parts of the input sequence are most relevant when processing each token, allowing it to capture long-range dependencies and contextual relationships.&lt;/p&gt; &lt;h1&gt;Why we scale by ‚àöd_k&lt;/h1&gt; &lt;p&gt;The scaled part of scaled dot-product attention comes from dividing the attention scores by the square root of the key dimension. This scaling is crucial for training stability.&lt;/p&gt; &lt;p&gt;When we compute the dot product between query and key vectors, the magnitude of the result grows with the dimension. For large embedding dimensions (typically 768, or even larger in modern models), these dot products can become very large.&lt;/p&gt; &lt;p&gt;Large dot products cause problems with the softmax function. When the input to softmax has very large values, the function behaves more like a step function, producing very sharp distributions where almost all attention goes to a single token. This creates two problems:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Gradient issues&lt;/strong&gt;: Very sharp softmax distributions result in very small gradients during backpropagation, which can drastically slow down learning or cause training to stagnate.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Loss of information&lt;/strong&gt;: When attention is too focused on a single token, the model loses the ability to attend to multiple relevant tokens simultaneously, which is important for understanding complex relationships.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;By scaling the scores by ‚àöd_k, we keep the dot products in a reasonable range, ensuring that the softmax function produces well-distributed attention weights. This allows the model to attend to multiple relevant tokens rather than focusing too heavily on just one, while also maintaining stable gradients during training.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; If you want to see how this looks in practice, please check the video above or the Google Colab link &lt;a href="https://colab.research.google.com/drive/1Ux1qrHL5DII8088tmTc4tCJfHqt2zvlw?usp=sharing"&gt;https://colab.research.google.com/drive/1Ux1qrHL5DII8088tmTc4tCJfHqt2zvlw?usp=sharing&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Why we use Softmax&lt;/h1&gt; &lt;p&gt;The softmax function converts the raw similarity scores (which can be any real numbers) into attention weights that represent how much focus should be placed on each token. Softmax ensures that:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;All attention weights sum to 1&lt;/strong&gt;: This creates a probability distribution, making the weights interpretable as proportions of attention.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Larger scores get more attention&lt;/strong&gt;: Tokens with higher similarity scores receive higher attention weights, but the normalization ensures that attention is distributed across all tokens proportionally.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multiple tokens can be attended to&lt;/strong&gt;: Unlike a hard selection mechanism, softmax allows the model to attend to multiple relevant tokens simultaneously, which is crucial for understanding complex linguistic relationships.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; If you want to see how this looks in practice, please check the video above or the Google Colab link &lt;/p&gt; &lt;h1&gt;Summary&lt;/h1&gt; &lt;p&gt;Self-attention is not just a component of transformer architectures; it is the fundamental mechanism that enables these models to understand context, relationships, and meaning in sequences of text. Without it, language models cannot capture the connections between words that make language meaningful.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prashant-Lakhera"&gt; /u/Prashant-Lakhera &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn0cik/day_7_21_days_of_building_a_small_language_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn0cik/day_7_21_days_of_building_a_small_language_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pn0cik/day_7_21_days_of_building_a_small_language_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T06:16:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmx49s</id>
    <title>I pitted GPT-5.2 against Opus 4.5 and Gemini 3 in a robot coding tournament</title>
    <updated>2025-12-15T03:19:51+00:00</updated>
    <author>
      <name>/u/Inevitable_Can598</name>
      <uri>https://old.reddit.com/user/Inevitable_Can598</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently revived the classic coding game Robocode (Java-based tank battles) to test how LLMs perform against top-tier robots. Unlike static coding challenges (like LeetCode), these bots must balance tradeoffs, adapt to enemy strategies in real-time, and adopt unconventional approaches to remain unpredictable.&lt;/p&gt; &lt;p&gt;I prompted each model to build a robot, providing iterative feedback until progress stalled, and then submitted the best versions to the Robocode Arena.&lt;/p&gt; &lt;h1&gt;Final results&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Final ELO&lt;/th&gt; &lt;th align="left"&gt;Rank&lt;/th&gt; &lt;th align="left"&gt;Iterations to peak&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Opus-4.5&lt;/td&gt; &lt;td align="left"&gt;1412&lt;/td&gt; &lt;td align="left"&gt;17&lt;/td&gt; &lt;td align="left"&gt;3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPT-5.2-thinking&lt;/td&gt; &lt;td align="left"&gt;1229&lt;/td&gt; &lt;td align="left"&gt;25&lt;/td&gt; &lt;td align="left"&gt;3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Gemini-3-thinking&lt;/td&gt; &lt;td align="left"&gt;973&lt;/td&gt; &lt;td align="left"&gt;42&lt;/td&gt; &lt;td align="left"&gt;4&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPT-5.2-instant&lt;/td&gt; &lt;td align="left"&gt;953&lt;/td&gt; &lt;td align="left"&gt;43&lt;/td&gt; &lt;td align="left"&gt;3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Gemini-3-fast&lt;/td&gt; &lt;td align="left"&gt;917&lt;/td&gt; &lt;td align="left"&gt;46&lt;/td&gt; &lt;td align="left"&gt;7&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPT-5.1-thinking&lt;/td&gt; &lt;td align="left"&gt;835&lt;/td&gt; &lt;td align="left"&gt;49&lt;/td&gt; &lt;td align="left"&gt;8&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Haiku-4.5&lt;/td&gt; &lt;td align="left"&gt;811&lt;/td&gt; &lt;td align="left"&gt;50&lt;/td&gt; &lt;td align="left"&gt;8&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPT-5.1-instant&lt;/td&gt; &lt;td align="left"&gt;626&lt;/td&gt; &lt;td align="left"&gt;53&lt;/td&gt; &lt;td align="left"&gt;8&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Key findings&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;GPT-5.2 is a major upgrade over 5.1, scoring nearly 400 ELO points higher on the ladder. It figured out working strategies almost immediately, whereas 5.1 really struggled to make anything competitive even with a lot of help.&lt;/li&gt; &lt;li&gt;OpenAI is clearly pulling ahead of Google here; GPT-5.2 Thinking beat Gemini 3 Pro Thinking comfortably. Even the Instant GPT-5.2 model basically tied with Google's Thinking model, which was pretty surprising.&lt;/li&gt; &lt;li&gt;Opus 4.5 actually took the #1 spot because it acts more like a reliable coder than a tinkerer. While GPT-5.2 kept breaking its own code trying to optimize it, Opus nailed the complex math/physics on the first try and didn't regress.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I don't have an appropriate setup for a local LLM but I will be working on testing that next.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Inevitable_Can598"&gt; /u/Inevitable_Can598 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmx49s/i_pitted_gpt52_against_opus_45_and_gemini_3_in_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmx49s/i_pitted_gpt52_against_opus_45_and_gemini_3_in_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pmx49s/i_pitted_gpt52_against_opus_45_and_gemini_3_in_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T03:19:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1pn4yrf</id>
    <title>I scored 100+ architectures on "Hardware Friction." Why KANs fry tensor cores and MoEs have a context trap.</title>
    <updated>2025-12-15T11:21:40+00:00</updated>
    <author>
      <name>/u/petroslamb</name>
      <uri>https://old.reddit.com/user/petroslamb</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been trying to figure out why technically superior architectures like Neural ODEs often die while the Transformer remains dominant. I ended up writing a deep dive on what I call the &amp;quot;Hardware Friction Map,&amp;quot; arguing that GPUs don't actually reject ideas. They just charge a &amp;quot;compute tax&amp;quot; based on how much an idea deviates from optimized primitives like dense matrix multiplications.&lt;/p&gt; &lt;p&gt;I also compiled a GitHub dataset scoring over 100 architectures on their hardware efficiency, which I linked below. There are a few specific findings that I think matter for those of us running models locally.&lt;/p&gt; &lt;p&gt;The first big one is the &amp;quot;Context Trap&amp;quot; with Mixture of Experts. We all like MoEs for the inference speedup, but the data suggests that the &amp;quot;5x faster&amp;quot; marketing claims usually only hold up at very short context lengths. When you look at the benchmarks for 16k to 32k context, the throughput often drops to roughly 30% or 40% of the baseline. The issue is that the routing logic and KV cache traffic start to dominate the sparse expert compute. MoEs are great throughput optimizers, but unless the architecture is specifically co-designed for long context like the new DeepSeek V3, they struggle when you load them up with history.&lt;/p&gt; &lt;p&gt;Then there are the &amp;quot;Red Zone&amp;quot; architectures like KANs (Kolmogorov-Arnold Networks). They look great on paper, but they are basically unusable for local inference right now. KANs rely on edge-based spline evaluations, which are essentially hundreds of tiny, irregular operations. Current GPUs need big batched matrix multiplications to hit peak performance, so KANs end up dropping tensor core utilization to around 10%. Until hardware changes, they are just too expensive to run efficiently.&lt;/p&gt; &lt;p&gt;I also noticed a hard limit with pure State Space Models (SSMs) like Mamba. They seem to be production-ready at the 7B scale, which is why Falcon Mamba 7B works well. But once you cross the 13B parameter threshold, the training parallelism gap compounds and memory bandwidth becomes a bottleneck for state propagation. That appears to be why every major deployment larger than 13B, like Jamba or Falcon-H1, is forced to use a hybrid architecture of Attention plus SSMs.&lt;/p&gt; &lt;p&gt;This friction also explains the gap between models like Llama 3.1 and DeepSeek V3. Llama used a standard stack that we can run easily. DeepSeek V3 required them to rewrite their entire cluster scheduler and spend six months on custom routing kernels. That high friction is a massive moat for them, but it is also why it takes about 20 months for the open ecosystem tools like vLLM or llama.cpp to fully catch up to those custom internals.&lt;/p&gt; &lt;p&gt;I have linked the full breakdown and the architecture scoring dataset below. I am curious if your experience with local inference matches the context trap numbers I found for MoEs.&lt;/p&gt; &lt;p&gt;- (dataset) &lt;a href="https://github.com/petroslamb/hardware-friction-map-2025"&gt;https://github.com/petroslamb/hardware-friction-map-2025&lt;/a&gt;&lt;br /&gt; - (article) &lt;a href="https://lambpetros.substack.com/p/the-hardware-friction-map"&gt;https://lambpetros.substack.com/p/the-hardware-friction-map&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/petroslamb"&gt; /u/petroslamb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn4yrf/i_scored_100_architectures_on_hardware_friction/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn4yrf/i_scored_100_architectures_on_hardware_friction/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pn4yrf/i_scored_100_architectures_on_hardware_friction/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T11:21:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1pn8upp</id>
    <title>NVIDIA releases Nemotron 3 Nano, a new 30B hybrid reasoning model!</title>
    <updated>2025-12-15T14:34:28+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn8upp/nvidia_releases_nemotron_3_nano_a_new_30b_hybrid/"&gt; &lt;img alt="NVIDIA releases Nemotron 3 Nano, a new 30B hybrid reasoning model!" src="https://preview.redd.it/sic85bvhpd7g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d01067e3a3899e680b913799c37c8ef9b609ff4c" title="NVIDIA releases Nemotron 3 Nano, a new 30B hybrid reasoning model!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Unsloth GGUF: &lt;a href="https://huggingface.co/unsloth/Nemotron-3-Nano-30B-A3B-GGUF"&gt;https://huggingface.co/unsloth/Nemotron-3-Nano-30B-A3B-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Nemotron 3 has a 1M context window and the best in class performance for SWE-Bench, reasoning and chat.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sic85bvhpd7g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn8upp/nvidia_releases_nemotron_3_nano_a_new_30b_hybrid/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pn8upp/nvidia_releases_nemotron_3_nano_a_new_30b_hybrid/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T14:34:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1pn6ijr</id>
    <title>How to do a RTX Pro 6000 build right</title>
    <updated>2025-12-15T12:48:06+00:00</updated>
    <author>
      <name>/u/GPTrack_dot_ai</name>
      <uri>https://old.reddit.com/user/GPTrack_dot_ai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn6ijr/how_to_do_a_rtx_pro_6000_build_right/"&gt; &lt;img alt="How to do a RTX Pro 6000 build right" src="https://a.thumbs.redditmedia.com/In8IH1Q_hHUabRuaqY3yOiZcHAQpBhmIHFzJ_jWQyx4.jpg" title="How to do a RTX Pro 6000 build right" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The RTX PRO 6000 is missing NVlink, that is why Nvidia came up with idea to integrate high-speed networking directly at each GPU. This is called the RTX PRO server. There are 8 PCIe slots for 8 RTX Pro 6000 server version cards and each one has a 400G networking connection. The good thing is that it is basically ready to use. The only thing you need to decide on is Switch, CPU, RAM and storage. Not much can go wrong there. If you want multiple RTX PRO 6000 this the way to go.&lt;/p&gt; &lt;p&gt;Exemplary Specs:&lt;br /&gt; 8x Nvidia RTX PRO 6000 Blackwell Server Edition GPU&lt;br /&gt; 8x Nvidia ConnectX-8 1-port 400G QSFP112&lt;br /&gt; 1x Nvidia Bluefield-3 2-port 200G total 400G QSFP112 (optional)&lt;br /&gt; 2x Intel Xeon 6500/6700&lt;br /&gt; 32x 6400 RDIMM or 8000 MRDIMM&lt;br /&gt; 6000W TDP&lt;br /&gt; 4x High-efficiency 3200W PSU&lt;br /&gt; 2x PCIe gen4 M.2 slots on board&lt;br /&gt; 8x PCIe gen5 U.2&lt;br /&gt; 2x USB 3.2 port&lt;br /&gt; 2x RJ45 10GbE ports&lt;br /&gt; RJ45 IPMI port&lt;br /&gt; Mini display port&lt;br /&gt; 10x 80x80x80mm fans&lt;br /&gt; 4U 438 x 176 x 803 mm (17.2 x 7 x 31.6&amp;quot;)&lt;br /&gt; 70 kg (150 lbs)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GPTrack_dot_ai"&gt; /u/GPTrack_dot_ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pn6ijr"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn6ijr/how_to_do_a_rtx_pro_6000_build_right/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pn6ijr/how_to_do_a_rtx_pro_6000_build_right/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T12:48:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1pn7c3f</id>
    <title>Alibaba Tongyi Open Sources Two Audio Models: Fun-CosyVoice 3.0 (TTS) and Fun-ASR-Nano-2512 (ASR)</title>
    <updated>2025-12-15T13:28:12+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn7c3f/alibaba_tongyi_open_sources_two_audio_models/"&gt; &lt;img alt="Alibaba Tongyi Open Sources Two Audio Models: Fun-CosyVoice 3.0 (TTS) and Fun-ASR-Nano-2512 (ASR)" src="https://preview.redd.it/1v2kztejdd7g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=756f684594b46bdb6022b8853734dc4f1ad2ec1c" title="Alibaba Tongyi Open Sources Two Audio Models: Fun-CosyVoice 3.0 (TTS) and Fun-ASR-Nano-2512 (ASR)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Fun-ASR-Nano (0.8B) ‚Äî Open-sourced - Lightweight Fun-ASR variant - Lower inference cost - Local deployment &amp;amp; custom fine-tuning supported&lt;/p&gt; &lt;p&gt;Fun-CosyVoice3 (0.5B) ‚Äî Open-sourced - Zero-shot voice cloning - Local deployment &amp;amp; secondary development ready&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1v2kztejdd7g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn7c3f/alibaba_tongyi_open_sources_two_audio_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pn7c3f/alibaba_tongyi_open_sources_two_audio_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T13:28:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1pn2e1c</id>
    <title>llama.cpp: Automation for GPU layers, tensor split, tensor overrides, and context size (with MoE optimizations)</title>
    <updated>2025-12-15T08:28:56+00:00</updated>
    <author>
      <name>/u/Remove_Ayys</name>
      <uri>https://old.reddit.com/user/Remove_Ayys</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;CPU + GPU hybrid inference has been a core feature of llama.cpp since early on, and I would argue, one of the major selling points vs. projects like ExLlama. The way to control memory use until now was to manually set parameter like &lt;code&gt;--n-gpu-layers&lt;/code&gt; and &lt;code&gt;--tensor-split&lt;/code&gt; to fit memory use to free VRAM. However, this is of course suboptimal in terms of usability. Downstream projects like Ollama and KoboldCpp have implemented mechanisms for automating memory allocation but those rely on rough heuristics and tend to be inaccurate. As a consequence, to avoid running out of memory in some cases the heuristics are rather conservative and leave potential performance on the table. The problem becomes even harder when running models across multiple GPUs or when running MoE models where the dense tensors should be prioritized over the sparse MoE tensors for optimal performance.&lt;/p&gt; &lt;p&gt;On the latest llama.cpp version following &lt;a href="https://github.com/ggml-org/llama.cpp/pull/16653"&gt;https://github.com/ggml-org/llama.cpp/pull/16653&lt;/a&gt; I implemented code to automate memory allocations across GPUs. It works by doing virtual test allocations and using those as feedback to iteratively reduce memory use until the model fits across all GPUs. The metric for memory use is the same as in the &amp;quot;memory breakdown&amp;quot; that you may have seen in recent llama.cpp versions. The implementation is generic and should work for any ggml backend as long as it supports CPU + GPU hybrid inference and the memory breakdown is correct. If you encounter problems using this new functionality, please open an issue instead of commenting here as this will make the process easier from my side.&lt;/p&gt; &lt;p&gt;The code starts by first checking whether the model is projected to fit as-is. If yes, no changes are made. If not, it first reduces the context size to free up memory. If that is still not enough it starts moving tensors from VRAM to RAM. Dense tensors are prioritized for better MoE performance. Ideally one would only assign whole layers to GPUs for simplicity. However, as individual layers can be very large against &amp;quot;small&amp;quot; GPUs with only 24 GiB VRAM this would result in significant waste. For this reason, layers can &amp;quot;overflow&amp;quot;, meaning that parts of them are moved to the next GPU in line or to system RAM.&lt;/p&gt; &lt;h3&gt;Command-Line Interface&lt;/h3&gt; &lt;p&gt;The fitting of runtime parameters can be controlled as follows:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;--fit&lt;/code&gt;, &lt;code&gt;-fit&lt;/code&gt;: set to &lt;code&gt;on&lt;/code&gt; by default, can be set to &lt;code&gt;off&lt;/code&gt; to disable parameter fitting.&lt;/li&gt; &lt;li&gt;&lt;code&gt;--fit-target&lt;/code&gt;, &lt;code&gt;-fitt&lt;/code&gt;: target amount of free memory to leave on each GPU. As of right now this is the same value for all GPUs and it is not possible to specify e.g. an amount that should be used regardless of free memory.&lt;/li&gt; &lt;li&gt;&lt;code&gt;--fit-ctx&lt;/code&gt;, &lt;code&gt;-fitc&lt;/code&gt;: minimum context size that can be set automatically. If &lt;code&gt;--ctx-size&lt;/code&gt; is explicitly set by the user it is not changed.&lt;/li&gt; &lt;li&gt;If arguments like &lt;code&gt;--n-gpu-layers&lt;/code&gt;, &lt;code&gt;--tensor-split&lt;/code&gt;, or &lt;code&gt;--override-tensor&lt;/code&gt; that affect memory allocation are set by the user, there is no change to that memory allocation. There is no support for automatic modification of only one of these arguments, they are either wholly under user control or wholly under program control.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;There is a new tool &lt;code&gt;llama-fit-params&lt;/code&gt; that can be used to retrieve the parameters that would be set by the new parameter fitting logic. For example:&lt;/p&gt; &lt;p&gt;```bash&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;$ ./build/bin/llama-fit-params --model models/opt/gpt-oss-120b-mxfp4-00001-of-00003.gguf -ub 4096 -b 4096 ggml&lt;em&gt;cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 2 CUDA devices: Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes build: 7413 (ae534ec0c) with GNU 15.2.1 for Linux x86_64 llama_params_fit_impl: projected memory use with initial parameters [MiB]: llama_params_fit_impl: - CUDA0 (NVIDIA GeForce RTX 4090): 24080 total, 34873 used, 11187 deficit llama_params_fit_impl: - CUDA1 (NVIDIA GeForce RTX 4090): 24080 total, 31847 used, 8161 deficit llama_params_fit_impl: projected to use 66721 MiB of device memory vs. 48161 MiB of free device memory llama_params_fit_impl: cannot fulfill margin of 1024 MiB on all devices, need to use 21397 MiB less in total llama_params_fit_impl: context size reduced from 131072 to 4096 -&amp;gt; need 4490 MiB less memory in total llama_params_fit_impl: with only dense weights in device memory there is a total surplus of 42064 MiB llama_params_fit_impl: filling dense-only layers back-to-front: llama_params_fit_impl: - CUDA1 (NVIDIA GeForce RTX 4090): 36 layers, 2201 MiB used, 21484 MiB free llama_params_fit_impl: - CUDA0 (NVIDIA GeForce RTX 4090): 0 layers, 985 MiB used, 22700 MiB free llama_params_fit_impl: converting dense-only layers to full layers and filling them front-to-back with overflow to next device/system memory: llama_params_fit_impl: - CUDA0 (NVIDIA GeForce RTX 4090): 14 layers ( 1 overflowing), 22576 MiB used, 1109 MiB free llama_params_fit_impl: - CUDA1 (NVIDIA GeForce RTX 4090): 22 layers (11 overflowing), 22208 MiB used, 1477 MiB free llama_params_fit: successfully fit params to free device memory llama_params_fit: fitting params to free memory took 8.81 seconds Printing fitted CLI arguments to stdout... -c 4096 -ngl 37 -ts 14,23 -ot blk.13.ffn&lt;/em&gt;(up|gate|down).&lt;em&gt;=CUDA1,blk.25.ffn_down.&lt;/em&gt;=CPU,blk.26.ffn&lt;em&gt;(up|down|gate)&lt;/em&gt;(ch|)exps=CPU,blk.27.ffn&lt;em&gt;(up|down|gate)&lt;/em&gt;(ch|)exps=CPU,blk.28.ffn&lt;em&gt;(up|down|gate)&lt;/em&gt;(ch|)exps=CPU,blk.29.ffn&lt;em&gt;(up|down|gate)&lt;/em&gt;(ch|)exps=CPU,blk.30.ffn&lt;em&gt;(up|down|gate)&lt;/em&gt;(ch|)exps=CPU,blk.31.ffn&lt;em&gt;(up|down|gate)&lt;/em&gt;(ch|)exps=CPU,blk.32.ffn&lt;em&gt;(up|down|gate)&lt;/em&gt;(ch|)exps=CPU,blk.33.ffn&lt;em&gt;(up|down|gate)&lt;/em&gt;(ch|)exps=CPU,blk.34.ffn&lt;em&gt;(up|down|gate)&lt;/em&gt;(ch|)exps=CPU,blk.35.ffn&lt;em&gt;(up|down|gate)&lt;/em&gt;(ch|)exps=CPU ```&lt;/p&gt; &lt;/blockquote&gt; &lt;h3&gt;Benchmark&lt;/h3&gt; &lt;p&gt;As of right now &lt;code&gt;llama-bench&lt;/code&gt; does not have support for &lt;code&gt;-fit&lt;/code&gt;, &lt;code&gt;-fitt&lt;/code&gt;, and &lt;code&gt;-fitc&lt;/code&gt;. For this reason, the following workaround was used to feed the results from &lt;code&gt;llama-fit-params&lt;/code&gt; into &lt;code&gt;llama-bench&lt;/code&gt;:&lt;/p&gt; &lt;p&gt;&lt;code&gt;bash ./build/bin/llama-fit-params -m models/opt/${model_name}-${quantization}.gguf -b 4096 -ub 4096 | tee tmp.txt ./build/bin/llama-bench -m models/opt/${model_name}-${quantization}.gguf -r 1 -fa 1 $(tail -c +17 tmp.txt | tr ',' ';') &lt;/code&gt;&lt;/p&gt; &lt;p&gt;The benchmark was done on a system with an AMD EPYC 7742 CPU and 8 3200 &amp;quot;MHz&amp;quot; DIMMs.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;GPUs&lt;/th&gt; &lt;th&gt;Time to fit [s]&lt;/th&gt; &lt;th&gt;Fully in VRAM?&lt;/th&gt; &lt;th&gt;VRAM utilization&lt;/th&gt; &lt;th&gt;pp4096 [t/s]&lt;/th&gt; &lt;th&gt;tg128 [t/s]&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Qwen 3 Next BF16&lt;/td&gt; &lt;td&gt;None&lt;/td&gt; &lt;td&gt;-&lt;/td&gt; &lt;td&gt;No&lt;/td&gt; &lt;td&gt;-&lt;/td&gt; &lt;td&gt;38.89&lt;/td&gt; &lt;td&gt;6.23&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen 3 Next BF16&lt;/td&gt; &lt;td&gt;1x RTX 4090&lt;/td&gt; &lt;td&gt;4.89&lt;/td&gt; &lt;td&gt;No&lt;/td&gt; &lt;td&gt;88.1%&lt;/td&gt; &lt;td&gt;381.52&lt;/td&gt; &lt;td&gt;19.01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen 3 Next BF16&lt;/td&gt; &lt;td&gt;2x RTX 4090&lt;/td&gt; &lt;td&gt;7.75&lt;/td&gt; &lt;td&gt;No&lt;/td&gt; &lt;td&gt;88.5%&lt;/td&gt; &lt;td&gt;246.29&lt;/td&gt; &lt;td&gt;20.89&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen 3 Next BF16&lt;/td&gt; &lt;td&gt;3x RTX 4090&lt;/td&gt; &lt;td&gt;10.70&lt;/td&gt; &lt;td&gt;No&lt;/td&gt; &lt;td&gt;88.3%&lt;/td&gt; &lt;td&gt;340.88&lt;/td&gt; &lt;td&gt;22.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen 3 Next BF16&lt;/td&gt; &lt;td&gt;4x RTX 4090&lt;/td&gt; &lt;td&gt;13.87&lt;/td&gt; &lt;td&gt;No&lt;/td&gt; &lt;td&gt;89.3%&lt;/td&gt; &lt;td&gt;433.10&lt;/td&gt; &lt;td&gt;24.70&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen 3 Next BF16&lt;/td&gt; &lt;td&gt;4x RTX 4090, 1x RTX 5090&lt;/td&gt; &lt;td&gt;16.93&lt;/td&gt; &lt;td&gt;No&lt;/td&gt; &lt;td&gt;89.7%&lt;/td&gt; &lt;td&gt;526.71&lt;/td&gt; &lt;td&gt;26.19&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen 3 Next BF16&lt;/td&gt; &lt;td&gt;4x RTX 4090, 1x RTX 5090, 1x RTX 3090&lt;/td&gt; &lt;td&gt;20.39&lt;/td&gt; &lt;td&gt;No&lt;/td&gt; &lt;td&gt;90.2%&lt;/td&gt; &lt;td&gt;599.86&lt;/td&gt; &lt;td&gt;31.37&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen 3 Next q8_0&lt;/td&gt; &lt;td&gt;None&lt;/td&gt; &lt;td&gt;-&lt;/td&gt; &lt;td&gt;No&lt;/td&gt; &lt;td&gt;-&lt;/td&gt; &lt;td&gt;44.81&lt;/td&gt; &lt;td&gt;7.17&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen 3 Next q8_0&lt;/td&gt; &lt;td&gt;1x RTX 4090&lt;/td&gt; &lt;td&gt;4.98&lt;/td&gt; &lt;td&gt;No&lt;/td&gt; &lt;td&gt;87.3%&lt;/td&gt; &lt;td&gt;904.49&lt;/td&gt; &lt;td&gt;24.26&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen 3 Next q8_0&lt;/td&gt; &lt;td&gt;2x RTX 4090&lt;/td&gt; &lt;td&gt;7.51&lt;/td&gt; &lt;td&gt;No&lt;/td&gt; &lt;td&gt;88.5%&lt;/td&gt; &lt;td&gt;574.43&lt;/td&gt; &lt;td&gt;28.34&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen 3 Next q8_0&lt;/td&gt; &lt;td&gt;3x RTX 4090&lt;/td&gt; &lt;td&gt;10.22&lt;/td&gt; &lt;td&gt;No&lt;/td&gt; &lt;td&gt;89.3%&lt;/td&gt; &lt;td&gt;1086.23&lt;/td&gt; &lt;td&gt;33.33&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen 3 Next q8_0&lt;/td&gt; &lt;td&gt;4x RTX 4090&lt;/td&gt; &lt;td&gt;12.19&lt;/td&gt; &lt;td&gt;Yes&lt;/td&gt; &lt;td&gt;87.0%&lt;/td&gt; &lt;td&gt;2474.67&lt;/td&gt; &lt;td&gt;41.37&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;GPT OSS 120b mxfp4&lt;/td&gt; &lt;td&gt;None&lt;/td&gt; &lt;td&gt;-&lt;/td&gt; &lt;td&gt;No&lt;/td&gt; &lt;td&gt;-&lt;/td&gt; &lt;td&gt;115.78&lt;/td&gt; &lt;td&gt;23.63&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;GPT OSS 120b mxfp4&lt;/td&gt; &lt;td&gt;1x RTX 4090&lt;/td&gt; &lt;td&gt;5.56&lt;/td&gt; &lt;td&gt;No&lt;/td&gt; &lt;td&gt;83.7%&lt;/td&gt; &lt;td&gt;1733.20&lt;/td&gt; &lt;td&gt;52.09&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;GPT OSS 120b mxfp4&lt;/td&gt; &lt;td&gt;2x RTX 4090&lt;/td&gt; &lt;td&gt;10.48&lt;/td&gt; &lt;td&gt;No&lt;/td&gt; &lt;td&gt;89.4%&lt;/td&gt; &lt;td&gt;2452.52&lt;/td&gt; &lt;td&gt;78.27&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;GPT OSS 120b mxfp4&lt;/td&gt; &lt;td&gt;3x RTX 4090&lt;/td&gt; &lt;td&gt;11.47&lt;/td&gt; &lt;td&gt;Yes&lt;/td&gt; &lt;td&gt;86.0%&lt;/td&gt; &lt;td&gt;5499.52&lt;/td&gt; &lt;td&gt;180.29&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;GPT OSS 120b mxfp4&lt;/td&gt; &lt;td&gt;4x RTX 4090&lt;/td&gt; &lt;td&gt;1.55&lt;/td&gt; &lt;td&gt;Yes&lt;/td&gt; &lt;td&gt;68.2%&lt;/td&gt; &lt;td&gt;5219.51&lt;/td&gt; &lt;td&gt;182.89&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;The VRAM utilization is at ~85-90%. As the default &lt;code&gt;--fit-target&lt;/code&gt; is 1024 MiB, that would ideally leave ~4% of free VRAM on each GPU. However, since individual tensors can be several GB in size some amount of waste is inevitable.&lt;/p&gt; &lt;p&gt;The time to fit the parameters increases roughly linearly with the number of GPUs. Under ideal circumstances such as when running GPT OSS 120b on 4x RTX 4090 the code only needs to check that the VRAM is sufficient. For Qwen 3 Next there currently seems to be a bug where the memory needed for the context is not accounted correctly so a full fit is done. Time to fit is still fairly unoptimized.&lt;/p&gt; &lt;p&gt;Performance mostly increases as VRAM use increases, except when going from a single GPU to two GPUs (while still being bottlenecked by RAM) or when the model could already be fit on fewer GPUs. With better multi GPU code the performance should increase monotonically as more GPUs are added.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Remove_Ayys"&gt; /u/Remove_Ayys &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn2e1c/llamacpp_automation_for_gpu_layers_tensor_split/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn2e1c/llamacpp_automation_for_gpu_layers_tensor_split/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pn2e1c/llamacpp_automation_for_gpu_layers_tensor_split/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T08:28:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1pn8h5h</id>
    <title>NVIDIA Nemotron 3 Nano 30B A3B released</title>
    <updated>2025-12-15T14:18:28+00:00</updated>
    <author>
      <name>/u/rerri</name>
      <uri>https://old.reddit.com/user/rerri</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16"&gt;https://huggingface.co/nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-Base-BF16"&gt;https://huggingface.co/nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-Base-BF16&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Unsloth GGUF quants: &lt;a href="https://huggingface.co/unsloth/Nemotron-3-Nano-30B-A3B-GGUF/tree/main"&gt;https://huggingface.co/unsloth/Nemotron-3-Nano-30B-A3B-GGUF/tree/main&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Nvidia blog post: &lt;a href="https://developer.nvidia.com/blog/inside-nvidia-nemotron-3-techniques-tools-and-data-that-make-it-efficient-and-accurate/"&gt;https://developer.nvidia.com/blog/inside-nvidia-nemotron-3-techniques-tools-and-data-that-make-it-efficient-and-accurate/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;HF blog post: &lt;a href="https://huggingface.co/blog/nvidia/nemotron-3-nano-efficient-open-intelligent-models"&gt;https://huggingface.co/blog/nvidia/nemotron-3-nano-efficient-open-intelligent-models&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Highlights (copy-pasta from HF blog):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Hybrid Mamba-Transformer MoE architecture:&lt;/strong&gt; Mamba‚Äë2 for long-context, low-latency inference combined with transformer attention for high-accuracy, fine-grained reasoning&lt;/li&gt; &lt;li&gt;&lt;strong&gt;31.6B total parameters, ~3.6B active per token:&lt;/strong&gt; Designed for high throughput and low latency&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Exceptional inference efficiency:&lt;/strong&gt; Up to 4x faster than Nemotron Nano 2 and up to 3.3x faster than leading models in its size category&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Best-in-class reasoning accuracy:&lt;/strong&gt; Across reasoning, coding, tools, and multi-step agentic tasks&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Reasoning controls:&lt;/strong&gt; Reasoning ON/OFF modes plus a configurable thinking budget to cap ‚Äúthinking‚Äù tokens and keep inference cost predictable&lt;/li&gt; &lt;li&gt;&lt;strong&gt;1M-token context window:&lt;/strong&gt; Ideal for long-horizon workflows, retrieval-augmented tasks, and persistent memory&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Fully open:&lt;/strong&gt; Open Weights, datasets, training recipes, and framework&lt;/li&gt; &lt;li&gt;&lt;strong&gt;A full open data stack&lt;/strong&gt;: 3T new high-quality pre-training tokens, 13M cross-disciplinary post-training samples, 10+ RL environments with datasets covering more than 900k tasks in math, coding, reasoning, and tool-use, and ~11k agent-safety traces&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Easy deployment:&lt;/strong&gt; Seamless serving with vLLM and SGLang, and integration via OpenRouter, popular inference service providers, and &lt;a href="http://build.nvidia.com"&gt;build.nvidia.com&lt;/a&gt; endpoints&lt;/li&gt; &lt;li&gt;&lt;strong&gt;License:&lt;/strong&gt; Released under the &lt;a href="https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-open-model-license/"&gt;nvidia-open-model-license&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rerri"&gt; /u/rerri &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn8h5h/nvidia_nemotron_3_nano_30b_a3b_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn8h5h/nvidia_nemotron_3_nano_30b_a3b_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pn8h5h/nvidia_nemotron_3_nano_30b_a3b_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T14:18:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmungj</id>
    <title>Aaaand... is gone...</title>
    <updated>2025-12-15T01:18:27+00:00</updated>
    <author>
      <name>/u/HumanDrone8721</name>
      <uri>https://old.reddit.com/user/HumanDrone8721</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmungj/aaaand_is_gone/"&gt; &lt;img alt="Aaaand... is gone..." src="https://preview.redd.it/g7ahg4per97g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3cb28b1a23cbf35375171b5f0b3fcb2d63310818" title="Aaaand... is gone..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HumanDrone8721"&gt; /u/HumanDrone8721 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/g7ahg4per97g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmungj/aaaand_is_gone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pmungj/aaaand_is_gone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T01:18:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1pn37mw</id>
    <title>New Google model incoming!!!</title>
    <updated>2025-12-15T09:26:05+00:00</updated>
    <author>
      <name>/u/R46H4V</name>
      <uri>https://old.reddit.com/user/R46H4V</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn37mw/new_google_model_incoming/"&gt; &lt;img alt="New Google model incoming!!!" src="https://preview.redd.it/ho8nhiae6c7g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=53bdb437b0e3d6b162a1f97be9b2f4ae540eda69" title="New Google model incoming!!!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/osanseviero/status/2000493503860892049?s=20"&gt;https://x.com/osanseviero/status/2000493503860892049?s=20&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/google"&gt;https://huggingface.co/google&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/R46H4V"&gt; /u/R46H4V &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ho8nhiae6c7g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn37mw/new_google_model_incoming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pn37mw/new_google_model_incoming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T09:26:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
