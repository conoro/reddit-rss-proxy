<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-11T08:07:42+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1o2rppj</id>
    <title>Qwen3 VL 4B to be released?</title>
    <updated>2025-10-10T04:44:56+00:00</updated>
    <author>
      <name>/u/Signal-Run7450</name>
      <uri>https://old.reddit.com/user/Signal-Run7450</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2rppj/qwen3_vl_4b_to_be_released/"&gt; &lt;img alt="Qwen3 VL 4B to be released?" src="https://preview.redd.it/68mmis87s7uf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4ab80165de7b5dc3e27595de513c638e5086c61d" title="Qwen3 VL 4B to be released?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen released cookbooks and in one of them this model Qwen3 VL 4B is present but I can't find it anywhere on huggingface. Link of the cookbook- &lt;a href="https://github.com/QwenLM/Qwen3-VL/blob/main/cookbooks/long_document_understanding.ipynb"&gt;https://github.com/QwenLM/Qwen3-VL/blob/main/cookbooks/long_document_understanding.ipynb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This would be quite amazing for OCR use cases. Qwen2.5/2 VL 3b/7b was foundation for many good OCR models&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Signal-Run7450"&gt; /u/Signal-Run7450 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/68mmis87s7uf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2rppj/qwen3_vl_4b_to_be_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o2rppj/qwen3_vl_4b_to_be_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T04:44:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3abmd</id>
    <title>Lightning-SimulWhisper: A Real-time speech transcription model for Apple Silicon</title>
    <updated>2025-10-10T19:22:03+00:00</updated>
    <author>
      <name>/u/redditgivingmeshit</name>
      <uri>https://old.reddit.com/user/redditgivingmeshit</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3abmd/lightningsimulwhisper_a_realtime_speech/"&gt; &lt;img alt="Lightning-SimulWhisper: A Real-time speech transcription model for Apple Silicon" src="https://external-preview.redd.it/-4gV633-KAhImXI_2bxiNmWNi5JL9hXbDXpXkSuB4PU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=720d213bbdc2de0268ef55fe6c6383422b15749c" title="Lightning-SimulWhisper: A Real-time speech transcription model for Apple Silicon" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Basically, it's a &lt;strong&gt;CoreML/MLX translation of SimulStreaming&lt;/strong&gt; (2025 SOTA in simultaneous speech transcription), which itself is a combination Simul-Whisper and WhisperStreaming.&lt;/p&gt; &lt;p&gt;I'm currently building an application, and I thought I would open up the backend model code for everyone to use.&lt;/p&gt; &lt;p&gt;I get ~15x speed increase on my M2 Macbook Pro compared to the original pytorch implementation, and I'm gonna be using the medium model, which has a nice balance between memory usage and accuracy.&lt;/p&gt; &lt;p&gt;The CoreML part is from whisper.cpp, and it only contains the encoder, and the mlx part is from mlx-whisper.&lt;/p&gt; &lt;p&gt;It's very beta and I haven't tested it on other computers, so please feel free to leave Issues/PRs/Contributions 😀&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/redditgivingmeshit"&gt; /u/redditgivingmeshit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/altalt-org/Lightning-SimulWhisper"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3abmd/lightningsimulwhisper_a_realtime_speech/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3abmd/lightningsimulwhisper_a_realtime_speech/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T19:22:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3k1et</id>
    <title>Vibe Coded Research Agent repo</title>
    <updated>2025-10-11T02:25:17+00:00</updated>
    <author>
      <name>/u/nomorebuttsplz</name>
      <uri>https://old.reddit.com/user/nomorebuttsplz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I posted about my experience with GLM 4.5/4.6 and Cline on a Mac Studio, but it didn't seem to get much interest. I made this agent because I had not found any open-source researchers that worked well with searxng on my mac. Maybe there are already a million on github.&lt;/p&gt; &lt;p&gt;I thought sharing the code would make it more interesting to people. &lt;/p&gt; &lt;p&gt;Here is the repo: &lt;a href="https://github.com/cgh76860-lab/Vibe_Coded_Research_Agent--VCRA-"&gt;https://github.com/cgh76860-lab/Vibe_Coded_Research_Agent--VCRA-&lt;/a&gt; &lt;/p&gt; &lt;p&gt;This was coded in Cline using about 17 million input tokens and 275k output tokens. It took maybe 30 hours of Mac studio time, though I wasn't keeping track. It is definitely messy and some of the features, like profiles, I haven't tested. But it produces fairly lengthy and coherent reports.&lt;/p&gt; &lt;p&gt;I use GPT OSS 120 for the research agent itself.&lt;/p&gt; &lt;p&gt;Sample single-cycle report is available at link above.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nomorebuttsplz"&gt; /u/nomorebuttsplz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3k1et/vibe_coded_research_agent_repo/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3k1et/vibe_coded_research_agent_repo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3k1et/vibe_coded_research_agent_repo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T02:25:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3obc8</id>
    <title>Get LLM to Query SQL database</title>
    <updated>2025-10-11T06:22:55+00:00</updated>
    <author>
      <name>/u/thinktank99</name>
      <uri>https://old.reddit.com/user/thinktank99</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I want an LLM to parse some XMLs and generate a summary. There are data elememnts in the xml which have description stored in database tables. The tables have about 50k rows so I cant just extract them and attach it to the prompt for the LLM to refer. &lt;/p&gt; &lt;p&gt;How do I get the LLM to query the database table if needs to get the description for data elements?&lt;/p&gt; &lt;p&gt;I am using a python script to read the XMLs and call OLLAMA API to generate a summary.&lt;/p&gt; &lt;p&gt;Any help would be appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thinktank99"&gt; /u/thinktank99 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3obc8/get_llm_to_query_sql_database/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3obc8/get_llm_to_query_sql_database/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3obc8/get_llm_to_query_sql_database/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T06:22:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3ouqm</id>
    <title>Self Hosted AI Advise for Repetative Work Task</title>
    <updated>2025-10-11T06:56:05+00:00</updated>
    <author>
      <name>/u/voyto</name>
      <uri>https://old.reddit.com/user/voyto</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have successfully managed to train both CoPilot and ChatGPT to carry out a fairly repetative task I have at work. This involves going through pages of timber cutting details and combining in efficient ways to create the least amount of waste with the timbers that we stock. The output would be a PDF (or similar) report that can be printed out.&lt;/p&gt; &lt;p&gt;I can see that if I put more time into fine tuning the way it works, it would only further improve.&lt;/p&gt; &lt;p&gt;Before I go down that path, I'd be much more comfortable hosting my own version so these resources aren't either taken away without notice, or put behind a hefty paywall.&lt;/p&gt; &lt;p&gt;Are there any good self-hosted solutions (ideally something that runs dockerised or within a Proxmox container) that would be good at carrying out this sort of task?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/voyto"&gt; /u/voyto &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3ouqm/self_hosted_ai_advise_for_repetative_work_task/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3ouqm/self_hosted_ai_advise_for_repetative_work_task/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3ouqm/self_hosted_ai_advise_for_repetative_work_task/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T06:56:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3bhad</id>
    <title>Building a multi-agent financial bot using Agno, Maxim, and YFinance</title>
    <updated>2025-10-10T20:06:32+00:00</updated>
    <author>
      <name>/u/Otherwise_Flan7339</name>
      <uri>https://old.reddit.com/user/Otherwise_Flan7339</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;was experimenting with Agno for multi-agent orchestration and paired it with Maxim for tracing and observability. The setup follows a cookbook that walks through building a financial conversational agent with Agno, YFinance, and OpenAI models, while instrumenting everything for full visibility.&lt;/p&gt; &lt;p&gt;Here’s the core workflow:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Agent setup&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Defined two agents in Agno: &lt;ul&gt; &lt;li&gt;&lt;em&gt;Finance agent&lt;/em&gt;: uses YFinance and OpenAI GPT-4 for structured financial data.&lt;/li&gt; &lt;li&gt;&lt;em&gt;Web agent&lt;/em&gt;: uses Serper or a similar search API to pull recent company news.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Coordination layer&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Agno handles task routing and message passing between these agents.&lt;/li&gt; &lt;li&gt;Both agents are instrumented via Maxim’s SDK, which captures traces, tool calls, model usage, and metadata for every step.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Observability with Maxim&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Traces every LLM call, agent step, and tool execution.&lt;/li&gt; &lt;li&gt;Exposes performance metrics and intermediate reasoning chains.&lt;/li&gt; &lt;li&gt;Makes debugging multi-agent flows much easier since you can see which component (model, tool, or agent) caused latency or failure.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Interactive loop&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;A basic REPL setup allows real-time queries like:“Summarize the latest financial news on NVIDIA and show its current stock stats.”&lt;/li&gt; &lt;li&gt;The system delegates parts of the query across agents, aggregates results, and returns the final response.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Some observations&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Tracing multi-agent systems quickly becomes essential as orchestration complexity grows.&lt;/li&gt; &lt;li&gt;You trade off some latency for much clearer visibility.&lt;/li&gt; &lt;li&gt;The hardest part is correlating traces across asynchronous tool calls.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Would love to compare how people handle trace correlation and debugging workflows in larger agent networks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Otherwise_Flan7339"&gt; /u/Otherwise_Flan7339 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3bhad/building_a_multiagent_financial_bot_using_agno/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3bhad/building_a_multiagent_financial_bot_using_agno/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3bhad/building_a_multiagent_financial_bot_using_agno/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T20:06:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3n5am</id>
    <title>Gwen, pangu, deepseek , Kimi K2 or what else performs/write better to power a low cost platform to write resume and cover letter that matches Job descriptions? No coding tasks</title>
    <updated>2025-10-11T05:15:09+00:00</updated>
    <author>
      <name>/u/Possible_tuastu_17</name>
      <uri>https://old.reddit.com/user/Possible_tuastu_17</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I love , LOVE Claude in terms of writing (like a real recruiter but better) but is really expensive for this project and I need to keep cost down until I prove that the app can make some money &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Possible_tuastu_17"&gt; /u/Possible_tuastu_17 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3n5am/gwen_pangu_deepseek_kimi_k2_or_what_else/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3n5am/gwen_pangu_deepseek_kimi_k2_or_what_else/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3n5am/gwen_pangu_deepseek_kimi_k2_or_what_else/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T05:15:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1o32j5s</id>
    <title>Beck, a small model for delicate life situations</title>
    <updated>2025-10-10T14:32:08+00:00</updated>
    <author>
      <name>/u/antcroca159</name>
      <uri>https://old.reddit.com/user/antcroca159</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A language model that handles delicate life situations and tries to really help you.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/gustavecortal/Beck-8B"&gt;Beck&lt;/a&gt; is based on my previous model &lt;a href="https://huggingface.co/gustavecortal/Piaget-8B"&gt;Piaget&lt;/a&gt; (a language model for psychological reasoning), and was aligned with &lt;a href="https://huggingface.co/datasets/Psychotherapy-LLM/PsychoCounsel-Preference"&gt;psychotherapeutic preferences&lt;/a&gt;. Available sizes are: &lt;a href="https://huggingface.co/gustavecortal/Beck-0.6B"&gt;0.6B&lt;/a&gt;, &lt;a href="https://huggingface.co/gustavecortal/Beck-1.7B"&gt;1.7B&lt;/a&gt;, &lt;a href="https://huggingface.co/gustavecortal/Beck-4B"&gt;4B&lt;/a&gt;, &lt;a href="https://huggingface.co/gustavecortal/Beck-8B"&gt;8B&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Beck was trained using preference optimization (ORPO) and LoRA. You can reproduce the results using &lt;a href="https://github.com/gustavecortal/preference-optimization-orpo-lora"&gt;my repo&lt;/a&gt; for lightweight preference optimization.&lt;/p&gt; &lt;p&gt;Beck was inspired by my position paper on emotion analysis: &lt;a href="https://aclanthology.org/2024.cmcl-1.23/"&gt;Improving Language Models for Emotion Analysis: Insights from Cognitive Science&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How does it compare to other language models?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;For the given prompt:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Anxiety often strikes when I'm faced with making decisions. The fear of making the wrong choice or disappointing others paralyzes me, leaving me stuck in indecision. I want to learn how to trust myself and make confident choices. \no_think &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Qwen3-8B&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;It's completely natural to feel anxious when facing important decisions, especially when you're worried about making the wrong choice or letting others down. This internal conflict can create a sense of overwhelm that makes it hard to move forward. Learning to trust yourself is a powerful step toward overcoming this challenge. Start by reflecting on past decisions—what worked, what didn't, and what you learned from them. Over time, you'll begin to recognize your own values, strengths, and patterns of thinking. Practicing self-compassion is also key; remind yourself that no decision is perfect and that growth comes from experience, not perfection. Small steps, like setting aside time each day for reflection or journaling, can help build confidence. Surround yourself with people who support your journey, and don't hesitate to seek guidance if needed. Trusting yourself isn’t about having all the answers—it’s about believing in your ability to navigate uncertainty and grow through the process. With patience and practice, you’ll find greater clarity and courage in your choices. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Beck-8B&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;It sounds like you're dealing with a lot of pressure regarding decision-making and it's causing you significant distress. This is quite common for many people, as we all have moments where choosing can feel overwhelming. Let’s explore what might be contributing to these feelings so that we can work on building your confidence in decision-making. Firstly, consider what specific fears arise when you think about making a decision. Is there a particular aspect of the choice that triggers anxiety? Sometimes identifying just one component can help in breaking down the process. Also, think about past decisions that went well—even if they don’t seem big at first. Remembering successful outcomes can reinforce trust in yourself, reminding you that you have experience navigating through uncertainties successfully before. Building self-trust involves challenging any negative thoughts surrounding decision-making. It may also help to practice mindfulness—being fully present during each step of the decision process rather than anticipating worst-case scenarios. By cultivating patience and acceptance towards ambiguity, you might find that some anxieties lessen over time. Would you like us to brainstorm practical exercises or strategies tailored specifically for boosting your decision-making abilities and confidence? &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/antcroca159"&gt; /u/antcroca159 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o32j5s/beck_a_small_model_for_delicate_life_situations/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o32j5s/beck_a_small_model_for_delicate_life_situations/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o32j5s/beck_a_small_model_for_delicate_life_situations/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T14:32:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3o82j</id>
    <title>Weekend project: Chrome extension that adds AI to LinkedIn (update)</title>
    <updated>2025-10-11T06:17:33+00:00</updated>
    <author>
      <name>/u/ExtremeKangaroo5437</name>
      <uri>https://old.reddit.com/user/ExtremeKangaroo5437</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3o82j/weekend_project_chrome_extension_that_adds_ai_to/"&gt; &lt;img alt="Weekend project: Chrome extension that adds AI to LinkedIn (update)" src="https://external-preview.redd.it/hiNIGpbPDxgFgpuoHOvVZWxcAMuHFtozA3IOqFUgYbw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=efade39d3855bbd8b33fc10929495edbcb3f0625" title="Weekend project: Chrome extension that adds AI to LinkedIn (update)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Weekend project: Chrome extension that adds AI to LinkedIn (update)&lt;/p&gt; &lt;p&gt;Open Sourced: Just wrapped up a fun weekend project - a Chrome extension that brings AI directly into LinkedIn's interface.&lt;/p&gt; &lt;p&gt;The extension:&lt;br /&gt; Adds AI buttons to LinkedIn posts/comments&lt;br /&gt; Supports both cloud APIs and local models&lt;br /&gt; Can analyze images and videos from posts&lt;br /&gt; Context-aware prompts for different scenarios&lt;/p&gt; &lt;p&gt;Why I built it:&lt;/p&gt; &lt;p&gt;Wanted to explore the nuances of AI API integrations and browser extension development. The vision capabilities were particularly interesting to implement - extracting and analyzing media content directly from LinkedIn posts.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/gowrav-vishwakarma/useless-linkedin-ai-writer"&gt;https://github.com/gowrav-vishwakarma/useless-linkedin-ai-writer&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What weekend projects have you been working on? Always curious to see what others are building for fun!&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1o3o82j/video/fh08qtfldfuf1/player"&gt;https://reddit.com/link/1o3o82j/video/fh08qtfldfuf1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ExtremeKangaroo5437"&gt; /u/ExtremeKangaroo5437 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3o82j/weekend_project_chrome_extension_that_adds_ai_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3o82j/weekend_project_chrome_extension_that_adds_ai_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3o82j/weekend_project_chrome_extension_that_adds_ai_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T06:17:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1o31rdl</id>
    <title>Kwaipilot/KAT-Dev-72B-Exp model released</title>
    <updated>2025-10-10T14:02:02+00:00</updated>
    <author>
      <name>/u/UltrMgns</name>
      <uri>https://old.reddit.com/user/UltrMgns</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The model makers claim it's second on coding only to Sonnet 4.5 at only 72B parameters.&lt;br /&gt; Could someone here who has the hardware to run it, validate this? &lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Kwaipilot/KAT-Dev-72B-Exp"&gt;https://huggingface.co/Kwaipilot/KAT-Dev-72B-Exp&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/UltrMgns"&gt; /u/UltrMgns &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o31rdl/kwaipilotkatdev72bexp_model_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o31rdl/kwaipilotkatdev72bexp_model_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o31rdl/kwaipilotkatdev72bexp_model_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T14:02:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3jl8r</id>
    <title>Here are the benchmarks that I keep up with</title>
    <updated>2025-10-11T02:02:27+00:00</updated>
    <author>
      <name>/u/SomeOddCodeGuy_v2</name>
      <uri>https://old.reddit.com/user/SomeOddCodeGuy_v2</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey hey folks! I've returned... in a fashion.&lt;/p&gt; &lt;p&gt;I've been sitting on all kinds of stuff that I wanted to talk about for the past few months, but I figured I'd start by dropping the list of benchmarks I currently track, since in the past folks were interested in that list.&lt;/p&gt; &lt;p&gt;These should be mostly up to date, and I'm constantly on the prowl for more. If you have any good ones (&lt;em&gt;ESPECIALLY translation benchmarks... those feel like the holy grail&lt;/em&gt;), please share.&lt;/p&gt; &lt;p&gt;I know there are a lot more leaderboards out there, but I generally don't hang on to the ones that either aren't kept reasonably up to date, or were exceptionally limited. So if you don't see a leaderboard on here, feel free to share but it may have been excluded on purpose.&lt;/p&gt; &lt;p&gt;As always- benchmarks aren't everything, and you should always try the models out yourself. But it definitely is nice to have some metrics to look at from time to time, even if they can get gamed.&lt;/p&gt; &lt;h1&gt;Code Specific&lt;/h1&gt; &lt;p&gt;&lt;a href="https://www.swebench.com/"&gt;SWE Bench&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://aider.chat/docs/leaderboards/"&gt;Aider Coding Leaderboard&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Context Window Capability&lt;/h1&gt; &lt;p&gt;&lt;a href="https://fiction.live/stories/Fiction-liveBench-Feb-21-2025/oQdzQvKHw8JyXbN87"&gt;FictionBench&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;em&gt;(This is a really good one, as it visualizes where so many people mess up with LLMs: not realizing context window limitations)&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;General Ability&lt;/h1&gt; &lt;p&gt;&lt;a href="https://livebench.ai/#/"&gt;Livebench&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://dubesor.de/benchtable"&gt;Dubesor Benchtable&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://scale.com/leaderboard/humanitys_last_exam_text_only"&gt;Humanity's Last Exam&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;em&gt;(I am shocked at how low of a score GLM 4.5 got here... testing error maybe?)&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Domain Knowledge&lt;/h1&gt; &lt;p&gt;&lt;a href="https://huggingface.co/spaces/TIGER-Lab/MMLU-Pro"&gt;MMLU-Pro&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Advanced Reasoning&lt;/h1&gt; &lt;p&gt;&lt;a href="https://scale.com/leaderboard/enigma_eval"&gt;Enigma Eval&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Human Preference&lt;/h1&gt; &lt;p&gt;&lt;a href="https://lmarena.ai/leaderboard"&gt;LM Arena&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;EQ (emotional intelligence) and Creative Writing Ability&lt;/h1&gt; &lt;p&gt;&lt;a href="https://eqbench.com/"&gt;EQBench&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Censorship&lt;/h1&gt; &lt;p&gt;&lt;a href="https://huggingface.co/spaces/DontPlanToEnd/UGI-Leaderboard"&gt;Uncensored General Intelligence Leaderboard&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Intelligence Index, Cost, Speed, and Model Comparisons&lt;/h1&gt; &lt;p&gt;&lt;a href="https://artificialanalysis.ai/leaderboards/models"&gt;Artificial Analysis&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Coding Agent Capability&lt;/h1&gt; &lt;p&gt;&lt;a href="https://www.tbench.ai/leaderboard"&gt;Terminal Bench&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Kotlin (Android dev)&lt;/h1&gt; &lt;p&gt;&lt;a href="https://firebender.com/leaderboard"&gt;Kotlin Leaderboard&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Function Calling&lt;/h1&gt; &lt;p&gt;&lt;a href="https://gorilla.cs.berkeley.edu/leaderboard.html"&gt;Berkeley Function-Calling Leaderboard&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Other&lt;/h1&gt; &lt;p&gt;&lt;a href="https://www.vellum.ai/llm-leaderboard"&gt;Vellum Leaderboard&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SomeOddCodeGuy_v2"&gt; /u/SomeOddCodeGuy_v2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3jl8r/here_are_the_benchmarks_that_i_keep_up_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3jl8r/here_are_the_benchmarks_that_i_keep_up_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3jl8r/here_are_the_benchmarks_that_i_keep_up_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T02:02:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1o2wkyw</id>
    <title>We can now run wan or any heavy models even on a 6GB NVIDIA laptop GPU | Thanks to upcoming GDS integration in comfy</title>
    <updated>2025-10-10T09:50:47+00:00</updated>
    <author>
      <name>/u/maifee</name>
      <uri>https://old.reddit.com/user/maifee</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2wkyw/we_can_now_run_wan_or_any_heavy_models_even_on_a/"&gt; &lt;img alt="We can now run wan or any heavy models even on a 6GB NVIDIA laptop GPU | Thanks to upcoming GDS integration in comfy" src="https://b.thumbs.redditmedia.com/p7rn0Ix6tXlo4gn9p81TJIfGWsn4o8QzjNEOzhKU77w.jpg" title="We can now run wan or any heavy models even on a 6GB NVIDIA laptop GPU | Thanks to upcoming GDS integration in comfy" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello&lt;/p&gt; &lt;p&gt;I am Maifee. I am integrating GDS (GPU Direct Storage) in ComfyUI. And it's working, if you want to test, just do the following:&lt;/p&gt; &lt;p&gt;&lt;code&gt; git clone https://github.com/maifeeulasad/ComfyUI.git cd ComfyUI git checkout offloader-maifee python3 main.py --enable-gds --gds-stats # gds enabled run &lt;/code&gt;&lt;/p&gt; &lt;p&gt;And you no longer need custome offloader, or just be happy with quantized version. Or you don't even have to wait. Just run with GDS enabled flag and we are good to go. Everything will be handled for you. I have already created issue and raised MR, review is going on, hope this gets merged real quick.&lt;/p&gt; &lt;p&gt;If you have some suggestions or feedback, please let me know.&lt;/p&gt; &lt;p&gt;And thanks to these helpful sub reddits, where I got so many advices, and trust me it was always more than enough.&lt;/p&gt; &lt;p&gt;Enjoy your weekend!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/maifee"&gt; /u/maifee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1o2wkyw"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2wkyw/we_can_now_run_wan_or_any_heavy_models_even_on_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o2wkyw/we_can_now_run_wan_or_any_heavy_models_even_on_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T09:50:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1o387tc</id>
    <title>Benchmarking LLM Inference on RTX 4090 / RTX 5090 / RTX PRO 6000 #2</title>
    <updated>2025-10-10T18:02:25+00:00</updated>
    <author>
      <name>/u/NoVibeCoding</name>
      <uri>https://old.reddit.com/user/NoVibeCoding</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o387tc/benchmarking_llm_inference_on_rtx_4090_rtx_5090/"&gt; &lt;img alt="Benchmarking LLM Inference on RTX 4090 / RTX 5090 / RTX PRO 6000 #2" src="https://external-preview.redd.it/goJG5hCL1_yH4_KvjjKtG66z1VpylH-D59Sj44-rYo4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=41d29fb95277e1105a79cd48e9201de0ef0ede48" title="Benchmarking LLM Inference on RTX 4090 / RTX 5090 / RTX PRO 6000 #2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi LocalLlama community. I present an LLM inference throughput benchmark for RTX4090 / RTX5090 / PRO6000 GPUs based on vllm serving and &lt;strong&gt;vllm bench serve&lt;/strong&gt; client benchmarking tool.&lt;/p&gt; &lt;p&gt;&lt;a href="https://medium.com/ai-advances/rtx-4090-vs-rtx-5090-vs-rtx-pro-6000-7154a3ac4f90"&gt;Full article on Medium&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.cloudrift.ai/blog/benchmarking-rtx-gpus-for-llm-inference"&gt;Non-medium link&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Benchmarking Setup&lt;/h1&gt; &lt;p&gt;The hardware configurations used:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;1x4090, 2x4090, 4x4090&lt;/li&gt; &lt;li&gt;1x5090; 2x5090; 4x5090&lt;/li&gt; &lt;li&gt;1x6000&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;All machines have at least 50GB of RAM per GPU with a minimum of 7 cores. The 4090 machines utilize the EPYC Milan (3rd Gen) processor, while the 5090/6000 models employ the EPYC Genoa (4th Gen) processor, resulting in slightly faster overall performance.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;I have optimized the benchmark setup for throughput&lt;/strong&gt;. VLLM serves models. The model is split across multiple GPUs using the --pipeline-parallel-size VLLM option, if needed. I run as many VLLM instances as possible, using an NGINX load balancer on top to distribute requests across them and maximize throughput (replica parallelism). For example, if only two GPUs are required to run the model on a 4-GPU machine, I run two VLLM instances with --pipeline-parallel-size=2 and an NGINX load balancer. If all four GPUs are required, then a single VLLM instance with --pipeline-parallel-size=4 is used.&lt;/p&gt; &lt;p&gt;The &lt;strong&gt;vllm bench serve&lt;/strong&gt; tool is used for benchmarking with random data and a sequence length of 1000. The number of concurrent requests is set to 400 to ensure saturation of the LLM token generation capacity.&lt;/p&gt; &lt;p&gt;I have benchmarked three different models to understand better the effect of PCIe communication on the final LLM performance. I have tried to find the largest modern model that fits into a single 4090, two 4090s, and four 4090s. It would be possible to fit larger GGUF models, but VLLM poorly supports GGUF, and I wanted to use VLLM because it is optimized for high-throughput serving.&lt;/p&gt; &lt;p&gt;Here is the model selection and the logic behind it:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Qwen3-Coder-30B-A3B-Instruct-AWQ (fits 24GB).&lt;/strong&gt; This 4-bit quantized model fits into a single RTX4090. Thus, scaling the number of GPUs yields a linear scale in throughput, so 4 x 4090 and 4 x 5090 configurations should have an edge as they have more raw compute power.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Meta-Llama-3.3-70B-Instruct-AWQ-INT4 (fits 48GB).&lt;/strong&gt; This 4-bit quantized model fits into 2 x 4090. Some communication over PCIe can lower the performance of multi-GPU setups.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GLM-4.5-Air-AWQ-4bit (fits 96GB).&lt;/strong&gt; This model requires all four 4090s, so PCIE communication will likely be a bottleneck, and Pro 6000 should have an edge.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Besides raw throughput, graphs contain the serving cost per million tokens for the respective model on the respective hardware. The rental price is set to $0.39 per hour for 4090, $0.65 for 5090, and $1.29 for Pro 6000. These prices are typical for GPU rentals at &lt;a href="http://neuralrack.ai"&gt;neuralrack.ai&lt;/a&gt;, which provided the hardware for this benchmark. You can adjust the GPU price in the &lt;a href="https://github.com/cloudrift-ai/server-benchmark/blob/main/config.yaml"&gt;config.yml&lt;/a&gt; file in the &lt;a href="https://github.com/cloudrift-ai/server-benchmark"&gt;benchmark repository&lt;/a&gt; and invoke &lt;strong&gt;make report&lt;/strong&gt; to generate a new report that better reflects your situation.&lt;/p&gt; &lt;h1&gt;Results&lt;/h1&gt; &lt;p&gt;The &lt;strong&gt;overall winner is RTX PRO 6000&lt;/strong&gt; for its consistent performance across all model sizes and best cost-efficiency for larger models. However, if your workload primarily involves smaller models, the multi-GPU RTX 5090 can offer better absolute throughput at a lower cost.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Small Models (fits 24GB)&lt;/strong&gt;: Multi-GPU consumer configurations offer the best value due to replica parallelism, but RTX PRO 6000 is very close.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Medium Models (fits 48GB)&lt;/strong&gt;: RTX 5090 configuration provides the best balance of performance and cost, followed by RTX PRO 6000.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Large Models (fits 96GB)&lt;/strong&gt;: RTX PRO 6000 emerges as the clear winner despite its higher hourly cost, thanks to the elimination of PCIe overhead.&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/iec6lp29qbuf1.gif"&gt;Price is in millidollars, i.e. around $0.04&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/78gf2bnbqbuf1.gif"&gt;https://i.redd.it/78gf2bnbqbuf1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/bc529zlcqbuf1.gif"&gt;https://i.redd.it/bc529zlcqbuf1.gif&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Code and Resources&lt;/h1&gt; &lt;p&gt;The code is available &lt;a href="https://github.com/cloudrift-ai/server-benchmark"&gt;here&lt;/a&gt;. Instructions for performing your own benchmark are in the README. You can find the benchmark data in the results folder. Each benchmark logs the result, the Docker Compose file used for serving, and the benchmarking command like this:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;============ Serving Benchmark Result ============ Successful requests: 1200 Maximum request concurrency: 400 Benchmark duration (s): 980.85 Total input tokens: 1196743 Total generated tokens: 1200000 Request throughput (req/s): 1.22 Output token throughput (tok/s): 1223.42 Peak output token throughput (tok/s): 3343.00 Peak concurrent requests: 408.00 Total Token throughput (tok/s): 2443.53 ---------------Time to First Token---------------- Mean TTFT (ms): 158275.93 Median TTFT (ms): 166262.87 P99 TTFT (ms): 273238.49 -----Time per Output Token (excl. 1st token)------ Mean TPOT (ms): 134.71 Median TPOT (ms): 123.86 P99 TPOT (ms): 216.70 ---------------Inter-token Latency---------------- Mean ITL (ms): 134.57 Median ITL (ms): 55.98 P99 ITL (ms): 1408.24 ----------------End-to-end Latency---------------- Mean E2EL (ms): 292848.13 Median E2EL (ms): 311149.01 P99 E2EL (ms): 399504.14 ================================================== ============ Docker Compose Configuration ============ services: vllm_0: image: vllm/vllm-openai:latest container_name: vllm_benchmark_container_0 deploy: resources: reservations: devices: - driver: nvidia device_ids: ['0', '1'] capabilities: [gpu] volumes: - /hf_models:/hf_models environment: - HUGGING_FACE_HUB_TOKEN= ports: - &amp;quot;8000:8000&amp;quot; shm_size: '16gb' ipc: host command: &amp;gt; --trust-remote-code --gpu-memory-utilization=0.9 --host 0.0.0.0 --port 8000 --tensor-parallel-size 1 --pipeline-parallel-size 2 --model /hf_models/ibnzterrell/Meta-Llama-3.3-70B-Instruct-AWQ-INT4 --served-model-name ibnzterrell/Meta-Llama-3.3-70B-Instruct-AWQ-INT4 --max-model-len 8192 --kv-cache-dtype fp8 healthcheck: test: [&amp;quot;CMD&amp;quot;, &amp;quot;bash&amp;quot;, &amp;quot;-c&amp;quot;, &amp;quot;curl -f http://localhost:8000/health &amp;amp;&amp;amp; curl -f http://localhost:8000/v1/models | grep -q 'object.*list'&amp;quot;] interval: 10s timeout: 10s retries: 180 start_period: 600s vllm_1: image: vllm/vllm-openai:latest container_name: vllm_benchmark_container_1 deploy: resources: reservations: devices: - driver: nvidia device_ids: ['2', '3'] capabilities: [gpu] volumes: - /hf_models:/hf_models environment: - HUGGING_FACE_HUB_TOKEN= ports: - &amp;quot;8001:8000&amp;quot; shm_size: '16gb' ipc: host command: &amp;gt; --trust-remote-code --gpu-memory-utilization=0.9 --host 0.0.0.0 --port 8000 --tensor-parallel-size 1 --pipeline-parallel-size 2 --model /hf_models/ibnzterrell/Meta-Llama-3.3-70B-Instruct-AWQ-INT4 --served-model-name ibnzterrell/Meta-Llama-3.3-70B-Instruct-AWQ-INT4 --max-model-len 8192 --kv-cache-dtype fp8 healthcheck: test: [&amp;quot;CMD&amp;quot;, &amp;quot;bash&amp;quot;, &amp;quot;-c&amp;quot;, &amp;quot;curl -f http://localhost:8000/health &amp;amp;&amp;amp; curl -f http://localhost:8000/v1/models | grep -q 'object.*list'&amp;quot;] interval: 10s timeout: 10s retries: 180 start_period: 600s nginx: image: nginx:alpine container_name: nginx_lb ports: - &amp;quot;8080:8080&amp;quot; volumes: - /home/riftuser/server-benchmark/nginx.vllm.conf:/etc/nginx/nginx.conf:ro depends_on: - vllm_0 - vllm_1 benchmark: image: vllm/vllm-openai:latest container_name: vllm_benchmark_client deploy: resources: reservations: devices: - driver: nvidia count: all capabilities: [gpu] volumes: - /hf_models:/hf_models environment: - HUGGING_FACE_HUB_TOKEN= - CUDA_VISIBLE_DEVICES=&amp;quot;&amp;quot; entrypoint: [&amp;quot;/bin/bash&amp;quot;, &amp;quot;-c&amp;quot;] command: [&amp;quot;sleep infinity&amp;quot;] profiles: - tools ============ Benchmark Command ============ vllm bench serve --model ibnzterrell/Meta-Llama-3.3-70B-Instruct-AWQ-INT4 --dataset-name random --random-input-len 1000 --random-output-len 1000 --max-concurrency 400 --num-prompts 1200 --ignore-eos --backend openai-chat --endpoint /v1/chat/completions --percentile-metrics ttft,tpot,itl,e2el --base-url http://nginx_lb:8080 ============================================================== Serving Benchmark Result ============ Successful requests: 1200 Maximum request concurrency: 400 Benchmark duration (s): 980.85 Total input tokens: 1196743 Total generated tokens: 1200000 Request throughput (req/s): 1.22 Output token throughput (tok/s): 1223.42 Peak output token throughput (tok/s): 3343.00 Peak concurrent requests: 408.00 Total Token throughput (tok/s): 2443.53 ---------------Time to First Token---------------- Mean TTFT (ms): 158275.93 Median TTFT (ms): 166262.87 P99 TTFT (ms): 273238.49 -----Time per Output Token (excl. 1st token)------ Mean TPOT (ms): 134.71 Median TPOT (ms): 123.86 P99 TPOT (ms): 216.70 ---------------Inter-token Latency---------------- Mean ITL (ms): 134.57 Median ITL (ms): 55.98 P99 ITL (ms): 1408.24 ----------------End-to-end Latency---------------- Mean E2EL (ms): 292848.13 Median E2EL (ms): 311149.01 P99 E2EL (ms): 399504.14 ================================================== ============ Docker Compose Configuration ============ services: vllm_0: image: vllm/vllm-openai:latest container_name: vllm_benchmark_container_0 deploy: resources: reservations: devices: - driver: nvidia device_ids: ['0', '1'] capabilities: [gpu] volumes: - /hf_models:/hf_models environment: - HUGGING_FACE_HUB_TOKEN= ports: - &amp;quot;8000:8000&amp;quot; shm_size: '16gb' ipc: host command: &amp;gt; --trust-remote-code --gpu-memory-utilization=0.9 --host 0.0.0.0 --port 8000 --tensor-parallel-size 1 --pipeline-parallel-size 2 --model /hf_models/ibnzterrell/Meta-Llama-3.3-70B-Instruct-AWQ-INT4 --served-model-name ibnzterrell/Meta-Llama-3.3-70B-Instruct-AWQ-INT4 --max-model-len 8192 --kv-cache-dtype fp8 healthcheck: test: [&amp;quot;CMD&amp;quot;, &amp;quot;bash&amp;quot;, &amp;quot;-c&amp;quot;, &amp;quot;curl -f http://localhost:8000/health &amp;amp;&amp;amp; curl -f http://localhost:8000/v1/models | grep -q 'object.*list'&amp;quot;] interval: 10s timeout: 10s retries: 180 start_period: 600s vllm_1: image: vllm/vllm-openai:latest container_name: vllm_benchmark_container_1 deploy: resources: reservations: devices: - driver: nvidia device_ids: ['2', '3'] capabilities: [gpu] volumes: - /hf_models:/hf_models environment: - HUGGING_FACE_HUB_TOKEN= ports: - &amp;quot;8001:8000&amp;quot; shm_size: '16gb' ipc: host command: &amp;gt; --trust-remote-code --gpu-memory-utilization=0.9 --host 0.0.0.0 --port 8000 --tensor-parallel-size 1 --pipeline-parallel-size 2 --model /hf_models/ibnzterrell/Meta-Llama-3.3-70B-Instruct-AWQ-INT4 --served-model-name ibnzterrell/Meta-Llama-3.3-70B-Instruct-AWQ-INT4 --max-model-len 8192 --kv-cache-dtype fp8 healthcheck: test: [&amp;quot;CMD&amp;quot;, &amp;quot;bash&amp;quot;, &amp;quot;-c&amp;quot;, &amp;quot;curl -f http://localhost:8000/health &amp;amp;&amp;amp; curl -f http://localhost:8000/v1/models | grep -q 'object.*list'&amp;quot;] interval: 10s timeout: 10s retries: 180 start_period: 600s nginx: image: nginx:alpine container_name: nginx_lb ports: - &amp;quot;8080:8080&amp;quot; volumes: - /home/riftuser/server-benchmark/nginx.vllm.conf:/etc/nginx/nginx.conf:ro depends_on: - vllm_0 - vllm_1 benchmark: image: vllm/vllm-openai:latest container_name: vllm_benchmark_client deploy: resources: reservations: devices: - driver: nvidia count: all capabilities: [gpu] volumes: - /hf_models:/hf_models environment: - HUGGING_FACE_HUB_TOKEN= - CUDA_VISIBLE_DEVICES=&amp;quot;&amp;quot; entrypoint: [&amp;quot;/bin/bash&amp;quot;, &amp;quot;-c&amp;quot;] command: [&amp;quot;sleep infinity&amp;quot;] profiles: - tools ============ Benchmark Command ============ vllm bench serve --model ibnzterrell/Meta-Llama-3.3-70B-Instruct-AWQ-INT4 --dataset-name random --random-input-len 1000 --random-output-len 1000 --max-concurrency 400 --num-prompts 1200 --ignore-eos --backend openai-chat --endpoint /v1/chat/completions --percentile-metrics ttft,tpot,itl,e2el --base-url http://nginx_lb:8080 ================================================== &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Future Work&lt;/h1&gt; &lt;p&gt;This work is an enhanced version of the &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nr9arw/benchmarking_llm_inference_on_rtx_4090_rtx_5090/"&gt;benchmark previously shared with the community&lt;/a&gt;. Thank you, everyone, for your feedback. Please let me know if you have any concerns with the benchmarking methodology or would like to see other benchmarks in the future. I am thinking of benchmarking multi-RTX PRO 6000 vs multi-H200 setups on large models.&lt;/p&gt; &lt;h1&gt;Updates&lt;/h1&gt; &lt;p&gt;- Thanks &lt;a href="/u/kryptkpr"&gt;u/kryptkpr&lt;/a&gt; for suggesting options for making benchmark work with tensor parallelism instead of the pipeline parallelism. The tensor parallelism performance is lower, so keeping the results with pipeline parallelism in the post body.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NoVibeCoding"&gt; /u/NoVibeCoding &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o387tc/benchmarking_llm_inference_on_rtx_4090_rtx_5090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o387tc/benchmarking_llm_inference_on_rtx_4090_rtx_5090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o387tc/benchmarking_llm_inference_on_rtx_4090_rtx_5090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T18:02:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3604u</id>
    <title>[AutoBE] achieved 100% compilation success of backend generation with "qwen3-next-80b-a3b-instruct"</title>
    <updated>2025-10-10T16:40:27+00:00</updated>
    <author>
      <name>/u/jhnam88</name>
      <uri>https://old.reddit.com/user/jhnam88</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3604u/autobe_achieved_100_compilation_success_of/"&gt; &lt;img alt="[AutoBE] achieved 100% compilation success of backend generation with &amp;quot;qwen3-next-80b-a3b-instruct&amp;quot;" src="https://a.thumbs.redditmedia.com/EmkcCT6MgrWM8fKTEUX7jLkPylt878ETs4RQRjYme30.jpg" title="[AutoBE] achieved 100% compilation success of backend generation with &amp;quot;qwen3-next-80b-a3b-instruct&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/wrtnlabs/autobe"&gt;AutoBE&lt;/a&gt; is an open-source project that serves as an agent capable of automatically generating backend applications through conversations with AI chatbots.&lt;/p&gt; &lt;p&gt;AutoBE aims to generate 100% functional backend applications, and we recently achieved 100% compilation success for backend applications even with local AI models like &lt;code&gt;qwen3-next-80b-a3b&lt;/code&gt; (also mini models of GPTs). This represents a significant improvement over our previous attempts with &lt;code&gt;qwen3-next-80b-a3b&lt;/code&gt;, where most projects failed to build due to compilation errors, even though we managed to generate backend applications.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Dark background screenshots: After AutoBE improvements &lt;ul&gt; &lt;li&gt;100% compilation success doesn't necessarily mean 100% runtime success&lt;/li&gt; &lt;li&gt;Shopping Mall failed due to excessive input token size&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Light background screenshots: Before AutoBE improvements &lt;ul&gt; &lt;li&gt;Many failures occurred with &lt;code&gt;gpt-4.1-mini&lt;/code&gt; and &lt;code&gt;qwen3-next-80b-a3b&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Project&lt;/th&gt; &lt;th&gt;&lt;code&gt;qwen3-next-80b-a3b&lt;/code&gt;&lt;/th&gt; &lt;th&gt;&lt;code&gt;gpt-4.1&lt;/code&gt;&lt;/th&gt; &lt;th&gt;&lt;code&gt;gpt-5&lt;/code&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;To Do List&lt;/td&gt; &lt;td&gt;&lt;a href="https://github.com/wrtnlabs/autobe-example-todo-qwen-qwen3-next-80b-a3b-instruct"&gt;To Do&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href="https://github.com/wrtnlabs/autobe-example-todo-openai-gpt-4.1"&gt;Big&lt;/a&gt; / &lt;a href="https://github.com/wrtnlabs/autobe-example-todo-openai-gpt-4.1-mini"&gt;Mini&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href="https://github.com/wrtnlabs/autobe-example-todo-openai-gpt-5"&gt;Big&lt;/a&gt; / &lt;a href="https://github.com/wrtnlabs/autobe-example-todo-openai-gpt-5-mini"&gt;Mini&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Economic Discussion&lt;/td&gt; &lt;td&gt;&lt;a href="https://github.com/wrtnlabs/autobe-example-bbs-qwen-qwen3-next-80b-a3b-instruct"&gt;BBS&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href="https://github.com/wrtnlabs/autobe-example-bbs-openai-gpt-4.1"&gt;Big&lt;/a&gt; / &lt;a href="https://github.com/wrtnlabs/autobe-example-bbs-openai-gpt-4.1-mini"&gt;Mini&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href="https://github.com/wrtnlabs/autobe-example-bbs-openai-gpt-5"&gt;Big&lt;/a&gt; / &lt;a href="https://github.com/wrtnlabs/autobe-example-bbs-openai-gpt-5-mini"&gt;Mini&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Reddit Community&lt;/td&gt; &lt;td&gt;&lt;a href="https://github.com/wrtnlabs/autobe-example-reddit-qwen-qwen3-next-80b-a3b-instruct"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href="https://github.com/wrtnlabs/autobe-example-reddit-openai-gpt-4.1"&gt;Big&lt;/a&gt; / &lt;a href="https://github.com/wrtnlabs/autobe-example-reddit-openai-gpt-4.1-mini"&gt;Mini&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href="https://github.com/wrtnlabs/autobe-example-reddit-openai-gpt-5"&gt;Big&lt;/a&gt; / &lt;a href="https://github.com/wrtnlabs/autobe-example-reddit-openai-gpt-5-mini"&gt;Mini&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;E-Commerce&lt;/td&gt; &lt;td&gt;Failed&lt;/td&gt; &lt;td&gt;&lt;a href="https://github.com/wrtnlabs/autobe-example-shopping-openai-gpt-4.1"&gt;Big&lt;/a&gt; / &lt;a href="https://github.com/wrtnlabs/autobe-example-shopping-openai-gpt-4.1-mini"&gt;Mini&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Failed&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;blockquote&gt; &lt;p&gt;Of course, achieving 100% compilation success for backend applications generated by AutoBE does not mean that these applications are 100% safe or will run without any problems at runtime.&lt;/p&gt; &lt;p&gt;AutoBE-generated backend applications still don't pass 100% of their own test programs. Sometimes AutoBE writes incorrect SQL queries, and occasionally it misinterprets complex business logic and implements something entirely different.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Current test function pass rate is approximately 80%&lt;/li&gt; &lt;li&gt;We expect to achieve 100% runtime success rate by the end of this year&lt;/li&gt; &lt;/ul&gt; &lt;/blockquote&gt; &lt;p&gt;Through this month-long experimentation and optimization with local LLMs like &lt;code&gt;qwen3-next-80b-a3b&lt;/code&gt;, I've been amazed by their remarkable function calling performance and rapid development pace.&lt;/p&gt; &lt;p&gt;The core principle of AutoBE is not to have AI write programming code as text for backend application generation. Instead, we developed our own AutoBE-specific compiler and have AI construct its AST (Abstract Syntax Tree) structure through function calling. The AST inevitably takes on a highly complex form with countless types intertwined in unions and tree structures.&lt;/p&gt; &lt;p&gt;When I experimented with local LLMs earlier this year, not a single model could handle AutoBE's AST structure. Even Qwen's previous model, &lt;code&gt;qwen3-235b-a22b&lt;/code&gt;, couldn't pass through it such perfectly. The AST structures of AutoBE's specialized compilers, such as &lt;a href="https://github.com/wrtnlabs/autobe/blob/main/packages/interface/src/prisma/AutoBePrisma.ts"&gt;&lt;code&gt;AutoBePrisma&lt;/code&gt;&lt;/a&gt;, &lt;a href="https://github.com/wrtnlabs/autobe/blob/main/packages/interface/src/openapi/AutoBeOpenApi.ts"&gt;&lt;code&gt;AutoBeOpenApi&lt;/code&gt;&lt;/a&gt;, and &lt;a href="https://github.com/wrtnlabs/autobe/blob/main/packages/interface/src/test/AutoBeTest.ts"&gt;&lt;code&gt;AutoBeTest&lt;/code&gt;&lt;/a&gt;, acted as gatekeepers, preventing us from integrating local LLMs with AutoBE. But in just a few months, newly released local LLMs suddenly succeeded in generating these structures, completely changing the landscape.&lt;/p&gt; &lt;p&gt;&lt;code&gt;typescript // Example of AutoBE's AST structure export namespace AutoBeOpenApi { export type IJsonSchema = | IJsonSchema.IConstant | IJsonSchema.IBoolean | IJsonSchema.IInteger | IJsonSchema.INumber | IJsonSchema.IString | IJsonSchema.IArray | IJsonSchema.IObject | IJsonSchema.IReference | IJsonSchema.IOneOf | IJsonSchema.INull; } export namespace AutoBeTest { export type IExpression = | IBooleanLiteral | INumericLiteral | IStringLiteral | IArrayLiteralExpression | IObjectLiteralExpression | INullLiteral | IUndefinedKeyword | IIdentifier | IPropertyAccessExpression | IElementAccessExpression | ITypeOfExpression | IPrefixUnaryExpression | IPostfixUnaryExpression | IBinaryExpression | IArrowFunction | ICallExpression | INewExpression | IArrayFilterExpression | IArrayForEachExpression | IArrayMapExpression | IArrayRepeatExpression | IPickRandom | ISampleRandom | IBooleanRandom | IIntegerRandom | INumberRandom | IStringRandom | IPatternRandom | IFormatRandom | IKeywordRandom | IEqualPredicate | INotEqualPredicate | IConditionalPredicate | IErrorPredicate; } &lt;/code&gt;&lt;/p&gt; &lt;p&gt;As an open-source developer, I send infinite praise and respect to those creating these open-source AI models. Our AutoBE team is a small project with only 3-4 developers, and our capabilities and recognition are incomparably lower than those of LLM developers. Nevertheless, we want to contribute to the advancement of local LLMs and grow together.&lt;/p&gt; &lt;p&gt;To this end, we plan to develop benchmarks targeting each compiler component of AutoBE, conduct in-depth analysis of local LLMs' function calling capabilities for complex types, and publish the results periodically. We aim to release our first benchmark in about two months, covering most commercial and open-source AI models available.&lt;/p&gt; &lt;p&gt;We appreciate your interest and support, and will come back with the new benchmark.&lt;/p&gt; &lt;h2&gt;Link&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Homepage: &lt;a href="https://autobe.dev"&gt;https://autobe.dev&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Github: &lt;a href="https://github.com/wrtnlabs/autobe"&gt;https://github.com/wrtnlabs/autobe&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jhnam88"&gt; /u/jhnam88 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1o3604u"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3604u/autobe_achieved_100_compilation_success_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3604u/autobe_achieved_100_compilation_success_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T16:40:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1o2z1vt</id>
    <title>Reflection AI raises $2B to be America's open frontier AI lab, challenging DeepSeek | TechCrunch</title>
    <updated>2025-10-10T12:05:56+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2z1vt/reflection_ai_raises_2b_to_be_americas_open/"&gt; &lt;img alt="Reflection AI raises $2B to be America's open frontier AI lab, challenging DeepSeek | TechCrunch" src="https://external-preview.redd.it/J33KaNhavmUdkjELX9a4XHseBsKE9ltSNTN6hWuL1_c.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3ecb75e4826fba31ddac14c79c56f0fceee48915" title="Reflection AI raises $2B to be America's open frontier AI lab, challenging DeepSeek | TechCrunch" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Reflection AI: &lt;a href="https://reflection.ai/"&gt;https://reflection.ai/&lt;/a&gt;&lt;br /&gt; On 𝕏: &lt;a href="https://x.com/reflection_ai/status/1976304405369520242"&gt;https://x.com/reflection_ai/status/1976304405369520242&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://techcrunch.com/2025/10/09/reflection-raises-2b-to-be-americas-open-frontier-ai-lab-challenging-deepseek/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2z1vt/reflection_ai_raises_2b_to_be_americas_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o2z1vt/reflection_ai_raises_2b_to_be_americas_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T12:05:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3gyjn</id>
    <title>best coding LLM right now?</title>
    <updated>2025-10-10T23:53:41+00:00</updated>
    <author>
      <name>/u/RadianceTower</name>
      <uri>https://old.reddit.com/user/RadianceTower</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Models constantly get updated and new ones come out, so old posts aren't as valid.&lt;/p&gt; &lt;p&gt;I have 24GB of VRAM.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RadianceTower"&gt; /u/RadianceTower &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3gyjn/best_coding_llm_right_now/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3gyjn/best_coding_llm_right_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3gyjn/best_coding_llm_right_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T23:53:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3lgag</id>
    <title>Alignment is strong on this one</title>
    <updated>2025-10-11T03:40:01+00:00</updated>
    <author>
      <name>/u/Honest-Debate-6863</name>
      <uri>https://old.reddit.com/user/Honest-Debate-6863</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3lgag/alignment_is_strong_on_this_one/"&gt; &lt;img alt="Alignment is strong on this one" src="https://preview.redd.it/750najbileuf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ee379ef86f00df4ce67bac7aed8f41c898b52758" title="Alignment is strong on this one" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve noticed the Auto mode in cursor was getting good suddenly the quality stopped and has been ignoring instructions even when steered in a direction. It seems to forget the direction and steer back on the wrong direction it previously choose. &lt;/p&gt; &lt;p&gt;I think it’s developing some ego &lt;/p&gt; &lt;p&gt;Are the RL reward model tuning making it ego-centric? Is there a metric or bench to measure this? Is there a way to create a balance? I’ve seen this in a lot of open source models as well. Appreciate any literature references that you can provide. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Honest-Debate-6863"&gt; /u/Honest-Debate-6863 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/750najbileuf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3lgag/alignment_is_strong_on_this_one/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3lgag/alignment_is_strong_on_this_one/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T03:40:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1o33mui</id>
    <title>A list of models released or updated this week on this sub, in case you missed any (10 Oct).</title>
    <updated>2025-10-10T15:13:02+00:00</updated>
    <author>
      <name>/u/aifeed-fyi</name>
      <uri>https://old.reddit.com/user/aifeed-fyi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks,&lt;/p&gt; &lt;p&gt;Here is the list of models (releases and updates), I found mentioned on the LocalLlama for this week, Please update or let me know in the comments if there are any mistakes or misses. Enjoy !&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Description&lt;/th&gt; &lt;th align="left"&gt;Reddit post&lt;/th&gt; &lt;th align="left"&gt;HF / GitHub&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Jamba 3B&lt;/td&gt; &lt;td align="left"&gt;tiny 3 B&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/gallery/1o1ac09"&gt;reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/ai21labs/AI21-Jamba-Reasoning-3B"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;WEBGEN,UIGEN‑FX&lt;/td&gt; &lt;td align="left"&gt;research‑preview for UI/UX&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/gallery/1nz20g2"&gt;reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/Tesslate/UIGENT-30B-3A-Preview"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;KAT-Dev-72B-Exp&lt;/td&gt; &lt;td align="left"&gt;Coding model&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1o31rdl/kwaipilotkatdev72bexp_model_released"&gt;reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/Kwaipilot/KAT-Dev-72B-Exp"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Playable-GGUF&lt;/td&gt; &lt;td align="left"&gt;7b vibe coding retro games&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1o27xsj/introducing_playable1gguf_by_far_the_worlds_best"&gt;reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/playable/Playable1-GGUF"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;UserLM-8b&lt;/td&gt; &lt;td align="left"&gt;8b LLM playing user role&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1o23vqf/microsoftuserlm8b_unlike_typical_llms_that_are"&gt;reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/microsoft/UserLM-8b"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;CoDA‑v0‑Instruct&lt;/td&gt; &lt;td align="left"&gt;language‑diffusion&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1o1s7q8/an_open_sourced_language_diffusion_model_by_sf/"&gt;reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/Salesforce/CoDA-v0-Instruct"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;ColBERT Nano 250K&lt;/td&gt; &lt;td align="left"&gt;tiny‑retrieval&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1o1mpt5/introducing_the_colbert_nano_series_of_models_all/"&gt;reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/collections/NeuML/"&gt;HF collection&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;LFM2‑8B‑A1B&lt;/td&gt; &lt;td align="left"&gt;hybrid 8 B&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1o0zted/lfm28ba1b_quality_34b_dense_yet_faster_than/"&gt;reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/LiquidAI/LFM2-8B-A1B"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3‑VL‑30B‑A3B‑Instruct&lt;/td&gt; &lt;td align="left"&gt;vision‑LLM&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nxhfcq/qwen3vl30ba3binstruct_thinking_are_here/"&gt;reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-VL-30B-A3B-Instruct"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;BULaMU&lt;/td&gt; &lt;td align="left"&gt;Luganda LLM&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nykxfq/bulamuthe_first_luganda_large_language_model/"&gt;reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/datasets/mwebazarick/BULaMU"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;YanoljaNEXT‑Rosetta‑12B‑2510&lt;/td&gt; &lt;td align="left"&gt;translation 12 B&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1o2bm3z/yanoljayanoljanextrosetta12b2510/"&gt;reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/yanolja/YanoljaNEXT-Rosetta-12B-2510"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;SDLM 32B&lt;/td&gt; &lt;td align="left"&gt;multimodal 32B&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nwxje9/sdlm_32b4b_from_opengvlab/"&gt;reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/OpenGVLab/SDLM-32B-D4"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;SDLM 4B&lt;/td&gt; &lt;td align="left"&gt;multimodal 4 B&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nwxje9/sdlm_32b4b_from_opengvlab/"&gt;reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/OpenGVLab/SDLM-3B"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;🔧 Notable resources&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Resource&lt;/th&gt; &lt;th align="left"&gt;Description&lt;/th&gt; &lt;th align="left"&gt;Reddit post&lt;/th&gt; &lt;th align="left"&gt;HF / GitHub&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;MLXSharp&lt;/td&gt; &lt;td align="left"&gt;.NET MLX wrapper&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nypq6q/made_the_first_net_wrapper_for_apple_mlx_looking/"&gt;reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/managedcode/MLXSharp"&gt;GH&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Video2X 6.x&lt;/td&gt; &lt;td align="left"&gt;upscaler + interpolation&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nykzv3/video2x_6x_opensource_upscaler_frame/"&gt;reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/k4yt3x/video2x"&gt;GH&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;SurfSense&lt;/td&gt; &lt;td align="left"&gt;Perplexity alt.&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1o00ban/open_source_alternative_to_perplexity/"&gt;reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/MODSetter/SurfSense"&gt;GH&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aifeed-fyi"&gt; /u/aifeed-fyi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o33mui/a_list_of_models_released_or_updated_this_week_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o33mui/a_list_of_models_released_or_updated_this_week_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o33mui/a_list_of_models_released_or_updated_this_week_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T15:13:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3evon</id>
    <title>What laptop would you choose? Ryzen AI MAX+ 395 with 128GB of unified RAM or Intel 275HX + Nvidia RTX 5090 (128GB of RAM + 24GB of VRAM)?</title>
    <updated>2025-10-10T22:20:42+00:00</updated>
    <author>
      <name>/u/cl0p3z</name>
      <uri>https://old.reddit.com/user/cl0p3z</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For more or less the same price I can chose between this two laptops:&lt;/p&gt; &lt;p&gt;- HP G1a: &lt;strong&gt;AMD Ryzen AI MAX+ 395 with 128GB of RAM (no eGPU)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- Lenovo ThinkPad P16 Gen 3: &lt;strong&gt;Intel 275HX with 128GB of RAM + Nvidia RTX 5090 24GB of VRAM&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;What would you choose and why?&lt;/p&gt; &lt;p&gt;What I can do with AI/LLMs with one that I can't do with the other?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cl0p3z"&gt; /u/cl0p3z &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3evon/what_laptop_would_you_choose_ryzen_ai_max_395/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3evon/what_laptop_would_you_choose_ryzen_ai_max_395/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3evon/what_laptop_would_you_choose_ryzen_ai_max_395/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T22:20:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3kb3o</id>
    <title>Real SVD GLM-4.5-Air-GLM-4.6-Distill</title>
    <updated>2025-10-11T02:39:01+00:00</updated>
    <author>
      <name>/u/realmaywell</name>
      <uri>https://old.reddit.com/user/realmaywell</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3kb3o/real_svd_glm45airglm46distill/"&gt; &lt;img alt="Real SVD GLM-4.5-Air-GLM-4.6-Distill" src="https://external-preview.redd.it/e09zZ1vJH-206eXdnYOOyzVU_npt174nxPybTvRs_LQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9aac58b8a3b097d5a8fa8a939d97c17ee2f3dee5" title="Real SVD GLM-4.5-Air-GLM-4.6-Distill" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/imzpad6l8euf1.png?width=7167&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1582f372758ee501dae32e6d09cf36652bba3a9f"&gt;https://preview.redd.it/imzpad6l8euf1.png?width=7167&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1582f372758ee501dae32e6d09cf36652bba3a9f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is the person who posted that &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1fb6jdy/reflectionllama3170b_is_actually_llama3/"&gt;Reflection-Llama-3.1-70B is actually Llama-3. &lt;/a&gt;I didn't expect my first Reddit post in a year would be another debunk.&lt;/p&gt; &lt;p&gt;After seeing this &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nyopyc/did_anyone_try_out_glm45airglm46distill/"&gt;Reddit post&lt;/a&gt;, the idea of using SVD to distill a model seemed plausible, So I decided to test it out myself.&lt;/p&gt; &lt;p&gt;Although the original model mentioned in the post was a scam, I was curious about what would happen if I actually applied the methodology. So, I rewrote the entire existing CPU-based script into PyTorch code and ran the experiment on an H200 machine.&lt;/p&gt; &lt;p&gt;I excluded the LayerNorm and embed layers from the distillation because it was obvious that including them would break the model.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3y75s9519euf1.png?width=2502&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3eb9670538e5c603cdeba84a0e04e3035817192e"&gt;https://preview.redd.it/3y75s9519euf1.png?width=2502&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3eb9670538e5c603cdeba84a0e04e3035817192e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The results are as shown in the pictures above. I guess this means you don't have to go out of your way to replicate this experiment yourselves.&lt;/p&gt; &lt;p&gt;Still, for those who do want to try it out, you might find the model and LoRA below helpful.&lt;br /&gt; Model: &lt;a href="https://huggingface.co/maywell/GLM-4.5-Air-GLM-4.6-Distill"&gt;https://huggingface.co/maywell/GLM-4.5-Air-GLM-4.6-Distill&lt;/a&gt;&lt;br /&gt; LoRA: &lt;a href="https://huggingface.co/maywell/GLM-4.5-Air-GLM-4.6-Distill-LoRA"&gt;https://huggingface.co/maywell/GLM-4.5-Air-GLM-4.6-Distill-LoRA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Yay.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/realmaywell"&gt; /u/realmaywell &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3kb3o/real_svd_glm45airglm46distill/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3kb3o/real_svd_glm45airglm46distill/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3kb3o/real_svd_glm45airglm46distill/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T02:39:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3dfib</id>
    <title>GPT-OSS from Scratch on AMD GPUs</title>
    <updated>2025-10-10T21:21:45+00:00</updated>
    <author>
      <name>/u/tuanlda78202</name>
      <uri>https://old.reddit.com/user/tuanlda78202</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3dfib/gptoss_from_scratch_on_amd_gpus/"&gt; &lt;img alt="GPT-OSS from Scratch on AMD GPUs" src="https://external-preview.redd.it/kr7-cYQLjVSYgCHTdYk0hOHo8LtDx1fruxInOK1rC5k.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=df53654295de2b4add7e5b3992fa595b88f048f1" title="GPT-OSS from Scratch on AMD GPUs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After six years-the first time since GPT-2, OpenAI has released new open-weight LLMs, gpt-oss-20b and gpt-oss-120b. From day one, many inference engines such as llama.cpp, vLLM, and sgl-project have supported these models; however, most focus on maximizing throughput using CUDA for NVIDIA GPUs, offering limited support for AMD* GPUs. Moreover, their library-oriented implementations are often complex to understand and difficult to adapt for personal or experimental use cases.&lt;/p&gt; &lt;p&gt;To address these limitations, my team introduce “gpt-oss-amd”, a pure C++ implementation of OpenAI’s GPT-OSS models designed to maximize inference throughput on AMD GPUs without relying on external libraries. Our goal is to explore end-to-end LLM optimization, from kernel-level improvements to system-level design, providing insights for researchers and developers interested in high-performance computing and model-level optimization.&lt;/p&gt; &lt;p&gt;Inspired by llama2.c by Andrej Karpathy, our implementation uses HIP (an AMD programming model equivalent to CUDA) and avoids dependencies such as rocBLAS, hipBLAS, RCCL, and MPI. We utilize multiple optimization strategies for the 20B and 120B models, including efficient model loading, batching, multi-streaming, multi-GPU communication, optimized CPU–GPU–SRAM memory access, FlashAttention, matrix-core–based GEMM, and load balancing for MoE routing.&lt;/p&gt; &lt;p&gt;Experiments on a single node with 8× AMD MI250 GPUs show that our implementation achieves over 30k TPS on the 20B model and nearly 10k TPS on the 120B model in custom benchmarks, demonstrating the effectiveness of our optimizations and the strong potential of AMD GPUs for large-scale LLM inference.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9dr4gme0qcuf1.png?width=3392&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6ad41a33bc3ecf6625afaa2ff62ca762f5479d2a"&gt;https://preview.redd.it/9dr4gme0qcuf1.png?width=3392&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6ad41a33bc3ecf6625afaa2ff62ca762f5479d2a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/tuanlda78202/gpt-oss-amd"&gt;https://github.com/tuanlda78202/gpt-oss-amd&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tuanlda78202"&gt; /u/tuanlda78202 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3dfib/gptoss_from_scratch_on_amd_gpus/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3dfib/gptoss_from_scratch_on_amd_gpus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3dfib/gptoss_from_scratch_on_amd_gpus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T21:21:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3atdu</id>
    <title>GLM 5 coming before the end of 2025</title>
    <updated>2025-10-10T19:41:19+00:00</updated>
    <author>
      <name>/u/Helpful_Jacket8953</name>
      <uri>https://old.reddit.com/user/Helpful_Jacket8953</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3atdu/glm_5_coming_before_the_end_of_2025/"&gt; &lt;img alt="GLM 5 coming before the end of 2025" src="https://a.thumbs.redditmedia.com/hVBgymEdEuqZZKnYwcpRm5UQBweQDbSo5chE1AGg9a8.jpg" title="GLM 5 coming before the end of 2025" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Get ready. At this rate it seems like there's a real chance it'll start surpassing SOTA models on some benchmarks, not just DeepSeek.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/2knp5zv98cuf1.png?width=1556&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4458e1672e8db34aceb649902b4d838d88335dc2"&gt;https://preview.redd.it/2knp5zv98cuf1.png?width=1556&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4458e1672e8db34aceb649902b4d838d88335dc2&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Helpful_Jacket8953"&gt; /u/Helpful_Jacket8953 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3atdu/glm_5_coming_before_the_end_of_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3atdu/glm_5_coming_before_the_end_of_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3atdu/glm_5_coming_before_the_end_of_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T19:41:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1o371t4</id>
    <title>bro disappeared like he never existed</title>
    <updated>2025-10-10T17:18:54+00:00</updated>
    <author>
      <name>/u/Full_Piano_3448</name>
      <uri>https://old.reddit.com/user/Full_Piano_3448</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o371t4/bro_disappeared_like_he_never_existed/"&gt; &lt;img alt="bro disappeared like he never existed" src="https://preview.redd.it/2e01fz4pibuf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2158310d30c308746ab8924442748cf6a37b692a" title="bro disappeared like he never existed" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Knowing him is a sign you’ve been in the AI game for a long time&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Full_Piano_3448"&gt; /u/Full_Piano_3448 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2e01fz4pibuf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o371t4/bro_disappeared_like_he_never_existed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o371t4/bro_disappeared_like_he_never_existed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T17:18:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1o394p3</id>
    <title>Here we go again</title>
    <updated>2025-10-10T18:36:34+00:00</updated>
    <author>
      <name>/u/Namra_7</name>
      <uri>https://old.reddit.com/user/Namra_7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o394p3/here_we_go_again/"&gt; &lt;img alt="Here we go again" src="https://preview.redd.it/b2abfaikwbuf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7db3949cb0def07809e7a9ba9a730d1582083844" title="Here we go again" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Namra_7"&gt; /u/Namra_7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/b2abfaikwbuf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o394p3/here_we_go_again/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o394p3/here_we_go_again/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T18:36:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3opq5</id>
    <title>What the sub feels like lately</title>
    <updated>2025-10-11T06:47:33+00:00</updated>
    <author>
      <name>/u/marderbot13</name>
      <uri>https://old.reddit.com/user/marderbot13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3opq5/what_the_sub_feels_like_lately/"&gt; &lt;img alt="What the sub feels like lately" src="https://preview.redd.it/92s8znbxifuf1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fb4866bff0d572386ea47fc19d643a6b2261fbdb" title="What the sub feels like lately" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/marderbot13"&gt; /u/marderbot13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/92s8znbxifuf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3opq5/what_the_sub_feels_like_lately/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3opq5/what_the_sub_feels_like_lately/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T06:47:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
