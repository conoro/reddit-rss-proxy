<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-08T17:05:46+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1oruglr</id>
    <title>Proof of concept Max P sampler in PyTorch+transformers</title>
    <updated>2025-11-08T16:59:52+00:00</updated>
    <author>
      <name>/u/grimjim</name>
      <uri>https://old.reddit.com/user/grimjim</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I came up with a concept for a sampler that capped the maximum probability of logits as an indirect way to reduce repetition, redistributing the excess probability among the remaining tokens. The idea was to adjust creativity by moderating overconfidence in tokens.&lt;/p&gt; &lt;p&gt;To this end, I put together some code using pure PyTorch and HF transformers.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/jim-plus/maxp-sampler-poc"&gt;https://github.com/jim-plus/maxp-sampler-poc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Regardless of how well the sampler works, this shows that it's broadly possible to experiment with new samplers without having to wait on a PR for an inference engine.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/grimjim"&gt; /u/grimjim &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oruglr/proof_of_concept_max_p_sampler_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oruglr/proof_of_concept_max_p_sampler_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oruglr/proof_of_concept_max_p_sampler_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T16:59:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1orp9o9</id>
    <title>Text model that can produce nodes and edges in JSON</title>
    <updated>2025-11-08T13:21:40+00:00</updated>
    <author>
      <name>/u/BlueAdventurers</name>
      <uri>https://old.reddit.com/user/BlueAdventurers</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I need to draw knowledge graphs and I‚Äôm using Gemini 2.5 Flash to give me the JSON that renders it. However, it is too slow.&lt;/p&gt; &lt;p&gt;The output looks something like {‚Äútype‚Äù: ‚Äúnode‚Äù, ‚Äúid‚Äù: 123}, {‚Äútype‚Äù: ‚Äúedge‚Äù, ‚Äúfrom_id‚Äù: 123, ‚Äúto_id‚Äù: 456}&lt;/p&gt; &lt;p&gt;What model could I look into? It would need to reason on the free text input that describes the entities and their relationships. &lt;/p&gt; &lt;p&gt;A typical graph contains approx. 20 nodes and 30 edges.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BlueAdventurers"&gt; /u/BlueAdventurers &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orp9o9/text_model_that_can_produce_nodes_and_edges_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orp9o9/text_model_that_can_produce_nodes_and_edges_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1orp9o9/text_model_that_can_produce_nodes_and_edges_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T13:21:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1ordd5x</id>
    <title>[Web Demo] Qwen-Image-Edit ‚Äî Camera angle control (HF Space)</title>
    <updated>2025-11-08T02:10:18+00:00</updated>
    <author>
      <name>/u/freesysck</name>
      <uri>https://old.reddit.com/user/freesysck</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ordd5x/web_demo_qwenimageedit_camera_angle_control_hf/"&gt; &lt;img alt="[Web Demo] Qwen-Image-Edit ‚Äî Camera angle control (HF Space)" src="https://b.thumbs.redditmedia.com/cXAJ8Nam0JPRlqcjDrk_78dOvdtCDoLB1HC1snVFO1k.jpg" title="[Web Demo] Qwen-Image-Edit ‚Äî Camera angle control (HF Space)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Very Cool Tool.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/z1nmida0zxzf1.png?width=900&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fc5f06a9e2105a5db39e4644ef805606c65873eb"&gt;https://preview.redd.it/z1nmida0zxzf1.png?width=900&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fc5f06a9e2105a5db39e4644ef805606c65873eb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Upload an image, then tweak &lt;strong&gt;camera motion/rotation/lens&lt;/strong&gt; sliders to generate new viewpoints‚Äîright in your browser. &lt;a href="https://huggingface.co/spaces/linoyts/Qwen-Image-Edit-Angles?utm_source=chatgpt.com"&gt;Hugging Face&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Do things like move the camera (left/right/forward/down), rotate ¬±45¬∞/90¬∞ or go top-down, and switch between wide vs. close-up looks. &lt;/li&gt; &lt;li&gt;Built on &lt;strong&gt;Qwen Image Edit&lt;/strong&gt;; compatible community LoRAs enable multi-angle variants. &lt;/li&gt; &lt;li&gt;Tip: results can vary with busy backgrounds‚Äîshort prompts often work best.Try it: &lt;a href="https://huggingface.co/spaces/linoyts/Qwen-Image-Edit-Angles"&gt;&lt;code&gt;https://huggingface.co/spaces/linoyts/Qwen-Image-Edit-Angles&lt;/code&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/spaces/linoyts/Qwen-Image-Edit-Angles"&gt;Hugging Face&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/freesysck"&gt; /u/freesysck &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ordd5x/web_demo_qwenimageedit_camera_angle_control_hf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ordd5x/web_demo_qwenimageedit_camera_angle_control_hf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ordd5x/web_demo_qwenimageedit_camera_angle_control_hf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T02:10:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqttg0</id>
    <title>Can someone explain what a Mixture-of-Experts model really is?</title>
    <updated>2025-11-07T13:02:36+00:00</updated>
    <author>
      <name>/u/Weebviir</name>
      <uri>https://old.reddit.com/user/Weebviir</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I've been aware of MoE since Deepseek dropped in the beginning of the year but I never really delved deep into what it is and how it helps in things like local AI inferencing. This sub's been very helpful with my local AI related questions so I wanted to learn from the people here.&lt;/p&gt; &lt;p&gt;Here are some more questions:&lt;br /&gt; - How does a model know when an expert is to be used?&lt;br /&gt; - Are MoE models really easier to run than traditional models?&lt;br /&gt; - How do Activation parameters really work? Do they affect fine tuning processes later?&lt;br /&gt; - Why do MoE models work better than traditional models?&lt;br /&gt; - What are ‚Äúsparse‚Äù vs ‚Äúdense‚Äù MoE architectures?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Weebviir"&gt; /u/Weebviir &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqttg0/can_someone_explain_what_a_mixtureofexperts_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqttg0/can_someone_explain_what_a_mixtureofexperts_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oqttg0/can_someone_explain_what_a_mixtureofexperts_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T13:02:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1or1e7p</id>
    <title>I fine-tuned Gemma 3 1B for CLI command translation... but it runs 100% locally. 810MB, 1.5s inference on CPU.</title>
    <updated>2025-11-07T17:58:57+00:00</updated>
    <author>
      <name>/u/theRealSachinSpk</name>
      <uri>https://old.reddit.com/user/theRealSachinSpk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or1e7p/i_finetuned_gemma_3_1b_for_cli_command/"&gt; &lt;img alt="I fine-tuned Gemma 3 1B for CLI command translation... but it runs 100% locally. 810MB, 1.5s inference on CPU." src="https://b.thumbs.redditmedia.com/k4PYFs253tXR75-utWF1-v10OmEqGwkzGrkkq8FHHVo.jpg" title="I fine-tuned Gemma 3 1B for CLI command translation... but it runs 100% locally. 810MB, 1.5s inference on CPU." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;I built a locally-running NL‚ÜíCLI translator by fine-tuning Gemma 3 1B with QLoRA.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/pranavkumaarofficial/nlcli-wizard"&gt;[Link to repo]&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; Built a privacy-first CLI copilot. No API calls, no subscriptions. Just 810MB of local AI that converts natural language to CLI commands.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/jpo4dd4jivzf1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e3aa7bc9af223d3ab2e4c3eb9156907994885cf5"&gt;https://preview.redd.it/jpo4dd4jivzf1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e3aa7bc9af223d3ab2e4c3eb9156907994885cf5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I wanted to try out something like a CLI wizard: running locally and loaded within the package. Now of course there is an overhead of embedding an SLM in every package.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;But definitely makes sense for complex, domain-specific tools with non-obvious CLI patterns&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Instead of: kubectl get pods -n production --field-selector status.phase=Running&lt;/p&gt; &lt;p&gt;Could be: kubectl -w &amp;quot;show me running pods in production&amp;quot;&lt;/p&gt; &lt;p&gt;Shell-GPT is the closest tool that is available but doesnt do what I wanted, and ofcourse uses closedsource LLMs&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Here is what I tried:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Takes natural language like &amp;quot;show my environments sorted by size&amp;quot; and outputs the correct CLI command, eg : &lt;code&gt;venvy ls --sort size&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key stats:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;~1.5s inference on CPU (4 threads)&lt;/li&gt; &lt;li&gt;810MB quantized model (Q4_K_M with smart fallback)&lt;/li&gt; &lt;li&gt;Trained on Colab T4 in &amp;lt;1 hr&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;The Setup&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Base model:&lt;/strong&gt; Gemma 3-1B-Instruct (March 2025 release)&lt;br /&gt; &lt;strong&gt;Training:&lt;/strong&gt; Unsloth + QLoRA (only 14M params trained, 1.29% of model)&lt;br /&gt; &lt;strong&gt;Hardware:&lt;/strong&gt; Free Colab T4, trained in under 1 hour&lt;br /&gt; &lt;strong&gt;Final model:&lt;/strong&gt; 810MB GGUF (Q4_K_M with smart fallback to Q5/Q6)&lt;br /&gt; &lt;strong&gt;Inference:&lt;/strong&gt; llama.cpp, ~1.5s on CPU (4 threads, M1 Mac / Ryzen)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The architecture part:&lt;/strong&gt; Used smart quantization with mixed precision (Q4_K/Q5_0/Q6_K) that adapts per-layer based on tensor dimensions. Some layers can't be quantized to 4-bit without accuracy loss, so llama.cpp automatically upgrades them to 5/6-bit.&lt;/p&gt; &lt;p&gt;Training loss was extremely clean - 0.135 (train), 0.142 (val) with zero overfitting across 3 epochs.&lt;/p&gt; &lt;p&gt;Limitations (being honest here)&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Model size:&lt;/strong&gt; 810MB is chunky. Too big for Docker images, fine for dev machines.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Tool-specific:&lt;/strong&gt; Currently only works for &lt;code&gt;venvy&lt;/code&gt;. Need to retrain for kubectl/docker/etc.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Latency:&lt;/strong&gt; 1.5s isn't instant. Experts will still prefer muscle memory.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Accuracy:&lt;/strong&gt; 80-85% means you MUST verify before executing.&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Safety&lt;/h1&gt; &lt;p&gt;Always asks for confirmation before executing. I'm not &lt;em&gt;that&lt;/em&gt; reckless.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;confirm = input(&amp;quot;Execute? [Y/n] &amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Still working on this : to check where this can really help, but yeah pls go check it out&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/pranavkumaarofficial/nlcli-wizard"&gt;[Link to repo]&lt;/a&gt;&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;&lt;strong&gt;EDIT (24 hours later):&lt;/strong&gt;&lt;br /&gt; Thanks for the amazing feedback.&lt;br /&gt; Quick updates and answers to common questions:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Q: Can I use a bigger model (3B/7B)?&lt;/strong&gt;&lt;br /&gt; Yes! Any model...Just swap the model in the notebook:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;model_name = &amp;quot;unsloth/gemma-2-9b-it&amp;quot; # or Qwen2.5-3B, Phi-3 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Tradeoff:&lt;/strong&gt;&lt;br /&gt; 1B ‚âà 1.5s, 3B ‚âà 4‚Äì5s, 7B ‚âà 10s per inference.&lt;br /&gt; For Docker/git-heavy workflows, 3B+ is worth it.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Q: Where‚Äôs the Colab notebook?&lt;/strong&gt;&lt;br /&gt; Just pushed! Potential Google Colab issues fixed (inference + llama-quantize).&lt;br /&gt; Runs on &lt;strong&gt;free T4 in &amp;lt;2 hours&lt;/strong&gt;.&lt;br /&gt; Step-by-step explanations included: &lt;a href="https://colab.research.google.com/drive/1uBJJ_EqCMT8bMnCnVQHeN8USKu1ABddL"&gt;Colab Notebook&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Q: Why Docker &amp;amp; Kubernetes?&lt;/strong&gt;&lt;br /&gt; I really wanted to build this around everyday tools... Docker and Kubernetes are some tools I literally use everyday and I struggle to keep a track of all commands :P&lt;br /&gt; The goal was to make it locally running on the fly like:&lt;/p&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;strong&gt;Q: Error correction training (wrong ‚Üí right pairs)?&lt;/strong&gt;&lt;br /&gt; LOVE this idea! Imagine:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ docker run -p 8080 nginx Error: port needs colon üí° Try: docker run -p 8080:80 nginx [y/n]? &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Perfect for shell hook integration.&lt;br /&gt; Planning to create a GitHub issue to collaborate on this.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Q: Training data generation?&lt;/strong&gt;&lt;br /&gt; Fully programmatic: parse &lt;code&gt;--help&lt;/code&gt; + generate natural language variations.&lt;br /&gt; Code here: üîó &lt;a href="https://github.com/pranavkumaarofficial/nlcli-wizard/blob/main/nlcli_wizard/dataset.py"&gt;dataset.py&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Here‚Äôs exactly how I did it:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Step 1: Extract Ground Truth Commands&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Started with the actual CLI tool‚Äôs source code:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# venvy has these commands: venvy ls # list environments venvy ls --sort size # list sorted by size venvy create &amp;lt;name&amp;gt; # create new environment venvy activate &amp;lt;name&amp;gt; # activate environment # ... etc &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Basically scraped every valid command + flag combination from the --help docs and source code.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Step 2: Generate Natural Language Variations&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Example:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# Command: venvy ls --sort size variations = [ &amp;quot;show my environments sorted by size&amp;quot;, &amp;quot;list venvs by disk space&amp;quot;, &amp;quot;display environments largest first&amp;quot;, &amp;quot;show me which envs use most space&amp;quot;, &amp;quot;sort my virtual environments by size&amp;quot;, # ... 25+ more variations ] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I used GPT-5 with a prompt like:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Generate 30 different ways to express: &amp;quot;list environments sorted by size&amp;quot;. Vary: - Verbs (show, list, display, get, find) - Formality (&amp;quot;show me&amp;quot; vs &amp;quot;display&amp;quot;) - Word order (&amp;quot;size sorted&amp;quot; vs &amp;quot;sorted by size&amp;quot;) - Include typos/abbreviations (&amp;quot;envs&amp;quot; vs &amp;quot;environments&amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Step 3: Validation&lt;/strong&gt; I ran every generated command to make sure it actually works:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;for nl_input, command in training_data: result = subprocess.run(command, capture_output=True) if result.returncode != 0: print(f&amp;quot;Invalid command: {command}&amp;quot;) # Remove from dataset &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Final dataset: about 1,500 verified (natural_language ‚Üí command) pairs.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Training the Model&lt;/strong&gt; Format as instruction pairs:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;{ &amp;quot;instruction&amp;quot;: &amp;quot;show my environments sorted by size&amp;quot;, &amp;quot;output&amp;quot;: &amp;quot;venvy ls --sort size&amp;quot; } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;ALSO:&lt;br /&gt; &lt;strong&gt;Want to contribute? (planning on these next steps)&lt;/strong&gt;&lt;br /&gt; -&amp;gt; Docker dataset (500+ examples)&lt;br /&gt; -&amp;gt; Git dataset (500+ examples)&lt;br /&gt; -&amp;gt; Error correction pairs&lt;br /&gt; -&amp;gt; Mobile benchmarks&lt;/p&gt; &lt;p&gt;All contribution details here:&lt;br /&gt; üîó &lt;a href="https://github.com/pranavkumaarofficial/nlcli-wizard/blob/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/pranavkumaarofficial/nlcli-wizard"&gt;GITHUB&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks again for all the feedback and support! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/theRealSachinSpk"&gt; /u/theRealSachinSpk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or1e7p/i_finetuned_gemma_3_1b_for_cli_command/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or1e7p/i_finetuned_gemma_3_1b_for_cli_command/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1or1e7p/i_finetuned_gemma_3_1b_for_cli_command/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T17:58:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1orr6nl</id>
    <title>Would 4 2080Ti build work well for local AI models ? With coding as target</title>
    <updated>2025-11-08T14:46:29+00:00</updated>
    <author>
      <name>/u/UniqueAttourney</name>
      <uri>https://old.reddit.com/user/UniqueAttourney</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hi, i just found a used build with a threadripper 2920x, 128Gb RAM (DDR4), and 4 x 2080Ti GPUs, it is up for a $2700. Would it be a good build to rely on ? &lt;/p&gt; &lt;p&gt;My most demanding usage of AI is coding, background agents (mainly opencode and browser use). i already have a 3090 system and using qwen3 coder 30B, Devestral, gpt-oss-20b and these are very slow and quite stupid beyond 60k token context rendering them very bad at being used in codebases. &lt;/p&gt; &lt;p&gt;Would the 44GB of RAM even make a difference, maybe having 4 separate GPUs would kill equal out to having a single 3090 with approx. half the VRAM.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/UniqueAttourney"&gt; /u/UniqueAttourney &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orr6nl/would_4_2080ti_build_work_well_for_local_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orr6nl/would_4_2080ti_build_work_well_for_local_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1orr6nl/would_4_2080ti_build_work_well_for_local_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T14:46:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1ors6c4</id>
    <title>Need help with local AI build and using lots of compute</title>
    <updated>2025-11-08T15:27:16+00:00</updated>
    <author>
      <name>/u/JaccFromFoundry</name>
      <uri>https://old.reddit.com/user/JaccFromFoundry</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello! I hope this is the right place for this, and will also post in an AI sub but know that people here are knowledgeable. &lt;/p&gt; &lt;p&gt;I am a senior in college and help run a nonprofit that refurbishes and donates old tech. We have chapters at a few universities and highschools. Weve been growing quickly and are starting to try some other cool projects (open source development, digital literacy classes, research), and one of our highschool chapter leaders recently secured us a node of a supercomputer with 6 h100s for around 2 months. This is crazy (and super exciting), but I am a little worried because I want this to be a really cool experience for our guys and just dont know that much about actually producing AI, or how we can use this amazing gift weve been given to its full capacity (or most of). &lt;/p&gt; &lt;p&gt;Here is our brief plan: - We are going to fine tune a small local model for help with device repairs, and if time allows, fine tune a local ‚Äòcomputer tutor‚Äô to install on devices we donate to help people get used to and understand how to work with their device - Weve split into model and data teams, model team is figuring out what the best local model is to run on our devices/min spec (16gb ram, 500+gb storage, figuring out cpu but likely 2018 i5), and data team is scraping repair manuals and generating fine tuning data with them (question and response pairs generated with open ai api) - We have a $2k grant for a local AI development rig‚Äîplanning to complete data and model research in 2 weeks, then use our small local rig (that I need help building, more info below) to learn how to do LoRA and QLoRA fine tuning and begin to test our data and methods, and then 2 weeks after that to move to the hpc node and attempt full fine tuning&lt;/p&gt; &lt;p&gt;The help I need mainly focuses on two things: - Mainly, this local AI build. While I love computers and spend a lot of time working on them, I work with very old devices. I havent built a gaming pc in ~6 years and want to make sure we set ourselves as well as possible for the AI work. Our budget is approx ~$2k, and our current thinking was to get a 3090 and a ryzen 9, but its so much money and I am a little paralyzed because I want to make sure its spent as well as possible. I saw someone had 2 5060 tis, with 32 gb of vram and then just realized how little I understood about how to build for this stuff. We want to use it for fine tuning but also hopefully to run a larger model to serve to our members or have open for development. - I also need help understanding what interfacing with a hpc node looks like. Im worried well get our ssh keys or whatever and then be in this totally foreign environment and not know how to use it. I think it mostly revolves around process queuing? &lt;/p&gt; &lt;p&gt;Im not asking anyone to send me a full build or do my research for me, but would love any help anyone could give, specifically with this local AI development rig. &lt;/p&gt; &lt;p&gt;Tldr: Need help speccing ~$2k build to fine tune small models (3-7b at 4 bit quantization we are thinking)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JaccFromFoundry"&gt; /u/JaccFromFoundry &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ors6c4/need_help_with_local_ai_build_and_using_lots_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ors6c4/need_help_with_local_ai_build_and_using_lots_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ors6c4/need_help_with_local_ai_build_and_using_lots_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T15:27:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1ornvm0</id>
    <title>Best GUI for LLM based story writing that can access external models?</title>
    <updated>2025-11-08T12:11:29+00:00</updated>
    <author>
      <name>/u/StableLlama</name>
      <uri>https://old.reddit.com/user/StableLlama</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Most GUIs want to run the models themself, but I'd like to run it myself or use an on campus service that provide an OpenAI compatible API access. And for my Ooba installation the Playground extension isn't working at the moment.&lt;/p&gt; &lt;p&gt;So, long story short:&lt;/p&gt; &lt;p&gt;What are your recommendations for a GUI tool that's helping me to interactively write and edit stories - and can access the LLM through an OpenAI API?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StableLlama"&gt; /u/StableLlama &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ornvm0/best_gui_for_llm_based_story_writing_that_can/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ornvm0/best_gui_for_llm_based_story_writing_that_can/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ornvm0/best_gui_for_llm_based_story_writing_that_can/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T12:11:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1ortkiq</id>
    <title>Tips for someone new starting out on tinkering and self hosting LLMs</title>
    <updated>2025-11-08T16:23:33+00:00</updated>
    <author>
      <name>/u/MushroomDull4699</name>
      <uri>https://old.reddit.com/user/MushroomDull4699</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone, im fairly new to this and i got interested after bumping into Alex Ziskind‚Äôs video on recommend in a youtube channel. &lt;/p&gt; &lt;p&gt;I am a consultant here in SouthEast Asia who‚Äôs not fairly techy, but i use LLM‚Äôs a lot and i‚Äôve built my own pc 3x before (i play games on console and pc on a regular).&lt;/p&gt; &lt;p&gt;I plan to build or purchase a decent setup with a $3,000 busget that‚Äôs relatively future proof over the next 12-18 months and study python over the next 6 months (i have zero coding experience, but i believe studying python would help me go down this rabbit hole further)&lt;/p&gt; &lt;p&gt;I‚Äôm like just 2hrs away from Shenzhen and i‚Äôm looking to either buy parts and build my own setup or have one just built there with the ryzan ai max+395 128gb.&lt;/p&gt; &lt;p&gt;Is this a good plan? Or should i look at a different setup with my budget as well as study a different coding language?&lt;/p&gt; &lt;p&gt;I‚Äôm excited and i appreciate any tips and suggestions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MushroomDull4699"&gt; /u/MushroomDull4699 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ortkiq/tips_for_someone_new_starting_out_on_tinkering/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ortkiq/tips_for_someone_new_starting_out_on_tinkering/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ortkiq/tips_for_someone_new_starting_out_on_tinkering/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T16:23:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1or46rv</id>
    <title>Cerebras/Kimi-Linear-REAP-35B-A3B-Instruct ¬∑ Hugging Face</title>
    <updated>2025-11-07T19:44:19+00:00</updated>
    <author>
      <name>/u/maroule</name>
      <uri>https://old.reddit.com/user/maroule</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or46rv/cerebraskimilinearreap35ba3binstruct_hugging_face/"&gt; &lt;img alt="Cerebras/Kimi-Linear-REAP-35B-A3B-Instruct ¬∑ Hugging Face" src="https://external-preview.redd.it/A5NFpNf7XiO2gm9NBBYXrtttQxX4Zw8QmamAzVNdgao.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f4b51e11f834d5f8b364cbfd4018254e64276366" title="Cerebras/Kimi-Linear-REAP-35B-A3B-Instruct ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/maroule"&gt; /u/maroule &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/cerebras/Kimi-Linear-REAP-35B-A3B-Instruct"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or46rv/cerebraskimilinearreap35ba3binstruct_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1or46rv/cerebraskimilinearreap35ba3binstruct_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T19:44:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ors0ja</id>
    <title>Anyone actually coded with Kimi K2 Thinking?</title>
    <updated>2025-11-08T15:20:38+00:00</updated>
    <author>
      <name>/u/Federal_Spend2412</name>
      <uri>https://old.reddit.com/user/Federal_Spend2412</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Curious how its debug skills and long-context feel next to Claude 4.5 Sonnet‚Äîbetter, worse, or just hype? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Federal_Spend2412"&gt; /u/Federal_Spend2412 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ors0ja/anyone_actually_coded_with_kimi_k2_thinking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ors0ja/anyone_actually_coded_with_kimi_k2_thinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ors0ja/anyone_actually_coded_with_kimi_k2_thinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T15:20:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1orucf6</id>
    <title>Current SOTA coding model at around 30-70B?</title>
    <updated>2025-11-08T16:55:05+00:00</updated>
    <author>
      <name>/u/Crazyscientist1024</name>
      <uri>https://old.reddit.com/user/Crazyscientist1024</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What's the current SOTA model at around 30-70B for coding right now? I'm curious smth I can prob fine tune on a 1xH100 ideally, I got a pretty big coding dataset that I grinded up myself.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Crazyscientist1024"&gt; /u/Crazyscientist1024 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orucf6/current_sota_coding_model_at_around_3070b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orucf6/current_sota_coding_model_at_around_3070b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1orucf6/current_sota_coding_model_at_around_3070b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T16:55:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1or4q4m</id>
    <title>Kimi K2 Thinking SECOND most intelligent LLM according to Artificial Analysis</title>
    <updated>2025-11-07T20:04:53+00:00</updated>
    <author>
      <name>/u/teatime1983</name>
      <uri>https://old.reddit.com/user/teatime1983</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or4q4m/kimi_k2_thinking_second_most_intelligent_llm/"&gt; &lt;img alt="Kimi K2 Thinking SECOND most intelligent LLM according to Artificial Analysis" src="https://b.thumbs.redditmedia.com/rRHDbssmheavBuel7zlrGe8NTyudGIzlKqGjsplYEaI.jpg" title="Kimi K2 Thinking SECOND most intelligent LLM according to Artificial Analysis" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/4l3lbor55wzf1.png?width=488&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b25065a1147654cbbf9b413b322560870a44d41e"&gt;https://preview.redd.it/4l3lbor55wzf1.png?width=488&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b25065a1147654cbbf9b413b322560870a44d41e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The Kimi K2 Thinking API pricing is $0.60 per million input tokens and $2.50 per million output tokens.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/teatime1983"&gt; /u/teatime1983 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or4q4m/kimi_k2_thinking_second_most_intelligent_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or4q4m/kimi_k2_thinking_second_most_intelligent_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1or4q4m/kimi_k2_thinking_second_most_intelligent_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T20:04:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1or5j9z</id>
    <title>Nvidia may cancel the RTX 50 Super due to a shortage of 3GB GDDR7 memory</title>
    <updated>2025-11-07T20:36:28+00:00</updated>
    <author>
      <name>/u/Spiderboyz1</name>
      <uri>https://old.reddit.com/user/Spiderboyz1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For now it's just a rumor, but it seems the RTX Super cards will take a while to be released, if they ever are&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.techpowerup.com/342705/gddr7-shortage-could-stop-nvidia-geforce-rtx-50-series-super-rollout"&gt;https://www.techpowerup.com/342705/gddr7-shortage-could-stop-nvidia-geforce-rtx-50-series-super-rollout&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.guru3d.com/story/nvidia-may-cancel-or-delay-geforce-rtx-50-super-series-amid-gddr7-memory-shortage/"&gt;https://www.guru3d.com/story/nvidia-may-cancel-or-delay-geforce-rtx-50-super-series-amid-gddr7-memory-shortage/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And we also have RAM prices skyrocketing due to high demand&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Spiderboyz1"&gt; /u/Spiderboyz1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or5j9z/nvidia_may_cancel_the_rtx_50_super_due_to_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or5j9z/nvidia_may_cancel_the_rtx_50_super_due_to_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1or5j9z/nvidia_may_cancel_the_rtx_50_super_due_to_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T20:36:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1or8ehk</id>
    <title>Artificial Analysis has released a more in-depth benchmark breakdown of Kimi K2 Thinking (2nd image)</title>
    <updated>2025-11-07T22:30:30+00:00</updated>
    <author>
      <name>/u/averagebear_003</name>
      <uri>https://old.reddit.com/user/averagebear_003</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or8ehk/artificial_analysis_has_released_a_more_indepth/"&gt; &lt;img alt="Artificial Analysis has released a more in-depth benchmark breakdown of Kimi K2 Thinking (2nd image)" src="https://b.thumbs.redditmedia.com/UzJABCvVD7NHRog60608GSepBAlbSYFvvfYwhDULgvQ.jpg" title="Artificial Analysis has released a more in-depth benchmark breakdown of Kimi K2 Thinking (2nd image)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/averagebear_003"&gt; /u/averagebear_003 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1or8ehk"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or8ehk/artificial_analysis_has_released_a_more_indepth/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1or8ehk/artificial_analysis_has_released_a_more_indepth/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T22:30:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1or08aq</id>
    <title>OpenAI Pushes to Label Datacenters as ‚ÄòAmerican Manufacturing‚Äô Seeking Federal Subsidies After Preaching Independence</title>
    <updated>2025-11-07T17:15:05+00:00</updated>
    <author>
      <name>/u/Ok-Breakfast-4676</name>
      <uri>https://old.reddit.com/user/Ok-Breakfast-4676</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or08aq/openai_pushes_to_label_datacenters_as_american/"&gt; &lt;img alt="OpenAI Pushes to Label Datacenters as ‚ÄòAmerican Manufacturing‚Äô Seeking Federal Subsidies After Preaching Independence" src="https://preview.redd.it/jq1jrz6kbvzf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=258e2c389f69a6ed7cdcda3ea33b5a80b38a0fb3" title="OpenAI Pushes to Label Datacenters as ‚ÄòAmerican Manufacturing‚Äô Seeking Federal Subsidies After Preaching Independence" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;OpenAI is now lobbying to classify datacenter spending as ‚ÄúAmerican manufacturing.‚Äù&lt;/p&gt; &lt;p&gt;In their recent submission, they explicitly advocate for Federal loan guarantees the same kind used to subsidize large-scale industrial projects.&lt;/p&gt; &lt;p&gt;So after all the talk about independence and no need for government help‚Ä¶ Sam lied. Again.Ôøº&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Breakfast-4676"&gt; /u/Ok-Breakfast-4676 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jq1jrz6kbvzf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or08aq/openai_pushes_to_label_datacenters_as_american/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1or08aq/openai_pushes_to_label_datacenters_as_american/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T17:15:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1orrddh</id>
    <title>Minimax M2 Coding Plan Pricing Revealed</title>
    <updated>2025-11-08T14:54:27+00:00</updated>
    <author>
      <name>/u/lemon07r</name>
      <uri>https://old.reddit.com/user/lemon07r</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orrddh/minimax_m2_coding_plan_pricing_revealed/"&gt; &lt;img alt="Minimax M2 Coding Plan Pricing Revealed" src="https://a.thumbs.redditmedia.com/8aEeUqxK1pswvd_byPWSO7mB5CnEhSUGwY6HlasVrB8.jpg" title="Minimax M2 Coding Plan Pricing Revealed" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/frggt9tkp10g1.png?width=1120&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f28f09c1d3058be07b78d6f2f36a44b1f87608b4"&gt;https://preview.redd.it/frggt9tkp10g1.png?width=1120&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f28f09c1d3058be07b78d6f2f36a44b1f87608b4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Recieved the following in my user notifications on the minimax platform website. Here's the main portion of interest, in text form:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Coding Plans (Available Nov 10)&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Starter:&lt;/strong&gt; $10/ month&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Pro:&lt;/strong&gt; $20 / month&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Max:&lt;/strong&gt; $50 / month&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The coding plan pricing seems a lot more expensive than what was previously rumored. Usage provided is currently unknown, but I believe it was supposed to be &amp;quot;5x&amp;quot; the equivalent claude plans, but those rumors also said they were supposed to cost 20% of claude for the pro plan equivalent, and 8% for the other two max plans.&lt;/p&gt; &lt;p&gt;Seems to be a direct competitor to GLM coding plans, but I'm not sure how well this will pan out with those plans being as cheap as $3 a month for first month/quarter/year, and both offering similarly strong models. Chutes is also a strong contendor since they are able to offer both GLM and minimax models, and now K2 thinking as well at fairly cheap plans.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lemon07r"&gt; /u/lemon07r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orrddh/minimax_m2_coding_plan_pricing_revealed/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orrddh/minimax_m2_coding_plan_pricing_revealed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1orrddh/minimax_m2_coding_plan_pricing_revealed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T14:54:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1oro9ng</id>
    <title>ROCm(6.4, using latest LLVM) vs ROCm 7 (lemonade sdk)</title>
    <updated>2025-11-08T12:32:29+00:00</updated>
    <author>
      <name>/u/CyBerDreadWing</name>
      <uri>https://old.reddit.com/user/CyBerDreadWing</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;One observation I would like to paste in here:&lt;/p&gt; &lt;p&gt;By building llama.cpp with ROCm from scratch (HIP SDK version 6.4), I was able to get more performance than lemonade sdk for ROCm 7.&lt;/p&gt; &lt;p&gt;FYI: I keep changing path of llama.cpp so on first run path was given to ROCm 7 and on second run path was given to ROCm 6.4&lt;/p&gt; &lt;p&gt;Here are some sample outputs:&lt;br /&gt; ROCm 7:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;PS C:\Users\dreadwing\.lmstudio\models\lmstudio-community\Qwen3-Coder-30B-A3B-Instruct-GGUF&amp;gt; llama-bench -m .\Qwen3-Coder-30B-A3B-Instruct-Q8_0.gguf -ub 2048 -b 2048 -ngl 99 -t 16 --n-cpu-moe 2,3,4,5,6,7,8,9,30 -fa on ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 1 ROCm devices: Device 0: AMD Radeon RX 7900 GRE, gfx1100 (0x1100), VMM: no, Wave Size: 32 | model | size | params | backend | ngl | n_cpu_moe | threads | n_ubatch | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ---------: | ------: | -------: | --------------: | -------------------: | | qwen3moe 30B.A3B Q8_0 | 30.25 GiB | 30.53 B | ROCm | 99 | 2 | 16 | 2048 | pp512 | 247.95 ¬± 9.81 | | qwen3moe 30B.A3B Q8_0 | 30.25 GiB | 30.53 B | ROCm | 99 | 2 | 16 | 2048 | tg128 | 7.03 ¬± 0.18 | | qwen3moe 30B.A3B Q8_0 | 30.25 GiB | 30.53 B | ROCm | 99 | 3 | 16 | 2048 | pp512 | 243.92 ¬± 8.31 | | qwen3moe 30B.A3B Q8_0 | 30.25 GiB | 30.53 B | ROCm | 99 | 3 | 16 | 2048 | tg128 | 5.37 ¬± 0.19 | | qwen3moe 30B.A3B Q8_0 | 30.25 GiB | 30.53 B | ROCm | 99 | 4 | 16 | 2048 | pp512 | 339.53 ¬± 15.05 | | qwen3moe 30B.A3B Q8_0 | 30.25 GiB | 30.53 B | ROCm | 99 | 4 | 16 | 2048 | tg128 | 4.31 ¬± 0.09 | | qwen3moe 30B.A3B Q8_0 | 30.25 GiB | 30.53 B | ROCm | 99 | 5 | 16 | 2048 | pp512 | 322.23 ¬± 23.39 | | qwen3moe 30B.A3B Q8_0 | 30.25 GiB | 30.53 B | ROCm | 99 | 5 | 16 | 2048 | tg128 | 3.71 ¬± 0.15 | | qwen3moe 30B.A3B Q8_0 | 30.25 GiB | 30.53 B | ROCm | 99 | 6 | 16 | 2048 | pp512 | 389.06 ¬± 27.76 | | qwen3moe 30B.A3B Q8_0 | 30.25 GiB | 30.53 B | ROCm | 99 | 6 | 16 | 2048 | tg128 | 3.02 ¬± 0.16 | | qwen3moe 30B.A3B Q8_0 | 30.25 GiB | 30.53 B | ROCm | 99 | 7 | 16 | 2048 | pp512 | 385.10 ¬± 46.43 | | qwen3moe 30B.A3B Q8_0 | 30.25 GiB | 30.53 B | ROCm | 99 | 7 | 16 | 2048 | tg128 | 2.75 ¬± 0.08 | | qwen3moe 30B.A3B Q8_0 | 30.25 GiB | 30.53 B | ROCm | 99 | 8 | 16 | 2048 | pp512 | 374.84 ¬± 59.77 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;ROCm 6.4 ( which I build using latest llvm):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;PS C:\Users\dreadwing\.lmstudio\models\lmstudio-community\Qwen3-Coder-30B-A3B-Instruct-GGUF&amp;gt; llama-bench -m .\Qwen3-Coder-30B-A3B-Instruct-Q8_0.gguf -ub 2048 -b 2048 -ngl 99 -t 16 --n-cpu-moe 6,5,30 -fa on ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 1 ROCm devices: Device 0: AMD Radeon RX 7900 GRE, gfx1100 (0x1100), VMM: no, Wave Size: 32 | model | size | params | backend | ngl | n_cpu_moe | threads | n_ubatch | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ---------: | ------: | -------: | --------------: | -------------------: | | qwen3moe 30B.A3B Q8_0 | 30.25 GiB | 30.53 B | ROCm | 99 | 6 | 16 | 2048 | pp512 | 229.92 ¬± 12.49 | | qwen3moe 30B.A3B Q8_0 | 30.25 GiB | 30.53 B | ROCm | 99 | 6 | 16 | 2048 | tg128 | 15.69 ¬± 0.10 | | qwen3moe 30B.A3B Q8_0 | 30.25 GiB | 30.53 B | ROCm | 99 | 5 | 16 | 2048 | pp512 | 338.65 ¬± 30.11 | | qwen3moe 30B.A3B Q8_0 | 30.25 GiB | 30.53 B | ROCm | 99 | 5 | 16 | 2048 | tg128 | 15.20 ¬± 0.04 | | qwen3moe 30B.A3B Q8_0 | 30.25 GiB | 30.53 B | ROCm | 99 | 30 | 16 | 2048 | pp512 | 206.16 ¬± 65.14 | | qwen3moe 30B.A3B Q8_0 | 30.25 GiB | 30.53 B | ROCm | 99 | 30 | 16 | 2048 | tg128 | 21.28 ¬± 0.07 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Can someone please explain why this is happening, (ROCm 7 is still in beta for windows, but thats my hard guess).&lt;/p&gt; &lt;p&gt;I am still figuring out TheRock build and vulkan build and will soon benchmark them as well.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CyBerDreadWing"&gt; /u/CyBerDreadWing &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oro9ng/rocm64_using_latest_llvm_vs_rocm_7_lemonade_sdk/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oro9ng/rocm64_using_latest_llvm_vs_rocm_7_lemonade_sdk/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oro9ng/rocm64_using_latest_llvm_vs_rocm_7_lemonade_sdk/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T12:32:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1orn6q4</id>
    <title>Handy : Free, Offline AI dictation app for PC, supports Whisper and Parakeet models</title>
    <updated>2025-11-08T11:32:08+00:00</updated>
    <author>
      <name>/u/Technical-Love-8479</name>
      <uri>https://old.reddit.com/user/Technical-Love-8479</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Handy is a trending GitHub repo which is a free alternate for Wispr Flow for AI dictation. The app size is quite small and it supports all Parakeet (nvidia) and Whisper model for speech to text. &lt;/p&gt; &lt;p&gt;GitHub : &lt;a href="https://github.com/cjpais/Handy"&gt;https://github.com/cjpais/Handy&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Demo : &lt;a href="https://youtu.be/1QzXdhVeOkI?si=yli8cfejvOy3ERbo"&gt;https://youtu.be/1QzXdhVeOkI?si=yli8cfejvOy3ERbo&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Technical-Love-8479"&gt; /u/Technical-Love-8479 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orn6q4/handy_free_offline_ai_dictation_app_for_pc/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orn6q4/handy_free_offline_ai_dictation_app_for_pc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1orn6q4/handy_free_offline_ai_dictation_app_for_pc/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T11:32:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1orter8</id>
    <title>Added Kimi-K2-Thinking to the UGI-Leaderboard</title>
    <updated>2025-11-08T16:17:13+00:00</updated>
    <author>
      <name>/u/DontPlanToEnd</name>
      <uri>https://old.reddit.com/user/DontPlanToEnd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orter8/added_kimik2thinking_to_the_ugileaderboard/"&gt; &lt;img alt="Added Kimi-K2-Thinking to the UGI-Leaderboard" src="https://preview.redd.it/9kogdlk5620g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9be57e7458c6ad9fc1ebe6278bbb2b316a0b4ba9" title="Added Kimi-K2-Thinking to the UGI-Leaderboard" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/spaces/DontPlanToEnd/UGI-Leaderboard"&gt;https://huggingface.co/spaces/DontPlanToEnd/UGI-Leaderboard&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DontPlanToEnd"&gt; /u/DontPlanToEnd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9kogdlk5620g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orter8/added_kimik2thinking_to_the_ugileaderboard/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1orter8/added_kimik2thinking_to_the_ugileaderboard/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T16:17:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1orsdd9</id>
    <title>Meta‚Äôs AI hidden debt</title>
    <updated>2025-11-08T15:35:16+00:00</updated>
    <author>
      <name>/u/Ok-Breakfast-4676</name>
      <uri>https://old.reddit.com/user/Ok-Breakfast-4676</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orsdd9/metas_ai_hidden_debt/"&gt; &lt;img alt="Meta‚Äôs AI hidden debt" src="https://preview.redd.it/a6susixny10g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3e81cc9161b2f3c3a8b79ad468df241b09f83883" title="Meta‚Äôs AI hidden debt" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Meta‚Äôs hidden AI debt&lt;/p&gt; &lt;p&gt;Meta has parked $30B in AI infra debt off its balance sheet using SPVs the same financial engineering behind Enron and ‚Äô08.&lt;/p&gt; &lt;p&gt;Morgan Stanley sees tech firms needing $800B in private-credit SPVs by 2028. UBS says AI debt is growing $100B/quarter, raising red flags.&lt;/p&gt; &lt;p&gt;This isn‚Äôt dot-com equity growth it‚Äôs hidden leverage. When chips go obsolete in 3 years instead of 6, and exposure sits in short-term leases, transparency fades and that‚Äôs how bubbles start.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Breakfast-4676"&gt; /u/Ok-Breakfast-4676 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/a6susixny10g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orsdd9/metas_ai_hidden_debt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1orsdd9/metas_ai_hidden_debt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T15:35:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1orlqmh</id>
    <title>Honey we shrunk MiniMax M2</title>
    <updated>2025-11-08T10:04:06+00:00</updated>
    <author>
      <name>/u/arjunainfinity</name>
      <uri>https://old.reddit.com/user/arjunainfinity</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orlqmh/honey_we_shrunk_minimax_m2/"&gt; &lt;img alt="Honey we shrunk MiniMax M2" src="https://external-preview.redd.it/ehjUFeUe3VAS2s784qCwiz1JNp5TRqMKwWsfk9mVQNM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a556feed177eef7b3a5785bd56ea678c06d8ea4e" title="Honey we shrunk MiniMax M2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi folks, we pruned MiniMax M2 from 250B to 192B (~25%) with only ~5% loss in coding quality. We did this with $200 worth of 8XH200 compute. Our 50% pruned model is ETA 5 more days. Would love to hear your feedback and would you want a 50% pruned Kimi K2 Thinking?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/arjunainfinity"&gt; /u/arjunainfinity &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/VibeStudio/thrift"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orlqmh/honey_we_shrunk_minimax_m2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1orlqmh/honey_we_shrunk_minimax_m2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T10:04:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1ordgys</id>
    <title>We got this, we can do it! When is the REAP‚Äôd iQ_001_XXS GGUF dropping?</title>
    <updated>2025-11-08T02:15:28+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ordgys/we_got_this_we_can_do_it_when_is_the_reapd_iq_001/"&gt; &lt;img alt="We got this, we can do it! When is the REAP‚Äôd iQ_001_XXS GGUF dropping?" src="https://preview.redd.it/qfahc43zzxzf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1975edd06526787672ea84d9ae1d9904b84715e1" title="We got this, we can do it! When is the REAP‚Äôd iQ_001_XXS GGUF dropping?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/qfahc43zzxzf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ordgys/we_got_this_we_can_do_it_when_is_the_reapd_iq_001/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ordgys/we_got_this_we_can_do_it_when_is_the_reapd_iq_001/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T02:15:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1ortopy</id>
    <title>Kimi K2 Thinking 1-bit Unsloth Dynamic GGUFs</title>
    <updated>2025-11-08T16:28:21+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ortopy/kimi_k2_thinking_1bit_unsloth_dynamic_ggufs/"&gt; &lt;img alt="Kimi K2 Thinking 1-bit Unsloth Dynamic GGUFs" src="https://preview.redd.it/s190tdo2720g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8a07ae8f05f136602ecbf12323e286c3cca29d84" title="Kimi K2 Thinking 1-bit Unsloth Dynamic GGUFs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone! You can now run Kimi K2 Thinking locally with our Unsloth Dynamic 1bit GGUFs. We also collaborated with the Kimi team on a &lt;strong&gt;fix for K2&lt;/strong&gt; &lt;a href="https://huggingface.co/moonshotai/Kimi-K2-Thinking/discussions/12"&gt;&lt;strong&gt;Thinking's chat template&lt;/strong&gt;&lt;/a&gt; not prepending the default system prompt of &lt;code&gt;You are Kimi, an AI assistant created by Moonshot AI.&lt;/code&gt; on the 1st turn.&lt;/p&gt; &lt;p&gt;We also we &lt;strong&gt;fixed llama.cpp custom jinja separators&lt;/strong&gt; for tool calling - Kimi does &lt;code&gt;{&amp;quot;a&amp;quot;:&amp;quot;1&amp;quot;,&amp;quot;b&amp;quot;:&amp;quot;2&amp;quot;}&lt;/code&gt; and not with extra spaces like &lt;code&gt;{&amp;quot;a&amp;quot;: &amp;quot;1&amp;quot;, &amp;quot;b&amp;quot;: &amp;quot;2&amp;quot;}&lt;/code&gt;&lt;/p&gt; &lt;p&gt;The 1-bit GGUF will run on 247GB RAM. We shrank the 1T model to 245GB (-62%) &amp;amp; the accuracy recovery is comparable to our third-party &lt;a href="https://docs.unsloth.ai/basics/unsloth-dynamic-2.0-ggufs/unsloth-dynamic-ggufs-on-aider-polyglot"&gt;DeepSeek-V3.1 Aider Polyglot benchmarks&lt;/a&gt;&lt;/p&gt; &lt;p&gt;All 1bit, 2bit and other bit width GGUFs are at &lt;a href="https://huggingface.co/unsloth/Kimi-K2-Thinking-GGUF"&gt;https://huggingface.co/unsloth/Kimi-K2-Thinking-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The suggested temp is &lt;code&gt;temperature = 1.0&lt;/code&gt;. We also suggest a &lt;code&gt;min_p = 0.01&lt;/code&gt;. If you do not see &lt;code&gt;&amp;lt;think&amp;gt;&lt;/code&gt;, use &lt;code&gt;--special&lt;/code&gt;. The code for llama-cli is below which offloads MoE layers to CPU RAM, and leaves the rest of the model on GPU VRAM:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;export LLAMA_CACHE=&amp;quot;unsloth/Kimi-K2-Thinking-GGUF&amp;quot; ./llama.cpp/llama-cli \ -hf unsloth/Kimi-K2-Thinking-GGUF:UD-TQ1_0 \ --n-gpu-layers 99 \ --temp 1.0 \ --min-p 0.01 \ --ctx-size 16384 \ --seed 3407 \ -ot &amp;quot;.ffn_.*_exps.=CPU&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Step-by-step Guide + fix details: &lt;a href="https://docs.unsloth.ai/models/kimi-k2-thinking-how-to-run-locally"&gt;https://docs.unsloth.ai/models/kimi-k2-thinking-how-to-run-locally&lt;/a&gt; and GGUFs are &lt;a href="https://huggingface.co/unsloth/Kimi-K2-Thinking-GGUF"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Let us know if you have any questions and hope you have a great weekend!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/s190tdo2720g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ortopy/kimi_k2_thinking_1bit_unsloth_dynamic_ggufs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ortopy/kimi_k2_thinking_1bit_unsloth_dynamic_ggufs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T16:28:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1ormxoq</id>
    <title>Kimi K2 Thinking was trained with only $4.6 million</title>
    <updated>2025-11-08T11:16:59+00:00</updated>
    <author>
      <name>/u/InternationalAsk1490</name>
      <uri>https://old.reddit.com/user/InternationalAsk1490</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ormxoq/kimi_k2_thinking_was_trained_with_only_46_million/"&gt; &lt;img alt="Kimi K2 Thinking was trained with only $4.6 million" src="https://a.thumbs.redditmedia.com/L1ScXXMSwvWG0gMuGlVV23jh1shgAs9PlFDsIKJOn94.jpg" title="Kimi K2 Thinking was trained with only $4.6 million" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;OpenAI: &amp;quot;We need government support to cover $1.4 trillion in chips and data centers.&amp;quot;&lt;/p&gt; &lt;p&gt;Kimi:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/03swwvpfo00g1.png?width=1199&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f97e99e5a5aac5bb23a62ff58df048f5db678e09"&gt;https://preview.redd.it/03swwvpfo00g1.png?width=1199&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f97e99e5a5aac5bb23a62ff58df048f5db678e09&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InternationalAsk1490"&gt; /u/InternationalAsk1490 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ormxoq/kimi_k2_thinking_was_trained_with_only_46_million/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ormxoq/kimi_k2_thinking_was_trained_with_only_46_million/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ormxoq/kimi_k2_thinking_was_trained_with_only_46_million/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T11:16:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1olq14f</id>
    <title>[MEGATHREAD] Local AI Hardware - November 2025</title>
    <updated>2025-11-01T15:02:17+00:00</updated>
    <author>
      <name>/u/eck72</name>
      <uri>https://old.reddit.com/user/eck72</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is the monthly thread for sharing your local AI setups and the models you're running.&lt;/p&gt; &lt;p&gt;Whether you're using a single CPU, a gaming GPU, or a full rack, post what you're running and how it performs.&lt;/p&gt; &lt;p&gt;Post in any format you like. The list below is just a guide:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Hardware: CPU, GPU(s), RAM, storage, OS&lt;/li&gt; &lt;li&gt;Model(s): name + size/quant&lt;/li&gt; &lt;li&gt;Stack: (e.g. llama.cpp + custom UI)&lt;/li&gt; &lt;li&gt;Performance: t/s, latency, context, batch etc.&lt;/li&gt; &lt;li&gt;Power consumption&lt;/li&gt; &lt;li&gt;Notes: purpose, quirks, comments&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please share setup pics for eye candy!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Quick reminder&lt;/strong&gt;: You can share hardware purely to ask questions or get feedback. All experience levels welcome.&lt;/p&gt; &lt;p&gt;House rules: no buying/selling/promo.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eck72"&gt; /u/eck72 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T15:02:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqy1k7</id>
    <title>AMA Announcement: Moonshot AI, The Opensource Frontier Lab Behind Kimi K2 Thinking SoTA Model (Monday, 8AM-11AM PST)</title>
    <updated>2025-11-07T15:53:33+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqy1k7/ama_announcement_moonshot_ai_the_opensource/"&gt; &lt;img alt="AMA Announcement: Moonshot AI, The Opensource Frontier Lab Behind Kimi K2 Thinking SoTA Model (Monday, 8AM-11AM PST)" src="https://preview.redd.it/8v2luf5owuzf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b9bc34ec8dddd94422397eaa91e0310250da5ba3" title="AMA Announcement: Moonshot AI, The Opensource Frontier Lab Behind Kimi K2 Thinking SoTA Model (Monday, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8v2luf5owuzf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqy1k7/ama_announcement_moonshot_ai_the_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oqy1k7/ama_announcement_moonshot_ai_the_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T15:53:33+00:00</published>
  </entry>
</feed>
