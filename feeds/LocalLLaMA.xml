<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-13T00:55:59+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1pkhudf</id>
    <title>US Administration Issues Executive Order Opposing State-Level Regulation of AI Industry</title>
    <updated>2025-12-12T03:42:45+00:00</updated>
    <author>
      <name>/u/ttkciar</name>
      <uri>https://old.reddit.com/user/ttkciar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The EO:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.whitehouse.gov/presidential-actions/2025/12/eliminating-state-law-obstruction-of-national-artificial-intelligence-policy/"&gt;https://www.whitehouse.gov/presidential-actions/2025/12/eliminating-state-law-obstruction-of-national-artificial-intelligence-policy/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;My take: The EO orders the US AG to set up a task force to sue states which have legislated their own AI industry regulations, orders other agencies to prepare a report on how states might be denied federal funds, and orders that a set of recommendations be made to Congress to draft and pass new laws.&lt;/p&gt; &lt;p&gt;It seems like Christmas came early for commercial inference services, this year.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ttkciar"&gt; /u/ttkciar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkhudf/us_administration_issues_executive_order_opposing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkhudf/us_administration_issues_executive_order_opposing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pkhudf/us_administration_issues_executive_order_opposing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T03:42:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkvu49</id>
    <title>Best LLM under 30/40B for writing, chatting, talking.</title>
    <updated>2025-12-12T16:12:49+00:00</updated>
    <author>
      <name>/u/tombino104</name>
      <uri>https://old.reddit.com/user/tombino104</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone, I‚Äôm still a novice in these artificial intelligence issues.&lt;/p&gt; &lt;p&gt;Since I‚Äôm a bit sick of GPT of all those seemingly free artificial intelligence models, since you notice our data, I decided to experiment a little with local LLMs.&lt;/p&gt; &lt;p&gt;I was looking for a model to use mainly to chat, so maybe discuss topics, but a model that is specialized above all in the text, precisely speak and remain consistent with what it says, and that is also very informed in the knowledge, that it is in-depth knowledge and not basic.&lt;/p&gt; &lt;p&gt;It‚Äôs fine even if it‚Äôs able to make translations, summarize texts or rewrite them according to certain styles, in short, a bit like writing instruments, maybe, even better. I‚Äôm NOT looking for a model to write code.&lt;/p&gt; &lt;p&gt;If the model is thinking or can also take input the images, even better, since these two features would be very convenient for me.&lt;/p&gt; &lt;p&gt;I‚Äôm mainly using them in LM Studio.&lt;/p&gt; &lt;p&gt;From my computer, I can load a model up to 30/40B even if the model is medium large, it‚Äôs not a problem.&lt;/p&gt; &lt;p&gt;Thanks again for the help! üôè&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tombino104"&gt; /u/tombino104 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkvu49/best_llm_under_3040b_for_writing_chatting_talking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkvu49/best_llm_under_3040b_for_writing_chatting_talking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pkvu49/best_llm_under_3040b_for_writing_chatting_talking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T16:12:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkr9ak</id>
    <title>Emoji Translator: Convert English to Expressive Emoji Sequences üé≠ (Fun Side Project)</title>
    <updated>2025-12-12T12:59:08+00:00</updated>
    <author>
      <name>/u/ReplacementMoney2484</name>
      <uri>https://old.reddit.com/user/ReplacementMoney2484</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I built a fun open-source tool called the Emoji Translator that converts English sentences into expressive emoji sequences, instead of a simple dictionary lookup (like replacing &amp;quot;cat&amp;quot; with üê±), I fine-tuned BART-Large using LoRA so it actually understands context and sentiment.&lt;/p&gt; &lt;h1&gt;Some funny/interesting results:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&amp;quot;I feel misunderstood.&amp;quot; ‚Üí ü§¨üò¨&lt;/li&gt; &lt;li&gt;&amp;quot;I am happy.&amp;quot; ‚Üí üòÅü§ò&lt;/li&gt; &lt;li&gt;&amp;quot;My parents want to have a new baby&amp;quot; ‚Üí üë∂üë™ü§∞&lt;/li&gt; &lt;li&gt;&amp;quot;I tweeted the news to my followers.&amp;quot; ‚Üí ü§≥ü§†ü§≥&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Technicals for the nerds:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Dataset:&lt;/strong&gt; I used &lt;em&gt;Gemini 3 Pro&lt;/em&gt; to generate a synthetic dataset because scraping clean emoji data is hard.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Training:&lt;/strong&gt; I implemented Curriculum Learning with 6 stages of difficulty. I started by teaching the model simple object-emoji pairs and progressively introduced complex sentences and abstract concepts. This helped stabilize convergence significantly compared to throwing all the data at it at once.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Try it out:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Live Demo:&lt;/strong&gt; &lt;a href="https://huggingface.co/spaces/mohamedmostafa259/emoji-translator-demo"&gt;HuggingFace Space&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/mohamedmostafa259/emoji-translator"&gt;mohamedmostafa259/emoji-translator&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model:&lt;/strong&gt; &lt;a href="https://huggingface.co/mohamedmostafa259/bart-emoji-translator"&gt;HuggingFace Hub&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Dataset:&lt;/strong&gt; &lt;a href="https://www.kaggle.com/datasets/mohamedmostafa259/english-to-emoji"&gt;Kaggle Dataset&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Training notebook:&lt;/strong&gt; &lt;a href="https://www.kaggle.com/code/mohamedmostafa259/emoji-translator-curriculum-learning"&gt;Kaggle Notebook&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It's completely open source. Would love to see what weird translations you can get it to generate!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ReplacementMoney2484"&gt; /u/ReplacementMoney2484 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkr9ak/emoji_translator_convert_english_to_expressive/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkr9ak/emoji_translator_convert_english_to_expressive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pkr9ak/emoji_translator_convert_english_to_expressive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T12:59:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1pl1zpa</id>
    <title>Evening fun with Grace and Hopper unified memory, or how to speed up llama.cpp and DeepSeek V3.1 on NVIDIA GH200</title>
    <updated>2025-12-12T20:16:57+00:00</updated>
    <author>
      <name>/u/fairydreaming</name>
      <uri>https://old.reddit.com/user/fairydreaming</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For the past 2 days I had the pleasure of having remote access to a NVIDIA GH200 system kindly shared by &lt;a href="/u/GPTShop"&gt;u/GPTShop&lt;/a&gt;. It's a similar machine to the one that &lt;a href="/u/Reddactor"&gt;u/Reddactor&lt;/a&gt; has shown in his &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1pjbhyz/i_bought_a_gracehopper_server_for_75k_on_reddit/"&gt;recent post&lt;/a&gt;, but with only a single GH200 module inside. I wanted to see how the unified memory works and what performance we can get on llama.cpp with this hardware.&lt;/p&gt; &lt;p&gt;Initial results were disappointing with pp512 of 41.63 t/s and tg128 of 8.86 t/s. Even my Epyc workstation does better.&lt;/p&gt; &lt;p&gt;To make it faster I added some code that advised CUDA to place model expert tensors (except shared experts) on CPU LPDDR5X memory and all remaining tensors on GPU memory. It was only a dozen of lines, after applying the patch llama-bench results were:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ GGML_CUDA_ENABLE_UNIFIED_MEMORY=1 ./bin/llama-bench -m ~/fairydreaming/models/DeepSeek-V3.1-Terminus-Q4_K_M-00001-of-00009.gguf -fa 1 ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 1 CUDA devices: Device 0: NVIDIA GH200 144G HBM3e, compute capability 9.0, VMM: yes | model | size | params | backend | ngl | fa | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | -: | --------------: | -------------------: | | deepseek2 671B Q4_K - Medium | 377.55 GiB | 671.03 B | CUDA | 99 | 1 | pp512 | 276.84 ¬± 1.49 | | deepseek2 671B Q4_K - Medium | 377.55 GiB | 671.03 B | CUDA | 99 | 1 | tg128 | 16.95 ¬± 0.01 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I ran some more tests with different context lengths and larger ubatch:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ GGML_CUDA_ENABLE_UNIFIED_MEMORY=1 ./bin/llama-bench -m ~/fairydreaming/models/DeepSeek-V3.1-Terminus-Q4_K_M-00001-of-00009.gguf -fa 1 -d 0,4096,8192,16384,32768 -p 2048 -n 32 -ub 2048 ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 1 CUDA devices: Device 0: NVIDIA GH200 144G HBM3e, compute capability 9.0, VMM: yes | model | size | params | backend | ngl | n_ubatch | fa | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | -------: | -: | --------------: | -------------------: | | deepseek2 671B Q4_K - Medium | 377.55 GiB | 671.03 B | CUDA | 99 | 2048 | 1 | pp2048 | 576.82 ¬± 2.38 | | deepseek2 671B Q4_K - Medium | 377.55 GiB | 671.03 B | CUDA | 99 | 2048 | 1 | tg32 | 16.92 ¬± 0.02 | | deepseek2 671B Q4_K - Medium | 377.55 GiB | 671.03 B | CUDA | 99 | 2048 | 1 | pp2048 @ d4096 | 483.90 ¬± 0.93 | | deepseek2 671B Q4_K - Medium | 377.55 GiB | 671.03 B | CUDA | 99 | 2048 | 1 | tg32 @ d4096 | 16.20 ¬± 0.06 | | deepseek2 671B Q4_K - Medium | 377.55 GiB | 671.03 B | CUDA | 99 | 2048 | 1 | pp2048 @ d8192 | 402.99 ¬± 1.07 | | deepseek2 671B Q4_K - Medium | 377.55 GiB | 671.03 B | CUDA | 99 | 2048 | 1 | tg32 @ d8192 | 16.05 ¬± 0.12 | | deepseek2 671B Q4_K - Medium | 377.55 GiB | 671.03 B | CUDA | 99 | 2048 | 1 | pp2048 @ d16384 | 299.70 ¬± 1.25 | | deepseek2 671B Q4_K - Medium | 377.55 GiB | 671.03 B | CUDA | 99 | 2048 | 1 | tg32 @ d16384 | 15.98 ¬± 0.14 | | deepseek2 671B Q4_K - Medium | 377.55 GiB | 671.03 B | CUDA | 99 | 2048 | 1 | pp2048 @ d32768 | 190.55 ¬± 0.67 | | deepseek2 671B Q4_K - Medium | 377.55 GiB | 671.03 B | CUDA | 99 | 2048 | 1 | tg32 @ d32768 | 15.34 ¬± 0.35 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now we are talking, very nice prompt processing performance (compared to before). I haven't seen numbers like this even with ktransformers or Mac M3 Ultra benchmark results.&lt;/p&gt; &lt;p&gt;Also the token generation rate doesn't seem to go down much as the context size increases.&lt;/p&gt; &lt;p&gt;Hopefully it's possible to make it even faster, for example by placing some experts on the GPU memory (there's still free space here). Uh, now my Epyc workstation feels somewhat slow.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fairydreaming"&gt; /u/fairydreaming &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pl1zpa/evening_fun_with_grace_and_hopper_unified_memory/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pl1zpa/evening_fun_with_grace_and_hopper_unified_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pl1zpa/evening_fun_with_grace_and_hopper_unified_memory/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T20:16:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1pl6kk9</id>
    <title>adam-atan2 Installation Guide</title>
    <updated>2025-12-12T23:32:26+00:00</updated>
    <author>
      <name>/u/damat-le</name>
      <uri>https://old.reddit.com/user/damat-le</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was experimenting with two recently introduced models: Hierarchical Reasoning Model (HRM) and Tiny Recursive Model (TRM).&lt;/p&gt; &lt;p&gt;Both depend on the `adam-atan2` package (&lt;a href="https://github.com/imoneoi/adam-atan2"&gt;https://github.com/imoneoi/adam-atan2&lt;/a&gt;), but I had a lot of trouble installing it. &lt;/p&gt; &lt;p&gt;Since I couldn't find a suitable installation guide online, I created one myself: &lt;a href="https://github.com/damat-le/adam-atan2-installation-guide"&gt;https://github.com/damat-le/adam-atan2-installation-guide&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I hope it will be useful to others who have the same problems.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/damat-le"&gt; /u/damat-le &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pl6kk9/adamatan2_installation_guide/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pl6kk9/adamatan2_installation_guide/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pl6kk9/adamatan2_installation_guide/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T23:32:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkhzf0</id>
    <title>Reverse-Engineering the RK3588 NPU: Hacking Memory Limits to run massive Vision Transformers</title>
    <updated>2025-12-12T03:49:27+00:00</updated>
    <author>
      <name>/u/one_does_not_just</name>
      <uri>https://old.reddit.com/user/one_does_not_just</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I worked on a &amp;quot;fun&amp;quot; project for my grad school class. I decided to write a blog post about it, maybe its useful to someone who is dealing with problems deploying vision transformers on edge devices&lt;/p&gt; &lt;p&gt;&lt;a href="https://amohan.dev/blog/2025/shard-optimizing-vision-transformers-edge-npu/"&gt;https://amohan.dev/blog/2025/shard-optimizing-vision-transformers-edge-npu/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Edit: Removed massive from title, but reddit won't let me change title, sorry about that&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/one_does_not_just"&gt; /u/one_does_not_just &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkhzf0/reverseengineering_the_rk3588_npu_hacking_memory/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkhzf0/reverseengineering_the_rk3588_npu_hacking_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pkhzf0/reverseengineering_the_rk3588_npu_hacking_memory/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T03:49:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1pl7r2j</id>
    <title>The ‚Äòskills vs tools‚Äô debate is mostly missing the real production bottleneck</title>
    <updated>2025-12-13T00:26:24+00:00</updated>
    <author>
      <name>/u/Ok-Classic6022</name>
      <uri>https://old.reddit.com/user/Ok-Classic6022</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pl7r2j/the_skills_vs_tools_debate_is_mostly_missing_the/"&gt; &lt;img alt="The ‚Äòskills vs tools‚Äô debate is mostly missing the real production bottleneck" src="https://external-preview.redd.it/kS0Xyv4ZUoBgG-zvPjRxyF685f-y03c6CkoZ9rR-C_A.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=41589fad0f505386db16c20639a0e496d9ef100b" title="The ‚Äòskills vs tools‚Äô debate is mostly missing the real production bottleneck" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There‚Äôs a lot of debate right now about ‚Äúagent skills‚Äù vs ‚Äútools.‚Äù&lt;/p&gt; &lt;p&gt;After building and debugging real agents, I think this debate is mostly backwards.&lt;/p&gt; &lt;p&gt;From the model‚Äôs perspective, &lt;em&gt;everything&lt;/em&gt; collapses into the same thing:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;a description&lt;/li&gt; &lt;li&gt;an invocation surface&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Skills, tools, function calls, MCP servers ‚Äî they all end up as options the model selects from.&lt;/p&gt; &lt;p&gt;The distinction &lt;em&gt;does&lt;/em&gt; matter architecturally (token cost, security surface, portability), but it matters far less than whether the agent can actually execute reliably in production.&lt;/p&gt; &lt;p&gt;In practice, the failures I keep seeing aren‚Äôt about choosing skills vs tools. They‚Äôre about:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;massive schema dumps blowing context windows&lt;/li&gt; &lt;li&gt;tools that only work for a single user&lt;/li&gt; &lt;li&gt;OAuth flows that assume a human + browser&lt;/li&gt; &lt;li&gt;agents that look great locally and die the moment you add a second user&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We wrote this up with concrete examples from Anthropic, OpenAI, LangChain, and teams shipping agents in prod.&lt;/p&gt; &lt;p&gt;Curious how others here are handling:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;tool count vs reliability&lt;/li&gt; &lt;li&gt;auth for multi-user agents&lt;/li&gt; &lt;li&gt;when to encode ‚Äúexpertise‚Äù vs executable actions&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Would love to hear real deployments, not demos.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Classic6022"&gt; /u/Ok-Classic6022 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://blog.arcade.dev/what-are-agent-skills-and-tools"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pl7r2j/the_skills_vs_tools_debate_is_mostly_missing_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pl7r2j/the_skills_vs_tools_debate_is_mostly_missing_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-13T00:26:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1pl1keu</id>
    <title>llada2.0 benchmarks</title>
    <updated>2025-12-12T19:59:47+00:00</updated>
    <author>
      <name>/u/kaggleqrdl</name>
      <uri>https://old.reddit.com/user/kaggleqrdl</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pl1keu/llada20_benchmarks/"&gt; &lt;img alt="llada2.0 benchmarks" src="https://b.thumbs.redditmedia.com/ohqtSiQ31qvoS1zALbwdhqAvVA2sMqNF2_SgfXSjdcY.jpg" title="llada2.0 benchmarks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/ygxj4kyowt6g1.png?width=763&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4fa4ba2d0206d65bb67fe975bb455df5cdfc06e3"&gt;https://preview.redd.it/ygxj4kyowt6g1.png?width=763&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4fa4ba2d0206d65bb67fe975bb455df5cdfc06e3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/inclusionAI/LLaDA2.0"&gt;https://github.com/inclusionAI/LLaDA2.0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Has anyone had a chance to reproduce this?&lt;/p&gt; &lt;p&gt;As a diffusion model, it's pretty interesting for sure.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ixm4dthozt6g1.png?width=760&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=def677edf64f1d1c5a3194e2c77ea629becabd6b"&gt;https://preview.redd.it/ixm4dthozt6g1.png?width=760&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=def677edf64f1d1c5a3194e2c77ea629becabd6b&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kaggleqrdl"&gt; /u/kaggleqrdl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pl1keu/llada20_benchmarks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pl1keu/llada20_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pl1keu/llada20_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T19:59:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkdkjo</id>
    <title>Agentic Local AI on CPU = Mistral Vibe + Granite-4-h-1b</title>
    <updated>2025-12-12T00:22:10+00:00</updated>
    <author>
      <name>/u/PotentialFunny7143</name>
      <uri>https://old.reddit.com/user/PotentialFunny7143</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkdkjo/agentic_local_ai_on_cpu_mistral_vibe_granite4h1b/"&gt; &lt;img alt="Agentic Local AI on CPU = Mistral Vibe + Granite-4-h-1b" src="https://external-preview.redd.it/NDYwbGgydmYybzZnMf2LvdJmBzIyNzEDfN0eOt2yDrF46dRxJq4WcX4O0NUM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4363f44505584728345cc48958c232a7ab91036f" title="Agentic Local AI on CPU = Mistral Vibe + Granite-4-h-1b" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A a3b LLM is all you need :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PotentialFunny7143"&gt; /u/PotentialFunny7143 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/vewmcluf2o6g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkdkjo/agentic_local_ai_on_cpu_mistral_vibe_granite4h1b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pkdkjo/agentic_local_ai_on_cpu_mistral_vibe_granite4h1b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T00:22:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkw795</id>
    <title>Umar Jamil explains how Mistral‚Äôs Magistral model was trained</title>
    <updated>2025-12-12T16:27:19+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkw795/umar_jamil_explains_how_mistrals_magistral_model/"&gt; &lt;img alt="Umar Jamil explains how Mistral‚Äôs Magistral model was trained" src="https://external-preview.redd.it/XAos23Rbc14Lk2myUzbfAJlF7p5RC2BmrcP6tzPIbEI.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d01fb73b7aa05eeff9953a869ece78c195721d76" title="Umar Jamil explains how Mistral‚Äôs Magistral model was trained" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=S4EsRyZQKEc&amp;amp;t=977s"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkw795/umar_jamil_explains_how_mistrals_magistral_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pkw795/umar_jamil_explains_how_mistrals_magistral_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T16:27:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkwarb</id>
    <title>Anyone else hitting RAM creep with long local LLM runs?</title>
    <updated>2025-12-12T16:31:10+00:00</updated>
    <author>
      <name>/u/CommunityGlobal8094</name>
      <uri>https://old.reddit.com/user/CommunityGlobal8094</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been running local Llama models (mostly via Ollama) in longer pipelines, batch inference, multi-step processing, some light RAG ad I keep seeing memory usage slowly climb over time. Nothing crashes immediately, but after a few hours the process is way heavier than it should be. I‚Äôve tried restarting workers, simplifying loops, even running smaller batches, but the creep keeps coming back. Curious if this is just the reality of Python-based orchestration around local LLMs, or if there‚Äôs a cleaner way to run long-lived local pipelines without things slowly eating RAM.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CommunityGlobal8094"&gt; /u/CommunityGlobal8094 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkwarb/anyone_else_hitting_ram_creep_with_long_local_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkwarb/anyone_else_hitting_ram_creep_with_long_local_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pkwarb/anyone_else_hitting_ram_creep_with_long_local_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T16:31:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1pko16f</id>
    <title>7B MoE with 1B active</title>
    <updated>2025-12-12T09:49:48+00:00</updated>
    <author>
      <name>/u/lossless-compression</name>
      <uri>https://old.reddit.com/user/lossless-compression</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I found that models in that range are relatively rare,I found some models such as (may not be exactly 7B and exactly 1B activated but in that range) are&lt;/p&gt; &lt;ul&gt; &lt;li&gt;1- Granite-4-tiny&lt;/li&gt; &lt;li&gt;2- LFM2-8B-A1B&lt;/li&gt; &lt;li&gt;3- Trinity-nano 6B&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Most of SLMs that are in that range are made of high amount of experts (tiny experts) where larger amount of experts gets activated but the overall parameters activated are ~1B so the model can specialize well.&lt;/p&gt; &lt;p&gt;I really wonder why that range isn't popular,I tried those models and Trinity nano is a very good researcher and it got a good character too and I asked a few general question it answered well,LFM feels like a RAG model even the standard one,it feels so robotic and answers are not the best,even the 350M can be coherent but it still feels like a RAG model, didn't test Granite 4 tiny yet.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lossless-compression"&gt; /u/lossless-compression &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pko16f/7b_moe_with_1b_active/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pko16f/7b_moe_with_1b_active/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pko16f/7b_moe_with_1b_active/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T09:49:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkr0x0</id>
    <title>Building an offline legal compliance AI on RTX 3090 ‚Äì am I doing this right or completely overengineering it?</title>
    <updated>2025-12-12T12:47:30+00:00</updated>
    <author>
      <name>/u/Motijani28</name>
      <uri>https://old.reddit.com/user/Motijani28</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;I'm building an AI system for insurance policy compliance that needs to run &lt;strong&gt;100% offline&lt;/strong&gt; for legal/privacy reasons. Think: processing payslips, employment contracts, medical records, and cross-referencing them against 300+ pages of insurance regulations to auto-detect claim discrepancies.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What's working so far:&lt;/strong&gt; - Ryzen 9 9950X, 96GB DDR5, RTX 3090 24GB, Windows 11 + Docker + WSL2 - Python 3.11 + Ollama + Tesseract OCR - Built a payslip extractor (OCR + regex) that pulls employee names, national registry numbers, hourly wage (‚Ç¨16.44/hr baseline), sector codes, and hours worked ‚Üí &lt;strong&gt;70-80% accuracy, good enough for PoC&lt;/strong&gt; - Tested Qwen 2.5 14B/32B models locally - Got structured test dataset ready: 13 docs (payslips, contracts, work schedules) from a real anonymized case&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What didn't work:&lt;/strong&gt; - Open WebUI didn't cut it for this use case ‚Äì too generic, not flexible enough for legal document workflows&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What I'm building next:&lt;/strong&gt; - RAG pipeline (LlamaIndex) to index legal sources (insurance regulation PDFs) - Auto-validation: extract payslip data ‚Üí query RAG ‚Üí check compliance ‚Üí generate report with legal citations - Multi-document comparison (contract ‚Üî payslip ‚Üî work hours) - Demo ready by March 2026&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My questions:&lt;/strong&gt; 1. &lt;strong&gt;Model choice:&lt;/strong&gt; Currently eyeing &lt;strong&gt;Qwen 3 30B-A3B (MoE)&lt;/strong&gt; ‚Äì is this the right call for legal reasoning on 24GB VRAM, or should I go with dense 32B? Thinking mode seems clutch for compliance checks. 2. &lt;strong&gt;RAG chunking:&lt;/strong&gt; Fixed-size (1000 tokens) vs section-aware splitting for legal docs? What actually works in production? 3. &lt;strong&gt;Anyone done similar compliance/legal document AI locally?&lt;/strong&gt; What were your pain points? Did it actually work or just benchmarketing bullshit? 4. &lt;strong&gt;Better alternatives to LlamaIndex for this?&lt;/strong&gt; Or am I on the right track?&lt;/p&gt; &lt;p&gt;I'm targeting 70-80% automation for document analysis ‚Äì still needs human review, AI just flags potential issues and cross-references regulations. Not trying to replace legal experts, just speed up the tedious document processing work.&lt;/p&gt; &lt;p&gt;Any tips, similar projects, or &amp;quot;you're doing it completely wrong&amp;quot; feedback welcome. Tight deadline, don't want to waste 3 months going down the wrong path.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; Building offline legal compliance AI (insurance claims) on RTX 3090. Payslip extraction works (70-80%), now adding RAG for legal validation. Qwen 3 30B-A3B good choice? Anyone done similar projects that actually worked? Need it done by March 2026.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Motijani28"&gt; /u/Motijani28 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkr0x0/building_an_offline_legal_compliance_ai_on_rtx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkr0x0/building_an_offline_legal_compliance_ai_on_rtx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pkr0x0/building_an_offline_legal_compliance_ai_on_rtx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T12:47:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkidf6</id>
    <title>What is the smartest uncensored nsfw LLM you can run with 12GB VRAM and 32GB RAM?</title>
    <updated>2025-12-12T04:08:01+00:00</updated>
    <author>
      <name>/u/Dex921</name>
      <uri>https://old.reddit.com/user/Dex921</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I don't know if it's allowed, but I am asking about ALL available LLMs including ones that are closed source and cannot be run locally (like chatgpt or gemini, and in that case obviously the ram limit doesn't apply)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dex921"&gt; /u/Dex921 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkidf6/what_is_the_smartest_uncensored_nsfw_llm_you_can/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkidf6/what_is_the_smartest_uncensored_nsfw_llm_you_can/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pkidf6/what_is_the_smartest_uncensored_nsfw_llm_you_can/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T04:08:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1pl0u6w</id>
    <title>What do you do, if you invent AGI? (seriously)</title>
    <updated>2025-12-12T19:29:47+00:00</updated>
    <author>
      <name>/u/teachersecret</name>
      <uri>https://old.reddit.com/user/teachersecret</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Some of you know me. I'm the resident LocalLlama silly person who tries to get my 4090 to do ridiculously fast things. I've posted some things here before, like controlling swarms of little bots, making an AI make weird sounds from its mouth, and getting AI to do agentic tasks, like my wacky effort to get thousands of tokens of GPT-OSS-20b output per second to fly an ASTEROIDS spaceship in real time.&lt;/p&gt; &lt;p&gt;Anyway... lately I've been playing around with some fast AI training tricks, figuring out how to turn my 'scrap in a cave' 4090 into something a bit more useful. I recently trained a gpt-2 124m equivalent to 3.28 loss in less than an hour. It seems to me that the scale we need to hit AGI might exist at consumer level, and today I'm asking...&lt;/p&gt; &lt;p&gt;What if YOU invent it?&lt;/p&gt; &lt;p&gt;I know I can't be the only one out here messing around on the fringe. And I'm probably not the only one who's made some headway (I'm looking at you, fpantsham... pew... you unsloth guys...). &lt;/p&gt; &lt;p&gt;What would you do? What the heck DO you do? I'm assuming most of you aren't working directly in the industry. Lets say you're just sitting here one afternoon banging away in Claude and there it is. Done. Undeniable. You probably don't know Sam Altman. Neither do I. I'm guessing walking into the door of Google shouting you have AGI isn't gonna work. What do you do?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/teachersecret"&gt; /u/teachersecret &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pl0u6w/what_do_you_do_if_you_invent_agi_seriously/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pl0u6w/what_do_you_do_if_you_invent_agi_seriously/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pl0u6w/what_do_you_do_if_you_invent_agi_seriously/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T19:29:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1pl3sfz</id>
    <title>Old but still gold</title>
    <updated>2025-12-12T21:31:53+00:00</updated>
    <author>
      <name>/u/kuyermanza</name>
      <uri>https://old.reddit.com/user/kuyermanza</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pl3sfz/old_but_still_gold/"&gt; &lt;img alt="Old but still gold" src="https://b.thumbs.redditmedia.com/yzmDPJV0-IyjdPpKlRTgcWXtY2Ft5Bcjq2k5kgDf-Vw.jpg" title="Old but still gold" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I don‚Äôt see much love given to old server GPUs like the V340Ls and MI25s so I set my mission to get a rig built for under $1000. &lt;/p&gt; &lt;p&gt;The workstation in the test bench frame is 4x V340Ls and an RTX2060, total of 76GB of VRAM. This one I built to try and sell on Facebook marketplace (so far no taker).&lt;/p&gt; &lt;p&gt;My personal rig was my mining rig with half dead GPUs, so I replaced them with 3x V340Ls and 2x MI25s in addition to the 2x RX5700s and RTX3060. Right now it‚Äôs got 108GB or VRAM.&lt;/p&gt; &lt;p&gt;I‚Äôm able to use ROCm 6.2.3 on Ubuntu 2404 and compile llamacpp from source targeting gfx900 and gfx1010. I see a pretty decent performance of about 10-40TPS on GPT-OSS 120B Q4 (26k context). I think it‚Äôs safe to say if you‚Äôre looking to build a rig right now and on budget, you should look into grabbing these older GPUs.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kuyermanza"&gt; /u/kuyermanza &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pl3sfz"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pl3sfz/old_but_still_gold/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pl3sfz/old_but_still_gold/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T21:31:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1pl3kzb</id>
    <title>The mistral-vibe CLI can work super well with gpt-oss</title>
    <updated>2025-12-12T21:23:06+00:00</updated>
    <author>
      <name>/u/tarruda</name>
      <uri>https://old.reddit.com/user/tarruda</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;To use it with GPT-OSS, you need my fork which sends reasoning content back to llama.cpp server: &lt;code&gt;uv tool install &amp;quot;mistral-vibe@git+https://github.com/tarruda/mistral-vibe.git@include-reasoning-content&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;I also sent a PR to merge the changes upstream: &lt;a href="https://github.com/mistralai/mistral-vibe/pull/123"&gt;https://github.com/mistralai/mistral-vibe/pull/123&lt;/a&gt;&lt;/p&gt; &lt;p&gt;On GPT-OSS 20b: Sometimes it gets confused with some of the tools. Specifically it sometimes tries to use &lt;code&gt;search_and_replace&lt;/code&gt;(which is designed to edit files) to grep for text.&lt;/p&gt; &lt;p&gt;But IMO it yields a better experience than devstral-2 due to how fast it is. In my testing it is also much better at coding than devstral-2.&lt;/p&gt; &lt;p&gt;I bet with a small dataset it would be possible to finetune gpt-oss to master using mistral-vibe tools.&lt;/p&gt; &lt;p&gt;And of course: If you can run GPT-OSS-120b it should definitely be better.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tarruda"&gt; /u/tarruda &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pl3kzb/the_mistralvibe_cli_can_work_super_well_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pl3kzb/the_mistralvibe_cli_can_work_super_well_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pl3kzb/the_mistralvibe_cli_can_work_super_well_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T21:23:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkxj0i</id>
    <title>Dolphin-v2, Universal Document Parsing Model from ByteDance Open Source</title>
    <updated>2025-12-12T17:18:49+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkxj0i/dolphinv2_universal_document_parsing_model_from/"&gt; &lt;img alt="Dolphin-v2, Universal Document Parsing Model from ByteDance Open Source" src="https://external-preview.redd.it/azQ1aTBvNWwzdDZnMWGXoQeCCsCSbT01XS-4Qf-TxasLW4Bw-m6-HAAzWjW4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f391a2f482d7600f705e7b3beeab48051894acfb" title="Dolphin-v2, Universal Document Parsing Model from ByteDance Open Source" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Dolphin-v2 is an enhanced universal document parsing model that substantially improves upon the original Dolphin.&lt;/p&gt; &lt;p&gt;Dolphin-v2 is built on &lt;strong&gt;Qwen2.5-VL-3B&lt;/strong&gt; backbone with:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Vision encoder based on Native Resolution Vision Transformer (NaViT)&lt;/li&gt; &lt;li&gt;Autoregressive decoder for structured output generation&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/ByteDance/Dolphin-v2#%F0%9F%93%88-performance"&gt;&lt;/a&gt;&lt;/h1&gt; &lt;p&gt;&lt;a href="https://huggingface.co/ByteDance/Dolphin-v2"&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Dolphin-v2 introduces several major enhancements over the original Dolphin:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Universal Document Support&lt;/strong&gt;: Handles both digital-born and photographed documents with realistic distortions&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Expanded Element Coverage&lt;/strong&gt;: Supports 21 element categories (up from 14), including dedicated code blocks and formulas&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Enhanced Precision&lt;/strong&gt;: Uses absolute pixel coordinates for more accurate spatial localization&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hybrid Parsing Strategy&lt;/strong&gt;: Element-wise parallel parsing for digital documents + holistic parsing for photographed documents&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Specialized Modules&lt;/strong&gt;: Dedicated parsing for code blocks with indentation preservation&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://huggingface.co/ByteDance/Dolphin-v2"&gt;Hugging Face Model Card &lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/xkkz615l3t6g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkxj0i/dolphinv2_universal_document_parsing_model_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pkxj0i/dolphinv2_universal_document_parsing_model_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T17:18:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkya3n</id>
    <title>Europe must be ready when the AI bubble bursts | ft.com</title>
    <updated>2025-12-12T17:48:11+00:00</updated>
    <author>
      <name>/u/ttkciar</name>
      <uri>https://old.reddit.com/user/ttkciar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkya3n/europe_must_be_ready_when_the_ai_bubble_bursts/"&gt; &lt;img alt="Europe must be ready when the AI bubble bursts | ft.com" src="https://external-preview.redd.it/Z434T0fKEWMAcK7uGeTuD__MBt7NsyTEjfA31tg2gfQ.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=55b69afa73db9b378351bcc9797e33a0fd5dc1c2" title="Europe must be ready when the AI bubble bursts | ft.com" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ttkciar"&gt; /u/ttkciar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.ft.com/content/0308f405-19ba-4aa8-9df1-40032e5ddc4e"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkya3n/europe_must_be_ready_when_the_ai_bubble_bursts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pkya3n/europe_must_be_ready_when_the_ai_bubble_bursts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T17:48:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkpsee</id>
    <title>Training an LLM only on 1800s London texts - 90GB dataset</title>
    <updated>2025-12-12T11:40:11+00:00</updated>
    <author>
      <name>/u/Remarkable-Trick-177</name>
      <uri>https://old.reddit.com/user/Remarkable-Trick-177</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, you may have seen a few of my posts here a couple months ago. If not, hi. I‚Äôm working on an open source project called TimeCapsuleLLM, where I train LLMs from scratch using only 1800-1875 London texts.&lt;/p&gt; &lt;p&gt;Until recently most of my work has been done on a small scale but over the past 3 months I‚Äôve been working on a much larger dataset for the next model. My newest dataset is 90GB with 135,000 documents, it contains basically every usable document that I could find on the Internet Archive for that time period. &lt;/p&gt; &lt;p&gt;Before doing any training, I ran an inspection across every file and generated a bias report covering temporal bias, gender/pronoun bias and geographic bias. Given the time period it‚Äôs strongly biased, but it‚Äôs important to study this. You can find the report on my GitHub if anyone wants to take a look. I‚Äôve also trained a small evaluation model on a 15GB subset to evaluate the dataset before I scale up to all 90GB. It‚Äôs a LlaMA style model (300M parameters) trained to 10K steps. Example output: &lt;/p&gt; &lt;p&gt;Prompt: Who is Charles Dickens?&lt;/p&gt; &lt;p&gt;Output with fixed spacing: ‚ÄúWho is Charles Dickens? Does that work more of his excellent stirring, in his plays, in the Great Company's farm? What I have yet to quote from Jack Pickett? Do you not know that they were a species of galloping, or sawing of their breasts, or what was to be done about the time when Jackson was looking on the window? What is the success of an Englishman, and which his son has not been discovering to me, whereby to accomplish such a weight? Did you ever make a passage into the old roadway, or to an anchor-breeze at the foot of our boat, which you must leave us? The fact is, that whether the wind would rise up from the plain on Saturday night or noontide, or till the north, or otherwise, we shall be compelled to describe a formidable barrier, with the same effects as the present. In this situation, at least, it is not too much to say that we have left that room. I believe there are three copies in the 'Five Hundred-fold,' to be referred to, as the first number of our readers who wish to.‚Äù&lt;/p&gt; &lt;p&gt;This type of output is expected since 10,000 steps is very early and it‚Äôs not a QA model. The model has already learned long, winding sentence structures, but can‚Äôt connect ideas logically yet. The main goal here was to see how clean the output would be. &lt;/p&gt; &lt;p&gt;One issue that came up was with the tokenizer, it over-split the text, splitting words into individual characters and subparts. So the model by default gives output like this: &lt;/p&gt; &lt;p&gt;Original output: ‚ÄúW ho is Charles D ic ens ? D oes that work more of h ise x cell ent st ir ring , in his pl ays , int he G reat C omp any 's f arm ? What I have y et to qu ote from J ack P ick ett ?‚Äù&lt;/p&gt; &lt;p&gt;It doubled the tokens for the same amount of data, making learning harder. Next steps are training another eval model and then scaling to the full 90GB dataset for a 1.2B parameter model. The eval model is already on Hugging Face and you can find a run script for it on my GitHub. I‚Äôll upload the 15GB subset to Hugging Face once the tokenizer is corrected.&lt;/p&gt; &lt;p&gt;I also want to thank everyone in this subreddit. This is the only place I‚Äôve shared the project other than github, and a lot of the early guidance came directly from here. I really appreciate how generous people here have been with advice. More updates soon.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/haykgrigo3/TimeCapsuleLLM"&gt;haykgrigo3/TimeCapsuleLLM: A LLM trained only on data from certain time periods to reduce modern bias&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/haykgrigorian/v2mini-eval1"&gt;haykgrigorian/v2mini-eval1 ¬∑ Hugging Face&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Remarkable-Trick-177"&gt; /u/Remarkable-Trick-177 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkpsee/training_an_llm_only_on_1800s_london_texts_90gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkpsee/training_an_llm_only_on_1800s_london_texts_90gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pkpsee/training_an_llm_only_on_1800s_london_texts_90gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T11:40:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1pky5u4</id>
    <title>Olmo 3.1 32B Think &amp; Instruct: New Additions to the Olmo Model Family</title>
    <updated>2025-12-12T17:43:36+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pky5u4/olmo_31_32b_think_instruct_new_additions_to_the/"&gt; &lt;img alt="Olmo 3.1 32B Think &amp;amp; Instruct: New Additions to the Olmo Model Family" src="https://preview.redd.it/bwgy5ldc8t6g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9fca8f97eb697220d6926581cc50bcb50e944a99" title="Olmo 3.1 32B Think &amp;amp; Instruct: New Additions to the Olmo Model Family" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Olmo 3.1 32B Think and Olmo 3.1 32B Instruct are the newest 32-billion-parameter models in the Olmo family, each optimized for different yet complementary use cases. &lt;/p&gt; &lt;ul&gt; &lt;li&gt;The &lt;strong&gt;Think model&lt;/strong&gt; is a deep-reasoning specialist, trained with extended reinforcement learning on the Dolci-Think-RL dataset to improve multi-step reasoning, math, logic, and code generation. &lt;/li&gt; &lt;li&gt;In contrast, the &lt;strong&gt;Instruct model&lt;/strong&gt; applies the Olmo instruction-tuning recipe at 32B scale, making it a strong fully open chat and agent foundation focused on instruction following, conversational fluency, and tool-use capabilities.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/allenai/olmo-31"&gt;HuggingFace Model Collection &lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bwgy5ldc8t6g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pky5u4/olmo_31_32b_think_instruct_new_additions_to_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pky5u4/olmo_31_32b_think_instruct_new_additions_to_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T17:43:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1pl4njj</id>
    <title>Running an LLM on a 3DS</title>
    <updated>2025-12-12T22:08:16+00:00</updated>
    <author>
      <name>/u/vreab</name>
      <uri>https://old.reddit.com/user/vreab</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pl4njj/running_an_llm_on_a_3ds/"&gt; &lt;img alt="Running an LLM on a 3DS" src="https://external-preview.redd.it/YXA0dDFoZXFqdTZnMayDKB9rDenP9HyWtMAfrMDzC_OwePMKvB7zq1t1dTfu.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=799b8c5d268a4ce59bfb456e310307c58099033f" title="Running an LLM on a 3DS" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vreab"&gt; /u/vreab &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9545t3eqju6g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pl4njj/running_an_llm_on_a_3ds/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pl4njj/running_an_llm_on_a_3ds/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T22:08:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1pl0ojb</id>
    <title>The new monster-server</title>
    <updated>2025-12-12T19:23:12+00:00</updated>
    <author>
      <name>/u/eribob</name>
      <uri>https://old.reddit.com/user/eribob</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pl0ojb/the_new_monsterserver/"&gt; &lt;img alt="The new monster-server" src="https://preview.redd.it/5kas5xaklt6g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5ebd2b38fa23a6f8f0aca6d1817cac736fa1e6d0" title="The new monster-server" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! &lt;/p&gt; &lt;p&gt;Just wanted to share my upgraded monster-server! I have bought the largest chassi I could reasonably find (Phanteks Enthoo pro 2 server) and filled it to the brim with GPU:s to run local LLM:s alongside my homelab. I am very happy how it has evloved / turned out! &lt;/p&gt; &lt;p&gt;&lt;strong&gt;I call it the &amp;quot;Monster server&amp;quot; :)&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Based on my trusted old X570 Taichi motherboard (extremely good!) and the Ryzen 3950x that I bought in 2019, that is still PLENTY fast today. I did not feel like spending a lot of money on a EPYC CPU/motherboard and new RAM, so instead I maxed out what I had. &lt;/p&gt; &lt;p&gt;The 24 PCI-e lanes are divided among the following: &lt;/p&gt; &lt;p&gt;3 GPU:s&lt;br /&gt; - 2 x RTX 3090 - both dual slot versions (inno3d RTX 3090 x3 and ASUS turbo RTX 3090)&lt;br /&gt; - 1 x RTX 4090 (an extremely chonky boi, 4 slots! ASUS TUF Gaming OC, that I got for reasonably cheap, around 1300USD equivalent). I run it on the &amp;quot;quiet&amp;quot; mode using the hardware switch hehe.&lt;/p&gt; &lt;p&gt;The 4090 runs off an M2 -&amp;gt; oculink -&amp;gt; PCIe adapter and a second PSU. The PSU is plugged in to the adapter board with its 24-pin connector and it powers on automatically when the rest of the system starts, very handy!&lt;br /&gt; &lt;a href="https://www.amazon.se/dp/B0DMTMJ95J"&gt;https://www.amazon.se/dp/B0DMTMJ95J&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Network: I have 10GB fiber internet for around 50 USD per month hehe...&lt;br /&gt; - 1 x 10GBe NIC - also connected using an M2 -&amp;gt; PCIe adapter. I had to mount this card creatively... &lt;/p&gt; &lt;p&gt;Storage:&lt;br /&gt; - 1 x Intel P4510 8TB U.2 enterprise NVMe. Solid storage for all my VM:s!&lt;br /&gt; - 4 x 18TB Seagate Exos HDD:s. For my virtualised TrueNAS. &lt;/p&gt; &lt;p&gt;RAM: 128GB Corsair Vengeance DDR4. Running at 2100MHz because I cannot get it stable when I try to run it faster, but whatever... LLMs are in VRAM anyway. &lt;/p&gt; &lt;p&gt;So what do I run on it?&lt;br /&gt; - GPT-OSS-120B, fully in VRAM, &amp;gt;100t/s tg. I did not yet find a better model, despite trying many... I use it for research, coding, and generally instead of google sometimes...&lt;br /&gt; I tried GLM4.5 air but it does not seem much smarter to me? Also slower. I would like to find a reasonably good model that I could run alongside FLUX1-dev-fp8 though, so I can generate images on the fly without having to switch. I am evaluating Qwen3-VL-32B for this&lt;/p&gt; &lt;p&gt;- Media server, Immich, Gitea, n8n&lt;/p&gt; &lt;p&gt;- My personal cloud using Seafile&lt;/p&gt; &lt;p&gt;- TrueNAS in a VM&lt;/p&gt; &lt;p&gt;- PBS for backups that is synced to a offsite PBS server at my brothers apartment&lt;/p&gt; &lt;p&gt;- a VM for coding, trying out devcontainers. &lt;/p&gt; &lt;p&gt;-&amp;gt; I also have a second server with a virtualised OPNsense VM as router. It runs other more &amp;quot;essential&amp;quot; services like PiHole, Traefik, Authelia, Headscale/tailscale, vaultwarden, a matrix server, anytype-sync and some other stuff... &lt;/p&gt; &lt;p&gt;---&lt;br /&gt; FINALLY: Why did I build this expensive machine? To make money by vibe-coding the next super-website? To cheat the stock market? To become the best AI engineer at Google? NO! Because I think it is fun to tinker around with computers, it is a hobby... &lt;/p&gt; &lt;p&gt;Thanks Reddit for teaching me all I needed to know to set this up! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eribob"&gt; /u/eribob &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5kas5xaklt6g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pl0ojb/the_new_monsterserver/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pl0ojb/the_new_monsterserver/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T19:23:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkpxss</id>
    <title>Someone from NVIDIA made a big mistake and uploaded the parent folder of their upcoming model on Hugging Face</title>
    <updated>2025-12-12T11:49:10+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkpxss/someone_from_nvidia_made_a_big_mistake_and/"&gt; &lt;img alt="Someone from NVIDIA made a big mistake and uploaded the parent folder of their upcoming model on Hugging Face" src="https://preview.redd.it/7r3bnj5ugr6g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0c3d5909063dd5ce912e8ebc203168db53b765be" title="Someone from NVIDIA made a big mistake and uploaded the parent folder of their upcoming model on Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From Xeophon on ùïè: &lt;a href="https://x.com/xeophon_/status/1999394570967089630"&gt;https://x.com/xeophon_/status/1999394570967089630&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7r3bnj5ugr6g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkpxss/someone_from_nvidia_made_a_big_mistake_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pkpxss/someone_from_nvidia_made_a_big_mistake_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T11:49:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
