<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-05T21:06:31+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ped5p2</id>
    <title>At What Point Does Owning GPUs Become Cheaper Than LLM APIs ? I</title>
    <updated>2025-12-04T21:58:14+00:00</updated>
    <author>
      <name>/u/Chimchimai</name>
      <uri>https://old.reddit.com/user/Chimchimai</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, I often see people say that using APIs is always cheaper and that running models locally is mainly for other reasons like privacy or control.&lt;/p&gt; &lt;p&gt;I am choosing infrastructure for my company with LLM features and I am trying to decide between frontier model APIs, AWS GPU rentals, or buying and self hosting GPUs.&lt;/p&gt; &lt;p&gt;My expected load is a few thousand users with peak concurrency around 256 requests per minute, plus heavy use of tool calls and multi step agents with steady daily traffic.&lt;/p&gt; &lt;p&gt;Based on my estimates, API token costs grow very fast at this scale, and AWS rentals seem to reach the full hardware price in about a year. For a long term 24/7 product, buying GPUs looks cheaper to me.&lt;/p&gt; &lt;p&gt;For those with real production experience, at what scale or workload does API or cloud rental still make more financial sense than owning the hardware? What costs am I likely underestimating ?&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;UPDATE (Also in comment below) : Hi everyone, I didn't expect my post to generate that much comments! To answer to the most asked questions :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;I will have some constraints if I go the api way, as the providers will need to serve models from the country in EU where I am preferably or at very least be a company from EU and have compute servers IN EU. (Clients need guarantees that their datas stay in this country), and I only have 2 main providers for that that I identified, with open source models not really up to date. (No glm, no deepseek, qwen 235b but with limited rates ..) So no OpenAI, Anthropic, .. (Even not Mistral like I first thought as they might send the computes outside EU in some cases for api calls) It also might mean that we could be easily in troubles if the two providers have issues. (Not a lot of redundancy)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;I understand now that I might have been wrong in my calculations concerning api tokens cost vs cost of hardware, thanks to all! Very insightful. I haven't thought of comparing actual token costs vs max throughput expected per day with possible hardware confs. This was exactly what I needed to see!&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;To clarify, usage won't be for coding, but mainly for multi agents human assistant for everyday tasks. So I'm not expecting context per user to be too large and was planning to have a hard limit around 50k tokens for in context.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;For the hardware, I imagined that it could be hosted in a datacenter, so security outside of classical server conf (fail2ban, firewall, etc) would be taken care by the DC where I will host my hardware.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So now I understand that price wise, indeed, I was completely wrong, but still make sense in a privacy, control, model choice, ways. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Chimchimai"&gt; /u/Chimchimai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ped5p2/at_what_point_does_owning_gpus_become_cheaper/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ped5p2/at_what_point_does_owning_gpus_become_cheaper/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ped5p2/at_what_point_does_owning_gpus_become_cheaper/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T21:58:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdzn2n</id>
    <title>legends</title>
    <updated>2025-12-04T13:11:47+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdzn2n/legends/"&gt; &lt;img alt="legends" src="https://preview.redd.it/vu26lxrns65g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8a61d2260347cccaa67517ffc3812c121edcd5d0" title="legends" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vu26lxrns65g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdzn2n/legends/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdzn2n/legends/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T13:11:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1pf0qbz</id>
    <title>been experimenting with parallel agent execution locally. way harder than i thought</title>
    <updated>2025-12-05T17:13:29+00:00</updated>
    <author>
      <name>/u/New-Needleworker1755</name>
      <uri>https://old.reddit.com/user/New-Needleworker1755</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;saw some commercial tools doing parallel agent execution (multiple ai agents working on different parts of code at the same time). wanted to replicate it locally&lt;/p&gt; &lt;p&gt;been trying for about 2 weeks. its way more complex than i expected&lt;/p&gt; &lt;p&gt;the idea sounds simple:&lt;/p&gt; &lt;p&gt;- agent 1 works on the api layer&lt;/p&gt; &lt;p&gt;- agent 2 works on database schema&lt;/p&gt; &lt;p&gt;- agent 3 writes tests&lt;/p&gt; &lt;p&gt;- all running at the same time&lt;/p&gt; &lt;p&gt;should be faster than sequential right&lt;/p&gt; &lt;p&gt;what ive tried:&lt;/p&gt; &lt;p&gt;started with qwen2.5-coder 32b. runs fine on my 3090 but coordinating multiple instances is the problem&lt;/p&gt; &lt;p&gt;tried running 3 separate ollama instances. works but they have no idea what the others are doing. agent 1 creates a function, agent 2 tries to call it but uses wrong parameters cause it doesnt know the signature&lt;/p&gt; &lt;p&gt;context sharing is the hard part. how do you keep multiple agents in sync without them stepping on each other&lt;/p&gt; &lt;p&gt;looked into langchain for orchestration. helps with the coordination but adds so much overhead. responses slow down cause of all the context passing&lt;/p&gt; &lt;p&gt;also tried using git worktrees (separate branches for each agent). better for avoiding conflicts but merging is still manual. and agents dont understand git well enough to resolve conflicts themselves&lt;/p&gt; &lt;p&gt;the isolation problem:&lt;/p&gt; &lt;p&gt;if agents are too isolated they make incompatible changes&lt;/p&gt; &lt;p&gt;if they share too much context they slow down and lose the parallel benefit&lt;/p&gt; &lt;p&gt;havent found the right balance yet&lt;/p&gt; &lt;p&gt;memory usage is also brutal. 3 instances need way more vram than i have. even with shared weights its too much for my setup. had to drop to 14b models which are less capable&lt;/p&gt; &lt;p&gt;commercial tools like verdent apparently solve this with cloud infrastructure and custom orchestration. makes sense but defeats the purpose of running locally&lt;/p&gt; &lt;p&gt;why even do this locally:&lt;/p&gt; &lt;p&gt;privacy, code never leaves my machine&lt;/p&gt; &lt;p&gt;cost, no api fees&lt;/p&gt; &lt;p&gt;control, can customize everything&lt;/p&gt; &lt;p&gt;learning, understanding how this stuff works&lt;/p&gt; &lt;p&gt;but honestly the complexity might not be worth it. single model sequential execution is way simpler and works fine for most stuff&lt;/p&gt; &lt;p&gt;current status:&lt;/p&gt; &lt;p&gt;got it working for simple cases (independent files, clear boundaries)&lt;/p&gt; &lt;p&gt;fails for complex cases (shared state, dependencies between agents)&lt;/p&gt; &lt;p&gt;performance gain is maybe 20-30% when it works. not the 3x i hoped for&lt;/p&gt; &lt;p&gt;thinking of giving up and just using a single local model or paying for a commercial tool that actually works&lt;/p&gt; &lt;p&gt;has anyone successfully done parallel agent execution locally? what models did you use, how did you handle coordination&lt;/p&gt; &lt;p&gt;or is this just not feasible without serious infrastructure&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/New-Needleworker1755"&gt; /u/New-Needleworker1755 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pf0qbz/been_experimenting_with_parallel_agent_execution/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pf0qbz/been_experimenting_with_parallel_agent_execution/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pf0qbz/been_experimenting_with_parallel_agent_execution/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T17:13:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1pf6jcj</id>
    <title>For those of you with ai max+ 395 mini pc that have experience or no bias hate with mac computers: Would you recommend a max 395+ to someone where it currently or are you thinking of switching to or back to mac?</title>
    <updated>2025-12-05T21:00:00+00:00</updated>
    <author>
      <name>/u/Smart_Frosting9846</name>
      <uri>https://old.reddit.com/user/Smart_Frosting9846</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am starting to feel with these insane prices the only logical option for reliability, peace of mind, and a plug and play experience a mac studio would be my best bet. I am wanting to use 70B models. Just looking for a computer to last me atleast the next 2 years. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Smart_Frosting9846"&gt; /u/Smart_Frosting9846 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pf6jcj/for_those_of_you_with_ai_max_395_mini_pc_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pf6jcj/for_those_of_you_with_ai_max_395_mini_pc_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pf6jcj/for_those_of_you_with_ai_max_395_mini_pc_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T21:00:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1pf6juo</id>
    <title>HMLR ‚Äì open-source memory system with perfect 1.00/1.00 RAGAS on every hard long-term-memory test (gpt-4.1-mini)</title>
    <updated>2025-12-05T21:00:31+00:00</updated>
    <author>
      <name>/u/JournalistGlum8326</name>
      <uri>https://old.reddit.com/user/JournalistGlum8326</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just shipped HMLR ‚Äî a complete memory system that gives you ‚Äúfriend who never forgets‚Äù behavior on gpt-4.1-mini (or any OpenAI-compatible endpoint).&lt;/p&gt; &lt;p&gt;Five tests everything else fails ‚Äî all 1.00/1.00 RAGAS:&lt;br /&gt; - 30-day multi-hop with zero keywords&lt;br /&gt; - ‚Äúignore everything you know about me‚Äù constraint trap&lt;br /&gt; - 5√ó fact rotation (timestamp wins)&lt;br /&gt; - 10-turn vague recall&lt;br /&gt; - cross-topic invariants&lt;/p&gt; &lt;p&gt;All tests fully reproducable and included as part of repo. see notes about testing.&lt;/p&gt; &lt;p&gt;Public proof (no login):&lt;br /&gt; &lt;a href="https://smith.langchain.com/public/4b3ee453-a530-49c1-abbf-8b85561e6beb/d"&gt;https://smith.langchain.com/public/4b3ee453-a530-49c1-abbf-8b85561e6beb/d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;MIT license, solo dev, works with local models via OpenAI-compatible endpoint.&lt;/p&gt; &lt;p&gt;Repo &lt;a href="https://github.com/Sean-V-Dev/HMLR-Agentic-AI-Memory-System"&gt;https://github.com/Sean-V-Dev/HMLR-Agentic-AI-Memory-System&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JournalistGlum8326"&gt; /u/JournalistGlum8326 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pf6juo/hmlr_opensource_memory_system_with_perfect_100100/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pf6juo/hmlr_opensource_memory_system_with_perfect_100100/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pf6juo/hmlr_opensource_memory_system_with_perfect_100100/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T21:00:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1pebwh6</id>
    <title>[open source] I finetuned my own LLM in 20m on my personal notes. Now it thinks in my style.</title>
    <updated>2025-12-04T21:08:58+00:00</updated>
    <author>
      <name>/u/Robert-treboR</name>
      <uri>https://old.reddit.com/user/Robert-treboR</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pebwh6/open_source_i_finetuned_my_own_llm_in_20m_on_my/"&gt; &lt;img alt="[open source] I finetuned my own LLM in 20m on my personal notes. Now it thinks in my style." src="https://external-preview.redd.it/cnp4eTBhb3U1OTVnMfuPSbsqUMLpJROMWbsiBCXZtzJPCMmpR4Hze4lcXzSH.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=adfad9a694bdb5ac7bebdf3c924e2842afcf2999" title="[open source] I finetuned my own LLM in 20m on my personal notes. Now it thinks in my style." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I keep all of my notes as files in cursor&lt;br /&gt; It took me 20min to finetune/RL my personal DeepSeek model on them&lt;br /&gt; I used tinker API &amp;amp; Lora with Gemini to create train dataset&lt;br /&gt; Now I have a model that literally &lt;strong&gt;THINKS&lt;/strong&gt; like me. made it open source repo + tutorial&lt;/p&gt; &lt;p&gt;Github repo :&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/OneInterface/Finetune-your-notes"&gt;https://github.com/OneInterface/Finetune-your-notes&lt;/a&gt; &lt;/p&gt; &lt;p&gt;I like playing around with data and models. I see some interesting use cases in the industry. Who wants to bounce idea's?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Robert-treboR"&gt; /u/Robert-treboR &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/rnc81tnu595g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pebwh6/open_source_i_finetuned_my_own_llm_in_20m_on_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pebwh6/open_source_i_finetuned_my_own_llm_in_20m_on_my/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T21:08:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1peql6c</id>
    <title>Mistral 3 14b against the competition ?</title>
    <updated>2025-12-05T09:16:35+00:00</updated>
    <author>
      <name>/u/EffectiveGlove1651</name>
      <uri>https://old.reddit.com/user/EffectiveGlove1651</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;did you tried the new mistral models particularly 14B, and if yes how does it compare to the competition at the same range of parameters (between 10 and 30B) ?&lt;/p&gt; &lt;p&gt;Thanks in advance, &lt;/p&gt; &lt;p&gt;Pierre&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EffectiveGlove1651"&gt; /u/EffectiveGlove1651 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1peql6c/mistral_3_14b_against_the_competition/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1peql6c/mistral_3_14b_against_the_competition/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1peql6c/mistral_3_14b_against_the_competition/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T09:16:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1pec8hz</id>
    <title>speed optimizations for Qwen Next on CUDA have been merged into llama.cpp</title>
    <updated>2025-12-04T21:22:04+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/17584"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pec8hz/speed_optimizations_for_qwen_next_on_cuda_have/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pec8hz/speed_optimizations_for_qwen_next_on_cuda_have/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T21:22:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1pexsdg</id>
    <title>Thinking about buying 5060ti 16gb is it smart at current market</title>
    <updated>2025-12-05T15:21:05+00:00</updated>
    <author>
      <name>/u/ResponsibleTruck4717</name>
      <uri>https://old.reddit.com/user/ResponsibleTruck4717</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know there are a lot of we don't know, I don't expect anyone to give me definitive answer.&lt;/p&gt; &lt;p&gt;Currently I'm using 4060 it's small and not mighty but it allows me to run 12b - 15b models at 5-12 tokens per second depend on the context size.&lt;/p&gt; &lt;p&gt;llm for me is hobby I love tinker with stuff, so while the 4060 is small it does allows me some tinkering.&lt;/p&gt; &lt;p&gt;The plan was to wait until 60x0 series and then upgrade but with current market I'm not sure how long I will have to wait and what prices we will expect.&lt;/p&gt; &lt;p&gt;I was thinking on buying 5060ti 16gb and move the the 4060 to pcie 4x to offload.&lt;/p&gt; &lt;p&gt;how smart is it? anyway I would love hearing your opinions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResponsibleTruck4717"&gt; /u/ResponsibleTruck4717 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pexsdg/thinking_about_buying_5060ti_16gb_is_it_smart_at/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pexsdg/thinking_about_buying_5060ti_16gb_is_it_smart_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pexsdg/thinking_about_buying_5060ti_16gb_is_it_smart_at/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T15:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1pevec3</id>
    <title>Modern RAG Setups</title>
    <updated>2025-12-05T13:43:28+00:00</updated>
    <author>
      <name>/u/SlowFail2433</name>
      <uri>https://old.reddit.com/user/SlowFail2433</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello I don‚Äôt do RAG too much and I fell behind.&lt;/p&gt; &lt;p&gt;What do you think is best in RAG these days?&lt;/p&gt; &lt;p&gt;Which open source RAG repos have you particularly liked recently?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SlowFail2433"&gt; /u/SlowFail2433 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pevec3/modern_rag_setups/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pevec3/modern_rag_setups/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pevec3/modern_rag_setups/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T13:43:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1pf5mza</id>
    <title>Explanation of This Question</title>
    <updated>2025-12-05T20:23:27+00:00</updated>
    <author>
      <name>/u/Mindless_Pain1860</name>
      <uri>https://old.reddit.com/user/Mindless_Pain1860</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pf5mza/explanation_of_this_question/"&gt; &lt;img alt="Explanation of This Question" src="https://preview.redd.it/a3uh30qm2g5g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7f31d012c3ad93aa8fca6501dc260dd9ec753fab" title="Explanation of This Question" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There was a lot of discussion, and I got a lot of downvotes from people, so let me clarify my points.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;From the model‚Äôs perspective&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The model doesn‚Äôt ‚Äúsee‚Äù raw text. It only sees sequences of token IDs, which it immediately turns into vectors using an embedding table.&lt;/p&gt; &lt;p&gt;There‚Äôs an embedding matrix: one row per token ID.&lt;/p&gt; &lt;p&gt;Given a token ID, the model looks up its embedding vector (a list of numbers).&lt;/p&gt; &lt;p&gt;These vectors are usually initialized randomly and then updated during training, so each token ends up with a distinct embedding that captures how it‚Äôs used in the training data.&lt;/p&gt; &lt;p&gt;In a Transformer, individual vectors (for example, the activations on special ‚Äògist‚Äô tokens) can be trained to encode information equivalent to a whole sentence or prompt, so a small set of vectors can stand in for a much longer piece of text. &lt;a href="https://arxiv.org/abs/2304.08467"&gt;&lt;sup&gt;\&lt;/sup&gt;1])&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The model then processes these embeddings through many layers and finally outputs a probability distribution over the next token ID. A sampling strategy (greedy, top-k, nucleus, etc.) picks one token, and when you decode the sequence of tokens back to text, that‚Äôs what you see as the model‚Äôs ‚Äúoutput‚Äù.&lt;/p&gt; &lt;p&gt;So, internally the model works with embeddings and token IDs, not with human-readable text.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;From the tokenizer‚Äôs perspective&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The tokenizer does see text (usually UTF-8). Its job is to map between text and token IDs.&lt;/p&gt; &lt;p&gt;It uses some algorithm (e.g., BPE, unigram, WordPiece) and a vocabulary that assigns each token string (like &amp;quot;ing&amp;quot;, &amp;quot;hello&amp;quot;, &amp;quot;„ÅÆ&amp;quot;, etc.) to an integer ID.&lt;/p&gt; &lt;p&gt;When encoding, it splits the text into tokens and returns their IDs.&lt;/p&gt; &lt;p&gt;When decoding, it maps IDs back to token strings and joins them into text.&lt;/p&gt; &lt;p&gt;During training, the model learns embeddings for those token IDs so that their vectors reflect how the corresponding token strings are used in context. In that sense, each token ‚Äúmeans‚Äù whatever its token text tends to represent in the training data.&lt;/p&gt; &lt;p&gt;Most tokenizers also define special tokens (like &amp;lt;bos&amp;gt;, &amp;lt;eos&amp;gt;, &amp;lt;pad&amp;gt;, &amp;lt;user&amp;gt;, etc.). Libraries often let you choose whether to allow or disallow special tokens when encoding normal text.&lt;a href="https://github.com/openai/tiktoken/blob/97e49cbadd500b5cc9dbb51a486f0b42e6701bee/src/py.rs#L39"&gt;&lt;sup&gt;\&lt;/sup&gt;2])&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;From the inference engine‚Äôs perspective&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;An inference engine (like vLLM, text-generation-inference, etc.) is the glue around the model and tokenizer.&lt;/p&gt; &lt;p&gt;It can:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Take a user‚Äôs text, apply a chat template (e.g. wrap it in system/user/assistant tags), combine everything into one prompt string, tokenize it, and feed the resulting IDs to the model.&lt;/li&gt; &lt;li&gt;Or accept already tokenized input (precomputed token IDs) and send those directly to the model.&lt;a href="https://docs.vllm.ai/en/v0.4.3/dev/offline_inference/llm_inputs.html"&gt;&lt;sup&gt;\&lt;/sup&gt;3])&lt;/a&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Either way, the model itself only ever sees token IDs -&amp;gt; embeddings, and returns probabilities over token IDs.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My defense&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. ‚ÄúHow will the tokenizer know which &amp;lt;message&amp;gt; is schema vs user input?‚Äù&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;It doesn‚Äôt have to ‚Äúknow‚Äù that at all.&lt;/p&gt; &lt;p&gt;The tokenizer just turns &lt;strong&gt;strings into IDs&lt;/strong&gt;. The distinction between:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;‚Äúthis &lt;code&gt;&amp;lt;message&amp;gt;&lt;/code&gt; came from my chat template‚Äù and&lt;/li&gt; &lt;li&gt;‚Äúthis &lt;code&gt;&amp;lt;message&amp;gt;&lt;/code&gt; came from the user‚Äôs raw text‚Äù&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;is made &lt;strong&gt;before tokenization&lt;/strong&gt;, in the application code / chat framework.&lt;/p&gt; &lt;p&gt;In practice you do something like:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Encode your &lt;strong&gt;schema/template&lt;/strong&gt; with special tokens enabled (so &lt;code&gt;&amp;lt;message&amp;gt;&lt;/code&gt; can map to a single special token if you want).&lt;/li&gt; &lt;li&gt;Encode &lt;strong&gt;user content&lt;/strong&gt; with special tokens &lt;em&gt;disabled&lt;/em&gt; (or with a different &lt;code&gt;allowed_special&lt;/code&gt; set).&lt;/li&gt; &lt;li&gt;Concatenate the token ID sequences and send that to the model.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;There is no guessing:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The app already knows which part is schema vs user ‚Äì it‚Äôs literally in separate variables/structures.&lt;/li&gt; &lt;li&gt;You just use different tokenization settings for those parts, or pretokenize them separately.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you &lt;em&gt;choose&lt;/em&gt; to mash everything into one raw string and run the tokenizer once with the same settings, that‚Äôs your own design choice ‚Äì not a fundamental limitation of using &lt;code&gt;&amp;lt;message&amp;gt;&lt;/code&gt;-style markers.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. ‚ÄúBut &amp;lt;message&amp;gt; can appear in user input as text‚Äù&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;If the user literally types &lt;code&gt;&amp;lt;message&amp;gt;&lt;/code&gt;, and I‚Äôm encoding &lt;strong&gt;user text&lt;/strong&gt; with special tokens disabled, it just becomes a normal sequence of tokens like &lt;code&gt;&amp;lt;&lt;/code&gt;, &lt;code&gt;message&lt;/code&gt;, &lt;code&gt;&amp;gt;&lt;/code&gt; or whatever the vocab uses. The only &lt;code&gt;&amp;lt;message&amp;gt;&lt;/code&gt; that gets mapped to a special token is the one &lt;strong&gt;I&lt;/strong&gt; insert as part of the template, under a different encode call / config.&lt;/p&gt; &lt;p&gt;Context is controlled by the application, not inferred magically by the tokenizer.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3. ‚ÄúStoring the conversation in an XML file with &amp;lt;message&amp;gt;[conversation]&amp;lt;/message&amp;gt; will break‚Äù&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;If you‚Äôre putting arbitrary text inside XML, you already have to deal with:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;&amp;lt;&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;&amp;amp;&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;]]&amp;gt;&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;and so on, &lt;em&gt;regardless&lt;/em&gt; of any LLM stuff. This is just basic XML hygiene.&lt;/p&gt; &lt;p&gt;Solutions that have existed for decades:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Escape content (&lt;code&gt;&amp;amp;lt;&lt;/code&gt;, &lt;code&gt;&amp;amp;amp;&lt;/code&gt;, etc.), or&lt;/li&gt; &lt;li&gt;Store the text inside &lt;code&gt;&amp;lt;![CDATA[ ... ]]&amp;gt;&lt;/code&gt;, or&lt;/li&gt; &lt;li&gt;Store the actual conversation payload as JSON/base64/whatever inside a node.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you dump raw, unescaped prompt text containing &lt;code&gt;&amp;lt;message&amp;gt;&lt;/code&gt; or &lt;code&gt;&amp;lt;foo&amp;gt;&lt;/code&gt; directly into XML and your parser chokes, that‚Äôs not a ‚Äúspecial token‚Äù problem, that‚Äôs ‚ÄúI didn‚Äôt escape my XML‚Äù problem. The same issue would happen if the model output literal &lt;code&gt;&amp;lt;div&amp;gt;&lt;/code&gt; or &lt;code&gt;&amp;lt;script&amp;gt;&lt;/code&gt; tags.&lt;/p&gt; &lt;p&gt;XML compatibility is solved at the &lt;strong&gt;storage layer&lt;/strong&gt;, not by forbidding certain delimiter strings in your prompt template.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;4. ‚ÄúParsers/displayers break, so those tags are just a bad choice‚Äù&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Only if those parsers are naively treating everything as markup instead of text.&lt;/p&gt; &lt;p&gt;If a UI component is supposed to show ‚Äúthe conversation text‚Äù, then its input should already be:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Properly escaped (for HTML/XML), or&lt;/li&gt; &lt;li&gt;Coming from a plain-text field, not re-parsed as XML/HTML on display.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Blaming &lt;code&gt;&amp;lt;message&amp;gt;&lt;/code&gt; for that is like saying ‚Äúwe can‚Äôt show &lt;code&gt;&amp;lt;b&amp;gt;&lt;/code&gt; in a chat app because a browser might try to render it bold‚Äù ‚Äì no, you escape it correctly and it‚Äôs fine.&lt;/p&gt; &lt;p&gt;It‚Äôs a &lt;strong&gt;plumbing&lt;/strong&gt; issue, not a model/tokenizer issue.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;5. ‚ÄúDisabling special tokenization would break instruct models that expect those tokens‚Äù&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;You don‚Äôt globally ‚Äúturn off special tokens‚Äù for the entire prompt. You:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Insert the &lt;strong&gt;template&lt;/strong&gt; pieces (system, role markers, etc.) yourself, possibly using special tokens or fixed strings.&lt;/li&gt; &lt;li&gt;Encode those template pieces with special tokens enabled.&lt;/li&gt; &lt;li&gt;Encode &lt;strong&gt;user text&lt;/strong&gt; separately with special tokens disabled (or a restricted &lt;code&gt;allowed_special&lt;/code&gt;).&lt;/li&gt; &lt;li&gt;Concatenate the token IDs and feed that to the model.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The model still sees exactly the chat template it was trained on. You‚Äôre not depriving it of its special markers. You‚Äôre just making sure &lt;strong&gt;user text doesn‚Äôt accidentally get interpreted as those markers&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;If your stack only supports ‚Äúhere‚Äôs one giant string, please tokenize with one set of rules‚Äù, that‚Äôs a limitation of that particular wrapper/API, not of the underlying model or tokenizer design. Many libraries already support:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Pre-tokenized input, or&lt;/li&gt; &lt;li&gt;Different &lt;code&gt;allowed_special&lt;/code&gt; / &lt;code&gt;disallowed_special&lt;/code&gt; configs per encode call.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Reference:&lt;br /&gt; [1]: &lt;a href="https://arxiv.org/abs/2304.08467"&gt;[2304.08467] Learning to Compress Prompts with Gist Tokens&lt;/a&gt;&lt;br /&gt; [2]: &lt;a href="https://github.com/openai/tiktoken/blob/97e49cbadd500b5cc9dbb51a486f0b42e6701bee/src/py.rs#L39"&gt;Tiktoken&lt;/a&gt;&lt;br /&gt; [3]: &lt;a href="https://docs.vllm.ai/en/v0.4.3/dev/offline_inference/llm_inputs.html"&gt;LLM Inputs ‚Äî vLLM&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mindless_Pain1860"&gt; /u/Mindless_Pain1860 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/a3uh30qm2g5g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pf5mza/explanation_of_this_question/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pf5mza/explanation_of_this_question/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T20:23:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1pf3ai8</id>
    <title>Llama 405B is worse than Gemma 3 12B?</title>
    <updated>2025-12-05T18:51:31+00:00</updated>
    <author>
      <name>/u/Express_Seesaw_8418</name>
      <uri>https://old.reddit.com/user/Express_Seesaw_8418</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pf3ai8/llama_405b_is_worse_than_gemma_3_12b/"&gt; &lt;img alt="Llama 405B is worse than Gemma 3 12B?" src="https://b.thumbs.redditmedia.com/IlhYPt_IftUhVM_ROE1INTbRT5_7veltkfuDGvYdCjE.jpg" title="Llama 405B is worse than Gemma 3 12B?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/h6ligvsujf5g1.png?width=1228&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5b6c9e928a752c5b4dfce1b2eda868d27f425a3a"&gt;https://preview.redd.it/h6ligvsujf5g1.png?width=1228&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5b6c9e928a752c5b4dfce1b2eda868d27f425a3a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I was browsing LMArena and discovered that Llama 405B ranked lower than many smaller models (gemma-3-12b-it, Qwen3-30B-A3B-Instruct-2507, mistral-small-2506).&lt;/p&gt; &lt;p&gt;I assumed the leaderboard isn't perfect but to me this seems crazy and I'm curious what the deal is. Am I wrong for assuming LMArena is roughly accurate?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Express_Seesaw_8418"&gt; /u/Express_Seesaw_8418 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pf3ai8/llama_405b_is_worse_than_gemma_3_12b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pf3ai8/llama_405b_is_worse_than_gemma_3_12b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pf3ai8/llama_405b_is_worse_than_gemma_3_12b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T18:51:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1per0lz</id>
    <title>100% Local AI for VSCode?</title>
    <updated>2025-12-05T09:44:50+00:00</updated>
    <author>
      <name>/u/Baldur-Norddahl</name>
      <uri>https://old.reddit.com/user/Baldur-Norddahl</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am using VS Code with Roo Code and GLM 4.5 Air or GPT-OSS 120b running 100% locally. But there ara bits and pieces of build in AI in VS Code that I can't seem to get rid of. And those things will upload my code to unknown parties, which I definitely don't like.&lt;/p&gt; &lt;p&gt;First is the code completion (Copilot) - this is tied to my Github subscription. How do I replace it with local AI instead?&lt;/p&gt; &lt;p&gt;We also have the autogenerate a git commit message using AI. Can I use a local model instead of whatever it uses by default? Maybe even get more useful messages, because the ones it generates are often quite useless.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Baldur-Norddahl"&gt; /u/Baldur-Norddahl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1per0lz/100_local_ai_for_vscode/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1per0lz/100_local_ai_for_vscode/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1per0lz/100_local_ai_for_vscode/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T09:44:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1pext5t</id>
    <title>Why did GLM stop creating smaller models?</title>
    <updated>2025-12-05T15:21:58+00:00</updated>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GLM 30B 3B MoE would be really great.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pext5t/why_did_glm_stop_creating_smaller_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pext5t/why_did_glm_stop_creating_smaller_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pext5t/why_did_glm_stop_creating_smaller_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T15:21:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1pei0q3</id>
    <title>Mistral 3 Large 675B up on huggingface</title>
    <updated>2025-12-05T01:27:34+00:00</updated>
    <author>
      <name>/u/someone383726</name>
      <uri>https://old.reddit.com/user/someone383726</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyone got 1.35TB of VRAM I could borrow?&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mistralai/Mistral-Large-3-675B-Instruct-2512-BF16"&gt;https://huggingface.co/mistralai/Mistral-Large-3-675B-Instruct-2512-BF16&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/someone383726"&gt; /u/someone383726 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pei0q3/mistral_3_large_675b_up_on_huggingface/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pei0q3/mistral_3_large_675b_up_on_huggingface/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pei0q3/mistral_3_large_675b_up_on_huggingface/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T01:27:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1pepwnn</id>
    <title>Qwen3-Next-80B-A3B or Gpt-oss-120b?</title>
    <updated>2025-12-05T08:32:39+00:00</updated>
    <author>
      <name>/u/custodiam99</name>
      <uri>https://old.reddit.com/user/custodiam99</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I mainly used Gpt-oss-120b (High reasoning) in the last months (summarizing, knowledge search, complex reasoning) and it proved very useful. Apart from being censored heavily (sometimes in a quite irrational way) it is a wonderful model. But I was excited to try the new Qwen model. So I downloaded Qwen3-Next-80B-A3B q6 (Thinking and Instruct) - and &lt;strong&gt;&lt;em&gt;I wasn't impressed&lt;/em&gt;&lt;/strong&gt;. It does not seem to be any better, in fact it seems less intelligent. Am I wrong? Let's talk about it!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/custodiam99"&gt; /u/custodiam99 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pepwnn/qwen3next80ba3b_or_gptoss120b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pepwnn/qwen3next80ba3b_or_gptoss120b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pepwnn/qwen3next80ba3b_or_gptoss120b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T08:32:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1pf4nuf</id>
    <title>Are models creators choosing to not do QAT?</title>
    <updated>2025-12-05T19:44:55+00:00</updated>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;QAT is fairly cheap process compared to full training,why are so many companies publishing their models in full precision without investing in QAT? And I'm not saying that &amp;quot;just publish 4-bit weights and leave it&amp;quot; it's VERY CHEAP to serve both FP16 and FP4/INT4 weights on HuggingFace,it will practically cost the company nothing additional compared to the full training run.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pf4nuf/are_models_creators_choosing_to_not_do_qat/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pf4nuf/are_models_creators_choosing_to_not_do_qat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pf4nuf/are_models_creators_choosing_to_not_do_qat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T19:44:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1peqbu0</id>
    <title>Local LLMs were supposed to simplify my life‚Ä¶ now I need a guide for my guides</title>
    <updated>2025-12-05T09:00:09+00:00</updated>
    <author>
      <name>/u/Fab_Terminator</name>
      <uri>https://old.reddit.com/user/Fab_Terminator</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I installed Ollama ‚Äújust to try it.‚Äù Then I discovered text-generation-webui. Then I discovered LM Studio. Then I discovered quantizations‚Ä¶ rope scaling‚Ä¶ vocab merging‚Ä¶ GPU offloading‚Ä¶&lt;/p&gt; &lt;p&gt;Now I'm 30 hours deep into tweaking settings so I can ask my computer, ‚ÄúWhat should I cook today?‚Äù&lt;/p&gt; &lt;p&gt;Does anyone else feel like local AI is the new homelab rabbit hole?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fab_Terminator"&gt; /u/Fab_Terminator &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1peqbu0/local_llms_were_supposed_to_simplify_my_life_now/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1peqbu0/local_llms_were_supposed_to_simplify_my_life_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1peqbu0/local_llms_were_supposed_to_simplify_my_life_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T09:00:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1peuh30</id>
    <title>https://livebench.ai - Open Weight Models Only</title>
    <updated>2025-12-05T13:01:22+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1peuh30/httpslivebenchai_open_weight_models_only/"&gt; &lt;img alt="https://livebench.ai - Open Weight Models Only" src="https://preview.redd.it/ohayhhgivd5g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=21252697c2c953c1d038980ffc92cd091416be50" title="https://livebench.ai - Open Weight Models Only" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There were some questions about how Qwen 3 Next compares to GPT-OSS. I think whole table may be useful. What do you think about this ordering?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ohayhhgivd5g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1peuh30/httpslivebenchai_open_weight_models_only/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1peuh30/httpslivebenchai_open_weight_models_only/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T13:01:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1pezh1k</id>
    <title>Blood and stardust! Watch 9 local LLMs debate Star Wars vs Star Trek</title>
    <updated>2025-12-05T16:26:16+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pezh1k/blood_and_stardust_watch_9_local_llms_debate_star/"&gt; &lt;img alt="Blood and stardust! Watch 9 local LLMs debate Star Wars vs Star Trek" src="https://external-preview.redd.it/aTRpOTR6dzBzZTVnMSe6y4zOHIyGUsL1YtaqMqowYCso8PTyfwm1haQrI9uz.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=63024def9b5244ed1828e41a7d4ab09eeb725073" title="Blood and stardust! Watch 9 local LLMs debate Star Wars vs Star Trek" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The last post was too much fun, so here we go again.&lt;/p&gt; &lt;p&gt;Debate Arena v2 adds the top suggestions from last time:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;NO MORE TIES&lt;/strong&gt; for &lt;a href="/u/NodeTraverser"&gt;u/NodeTraverser&lt;/a&gt;, the 9th model guarantees one side wins&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Smooth setup&lt;/strong&gt; for &lt;a href="/u/Vercinthia"&gt;u/Vercinthia&lt;/a&gt; and &lt;a href="/u/work__reddit"&gt;u/work__reddit&lt;/a&gt;, the web app helps you install, start the backend, and download models&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Scoreboard&lt;/strong&gt; for &lt;a href="/u/Zissuo"&gt;u/Zissuo&lt;/a&gt;, know which LLMs betrayed your ideals&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Enhanced debating&lt;/strong&gt; for &lt;a href="/u/r4in311"&gt;u/r4in311&lt;/a&gt; and &lt;a href="/u/slolobdill44"&gt;u/slolobdill44&lt;/a&gt;, 5 debate stages with their own purpose and system prompt&lt;/li&gt; &lt;/ul&gt; &lt;blockquote&gt; &lt;pre&gt;&lt;code&gt; üé§ Phase 1: Hot Takes üí¨ Phase 2: Reactions üçø Phase 3: The Plot Thickens üéØ Phase 4: Final Thoughts &amp;amp; Voting ‚ö° Phase 5: Lightning Round - Vote Now &lt;/code&gt;&lt;/pre&gt; &lt;/blockquote&gt; &lt;p&gt;Details and quick start instructions are &lt;a href="https://github.com/lemonade-sdk/lemonade/blob/main/examples/demos/debate-arena.md"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Have I taken this too far, or not far enough? Tell me your burning yes/no questions and feature suggestions and I might do a v3 next week!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/t6y4gtw0se5g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pezh1k/blood_and_stardust_watch_9_local_llms_debate_star/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pezh1k/blood_and_stardust_watch_9_local_llms_debate_star/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T16:26:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1pez5ch</id>
    <title>Why do LLM response formats often use &lt;| |&gt; (as in &lt;|message|&gt;) instead of &lt;message&gt;, and why do they use &lt;|end|&gt; instead of &lt;/message&gt;?</title>
    <updated>2025-12-05T16:14:03+00:00</updated>
    <author>
      <name>/u/Amazydayzee</name>
      <uri>https://old.reddit.com/user/Amazydayzee</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pez5ch/why_do_llm_response_formats_often_use_as_in/"&gt; &lt;img alt="Why do LLM response formats often use &amp;lt;| |&amp;gt; (as in &amp;lt;|message|&amp;gt;) instead of &amp;lt;message&amp;gt;, and why do they use &amp;lt;|end|&amp;gt; instead of &amp;lt;/message&amp;gt;?" src="https://preview.redd.it/5e5ir2zlte5g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d7b47f1bd3dabcdabf34fcf757aaea013f0a0c73" title="Why do LLM response formats often use &amp;lt;| |&amp;gt; (as in &amp;lt;|message|&amp;gt;) instead of &amp;lt;message&amp;gt;, and why do they use &amp;lt;|end|&amp;gt; instead of &amp;lt;/message&amp;gt;?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If I had to guess, I'd assume it's tokenization because &amp;quot;&amp;lt;|&amp;quot; is not a very commonly occurring pattern in pre-training, which allows devs to make &amp;quot;&amp;lt;|message|&amp;gt;&amp;quot; a single token.&lt;/p&gt; &lt;p&gt;That being said, the &amp;lt;|end|&amp;gt; is still a bit disorienting, at least to me reading as a human. You can see that the &amp;lt;|start|&amp;gt; block ends with another &amp;lt;|start|&amp;gt; block, but the &amp;lt;|message|&amp;gt; block ends in a &amp;lt;|end|&amp;gt; block.&lt;/p&gt; &lt;p&gt;This image is from &lt;a href="https://github.com/openai/harmony"&gt;openai's harmony response template&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Amazydayzee"&gt; /u/Amazydayzee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5e5ir2zlte5g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pez5ch/why_do_llm_response_formats_often_use_as_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pez5ch/why_do_llm_response_formats_often_use_as_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T16:14:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1pexnfp</id>
    <title>LongCat-Image: 6B model with strong efficiency, photorealism, and Chinese text rendering</title>
    <updated>2025-12-05T15:15:46+00:00</updated>
    <author>
      <name>/u/abdouhlili</name>
      <uri>https://old.reddit.com/user/abdouhlili</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pexnfp/longcatimage_6b_model_with_strong_efficiency/"&gt; &lt;img alt="LongCat-Image: 6B model with strong efficiency, photorealism, and Chinese text rendering" src="https://external-preview.redd.it/wKVXYkAgQd2YCzTWH9wJHT9a9O4yMSOT8w5RQDj-cGQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=266d079c79f26252dc4def3cc7e476d0209bb0af" title="LongCat-Image: 6B model with strong efficiency, photorealism, and Chinese text rendering" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdouhlili"&gt; /u/abdouhlili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/meituan-longcat/LongCat-Image"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pexnfp/longcatimage_6b_model_with_strong_efficiency/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pexnfp/longcatimage_6b_model_with_strong_efficiency/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T15:15:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1pes3pu</id>
    <title>Basketball AI with RF-DETR, SAM2, and SmolVLM2</title>
    <updated>2025-12-05T10:53:12+00:00</updated>
    <author>
      <name>/u/RandomForests92</name>
      <uri>https://old.reddit.com/user/RandomForests92</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pes3pu/basketball_ai_with_rfdetr_sam2_and_smolvlm2/"&gt; &lt;img alt="Basketball AI with RF-DETR, SAM2, and SmolVLM2" src="https://external-preview.redd.it/N2czYjlxanU4ZDVnMZ78lEX-DYraHupkrsvdafpxwsSm-SfqaN6z7l9OZr1B.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aab29ff74cd044468cb8bd288eeaf647b5329d32" title="Basketball AI with RF-DETR, SAM2, and SmolVLM2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;resources: &lt;a href="https://www.youtube.com/watch?v=yGQb9KkvQ1Q"&gt;youtube&lt;/a&gt;, &lt;a href="https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/basketball-ai-how-to-detect-track-and-identify-basketball-players.ipynb"&gt;code&lt;/a&gt;, &lt;a href="https://blog.roboflow.com/identify-basketball-players"&gt;blog&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- player and number detection with RF-DETR&lt;/p&gt; &lt;p&gt;- player tracking with SAM2&lt;/p&gt; &lt;p&gt;- team clustering with SigLIP, UMAP and K-Means&lt;/p&gt; &lt;p&gt;- number recognition with SmolVLM2&lt;/p&gt; &lt;p&gt;- perspective conversion with homography&lt;/p&gt; &lt;p&gt;- player trajectory correction&lt;/p&gt; &lt;p&gt;- shot detection and classification&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RandomForests92"&gt; /u/RandomForests92 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/k6kmogju8d5g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pes3pu/basketball_ai_with_rfdetr_sam2_and_smolvlm2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pes3pu/basketball_ai_with_rfdetr_sam2_and_smolvlm2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T10:53:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1pf0q99</id>
    <title>You will own nothing and you will be happy!</title>
    <updated>2025-12-05T17:13:24+00:00</updated>
    <author>
      <name>/u/dreamyrhodes</name>
      <uri>https://old.reddit.com/user/dreamyrhodes</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Come and put everything in to cloud. We now getting into hardware as a service. The RAM craze will impact everything to the point where consumers can't afford normal hardware anymore because it's all scraped off, locked away and put into datacenters to sell to you services to store your data. (Of course that data also will be used to train AI models to sell to you as a service as well lol.)&lt;/p&gt; &lt;p&gt;You don't need RAM anymore nor do you need SSDs. You will store and process every byte of your digital life in some datacenter and pay a monthly fee to access and process it.&lt;/p&gt; &lt;p&gt;You will own nothing and you will be happy!&lt;/p&gt; &lt;p&gt;GN: WTF Just Happened? | The Corrupt Memory Industry &amp;amp; Micron&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=9A-eeJP0J7c"&gt;https://www.youtube.com/watch?v=9A-eeJP0J7c&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dreamyrhodes"&gt; /u/dreamyrhodes &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pf0q99/you_will_own_nothing_and_you_will_be_happy/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pf0q99/you_will_own_nothing_and_you_will_be_happy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pf0q99/you_will_own_nothing_and_you_will_be_happy/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T17:13:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
