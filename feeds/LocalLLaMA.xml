<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-05T16:41:46+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1peumxr</id>
    <title>[Project] A Distributed Orchestrator Architecture to replace Search Indexing</title>
    <updated>2025-12-05T13:09:02+00:00</updated>
    <author>
      <name>/u/sotpak_</name>
      <uri>https://old.reddit.com/user/sotpak_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am proposing a shift in how we connect LLMs to real-time data. Currently, we rely on Search Engines (RAG over indexed HTML) or Function Calling (which has a scalability limit inside the Context Window).&lt;/p&gt; &lt;p&gt;I built a POC called &lt;strong&gt;Agent Orchestrator&lt;/strong&gt; that moves the logic layer out of the LLM and into a distributed REST network.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Architecture:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Intent Classification:&lt;/strong&gt; The LLM receives a user query and hands it to the Orchestrator.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Async Routing:&lt;/strong&gt; Instead of the LLM selecting a tool, the Orchestrator queries a registry and triggers relevant external agents via REST API in parallel.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Local Inference:&lt;/strong&gt; The external agent (the website) runs its own inference/lookup locally and returns a synthesized answer.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Aggregation:&lt;/strong&gt; The Orchestrator aggregates the results and feeds them back to the user's LLM.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;What do you think about this concept?&lt;br /&gt; Would you add an ‚ÄúAgent Endpoint‚Äù to your webpage to generate answers for customers and appearing in their LLM conversations?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Repo:&lt;/strong&gt; &lt;a href="https://github.com/yaruchyo/octopus"&gt;&lt;strong&gt;https://github.com/yaruchyo/octopus&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;ReDeep Dive:&lt;/strong&gt; &lt;a href="https://www.aipetris.com/post/12"&gt;https://www.aipetris.com/post/12&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sotpak_"&gt; /u/sotpak_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1peumxr/project_a_distributed_orchestrator_architecture/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1peumxr/project_a_distributed_orchestrator_architecture/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1peumxr/project_a_distributed_orchestrator_architecture/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T13:09:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1petj5z</id>
    <title>Translating WEBPAGES using LOCAL MODEL on IPAD - pushing what is possible on mobile devices</title>
    <updated>2025-12-05T12:13:51+00:00</updated>
    <author>
      <name>/u/Glad-Speaker3006</name>
      <uri>https://old.reddit.com/user/Glad-Speaker3006</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1petj5z/translating_webpages_using_local_model_on_ipad/"&gt; &lt;img alt="Translating WEBPAGES using LOCAL MODEL on IPAD - pushing what is possible on mobile devices" src="https://external-preview.redd.it/MG1lYWNkNWdrZDVnMbGZuE73apWWV4OXUh_E1FgYvcn3hyCVV1TYFjL5UHZI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b1fb22b1a3d3b8334700811b62c218bbf7b9c3c8" title="Translating WEBPAGES using LOCAL MODEL on IPAD - pushing what is possible on mobile devices" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Browsers comes with translation tool - but few of them provide legible translations. We are not used to the high quality translation provided by LLMs, and we expect the same experience with webpages translation when browsing.&lt;/p&gt; &lt;p&gt;I am pleased to announce that Vector Space now integrates Webpage translation. Featuring:&lt;/p&gt; &lt;p&gt;- Use a LLM instead of translation APIs&lt;/p&gt; &lt;p&gt;- Works on mobile&lt;/p&gt; &lt;p&gt;- Call local models for unlimited and private, translation&lt;/p&gt; &lt;p&gt;- Perserve HTML structures and visuals&lt;/p&gt; &lt;p&gt;- Connect to OpenAI API for faster transaction (enter your API in the settings)&lt;/p&gt; &lt;p&gt;Result is some very nice translations! Please see the video. It is filmed on a M1 iPad.&lt;/p&gt; &lt;p&gt;Try it out here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://short.yomer.jp/vector-space"&gt;https://short.yomer.jp/vector-space&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Limitations and next directions:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Right now a relative large model (~4B) is needed for preserving HTML tags and improving translation quality. I believe a fine tuned model of a much smaller size can do the trick. With enough people supporting me I can work on it to increase translation speed at least 10x.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Due to Apple restriction on running GPU work in the background, currently only iPad multi tasking is supported on iOS. I believe this is solvable by either looking at Background Tasks framework or move to neural engine. &lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Glad-Speaker3006"&gt; /u/Glad-Speaker3006 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/nshmwk5gkd5g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1petj5z/translating_webpages_using_local_model_on_ipad/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1petj5z/translating_webpages_using_local_model_on_ipad/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T12:13:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1pejibu</id>
    <title>Ellora: Enhancing LLMs with LoRA - Standardized Recipes for Capability Enhancement</title>
    <updated>2025-12-05T02:37:23+00:00</updated>
    <author>
      <name>/u/asankhs</name>
      <uri>https://old.reddit.com/user/asankhs</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pejibu/ellora_enhancing_llms_with_lora_standardized/"&gt; &lt;img alt="Ellora: Enhancing LLMs with LoRA - Standardized Recipes for Capability Enhancement" src="https://external-preview.redd.it/eRk6CHfTA5K57x3-NJ52ExeaoCjq5mlJUWa3K9ehaSs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=67dc0f62319ecb984a7ebf6f253adcdcfaccc0ce" title="Ellora: Enhancing LLMs with LoRA - Standardized Recipes for Capability Enhancement" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/asankhs"&gt; /u/asankhs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/blog/codelion/ellora-lora-recipes"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pejibu/ellora_enhancing_llms_with_lora_standardized/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pejibu/ellora_enhancing_llms_with_lora_standardized/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T02:37:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1penmet</id>
    <title>Key Insights from the State of AI Report: What 100T Tokens Reveal About Model Usage</title>
    <updated>2025-12-05T06:07:40+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1penmet/key_insights_from_the_state_of_ai_report_what/"&gt; &lt;img alt="Key Insights from the State of AI Report: What 100T Tokens Reveal About Model Usage" src="https://external-preview.redd.it/I8s1kreihjYxvww-6N97nYVDeyvOSua5e5pQ0I02dIM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ee04018db47f903b764c2104ae1d522683a51250" title="Key Insights from the State of AI Report: What 100T Tokens Reveal About Model Usage" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently come across this &amp;quot;State of AI&amp;quot; report which provides a lot of insights regarding AI models usage based on 100 trillion token study.&lt;/p&gt; &lt;p&gt;Here is the brief summary of key insights from this report.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. Shift from Text Generation to Reasoning Models&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The release of reasoning models like o1 triggered a major transition from simple text-completion to multi-step, deliberate reasoning in real-world AI usage.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. Open-Source Models Rapidly Gaining Share&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Open-source models now account for roughly one-third of usage, showing strong adoption and growing competitiveness against proprietary models.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3. Rise of Medium-Sized Models (15B‚Äì70B)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Medium-sized models have become the preferred sweet spot for cost-performance balance, overtaking small models and competing with large ones.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;4. Rise of Multiple Open-Source Family Models&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The open-source landscape is no longer dominated by a single model family; multiple strong contenders now share meaningful usage.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;5. Coding &amp;amp; Productivity Still Major Use Cases&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Beyond creative usage, programming help, Q&amp;amp;A, translation, and productivity tasks remain high-volume practical applications.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;6. Growth of Agentic Inference&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Users increasingly employ LLMs in multi-step ‚Äúagentic‚Äù workflows involving planning, tool use, search, and iterative reasoning instead of single-turn chat.&lt;/p&gt; &lt;p&gt;I found &lt;strong&gt;2, 3 &amp;amp; 4 insights most exciting as they reveal the rise and adoption of open-source models&lt;/strong&gt;. Let me know insights from your experience with LLMs.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://openrouter.ai/state-of-ai"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1penmet/key_insights_from_the_state_of_ai_report_what/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1penmet/key_insights_from_the_state_of_ai_report_what/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T06:07:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1pekx2i</id>
    <title>RAG Paper 25.12.04</title>
    <updated>2025-12-05T03:44:53+00:00</updated>
    <author>
      <name>/u/Cheryl_Apple</name>
      <uri>https://old.reddit.com/user/Cheryl_Apple</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ol&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2512.05012v1"&gt;Factuality and Transparency Are All RAG Needs! Self-Explaining Contrastive Evidence Re-ranking&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2512.04790v1"&gt;Spatially-Enhanced Retrieval-Augmented Generation for Walkability and Urban Discovery&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2512.04738v1"&gt;OsmT: Bridging OpenStreetMap Queries and Natural Language with Open-source Tag-aware Language Models&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2512.04416v1"&gt;GovBench: Benchmarking LLM Agents for Real-World Data Governance Workflows&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2512.04343v1"&gt;The Personalization Paradox: Semantic Loss vs. Reasoning Gains in Agentic AI Q&amp;amp;A&lt;/a&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Collected by OpenBMB, transferred by&lt;/strong&gt; &lt;a href="https://www.ragview.ai/"&gt;&lt;strong&gt;RagView.ai&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;/&lt;/strong&gt; &lt;a href="https://github.com/RagView/RagView"&gt;&lt;strong&gt;github/RagView&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cheryl_Apple"&gt; /u/Cheryl_Apple &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pekx2i/rag_paper_251204/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pekx2i/rag_paper_251204/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pekx2i/rag_paper_251204/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T03:44:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1peyae6</id>
    <title>My eBay bargain ¬£720 workstation</title>
    <updated>2025-12-05T15:41:09+00:00</updated>
    <author>
      <name>/u/BigYoSpeck</name>
      <uri>https://old.reddit.com/user/BigYoSpeck</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1peyae6/my_ebay_bargain_720_workstation/"&gt; &lt;img alt="My eBay bargain ¬£720 workstation" src="https://b.thumbs.redditmedia.com/IBrX3iC-F3i-YWnY6NwRYLw0CudxJSc2JYwKQz9fpfg.jpg" title="My eBay bargain ¬£720 workstation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Since LLM's burst onto the scene I've been slumming it with my i7-1185G7 16gb laptop. If I'm patient you can squeeze a 12-14b model in and get 2-3tok/s or even gpt-oss-20b at about 10tok/s if you don't want any memory left to do anything else with.&lt;/p&gt; &lt;p&gt;I've got children and despite my advocating, skipping a Christmas or two to invest in a powerful system never quite got signed off on, so over the last couple of years I've been tucking bits and bobs aside from selling old gear, sticking with my Pixel 6 phone and pocketing the contract difference, and finally got to an amount of money that might just get me something useful.&lt;/p&gt; &lt;p&gt;In an ideal world I'd be getting an RTX 3090 based system but nothing ever quite showed up within budget. But something close enough popped up on eBay recently and the auction was mine for the princely sum of ¬£720.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Ryzen 9 5900X&lt;/li&gt; &lt;li&gt;64gb DDR4 3600mhz (2x32gb)&lt;/li&gt; &lt;li&gt;Radeon RX 6800 XT 16gb&lt;/li&gt; &lt;li&gt;18TB SSD?? (more on that in a minute)&lt;/li&gt; &lt;li&gt;Phantek P600S case&lt;/li&gt; &lt;li&gt;360mm AIO cooler&lt;/li&gt; &lt;li&gt;MSI MPG A1000G PSU&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;And a few extras which I wasn't in the market for but sweetened the deal (34&amp;quot; Samsung Ultrawide monitor, Steelseries Apex 5 keyboard + Rival 600 mouse, gaming chair, all the fans swapped out for Be Quiet Silent Wings 4's, Logitech BRIO 4k webcam and a USB condenser micrphone).&lt;/p&gt; &lt;p&gt;As I mentioned above it listed 18tb of SSD storage without specifics. I spied a mechanical harddrive in the pictures that wasn't listed in the spec and just assumed probably an honest mistake where it would have a 16tb mechanical drive, and a perfectly good 2tb SSD which I would be happy with at the price.&lt;/p&gt; &lt;p&gt;So auction won, I set off, inspected the physical condition, made sure the important specs were as advertised, gave it some stress testing to make sure it didn't die, and brought it all home.&lt;/p&gt; &lt;p&gt;Well the advertised 18tb of SSD storage I was almost right about. It has a 6tb WD Red (5.5tb formatted), a 2tb WD Black SN850X (1.8tb formatted), and the missing 11.7tb from the advert? Alas it's actually only 11.6tb when formatted. But it's a Samsung PM1735 12.8TB PCIe SSD. Might be worth more than the entire computer.&lt;/p&gt; &lt;p&gt;I've been putting it to work testing bigger models, first installing Ubuntu 25.10 (planning to settle with 26.04 when released) and then seeing how well 16gb of Radeon works with ROCm.&lt;/p&gt; &lt;p&gt;gpt-oss-20 was first. Entirely offloaded to VRAM with 80k context. Starts at about 120tok/s and by the time context is filling up down to about 60.&lt;/p&gt; &lt;p&gt;Then I wanted to see if it was even possible to get gpt-oss-120b running with only 64gb of system RAM. And the good news is it does, 17tok/s early on, and by the time it's getting to about 40k context that's down to 15tok/s. Prompt processing is slow, nearly 11 minutes to process a 40k token prompt&lt;/p&gt; &lt;p&gt;Next step is to keep saving (or maybe sell that PM1735 and the Steelseries mouse+kb), get a 3090 or 4090 and if memory prices ever normalise up to 96 or 128gb&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/1kwboplxne5g1.png?width=1345&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=be7eedc3a3daa444859b16f67b421cd7089f7bad"&gt;https://preview.redd.it/1kwboplxne5g1.png?width=1345&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=be7eedc3a3daa444859b16f67b421cd7089f7bad&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BigYoSpeck"&gt; /u/BigYoSpeck &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1peyae6/my_ebay_bargain_720_workstation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1peyae6/my_ebay_bargain_720_workstation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1peyae6/my_ebay_bargain_720_workstation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T15:41:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1pedmsi</id>
    <title>State of AI | OpenRouter | Paper</title>
    <updated>2025-12-04T22:16:54+00:00</updated>
    <author>
      <name>/u/adumdumonreddit</name>
      <uri>https://old.reddit.com/user/adumdumonreddit</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pedmsi/state_of_ai_openrouter_paper/"&gt; &lt;img alt="State of AI | OpenRouter | Paper" src="https://external-preview.redd.it/I8s1kreihjYxvww-6N97nYVDeyvOSua5e5pQ0I02dIM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ee04018db47f903b764c2104ae1d522683a51250" title="State of AI | OpenRouter | Paper" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;New paper/blog/thing from OpenRouter in collaboration with a16z on token/model usage on OpenRouter. Some interesting insights like how medium sized open source models are the new small, and Chinese vs. Rest of World releases&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/adumdumonreddit"&gt; /u/adumdumonreddit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://openrouter.ai/state-of-ai"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pedmsi/state_of_ai_openrouter_paper/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pedmsi/state_of_ai_openrouter_paper/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T22:16:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1pefrht</id>
    <title>is the new Deepseek v3.2 that bad?</title>
    <updated>2025-12-04T23:47:23+00:00</updated>
    <author>
      <name>/u/Caffdy</name>
      <uri>https://old.reddit.com/user/Caffdy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pefrht/is_the_new_deepseek_v32_that_bad/"&gt; &lt;img alt="is the new Deepseek v3.2 that bad?" src="https://preview.redd.it/vwvxerd4y95g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f3656af4dd7e0fe4438bebcc3c39d130546066e3" title="is the new Deepseek v3.2 that bad?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Caffdy"&gt; /u/Caffdy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vwvxerd4y95g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pefrht/is_the_new_deepseek_v32_that_bad/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pefrht/is_the_new_deepseek_v32_that_bad/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T23:47:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ped5p2</id>
    <title>At What Point Does Owning GPUs Become Cheaper Than LLM APIs ? I</title>
    <updated>2025-12-04T21:58:14+00:00</updated>
    <author>
      <name>/u/Chimchimai</name>
      <uri>https://old.reddit.com/user/Chimchimai</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, I often see people say that using APIs is always cheaper and that running models locally is mainly for other reasons like privacy or control.&lt;/p&gt; &lt;p&gt;I am choosing infrastructure for my company with LLM features and I am trying to decide between frontier model APIs, AWS GPU rentals, or buying and self hosting GPUs.&lt;/p&gt; &lt;p&gt;My expected load is a few thousand users with peak concurrency around 256 requests per minute, plus heavy use of tool calls and multi step agents with steady daily traffic.&lt;/p&gt; &lt;p&gt;Based on my estimates, API token costs grow very fast at this scale, and AWS rentals seem to reach the full hardware price in about a year. For a long term 24/7 product, buying GPUs looks cheaper to me.&lt;/p&gt; &lt;p&gt;For those with real production experience, at what scale or workload does API or cloud rental still make more financial sense than owning the hardware? What costs am I likely underestimating ?&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;UPDATE (Also in comment below) : Hi everyone, I didn't expect my post to generate that much comments! To answer to the most asked questions :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;I will have some constraints if I go the api way, as the providers will need to serve models from the country in EU where I am preferably or at very least be a company from EU and have compute servers IN EU. (Clients need guarantees that their datas stay in this country), and I only have 2 main providers for that that I identified, with open source models not really up to date. (No glm, no deepseek, qwen 235b but with limited rates ..) So no OpenAI, Anthropic, .. (Even not Mistral like I first thought as they might send the computes outside EU in some cases for api calls) It also might mean that we could be easily in troubles if the two providers have issues. (Not a lot of redundancy)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;I understand now that I might have been wrong in my calculations concerning api tokens cost vs cost of hardware, thanks to all! Very insightful. I haven't thought of comparing actual token costs vs max throughput expected per day with possible hardware confs. This was exactly what I needed to see!&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;To clarify, usage won't be for coding, but mainly for multi agents human assistant for everyday tasks. So I'm not expecting context per user to be too large and was planning to have a hard limit around 50k tokens for in context.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;For the hardware, I imagined that it could be hosted in a datacenter, so security outside of classical server conf (fail2ban, firewall, etc) would be taken care by the DC where I will host my hardware.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So now I understand that price wise, indeed, I was completely wrong, but still make sense in a privacy, control, model choice, ways. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Chimchimai"&gt; /u/Chimchimai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ped5p2/at_what_point_does_owning_gpus_become_cheaper/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ped5p2/at_what_point_does_owning_gpus_become_cheaper/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ped5p2/at_what_point_does_owning_gpus_become_cheaper/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T21:58:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdzn2n</id>
    <title>legends</title>
    <updated>2025-12-04T13:11:47+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdzn2n/legends/"&gt; &lt;img alt="legends" src="https://preview.redd.it/vu26lxrns65g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8a61d2260347cccaa67517ffc3812c121edcd5d0" title="legends" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vu26lxrns65g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdzn2n/legends/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdzn2n/legends/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T13:11:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1pevec3</id>
    <title>Modern RAG Setups</title>
    <updated>2025-12-05T13:43:28+00:00</updated>
    <author>
      <name>/u/SlowFail2433</name>
      <uri>https://old.reddit.com/user/SlowFail2433</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello I don‚Äôt do RAG too much and I fell behind.&lt;/p&gt; &lt;p&gt;What do you think is best in RAG these days?&lt;/p&gt; &lt;p&gt;Which open source RAG repos have you particularly liked recently?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SlowFail2433"&gt; /u/SlowFail2433 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pevec3/modern_rag_setups/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pevec3/modern_rag_setups/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pevec3/modern_rag_setups/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T13:43:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1pebwh6</id>
    <title>[open source] I finetuned my own LLM in 20m on my personal notes. Now it thinks in my style.</title>
    <updated>2025-12-04T21:08:58+00:00</updated>
    <author>
      <name>/u/Robert-treboR</name>
      <uri>https://old.reddit.com/user/Robert-treboR</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pebwh6/open_source_i_finetuned_my_own_llm_in_20m_on_my/"&gt; &lt;img alt="[open source] I finetuned my own LLM in 20m on my personal notes. Now it thinks in my style." src="https://external-preview.redd.it/cnp4eTBhb3U1OTVnMfuPSbsqUMLpJROMWbsiBCXZtzJPCMmpR4Hze4lcXzSH.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=adfad9a694bdb5ac7bebdf3c924e2842afcf2999" title="[open source] I finetuned my own LLM in 20m on my personal notes. Now it thinks in my style." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I keep all of my notes as files in cursor&lt;br /&gt; It took me 20min to finetune/RL my personal DeepSeek model on them&lt;br /&gt; I used tinker API &amp;amp; Lora with Gemini to create train dataset&lt;br /&gt; Now I have a model that literally &lt;strong&gt;THINKS&lt;/strong&gt; like me. made it open source repo + tutorial&lt;/p&gt; &lt;p&gt;Github repo :&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/OneInterface/Finetune-your-notes"&gt;https://github.com/OneInterface/Finetune-your-notes&lt;/a&gt; &lt;/p&gt; &lt;p&gt;I like playing around with data and models. I see some interesting use cases in the industry. Who wants to bounce idea's?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Robert-treboR"&gt; /u/Robert-treboR &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/rnc81tnu595g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pebwh6/open_source_i_finetuned_my_own_llm_in_20m_on_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pebwh6/open_source_i_finetuned_my_own_llm_in_20m_on_my/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T21:08:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1pec8hz</id>
    <title>speed optimizations for Qwen Next on CUDA have been merged into llama.cpp</title>
    <updated>2025-12-04T21:22:04+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/17584"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pec8hz/speed_optimizations_for_qwen_next_on_cuda_have/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pec8hz/speed_optimizations_for_qwen_next_on_cuda_have/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T21:22:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1peql6c</id>
    <title>Mistral 3 14b against the competition ?</title>
    <updated>2025-12-05T09:16:35+00:00</updated>
    <author>
      <name>/u/EffectiveGlove1651</name>
      <uri>https://old.reddit.com/user/EffectiveGlove1651</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;did you tried the new mistral models particularly 14B, and if yes how does it compare to the competition at the same range of parameters (between 10 and 30B) ?&lt;/p&gt; &lt;p&gt;Thanks in advance, &lt;/p&gt; &lt;p&gt;Pierre&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EffectiveGlove1651"&gt; /u/EffectiveGlove1651 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1peql6c/mistral_3_14b_against_the_competition/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1peql6c/mistral_3_14b_against_the_competition/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1peql6c/mistral_3_14b_against_the_competition/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T09:16:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1per0lz</id>
    <title>100% Local AI for VSCode?</title>
    <updated>2025-12-05T09:44:50+00:00</updated>
    <author>
      <name>/u/Baldur-Norddahl</name>
      <uri>https://old.reddit.com/user/Baldur-Norddahl</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am using VS Code with Roo Code and GLM 4.5 Air or GPT-OSS 120b running 100% locally. But there ara bits and pieces of build in AI in VS Code that I can't seem to get rid of. And those things will upload my code to unknown parties, which I definitely don't like.&lt;/p&gt; &lt;p&gt;First is the code completion (Copilot) - this is tied to my Github subscription. How do I replace it with local AI instead?&lt;/p&gt; &lt;p&gt;We also have the autogenerate a git commit message using AI. Can I use a local model instead of whatever it uses by default? Maybe even get more useful messages, because the ones it generates are often quite useless.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Baldur-Norddahl"&gt; /u/Baldur-Norddahl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1per0lz/100_local_ai_for_vscode/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1per0lz/100_local_ai_for_vscode/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1per0lz/100_local_ai_for_vscode/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T09:44:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1pext5t</id>
    <title>Why did GLM stop creating smaller models?</title>
    <updated>2025-12-05T15:21:58+00:00</updated>
    <author>
      <name>/u/AI-Man-75</name>
      <uri>https://old.reddit.com/user/AI-Man-75</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GLM 30B 3B MoE would be really great.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AI-Man-75"&gt; /u/AI-Man-75 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pext5t/why_did_glm_stop_creating_smaller_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pext5t/why_did_glm_stop_creating_smaller_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pext5t/why_did_glm_stop_creating_smaller_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T15:21:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1pez5ch</id>
    <title>Why do LLM response formats often use &lt;| |&gt; (as in &lt;|message|&gt;) instead of &lt;message&gt;, and why do they use &lt;|end|&gt; instead of &lt;/message&gt;?</title>
    <updated>2025-12-05T16:14:03+00:00</updated>
    <author>
      <name>/u/Amazydayzee</name>
      <uri>https://old.reddit.com/user/Amazydayzee</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pez5ch/why_do_llm_response_formats_often_use_as_in/"&gt; &lt;img alt="Why do LLM response formats often use &amp;lt;| |&amp;gt; (as in &amp;lt;|message|&amp;gt;) instead of &amp;lt;message&amp;gt;, and why do they use &amp;lt;|end|&amp;gt; instead of &amp;lt;/message&amp;gt;?" src="https://preview.redd.it/5e5ir2zlte5g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d7b47f1bd3dabcdabf34fcf757aaea013f0a0c73" title="Why do LLM response formats often use &amp;lt;| |&amp;gt; (as in &amp;lt;|message|&amp;gt;) instead of &amp;lt;message&amp;gt;, and why do they use &amp;lt;|end|&amp;gt; instead of &amp;lt;/message&amp;gt;?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If I had to guess, I'd assume it's tokenization because &amp;quot;&amp;lt;|&amp;quot; is not a very commonly occurring pattern in pre-training, which allows devs to make &amp;quot;&amp;lt;|message|&amp;gt;&amp;quot; a single token.&lt;/p&gt; &lt;p&gt;That being said, the &amp;lt;|end|&amp;gt; is still a bit disorienting, at least to me reading as a human. You can see that the &amp;lt;|start|&amp;gt; block ends with another &amp;lt;|start|&amp;gt; block, but the &amp;lt;|message|&amp;gt; block ends in a &amp;lt;|end|&amp;gt; block.&lt;/p&gt; &lt;p&gt;This image is from &lt;a href="https://github.com/openai/harmony"&gt;openai's harmony response template&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Amazydayzee"&gt; /u/Amazydayzee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5e5ir2zlte5g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pez5ch/why_do_llm_response_formats_often_use_as_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pez5ch/why_do_llm_response_formats_often_use_as_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T16:14:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1pei0q3</id>
    <title>Mistral 3 Large 675B up on huggingface</title>
    <updated>2025-12-05T01:27:34+00:00</updated>
    <author>
      <name>/u/someone383726</name>
      <uri>https://old.reddit.com/user/someone383726</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyone got 1.35TB of VRAM I could borrow?&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mistralai/Mistral-Large-3-675B-Instruct-2512-BF16"&gt;https://huggingface.co/mistralai/Mistral-Large-3-675B-Instruct-2512-BF16&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/someone383726"&gt; /u/someone383726 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pei0q3/mistral_3_large_675b_up_on_huggingface/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pei0q3/mistral_3_large_675b_up_on_huggingface/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pei0q3/mistral_3_large_675b_up_on_huggingface/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T01:27:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1pepwnn</id>
    <title>Qwen3-Next-80B-A3B or Gpt-oss-120b?</title>
    <updated>2025-12-05T08:32:39+00:00</updated>
    <author>
      <name>/u/custodiam99</name>
      <uri>https://old.reddit.com/user/custodiam99</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I mainly used Gpt-oss-120b (High reasoning) in the last months (summarizing, knowledge search, complex reasoning) and it proved very useful. Apart from being censored heavily (sometimes in a quite irrational way) it is a wonderful model. But I was excited to try the new Qwen model. So I downloaded Qwen3-Next-80B-A3B q6 (Thinking and Instruct) - and &lt;strong&gt;&lt;em&gt;I wasn't impressed&lt;/em&gt;&lt;/strong&gt;. It does not seem to be any better, in fact it seems less intelligent. Am I wrong? Let's talk about it!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/custodiam99"&gt; /u/custodiam99 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pepwnn/qwen3next80ba3b_or_gptoss120b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pepwnn/qwen3next80ba3b_or_gptoss120b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pepwnn/qwen3next80ba3b_or_gptoss120b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T08:32:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1pezh1k</id>
    <title>Blood and stardust! Watch 9 local LLMs debate Star Wars vs Star Trek</title>
    <updated>2025-12-05T16:26:16+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pezh1k/blood_and_stardust_watch_9_local_llms_debate_star/"&gt; &lt;img alt="Blood and stardust! Watch 9 local LLMs debate Star Wars vs Star Trek" src="https://external-preview.redd.it/aTRpOTR6dzBzZTVnMSe6y4zOHIyGUsL1YtaqMqowYCso8PTyfwm1haQrI9uz.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=63024def9b5244ed1828e41a7d4ab09eeb725073" title="Blood and stardust! Watch 9 local LLMs debate Star Wars vs Star Trek" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The last post was too much fun, so here we go again.&lt;/p&gt; &lt;p&gt;Debate Arena v2 adds the top suggestions from last time:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;NO MORE TIES&lt;/strong&gt; for &lt;a href="/u/NodeTraverser"&gt;u/NodeTraverser&lt;/a&gt;, the 9th model guarantees one side wins&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Smooth setup&lt;/strong&gt; for &lt;a href="/u/Vercinthia"&gt;u/Vercinthia&lt;/a&gt; and &lt;a href="/u/work__reddit"&gt;u/work__reddit&lt;/a&gt;, the web app helps you install, start the backend, and download models&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Scoreboard&lt;/strong&gt; for &lt;a href="/u/Zissuo"&gt;u/Zissuo&lt;/a&gt;, know which LLMs betrayed your ideals&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Enhanced debating&lt;/strong&gt; for &lt;a href="/u/r4in311"&gt;u/r4in311&lt;/a&gt; and &lt;a href="/u/slolobdill44"&gt;u/slolobdill44&lt;/a&gt;, 5 debate stages with their own purpose and system prompt&lt;/li&gt; &lt;/ul&gt; &lt;blockquote&gt; &lt;pre&gt;&lt;code&gt; üé§ Phase 1: Hot Takes üí¨ Phase 2: Reactions üçø Phase 3: The Plot Thickens üéØ Phase 4: Final Thoughts &amp;amp; Voting ‚ö° Phase 5: Lightning Round - Vote Now &lt;/code&gt;&lt;/pre&gt; &lt;/blockquote&gt; &lt;p&gt;Details and quick start instructions are &lt;a href="https://github.com/lemonade-sdk/lemonade/blob/main/examples/demos/debate-arena.md"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Have I taken this too far, or not far enough? Tell me your burning yes/no questions and feature suggestions and I might do a v3 next week!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/t6y4gtw0se5g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pezh1k/blood_and_stardust_watch_9_local_llms_debate_star/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pezh1k/blood_and_stardust_watch_9_local_llms_debate_star/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T16:26:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1peqbu0</id>
    <title>Local LLMs were supposed to simplify my life‚Ä¶ now I need a guide for my guides</title>
    <updated>2025-12-05T09:00:09+00:00</updated>
    <author>
      <name>/u/Fab_Terminator</name>
      <uri>https://old.reddit.com/user/Fab_Terminator</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I installed Ollama ‚Äújust to try it.‚Äù Then I discovered text-generation-webui. Then I discovered LM Studio. Then I discovered quantizations‚Ä¶ rope scaling‚Ä¶ vocab merging‚Ä¶ GPU offloading‚Ä¶&lt;/p&gt; &lt;p&gt;Now I'm 30 hours deep into tweaking settings so I can ask my computer, ‚ÄúWhat should I cook today?‚Äù&lt;/p&gt; &lt;p&gt;Does anyone else feel like local AI is the new homelab rabbit hole?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fab_Terminator"&gt; /u/Fab_Terminator &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1peqbu0/local_llms_were_supposed_to_simplify_my_life_now/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1peqbu0/local_llms_were_supposed_to_simplify_my_life_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1peqbu0/local_llms_were_supposed_to_simplify_my_life_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T09:00:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1peuh30</id>
    <title>https://livebench.ai - Open Weight Models Only</title>
    <updated>2025-12-05T13:01:22+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1peuh30/httpslivebenchai_open_weight_models_only/"&gt; &lt;img alt="https://livebench.ai - Open Weight Models Only" src="https://preview.redd.it/ohayhhgivd5g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=21252697c2c953c1d038980ffc92cd091416be50" title="https://livebench.ai - Open Weight Models Only" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There were some questions about how Qwen 3 Next compares to GPT-OSS. I think whole table may be useful. What do you think about this ordering?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ohayhhgivd5g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1peuh30/httpslivebenchai_open_weight_models_only/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1peuh30/httpslivebenchai_open_weight_models_only/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T13:01:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1pexnfp</id>
    <title>LongCat-Image: 6B model with strong efficiency, photorealism, and Chinese text rendering</title>
    <updated>2025-12-05T15:15:46+00:00</updated>
    <author>
      <name>/u/abdouhlili</name>
      <uri>https://old.reddit.com/user/abdouhlili</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pexnfp/longcatimage_6b_model_with_strong_efficiency/"&gt; &lt;img alt="LongCat-Image: 6B model with strong efficiency, photorealism, and Chinese text rendering" src="https://external-preview.redd.it/wKVXYkAgQd2YCzTWH9wJHT9a9O4yMSOT8w5RQDj-cGQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=266d079c79f26252dc4def3cc7e476d0209bb0af" title="LongCat-Image: 6B model with strong efficiency, photorealism, and Chinese text rendering" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdouhlili"&gt; /u/abdouhlili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/meituan-longcat/LongCat-Image"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pexnfp/longcatimage_6b_model_with_strong_efficiency/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pexnfp/longcatimage_6b_model_with_strong_efficiency/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T15:15:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1pes3pu</id>
    <title>Basketball AI with RF-DETR, SAM2, and SmolVLM2</title>
    <updated>2025-12-05T10:53:12+00:00</updated>
    <author>
      <name>/u/RandomForests92</name>
      <uri>https://old.reddit.com/user/RandomForests92</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pes3pu/basketball_ai_with_rfdetr_sam2_and_smolvlm2/"&gt; &lt;img alt="Basketball AI with RF-DETR, SAM2, and SmolVLM2" src="https://external-preview.redd.it/N2czYjlxanU4ZDVnMZ78lEX-DYraHupkrsvdafpxwsSm-SfqaN6z7l9OZr1B.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aab29ff74cd044468cb8bd288eeaf647b5329d32" title="Basketball AI with RF-DETR, SAM2, and SmolVLM2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;resources: &lt;a href="https://www.youtube.com/watch?v=yGQb9KkvQ1Q"&gt;youtube&lt;/a&gt;, &lt;a href="https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/basketball-ai-how-to-detect-track-and-identify-basketball-players.ipynb"&gt;code&lt;/a&gt;, &lt;a href="https://blog.roboflow.com/identify-basketball-players"&gt;blog&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- player and number detection with RF-DETR&lt;/p&gt; &lt;p&gt;- player tracking with SAM2&lt;/p&gt; &lt;p&gt;- team clustering with SigLIP, UMAP and K-Means&lt;/p&gt; &lt;p&gt;- number recognition with SmolVLM2&lt;/p&gt; &lt;p&gt;- perspective conversion with homography&lt;/p&gt; &lt;p&gt;- player trajectory correction&lt;/p&gt; &lt;p&gt;- shot detection and classification&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RandomForests92"&gt; /u/RandomForests92 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/k6kmogju8d5g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pes3pu/basketball_ai_with_rfdetr_sam2_and_smolvlm2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pes3pu/basketball_ai_with_rfdetr_sam2_and_smolvlm2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T10:53:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
