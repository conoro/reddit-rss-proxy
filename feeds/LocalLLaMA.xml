<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-21T11:48:50+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1p2vwu6</id>
    <title>gemma 2</title>
    <updated>2025-11-21T11:11:50+00:00</updated>
    <author>
      <name>/u/Head-Effective-4061</name>
      <uri>https://old.reddit.com/user/Head-Effective-4061</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i am currently working on a project and trying to make a chatbot, i am using gemma 2 since its free and offline... i have not fine tuned the model yet... what are the major things i should take into account for getting precise and accurate responses in case of making extractions from the user and asking relevant questions based on the answers....&lt;/p&gt; &lt;p&gt;any one kindly guide me through&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Head-Effective-4061"&gt; /u/Head-Effective-4061 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2vwu6/gemma_2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2vwu6/gemma_2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2vwu6/gemma_2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T11:11:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1u9gv</id>
    <title>Spark Cluster!</title>
    <updated>2025-11-20T04:47:00+00:00</updated>
    <author>
      <name>/u/SashaUsesReddit</name>
      <uri>https://old.reddit.com/user/SashaUsesReddit</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1u9gv/spark_cluster/"&gt; &lt;img alt="Spark Cluster!" src="https://preview.redd.it/zmr4gy3ydc2g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7f25d102d17380204b2d6175e9e34708025777a7" title="Spark Cluster!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Doing dev and expanded my spark desk setup to eight!&lt;/p&gt; &lt;p&gt;Anyone have anything fun they want to see run on this HW?&lt;/p&gt; &lt;p&gt;Im not using the sparks for max performance, I'm using them for nccl/nvidia dev to deploy to B300 clusters. Really great platform to do small dev before deploying on large HW&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SashaUsesReddit"&gt; /u/SashaUsesReddit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zmr4gy3ydc2g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1u9gv/spark_cluster/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p1u9gv/spark_cluster/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-20T04:47:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2jbbh</id>
    <title>A small fine-tune of Gemma 3 4B focused on translation and text transformation</title>
    <updated>2025-11-20T23:54:38+00:00</updated>
    <author>
      <name>/u/thecalmgreen</name>
      <uri>https://old.reddit.com/user/thecalmgreen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2jbbh/a_small_finetune_of_gemma_3_4b_focused_on/"&gt; &lt;img alt="A small fine-tune of Gemma 3 4B focused on translation and text transformation" src="https://b.thumbs.redditmedia.com/7uF4A2T7aup6cy5bCwGhHqsKQIOm2Gk2wRf156mMjEE.jpg" title="A small fine-tune of Gemma 3 4B focused on translation and text transformation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/fczv34fm2i2g1.png?width=1233&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=db04767eb2aa0701d4e7a0f33a124b859cfc8cab"&gt;Gemma 3 4B Polyglot v1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hey everyone,&lt;br /&gt; I’ve been working a lot with local models for translation, rewriting and quick text adjustments. Gemma 3 4B is already great, but I wanted something a bit more predictable, a bit more &amp;quot;fluid&amp;quot; and especially something that behaves really well inside my daily workflow.&lt;/p&gt; &lt;p&gt;Because of that, I fine-tuned a version that integrates beautifully with Polyglot Air. It was literally made for that. If you use Polyglot Air to translate selected text, correct grammar, switch tone or summarize with suffix-style commands, this model tends to respond in a cleaner and more consistent way.&lt;/p&gt; &lt;p&gt;It is not a big project. Just a simple fine-tune I made because I wanted smoother translation and rewriting with a local model. Since it improved my workflow, I’m sharing it here in case someone else finds it useful.&lt;/p&gt; &lt;h1&gt;What this fine-tune improves&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;More natural and accurate translations&lt;/li&gt; &lt;li&gt;Better consistency for grammar correction&lt;/li&gt; &lt;li&gt;Smoother tone shifts and rewriting&lt;/li&gt; &lt;li&gt;More stable behavior with suffix-based text transformations&lt;/li&gt; &lt;li&gt;Lightweight and friendly to run locally&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Model Weights&lt;/h1&gt; &lt;p&gt;HF repo:&lt;br /&gt; &lt;a href="https://huggingface.co/CalmState/gemma-3-4b-polyglot-v1"&gt;https://huggingface.co/CalmState/gemma-3-4b-polyglot-v1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GGUF build:&lt;br /&gt; &lt;a href="https://huggingface.co/CalmState/gemma-3-4b-polyglot-v1-Q8_0-GGUF"&gt;https://huggingface.co/CalmState/gemma-3-4b-polyglot-v1-Q8_0-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you try it, I’d love to hear any feedback.&lt;br /&gt; Hope it helps make someone's workflow a little calmer and smoother. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thecalmgreen"&gt; /u/thecalmgreen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2jbbh/a_small_finetune_of_gemma_3_4b_focused_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2jbbh/a_small_finetune_of_gemma_3_4b_focused_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2jbbh/a_small_finetune_of_gemma_3_4b_focused_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-20T23:54:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2n3f8</id>
    <title>Read long podcasts locally with Whisper + LLM, open sourced</title>
    <updated>2025-11-21T02:45:08+00:00</updated>
    <author>
      <name>/u/tonyc1118</name>
      <uri>https://old.reddit.com/user/tonyc1118</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The high quality podcasts and YT interviews are getting longer and longer these days, like Lex Fridman, Acquired, No Priors... It's often hard to find time to finish them. So I built a website that summarized our favorite podcasts into 3-5 minute insight sheets with key quotes. (Yes I tried other existing products. Their outputs are often vague GPT style and miss real key points)&lt;/p&gt; &lt;p&gt;Recently I added local inference and open sourced our project. So if you have a Mac with an M chip, you can run whisper+LLM on your machine. &lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/tonyc-ship/latios-insights"&gt;https://github.com/tonyc-ship/latios-insights&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It’s still early, and I’d love feedback. Happy to answer setup questions too.&lt;/p&gt; &lt;p&gt;Some future features on my mind:&lt;br /&gt; - setup a public database so that all open source users can share what have been summarized &lt;/p&gt; &lt;p&gt;- build a vector DB so that you can search and ask across what you have read&lt;/p&gt; &lt;p&gt;- stretch but what I really loved: make it a mobile app and run the local inference on your phone&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tonyc1118"&gt; /u/tonyc1118 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2n3f8/read_long_podcasts_locally_with_whisper_llm_open/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2n3f8/read_long_podcasts_locally_with_whisper_llm_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2n3f8/read_long_podcasts_locally_with_whisper_llm_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T02:45:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2q65r</id>
    <title>Struggling to find a working model within my hardware. RTX5070ti/64gb ram</title>
    <updated>2025-11-21T05:16:24+00:00</updated>
    <author>
      <name>/u/krisco65</name>
      <uri>https://old.reddit.com/user/krisco65</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello friends. Been at it for a week now trying to find a model that is uncensored/abliterated for my local ai project on my desktop. &lt;/p&gt; &lt;p&gt;Ive been using claude sonnet on the web as my guide/prompt constructor and claude code in terminal on my pc to do everything.&lt;/p&gt; &lt;p&gt;PC is:&lt;br /&gt; ryzen 7 9700x&lt;br /&gt; RTX 5070ti (16gb)&lt;br /&gt; 64gb 4800 RAM&lt;/p&gt; &lt;p&gt;I started at the 70b models that supposedly would run on my gpu with offloading to my RAM and cpu, then dropped down in quant as far as possible. None of the models would run.&lt;/p&gt; &lt;p&gt;then started down the rabbit hole of 34s, 30s, 24s etc etc.&lt;/p&gt; &lt;p&gt;I can obviously run anything around 7b/8b like the generic mistral and dolphins etc. However they barely touch 8gb of my 16gb available.&lt;/p&gt; &lt;p&gt;Is there a solution I just dont know of? Im not looking for anything above 40t/s as long as its smart and reasons well, however uncensored is a MUST and i need it to be willing ti give advice on dosages of things like TRT etc.&lt;/p&gt; &lt;p&gt;Thanks in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/krisco65"&gt; /u/krisco65 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2q65r/struggling_to_find_a_working_model_within_my/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2q65r/struggling_to_find_a_working_model_within_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2q65r/struggling_to_find_a_working_model_within_my/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T05:16:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2lxqx</id>
    <title>watercooled server adventures</title>
    <updated>2025-11-21T01:51:20+00:00</updated>
    <author>
      <name>/u/j4ys0nj</name>
      <uri>https://old.reddit.com/user/j4ys0nj</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2lxqx/watercooled_server_adventures/"&gt; &lt;img alt="watercooled server adventures" src="https://b.thumbs.redditmedia.com/yLDRamdBMvfAbRCl11YkKSpSfpMdEpCLlgnmSd52qYQ.jpg" title="watercooled server adventures" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When I set out on this journey, it was not a journey, but now it is.&lt;/p&gt; &lt;p&gt;All I did was buy some cheap waterblocks for the pair of RTX A4500s I had at the time. I did already have a bunch of other GPUs... and now they will feel the cool flow of water over their chips as well.&lt;/p&gt; &lt;p&gt;How do you add watercooling to a server with 2x 5090s and an RTX PRO? Initially I thought 2x or 3x 360mm (120x3) radiators would do it. 3 might, but at full load for a few days... might not. My chassis can fit 2x 360mm rads, but 3.. I'd have to get creative.. or get a new chassis. Fine.&lt;/p&gt; &lt;p&gt;Then I had an idea. I knew Koolance made some external water cooling units.. but they were all out of stock, and cost more than I wanted to pay. &lt;/p&gt; &lt;p&gt;Maybe you see where this has taken me now..&lt;/p&gt; &lt;p&gt;An old 2U chassis, 2x 360mm rads and one.. I don't know what they call these.. 120x9 radiator, lots of EPDM tubing, more quick connects than I wanted to buy, pumps, fans, this aquaero 6 thing to control it all.. that might actually be old stock from like 10 years ago, some supports printed out of carbon fiber nylon and entirely too many G1/4 connectors. Still not sure how I'm going to power it, but I think an old 1U PSU can work.&lt;/p&gt; &lt;p&gt;Also - shout out to Bykski for making cool shit.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.bykski.us/products/bykski-durable-metal-pom-gpu-water-block-and-backplate-for-nvidia-rtx-pro-6000-blackwell-server-edition-n-rtxpro6000-sr-continuous-usage"&gt;RTX PRO 6000 SE Waterblock&lt;/a&gt;&lt;br /&gt; &lt;a href="https://www.bykski.us/products/bykski-durable-metal-pom-gpu-water-block-and-backplate-for-nvidia-geforce-rtx-5090-founders-edition-continuous-usage"&gt;RTX 5090 FE Waterblock&lt;/a&gt;&lt;br /&gt; &lt;a href="https://www.bykski.us/collections/nine-fan-1080mm/products/bykski-1080mm-x-46mm-rd-series-radiator-120mm-x-9-nine-fan-b-rd1080tk-v2"&gt;This big radiator&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I've since grabbed 2 more A4500s with waterblocks, so we'll be looking at 8x watercooled GPUs in the end. Which is about 3200W total. This setup can probably handle 3500W, or thereabouts. It's obviously not done yet.. but solid progress. Once I figure out the power supply thing and where to mount it, I might be good to go.&lt;/p&gt; &lt;p&gt;What you think? Where did I go wrong? How did I end up here... &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/lihi6ofwli2g1.png?width=1127&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b4954aea0a0dd4fbcb6920922412026b0fb4bc13"&gt;quick connects for all of the GPUs + CPU!&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9kwzfaizli2g1.png?width=1123&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7aead302957c715faccb67bf889d9051a371f019"&gt;dry fit, no water in it yet&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/vbb04jf7mi2g1.png?width=1124&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=00692e72d6b0f63d664f8144b065f2d7b4f50d1f"&gt;fill port on the side&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/aicymrnhmi2g1.png?width=1119&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=05649f52b1d08dfbc8fefcbeb19e39a8f4fa1148"&gt;temporary solution for the CPU. 140x60mm rad.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/rnsbohbbni2g1.png?width=1121&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9732c8dd5621e91bb4fd18c3ee05b248d74906df"&gt;Other box with a watercooled 4090. 140x60mm rad mounted on the back, 120x60mm up front. Actually works really well. Everything stays cool, believe it or not.&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/j4ys0nj"&gt; /u/j4ys0nj &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2lxqx/watercooled_server_adventures/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2lxqx/watercooled_server_adventures/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2lxqx/watercooled_server_adventures/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T01:51:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1p21385</id>
    <title>GigaChat3-702B-A36B-preview is now available on Hugging Face</title>
    <updated>2025-11-20T11:46:44+00:00</updated>
    <author>
      <name>/u/Any-Ship9886</name>
      <uri>https://old.reddit.com/user/Any-Ship9886</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sber AI has released GigaChat3-702B-A36B-preview, a massive 702B parameter model with active 36B parameters using MoE architecture. There are versions in fp8 and bf16. This is one of the largest openly available Russian LLMs to date.&lt;/p&gt; &lt;p&gt;Key specifications:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;702B total parameters with 36B active per token&lt;/li&gt; &lt;li&gt;128K context window&lt;/li&gt; &lt;li&gt;Supports Russian, English, and code generation&lt;/li&gt; &lt;li&gt;Released under MIT license&lt;/li&gt; &lt;li&gt;Trained on diverse Russian and multilingual datasets&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The model uses Mixture of Experts routing, making it feasible to run despite the enormous parameter count. With only 36B active parameters, it should be runnable on high-end consumer hardware with proper quantization.&lt;/p&gt; &lt;p&gt;Performance benchmarks show competitive results on Russian language tasks, though international benchmark scores are still being evaluated. Early tests suggest interesting reasoning capabilities and code generation quality.&lt;/p&gt; &lt;p&gt;Model card: &lt;a href="https://huggingface.co/ai-sage/GigaChat3-702B-A36B-preview"&gt;https://huggingface.co/ai-sage/GigaChat3-702B-A36B-preview&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any-Ship9886"&gt; /u/Any-Ship9886 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p21385/gigachat3702ba36bpreview_is_now_available_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p21385/gigachat3702ba36bpreview_is_now_available_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p21385/gigachat3702ba36bpreview_is_now_available_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-20T11:46:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1p24d2c</id>
    <title>Olmo3</title>
    <updated>2025-11-20T14:20:04+00:00</updated>
    <author>
      <name>/u/BreakfastFriendly728</name>
      <uri>https://old.reddit.com/user/BreakfastFriendly728</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p24d2c/olmo3/"&gt; &lt;img alt="Olmo3" src="https://b.thumbs.redditmedia.com/UJppPEN0RZP8y3BZ6uMmEsmjApLD6fufweSjic6DGkY.jpg" title="Olmo3" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ai2 released a series of new olmo 3 weights, including Olmo-3-32B-Think, along with data, code for training and evalution.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/allenai/olmo-3"&gt;https://huggingface.co/collections/allenai/olmo-3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/i98wyfc8bf2g1.png?width=2220&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d52ee933127ce4b88e2e94330d10f9d58bc1b5bb"&gt;https://preview.redd.it/i98wyfc8bf2g1.png?width=2220&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d52ee933127ce4b88e2e94330d10f9d58bc1b5bb&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BreakfastFriendly728"&gt; /u/BreakfastFriendly728 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p24d2c/olmo3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p24d2c/olmo3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p24d2c/olmo3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-20T14:20:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2whr9</id>
    <title>RTX 3090 vs RX 7900 with ROCm, also Vulcan</title>
    <updated>2025-11-21T11:44:55+00:00</updated>
    <author>
      <name>/u/lfiction</name>
      <uri>https://old.reddit.com/user/lfiction</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Haven’t had time to try out Vulcan, really excited for it after everything I’ve been hearing!&lt;/p&gt; &lt;p&gt;Probably going to pick up a couple 3090s or 7900s, curious to know what folks’ experience has been using Radeon cards with ROCm and/or Vulcan?&lt;/p&gt; &lt;p&gt;Also does brand matter, Zotac / ASRock / ASUS / Gigabyte etc? &lt;/p&gt; &lt;p&gt;Do folks here roll the dice with refurbished, or buy new? &lt;/p&gt; &lt;p&gt;I will be using Linux, most likely Ubuntu. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lfiction"&gt; /u/lfiction &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2whr9/rtx_3090_vs_rx_7900_with_rocm_also_vulcan/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2whr9/rtx_3090_vs_rx_7900_with_rocm_also_vulcan/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2whr9/rtx_3090_vs_rx_7900_with_rocm_also_vulcan/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T11:44:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2utxz</id>
    <title>Dual RTX 5090 FE - Case Suggestion help request</title>
    <updated>2025-11-21T10:06:00+00:00</updated>
    <author>
      <name>/u/Firepin77</name>
      <uri>https://old.reddit.com/user/Firepin77</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Thank you in advance for any help you can give me!&lt;/p&gt; &lt;p&gt;I am contemplating building an (roleplaying mainly, silly tavern) inference and gaming pc.&lt;br /&gt; i think the following are probably good parts:&lt;/p&gt; &lt;p&gt;- AMD Ryzen 7 9800X3D&lt;br /&gt; - Noctua NH-D15 &lt;a href="http://chromax.black"&gt;chromax.black&lt;/a&gt;&lt;br /&gt; - ASUS ProArt X870E-Creator WiFi&lt;br /&gt; - Seasonic Prime TX TX-1600W&lt;br /&gt; - 2x Nvidia RTX 5090 FE (Founders Edition)&lt;/p&gt; &lt;p&gt;so price of case is not much of a concern, because this would be a longterm powerhouse and could change parts, case is more critical because it would be the &amp;quot;house&amp;quot; where the changing parts &amp;quot;live&amp;quot;.&lt;/p&gt; &lt;p&gt;Bad is probably the 5090 FE because it blows the air from the lower card to the top card in the fe edition.&lt;br /&gt; I could choose other 5090 cards but they seem to be absurdly priced (3000 €? could get a rtx 6000 pro almost for that price?) or big monolithic 3-4 slots gpus like the MSI Suprim rtx 5090.&lt;/p&gt; &lt;p&gt;- Any suggestions for cases that comfortably fit and cool good two rtx 5090 (fe or good alternative aibs?) (good build quality?)&lt;br /&gt; - any real alternative to the rtx 5090 FE to fit two cards in one case? (not rtx 6000 pro just 5090)&lt;/p&gt; &lt;p&gt;- no watercooling only aircooling.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Firepin77"&gt; /u/Firepin77 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2utxz/dual_rtx_5090_fe_case_suggestion_help_request/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2utxz/dual_rtx_5090_fe_case_suggestion_help_request/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2utxz/dual_rtx_5090_fe_case_suggestion_help_request/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T10:06:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2lqi7</id>
    <title>Are any of the M series mac macbooks and mac minis, worth saving up for?</title>
    <updated>2025-11-21T01:41:53+00:00</updated>
    <author>
      <name>/u/No_Strawberry_8719</name>
      <uri>https://old.reddit.com/user/No_Strawberry_8719</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Like for ai locally and general tasks, are the mac m series worth the hype or are there better ways to run ai locally?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Strawberry_8719"&gt; /u/No_Strawberry_8719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2lqi7/are_any_of_the_m_series_mac_macbooks_and_mac/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2lqi7/are_any_of_the_m_series_mac_macbooks_and_mac/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2lqi7/are_any_of_the_m_series_mac_macbooks_and_mac/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T01:41:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2v5ap</id>
    <title>Epstein Files Document Embeddings (768D, Nomic)</title>
    <updated>2025-11-21T10:25:10+00:00</updated>
    <author>
      <name>/u/qwer1627</name>
      <uri>https://old.reddit.com/user/qwer1627</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Text embeddings generated from the House Oversight Committee's Epstein document release. (768D, Nomic)&lt;/p&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/datasets/svetfm/epstein-files-nov11-25-house-post-ocr-embeddings#source-dataset"&gt;&lt;/a&gt;Source Dataset&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;This dataset is derived from:&lt;/strong&gt; &lt;a href="https://huggingface.co/datasets/tensonaut/EPSTEIN_FILES_20K"&gt;tensonaut/EPSTEIN_FILES_20K&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The source dataset contains OCR'd text from the original House Oversight Committee PDF release.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/datasets/svetfm/epstein-files-nov11-25-house-post-ocr-embeddings"&gt;https://huggingface.co/datasets/svetfm/epstein-files-nov11-25-house-post-ocr-embeddings&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/qwer1627"&gt; /u/qwer1627 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2v5ap/epstein_files_document_embeddings_768d_nomic/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2v5ap/epstein_files_document_embeddings_768d_nomic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2v5ap/epstein_files_document_embeddings_768d_nomic/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T10:25:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2q5e5</id>
    <title>Small language models for agentic use?</title>
    <updated>2025-11-21T05:15:18+00:00</updated>
    <author>
      <name>/u/Swimming-Ratio4879</name>
      <uri>https://old.reddit.com/user/Swimming-Ratio4879</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I saw some posts about VibeThinker, which is a math model,and I thought why not create a multi-agent system that's literally made of Small language models that has a shared context or can communicate to pass context, like user said 1+1,the router route it to VibeThinker and another model, let's call it AgenticThinker,both models run in parallel based on the router's request,the challenge here would be to keep the answer coherent because the model will be filled with fine-tuning data and for that size it will just start doing calculations because you said &amp;quot;hi&amp;quot; that can be solved because we can use a small model that parallize the context between 2 and generate answer while both are communicating via context or generating the answer,that would lower ram usage (it's not an MoE that must be fully loaded) as it will activate from disk-to-vram based on demand,we can even speed it up by using ram to store the models if the router &amp;quot;thought&amp;quot; that it needs to pick a specific model based on the conversation flow,that would truly allow people with low vram to have multiple models and each activated automatically based on task!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Swimming-Ratio4879"&gt; /u/Swimming-Ratio4879 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2q5e5/small_language_models_for_agentic_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2q5e5/small_language_models_for_agentic_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2q5e5/small_language_models_for_agentic_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T05:15:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1p274rk</id>
    <title>Your local LLM agents can be just as good as closed-source models - I open-sourced Stanford's ACE framework that makes agents learn from mistakes</title>
    <updated>2025-11-20T16:09:43+00:00</updated>
    <author>
      <name>/u/cheetguy</name>
      <uri>https://old.reddit.com/user/cheetguy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I implemented Stanford's &lt;a href="https://arxiv.org/abs/2510.04618"&gt;Agentic Context Engineering paper&lt;/a&gt;. The framework makes agents learn from their own execution feedback through in-context learning instead of fine-tuning.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How it works:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Agent runs task → reflects on what worked/failed → curates strategies into playbook → uses playbook on next run&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Improvement:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Paper shows +17.1pp accuracy improvement vs base LLM (≈+40% relative improvement) on agent benchmarks (DeepSeek-V3.1 non-thinking mode), helping close the gap with closed-source models. All through in-context learning (no fine-tuning needed).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My Open-Source Implementation:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Drop into existing agents in ~10 lines of code&lt;/li&gt; &lt;li&gt;Works with local or API models&lt;/li&gt; &lt;li&gt;Real-world test on browser automation agent: &lt;ul&gt; &lt;li&gt;30% → 100% success rate&lt;/li&gt; &lt;li&gt;82% fewer steps&lt;/li&gt; &lt;li&gt;65% decrease in token cost&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Get started:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GitHub: &lt;a href="https://github.com/kayba-ai/agentic-context-engine"&gt;https://github.com/kayba-ai/agentic-context-engine&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Local Model Starter Templates (Ollama, LM Studio, LiteLLM): &lt;a href="https://github.com/kayba-ai/agentic-context-engine/tree/main/examples"&gt;https://github.com/kayba-ai/agentic-context-engine/tree/main/examples&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Would love to hear if anyone tries this with their local setups! Especially curious how it performs with different models.&lt;/p&gt; &lt;p&gt;I'm currently actively improving this based on feedback - &lt;a href="https://github.com/kayba-ai/agentic-context-engine"&gt;⭐ the repo&lt;/a&gt; so you can stay updated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cheetguy"&gt; /u/cheetguy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p274rk/your_local_llm_agents_can_be_just_as_good_as/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p274rk/your_local_llm_agents_can_be_just_as_good_as/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p274rk/your_local_llm_agents_can_be_just_as_good_as/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-20T16:09:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2ncrr</id>
    <title>ubergarm/GigaChat3-10B-A1.8B-GGUF ~11GiB Q8_0</title>
    <updated>2025-11-21T02:57:30+00:00</updated>
    <author>
      <name>/u/VoidAlchemy</name>
      <uri>https://old.reddit.com/user/VoidAlchemy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2ncrr/ubergarmgigachat310ba18bgguf_11gib_q8_0/"&gt; &lt;img alt="ubergarm/GigaChat3-10B-A1.8B-GGUF ~11GiB Q8_0" src="https://external-preview.redd.it/rsMYYo-PBl_LaTyTfnfLp1CLd1qQqrGp0cpYiQ-K3U0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cbdff364a576d27ab13892b08d6753835c752f2b" title="ubergarm/GigaChat3-10B-A1.8B-GGUF ~11GiB Q8_0" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Needs a PR to get running for llama.cpp: * &lt;a href="https://github.com/ggml-org/llama.cpp/pull/17420"&gt;https://github.com/ggml-org/llama.cpp/pull/17420&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Issue open for ik_llama.cpp folks: * &lt;a href="https://github.com/ikawrakow/ik_llama.cpp/issues/994"&gt;https://github.com/ikawrakow/ik_llama.cpp/issues/994&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The chat template is missing a docstring out of the middle that wasn't parsing correctly. So you might be able to bring your own chat template using the instructions on the model card and if someone replies here: * &lt;a href="https://huggingface.co/ai-sage/GigaChat3-702B-A36B-preview-bf16/discussions/1"&gt;https://huggingface.co/ai-sage/GigaChat3-702B-A36B-preview-bf16/discussions/1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Though DevQuasar mentioned having a fixed template for the bigger 702B here: * &lt;a href="https://huggingface.co/DevQuasar/ai-sage.GigaChat3-702B-A36B-preview-bf16-GGUF/discussions/1"&gt;https://huggingface.co/DevQuasar/ai-sage.GigaChat3-702B-A36B-preview-bf16-GGUF/discussions/1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VoidAlchemy"&gt; /u/VoidAlchemy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/ubergarm/GigaChat3-10B-A1.8B-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2ncrr/ubergarmgigachat310ba18bgguf_11gib_q8_0/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2ncrr/ubergarmgigachat310ba18bgguf_11gib_q8_0/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T02:57:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2ivv2</id>
    <title>Faster NeuTTS: can generate over 200 seconds of audio in a single second!</title>
    <updated>2025-11-20T23:35:16+00:00</updated>
    <author>
      <name>/u/SplitNice1982</name>
      <uri>https://old.reddit.com/user/SplitNice1982</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I previously open sourced FastMaya which was also really fast but then set sights on NeuTTS-air. NeuTTS is much smaller and supports better voice cloning as well. So, I heavily optimized it using LMdeploy and some custom batching code for the codec to make it really fast.&lt;/p&gt; &lt;h1&gt;Benefits of this repo&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Much faster, not only for batching but for single batch sizes(1.8x realtime for Maya1 vs 7x realtime for NeuTTS-air)&lt;/li&gt; &lt;li&gt;Works with multiple gpus using tensor parallel for even more speedups. &lt;/li&gt; &lt;li&gt;Great for not only generating audiobooks but voice assistants and much more&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I am working on supporting the multilingual models as well and adding multi speaker synthesis. Also, streaming support and online inference (for serving to many users) should come as well. Initial results are showing **100ms** latency!&lt;/p&gt; &lt;p&gt;I will also add an upsampler to increase audio quality soon. If you have other requests, I will try my best to fulfill them.&lt;/p&gt; &lt;p&gt;Hope this helps people, thanks! Link: &lt;a href="https://github.com/ysharma3501/FastNeuTTS.git"&gt;https://github.com/ysharma3501/FastNeuTTS.git&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SplitNice1982"&gt; /u/SplitNice1982 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2ivv2/faster_neutts_can_generate_over_200_seconds_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2ivv2/faster_neutts_can_generate_over_200_seconds_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2ivv2/faster_neutts_can_generate_over_200_seconds_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-20T23:35:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1p29jwc</id>
    <title>Leak: Qwen3-15B-A2B-Base</title>
    <updated>2025-11-20T17:40:12+00:00</updated>
    <author>
      <name>/u/TroyDoesAI</name>
      <uri>https://old.reddit.com/user/TroyDoesAI</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Unmolested and Unreleased Base Qwen3 MoE:&lt;br /&gt; &lt;a href="https://huggingface.co/TroyDoesAI/Qwen3-15B-A2B-Base"&gt;https://huggingface.co/TroyDoesAI/Qwen3-15B-A2B-Base&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TroyDoesAI"&gt; /u/TroyDoesAI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p29jwc/leak_qwen315ba2bbase/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p29jwc/leak_qwen315ba2bbase/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p29jwc/leak_qwen315ba2bbase/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-20T17:40:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2w5i6</id>
    <title>HunyuanVideo-1.5: A leading lightweight video generation model</title>
    <updated>2025-11-21T11:25:33+00:00</updated>
    <author>
      <name>/u/abdouhlili</name>
      <uri>https://old.reddit.com/user/abdouhlili</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/tencent/HunyuanVideo-1.5"&gt;https://huggingface.co/tencent/HunyuanVideo-1.5&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdouhlili"&gt; /u/abdouhlili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2w5i6/hunyuanvideo15_a_leading_lightweight_video/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2w5i6/hunyuanvideo15_a_leading_lightweight_video/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2w5i6/hunyuanvideo15_a_leading_lightweight_video/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T11:25:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1p24aet</id>
    <title>Ai2 just announced Olmo 3, a leading fully open LM suite built for reasoning, chat, &amp; tool use</title>
    <updated>2025-11-20T14:16:57+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p24aet/ai2_just_announced_olmo_3_a_leading_fully_open_lm/"&gt; &lt;img alt="Ai2 just announced Olmo 3, a leading fully open LM suite built for reasoning, chat, &amp;amp; tool use" src="https://b.thumbs.redditmedia.com/xQA2jdAbLxju3aNrYQFjsQ9bmSXaOmQiXK2aPKXj8vw.jpg" title="Ai2 just announced Olmo 3, a leading fully open LM suite built for reasoning, chat, &amp;amp; tool use" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Try Olmo 3 in the Ai2 Playground → &lt;a href="https://playground.allenai.org/"&gt;https://playground.allenai.org/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Download: &lt;a href="https://huggingface.co/collections/allenai/olmo-3-68e80f043cc0d3c867e7efc6"&gt;https://huggingface.co/collections/allenai/olmo-3-68e80f043cc0d3c867e7efc6&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog: &lt;a href="https://allenai.org/blog/olmo3"&gt;https://allenai.org/blog/olmo3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Technical report: &lt;a href="https://allenai.org/papers/olmo3"&gt;https://allenai.org/papers/olmo3&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1p24aet"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p24aet/ai2_just_announced_olmo_3_a_leading_fully_open_lm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p24aet/ai2_just_announced_olmo_3_a_leading_fully_open_lm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-20T14:16:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2v0fe</id>
    <title>Deep Cogito v2.1, a new open weights 671B MoE model</title>
    <updated>2025-11-21T10:16:47+00:00</updated>
    <author>
      <name>/u/BreakfastFriendly728</name>
      <uri>https://old.reddit.com/user/BreakfastFriendly728</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2v0fe/deep_cogito_v21_a_new_open_weights_671b_moe_model/"&gt; &lt;img alt="Deep Cogito v2.1, a new open weights 671B MoE model" src="https://b.thumbs.redditmedia.com/ZFcSSCHd3EryXfkTwWHW8bx8DkNYSKpulONBXzIgxiQ.jpg" title="Deep Cogito v2.1, a new open weights 671B MoE model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/collections/deepcogito/cogito-v21"&gt;https://huggingface.co/collections/deepcogito/cogito-v21&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/wgqv3iva5l2g1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7b23a040098d2ed9caa81a6a322d02e18d51cc0e"&gt;https://preview.redd.it/wgqv3iva5l2g1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7b23a040098d2ed9caa81a6a322d02e18d51cc0e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/4rfhao3d5l2g1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=82dd4fcc80106c78f950f6516116123dad2f1b49"&gt;https://preview.redd.it/4rfhao3d5l2g1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=82dd4fcc80106c78f950f6516116123dad2f1b49&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/l88vmsue5l2g1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=da35111b441df51d43d4d5f04be4fb289b029525"&gt;https://preview.redd.it/l88vmsue5l2g1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=da35111b441df51d43d4d5f04be4fb289b029525&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BreakfastFriendly728"&gt; /u/BreakfastFriendly728 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2v0fe/deep_cogito_v21_a_new_open_weights_671b_moe_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2v0fe/deep_cogito_v21_a_new_open_weights_671b_moe_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2v0fe/deep_cogito_v21_a_new_open_weights_671b_moe_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T10:16:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2l36u</id>
    <title>Echo TTS - 44.1kHz, Fast, Fits under 8GB VRAM - SoTA Voice Cloning</title>
    <updated>2025-11-21T01:11:55+00:00</updated>
    <author>
      <name>/u/HelpfulHand3</name>
      <uri>https://old.reddit.com/user/HelpfulHand3</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;New diffusion based multi-speaker capable TTS model released today by the engineer who made Parakeet (the arch that Dia was based on).&lt;br /&gt; &lt;strong&gt;Voice cloning is available on the&lt;/strong&gt; &lt;a href="https://huggingface.co/spaces/jordand/echo-tts-preview"&gt;HF space&lt;/a&gt; but for safety reasons (voice similarity with this model is very high) he has decided for now not to release the speaker encoder. It does come with a large voice bank however.&lt;/p&gt; &lt;p&gt;Supports some tags like (laughs), (coughs), (applause), (singing) etc.&lt;/p&gt; &lt;p&gt;Runs on consumer cards with at least 8GB VRAM.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Echo is a 2.4B DiT that generates Fish Speech S1-DAC latents (and can thus generate 44.1kHz audio; credit to Fish Speech for having trained such a great autoencoder). On an A100, Echo can generate a single 30-second sample of audio in 1.4 seconds (including decoding).&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;License: &lt;strong&gt;CC-BY-NC due to the S1 DAC autoencoder license&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Release Blog Post: &lt;a href="https://jordandarefsky.com/blog/2025/echo/"&gt;https://jordandarefsky.com/blog/2025/echo/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Demo HF Space: &lt;a href="https://huggingface.co/spaces/jordand/echo-tts-preview"&gt;https://huggingface.co/spaces/jordand/echo-tts-preview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Weights: &lt;a href="https://huggingface.co/jordand/echo-tts-no-speaker"&gt;https://huggingface.co/jordand/echo-tts-no-speaker&lt;/a&gt; &lt;a href="https://huggingface.co/jordand/fish-s1-dac-min"&gt;https://huggingface.co/jordand/fish-s1-dac-min&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Code/Github: Coming soon&lt;/p&gt; &lt;p&gt;I haven't had this much fun playing with a TTS since Higgs. This is easily up there with VibeVoice 7b and Higgs Audio v2 despite being 2.4b.&lt;/p&gt; &lt;p&gt;It can clone voices that no other model has been able to do well for me:&lt;/p&gt; &lt;p&gt;&lt;a href="https://vocaroo.com/19PQroylYsoP"&gt;https://vocaroo.com/19PQroylYsoP&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HelpfulHand3"&gt; /u/HelpfulHand3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2l36u/echo_tts_441khz_fast_fits_under_8gb_vram_sota/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2l36u/echo_tts_441khz_fast_fits_under_8gb_vram_sota/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2l36u/echo_tts_441khz_fast_fits_under_8gb_vram_sota/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T01:11:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2padh</id>
    <title>Unsloth just released their Olmo 3 dynamic quants!</title>
    <updated>2025-11-21T04:28:41+00:00</updated>
    <author>
      <name>/u/Aromatic-Distance817</name>
      <uri>https://old.reddit.com/user/Aromatic-Distance817</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2padh/unsloth_just_released_their_olmo_3_dynamic_quants/"&gt; &lt;img alt="Unsloth just released their Olmo 3 dynamic quants!" src="https://external-preview.redd.it/48d9roHWO9vPhqtCoIVGxdhD9jO5DC8s9h8U3EqHoCc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2c1dca215120c6d942685f73783d2b00bbdb86e8" title="Unsloth just released their Olmo 3 dynamic quants!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aromatic-Distance817"&gt; /u/Aromatic-Distance817 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/unsloth/Olmo-3-32B-Think-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2padh/unsloth_just_released_their_olmo_3_dynamic_quants/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2padh/unsloth_just_released_their_olmo_3_dynamic_quants/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T04:28:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2vjym</id>
    <title>As to why I use ChatGPT even though I have a Gemini subscription:</title>
    <updated>2025-11-21T10:50:26+00:00</updated>
    <author>
      <name>/u/EmirTanis</name>
      <uri>https://old.reddit.com/user/EmirTanis</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2vjym/as_to_why_i_use_chatgpt_even_though_i_have_a/"&gt; &lt;img alt="As to why I use ChatGPT even though I have a Gemini subscription:" src="https://preview.redd.it/0l4pu8bzal2g1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f58974ff373664eac33931baeb94cebb9c1644fb" title="As to why I use ChatGPT even though I have a Gemini subscription:" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EmirTanis"&gt; /u/EmirTanis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0l4pu8bzal2g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2vjym/as_to_why_i_use_chatgpt_even_though_i_have_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2vjym/as_to_why_i_use_chatgpt_even_though_i_have_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T10:50:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1ozopxx</id>
    <title>AMA Announcement: MiniMax, The Opensource Lab Behind MiniMax-M2 + Gifts to Our Community (Wednesday, 8AM-11AM PST)</title>
    <updated>2025-11-17T18:50:44+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozopxx/ama_announcement_minimax_the_opensource_lab/"&gt; &lt;img alt="AMA Announcement: MiniMax, The Opensource Lab Behind MiniMax-M2 + Gifts to Our Community (Wednesday, 8AM-11AM PST)" src="https://preview.redd.it/e3scgr4j5v1g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ca0bf85cb4cf2a2b7e12e8d99d515186275c15e3" title="AMA Announcement: MiniMax, The Opensource Lab Behind MiniMax-M2 + Gifts to Our Community (Wednesday, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/e3scgr4j5v1g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozopxx/ama_announcement_minimax_the_opensource_lab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ozopxx/ama_announcement_minimax_the_opensource_lab/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-17T18:50:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1b550</id>
    <title>AMA with MiniMax — Ask Us Anything!</title>
    <updated>2025-11-19T15:46:38+00:00</updated>
    <author>
      <name>/u/OccasionNo6699</name>
      <uri>https://old.reddit.com/user/OccasionNo6699</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;! We’re really excited to be here, thanks for having us.&lt;/p&gt; &lt;p&gt;I’m Skyler (&lt;a href="https://www.reddit.com/user/OccasionNo6699/"&gt;u/OccasionNo6699&lt;/a&gt;), head of engineering at MiniMax, the lab behind:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://x.com/MiniMax__AI/status/1982674798649160175?s=20"&gt;MiniMax-M2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983382728343994414"&gt;Hailuo 2.3&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983661667872600296"&gt;MiniMax Speech 2.6&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983964920493568296"&gt;MiniMax Music 2.0&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Joining me today are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Pengyu Zhao, &lt;a href="https://www.reddit.com/user/Wise_Evidence9973/"&gt;u/Wise_Evidence9973&lt;/a&gt; — Head of LLM Research&lt;/li&gt; &lt;li&gt;Jade Cai, &lt;a href="https://www.reddit.com/user/srtng/"&gt;u/srtng&lt;/a&gt; — Head of Developer Community&lt;/li&gt; &lt;li&gt;midnight_compile , &lt;a href="/u/Top_Cattle_2098"&gt;u/Top_Cattle_2098&lt;/a&gt; — LLM Researcher&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 8AM-11AM PST with our core MiniMax tech team continuing to follow up on questions over the next 48 hours.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OccasionNo6699"&gt; /u/OccasionNo6699 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T15:46:38+00:00</published>
  </entry>
</feed>
