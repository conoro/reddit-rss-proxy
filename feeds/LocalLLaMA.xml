<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-06T09:07:11+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1pfk8aw</id>
    <title>New paper: True AI autonomy isn't about bigger models - it's the 4 pillars of cognition</title>
    <updated>2025-12-06T08:21:48+00:00</updated>
    <author>
      <name>/u/web3nomad</name>
      <uri>https://old.reddit.com/user/web3nomad</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Perception, Reasoning, Memory, Action - not just scaling parameters. Interesting framework from this new paper on autonomous agents.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/web3nomad"&gt; /u/web3nomad &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/heyrimsha/status/1997209742251643088"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfk8aw/new_paper_true_ai_autonomy_isnt_about_bigger/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfk8aw/new_paper_true_ai_autonomy_isnt_about_bigger/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T08:21:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfh06s</id>
    <title>A 5-second MLP beat my Llama-3 fine-tune (+2.7% across 3 seeds). Benchmarks + repo.</title>
    <updated>2025-12-06T05:07:42+00:00</updated>
    <author>
      <name>/u/anima-core</name>
      <uri>https://old.reddit.com/user/anima-core</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been exploring how much task-relevant structure is already present in frozen transformer representations, and I finally decided to package a reproducible slice of that work into a public repo.&lt;/p&gt; &lt;p&gt;This isn‚Äôt my full system or the architecture I‚Äôve been developing privately. It‚Äôs just a clean baseline that anyone can run. The goal was to make it easy for people to independently verify the pattern I‚Äôve been seeing for a while.&lt;/p&gt; &lt;p&gt;The setup is simple:&lt;/p&gt; &lt;p&gt;‚Ä¢ fine-tune a frozen transformer on SST-2 or MNLI ‚Ä¢ capture hidden states from a few layers during that run ‚Ä¢ pool them into feature vectors ‚Ä¢ train a small MLP on those frozen vectors&lt;/p&gt; &lt;p&gt;No distillation, no extra transformer passes, no architectural claims. Just a representation probe.&lt;/p&gt; &lt;p&gt;Across seeds and models, the results were surprisingly consistent. On SST-2, a small classifier trained on the frozen representations beat my Llama-3-8B fine-tune by +2.67 percent on average across three seeds. Training took about five to sixty seconds depending on hidden size. GPT-Neo models showed the same pattern, and I even saw comparable behavior on MNLI with a weaker teacher.&lt;/p&gt; &lt;p&gt;Repo with code, logs, and scripts: &lt;a href="https://github.com/Anima-Core/an1-meaning-field"&gt;https://github.com/Anima-Core/an1-meaning-field&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is not a claim about a new model or a transformer replacement. It‚Äôs simply a baseline measurement, a small part of a broader direction I‚Äôm working on privately. But the consistency of the pattern made it worth sharing.&lt;/p&gt; &lt;p&gt;If you try it, I‚Äôd be curious whether you see the same behavior.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/anima-core"&gt; /u/anima-core &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfh06s/a_5second_mlp_beat_my_llama3_finetune_27_across_3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfh06s/a_5second_mlp_beat_my_llama3_finetune_27_across_3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfh06s/a_5second_mlp_beat_my_llama3_finetune_27_across_3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T05:07:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1pf6jcj</id>
    <title>For those of you with ai max+ 395 mini pc that have experience or no bias hate with mac computers: Would you recommend a max 395+ to someone where it currently or are you thinking of switching to or back to mac?</title>
    <updated>2025-12-05T21:00:00+00:00</updated>
    <author>
      <name>/u/Smart_Frosting9846</name>
      <uri>https://old.reddit.com/user/Smart_Frosting9846</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am starting to feel with these insane prices the only logical option for reliability, peace of mind, and a plug and play experience a mac studio would be my best bet. I am wanting to use 70B models. Just looking for a computer to last me atleast the next 2 years. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Smart_Frosting9846"&gt; /u/Smart_Frosting9846 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pf6jcj/for_those_of_you_with_ai_max_395_mini_pc_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pf6jcj/for_those_of_you_with_ai_max_395_mini_pc_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pf6jcj/for_those_of_you_with_ai_max_395_mini_pc_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T21:00:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1peqbu0</id>
    <title>Local LLMs were supposed to simplify my life‚Ä¶ now I need a guide for my guides</title>
    <updated>2025-12-05T09:00:09+00:00</updated>
    <author>
      <name>/u/Fab_Terminator</name>
      <uri>https://old.reddit.com/user/Fab_Terminator</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I installed Ollama ‚Äújust to try it.‚Äù Then I discovered text-generation-webui. Then I discovered LM Studio. Then I discovered quantizations‚Ä¶ rope scaling‚Ä¶ vocab merging‚Ä¶ GPU offloading‚Ä¶&lt;/p&gt; &lt;p&gt;Now I'm 30 hours deep into tweaking settings so I can ask my computer, ‚ÄúWhat should I cook today?‚Äù&lt;/p&gt; &lt;p&gt;Does anyone else feel like local AI is the new homelab rabbit hole?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fab_Terminator"&gt; /u/Fab_Terminator &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1peqbu0/local_llms_were_supposed_to_simplify_my_life_now/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1peqbu0/local_llms_were_supposed_to_simplify_my_life_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1peqbu0/local_llms_were_supposed_to_simplify_my_life_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T09:00:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfjvry</id>
    <title>Why so few benchmarks with the pcie p2p patches kernel module?</title>
    <updated>2025-12-06T07:59:49+00:00</updated>
    <author>
      <name>/u/unfortunate_jargon</name>
      <uri>https://old.reddit.com/user/unfortunate_jargon</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've seen a lot of inference benchmarks on here, but I'm consistently baffled why it seems that nearly no one is using the various patched Nvidia kernel modules available which enabled pcie p2p.&lt;/p&gt; &lt;p&gt;It reduces the latency between RTX 30/40/50 cards by an order of magnitude, and makes tensor and export parallelism highly viable (leading to _drastically_ improved throughput)&lt;/p&gt; &lt;p&gt;Is this common knowledge around here? If not, then I highly encourage doing some testing with your multi-RTX GPU systems, because running without it is handicapping your performance by multiples.&lt;/p&gt; &lt;p&gt;edit: tinycorp was the first author I'm aware of that released a patch that was widely circulated, but others have forked and improved it, as well as rebasing against newer versions of the kernel module. here's an example I just pulled from chatgpt: &lt;a href="https://github.com/aikitoria/open-gpu-kernel-modules"&gt;https://github.com/aikitoria/open-gpu-kernel-modules&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/unfortunate_jargon"&gt; /u/unfortunate_jargon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfjvry/why_so_few_benchmarks_with_the_pcie_p2p_patches/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfjvry/why_so_few_benchmarks_with_the_pcie_p2p_patches/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfjvry/why_so_few_benchmarks_with_the_pcie_p2p_patches/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T07:59:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfk26q</id>
    <title>How can I make Gemma-3 4B better at generating a specific language?</title>
    <updated>2025-12-06T08:10:59+00:00</updated>
    <author>
      <name>/u/NoAdhesiveness7595</name>
      <uri>https://old.reddit.com/user/NoAdhesiveness7595</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm experimenting with the Gemma-3 4B model and I want it to be more fluent/accurate in a specific language (not English). What‚Äôs the best way to improve its output?&lt;br /&gt; Should I fine-tune it, use DPO, add prompts, or something else?&lt;br /&gt; Looking for practical steps, tools, or examples.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NoAdhesiveness7595"&gt; /u/NoAdhesiveness7595 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfk26q/how_can_i_make_gemma3_4b_better_at_generating_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfk26q/how_can_i_make_gemma3_4b_better_at_generating_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfk26q/how_can_i_make_gemma3_4b_better_at_generating_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T08:10:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1pf3ai8</id>
    <title>Llama 405B is worse than Gemma 3 12B?</title>
    <updated>2025-12-05T18:51:31+00:00</updated>
    <author>
      <name>/u/Express_Seesaw_8418</name>
      <uri>https://old.reddit.com/user/Express_Seesaw_8418</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pf3ai8/llama_405b_is_worse_than_gemma_3_12b/"&gt; &lt;img alt="Llama 405B is worse than Gemma 3 12B?" src="https://b.thumbs.redditmedia.com/IlhYPt_IftUhVM_ROE1INTbRT5_7veltkfuDGvYdCjE.jpg" title="Llama 405B is worse than Gemma 3 12B?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/h6ligvsujf5g1.png?width=1228&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5b6c9e928a752c5b4dfce1b2eda868d27f425a3a"&gt;https://preview.redd.it/h6ligvsujf5g1.png?width=1228&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5b6c9e928a752c5b4dfce1b2eda868d27f425a3a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I was browsing LMArena and discovered that Llama 405B ranked lower than many smaller models (gemma-3-12b-it, Qwen3-30B-A3B-Instruct-2507, mistral-small-2506).&lt;/p&gt; &lt;p&gt;I assumed the leaderboard isn't perfect but to me this seems crazy and I'm curious what the deal is. Am I wrong for assuming LMArena is roughly accurate?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Express_Seesaw_8418"&gt; /u/Express_Seesaw_8418 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pf3ai8/llama_405b_is_worse_than_gemma_3_12b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pf3ai8/llama_405b_is_worse_than_gemma_3_12b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pf3ai8/llama_405b_is_worse_than_gemma_3_12b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T18:51:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1pf4w4e</id>
    <title>how are you supposed to pronounce the name Qwen?</title>
    <updated>2025-12-05T19:53:54+00:00</updated>
    <author>
      <name>/u/ridablellama</name>
      <uri>https://old.reddit.com/user/ridablellama</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just saw Jensen pronounce is like Que-When on youtube. I have been saying it more like Quen in my head....Claude says this: &amp;quot;Qwen&amp;quot; is pronounced like &lt;strong&gt;&amp;quot;chwen&amp;quot;&lt;/strong&gt; (rhymes with &amp;quot;when&amp;quot;), with the &amp;quot;Q&amp;quot; making a &amp;quot;ch&amp;quot; sound as in Mandarin Chinese pinyin.&lt;/p&gt; &lt;p&gt;Pretty sure no one on youtube says it like that. Can anyone with some Chinese language experience please step in and give us the real deal!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ridablellama"&gt; /u/ridablellama &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pf4w4e/how_are_you_supposed_to_pronounce_the_name_qwen/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pf4w4e/how_are_you_supposed_to_pronounce_the_name_qwen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pf4w4e/how_are_you_supposed_to_pronounce_the_name_qwen/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T19:53:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1pf4nuf</id>
    <title>Are models creators choosing to not do QAT?</title>
    <updated>2025-12-05T19:44:55+00:00</updated>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;QAT is fairly cheap process compared to full training,why are so many companies publishing their models in full precision without investing in QAT? And I'm not saying that &amp;quot;just publish 4-bit weights and leave it&amp;quot; it's VERY CHEAP to serve both FP16 and FP4/INT4 weights on HuggingFace,it will practically cost the company nothing additional compared to the full training run.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pf4nuf/are_models_creators_choosing_to_not_do_qat/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pf4nuf/are_models_creators_choosing_to_not_do_qat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pf4nuf/are_models_creators_choosing_to_not_do_qat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T19:44:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1peuh30</id>
    <title>https://livebench.ai - Open Weight Models Only</title>
    <updated>2025-12-05T13:01:22+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1peuh30/httpslivebenchai_open_weight_models_only/"&gt; &lt;img alt="https://livebench.ai - Open Weight Models Only" src="https://preview.redd.it/ohayhhgivd5g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=21252697c2c953c1d038980ffc92cd091416be50" title="https://livebench.ai - Open Weight Models Only" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There were some questions about how Qwen 3 Next compares to GPT-OSS. I think whole table may be useful. What do you think about this ordering?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ohayhhgivd5g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1peuh30/httpslivebenchai_open_weight_models_only/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1peuh30/httpslivebenchai_open_weight_models_only/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T13:01:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfjg34</id>
    <title>How is the agent system inside Cursor (or similar IDE agent workflows) actually designed?</title>
    <updated>2025-12-06T07:32:08+00:00</updated>
    <author>
      <name>/u/v0k3r</name>
      <uri>https://old.reddit.com/user/v0k3r</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm trying to understand how modern AI-powered IDEs like Cursor structure their internal agent systems.&lt;/p&gt; &lt;p&gt;From the outside, it looks like the tool is able to:&lt;br /&gt; ‚Äì break a user request into multiple steps,&lt;br /&gt; ‚Äì apply patches to the codebase,&lt;br /&gt; ‚Äì run commands (install deps, start dev server),&lt;br /&gt; ‚Äì detect errors,&lt;br /&gt; ‚Äì and then automatically fix them in a loop.&lt;/p&gt; &lt;p&gt;is it?&lt;/p&gt; &lt;ul&gt; &lt;li&gt;a chain of multiple agents calling each other,&lt;/li&gt; &lt;li&gt;a single agent with tool-calling and a feedback loop,&lt;/li&gt; &lt;li&gt;or some kind of planner‚Äìexecutor architecture?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;How do they coordinate step-by-step tasks?&lt;br /&gt; Is there a public technical breakdown of how this ‚Äúagentic IDE‚Äù architecture works?&lt;/p&gt; &lt;p&gt;I‚Äôd really appreciate a detailed explanation or any deep-dive resources.&lt;/p&gt; &lt;p&gt;Maybe links or explanation here&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/v0k3r"&gt; /u/v0k3r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfjg34/how_is_the_agent_system_inside_cursor_or_similar/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfjg34/how_is_the_agent_system_inside_cursor_or_similar/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfjg34/how_is_the_agent_system_inside_cursor_or_similar/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T07:32:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1pezh1k</id>
    <title>Blood and stardust! Watch 9 local LLMs debate Star Wars vs Star Trek</title>
    <updated>2025-12-05T16:26:16+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pezh1k/blood_and_stardust_watch_9_local_llms_debate_star/"&gt; &lt;img alt="Blood and stardust! Watch 9 local LLMs debate Star Wars vs Star Trek" src="https://external-preview.redd.it/aTRpOTR6dzBzZTVnMSe6y4zOHIyGUsL1YtaqMqowYCso8PTyfwm1haQrI9uz.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=63024def9b5244ed1828e41a7d4ab09eeb725073" title="Blood and stardust! Watch 9 local LLMs debate Star Wars vs Star Trek" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The last post was too much fun, so here we go again.&lt;/p&gt; &lt;p&gt;Debate Arena v2 adds the top suggestions from last time:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;NO MORE TIES&lt;/strong&gt; for &lt;a href="/u/NodeTraverser"&gt;u/NodeTraverser&lt;/a&gt;, the 9th model guarantees one side wins&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Smooth setup&lt;/strong&gt; for &lt;a href="/u/Vercinthia"&gt;u/Vercinthia&lt;/a&gt; and &lt;a href="/u/work__reddit"&gt;u/work__reddit&lt;/a&gt;, the web app helps you install, start the backend, and download models&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Scoreboard&lt;/strong&gt; for &lt;a href="/u/Zissuo"&gt;u/Zissuo&lt;/a&gt;, know which LLMs betrayed your ideals&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Enhanced debating&lt;/strong&gt; for &lt;a href="/u/r4in311"&gt;u/r4in311&lt;/a&gt; and &lt;a href="/u/slolobdill44"&gt;u/slolobdill44&lt;/a&gt;, 5 debate stages with their own purpose and system prompt&lt;/li&gt; &lt;/ul&gt; &lt;blockquote&gt; &lt;pre&gt;&lt;code&gt; üé§ Phase 1: Hot Takes üí¨ Phase 2: Reactions üçø Phase 3: The Plot Thickens üéØ Phase 4: Final Thoughts &amp;amp; Voting ‚ö° Phase 5: Lightning Round - Vote Now &lt;/code&gt;&lt;/pre&gt; &lt;/blockquote&gt; &lt;p&gt;Details and quick start instructions are &lt;a href="https://github.com/lemonade-sdk/lemonade/blob/main/examples/demos/debate-arena.md"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Have I taken this too far, or not far enough? Tell me your burning yes/no questions and feature suggestions and I might do a v3 next week!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/t6y4gtw0se5g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pezh1k/blood_and_stardust_watch_9_local_llms_debate_star/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pezh1k/blood_and_stardust_watch_9_local_llms_debate_star/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T16:26:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfggfl</id>
    <title>How do I report to my PhD Supervisor about the performance of Nvidia Jetson Thor for LLM related projects (Reward Model Training, Finetuning, Inference and VLLM related projects)? We are trying to move to local training and inference.</title>
    <updated>2025-12-06T04:37:57+00:00</updated>
    <author>
      <name>/u/Furiousguy79</name>
      <uri>https://old.reddit.com/user/Furiousguy79</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My professor bought an Nvidia Jetson Thor for our lab's dire need for hardware (we were previously using AWS Research Credits, which allowed us to use A100 and similar GPUs for free for some time, but it has expired). They have tasked me to test it so that we can return it if necessary. My workload is mainly about reward model training using GRPO/PPO for Reinforcement Learning based finetuning. I also have a pipeline where I had to simultaneously load three 3B models on the GPU. Other lab members are working on VLLM and stuff like that.&lt;/p&gt; &lt;p&gt;So, how viable is Jetson Thor for this type of stuff? Previous posts have mentioned that it is very slow and the Mac Studio is better or multiple 3090s since Thor has worse bandwidth. But how do I show him the performance? Or if it is a not good choice for our lab, what are good alternatives (except cloud solutions like AWS, Runpod)? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Furiousguy79"&gt; /u/Furiousguy79 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfggfl/how_do_i_report_to_my_phd_supervisor_about_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfggfl/how_do_i_report_to_my_phd_supervisor_about_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfggfl/how_do_i_report_to_my_phd_supervisor_about_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T04:37:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1pf92li</id>
    <title>Best model in the 8B range for RAG in 2025</title>
    <updated>2025-12-05T22:43:49+00:00</updated>
    <author>
      <name>/u/Hour-Entertainer-478</name>
      <uri>https://old.reddit.com/user/Hour-Entertainer-478</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What are your personal favourite (self hosted) model(s) in the &lt;strong&gt;8B&lt;/strong&gt; range for &lt;strong&gt;RAG&lt;/strong&gt; ?&lt;/p&gt; &lt;p&gt;I'm creating a &lt;strong&gt;RAG system&lt;/strong&gt; for a university project, and ideally i want a model that:&lt;br /&gt; * Hallucinates less and refuses to answer, if it doesn't find relevant information in it's context. If it finds partial info, then only answer with that partial piece of info found. won't fill in gaps with general knowledge. I want it strictly based on context.&lt;/p&gt; &lt;p&gt;* Follows instruction well, would do as asked.&lt;/p&gt; &lt;p&gt;* Can find info buried in chunks, and stitch info together to generate an answer. Not hallucinate stuff, but just put 2 and 2 together (instead of expecting a direct call out), make sense of the info, and answer the question.&lt;/p&gt; &lt;p&gt;* Fit in the &lt;strong&gt;&amp;lt;9B&lt;/strong&gt; range and run on a gpu with roughly &lt;strong&gt;8-10 gb vram&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;I'll also share what i've found so far:&lt;br /&gt; * I've found &lt;strong&gt;gemma3:12b-it-qat&lt;/strong&gt; as the best model, that fulfils my criteria well. But the problem it's not in my range, and i run out of memory issues. I'm pretty constrained here unfortunately.&lt;/p&gt; &lt;p&gt;* Reading lots of people speak highly of &lt;strong&gt;qwen3:4b-instruct-2507&lt;/strong&gt; here on reddit, i tried it, but didn't quite like it's ability to synthesise / stitch pieces of info together to answer. It's good at following instruction, and not making shit up generally but It would kinda expect a direct / callout . I tried lots of different prompts, but it was either the model refusing to answer, if it wasn't directly mentioned, or it would make shit up and use info from general knowledge, something that wasn't part of context. It was good instruction following.&lt;/p&gt; &lt;p&gt;* I also tried &lt;strong&gt;qwen3:8b&lt;/strong&gt; , it was good at stiching pieces of info together, but it would just make a lot of shit up instead of refusing to answer. fill in those missing gaps with either it's general knowledge or made up info.&lt;/p&gt; &lt;p&gt;* I also &lt;strong&gt;llama 3.2:8b&lt;/strong&gt; quantised, but it didn't follow instructions well.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My current setup:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;qwen3:4b-instruct-2507-q4 for deciding whether to call the tool + rephrase the user queries. &lt;/p&gt; &lt;p&gt;gemma3:12b-it-qat for generating response (this is where i need recommendations)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What I want ?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;If you have developed a RAG solution with ollama models, please do write the model you found working well for your use case. I'm feel overwhelmed and kinda lost here, kinda feeling like an idiot, since i tried lots of models, and all of them seem to do some part of the job, not all. I know there are bigger models out there, that'd do the exact job pretty well, but i hope with all these developements, there must be some model in my range that would get the job done well.&lt;/p&gt; &lt;p&gt;It would be a huge help, to have your insights/recommendations if you've come across a similar problem. I'd highly welcome any comment, answer, suggestion.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Big thanks&lt;/strong&gt; in advance ‚ù§Ô∏è.&lt;/p&gt; &lt;p&gt;Edit: If you dont know the answer, but would love to find out, please upvote so that it gets a better reach :) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hour-Entertainer-478"&gt; /u/Hour-Entertainer-478 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pf92li/best_model_in_the_8b_range_for_rag_in_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pf92li/best_model_in_the_8b_range_for_rag_in_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pf92li/best_model_in_the_8b_range_for_rag_in_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T22:43:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfe5ou</id>
    <title>Open Unified TTS - Turn any TTS into an unlimited-length audio generator</title>
    <updated>2025-12-06T02:38:57+00:00</updated>
    <author>
      <name>/u/SouthernFriedAthiest</name>
      <uri>https://old.reddit.com/user/SouthernFriedAthiest</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Built an open-source TTS proxy that lets you generate unlimited-length audio from local backends without hitting their length limits.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The problem:&lt;/strong&gt; Most local TTS models break after 50-100 words. Voice clones are especially bad - send a paragraph and you get gibberish, cutoffs, or errors.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The solution:&lt;/strong&gt; Smart chunking + crossfade stitching. Text splits at natural sentence boundaries, each chunk generates within model limits, then seamlessly joins with 50ms crossfades. No audible seams.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Demos:&lt;/strong&gt; - &lt;a href="https://github.com/loserbcc/open-unified-tts/blob/main/demo/intro.mp4"&gt;30-second intro&lt;/a&gt; - &lt;a href="https://github.com/loserbcc/open-unified-tts/blob/main/demo/live_demo.mp4"&gt;4-minute live demo&lt;/a&gt; showing it in action&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Features:&lt;/strong&gt; - OpenAI TTS-compatible API (drop-in for OpenWebUI, SillyTavern, etc.) - Per-voice backend routing (send &amp;quot;morgan&amp;quot; to VoxCPM, &amp;quot;narrator&amp;quot; to Kokoro) - Works with any TTS that has an API endpoint&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Tested with:&lt;/strong&gt; Kokoro, VibeVoice, OpenAudio S1-mini, FishTTS, VoxCPM, MiniMax TTS, Chatterbox, Higgs Audio, Kyutai/Moshi&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/loserbcc/open-unified-tts"&gt;https://github.com/loserbcc/open-unified-tts&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Designed with Claude and Z.ai (with me in the passenger seat).&lt;/p&gt; &lt;p&gt;Feedback welcome - what backends should I add adapters for?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SouthernFriedAthiest"&gt; /u/SouthernFriedAthiest &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfe5ou/open_unified_tts_turn_any_tts_into_an/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfe5ou/open_unified_tts_turn_any_tts_into_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfe5ou/open_unified_tts_turn_any_tts_into_an/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T02:38:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1pez5ch</id>
    <title>Why do LLM response formats often use &lt;| |&gt; (as in &lt;|message|&gt;) instead of &lt;message&gt;, and why do they use &lt;|end|&gt; instead of &lt;/message&gt;?</title>
    <updated>2025-12-05T16:14:03+00:00</updated>
    <author>
      <name>/u/Amazydayzee</name>
      <uri>https://old.reddit.com/user/Amazydayzee</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pez5ch/why_do_llm_response_formats_often_use_as_in/"&gt; &lt;img alt="Why do LLM response formats often use &amp;lt;| |&amp;gt; (as in &amp;lt;|message|&amp;gt;) instead of &amp;lt;message&amp;gt;, and why do they use &amp;lt;|end|&amp;gt; instead of &amp;lt;/message&amp;gt;?" src="https://preview.redd.it/5e5ir2zlte5g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d7b47f1bd3dabcdabf34fcf757aaea013f0a0c73" title="Why do LLM response formats often use &amp;lt;| |&amp;gt; (as in &amp;lt;|message|&amp;gt;) instead of &amp;lt;message&amp;gt;, and why do they use &amp;lt;|end|&amp;gt; instead of &amp;lt;/message&amp;gt;?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If I had to guess, I'd assume it's tokenization because &amp;quot;&amp;lt;|&amp;quot; is not a very commonly occurring pattern in pre-training, which allows devs to make &amp;quot;&amp;lt;|message|&amp;gt;&amp;quot; a single token.&lt;/p&gt; &lt;p&gt;That being said, the &amp;lt;|end|&amp;gt; is still a bit disorienting, at least to me reading as a human. You can see that the &amp;lt;|start|&amp;gt; block ends with another &amp;lt;|start|&amp;gt; block, but the &amp;lt;|message|&amp;gt; block ends in a &amp;lt;|end|&amp;gt; block.&lt;/p&gt; &lt;p&gt;This image is from &lt;a href="https://github.com/openai/harmony"&gt;openai's harmony response template&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Amazydayzee"&gt; /u/Amazydayzee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5e5ir2zlte5g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pez5ch/why_do_llm_response_formats_often_use_as_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pez5ch/why_do_llm_response_formats_often_use_as_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T16:14:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfibu4</id>
    <title>What agentic capabilities are you guys using llms for?</title>
    <updated>2025-12-06T06:23:46+00:00</updated>
    <author>
      <name>/u/Odd-Ordinary-5922</name>
      <uri>https://old.reddit.com/user/Odd-Ordinary-5922</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;just curious&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Odd-Ordinary-5922"&gt; /u/Odd-Ordinary-5922 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfibu4/what_agentic_capabilities_are_you_guys_using_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfibu4/what_agentic_capabilities_are_you_guys_using_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfibu4/what_agentic_capabilities_are_you_guys_using_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T06:23:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1pexnfp</id>
    <title>LongCat-Image: 6B model with strong efficiency, photorealism, and Chinese text rendering</title>
    <updated>2025-12-05T15:15:46+00:00</updated>
    <author>
      <name>/u/abdouhlili</name>
      <uri>https://old.reddit.com/user/abdouhlili</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pexnfp/longcatimage_6b_model_with_strong_efficiency/"&gt; &lt;img alt="LongCat-Image: 6B model with strong efficiency, photorealism, and Chinese text rendering" src="https://external-preview.redd.it/wKVXYkAgQd2YCzTWH9wJHT9a9O4yMSOT8w5RQDj-cGQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=266d079c79f26252dc4def3cc7e476d0209bb0af" title="LongCat-Image: 6B model with strong efficiency, photorealism, and Chinese text rendering" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdouhlili"&gt; /u/abdouhlili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/meituan-longcat/LongCat-Image"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pexnfp/longcatimage_6b_model_with_strong_efficiency/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pexnfp/longcatimage_6b_model_with_strong_efficiency/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T15:15:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1pes3pu</id>
    <title>Basketball AI with RF-DETR, SAM2, and SmolVLM2</title>
    <updated>2025-12-05T10:53:12+00:00</updated>
    <author>
      <name>/u/RandomForests92</name>
      <uri>https://old.reddit.com/user/RandomForests92</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pes3pu/basketball_ai_with_rfdetr_sam2_and_smolvlm2/"&gt; &lt;img alt="Basketball AI with RF-DETR, SAM2, and SmolVLM2" src="https://external-preview.redd.it/N2czYjlxanU4ZDVnMZ78lEX-DYraHupkrsvdafpxwsSm-SfqaN6z7l9OZr1B.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aab29ff74cd044468cb8bd288eeaf647b5329d32" title="Basketball AI with RF-DETR, SAM2, and SmolVLM2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;resources: &lt;a href="https://www.youtube.com/watch?v=yGQb9KkvQ1Q"&gt;youtube&lt;/a&gt;, &lt;a href="https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/basketball-ai-how-to-detect-track-and-identify-basketball-players.ipynb"&gt;code&lt;/a&gt;, &lt;a href="https://blog.roboflow.com/identify-basketball-players"&gt;blog&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- player and number detection with RF-DETR&lt;/p&gt; &lt;p&gt;- player tracking with SAM2&lt;/p&gt; &lt;p&gt;- team clustering with SigLIP, UMAP and K-Means&lt;/p&gt; &lt;p&gt;- number recognition with SmolVLM2&lt;/p&gt; &lt;p&gt;- perspective conversion with homography&lt;/p&gt; &lt;p&gt;- player trajectory correction&lt;/p&gt; &lt;p&gt;- shot detection and classification&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RandomForests92"&gt; /u/RandomForests92 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/k6kmogju8d5g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pes3pu/basketball_ai_with_rfdetr_sam2_and_smolvlm2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pes3pu/basketball_ai_with_rfdetr_sam2_and_smolvlm2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T10:53:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfiar0</id>
    <title>Qwen3-TTS</title>
    <updated>2025-12-06T06:21:51+00:00</updated>
    <author>
      <name>/u/Terrible_Scar_9890</name>
      <uri>https://old.reddit.com/user/Terrible_Scar_9890</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/spaces/Qwen/Qwen3-TTS-Demo"&gt;https://huggingface.co/spaces/Qwen/Qwen3-TTS-Demo&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Terrible_Scar_9890"&gt; /u/Terrible_Scar_9890 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfiar0/qwen3tts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfiar0/qwen3tts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfiar0/qwen3tts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T06:21:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfbo6o</id>
    <title>Is there any model truly open, that you can train yourself from zero?</title>
    <updated>2025-12-06T00:38:47+00:00</updated>
    <author>
      <name>/u/puthre</name>
      <uri>https://old.reddit.com/user/puthre</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As per title, is there any open source LLM that comes with all the data it was trained on and all the instructions that you can replicate yourself assuming you have access to the necesary hardware? And if not why not?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/puthre"&gt; /u/puthre &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfbo6o/is_there_any_model_truly_open_that_you_can_train/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfbo6o/is_there_any_model_truly_open_that_you_can_train/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfbo6o/is_there_any_model_truly_open_that_you_can_train/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T00:38:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfcatm</id>
    <title>VoxCPM 1.5B just got released!</title>
    <updated>2025-12-06T01:07:54+00:00</updated>
    <author>
      <name>/u/Hefty_Wolverine_553</name>
      <uri>https://old.reddit.com/user/Hefty_Wolverine_553</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfcatm/voxcpm_15b_just_got_released/"&gt; &lt;img alt="VoxCPM 1.5B just got released!" src="https://external-preview.redd.it/MIb2iimHkfYqVDgmZztu-h5tz8yFqiAztGcy6umK7o8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=08d7fc02333ca119537bfd3af70a4c74b40c2e98" title="VoxCPM 1.5B just got released!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was just visiting the &lt;a href="https://github.com/OpenBMB/VoxCPM"&gt;GitHub page&lt;/a&gt; today (setting up a FastAPI TTS server) when I realized that they released a new version of the VoxCPM model. The original VoxCPM-0.5B was already very good in my testing, but this model looks like a straight improvement (it's still a 0.5B model, despite the rather confusing naming scheme).&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Feature&lt;/th&gt; &lt;th align="left"&gt;VoxCPM&lt;/th&gt; &lt;th align="left"&gt;VoxCPM1.5&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Audio VAE Sampling Rate&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;16kHz&lt;/td&gt; &lt;td align="left"&gt;44.1kHz&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;LM Token Rate&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;12.5Hz&lt;/td&gt; &lt;td align="left"&gt;6.25Hz&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Patch Size&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;4&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;SFT Support&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;‚úÖ&lt;/td&gt; &lt;td align="left"&gt;‚úÖ&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;LoRA Support&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;‚úÖ&lt;/td&gt; &lt;td align="left"&gt;‚úÖ&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;They also added fine-tuning support as well as a guide &lt;a href="https://github.com/OpenBMB/VoxCPM/blob/main/docs/finetune.md"&gt;https://github.com/OpenBMB/VoxCPM/blob/main/docs/finetune.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Example output: &lt;a href="https://voca.ro/147qPjN98F6g"&gt;https://voca.ro/147qPjN98F6g&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hefty_Wolverine_553"&gt; /u/Hefty_Wolverine_553 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/openbmb/VoxCPM1.5"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfcatm/voxcpm_15b_just_got_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfcatm/voxcpm_15b_just_got_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T01:07:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfg0rh</id>
    <title>The Best Open-Source 8B-Parameter LLM Built in the USA</title>
    <updated>2025-12-06T04:14:17+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfg0rh/the_best_opensource_8bparameter_llm_built_in_the/"&gt; &lt;img alt="The Best Open-Source 8B-Parameter LLM Built in the USA" src="https://preview.redd.it/r6muiibadi5g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2f50b4cb0f889ed02690c8f3ff7e90713b46562c" title="The Best Open-Source 8B-Parameter LLM Built in the USA" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Rnj-1 is a family of 8B parameter open-weight, dense models trained from scratch by Essential AI, optimized for code and STEM with capabilities on par with SOTA open-weight models.&lt;/p&gt; &lt;p&gt;These models &lt;/p&gt; &lt;ul&gt; &lt;li&gt;perform well across a range of programming languages. &lt;/li&gt; &lt;li&gt;boast strong agentic capabilities (e.g., inside agentic frameworks like mini-SWE-agent). &lt;/li&gt; &lt;li&gt;excel at tool-calling.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Both raw and instruct variants are available on &lt;a href="https://huggingface.co/collections/EssentialAI/rnj-1"&gt;Hugging Face platform&lt;/a&gt;. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Model Architecture Overview&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Rnj-1's architecture is similar to Gemma 3, except that it uses only global attention, and YaRN for long-context extension.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Training Dynamics&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;rnj-1&lt;/code&gt; was pre-trained on 8.4T tokens with an 8K context length, after which the model‚Äôs context window was extended to &lt;strong&gt;32K&lt;/strong&gt; through an additional 380B-token mid-training stage. &lt;/p&gt; &lt;p&gt;A final 150B-token SFT stage completed the training to produce &lt;code&gt;rnj-1-instruct&lt;/code&gt;. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/r6muiibadi5g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfg0rh/the_best_opensource_8bparameter_llm_built_in_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfg0rh/the_best_opensource_8bparameter_llm_built_in_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T04:14:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1pf0q99</id>
    <title>You will own nothing and you will be happy!</title>
    <updated>2025-12-05T17:13:24+00:00</updated>
    <author>
      <name>/u/dreamyrhodes</name>
      <uri>https://old.reddit.com/user/dreamyrhodes</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Come and put everything in to cloud. We now getting into hardware as a service. The RAM craze will impact everything to the point where consumers can't afford normal hardware anymore because it's all scraped off, locked away and put into datacenters to sell to you services to store your data. (Of course that data also will be used to train AI models to sell to you as a service as well lol.)&lt;/p&gt; &lt;p&gt;You don't need RAM anymore nor do you need SSDs. You will store and process every byte of your digital life in some datacenter and pay a monthly fee to access and process it.&lt;/p&gt; &lt;p&gt;You will own nothing and you will be happy!&lt;/p&gt; &lt;p&gt;GN: WTF Just Happened? | The Corrupt Memory Industry &amp;amp; Micron&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=9A-eeJP0J7c"&gt;https://www.youtube.com/watch?v=9A-eeJP0J7c&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dreamyrhodes"&gt; /u/dreamyrhodes &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pf0q99/you_will_own_nothing_and_you_will_be_happy/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pf0q99/you_will_own_nothing_and_you_will_be_happy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pf0q99/you_will_own_nothing_and_you_will_be_happy/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T17:13:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
