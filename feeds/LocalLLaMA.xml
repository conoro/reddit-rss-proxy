<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-04T13:34:29+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI. Hey, Tony</subtitle>
  <entry>
    <id>t3_1nx8igd</id>
    <title>Best LLMs for writing (not coding)</title>
    <updated>2025-10-03T19:21:05+00:00</updated>
    <author>
      <name>/u/FrequentHelp2203</name>
      <uri>https://old.reddit.com/user/FrequentHelp2203</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It seems most of the LLMs I see are being ranked on coding ability and I understand why I think but for the rest of us, what are some of best LLM for writing. Not writing for you but analysis and critique to better develop your writing such as an essay or story. &lt;/p&gt; &lt;p&gt;Thank you for your time. &lt;/p&gt; &lt;p&gt;Update: thanks for all the help. Appreciate it&lt;/p&gt; &lt;p&gt;Update: I’m writing my own stuff. Essays mostly. I need LLMs that can improve it with discussion and analysis. I write far better than the LLMs I’ve tried so hoping to hear what’s really good out there. Again appreciate your time and tips. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FrequentHelp2203"&gt; /u/FrequentHelp2203 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nx8igd/best_llms_for_writing_not_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nx8igd/best_llms_for_writing_not_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nx8igd/best_llms_for_writing_not_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T19:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwxxya</id>
    <title>Bought a used 5090 only to find out it was tampered with</title>
    <updated>2025-10-03T12:36:48+00:00</updated>
    <author>
      <name>/u/a201905</name>
      <uri>https://old.reddit.com/user/a201905</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just a angry/disappointment/frustration post from someone who was very excited at the opportunity to upgrade from 3080 to a 5090 at a discount to run local LLM.&lt;/p&gt; &lt;p&gt;A MSI rtx 5090 came up at my local, trustworthy auction house and I won it for around $2k. It was a stretch on my budget but it was too good of an opportunity so I jumped on it. I was extremely excited and upgraded the PSU but when I tried to put everything together, the system would not boot. I tried everything for hours until I remembered reading the article about people stealing GPU cores. &lt;/p&gt; &lt;p&gt;So I looked at the back and noticed the warranty tamper sticker was voided. i looked back at the auction site and I can see the image they posted with the screw tampered. I was blinded by the potential happiness this was going to bring me and I just didn't pay attention.&lt;/p&gt; &lt;p&gt;What a disappointment. Why do people do this garbage to others. I hope karma bites you in the ass. &lt;/p&gt; &lt;p&gt;Edit: I should have been clearer, i opened it and it's missing the core. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/a201905"&gt; /u/a201905 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwxxya/bought_a_used_5090_only_to_find_out_it_was/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwxxya/bought_a_used_5090_only_to_find_out_it_was/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwxxya/bought_a_used_5090_only_to_find_out_it_was/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T12:36:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxibik</id>
    <title>Where’s the lip reading ai?</title>
    <updated>2025-10-04T02:31:46+00:00</updated>
    <author>
      <name>/u/Trustingmeerkat</name>
      <uri>https://old.reddit.com/user/Trustingmeerkat</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m sure there are some projects out there making real progress on this, but given how quickly tech has advanced in recent years, I’m honestly surprised nothing has surfaced with strong accuracy in converting video to transcript purely through lip reading.&lt;/p&gt; &lt;p&gt;From what I’ve seen, personalized models trained on specific individuals do quite well with front facing footage, but where’s the model that can take any video and give a reasonably accurate idea of what was said? Putting privacy concerns aside for a second, it feels like we should already be 80 percent of the way there. With the amount of spoken video data that already has transcripts, a solid model paired with a standard LLM technique could fill in the blanks with high confidence.&lt;/p&gt; &lt;p&gt;If that doesn’t exist yet, let’s make it, I’m down to even spin it up as a DAO, which is something I’ve wanted to experiment with.&lt;/p&gt; &lt;p&gt;Bonus question: what historical videos would be the most fascinating or valuable to finally understand what was said on camera?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Trustingmeerkat"&gt; /u/Trustingmeerkat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxibik/wheres_the_lip_reading_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxibik/wheres_the_lip_reading_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxibik/wheres_the_lip_reading_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T02:31:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxh2n8</id>
    <title>Paper | Apriel-1.5-15B-Thinker: Mid-training is all you need</title>
    <updated>2025-10-04T01:28:45+00:00</updated>
    <author>
      <name>/u/touhidul002</name>
      <uri>https://old.reddit.com/user/touhidul002</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;(1) &lt;strong&gt;Integrated Multimodal Architecture&lt;/strong&gt;: Beginning with Pixtral-12B [9] as our foundation, we expand it to a model size capable of advanced reasoning across modalities, without requiring pretraining from scratch. &lt;/p&gt; &lt;p&gt;(2) &lt;strong&gt;Staged Multimodal Continual Pretraining (CPT)&lt;/strong&gt;: We adopt a two-phase CPT strategy. The first phase develops foundational text reasoning and broad multimodal capabilities, while the second enhances visual reasoning through synthetic data targeting spatial structure, compositional understanding, and fine-grained perception. This staged progression enables balanced strengthening of both modalities and provides a stable foundation for subsequent training stages, even when later stages emphasize a narrower set of modalities. &lt;/p&gt; &lt;p&gt;(3) &lt;strong&gt;High-Quality Supervised Fine-Tuning (SFT):&lt;/strong&gt; We curate a diverse, high-quality, and high-signal set of samples for supervised fine-tuning. Each response includes explicit reasoning traces, enabling the model to learn transparent thought processes. Coupled with the strong base model, this yields frontier-level performance across a broad range of reasoning benchmarks without requiring additional post-training. &lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2510.01141"&gt;https://arxiv.org/pdf/2510.01141&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/touhidul002"&gt; /u/touhidul002 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxh2n8/paper_apriel1515bthinker_midtraining_is_all_you/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxh2n8/paper_apriel1515bthinker_midtraining_is_all_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxh2n8/paper_apriel1515bthinker_midtraining_is_all_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T01:28:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxnq77</id>
    <title>best coding model under 40b parameters? preferably moe</title>
    <updated>2025-10-04T07:37:00+00:00</updated>
    <author>
      <name>/u/Odd-Ordinary-5922</name>
      <uri>https://old.reddit.com/user/Odd-Ordinary-5922</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;preferably moe&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Odd-Ordinary-5922"&gt; /u/Odd-Ordinary-5922 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxnq77/best_coding_model_under_40b_parameters_preferably/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxnq77/best_coding_model_under_40b_parameters_preferably/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxnq77/best_coding_model_under_40b_parameters_preferably/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T07:37:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxmq9b</id>
    <title>The Missing Link between the Transformer and Models of the Brain</title>
    <updated>2025-10-04T06:35:16+00:00</updated>
    <author>
      <name>/u/ramzeez88</name>
      <uri>https://old.reddit.com/user/ramzeez88</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A group of scientists at Pathway claim to have found a missing link . 'The massively parallel post-Transformer reasoning architecture which opens the door to generalization over time' Link to the paper : &lt;a href="https://arxiv.org/abs/2509.26507"&gt;https://arxiv.org/abs/2509.26507&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ramzeez88"&gt; /u/ramzeez88 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxmq9b/the_missing_link_between_the_transformer_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxmq9b/the_missing_link_between_the_transformer_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxmq9b/the_missing_link_between_the_transformer_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T06:35:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1nx1ot4</id>
    <title>Qwen3-VL-30B-A3B-Instruct &amp; Thinking (Now Hidden)</title>
    <updated>2025-10-03T15:06:48+00:00</updated>
    <author>
      <name>/u/TKGaming_11</name>
      <uri>https://old.reddit.com/user/TKGaming_11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nx1ot4/qwen3vl30ba3binstruct_thinking_now_hidden/"&gt; &lt;img alt="Qwen3-VL-30B-A3B-Instruct &amp;amp; Thinking (Now Hidden)" src="https://a.thumbs.redditmedia.com/iNETafBex6Qpbyi8P087geXMh_aBmkILehL6E7qn-m4.jpg" title="Qwen3-VL-30B-A3B-Instruct &amp;amp; Thinking (Now Hidden)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TKGaming_11"&gt; /u/TKGaming_11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nx1ot4"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nx1ot4/qwen3vl30ba3binstruct_thinking_now_hidden/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nx1ot4/qwen3vl30ba3binstruct_thinking_now_hidden/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T15:06:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxnszs</id>
    <title>Where do you think we'll be at for home inference in 2 years?</title>
    <updated>2025-10-04T07:41:51+00:00</updated>
    <author>
      <name>/u/TumbleweedDeep825</name>
      <uri>https://old.reddit.com/user/TumbleweedDeep825</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I suppose we'll never see any big price reduction jumps? Especially with inflation rising globally?&lt;/p&gt; &lt;p&gt;I'd love to be able to have a home SOTA tier model for under $15k. Like GLM 4.6, etc. But wouldn't we all?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TumbleweedDeep825"&gt; /u/TumbleweedDeep825 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxnszs/where_do_you_think_well_be_at_for_home_inference/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxnszs/where_do_you_think_well_be_at_for_home_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxnszs/where_do_you_think_well_be_at_for_home_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T07:41:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxt2r2</id>
    <title>Effective context engineering for AI agents by Anthropic</title>
    <updated>2025-10-04T12:47:27+00:00</updated>
    <author>
      <name>/u/Vast_Comedian_9370</name>
      <uri>https://old.reddit.com/user/Vast_Comedian_9370</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxt2r2/effective_context_engineering_for_ai_agents_by/"&gt; &lt;img alt="Effective context engineering for AI agents by Anthropic" src="https://b.thumbs.redditmedia.com/8DlafRLOkFCcnQ62VNCYkbQcmWQV3iEX4J_sWBGs3Ug.jpg" title="Effective context engineering for AI agents by Anthropic" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vast_Comedian_9370"&gt; /u/Vast_Comedian_9370 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nxt2r2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxt2r2/effective_context_engineering_for_ai_agents_by/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxt2r2/effective_context_engineering_for_ai_agents_by/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T12:47:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1nx8e2l</id>
    <title>Is this expected behaviour from Granite 4 32B? (Unsloth Q4XL, no system prompt)</title>
    <updated>2025-10-03T19:16:29+00:00</updated>
    <author>
      <name>/u/IonizedRay</name>
      <uri>https://old.reddit.com/user/IonizedRay</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nx8e2l/is_this_expected_behaviour_from_granite_4_32b/"&gt; &lt;img alt="Is this expected behaviour from Granite 4 32B? (Unsloth Q4XL, no system prompt)" src="https://preview.redd.it/uq9t3il85ysf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=86fe0496ab662fb43abd450fc0e2e5a75018e96b" title="Is this expected behaviour from Granite 4 32B? (Unsloth Q4XL, no system prompt)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IonizedRay"&gt; /u/IonizedRay &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/uq9t3il85ysf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nx8e2l/is_this_expected_behaviour_from_granite_4_32b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nx8e2l/is_this_expected_behaviour_from_granite_4_32b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T19:16:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxbbxe</id>
    <title>GLM 4.6 new best open weight overall on lmarena</title>
    <updated>2025-10-03T21:11:39+00:00</updated>
    <author>
      <name>/u/r3m8sh</name>
      <uri>https://old.reddit.com/user/r3m8sh</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Third on code after Qwen 235b (lmarena isn't agent based). #3 on hard prompts and #1 on creative writing.&lt;/p&gt; &lt;p&gt;Edit : in thinking mode (default).&lt;/p&gt; &lt;p&gt;&lt;a href="https://lmarena.ai/leaderboard/text/overall"&gt;https://lmarena.ai/leaderboard/text/overall&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/r3m8sh"&gt; /u/r3m8sh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxbbxe/glm_46_new_best_open_weight_overall_on_lmarena/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxbbxe/glm_46_new_best_open_weight_overall_on_lmarena/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxbbxe/glm_46_new_best_open_weight_overall_on_lmarena/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T21:11:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1nx18ax</id>
    <title>GLM 4.6 IS A FUKING AMAZING MODEL AND NOBODY CAN TELL ME OTHERWISE</title>
    <updated>2025-10-03T14:49:34+00:00</updated>
    <author>
      <name>/u/boneMechBoy69420</name>
      <uri>https://old.reddit.com/user/boneMechBoy69420</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Especially fuckin artificial analysis and their bullshit ass benchmark &lt;/p&gt; &lt;p&gt;Been using GLM 4.5 it on prod for a month now and I've got nothing but good feedback from the users , it's got way better autonomy than any other proprietary model I've tried (sonnet , gpt 5 and grok code) and it's probably the best ever model for tool call accuracy &lt;/p&gt; &lt;p&gt;One benchmark id recommend yall follow is the berkley function calling benchmark (v4 ig) &lt;a href="https://gorilla.cs.berkeley.edu/leaderboard.html"&gt;bfcl v4&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/boneMechBoy69420"&gt; /u/boneMechBoy69420 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nx18ax/glm_46_is_a_fuking_amazing_model_and_nobody_can/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nx18ax/glm_46_is_a_fuking_amazing_model_and_nobody_can/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nx18ax/glm_46_is_a_fuking_amazing_model_and_nobody_can/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T14:49:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxo3ao</id>
    <title>Best local model for open code?</title>
    <updated>2025-10-04T08:00:16+00:00</updated>
    <author>
      <name>/u/LastCulture3768</name>
      <uri>https://old.reddit.com/user/LastCulture3768</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Which LLM gives you satisfaction for tasks under open code with 12Go vram ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LastCulture3768"&gt; /u/LastCulture3768 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxo3ao/best_local_model_for_open_code/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxo3ao/best_local_model_for_open_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxo3ao/best_local_model_for_open_code/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T08:00:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxqxtl</id>
    <title>Anyone running llm on their 16GB android phone?</title>
    <updated>2025-10-04T10:58:27+00:00</updated>
    <author>
      <name>/u/Ok_Warning2146</name>
      <uri>https://old.reddit.com/user/Ok_Warning2146</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My 8gb dual channel phone is dying, so I would like buy a 16gb quad channel android phone to run llm.&lt;/p&gt; &lt;p&gt;I am interested in running gemma3-12b-qat-q4_0 on it. &lt;/p&gt; &lt;p&gt;If you have one, can you run it for me on pocketpal or chatterUI and report the performance (t/s for both prompt processing and inference)? Please also report your phone model such that I can link GPU GFLOPS and memory bandwidth to the performance.&lt;/p&gt; &lt;p&gt;Thanks a lot in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Warning2146"&gt; /u/Ok_Warning2146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxqxtl/anyone_running_llm_on_their_16gb_android_phone/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxqxtl/anyone_running_llm_on_their_16gb_android_phone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxqxtl/anyone_running_llm_on_their_16gb_android_phone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T10:58:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxr4gu</id>
    <title>Smartest model to run on 5090?</title>
    <updated>2025-10-04T11:08:10+00:00</updated>
    <author>
      <name>/u/eCityPlannerWannaBe</name>
      <uri>https://old.reddit.com/user/eCityPlannerWannaBe</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What’s the largest model I should run on 5090 for reasoning? E.g. GLM 4.6 - which version is ideal for one 5090?&lt;/p&gt; &lt;p&gt;Thanks. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eCityPlannerWannaBe"&gt; /u/eCityPlannerWannaBe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxr4gu/smartest_model_to_run_on_5090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxr4gu/smartest_model_to_run_on_5090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxr4gu/smartest_model_to_run_on_5090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T11:08:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxqabe</id>
    <title>Awesome Local LLM Speech-to-Speech Models &amp; Frameworks</title>
    <updated>2025-10-04T10:19:27+00:00</updated>
    <author>
      <name>/u/tleyden</name>
      <uri>https://old.reddit.com/user/tleyden</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxqabe/awesome_local_llm_speechtospeech_models_frameworks/"&gt; &lt;img alt="Awesome Local LLM Speech-to-Speech Models &amp;amp; Frameworks" src="https://external-preview.redd.it/sv2a4YrAVR9g08yOa0AOrBrIqErmKKSQAYIjNTCx_eI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d1770300a09702157f67115321b5768d01848a3a" title="Awesome Local LLM Speech-to-Speech Models &amp;amp; Frameworks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Did some digging into speech-to-speech models/frameworks for a project recently and ended up with a pretty comprehensive list. Figured I'd drop it here in case it helps anyone else avoid going down the same rabbit hole. &lt;/p&gt; &lt;p&gt;What made the cut:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Has &lt;strong&gt;LLM integration&lt;/strong&gt; (built-in or via modules)&lt;/li&gt; &lt;li&gt;Does &lt;strong&gt;full speech-to-speech&lt;/strong&gt; pipeline, not just STT or TTS alone&lt;/li&gt; &lt;li&gt;Works &lt;strong&gt;locally/self-hosted&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Had to trim quite a bit to keep this readable, but the full list with more details is on GitHub at &lt;a href="https://github.com/tleyden/awesome-llm-speech-to-speech"&gt;tleyden/awesome-llm-speech-to-speech&lt;/a&gt;. PRs welcome if you spot anything wrong or missing! &lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;&lt;strong&gt;Project&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;Open Source&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;Type&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;LLM + Tool Calling&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;Platforms&lt;/strong&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/kyutai-labs/unmute"&gt;&lt;strong&gt;Unmute.sh&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;✅ Yes&lt;/td&gt; &lt;td align="left"&gt;Cascading&lt;/td&gt; &lt;td align="left"&gt;Works with any local LLM · Tool calling not yet but planned&lt;/td&gt; &lt;td align="left"&gt;Linux only&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/fixie-ai/ultravox"&gt;&lt;strong&gt;Ultravox (Fixie)&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;✅ MIT&lt;/td&gt; &lt;td align="left"&gt;Hybrid (audio-native LLM + ASR + TTS)&lt;/td&gt; &lt;td align="left"&gt;Uses Llama/Mistral/Gemma · Full tool-calling via backend LLM&lt;/td&gt; &lt;td align="left"&gt;Windows / Linux&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/KoljaB/RealtimeVoiceChat"&gt;&lt;strong&gt;RealtimeVoiceChat&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;✅ MIT&lt;/td&gt; &lt;td align="left"&gt;Cascading&lt;/td&gt; &lt;td align="left"&gt;Pluggable LLM (local or remote) · Likely supports tool calling&lt;/td&gt; &lt;td align="left"&gt;Linux recommended&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/Lex-au/Vocalis"&gt;&lt;strong&gt;Vocalis&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;✅ Apache-2&lt;/td&gt; &lt;td align="left"&gt;Cascading&lt;/td&gt; &lt;td align="left"&gt;Fine-tuned LLaMA-3-8B-Instruct · Tool calling via backend LLM&lt;/td&gt; &lt;td align="left"&gt;macOS / Windows / Linux (runs on Apple Silicon)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.liquid.ai/blog/liquid-foundation-models-v2-our-second-series-of-generative-ai-models?ref=producthunt"&gt;&lt;strong&gt;LFM2&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;✅ Yes&lt;/td&gt; &lt;td align="left"&gt;End-to-End&lt;/td&gt; &lt;td align="left"&gt;Built-in LLM (E2E) · Native tool calling&lt;/td&gt; &lt;td align="left"&gt;Windows / Linux&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/gpt-omni/mini-omni2"&gt;&lt;strong&gt;Mini-omni2&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;✅ MIT&lt;/td&gt; &lt;td align="left"&gt;End-to-End&lt;/td&gt; &lt;td align="left"&gt;Built-in Qwen2 LLM · Tool calling TBD&lt;/td&gt; &lt;td align="left"&gt;Cross-platform&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/pipecat-ai/pipecat"&gt;&lt;strong&gt;Pipecat&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;✅ Yes&lt;/td&gt; &lt;td align="left"&gt;Cascading&lt;/td&gt; &lt;td align="left"&gt;Pluggable LLM, ASR, TTS · Explicit tool-calling support&lt;/td&gt; &lt;td align="left"&gt;Windows / macOS / Linux / iOS / Android&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Notes&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;“Cascading” = modular ASR → LLM → TTS&lt;/li&gt; &lt;li&gt;“E2E” = end-to-end LLM that directly maps speech-to-speech&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tleyden"&gt; /u/tleyden &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/tleyden/awesome-llm-speech-to-speech"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxqabe/awesome_local_llm_speechtospeech_models_frameworks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxqabe/awesome_local_llm_speechtospeech_models_frameworks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T10:19:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxjh4c</id>
    <title>GitHub - huawei-csl/SINQ: Welcome to the official repository of SINQ! A novel, fast and high-quality quantization method designed to make any Large Language Model smaller while preserving accuracy.</title>
    <updated>2025-10-04T03:30:30+00:00</updated>
    <author>
      <name>/u/Aiochedolor</name>
      <uri>https://old.reddit.com/user/Aiochedolor</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxjh4c/github_huaweicslsinq_welcome_to_the_official/"&gt; &lt;img alt="GitHub - huawei-csl/SINQ: Welcome to the official repository of SINQ! A novel, fast and high-quality quantization method designed to make any Large Language Model smaller while preserving accuracy." src="https://external-preview.redd.it/yP0CnjxBFJCXTVacHixSvy4H_F7MTnOAVtKcV29Lggk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c413349140863192c8413b0f7b8e7f32ec48822c" title="GitHub - huawei-csl/SINQ: Welcome to the official repository of SINQ! A novel, fast and high-quality quantization method designed to make any Large Language Model smaller while preserving accuracy." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aiochedolor"&gt; /u/Aiochedolor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/huawei-csl/SINQ"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxjh4c/github_huaweicslsinq_welcome_to_the_official/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxjh4c/github_huaweicslsinq_welcome_to_the_official/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T03:30:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxjhnj</id>
    <title>Behold, the jankiest setup ever</title>
    <updated>2025-10-04T03:31:15+00:00</updated>
    <author>
      <name>/u/T-VIRUS999</name>
      <uri>https://old.reddit.com/user/T-VIRUS999</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxjhnj/behold_the_jankiest_setup_ever/"&gt; &lt;img alt="Behold, the jankiest setup ever" src="https://b.thumbs.redditmedia.com/twOOoKU5XbRq6uFRGfXj_XqIEzieTWVvhWE3zg-T_qA.jpg" title="Behold, the jankiest setup ever" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I plan to get an open test bench, after I get my second P40 in a week or two (which will fit nicely on the other side of that fan) &lt;/p&gt; &lt;p&gt;Performance is as shown, Qwen 3 32B Q4 5.9T/sec&lt;/p&gt; &lt;p&gt;The fan is one of those stupidly powerful Delta electronics server fans that pushes out like 250cfm, so I needed to add a PWM controller to slow it down, and it wouldn't run without that giant capacitor, and it's powered by a Li-ion battery instead of the PSU (for now) &lt;/p&gt; &lt;p&gt;It's not stable at all, the whole system BSODs if a program tries to query the GPU while something else is using it (such as if I try to run GPUZ while LM Studio is running), but if only 1 thing touches the GPU at a time, it works &lt;/p&gt; &lt;p&gt;It has a Ryzen 5 5500GT, 16GB of DDR4, a 1000w PSU, a 512GB SSD, and 1 Nvidia P40 (soon to be 2) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/T-VIRUS999"&gt; /u/T-VIRUS999 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nxjhnj"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxjhnj/behold_the_jankiest_setup_ever/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxjhnj/behold_the_jankiest_setup_ever/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T03:31:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxi82t</id>
    <title>Why do private companies release open source models?</title>
    <updated>2025-10-04T02:26:58+00:00</updated>
    <author>
      <name>/u/desudesu15</name>
      <uri>https://old.reddit.com/user/desudesu15</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I love open source models. I feel they are an alternative for general knowledge, and since I started in this world, I stopped paying for subscriptions and started running models locally.&lt;/p&gt; &lt;p&gt;However, I don't understand the business model of companies like OpenAI launching an open source model. &lt;/p&gt; &lt;p&gt;How do they make money by launching an open source model? &lt;/p&gt; &lt;p&gt;Isn't it counterproductive to their subscription model?&lt;/p&gt; &lt;p&gt;Thank you, and forgive my ignorance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/desudesu15"&gt; /u/desudesu15 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxi82t/why_do_private_companies_release_open_source/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxi82t/why_do_private_companies_release_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxi82t/why_do_private_companies_release_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T02:26:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxs8tr</id>
    <title>GLM 4.6 Makes Incredible Front End Design with 2 prompts</title>
    <updated>2025-10-04T12:08:05+00:00</updated>
    <author>
      <name>/u/dev_is_active</name>
      <uri>https://old.reddit.com/user/dev_is_active</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxs8tr/glm_46_makes_incredible_front_end_design_with_2/"&gt; &lt;img alt="GLM 4.6 Makes Incredible Front End Design with 2 prompts" src="https://external-preview.redd.it/HRx3NTzzZMOIdtM0oRmpT2rIW9OnDaS9AE7D0C1FPSc.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9d08204e2ea136e84ee75e08ffa737f0e7653aea" title="GLM 4.6 Makes Incredible Front End Design with 2 prompts" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I've been playing with GLM 4.6, I've also implemented it inside Claud Code, and I'll be doing a new video on how to set up GLM 4.6 in Cloud Code, but I really wanted to show everybody how great z ai is with front end design.&lt;/p&gt; &lt;p&gt;In this video I take a screenshot of a website and I do one simple prompt and it kicks out a good design and then I ask it to enhance it, and then it turns it into an incredible design, you can watch it here&lt;/p&gt; &lt;p&gt;Would love to know what you think and if any of you are using GLM in Claude Code yet?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dev_is_active"&gt; /u/dev_is_active &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/AvHsytH-K84"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxs8tr/glm_46_makes_incredible_front_end_design_with_2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxs8tr/glm_46_makes_incredible_front_end_design_with_2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T12:08:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxjzbn</id>
    <title>Distributed Inference over wifi with 8x 3090 egpus performance</title>
    <updated>2025-10-04T03:57:59+00:00</updated>
    <author>
      <name>/u/Only_Situation_4713</name>
      <uri>https://old.reddit.com/user/Only_Situation_4713</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I smoked some really good weed recently and decided it was a good idea to buy more 3090s.&lt;/p&gt; &lt;p&gt;Naturally I didn't want to use a real build with server parts, put 8 3090s in one build on home depot racks? No thanks I'm lazy.&lt;/p&gt; &lt;p&gt;I got 4 3090 egpus from a guy on facebook. He's cool, sold them to me for 650 each with the egpu. &lt;/p&gt; &lt;p&gt;&lt;a href="https://www.gigabyte.com/Graphics-Card/GV-N3090IXEB-24GD"&gt;https://www.gigabyte.com/Graphics-Card/GV-N3090IXEB-24GD&lt;/a&gt; &amp;lt;--- these are the EGPUs&lt;/p&gt; &lt;p&gt;Then I got 4 other random 3090s of different brands and put them in 3 spare Pcs I have lying around.&lt;/p&gt; &lt;p&gt;Node #1&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Z390 Prime&lt;/li&gt; &lt;li&gt;9900K&lt;/li&gt; &lt;li&gt;64gb of DDR4&lt;/li&gt; &lt;li&gt;3090 (duh)&lt;/li&gt; &lt;li&gt;850W.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Node #2&lt;/p&gt; &lt;ul&gt; &lt;li&gt;MSI Unify ITX z690&lt;/li&gt; &lt;li&gt;12400K&lt;/li&gt; &lt;li&gt;64gb of DDR5&lt;/li&gt; &lt;li&gt;3090 (duh) &lt;/li&gt; &lt;li&gt;650W&lt;/li&gt; &lt;li&gt;2X 3090 EGPUs attached&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Node #3 (Host)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Z790 Maximus Hero&lt;/li&gt; &lt;li&gt;13700k&lt;/li&gt; &lt;li&gt;64gb of DDR5&lt;/li&gt; &lt;li&gt;1200W PSU&lt;/li&gt; &lt;li&gt;2x 3090s &lt;/li&gt; &lt;li&gt;2x 3090 EGPUs attached&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I ran all of it over VLLM with Ray to distribute the load. It's connected over Wifi, I got a good router so speed is about only 10% slower than ethernet from across the house. For now it's all pipeline parallel until the parts arrive then I'll do a 2 node system with 4 gpu each.&lt;/p&gt; &lt;p&gt;&lt;a href="https://rog.asus.com/us/networking/rog-rapture-gt-axe16000-model/"&gt;https://rog.asus.com/us/networking/rog-rapture-gt-axe16000-model/&lt;/a&gt; &amp;lt;--- my router(s).&lt;/p&gt; &lt;p&gt;Results:&lt;/p&gt; &lt;p&gt;At 128k context limit running GLM 4.5 Air AWQ 8 bit (that's Q8 for you gguf folks)&lt;/p&gt; &lt;p&gt;I get 5500 tokens/s prompt processing and 24 tokens a second for a 50k~ ish token prompt. &lt;/p&gt; &lt;p&gt;It works great over Roo.&lt;/p&gt; &lt;p&gt;Ray has a very annoying overhead cost so just assume that each system has like 1gb less vram. Running all my node in headless helps alot too.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Only_Situation_4713"&gt; /u/Only_Situation_4713 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxjzbn/distributed_inference_over_wifi_with_8x_3090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxjzbn/distributed_inference_over_wifi_with_8x_3090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxjzbn/distributed_inference_over_wifi_with_8x_3090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T03:57:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxrssl</id>
    <title>This is pretty cool</title>
    <updated>2025-10-04T11:45:15+00:00</updated>
    <author>
      <name>/u/wowsers7</name>
      <uri>https://old.reddit.com/user/wowsers7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxrssl/this_is_pretty_cool/"&gt; &lt;img alt="This is pretty cool" src="https://external-preview.redd.it/fWjpQVd5VjUiyz85oEUZ3MBMlQycdcPTlMPYbRWoE6A.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f573aa728fb79af617ec9e24df900618595c6abb" title="This is pretty cool" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://venturebeat.com/ai/huaweis-new-open-source-technique-shrinks-llms-to-make-them-run-on-less"&gt;https://venturebeat.com/ai/huaweis-new-open-source-technique-shrinks-llms-to-make-them-run-on-less&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/huawei-csl/SINQ/blob/main/README.md"&gt;https://github.com/huawei-csl/SINQ/blob/main/README.md&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wowsers7"&gt; /u/wowsers7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/huawei-csl/SINQ/blob/main/README.md"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxrssl/this_is_pretty_cool/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxrssl/this_is_pretty_cool/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T11:45:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwx1rx</id>
    <title>The most important AI paper of the decade. No debate</title>
    <updated>2025-10-03T11:55:32+00:00</updated>
    <author>
      <name>/u/PumpkinNarrow6339</name>
      <uri>https://old.reddit.com/user/PumpkinNarrow6339</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwx1rx/the_most_important_ai_paper_of_the_decade_no/"&gt; &lt;img alt="The most important AI paper of the decade. No debate" src="https://preview.redd.it/d2rcvb6nyvsf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0051a67e9886e507e2b0a35679f4d469050fda91" title="The most important AI paper of the decade. No debate" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PumpkinNarrow6339"&gt; /u/PumpkinNarrow6339 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/d2rcvb6nyvsf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwx1rx/the_most_important_ai_paper_of_the_decade_no/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwx1rx/the_most_important_ai_paper_of_the_decade_no/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T11:55:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxhfcq</id>
    <title>Qwen3-VL-30B-A3B-Instruct &amp; Thinking are here</title>
    <updated>2025-10-04T01:46:34+00:00</updated>
    <author>
      <name>/u/AlanzhuLy</name>
      <uri>https://old.reddit.com/user/AlanzhuLy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxhfcq/qwen3vl30ba3binstruct_thinking_are_here/"&gt; &lt;img alt="Qwen3-VL-30B-A3B-Instruct &amp;amp; Thinking are here" src="https://external-preview.redd.it/NiG8S3h1NHl5GO5rtLVQY3YBzVNQNV4xvZR0hFbk_vE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=88770649ad1f1c425c3a22e1502363d18f9727dc" title="Qwen3-VL-30B-A3B-Instruct &amp;amp; Thinking are here" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/xwkuqkkt20tf1.png?width=2994&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=16a4068b96a7c20f55817cc29987345c287c76a7"&gt;https://preview.redd.it/xwkuqkkt20tf1.png?width=2994&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=16a4068b96a7c20f55817cc29987345c287c76a7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-VL-30B-A3B-Instruct"&gt;https://huggingface.co/Qwen/Qwen3-VL-30B-A3B-Instruct&lt;/a&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/Qwen/Qwen3-VL-30B-A3B-Thinking"&gt;https://huggingface.co/Qwen/Qwen3-VL-30B-A3B-Thinking&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlanzhuLy"&gt; /u/AlanzhuLy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxhfcq/qwen3vl30ba3binstruct_thinking_are_here/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxhfcq/qwen3vl30ba3binstruct_thinking_are_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxhfcq/qwen3vl30ba3binstruct_thinking_are_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T01:46:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxshw2</id>
    <title>IBM granite 4.0-h-tiny leads the way for extra small MoEs</title>
    <updated>2025-10-04T12:20:23+00:00</updated>
    <author>
      <name>/u/GreenTreeAndBlueSky</name>
      <uri>https://old.reddit.com/user/GreenTreeAndBlueSky</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxshw2/ibm_granite_40htiny_leads_the_way_for_extra_small/"&gt; &lt;img alt="IBM granite 4.0-h-tiny leads the way for extra small MoEs" src="https://preview.redd.it/nlkf3btz73tf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1368a9b2fc469f876a31fef05020119815deb818" title="IBM granite 4.0-h-tiny leads the way for extra small MoEs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I hope the trend for those MoEs carries on. Normies with laverage laptops will soon be able to use decent models with little ressources. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GreenTreeAndBlueSky"&gt; /u/GreenTreeAndBlueSky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nlkf3btz73tf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxshw2/ibm_granite_40htiny_leads_the_way_for_extra_small/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxshw2/ibm_granite_40htiny_leads_the_way_for_extra_small/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T12:20:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvryo4</id>
    <title>AMA Announcement: Prime Intellect — The Open‑Source Distributed Training Lab (Thu, Oct 2 • 10 AM – 1 PM PDT)</title>
    <updated>2025-10-02T02:31:01+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"&gt; &lt;img alt="AMA Announcement: Prime Intellect — The Open‑Source Distributed Training Lab (Thu, Oct 2 • 10 AM – 1 PM PDT)" src="https://preview.redd.it/222kj50x0msf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c1c747c42690f74b8d08690dcdbb1ef23aece13b" title="AMA Announcement: Prime Intellect — The Open‑Source Distributed Training Lab (Thu, Oct 2 • 10 AM – 1 PM PDT)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/222kj50x0msf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T02:31:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwaoyd</id>
    <title>AMA with Prime Intellect — Ask Us Anything!</title>
    <updated>2025-10-02T17:47:44+00:00</updated>
    <author>
      <name>/u/kindacognizant</name>
      <uri>https://old.reddit.com/user/kindacognizant</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;AMA with Prime Intellect — Ask Us Anything!&lt;/h1&gt; &lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;! We’re excited for this AMA, thank you for having us.&lt;/p&gt; &lt;p&gt;I’m Kalomaze (&lt;a href="/u/kindacognizant"&gt;u/kindacognizant&lt;/a&gt;), a researcher at Prime Intellect, the lab behind:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Distributed training &lt;a href="https://www.primeintellect.ai/#research"&gt;efforts&lt;/a&gt; including INTELLECT-1 + INTELLECT-2&lt;/li&gt; &lt;li&gt;Open-source RL efforts including &lt;a href="https://github.com/PrimeIntellect-ai/verifiers"&gt;verifiers&lt;/a&gt;, &lt;a href="https://github.com/PrimeIntellect-ai/prime-rl"&gt;prime-rl&lt;/a&gt;, and the &lt;a href="https://app.primeintellect.ai/dashboard/environments"&gt;Environments Hub&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Our other participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Sami Jaghouar, &lt;a href="/u/samsja19"&gt;u/samsja19&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Will Brown, &lt;a href="/u/willccbb"&gt;u/willccbb&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Jack Min Ong, &lt;a href="/u/Cinamic"&gt;u/Cinamic&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Mika Senghaas, &lt;a href="/u/mikasenghaas"&gt;u/mikasenghaas&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 11:00 AM – 2:00 PM PST, with the Prime Intellect team continuing to follow up on questions over the next 48 hours.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kindacognizant"&gt; /u/kindacognizant &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwaoyd/ama_with_prime_intellect_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwaoyd/ama_with_prime_intellect_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwaoyd/ama_with_prime_intellect_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T17:47:44+00:00</published>
  </entry>
</feed>
