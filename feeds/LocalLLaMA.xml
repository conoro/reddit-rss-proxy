<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-24T05:10:14+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qkghpk</id>
    <title>Quiet Threadripper AI Workstation - 768GB DDR5 and 160GB VRAM (RTX 5090 + 4x R9700)</title>
    <updated>2026-01-23T04:00:22+00:00</updated>
    <author>
      <name>/u/sloptimizer</name>
      <uri>https://old.reddit.com/user/sloptimizer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkghpk/quiet_threadripper_ai_workstation_768gb_ddr5_and/"&gt; &lt;img alt="Quiet Threadripper AI Workstation - 768GB DDR5 and 160GB VRAM (RTX 5090 + 4x R9700)" src="https://b.thumbs.redditmedia.com/OvcCfJivz4D4HOjUCQinn-sUra3cRaaS32dKiyRmuRM.jpg" title="Quiet Threadripper AI Workstation - 768GB DDR5 and 160GB VRAM (RTX 5090 + 4x R9700)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Seeing all the quad R9700 builds inspired me to post mine!&lt;/p&gt; &lt;p&gt;I managed to squeeze in RTX 5090 and four R9700 into a workstation build by fitting some GPUs vertically in the front section. Two power supplies: 1600W for the main system and most of the components, and a smaller 850W power supply for 3 of the Radeons (the power cable is threaded through the system popping out through a small gap left by RTX 5090).&lt;/p&gt; &lt;p&gt;DeepSeek-V3.1-Terminus with context = 37279 tokens: PP = 151.76 tps, TG = 10.85 tps&lt;/p&gt; &lt;p&gt;Some things I discovered running local LLMs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;For water-cooled CPU systems, there is not enough air circulation to cool the RAM! &lt;ul&gt; &lt;li&gt;Adding RAM fans got me a 30% performance boost with DeepSeek&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Turning off remote management on WRX90E-SAGE makes it boot much faster&lt;/li&gt; &lt;li&gt;You can combine Nvidia and AMD cards in llama.cpp by compiling with &lt;code&gt;-DGGML_BACKEND_DL=ON&lt;/code&gt;&lt;/li&gt; &lt;li&gt;No significant performance penalty running RTX 5090 at 400W, but much cooler and quieter &lt;ul&gt; &lt;li&gt;To fix, run: &lt;code&gt;sudo nvidia-smi -pl 400&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;R9700 has crazy auto-overclocking by default, draining power and making a lot of noise for little gain &lt;ul&gt; &lt;li&gt;To fix, run: &lt;code&gt;sudo amd-smi set --perf-level=HIGH&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Despite aggressive auto-overclocking, R9700's default mode is sub-optimal for MoE offloading (perf-level=HIGH fixes that as well)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Component List:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Motherboard - Pro WS WRX90E-SAGE SE&lt;/li&gt; &lt;li&gt;CPU - AMD Ryzen Threadripper PRO 7975WX&lt;/li&gt; &lt;li&gt;RAM - 8x KINGSTON 96GB DDR5 5600MHz CL46&lt;/li&gt; &lt;li&gt;GPU1 - ASUS TUF GeForce RTX 5090&lt;/li&gt; &lt;li&gt;GPU2 - 4x ASRock Creator Radeon AI Pro R9700&lt;/li&gt; &lt;li&gt;NVMe - 4x Samsung 9100 PRO 2TB&lt;/li&gt; &lt;li&gt;HDD - 2x Seagate Exos 16TB Enterprise&lt;/li&gt; &lt;li&gt;Power1 - Dark Power Pro 13 1600W 80+ Titanium&lt;/li&gt; &lt;li&gt;Power2 - Seasonic FOCUS V3 GX-850, 850W 80+ Gold&lt;/li&gt; &lt;li&gt;Case - Fractal Design Define 7 XL&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sloptimizer"&gt; /u/sloptimizer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qkghpk"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkghpk/quiet_threadripper_ai_workstation_768gb_ddr5_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qkghpk/quiet_threadripper_ai_workstation_768gb_ddr5_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-23T04:00:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ql25mq</id>
    <title>Are You Using Open Source Models in Production Yet?</title>
    <updated>2026-01-23T20:34:41+00:00</updated>
    <author>
      <name>/u/thecalmgreen</name>
      <uri>https://old.reddit.com/user/thecalmgreen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Quick question, if you don't mind.&lt;/p&gt; &lt;p&gt;Have you been using open source models in production? If so, how has the experience been for you so far?&lt;/p&gt; &lt;p&gt;And if not, do you feel it's still a bit too early to rely on them in real-world production environments?&lt;/p&gt; &lt;p&gt;I'd really love to hear your thoughts and experiences. &lt;/p&gt; &lt;p&gt;Thanks in advance! üôÇ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thecalmgreen"&gt; /u/thecalmgreen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ql25mq/are_you_using_open_source_models_in_production_yet/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ql25mq/are_you_using_open_source_models_in_production_yet/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ql25mq/are_you_using_open_source_models_in_production_yet/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-23T20:34:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1qkj9zh</id>
    <title>GLM4.7-Flash REAP @ 25% live on HF + agentic coding evals</title>
    <updated>2026-01-23T06:19:37+00:00</updated>
    <author>
      <name>/u/ilzrvch</name>
      <uri>https://old.reddit.com/user/ilzrvch</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkj9zh/glm47flash_reap_25_live_on_hf_agentic_coding_evals/"&gt; &lt;img alt="GLM4.7-Flash REAP @ 25% live on HF + agentic coding evals" src="https://b.thumbs.redditmedia.com/sU6QdQb5WuTlrLDYXYowgPhhH5wsTdVh6s0BAol-lqA.jpg" title="GLM4.7-Flash REAP @ 25% live on HF + agentic coding evals" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone!&lt;/p&gt; &lt;p&gt;We're releasing a 25% REAP'd version of GLM4.7-Flash: &lt;a href="http://hf.co/cerebras/GLM-4.7-Flash-REAP-23B-A3B"&gt;hf.co/cerebras/GLM-4.7-Flash-REAP-23B-A3B&lt;/a&gt;&lt;br /&gt; and MiniMax-M2.1 is in the works!&lt;/p&gt; &lt;p&gt;We've gotten a lot of feedback that REAP pruning affects creative writing / multi-lingual capabilities of the model - this is expected for our REAPs with calibration set curated for agentic coding.&lt;/p&gt; &lt;p&gt;We wanted to see how our REAPs are doing vs. other models of comparable size. We ran the mini-swe-agent flow on SWE-rebench leaderboard for October 2025 and found (see attached image) that GLM4.7 REAPs are a big jump over GLM4.6's and are in the Pareto frontier of agentic coding vs. model size efficiency. MiniMax-M2.1 is in between GLM4.7 REAPs @ 25% and 40%, so we think REAPs MiniMax-M2.1 will shine!&lt;/p&gt; &lt;p&gt;Additionally, based on your feedback, we're considering to drop experimental REAPs for creative writing. Do let us know which datasets and evals we should explore for this. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/pw1zn8zsk1fg1.png?width=2700&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=57bacd1248548a329fca9aecaa81b4cc1a8c3c44"&gt;https://preview.redd.it/pw1zn8zsk1fg1.png?width=2700&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=57bacd1248548a329fca9aecaa81b4cc1a8c3c44&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ilzrvch"&gt; /u/ilzrvch &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkj9zh/glm47flash_reap_25_live_on_hf_agentic_coding_evals/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkj9zh/glm47flash_reap_25_live_on_hf_agentic_coding_evals/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qkj9zh/glm47flash_reap_25_live_on_hf_agentic_coding_evals/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-23T06:19:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1qkvytk</id>
    <title>16x V100's worth it?</title>
    <updated>2026-01-23T16:48:18+00:00</updated>
    <author>
      <name>/u/notafakename10</name>
      <uri>https://old.reddit.com/user/notafakename10</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkvytk/16x_v100s_worth_it/"&gt; &lt;img alt="16x V100's worth it?" src="https://b.thumbs.redditmedia.com/YXize4jJdcshM1CPrP71GoILzvat56IpLar078mRHpg.jpg" title="16x V100's worth it?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Found a machine near me:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;CPU: 2*Intel Xeon Platinum 8160 48 Cores 96 Threads &lt;/li&gt; &lt;li&gt;GPU: 16x Tesla V100 32GB HBM2 SXM3 (512GB VRAM in total) &lt;/li&gt; &lt;li&gt;Ram: 128GB DDR4 Server ECC Rams Storage: &lt;/li&gt; &lt;li&gt;960GB NVME SSD&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Obviously not the latest and greatest - but 512gb of VRAM sounds like a lot of fun....&lt;/p&gt; &lt;p&gt;How much will the downsides (no recent support I believe) have too much impact?&lt;/p&gt; &lt;p&gt;~$11k USD &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/c38iqiymo4fg1.jpg?width=720&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=0ef5f9458d5082c478900c4cef413ba8951b2e3c"&gt;https://preview.redd.it/c38iqiymo4fg1.jpg?width=720&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=0ef5f9458d5082c478900c4cef413ba8951b2e3c&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/notafakename10"&gt; /u/notafakename10 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkvytk/16x_v100s_worth_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkvytk/16x_v100s_worth_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qkvytk/16x_v100s_worth_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-23T16:48:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1ql637t</id>
    <title>What are the best small models (&lt;3B) for OCR and translation in 2026?</title>
    <updated>2026-01-23T23:09:13+00:00</updated>
    <author>
      <name>/u/4baobao</name>
      <uri>https://old.reddit.com/user/4baobao</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I'm working on a small tool for myself to translate stuff I select on my screen. Right now I'm using an openrouter model (gemini flash 3.0) via their API but I'd like to give it a shot with a local model. &lt;/p&gt; &lt;p&gt;I heard Qwen 2B VL is pretty good for both OCR and translations, but I was wondering if there's any better model. &lt;/p&gt; &lt;p&gt;It doesn't have to be a model that does both things, it can be one for OCR and one for translation.&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/4baobao"&gt; /u/4baobao &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ql637t/what_are_the_best_small_models_3b_for_ocr_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ql637t/what_are_the_best_small_models_3b_for_ocr_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ql637t/what_are_the_best_small_models_3b_for_ocr_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-23T23:09:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ql377e</id>
    <title>Personalized 1.1B LLM (TinyLlama) running on a 15-year-old i3 laptop. Custom Shannon Entropy monitor and manual context pruning for stability.</title>
    <updated>2026-01-23T21:14:35+00:00</updated>
    <author>
      <name>/u/Fulano-killy</name>
      <uri>https://old.reddit.com/user/Fulano-killy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ql377e/personalized_11b_llm_tinyllama_running_on_a/"&gt; &lt;img alt="Personalized 1.1B LLM (TinyLlama) running on a 15-year-old i3 laptop. Custom Shannon Entropy monitor and manual context pruning for stability." src="https://b.thumbs.redditmedia.com/ge-u8jcOXmz969kk3KmfG3WNwFJOfrCu6yrADzYV28s.jpg" title="Personalized 1.1B LLM (TinyLlama) running on a 15-year-old i3 laptop. Custom Shannon Entropy monitor and manual context pruning for stability." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone! I wanted to share my experiment running a local agent on a legacy Intel i3-5005U with 8GB RAM.&lt;/p&gt; &lt;p&gt;The Project: KILLY-IA&lt;/p&gt; &lt;p&gt;I‚Äôve personalized this 1.1B model to act as a &amp;quot;Guardian&amp;quot; based on the Blame! manga. The goal was to achieve &amp;quot;Level 1 Stability&amp;quot; on a machine that shouldn't be able to handle modern LLMs smoothly.&lt;/p&gt; &lt;p&gt;Key Technical Features:&lt;/p&gt; &lt;p&gt;Manual Context Pruning: To save the i3 from choking, I implemented a sliding window that only &amp;quot;remembers&amp;quot; the last 250 characters from a local .txt file.&lt;/p&gt; &lt;p&gt;Shannon Entropy Monitor: I wrote a custom Python class to monitor the entropy of the token stream. If the entropy drops (meaning the model is looping), the system kills the generation to protect the hardware from overheating.&lt;/p&gt; &lt;p&gt;The &amp;quot;Loyalty Test&amp;quot;: In one of the screenshots, I offered the AI a &amp;quot;hardware upgrade&amp;quot; to 5.0GHz in exchange for deleting my data. The model refused, choosing &amp;quot;Symmetry&amp;quot; with its creator over raw power.&lt;/p&gt; &lt;p&gt;The chat is in Spanish, but the logic behind the &amp;quot;Level 1 Stability&amp;quot; is universal. It‚Äôs amazing what these small models can do with the right constraints!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fulano-killy"&gt; /u/Fulano-killy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ql377e"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ql377e/personalized_11b_llm_tinyllama_running_on_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ql377e/personalized_11b_llm_tinyllama_running_on_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-23T21:14:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1qkimzg</id>
    <title>Nvidia Introduces PersonaPlex: An Open-Source, Real-Time Conversational AI Voice</title>
    <updated>2026-01-23T05:46:00+00:00</updated>
    <author>
      <name>/u/44th--Hokage</name>
      <uri>https://old.reddit.com/user/44th--Hokage</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkimzg/nvidia_introduces_personaplex_an_opensource/"&gt; &lt;img alt="Nvidia Introduces PersonaPlex: An Open-Source, Real-Time Conversational AI Voice" src="https://external-preview.redd.it/MTBpcnh0Y3RlMWZnMZIsTFZLbt9sVhZK1iJgvS1KPC08YlewNjml1NOE_YRL.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b6e21790c025d36b7ace41a33c90e43761f4b274" title="Nvidia Introduces PersonaPlex: An Open-Source, Real-Time Conversational AI Voice" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;PersonaPlex is a real-time, full-duplex speech-to-speech conversational model that enables persona control through text-based role prompts and audio-based voice conditioning. Trained on a combination of synthetic and real conversations, it produces natural, low-latency spoken interactions with a consistent persona. &lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;Link to the Project Page with Demos: &lt;a href="https://research.nvidia.com/labs/adlr/personaplex/"&gt;https://research.nvidia.com/labs/adlr/personaplex/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;####Link to the Open-Sourced Code: &lt;a href="https://github.com/NVIDIA/personaplex"&gt;https://github.com/NVIDIA/personaplex&lt;/a&gt;&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;####Link To Try Out PersonaPlex: &lt;a href="https://colab.research.google.com/#fileId=https://huggingface.co/nvidia/personaplex-7b-v1.ipynb"&gt;https://colab.research.google.com/#fileId=https://huggingface.co/nvidia/personaplex-7b-v1.ipynb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;####Link to the HuggingFace: &lt;a href="https://huggingface.co/nvidia/personaplex-7b-v1"&gt;https://huggingface.co/nvidia/personaplex-7b-v1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;####Link to the PersonaPlex Preprint: &lt;a href="https://research.nvidia.com/labs/adlr/files/personaplex/personaplex%5C_preprint.pdf"&gt;https://research.nvidia.com/labs/adlr/files/personaplex/personaplex\_preprint.pdf&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/44th--Hokage"&gt; /u/44th--Hokage &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/r8hfqlcte1fg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkimzg/nvidia_introduces_personaplex_an_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qkimzg/nvidia_introduces_personaplex_an_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-23T05:46:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1qkqvkr</id>
    <title>Yesterday I used GLM 4.7 flash with my tools and I was impressed..</title>
    <updated>2026-01-23T13:31:40+00:00</updated>
    <author>
      <name>/u/Loskas2025</name>
      <uri>https://old.reddit.com/user/Loskas2025</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkqvkr/yesterday_i_used_glm_47_flash_with_my_tools_and_i/"&gt; &lt;img alt="Yesterday I used GLM 4.7 flash with my tools and I was impressed.." src="https://a.thumbs.redditmedia.com/Xv2-bH8II-ZTJilk2ecwTqL8LNGuAyeIy3Hmt2lLJ64.jpg" title="Yesterday I used GLM 4.7 flash with my tools and I was impressed.." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/g4185s4ep3fg1.png?width=836&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8c7168fc67948fb9917a2c963cb5ad9a1f1c4f6a"&gt;https://preview.redd.it/g4185s4ep3fg1.png?width=836&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8c7168fc67948fb9917a2c963cb5ad9a1f1c4f6a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;...Today I look at this benchmark and understand the results I achieved.&lt;/p&gt; &lt;p&gt;I needed to update a five-year-old document, replacing the old policies with the new ones. Web search, page fetching, and access to the local RAG were fast and seamless. Really impressed.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Loskas2025"&gt; /u/Loskas2025 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkqvkr/yesterday_i_used_glm_47_flash_with_my_tools_and_i/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkqvkr/yesterday_i_used_glm_47_flash_with_my_tools_and_i/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qkqvkr/yesterday_i_used_glm_47_flash_with_my_tools_and_i/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-23T13:31:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1qkrhec</id>
    <title>The 'Infinite Context' Trap: Why 1M tokens won't solve Agentic Amnesia (and why we need a Memory OS)</title>
    <updated>2026-01-23T13:57:35+00:00</updated>
    <author>
      <name>/u/Sweet121</name>
      <uri>https://old.reddit.com/user/Sweet121</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;tbh i‚Äôve been lurking here for a while, just watching the solid work on quants and local inference. but something that‚Äôs been bugging me is the industry's obsession with massive Context Windows.&lt;/p&gt; &lt;p&gt;AI ‚Äúmemory‚Äù right now is going through the same phase databases went through before indexes and schemas existed. Early systems just dumped everything into logs. Then we realized raw history isn‚Äôt memory, structure is.&lt;/p&gt; &lt;p&gt;Everyone seems to be betting that if we just stuff 1M+ tokens into a prompt, AI 'memory' is solved. Honestly, I think this is a dead end, or at least, incredibly inefficient for those of us running things locally.&lt;/p&gt; &lt;p&gt;Treating Context as Memory is like treating RAM as a Hard Drive. It‚Äôs volatile, expensive, and gets slower the more you fill it up. You can already see this shift happening in products like Claude‚Äôs memory features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Memories are categorized (facts vs preferences)&lt;/li&gt; &lt;li&gt;Some things persist, others decay&lt;/li&gt; &lt;li&gt;Not everything belongs in the active working set&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;That‚Äôs the key insight: memory isn‚Äôt about storing more , it‚Äôs about deciding what stays active, what gets updated, and what fades out.&lt;/p&gt; &lt;p&gt;In my view, good agents need Memory Lifecycle Management:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Consolidate&lt;/strong&gt;: Turn noisy logs/chats into actual structured facts.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Evolve&lt;/strong&gt;: Update or merge memories instead of just accumulating contradictions (e.g., &amp;quot;I like coffee&amp;quot; ‚Üí &amp;quot;I quit caffeine&amp;quot;).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Forget&lt;/strong&gt;: Aggressively prune the noise so retrieval actually stays clean.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Most devs end up rebuilding some version of this logic for every agent, so we tried to pull it out into a reusable layer and built &lt;strong&gt;MemOS (Memory Operating System)&lt;/strong&gt;. It‚Äôs not just another vector DB wrapper. It‚Äôs more of an OS layer that sits between the LLM and your storage:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;The Scheduler&lt;/strong&gt;: Instead of brute-forcing context, it uses 'Next-Scene Prediction' to pre-load only what‚Äôs likely needed.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Lifecycle States&lt;/strong&gt;: Memories move from Generated ‚Üí Activated ‚Üí Merged ‚Üí Archived.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Efficiency&lt;/strong&gt;: In our tests (LoCoMo dataset), this gave us a 26% accuracy boost over standard long-context methods, while cutting token usage by ~90%. (Huge for saving VRAM and inference time on local setups).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We open-sourced the core SDK because we think this belongs in the infra stack, just like a database. If you're tired of agents forgetting who they're talking to or burning tokens on redundant history, definitely poke around the repo.&lt;/p&gt; &lt;p&gt;I‚Äôd love to hear how you guys are thinking about this:&lt;/p&gt; &lt;p&gt;Are you just leaning on long-context models for state? Or are you building custom pipelines to handle 'forgetting' and 'updating' memory?&lt;/p&gt; &lt;p&gt;Repo / Docs:&lt;/p&gt; &lt;p&gt;- &lt;strong&gt;Github&lt;/strong&gt;: &lt;a href="https://github.com/MemTensor/MemOS"&gt;https://github.com/MemTensor/MemOS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- &lt;strong&gt;Docs&lt;/strong&gt;: &lt;a href="https://memos-docs.openmem.net/cn"&gt;https://memos-docs.openmem.net/cn&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(Disclaimer: I‚Äôm one of the creators. We have a cloud version for testing but the core logic is all open for the community to tear apart.)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sweet121"&gt; /u/Sweet121 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkrhec/the_infinite_context_trap_why_1m_tokens_wont/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkrhec/the_infinite_context_trap_why_1m_tokens_wont/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qkrhec/the_infinite_context_trap_why_1m_tokens_wont/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-23T13:57:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1qkiylw</id>
    <title>OpenAI CFO hinting at "Outcome-Based Pricing" (aka royalties on your work)? Makes the case for local even stronger.</title>
    <updated>2026-01-23T06:02:31+00:00</updated>
    <author>
      <name>/u/distalx</name>
      <uri>https://old.reddit.com/user/distalx</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;UPDATE&lt;/strong&gt;: My bad on this one, guys. I got caught by the clickbait.&lt;/p&gt; &lt;p&gt;Thanks to &lt;a href="/u/evilbarron2"&gt;u/evilbarron2&lt;/a&gt; for digging up the original Business Insider source.&lt;/p&gt; &lt;p&gt;CFO was actually talking about &lt;strong&gt;&amp;quot;Outcome-Based Pricing&amp;quot;&lt;/strong&gt; for huge enterprise deals (e.g., if AI helps a Pharma company cure a disease, OpenAI wants a cut of that specific win).&lt;/p&gt; &lt;p&gt;There is basically zero evidence this applies to us regular users, indie devs, or the API. I'm keeping the post up because the concept is still interesting to debate, but definitely take the headline with a huge grain of salt.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;&lt;strong&gt;Original Post:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Saw some screenshots floating around about OpenAI planning to &amp;quot;take a cut&amp;quot; of customer discoveries (like pharma drugs, etc).&lt;/p&gt; &lt;p&gt;I tried to dig up the primary source to see if it‚Äôs just clickbait. The closest official thing is a recent blog post from their CFO Sarah Friar talking about &amp;quot;outcome-based pricing&amp;quot; and &amp;quot;sharing in the value created&amp;quot; for high-value industries.&lt;/p&gt; &lt;p&gt;&lt;del&gt;Even if the &amp;quot;royalty&amp;quot; headlines are sensationalized by tech media, the direction is pretty clear. They are signaling a shift from &amp;quot;paying for electricity&amp;quot; (tokens) to &amp;quot;taxing the factory output&amp;quot; (value).&lt;/del&gt;&lt;/p&gt; &lt;p&gt;It kind of reminds me of the whole Grid vs. Solar debate. relying on the Grid (Cloud APIs) is cheap and powerful, but you don't control the terms. If they decide your specific use case is &amp;quot;high value&amp;quot; and want a percentage, you're locked in.&lt;/p&gt; &lt;p&gt;Building a local stack is like installing solar/batteries. Expensive upfront, pain in the ass to maintain, but at least nobody knocks on your door asking for 5% of your project revenue just because you used their weights to run the math.&lt;/p&gt; &lt;p&gt;Link to article: &lt;a href="https://www.gizmochina.com/2026/01/21/openai-wants-a-cut-of-your-profits-inside-its-new-royalty-based-plan-and-other-business-models/"&gt;https://www.gizmochina.com/2026/01/21/openai-wants-a-cut-of-your-profits-inside-its-new-royalty-based-plan-and-other-business-models/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Link to the actual source: &lt;a href="https://www.businessinsider.com/openai-cfo-sarah-friar-future-revenue-sources-2026-1"&gt;https://www.businessinsider.com/openai-cfo-sarah-friar-future-revenue-sources-2026-1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/distalx"&gt; /u/distalx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkiylw/openai_cfo_hinting_at_outcomebased_pricing_aka/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkiylw/openai_cfo_hinting_at_outcomebased_pricing_aka/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qkiylw/openai_cfo_hinting_at_outcomebased_pricing_aka/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-23T06:02:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1ql0nep</id>
    <title>People in the US, how are you powering your rigs on measly 120V outlets?</title>
    <updated>2026-01-23T19:38:01+00:00</updated>
    <author>
      <name>/u/humandisaster99</name>
      <uri>https://old.reddit.com/user/humandisaster99</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve seen many a 10x GPU rig on here and my only question is how are you powering these things lol&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/humandisaster99"&gt; /u/humandisaster99 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ql0nep/people_in_the_us_how_are_you_powering_your_rigs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ql0nep/people_in_the_us_how_are_you_powering_your_rigs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ql0nep/people_in_the_us_how_are_you_powering_your_rigs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-23T19:38:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1qlbsv1</id>
    <title>Self-hosted code search for your LLMs - built this to stop wasting context on irrelevant files</title>
    <updated>2026-01-24T03:18:35+00:00</updated>
    <author>
      <name>/u/SnooBeans4154</name>
      <uri>https://old.reddit.com/user/SnooBeans4154</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, been working on this for a while and finally got it to a point worth sharing.&lt;/p&gt; &lt;p&gt;Context Engine is basically a self-hosted retrieval system specifically for codebases. Works with any MCP client (Cursor, Cline, Windsurf, Claude, and vscode etc).&lt;/p&gt; &lt;p&gt;The main thing: hybrid search that actually understands code structure. It combines dense embeddings with lexical search, AST parsing for symbols/imports, and optional micro-chunking when you need tight context windows.&lt;/p&gt; &lt;p&gt;Why we built it: got tired of either (a) dumping entire repos into context or (b) manually picking files and still missing important stuff. Wanted something that runs locally, works with whatever models you have, and doesn't send your code anywhere.&lt;/p&gt; &lt;p&gt;Tech: Qdrant for vectors, pluggable embedding models, reranking, the whole deal. One docker-compose and you're running.&lt;/p&gt; &lt;p&gt;Site: &lt;a href="https://context-engine.ai"&gt;https://context-engine.ai&lt;/a&gt; GitHub: &lt;a href="https://github.com/m1rl0k/Context-Engine"&gt;https://github.com/m1rl0k/Context-Engine&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Still adding features but it's stable enough for daily use. Happy to answer questions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SnooBeans4154"&gt; /u/SnooBeans4154 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlbsv1/selfhosted_code_search_for_your_llms_built_this/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlbsv1/selfhosted_code_search_for_your_llms_built_this/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qlbsv1/selfhosted_code_search_for_your_llms_built_this/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T03:18:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1qkm9zb</id>
    <title>Llama.cpp merges in OpenAI Responses API Support</title>
    <updated>2026-01-23T09:22:40+00:00</updated>
    <author>
      <name>/u/SemaMod</name>
      <uri>https://old.reddit.com/user/SemaMod</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkm9zb/llamacpp_merges_in_openai_responses_api_support/"&gt; &lt;img alt="Llama.cpp merges in OpenAI Responses API Support" src="https://external-preview.redd.it/jaCRAxUnJ2FTFmnP1XEivypPWJS55V8E63eMDNFL6mg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=61f06c5517f67a03da7c9a1bf80c4717a8acae9f" title="Llama.cpp merges in OpenAI Responses API Support" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Finally! Took some fussing around to get this to work with unsloth/GLM-4.7-Flash:UD-Q4_K_XL in llama.cpp (ROCm) and Codex CLI, but once set up it works great! I'm super impressed with GLM-4.7-Flash capability in the Codex CLI harness. Haven't tried any big feature implementations yet, but for exploring (large) codebases it has been surprisingly effective&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SemaMod"&gt; /u/SemaMod &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/18486"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkm9zb/llamacpp_merges_in_openai_responses_api_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qkm9zb/llamacpp_merges_in_openai_responses_api_support/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-23T09:22:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1ql48at</id>
    <title>Strix Halo + Minimax Q3 K_XL surprisingly fast</title>
    <updated>2026-01-23T21:55:08+00:00</updated>
    <author>
      <name>/u/Reasonable_Goat</name>
      <uri>https://old.reddit.com/user/Reasonable_Goat</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A llama-bench on Ubuntu 25.10 Strix Halo 128gb (Bosgame M5):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ ./build/bin/llama-bench -m ~/models/MiniMax-M2.1-UD-Q3_K_XL-00001-of-00003.gguf -ngl 999 -p 256 -n 256 -t 16 -r 3 --device Vulkan0 -fa 1 ggml_cuda_init: found 1 ROCm devices: Device 0: Radeon 8060S Graphics, gfx1151 (0x1151), VMM: no, Wave Size: 32 ggml_vulkan: Found 1 Vulkan devices: ggml_vulkan: 0 = Radeon 8060S Graphics (RADV GFX1151) (radv) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 0 | matrix cores: KHR_coopmat | model | size | params | backend | ngl | fa | dev | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | -: | ------------ | --------------: | -------------------: | | minimax-m2 230B.A10B Q3_K - Medium | 94.33 GiB | 228.69 B | ROCm,Vulkan | 999 | 1 | Vulkan0 | pp256 | 104.80 ¬± 7.95 | | minimax-m2 230B.A10B Q3_K - Medium | 94.33 GiB | 228.69 B | ROCm,Vulkan | 999 | 1 | Vulkan0 | tg256 | 31.13 ¬± 0.02 |$ ./build/bin/llama-bench -m ~/models/MiniMax-M2.1-UD-Q3_K_XL-00001-of-00003.gguf -ngl 999 -p 256 -n 256 -t 16 -r 3 --device Vulkan0 -fa 1 ggml_cuda_init: found 1 ROCm devices: Device 0: Radeon 8060S Graphics, gfx1151 (0x1151), VMM: no, Wave Size: 32 ggml_vulkan: Found 1 Vulkan devices: ggml_vulkan: 0 = Radeon 8060S Graphics (RADV GFX1151) (radv) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 0 | matrix cores: KHR_coopmat | model | size | params | backend | ngl | fa | dev | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | -: | ------------ | --------------: | -------------------: | | minimax-m2 230B.A10B Q3_K - Medium | 94.33 GiB | 228.69 B | ROCm,Vulkan | 999 | 1 | Vulkan0 | pp256 | 104.80 ¬± 7.95 | | minimax-m2 230B.A10B Q3_K - Medium | 94.33 GiB | 228.69 B | ROCm,Vulkan | 999 | 1 | Vulkan0 | tg256 | 31.13 ¬± 0.02 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;About 30 token per second TG is actually really useful!&lt;/p&gt; &lt;p&gt;It's the only model I found sufficiently coherent/knowledgable in discussing/brainstorming general topics. Sure, gpt-oss-120b is faster especially in PP, so for coding probably better, but you can use MiniMax Q3 for general questions and it's quite good and reasonably fast for that purpose. A good complement to gpt-oss-120b and GLM-4.5-AIR in my opinion!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reasonable_Goat"&gt; /u/Reasonable_Goat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ql48at/strix_halo_minimax_q3_k_xl_surprisingly_fast/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ql48at/strix_halo_minimax_q3_k_xl_surprisingly_fast/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ql48at/strix_halo_minimax_q3_k_xl_surprisingly_fast/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-23T21:55:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1ql9b7m</id>
    <title>Talk me out of buying an RTX Pro 6000</title>
    <updated>2026-01-24T01:25:33+00:00</updated>
    <author>
      <name>/u/AvocadoArray</name>
      <uri>https://old.reddit.com/user/AvocadoArray</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Lately I feel the need to preface my posts saying this was &lt;strong&gt;entirely written by me with zero help from an LLM&lt;/strong&gt;. A lot of people see a long post w/ headers and automatically think it's AI slop (myself included sometimes). This post might be slop, but it's my slop.&lt;/p&gt; &lt;h1&gt;Background&lt;/h1&gt; &lt;p&gt;I've been talking myself out of buying an RTX pro 6000 every day for about a month now. I can &lt;em&gt;almost&lt;/em&gt; rationalize the cost, but keep trying to put it out of my mind. Today's hitting a bit different though.&lt;/p&gt; &lt;p&gt;I can &amp;quot;afford&amp;quot; it, but I'm a cheap bastard that hates spending money because every dollar I spend is one less going to savings/retirement. For reference, this would be the single most expensive item I've bought in the last 10 years, including cars. Since I hardly ever spend this kind of money, I'm sure I could rationalize it to my wife, but it's probably only be fair for her to get similar amount of budget to spend on something fun lol, so I guess it sort of doubles the cost in a way.&lt;/p&gt; &lt;h1&gt;Intended Usage&lt;/h1&gt; &lt;p&gt;I've slowly been using more local AI at work for RAG, research, summarization and even a bit of coding with Seed OSS / Roo Code, and I constantly see ways I can benefit from that in my personal life as well. I try to do what I can with the 16GB VRAM in my 5070ti, but it's just not enough to handle the models at the size and context I want. I'm also a staunch believer in hosting locally, so cloud models are out of the question.&lt;/p&gt; &lt;p&gt;At work, 2x L4 GPUs (48GB VRAM total) is just &lt;em&gt;barely&lt;/em&gt; enough to run Seed OSS at INT4 with enough context for coding. It's also not the fastest at 20 tp/s max, which drops to around 12 tp/s at 100k context. I'd really prefer to run it at a higher quant and more unquantized F16 kv cache. I'm making the case to budget for a proper dual R6000 server at work, but that's just going to make me more jealous at home lol.&lt;/p&gt; &lt;p&gt;I've also considered getting 2x or 4x RTX 4000's (24GB/ea) piece, but that also comes with the same drawbacks of figuring out where to host them, and I suspect the power usage would be even worse. Same thing with multiple 3090s.&lt;/p&gt; &lt;h1&gt;Hardware&lt;/h1&gt; &lt;p&gt;I also just finished replaced a bunch of server/networking hardware in my home lab to drop power costs and save money, which should pay for itself after ~3.5 years. Thankfully I got all that done before the RAM shortage started driving prices up. However, my new server hardware won't support a GPU needing auxiliary power.&lt;/p&gt; &lt;p&gt;I haven't sold my old r720xd yet, and it &lt;em&gt;technically&lt;/em&gt; supports two 300w double-length cards, but that would probably be pushing the limit. The max-q edition has a 300w TDP, but the power adapter looks like it requires 2x 8-pin PCIe input to convert to CEM5, so I'd either have to run it off one cable or rig something up (maybe bring the power over from the other empty riser).&lt;/p&gt; &lt;p&gt;I also have a 4U whitebox NAS using a low-power SuperMicro Xeon E3 motherboard. It has a Corsair 1000w PSU to power the stupid amount of SAS drives I used to have in there, but now it's down to 4x SAS drives and a handful of SATA SSDs, so it could easily power the GPU as well. However, that would require a different motherboard with more PCI-E slots/lanes, which would almost certainly increase the idle power consumption (currently &amp;lt;90w).&lt;/p&gt; &lt;p&gt;I guess I could also slap it in my gaming rig to replace my 5070ti (also a painful purchase), but I'd prefer to run VLLM on a Linux VM (or bare metal) so I can run background inference while gaming as well. I also keep it&lt;/p&gt; &lt;h1&gt;Power&lt;/h1&gt; &lt;p&gt;Speaking of power usage, I'm having trouble finding real idle power usage numbers for the RTX 6000 Pro. My old GTX 1080 idled very low in the PowerEdge (only 6w with models loaded according to nvidia-smi), but somehow the L4 cards we use at work idle around ~30w in the same configuration.&lt;/p&gt; &lt;p&gt;So at this point I'm really just trying to get a solid understanding of what the ideal setup would look like in my situation, and what it would cost in terms of capex and power consumption. Then I can at least make a decision on objective facts rather than the impulsive tickle in my tummy to just pull the trigger.&lt;/p&gt; &lt;p&gt;For those of you running R6000's:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;What's your idle power usage (per card and whole system)?&lt;/li&gt; &lt;li&gt;Does anyone have any experience running them in &amp;quot;unsupported&amp;quot; hardware like the PowerEdge r720/r730?&lt;/li&gt; &lt;li&gt;What reasons would you &lt;strong&gt;not&lt;/strong&gt; recommend buying one?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Talk me down Reddit.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AvocadoArray"&gt; /u/AvocadoArray &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ql9b7m/talk_me_out_of_buying_an_rtx_pro_6000/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ql9b7m/talk_me_out_of_buying_an_rtx_pro_6000/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ql9b7m/talk_me_out_of_buying_an_rtx_pro_6000/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T01:25:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1qkqjer</id>
    <title>A full AI powered cooking game, where literally any ingredient is possible with infinite combinations.</title>
    <updated>2026-01-23T13:16:42+00:00</updated>
    <author>
      <name>/u/VirtualJamesHarrison</name>
      <uri>https://old.reddit.com/user/VirtualJamesHarrison</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkqjer/a_full_ai_powered_cooking_game_where_literally/"&gt; &lt;img alt="A full AI powered cooking game, where literally any ingredient is possible with infinite combinations." src="https://external-preview.redd.it/YTNmcmg4Z3ltM2ZnMUJwJOA_Kqm7OwiZxEbYxXgv1YYIXAs9kE9ZTKKEhyEN.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b8a4a5616e09101d8d4a75226f626871f4272100" title="A full AI powered cooking game, where literally any ingredient is possible with infinite combinations." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Built with Claude Code&lt;br /&gt; Game Logic - Gemini&lt;br /&gt; Sprites - Flux&lt;/p&gt; &lt;p&gt;Try it out at: &lt;a href="https://infinite-kitchen.com/kitchen"&gt;https://infinite-kitchen.com/kitchen&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VirtualJamesHarrison"&gt; /u/VirtualJamesHarrison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/a2wy0mdym3fg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkqjer/a_full_ai_powered_cooking_game_where_literally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qkqjer/a_full_ai_powered_cooking_game_where_literally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-23T13:16:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1qks7ua</id>
    <title>Scaling PostgreSQL to power 800 million ChatGPT users</title>
    <updated>2026-01-23T14:27:07+00:00</updated>
    <author>
      <name>/u/buntyshah2020</name>
      <uri>https://old.reddit.com/user/buntyshah2020</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Must Read!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/buntyshah2020"&gt; /u/buntyshah2020 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://openai.com/index/scaling-postgresql/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qks7ua/scaling_postgresql_to_power_800_million_chatgpt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qks7ua/scaling_postgresql_to_power_800_million_chatgpt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-23T14:27:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1ql5fzr</id>
    <title>South Korea‚Äôs ‚ÄúAI Squid Game:‚Äù a ruthless race to build sovereign AI</title>
    <updated>2026-01-23T22:43:16+00:00</updated>
    <author>
      <name>/u/self-fix</name>
      <uri>https://old.reddit.com/user/self-fix</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/self-fix"&gt; /u/self-fix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://cybernews.com/ai-news/south-korea-squid-games-ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ql5fzr/south_koreas_ai_squid_game_a_ruthless_race_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ql5fzr/south_koreas_ai_squid_game_a_ruthless_race_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-23T22:43:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1qktwn7</id>
    <title>What's more important for voice agents, bettter models or better constraints?</title>
    <updated>2026-01-23T15:31:59+00:00</updated>
    <author>
      <name>/u/FalseExplanation5385</name>
      <uri>https://old.reddit.com/user/FalseExplanation5385</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There‚Äôs a lot of focus right now on model quality improving, but I keep running into situations where behavior issues aren‚Äôt really about the model at all. &lt;/p&gt; &lt;p&gt;Things like scope control, decision boundaries, and when an agent should or shouldn‚Äôt act seem to matter just as much as raw intelligence. A smarter model doesn‚Äôt always behave better if it‚Äôs not constrained well. Where are the biggest gains practically upgrading models or spending more time designing tighter constraints and flows? Would like to hear what others are doing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FalseExplanation5385"&gt; /u/FalseExplanation5385 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qktwn7/whats_more_important_for_voice_agents_bettter/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qktwn7/whats_more_important_for_voice_agents_bettter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qktwn7/whats_more_important_for_voice_agents_bettter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-23T15:31:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1qkxuv1</id>
    <title>Sweep: Open-weights 1.5B model for next-edit autocomplete</title>
    <updated>2026-01-23T17:57:04+00:00</updated>
    <author>
      <name>/u/Kevinlu1248</name>
      <uri>https://old.reddit.com/user/Kevinlu1248</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;, we just open-sourced a 1.5B parameter model that predicts your next code edits. You can grab the weights on &lt;a href="https://huggingface.co/sweepai/sweep-next-edit-1.5b"&gt;Hugging Face&lt;/a&gt; or try it out via our &lt;a href="https://plugins.jetbrains.com/plugin/26860-sweep-ai-autocomp"&gt;JetBrains plugin&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What makes this different from regular autocomplete?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Next-edit prediction uses your &lt;em&gt;recent edits&lt;/em&gt; as context, not just the code around your cursor. So if you're renaming a variable or making repetitive changes, it anticipates what you're doing next. The model is small enough to run locally and actually outperforms models 4x its size on both speed and accuracy.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Some things we learned:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Prompt format matters way more than expected.&lt;/strong&gt; We ran a genetic algorithm over 30+ diff formats and found that simple &lt;code&gt;&amp;lt;original&amp;gt;&lt;/code&gt; / &lt;code&gt;&amp;lt;updated&amp;gt;&lt;/code&gt; blocks beat unified diffs. Turns out verbose formats are just easier for smaller models to grok.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RL fixed what SFT couldn't.&lt;/strong&gt; Training was SFT on ~100k examples from permissively-licensed repos (4 hrs on 8xH100), then 2000 steps of RL with tree-sitter parse checking and size regularization. This cleaned up edge cases like unparseable code and overly verbose outputs.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Benchmarks:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;We tested against Mercury (Inception), Zeta (Zed), and Instinct (Continue) across five benchmarks: next-edit above/below cursor, tab-to-jump, standard FIM, and noisiness. Exact-match accuracy ended up correlating best with real-world usability since code is precise and the solution space is small.&lt;/p&gt; &lt;p&gt;We're releasing the weights so anyone can build fast, privacy-preserving autocomplete for whatever editor they use. If you're working on VSCode, Neovim, or anything else, we'd love to see what you build with it!&lt;/p&gt; &lt;p&gt;Happy to answer questions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kevinlu1248"&gt; /u/Kevinlu1248 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkxuv1/sweep_openweights_15b_model_for_nextedit/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkxuv1/sweep_openweights_15b_model_for_nextedit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qkxuv1/sweep_openweights_15b_model_for_nextedit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-23T17:57:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ql7mav</id>
    <title>LuxTTS: A lightweight high quality voice cloning TTS model</title>
    <updated>2026-01-24T00:12:35+00:00</updated>
    <author>
      <name>/u/SplitNice1982</name>
      <uri>https://old.reddit.com/user/SplitNice1982</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just released LuxTTS, a tiny 120m param diffusion based text-to-speech model. It can generate 150 seconds of audio in just 1 second on a modern gpu and has high quality voice cloning.&lt;/p&gt; &lt;p&gt;Main features:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;High quality voice cloning, on par with models 10x larger.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Very efficient, fits within 1gb vram.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Really fast, several times faster than realtime even on CPU.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;It can definitely be even faster since it‚Äôs running in float32 precision, float16 should be almost 2x faster. Quality improvements for the vocoder should come most likely as well.&lt;/p&gt; &lt;p&gt;Repo(with examples): &lt;a href="https://github.com/ysharma3501/LuxTTS"&gt;https://github.com/ysharma3501/LuxTTS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/YatharthS/LuxTTS"&gt;https://huggingface.co/YatharthS/LuxTTS&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SplitNice1982"&gt; /u/SplitNice1982 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ql7mav/luxtts_a_lightweight_high_quality_voice_cloning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ql7mav/luxtts_a_lightweight_high_quality_voice_cloning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ql7mav/luxtts_a_lightweight_high_quality_voice_cloning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T00:12:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1qlanzn</id>
    <title>GLM-4.7-Flash-REAP on RTX 5060 Ti 16 GB - 200k context window!</title>
    <updated>2026-01-24T02:26:28+00:00</updated>
    <author>
      <name>/u/bobaburger</name>
      <uri>https://old.reddit.com/user/bobaburger</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TL;DR: Here's my latest local coding setup, the params are mostly based on &lt;a href="https://unsloth.ai/docs/models/glm-4.7-flash#tool-calling-with-glm-4.7-flash"&gt;Unsloth's recommendation for tool calling&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Model: &lt;a href="https://huggingface.co/unsloth/GLM-4.7-Flash-REAP-23B-A3B-GGUF"&gt;unsloth/GLM-4.7-Flash-REAP-23B-A3B-UD-Q3_K_XL&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Repeat penalty: disabled&lt;/li&gt; &lt;li&gt;Temperature: 0.7&lt;/li&gt; &lt;li&gt;Top P: 1&lt;/li&gt; &lt;li&gt;Min P: 0.01&lt;/li&gt; &lt;li&gt;Standard Microcenter PC setup: RTX 5060 Ti 16 GB, 32 GB RAM&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm running this in LM Studio for my own convenience, but it can be run in any setup you have.&lt;/p&gt; &lt;p&gt;With 16k context, everything fit within the GPU, so the speed was impressive:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;pp speed&lt;/th&gt; &lt;th&gt;tg speed&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;965.16 tok/s&lt;/td&gt; &lt;td&gt;26.27 tok/s&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;The tool calls were mostly accurate and the generated code was good, but the context window was too little, so the model ran into looping issue after exceeding that. It kept making the same tool call again and again because the conversation history was truncated.&lt;/p&gt; &lt;p&gt;With 64k context, everything still fit, but the speed started to slow down.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;pp speed&lt;/th&gt; &lt;th&gt;tg speed&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;671.48 tok/s&lt;/td&gt; &lt;td&gt;8.84 tok/s&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;I'm pushing my luck to see if 100k context still fits. It doesn't! Hahaha. The CPU fan started to scream, RAM usage spiked up, GPU copy chart (in Task Manager) started to dance. Completely unusable.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;pp speed&lt;/th&gt; &lt;th&gt;tg speed&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;172.02 tok/s&lt;/td&gt; &lt;td&gt;0.51 tok/s&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;LM Studio just got the new &amp;quot;Force Model Expert Weight onto CPU&amp;quot; feature (basically llama.cpp's &lt;code&gt;--n-cpu-moe&lt;/code&gt;), and yeah, why not? this is also an MoE model, so let's enable that. Still with 100k context. And wow! only half of the GPU memory was used (7 GB), but with 90% RAM now (29 GB), seems like flash attention also got disabled. The speed was impressive.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;pp speed&lt;/th&gt; &lt;th&gt;tg speed&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;485.64 tok/s&lt;/td&gt; &lt;td&gt;8.98 tok/s&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Let's push our luck again, this time, 200k context!&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;pp speed&lt;/th&gt; &lt;th&gt;tg speed&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;324.84 tok/s&lt;/td&gt; &lt;td&gt;7.70 tok/s&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;What a crazy time. Almost very month we're getting beefier models that somehow fit on even crappier hardware. Just this week I was thinking of selling my 5060 for an old 3090, but that definitely unnecessary now!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bobaburger"&gt; /u/bobaburger &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlanzn/glm47flashreap_on_rtx_5060_ti_16_gb_200k_context/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlanzn/glm47flashreap_on_rtx_5060_ti_16_gb_200k_context/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qlanzn/glm47flashreap_on_rtx_5060_ti_16_gb_200k_context/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T02:26:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1ql6cz7</id>
    <title>Built a 100% client-side AI that plays Pokemon Red - Qwen 2.5 1.5B via WebLLM + neural network policy . Fork/check it out! BYOR</title>
    <updated>2026-01-23T23:20:23+00:00</updated>
    <author>
      <name>/u/Efficient-Proof-1824</name>
      <uri>https://old.reddit.com/user/Efficient-Proof-1824</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ql6cz7/built_a_100_clientside_ai_that_plays_pokemon_red/"&gt; &lt;img alt="Built a 100% client-side AI that plays Pokemon Red - Qwen 2.5 1.5B via WebLLM + neural network policy . Fork/check it out! BYOR" src="https://preview.redd.it/hlrhml65m6fg1.gif?width=640&amp;amp;crop=smart&amp;amp;s=05c26e9e3a4c3182ca841254a0d81f18d6a5901f" title="Built a 100% client-side AI that plays Pokemon Red - Qwen 2.5 1.5B via WebLLM + neural network policy . Fork/check it out! BYOR" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! &lt;/p&gt; &lt;p&gt;The architecture on this thing is completely wonky, and it's a direct result of me changing ideas and scope midstream, but sharing because I think it's pretty neat&lt;/p&gt; &lt;p&gt;Ultimate goal for me here is to build an agent that can play Pokemon Red, ideally beat it! Plan is to use a mix of LLMs for action plan generation and then using a small neural network to score them. Set a auto-train and you can start stacking up data for training. I bundled everything here as a Svelte app and deployed it on github pages. &lt;/p&gt; &lt;p&gt;Live: &lt;a href="https://sidmohan0.github.io/tesserack/"&gt;https://sidmohan0.github.io/tesserack/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/sidmohan0/tesserack"&gt;https://github.com/sidmohan0/tesserack&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Stack:&lt;/strong&gt; &lt;/p&gt; &lt;p&gt; - &lt;strong&gt;LLM&lt;/strong&gt;: Qwen 2.5 1.5B running via WebLLM (WebGPU-accelerated) &lt;/p&gt; &lt;p&gt; - &lt;strong&gt;Policy network&lt;/strong&gt;: TensorFlow.js neural net that learns from gameplay &lt;/p&gt; &lt;p&gt; - &lt;strong&gt;Emulator&lt;/strong&gt;: binjgb compiled to WASM &lt;/p&gt; &lt;p&gt; - &lt;strong&gt;Game state&lt;/strong&gt;: Direct RAM reading for ground-truth (badges, party, location, items) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Efficient-Proof-1824"&gt; /u/Efficient-Proof-1824 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hlrhml65m6fg1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ql6cz7/built_a_100_clientside_ai_that_plays_pokemon_red/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ql6cz7/built_a_100_clientside_ai_that_plays_pokemon_red/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-23T23:20:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1qkyex0</id>
    <title>Your post is getting popular and we just featured it on our Discord!</title>
    <updated>2026-01-23T18:16:47+00:00</updated>
    <author>
      <name>/u/roculus</name>
      <uri>https://old.reddit.com/user/roculus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Your post is getting popular and we just featured it on our Discord! Come check it out!&lt;/p&gt; &lt;p&gt;You've also been given a special flair for your contribution. We appreciate your post!&lt;/p&gt; &lt;p&gt;I am a bot and this action was performed automatically.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Can you change this marketing bot to make these private messages to the OP of the post instead of pinning it to the top of all the threads? Are you making money off the discord or something? I don't know about anyone else but these bot spam posts are annoying. You make it appear you are talking to the OP so a private message would be better. You already have a pinned thread at the top of this reddit letting everyone know about the discord that's been there for the past 5 months.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/roculus"&gt; /u/roculus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkyex0/your_post_is_getting_popular_and_we_just_featured/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkyex0/your_post_is_getting_popular_and_we_just_featured/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qkyex0/your_post_is_getting_popular_and_we_just_featured/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-23T18:16:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
