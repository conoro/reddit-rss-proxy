<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-07T08:25:19+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1pfxrv5</id>
    <title>Convert Dense into MOE model?</title>
    <updated>2025-12-06T19:30:30+00:00</updated>
    <author>
      <name>/u/pmttyji</name>
      <uri>https://old.reddit.com/user/pmttyji</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I did a quick search on this here &amp;amp; found only 2 years old &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1cgo45x/converting_dense_models_into_moes/"&gt;thread&lt;/a&gt; with less replies. That's it.&lt;/p&gt; &lt;p&gt;So still no one figured it out this yet? Totally surprised that no one brought this topic here after that old thread.&lt;/p&gt; &lt;p&gt;I know it's a very big thing. But it would be a miracle if some one comes with this precious solution.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmttyji"&gt; /u/pmttyji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfxrv5/convert_dense_into_moe_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfxrv5/convert_dense_into_moe_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfxrv5/convert_dense_into_moe_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T19:30:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1pg0jrn</id>
    <title>Built an offline voice-to-text tool for macOS using Parakeet</title>
    <updated>2025-12-06T21:29:26+00:00</updated>
    <author>
      <name>/u/_gordonclark</name>
      <uri>https://old.reddit.com/user/_gordonclark</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pg0jrn/built_an_offline_voicetotext_tool_for_macos_using/"&gt; &lt;img alt="Built an offline voice-to-text tool for macOS using Parakeet" src="https://external-preview.redd.it/aO9ax1823LtOZGcI0aK2n4il89o5Hg6Q3c3cEopkN5Y.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dc887f665d70ccae44e7f3342c50b34faf6196e4" title="Built an offline voice-to-text tool for macOS using Parakeet" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been tinkering on a little side project called &lt;strong&gt;SilentKeys&lt;/strong&gt; and figured I‚Äôd share it here in case anyone finds it useful. &lt;/p&gt; &lt;p&gt;It‚Äôs basically &lt;strong&gt;realtime offline dictation for macOS&lt;/strong&gt;. No cloud, no accounts, nothing sent anywhere, it just listens locally and types straight into whatever app you have open. I built it because I wanted dictation that didn‚Äôt ship my voice to a server.&lt;/p&gt; &lt;p&gt;It‚Äôs still early and a bit rough around the edges, but it works surprisingly well. If you‚Äôre into privacy tools, voice workflows, accessibility stuff, or just like trying weird niche projects, I‚Äôd love to hear what you think.&lt;/p&gt; &lt;p&gt;Repo‚Äôs here: &lt;a href="https://github.com/gptguy/silentkeys"&gt;https://github.com/gptguy/silentkeys&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer questions or get roasted gently.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_gordonclark"&gt; /u/_gordonclark &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/gptguy/silentkeys"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pg0jrn/built_an_offline_voicetotext_tool_for_macos_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pg0jrn/built_an_offline_voicetotext_tool_for_macos_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T21:29:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1pg56he</id>
    <title>Need recommendations on training datasets</title>
    <updated>2025-12-07T00:58:59+00:00</updated>
    <author>
      <name>/u/Theotheraccounti_</name>
      <uri>https://old.reddit.com/user/Theotheraccounti_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello. I've built a model that is based on the Mixture of a Million Experts paper and trained on tinystories. &lt;/p&gt; &lt;p&gt;The thing is that I'd like to test it against models of a similar size to see if the architecture is actually good and I need a good dataset to train it on. Preferably one that is small and in question-answer pairs.&lt;/p&gt; &lt;p&gt;&lt;em&gt;I cannot use a big dataset due to being on a free colab account. *&lt;/em&gt;apologies if my english is kind of bad right now.&lt;/p&gt; &lt;p&gt;Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Theotheraccounti_"&gt; /u/Theotheraccounti_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pg56he/need_recommendations_on_training_datasets/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pg56he/need_recommendations_on_training_datasets/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pg56he/need_recommendations_on_training_datasets/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-07T00:58:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1pgc21j</id>
    <title>Stop making Agents guess pixels. I built a UI layer that exposes the "Hidden Business Domain" directly to the LLM (Intent-to-State).</title>
    <updated>2025-12-07T07:03:41+00:00</updated>
    <author>
      <name>/u/TraditionalListen994</name>
      <uri>https://old.reddit.com/user/TraditionalListen994</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgc21j/stop_making_agents_guess_pixels_i_built_a_ui/"&gt; &lt;img alt="Stop making Agents guess pixels. I built a UI layer that exposes the &amp;quot;Hidden Business Domain&amp;quot; directly to the LLM (Intent-to-State)." src="https://external-preview.redd.it/vQdoDtDvLlqWi5KmQGIcOqupsHmYAS7aibv0sIKnFts.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=973c6d4d79cf5dc4550ac689898fa9b0a85b0c0d" title="Stop making Agents guess pixels. I built a UI layer that exposes the &amp;quot;Hidden Business Domain&amp;quot; directly to the LLM (Intent-to-State)." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://i.redd.it/ng27lgf6fq5g1.gif"&gt;https://i.redd.it/ng27lgf6fq5g1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Real Problem:&lt;/strong&gt; We are trying to build Agents that use our software, but we give them the worst possible interface: &lt;strong&gt;The DOM.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The DOM only tells you &lt;em&gt;what&lt;/em&gt; is on the screen (pixels/tags). It doesn't tell you &lt;em&gt;why&lt;/em&gt; it's there.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Why is this button disabled? (Is it a permission issue? Or missing data?)&lt;/li&gt; &lt;li&gt;Why did this field suddenly appear? (Business rule dependency?)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This &amp;quot;Business Domain Logic&amp;quot; is usually hidden inside spaghetti code (&lt;code&gt;useEffect&lt;/code&gt;, backend validations), leaving the Agent to blindly guess and hallucinate.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Solution: Exposing the Domain Layer&lt;/strong&gt; I built &lt;strong&gt;Manifesto&lt;/strong&gt; (Open Source) to solve this. It extracts the &lt;strong&gt;Hidden Business Domain&lt;/strong&gt; and feeds it to the Agent as a structured JSON Schema.&lt;/p&gt; &lt;p&gt;Instead of just &amp;quot;seeing&amp;quot; a form, the Agent receives a &lt;strong&gt;Semantic State Snapshot&lt;/strong&gt; that explicitly declares:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Dependencies:&lt;/strong&gt; &lt;em&gt;&amp;quot;Field B is visible ONLY because Field A is 'Enterprise'.&amp;quot;&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Constraints:&lt;/strong&gt; &lt;em&gt;&amp;quot;This action is invalid right now because the user lacks 'Admin' role.&amp;quot;&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;State Machines:&lt;/strong&gt; &lt;em&gt;&amp;quot;Current status is 'Draft', so only 'Save' is allowed, 'Publish' is blocked.&amp;quot;&lt;/em&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;The Result:&lt;/strong&gt; The Agent doesn't act like a blind user clicking coordinates. It acts like a &lt;strong&gt;Domain Expert&lt;/strong&gt;. It understands the &lt;em&gt;rules of the game&lt;/em&gt; before it makes a move.&lt;/p&gt; &lt;p&gt;This turns the UI from a &amp;quot;Visual Challenge&amp;quot; into a &lt;strong&gt;Deterministic API&lt;/strong&gt; for your Agent.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Status:&lt;/strong&gt; I'm curious if this &amp;quot;Domain-First&amp;quot; approach aligns with how you guys are building local agentic workflows.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Repo:&lt;/strong&gt; &lt;a href="https://github.com/manifesto-ai/core"&gt;https://github.com/manifesto-ai/core&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Demo:&lt;/strong&gt; &lt;a href="https://playground.manifesto-ai.dev"&gt;https://playground.manifesto-ai.dev&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TraditionalListen994"&gt; /u/TraditionalListen994 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgc21j/stop_making_agents_guess_pixels_i_built_a_ui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgc21j/stop_making_agents_guess_pixels_i_built_a_ui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pgc21j/stop_making_agents_guess_pixels_i_built_a_ui/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-07T07:03:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1pgcx1x</id>
    <title>Repurposing old 15‚Äù MacBook Pro (16 GB RAM) for local LLMs ‚Äì best Linux distro, models, and possible eGPU?</title>
    <updated>2025-12-07T07:58:34+00:00</updated>
    <author>
      <name>/u/ba5av</name>
      <uri>https://old.reddit.com/user/ba5av</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have an older 15‚Äù MacBook Pro with 16 GB RAM that I‚Äôm thinking of repurposing purely for experimenting with local LLMs. Current status: ‚Ä¢ macOS 11.6.4 ‚Ä¢ 16 GB RAM, i7/i9 Intel CPU (15‚Äù model) ‚Ä¢ RAM is not upgradeable and GPU is fixed, but the machine has Thunderbolt 3 so an eGPU might be possible. My goals: ‚Ä¢ Install a lean Linux distro (or maybe stay on macOS) and run small, quantized LLMs locally. ‚Ä¢ Use it mainly for coding assistance, tinkering with open‚Äësource models, and learning about local deployment. ‚Ä¢ I‚Äôm okay with slower inference, but I want something reasonably usable on 16 GB RAM. Questions: 1. Which Linux distro would you recommend for this machine if the goal is ‚Äúlightweight but good for dev + LLMs‚Äù? (Xubuntu, Linux Mint XFCE, something else?) 2. For this hardware, what size/models and what quantization (4‚Äëbit vs 8‚Äëbit) are realistic for chat/coding? Any specific model recommendations? 3. Is it worth setting up an eGPU for local LLMs on this MacBook? If yes, any recommended enclosure + GPU combos and OS (macOS vs Linux) that actually work well nowadays? 4. Any gotchas for running Ollama/text‚Äëgeneration‚Äëwebui/LM Studio (or similar) on this kind of setup? Any tips, war stories, or ‚Äúdon‚Äôt bother, do X instead‚Äù are welcome. I‚Äôm mainly trying to squeeze as much learning and usefulness as possible out of this old MacBook without buying a whole new rig.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ba5av"&gt; /u/ba5av &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgcx1x/repurposing_old_15_macbook_pro_16_gb_ram_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgcx1x/repurposing_old_15_macbook_pro_16_gb_ram_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pgcx1x/repurposing_old_15_macbook_pro_16_gb_ram_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-07T07:58:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1pgcx6b</id>
    <title>M4 Max Mac ‚Äì Expert Needed to Fix MLX-LM Installation + Clean Migration Mess (1‚Äì2 hours max)</title>
    <updated>2025-12-07T07:58:47+00:00</updated>
    <author>
      <name>/u/183Vetnet</name>
      <uri>https://old.reddit.com/user/183Vetnet</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking for an Apple-Silicon + MLX specialist to fix a stubborn MLX-LM installation problem on a brand-new M4 Max 64 GB MacBook Pro (macOS Sequoia).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Symptoms&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;python3 -m mlx_lm.generate ‚Üí ‚ÄúModuleNotFoundError: No module named 'mlx_lm'‚Äù in every environment&lt;/li&gt; &lt;li&gt;Migration from 10-year-old MacBook Pro left Anaconda/Homebrew/Conda ghosts that keep hijacking PATH&lt;/li&gt; &lt;li&gt;mlx-lm 0.28.4 + Phi-3-Medium-128k-4bit &lt;strong&gt;was&lt;/strong&gt; working earlier in the session, then vanished&lt;/li&gt; &lt;li&gt;Goal: one single, reliable command that runs Phi-3 Medium at 55‚Äì60 tok/s every time&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What I need&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Remote session (TeamViewer/AnyDesk) or very clear step-by-step&lt;/li&gt; &lt;li&gt;Diagnose and kill every leftover Anaconda/Conda/Miniforge trace&lt;/li&gt; &lt;li&gt;Re-install the &lt;strong&gt;exact&lt;/strong&gt; working MLX + mlx-lm stack (Homebrew Python 3.12 or Miniforge ‚Äî whichever actually works)&lt;/li&gt; &lt;li&gt;Verify with a test generation command&lt;/li&gt; &lt;li&gt;Leave me with one permanent alias/script so it never breaks again&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Budget:&lt;/strong&gt; $80‚Äì120 fixed price (should be 1‚Äì2 hours for someone who‚Äôs done this 20 times)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Availability:&lt;/strong&gt; Today or tomorrow ‚Äì I‚Äôm ready now.&lt;/p&gt; &lt;p&gt;If you‚Äôve fixed this exact ‚Äúno matching distribution‚Äù + migration PATH hell on an M4 Max before, you‚Äôre the one.&lt;/p&gt; &lt;p&gt;Message me with ‚ÄúM4 Max MLX fix‚Äù and how long it will take you.&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/183Vetnet"&gt; /u/183Vetnet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgcx6b/m4_max_mac_expert_needed_to_fix_mlxlm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgcx6b/m4_max_mac_expert_needed_to_fix_mlxlm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pgcx6b/m4_max_mac_expert_needed_to_fix_mlxlm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-07T07:58:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1pgd5bi</id>
    <title>Human-Curated Benchmarking</title>
    <updated>2025-12-07T08:13:19+00:00</updated>
    <author>
      <name>/u/Nicholas_Matt_Quail</name>
      <uri>https://old.reddit.com/user/Nicholas_Matt_Quail</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ok, I will say it out loud first to get it out of the way. LLMs develop, benchmarks suck and become useless, were standing in place when it comes to the USEFUL benchmarking. Benchmarks literally mean nothing to the user at this point, it's not like typical benchmarks of different software or hardware anymore. Benchmarking LLMs stopped working somewhere around spring/summer 2024, in my opinion. It may be discussed, like anything, there are caveats, sure, but I come from this position, let's make it clear.&lt;/p&gt; &lt;p&gt;However, when enough time passes, a generalized consensus within the community arrives and you can usually trust it. It's something like - this scores high but sucks in actual coding, this is underestimated, this is unstable, this is stable but requires holding by hand through prompting, this is less stable but does job on its own, this treats instructions too literally and follows everything at once all the time, this treats them too loosely and picks one to follow randomly etc.&lt;/p&gt; &lt;p&gt;Those are generalized opinions about models so not a skill issue. When I really follow them and - huhuhu - irony - use AI to filter and summarize them up - I rarely find them to be wrong after trying different models.&lt;/p&gt; &lt;p&gt;Now - there are some human-curated tests I am aware of, asking different LLMs to do the same things and comparing the results, some even try being representative with multiple runs etc. - but it's all very use-case oriented so it's hard comparing the models in general. Some dudes test coding in Python, others test captioning stuff, others test summarizing internet articles or videos, yet others test roleplaying with anime girlfriends or solving math tests from actual exams.&lt;/p&gt; &lt;p&gt;It's all ok and actually, more useful than standard benchmarks these days - but a question arises:&lt;/p&gt; &lt;p&gt;Are we aware of some good quality, comparative repository with standardized, human-curated tests like that? Does anything standardized across the board exist and I am not aware of it? I know of the open router and hugging face user reviews/usage charts, which I use myself - but is there anything big, considered to be the current SOTA for human-curated tests? A database that tests just the actually useful models against each other in human-controlled tests of multiple use-cases, standardized across the board instead of one, very particular use case with particular methodology?&lt;/p&gt; &lt;p&gt;Thx in advance and cheers.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nicholas_Matt_Quail"&gt; /u/Nicholas_Matt_Quail &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgd5bi/humancurated_benchmarking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgd5bi/humancurated_benchmarking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pgd5bi/humancurated_benchmarking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-07T08:13:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfiar0</id>
    <title>Qwen3-TTS</title>
    <updated>2025-12-06T06:21:51+00:00</updated>
    <author>
      <name>/u/Terrible_Scar_9890</name>
      <uri>https://old.reddit.com/user/Terrible_Scar_9890</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/spaces/Qwen/Qwen3-TTS-Demo"&gt;https://huggingface.co/spaces/Qwen/Qwen3-TTS-Demo&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Terrible_Scar_9890"&gt; /u/Terrible_Scar_9890 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfiar0/qwen3tts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfiar0/qwen3tts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfiar0/qwen3tts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T06:21:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfxl6x</id>
    <title>[D] What I learned building code RAG without embeddings</title>
    <updated>2025-12-06T19:23:00+00:00</updated>
    <author>
      <name>/u/rozetyp</name>
      <uri>https://old.reddit.com/user/rozetyp</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been building a system to give LLMs relevant code context from any repo. The idea seemed simple: let an LLM look at the file tree + function signatures and pick which files to include. No embeddings, no vector DB.&lt;/p&gt; &lt;p&gt;Sharing what I learned because I wish someone had written this before I broke my eval three different ways.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. Don‚Äôt eval on famous repos&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I started testing on Flask and FastAPI. GPT got 7/10 without any context - it was just reciting training data, not using my retrieval.&lt;/p&gt; &lt;p&gt;I switched to private repos and obscure OSS (&amp;lt;1K stars). ‚ÄúNo context‚Äù dropped to ~4.9/10. That was the real baseline!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. File paths aren‚Äôt enough&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Showing the LLM `src/auth/handler.py` doesn‚Äôt really tell it what‚Äôs inside. I added AST-extracted symbols:&lt;/p&gt; &lt;p&gt;&lt;em&gt;src/auth/handler.py [login, logout, refresh_token]&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;src/auth/middleware.py [require_auth, rate_limit]&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Retrieval quality jumped noticeably (NDCG went from ~0.85 to ~0.92). The model doesn‚Äôt need to read the full file to know ‚Äúthis smells like auth.‚Äù&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3. Same-vendor judging is inflated&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;GPT-4 judging GPT-4‚Äôs answers gave suspiciously high scores! Switching to cross-vendor (GPT generates, Gemini judges) knocked about 0.5 off the scores and the reviews &lt;em&gt;felt&lt;/em&gt; more honest. The judge was much harsher on vague, confident answers.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;4. Generic eval criteria reward BS&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;My first judge prompt used vague criteria like ‚Äúshould explain error handling‚Äù. That rewarded confident wrong answers.&lt;/p&gt; &lt;p&gt;What worked better was forcing exact hooks:&lt;/p&gt; &lt;p&gt;&lt;em&gt;&lt;del&gt;‚ÄúShould explain the request lifecycle‚Äù&lt;/del&gt;&lt;/em&gt;&lt;em&gt;, &amp;quot;Must mention `RequestContext` and `full_dispatch_request()`‚Äù&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Anchoring eval on specific symbols/files made it much easier to spot hand-wavy nonsense.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Results after fixing eval (very rough):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;LLM file picker: ~0.92 NDCG, ~8.5/10 answer quality&lt;/li&gt; &lt;li&gt;Embeddings baseline: ~0.79 NDCG, ~8.6/10 answer quality&lt;/li&gt; &lt;li&gt;No context: ~4.9/10&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So the ‚ÄúLLM looks at the tree + symbols and picks files‚Äù setup landed roughly on par with embeddings on answer quality, without the indexing infrastructure. Good enough for me to keep using it.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Caveats!&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Small sample (177 questions, 14 repos)&lt;/li&gt; &lt;li&gt;I wrote the questions - probably biased toward what my approach handles&lt;/li&gt; &lt;li&gt;Private-repo results may not generalize beyond the ones I tested&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Questions for you:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;How are you building eval sets that the model &lt;em&gt;hasn‚Äôt&lt;/em&gt; basically memorized?&lt;/li&gt; &lt;li&gt;Any tricks for making LLM-as-judge less biased when you‚Äôre judging your own system?&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rozetyp"&gt; /u/rozetyp &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfxl6x/d_what_i_learned_building_code_rag_without/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfxl6x/d_what_i_learned_building_code_rag_without/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfxl6x/d_what_i_learned_building_code_rag_without/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T19:23:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfsntn</id>
    <title>convert: support Mistral 3 Large MoE by ngxson ¬∑ Pull Request #17730 ¬∑ ggml-org/llama.cpp</title>
    <updated>2025-12-06T16:00:37+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfsntn/convert_support_mistral_3_large_moe_by_ngxson/"&gt; &lt;img alt="convert: support Mistral 3 Large MoE by ngxson ¬∑ Pull Request #17730 ¬∑ ggml-org/llama.cpp" src="https://external-preview.redd.it/YXlCrbFuGSaJRzk-d-1JftjUbGO215ldNJVTXMLJQi4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4507263a618891c23289c740acf9be9cc8bee393" title="convert: support Mistral 3 Large MoE by ngxson ¬∑ Pull Request #17730 ¬∑ ggml-org/llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;You can now download GGUF&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/bartowski/mistralai_Mistral-Large-3-675B-Instruct-2512-GGUF"&gt;https://huggingface.co/bartowski/mistralai_Mistral-Large-3-675B-Instruct-2512-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;but can you run it...? &lt;/p&gt; &lt;p&gt;(that another PR is &lt;a href="https://github.com/ggml-org/llama.cpp/pull/17744"&gt;https://github.com/ggml-org/llama.cpp/pull/17744&lt;/a&gt;)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/17730"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfsntn/convert_support_mistral_3_large_moe_by_ngxson/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfsntn/convert_support_mistral_3_large_moe_by_ngxson/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T16:00:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfq0kd</id>
    <title>PaperDebugger: the Best Overleaf Companion!</title>
    <updated>2025-12-06T14:01:42+00:00</updated>
    <author>
      <name>/u/NuoJohnChen</name>
      <uri>https://old.reddit.com/user/NuoJohnChen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfq0kd/paperdebugger_the_best_overleaf_companion/"&gt; &lt;img alt="PaperDebugger: the Best Overleaf Companion!" src="https://b.thumbs.redditmedia.com/LSzFW-bVRkmLrP-afZdLmy0DNmjvCz1MK2UnMO8aqLo.jpg" title="PaperDebugger: the Best Overleaf Companion!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Chrome/APP Store: &lt;a href="https://www.paperdebugger.com/"&gt;https://www.paperdebugger.com/&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Paper: &lt;a href="https://arxiv.org/abs/2512.02589"&gt;https://arxiv.org/abs/2512.02589&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Code: &lt;a href="https://github.com/PaperDebugger/PaperDebugger"&gt;https://github.com/PaperDebugger/PaperDebugger&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Enhancer: &lt;a href="https://huggingface.co/Xtra-Computing/XtraGPT-7B"&gt;https://huggingface.co/Xtra-Computing/XtraGPT-7B&lt;/a&gt; &lt;/p&gt; &lt;p&gt;An NUS team just released &amp;quot;PaperDebugger&amp;quot;: an in-editor system that uses multiple agents (Reviewer, Researcher, Scorer) to rewrite and critique papers in real-time within Overleaf. Just simply select a rough section, and it launches the full pipeline. &lt;/p&gt; &lt;p&gt;Direct Integration: No copy-pasting. It patches the document with Git-style before/after diffs.&lt;/p&gt; &lt;p&gt;Deep Research: Can pull arXiv papers, summarize them, and generate comparison tables inline.&lt;/p&gt; &lt;p&gt;Tech Stack: Uses an MCP toolchain and Kubernetes to scale the agent reasoning.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NuoJohnChen"&gt; /u/NuoJohnChen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pfq0kd"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfq0kd/paperdebugger_the_best_overleaf_companion/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfq0kd/paperdebugger_the_best_overleaf_companion/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T14:01:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfl0d8</id>
    <title>How big an open source model can I run on 128 GB unified memory?</title>
    <updated>2025-12-06T09:13:03+00:00</updated>
    <author>
      <name>/u/nameless_me</name>
      <uri>https://old.reddit.com/user/nameless_me</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just took delivery of a Minisforum MS-S1 with AMD Ryzen Ai Max+ 395 cpu, 128 GB unified memory architecture and AMD Radeon 8060S Graphics. In the BIOS the UDMA memory for the iGPU is set to 96 GB. Running a Debian Linux terminal in WSL 2, I downloaded and ran ollama which works fine.&lt;/p&gt; &lt;p&gt;Trying a Deepseek-r1:70b model, it refused to load in ollama. I checked a few sources which ended saying this &amp;quot;&lt;strong&gt;DeepSeek-R1-70B INT4 GGUF still requires ~55‚Äì60 GB VRAM equivalent&lt;/strong&gt;. &lt;strong&gt;You cannot run this model on a single consumer APU&lt;/strong&gt;, even with ‚Äú128 GB unified memory‚Äù.&lt;/p&gt; &lt;p&gt;Is the above true? What is the largest LLM model I can run reasonably on this computer?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nameless_me"&gt; /u/nameless_me &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfl0d8/how_big_an_open_source_model_can_i_run_on_128_gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfl0d8/how_big_an_open_source_model_can_i_run_on_128_gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfl0d8/how_big_an_open_source_model_can_i_run_on_128_gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T09:13:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1pg0tbe</id>
    <title>Zebra-Llama: Towards Extremely Efficient Hybrid Models</title>
    <updated>2025-12-06T21:40:53+00:00</updated>
    <author>
      <name>/u/divide0verfl0w</name>
      <uri>https://old.reddit.com/user/divide0verfl0w</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://arxiv.org/abs/2505.17272"&gt;https://arxiv.org/abs/2505.17272&lt;/a&gt;&lt;/p&gt; &lt;p&gt;HN Link: &lt;a href="https://news.ycombinator.com/item?id=46176289"&gt;https://news.ycombinator.com/item?id=46176289&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thoughts?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/divide0verfl0w"&gt; /u/divide0verfl0w &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pg0tbe/zebrallama_towards_extremely_efficient_hybrid/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pg0tbe/zebrallama_towards_extremely_efficient_hybrid/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pg0tbe/zebrallama_towards_extremely_efficient_hybrid/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T21:40:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfg0rh</id>
    <title>The Best Open-Source 8B-Parameter LLM Built in the USA</title>
    <updated>2025-12-06T04:14:17+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfg0rh/the_best_opensource_8bparameter_llm_built_in_the/"&gt; &lt;img alt="The Best Open-Source 8B-Parameter LLM Built in the USA" src="https://preview.redd.it/r6muiibadi5g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2f50b4cb0f889ed02690c8f3ff7e90713b46562c" title="The Best Open-Source 8B-Parameter LLM Built in the USA" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Rnj-1 is a family of 8B parameter open-weight, dense models trained from scratch by Essential AI, optimized for code and STEM with capabilities on par with SOTA open-weight models.&lt;/p&gt; &lt;p&gt;These models &lt;/p&gt; &lt;ul&gt; &lt;li&gt;perform well across a range of programming languages. &lt;/li&gt; &lt;li&gt;boast strong agentic capabilities (e.g., inside agentic frameworks like mini-SWE-agent). &lt;/li&gt; &lt;li&gt;excel at tool-calling.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Both raw and instruct variants are available on &lt;a href="https://huggingface.co/collections/EssentialAI/rnj-1"&gt;Hugging Face platform&lt;/a&gt;. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Model Architecture Overview&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Rnj-1's architecture is similar to Gemma 3, except that it uses only global attention, and YaRN for long-context extension.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Training Dynamics&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;rnj-1&lt;/code&gt; was pre-trained on 8.4T tokens with an 8K context length, after which the model‚Äôs context window was extended to &lt;strong&gt;32K&lt;/strong&gt; through an additional 380B-token mid-training stage. &lt;/p&gt; &lt;p&gt;A final 150B-token SFT stage completed the training to produce &lt;code&gt;rnj-1-instruct&lt;/code&gt;. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/r6muiibadi5g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfg0rh/the_best_opensource_8bparameter_llm_built_in_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfg0rh/the_best_opensource_8bparameter_llm_built_in_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T04:14:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfwu8t</id>
    <title>Are MoE models harder to Fine-tune?</title>
    <updated>2025-12-06T18:52:21+00:00</updated>
    <author>
      <name>/u/ComplexType568</name>
      <uri>https://old.reddit.com/user/ComplexType568</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;really sorry if this is a stupid question, but ive been looking around huggingface A LOT and ive noticed a really big trend where theres a ton of dense models being fine-tuned/lora-ed, while most MoE models go untouched. are there any reasons for this? &lt;/p&gt; &lt;p&gt;i dont think its the model size, as ive seen big models like Llama 70B or even 405B turn into Hermes 4 models, Tulu, etc. while pretty good models like practically the entire Qwen3 series, GLM (besides GLM Steam), DeepSeek and Kimi are untouched, id get why DS and Kimi are untouched... but, seriously, Qwen3?? so far ive seen an ArliAI finetune only. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ComplexType568"&gt; /u/ComplexType568 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfwu8t/are_moe_models_harder_to_finetune/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfwu8t/are_moe_models_harder_to_finetune/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfwu8t/are_moe_models_harder_to_finetune/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T18:52:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1pg1rhf</id>
    <title>Minimax M2</title>
    <updated>2025-12-06T22:22:16+00:00</updated>
    <author>
      <name>/u/SlowFail2433</name>
      <uri>https://old.reddit.com/user/SlowFail2433</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What does the community think of Minimax M2?&lt;/p&gt; &lt;p&gt;Benches surprisingly well and the Minimax team tend to be strong at RL.&lt;/p&gt; &lt;p&gt;Any experiences with this model? Any tips or preferred use-cases?&lt;/p&gt; &lt;p&gt;Particularly interested in STEM, coding and agentic but all use-cases welcome&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SlowFail2433"&gt; /u/SlowFail2433 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pg1rhf/minimax_m2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pg1rhf/minimax_m2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pg1rhf/minimax_m2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T22:22:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1pg6jp6</id>
    <title>I built a minimal Claude Code clone to understand how AI coding agents work under the hood</title>
    <updated>2025-12-07T02:07:09+00:00</updated>
    <author>
      <name>/u/Money-Coast-3905</name>
      <uri>https://old.reddit.com/user/Money-Coast-3905</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pg6jp6/i_built_a_minimal_claude_code_clone_to_understand/"&gt; &lt;img alt="I built a minimal Claude Code clone to understand how AI coding agents work under the hood" src="https://preview.redd.it/7p800vqawo5g1.gif?width=640&amp;amp;crop=smart&amp;amp;s=622a2626b9442d79ae3232b55093a1ac1669ddbc" title="I built a minimal Claude Code clone to understand how AI coding agents work under the hood" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;I've been fascinated by tools like &lt;a href="https://claude.ai/code"&gt;Claude Code&lt;/a&gt; and &lt;a href="https://github.com/langchain-ai/deepagents"&gt;deepagents&lt;/a&gt; lately. While using them, I kept wondering:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;What does the system prompt actually look like?&lt;/li&gt; &lt;li&gt;How are tool schemas structured for the API?&lt;/li&gt; &lt;li&gt;How does the message flow work between turns?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So I decided to build a minimal implementation myself to understand these internals better. It's called &lt;strong&gt;yacc&lt;/strong&gt; (Yet Another Claude Code) - a simple AI coding assistant built with pure Python + Anthropic API (no LangChain).&lt;/p&gt; &lt;h3&gt;What I learned and documented:&lt;/h3&gt; &lt;p&gt;üìù &lt;strong&gt;System Prompts&lt;/strong&gt; - How to structure instructions for planning, filesystem operations, and tool usage&lt;/p&gt; &lt;p&gt;üîß &lt;strong&gt;Tool Schemas&lt;/strong&gt; - JSON schema definitions for tools like &lt;code&gt;read_file&lt;/code&gt;, &lt;code&gt;write_file&lt;/code&gt;, &lt;code&gt;edit_file&lt;/code&gt;, &lt;code&gt;grep&lt;/code&gt;, &lt;code&gt;bash&lt;/code&gt;, etc.&lt;/p&gt; &lt;p&gt;üîÑ &lt;strong&gt;Middleware patterns&lt;/strong&gt; - Prompt caching, context summarization (when tokens exceed limits), patching dangling tool calls&lt;/p&gt; &lt;p&gt;üí¨ &lt;strong&gt;Message flow&lt;/strong&gt; - How tool_use and tool_result blocks work in the conversation&lt;/p&gt; &lt;h3&gt;Not production-ready, but...&lt;/h3&gt; &lt;p&gt;This is definitely NOT a replacement for Claude Code or deepagents. It's more of a &lt;strong&gt;learning resource&lt;/strong&gt; for anyone curious about:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;How Claude's tool calling works in practice&lt;/li&gt; &lt;li&gt;What a typical agentic system prompt contains&lt;/li&gt; &lt;li&gt;How to manage context in long-running agent sessions&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;GitHub&lt;/h3&gt; &lt;p&gt;üîó &lt;a href="https://github.com/SeungyounShin/yet-another-claude-code"&gt;https://github.com/SeungyounShin/yet-another-claude-code&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The code is pretty readable and documented. Check out: - &lt;code&gt;src/prompts/system.py&lt;/code&gt; - System prompt structure - &lt;code&gt;src/tools/definitions.py&lt;/code&gt; - Tool schemas - &lt;code&gt;src/agent.py&lt;/code&gt; - Main orchestration loop - &lt;code&gt;src/middleware/&lt;/code&gt; - Context management&lt;/p&gt; &lt;p&gt;Hope this helps someone who's curious about the internals! Happy to answer any questions.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;&lt;em&gt;Inspired by &lt;a href="https://github.com/langchain-ai/deepagents"&gt;deepagents&lt;/a&gt; from LangChain team - they have a much more complete implementation if you need something production-ready.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Money-Coast-3905"&gt; /u/Money-Coast-3905 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7p800vqawo5g1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pg6jp6/i_built_a_minimal_claude_code_clone_to_understand/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pg6jp6/i_built_a_minimal_claude_code_clone_to_understand/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-07T02:07:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfyrwm</id>
    <title>Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length</title>
    <updated>2025-12-06T20:12:22+00:00</updated>
    <author>
      <name>/u/Educational-Pound269</name>
      <uri>https://old.reddit.com/user/Educational-Pound269</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfyrwm/live_avatar_streaming_realtime_audiodriven_avatar/"&gt; &lt;img alt="Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length" src="https://external-preview.redd.it/YjQybWV5bmY1bjVnMatgKKMAYanNbnGU9s9FiIXTW5q8AYgZBBw2qwcYT6Ul.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aad98486fbb5e0f55eb92c0c79b334973e80e088" title="Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;They just dropped a REALTIME, infinite length video generator.&lt;/p&gt; &lt;p&gt;Based on Wan, 20 fps, with dialogue&lt;/p&gt; &lt;p&gt;The code will be open source in early December.&lt;br /&gt; &lt;a href="https://liveavatar.github.io/"&gt;https://liveavatar.github.io/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Educational-Pound269"&gt; /u/Educational-Pound269 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/zewd3onf5n5g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfyrwm/live_avatar_streaming_realtime_audiodriven_avatar/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfyrwm/live_avatar_streaming_realtime_audiodriven_avatar/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T20:12:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1pg7f00</id>
    <title>Zen CPU Performance Uplift (Epyc &amp; Strix Halo) w/ ZenDNN Backend Integration for llama.cpp</title>
    <updated>2025-12-07T02:50:50+00:00</updated>
    <author>
      <name>/u/Noble00_</name>
      <uri>https://old.reddit.com/user/Noble00_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pg7f00/zen_cpu_performance_uplift_epyc_strix_halo_w/"&gt; &lt;img alt="Zen CPU Performance Uplift (Epyc &amp;amp; Strix Halo) w/ ZenDNN Backend Integration for llama.cpp" src="https://external-preview.redd.it/NDJTzKU3ltYG49f6LU-R2hFmqhxjjyJK3XNi_UF7GlA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6d8c9b4d0929a1c550bf37e26c240c96d38c9d9c" title="Zen CPU Performance Uplift (Epyc &amp;amp; Strix Halo) w/ ZenDNN Backend Integration for llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just happened to cross this and thought this seemed interesting. Here are some benchmarks:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Test Configuration&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Hardware&lt;/strong&gt;: AMD EPYC 9004 Series (Zen 4)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Threads&lt;/strong&gt;: 96&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Batch Size&lt;/strong&gt;: 4096&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Tool&lt;/strong&gt;: llama-bench&lt;/li&gt; &lt;li&gt;&lt;strong&gt;llama.cpp version&lt;/strong&gt;: 7134&lt;/li&gt; &lt;li&gt;&lt;strong&gt;ZenDNN version&lt;/strong&gt;: 1.0.0&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Environment&lt;/strong&gt;: &lt;code&gt;ZENDNNL_MATMUL_ALGO=2&lt;/code&gt; (Blocked AOCL BLIS)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;LLaMA 3.1 8B (BF16)&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Test&lt;/th&gt; &lt;th align="left"&gt;CPU t/s&lt;/th&gt; &lt;th align="left"&gt;ZenDNN t/s&lt;/th&gt; &lt;th align="left"&gt;Speedup&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;pp128&lt;/td&gt; &lt;td align="left"&gt;341.50&lt;/td&gt; &lt;td align="left"&gt;395.58&lt;/td&gt; &lt;td align="left"&gt;1.16x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;pp256&lt;/td&gt; &lt;td align="left"&gt;382.52&lt;/td&gt; &lt;td align="left"&gt;561.94&lt;/td&gt; &lt;td align="left"&gt;1.47x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;423.40&lt;/td&gt; &lt;td align="left"&gt;624.61&lt;/td&gt; &lt;td align="left"&gt;1.48x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;pp1024&lt;/td&gt; &lt;td align="left"&gt;414.12&lt;/td&gt; &lt;td align="left"&gt;637.97&lt;/td&gt; &lt;td align="left"&gt;1.54x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;pp2048&lt;/td&gt; &lt;td align="left"&gt;338.50&lt;/td&gt; &lt;td align="left"&gt;622.08&lt;/td&gt; &lt;td align="left"&gt;1.84x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;pp4096&lt;/td&gt; &lt;td align="left"&gt;308.53&lt;/td&gt; &lt;td align="left"&gt;534.76&lt;/td&gt; &lt;td align="left"&gt;1.73x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;7.28&lt;/td&gt; &lt;td align="left"&gt;10.53&lt;/td&gt; &lt;td align="left"&gt;1.45x&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;LLaMA 3.1 8B (F32)&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Test&lt;/th&gt; &lt;th align="left"&gt;CPU t/s&lt;/th&gt; &lt;th align="left"&gt;ZenDNN t/s&lt;/th&gt; &lt;th align="left"&gt;Speedup&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;pp128&lt;/td&gt; &lt;td align="left"&gt;184.44&lt;/td&gt; &lt;td align="left"&gt;293.39&lt;/td&gt; &lt;td align="left"&gt;1.59x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;pp256&lt;/td&gt; &lt;td align="left"&gt;189.69&lt;/td&gt; &lt;td align="left"&gt;384.71&lt;/td&gt; &lt;td align="left"&gt;2.03x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;234.74&lt;/td&gt; &lt;td align="left"&gt;431.21&lt;/td&gt; &lt;td align="left"&gt;1.84x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;pp1024&lt;/td&gt; &lt;td align="left"&gt;231.49&lt;/td&gt; &lt;td align="left"&gt;451.51&lt;/td&gt; &lt;td align="left"&gt;1.95x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;pp2048&lt;/td&gt; &lt;td align="left"&gt;220.05&lt;/td&gt; &lt;td align="left"&gt;425.65&lt;/td&gt; &lt;td align="left"&gt;1.93x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;pp4096&lt;/td&gt; &lt;td align="left"&gt;189.75&lt;/td&gt; &lt;td align="left"&gt;396.73&lt;/td&gt; &lt;td align="left"&gt;2.09x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;2.69&lt;/td&gt; &lt;td align="left"&gt;7.34&lt;/td&gt; &lt;td align="left"&gt;2.73x&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Merged: &lt;a href="https://github.com/ggml-org/llama.cpp/pull/17690"&gt;https://github.com/ggml-org/llama.cpp/pull/17690&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also, while disappointingly for Epyc and STX-H only it seems, it has been able to work on the Ryzen 7940HS, perhaps uplifts can be seen on consumer desktop.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Noble00_"&gt; /u/Noble00_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/discussions/17684"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pg7f00/zen_cpu_performance_uplift_epyc_strix_halo_w/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pg7f00/zen_cpu_performance_uplift_epyc_strix_halo_w/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-07T02:50:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfvt9e</id>
    <title>VibeVoice Realtime 0.5B - OpenAI Compatible /v1/audio/speech TTS Server</title>
    <updated>2025-12-06T18:10:19+00:00</updated>
    <author>
      <name>/u/marhensa</name>
      <uri>https://old.reddit.com/user/marhensa</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfvt9e/vibevoice_realtime_05b_openai_compatible/"&gt; &lt;img alt="VibeVoice Realtime 0.5B - OpenAI Compatible /v1/audio/speech TTS Server" src="https://b.thumbs.redditmedia.com/s91ewzR1qewE9gWdg8jOP6ycdK9l1T_UjLsYMD8uNoo.jpg" title="VibeVoice Realtime 0.5B - OpenAI Compatible /v1/audio/speech TTS Server" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Microsoft recently released &lt;a href="https://huggingface.co/microsoft/VibeVoice-Realtime-0.5B"&gt;VibeVoice-Realtime-0.5B&lt;/a&gt;, a lightweight &lt;strong&gt;&lt;em&gt;expressive&lt;/em&gt;&lt;/strong&gt; TTS model.&lt;/p&gt; &lt;p&gt;I wrapped it in an OpenAI-compatible API server so it works directly with Open WebUI's TTS settings.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/marhensa/vibevoice-realtime-openai-api.git"&gt;https://github.com/marhensa/vibevoice-realtime-openai-api.git&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Drop-in using OpenAI-compatible &lt;code&gt;/v1/audio/speech&lt;/code&gt; endpoint&lt;/li&gt; &lt;li&gt;Runs locally with Docker or Python venv (via uv)&lt;/li&gt; &lt;li&gt;Using only ~2GB of VRAM&lt;/li&gt; &lt;li&gt;CUDA-optimized (around ~1x RTF on RTX 3060 12GB)&lt;/li&gt; &lt;li&gt;Multiple voices with OpenAI name aliases (alloy, nova, etc.)&lt;/li&gt; &lt;li&gt;All models auto-download on first run&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1pfvt9e/video/7emfqdbdjm5g1/player"&gt;Video demonstration of \&amp;quot;Mike\&amp;quot; male voice. Audio üì¢ ON.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The expression and flow is better than Kokoro, imho. But Kokoro is faster.&lt;/p&gt; &lt;p&gt;But (for now) it lacks female voice model, there's just two female, and one is weirdly sounds like a male üòÖ.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6r87w5d9pm5g1.png?width=1073&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=adfd10fae1523fed7f2898c38ae92816130cbf2d"&gt;vibevoice-realtime-openai-api Settings on Open WebUI: Set chunk splitting to Paragraphs.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Contribution are welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/marhensa"&gt; /u/marhensa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfvt9e/vibevoice_realtime_05b_openai_compatible/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfvt9e/vibevoice_realtime_05b_openai_compatible/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfvt9e/vibevoice_realtime_05b_openai_compatible/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T18:10:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfx3d0</id>
    <title>We need open source hardware lithography</title>
    <updated>2025-12-06T19:02:38+00:00</updated>
    <author>
      <name>/u/bennmann</name>
      <uri>https://old.reddit.com/user/bennmann</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Perhaps it's time hardware was more democratized. RISC-V is only 1 step away.&lt;/p&gt; &lt;p&gt;There are real challenges with yield at small scales, requiring a clean environment. But perhaps a small scale system could be made &amp;quot;good enough&amp;quot;, or overcome with some clever tech or small vacuum chambers.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bennmann"&gt; /u/bennmann &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfx3d0/we_need_open_source_hardware_lithography/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfx3d0/we_need_open_source_hardware_lithography/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfx3d0/we_need_open_source_hardware_lithography/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T19:02:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1pg76jo</id>
    <title>Why local coding models are less popular than hosted coding models?</title>
    <updated>2025-12-07T02:39:17+00:00</updated>
    <author>
      <name>/u/WasteTechnology</name>
      <uri>https://old.reddit.com/user/WasteTechnology</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In theory, local coding models sound very good. You don't send your most valuable assets to another company, keep everything local and under control. However, the leading AI coding startups work with hosted models (correct me if I'm wrong). Why do you think it is so?&lt;/p&gt; &lt;p&gt;If you use one, please share your setup. Which model, which engine, which coding tool do you use?, What is your experience? Do you get productive enough with them compared to hosted options?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WasteTechnology"&gt; /u/WasteTechnology &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pg76jo/why_local_coding_models_are_less_popular_than/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pg76jo/why_local_coding_models_are_less_popular_than/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pg76jo/why_local_coding_models_are_less_popular_than/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-07T02:39:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1pg8jtk</id>
    <title>SGLang Diffusion + Cache-DiT = 20-165% Faster Local Image/Video Generation</title>
    <updated>2025-12-07T03:48:50+00:00</updated>
    <author>
      <name>/u/Expert-Pineapple-740</name>
      <uri>https://old.reddit.com/user/Expert-Pineapple-740</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Quick heads up: SGLang Diffusion now supports Cache-DiT integration, delivering 20-165% speedup for diffusion models with basically zero effort.&lt;/p&gt; &lt;p&gt;Just add some env variables and you're getting 46%+ faster inference on models like FLUX, Qwen-Image, HunyuanVideo, etc.&lt;/p&gt; &lt;p&gt;Works with torch.compile, quantization, and all the usual optimizations. Supports pretty much every major open-source DiT model.&lt;/p&gt; &lt;p&gt;Install: &lt;code&gt;uv pip install 'sglang[diffusion]' --prerelease=allow&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Docs: &lt;a href="https://github.com/sgl-project/sglang/blob/main/python/sglang/multimodal_gen/docs/cache_dit.md"&gt;https://github.com/sgl-project/sglang/blob/main/python/sglang/multimodal_gen/docs/cache_dit.md&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Expert-Pineapple-740"&gt; /u/Expert-Pineapple-740 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pg8jtk/sglang_diffusion_cachedit_20165_faster_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pg8jtk/sglang_diffusion_cachedit_20165_faster_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pg8jtk/sglang_diffusion_cachedit_20165_faster_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-07T03:48:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1pg8ix9</id>
    <title>My little decentralized Locallama setup, 216gb VRAM</title>
    <updated>2025-12-07T03:47:31+00:00</updated>
    <author>
      <name>/u/Goldkoron</name>
      <uri>https://old.reddit.com/user/Goldkoron</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pg8ix9/my_little_decentralized_locallama_setup_216gb_vram/"&gt; &lt;img alt="My little decentralized Locallama setup, 216gb VRAM" src="https://preview.redd.it/o1o7ekxycp5g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=66f62e54e1c923ea71f1a1d46415562ffdcbc1ba" title="My little decentralized Locallama setup, 216gb VRAM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Goldkoron"&gt; /u/Goldkoron &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/o1o7ekxycp5g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pg8ix9/my_little_decentralized_locallama_setup_216gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pg8ix9/my_little_decentralized_locallama_setup_216gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-07T03:47:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
