<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-15T07:10:21+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1pmw18r</id>
    <title>Is there a cold-GPU provider where I can run my finetuned Gemma Model on?</title>
    <updated>2025-12-15T02:25:35+00:00</updated>
    <author>
      <name>/u/inAbigworld</name>
      <uri>https://old.reddit.com/user/inAbigworld</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried Vertex AI and the cold GPU feature which is in Beta didn't work and left me with a hefty bill.&lt;/p&gt; &lt;p&gt;Amazon SageMaker doesn't allow that anymore.&lt;/p&gt; &lt;p&gt;Is there a trusted provider that provides such service where I pay only for the time I used the GPU?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/inAbigworld"&gt; /u/inAbigworld &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmw18r/is_there_a_coldgpu_provider_where_i_can_run_my/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmw18r/is_there_a_coldgpu_provider_where_i_can_run_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pmw18r/is_there_a_coldgpu_provider_where_i_can_run_my/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T02:25:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmbytz</id>
    <title>What do you think?</title>
    <updated>2025-12-14T11:31:12+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmbytz/what_do_you_think/"&gt; &lt;img alt="What do you think?" src="https://preview.redd.it/t7969gavn57g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=289aa266520653984b91a408333dbc5cbf2243a9" title="What do you think?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/t7969gavn57g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmbytz/what_do_you_think/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pmbytz/what_do_you_think/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-14T11:31:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmzvum</id>
    <title>How to continue the output seamless in Response API</title>
    <updated>2025-12-15T05:49:24+00:00</updated>
    <author>
      <name>/u/Technical_Pass_1858</name>
      <uri>https://old.reddit.com/user/Technical_Pass_1858</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am trying to implement a functionality, when the AI output is stopped because of reaching the limit of max_output_tokens, the agent should automatically send another request to AI, so the AI could continue the output. I try to put a user input message:‚Äùcontinue‚Äù, then AI will respond continuously. The problem is the second output has some extra words at the beginning of the response,is there any better method so the AI could just continue after the word of the first response?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Technical_Pass_1858"&gt; /u/Technical_Pass_1858 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmzvum/how_to_continue_the_output_seamless_in_response/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmzvum/how_to_continue_the_output_seamless_in_response/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pmzvum/how_to_continue_the_output_seamless_in_response/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T05:49:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmfqx5</id>
    <title>I trained a new TTS model with Zero-shot Voice Cloning and Duration Control!</title>
    <updated>2025-12-14T14:46:50+00:00</updated>
    <author>
      <name>/u/Aratako_LM</name>
      <uri>https://old.reddit.com/user/Aratako_LM</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmfqx5/i_trained_a_new_tts_model_with_zeroshot_voice/"&gt; &lt;img alt="I trained a new TTS model with Zero-shot Voice Cloning and Duration Control!" src="https://b.thumbs.redditmedia.com/OkW2I5v0LKvTOgQhUaGUmN0TaKW_odEF58sEPJiUBew.jpg" title="I trained a new TTS model with Zero-shot Voice Cloning and Duration Control!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/d2v3vbcnm67g1.png?width=1408&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7c9fd7be647e40ad46c85027924a818139297657"&gt;Model Architecture&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;I‚Äôve been working on a hobby project to build a multilingual TTS model using an Encoder-Decoder architecture, and I‚Äôm excited to finally share &lt;strong&gt;T5Gemma-TTS-2b-2b&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;It‚Äôs initialized from Google‚Äôs &lt;a href="https://huggingface.co/google/t5gemma-2b-2b-ul2"&gt;t5gemma-2b-2b-ul2&lt;/a&gt; and trained on about 170k hours of speech data (mainly &lt;a href="https://huggingface.co/datasets/amphion/Emilia-Dataset"&gt;Emilia&lt;/a&gt; and &lt;a href="https://huggingface.co/datasets/pkufool/libriheavy"&gt;Libriheavy&lt;/a&gt;). The architecture is inspired by &lt;a href="https://arxiv.org/abs/2505.19462"&gt;VoiceStar&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Multilingual:&lt;/strong&gt; Supports English, Chinese, and Japanese.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Zero-shot Voice Cloning:&lt;/strong&gt; Give it a reference audio, and it clones the voice.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Duration Control:&lt;/strong&gt; You can explicitly tell the model how many seconds the generated audio should be (e.g., &amp;quot;speak this sentence in exactly 5 seconds&amp;quot;).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Open Source Code:&lt;/strong&gt; Not just the weights‚ÄîI‚Äôve released the full training and inference scripts on GitHub.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;‚ö†Ô∏è The &amp;quot;Jank&amp;quot; (Limitations):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;It is slow.&lt;/strong&gt; Since it's autoregressive and not fully optimized yet, don't expect real-time performance. It's strictly for offline generation right now.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;License:&lt;/strong&gt; It is &lt;strong&gt;CC-BY-NC 4.0&lt;/strong&gt; (Non-Commercial). I know this sub prefers Apache/MIT, but the license is restricted by the dependencies on XCodec2 and the Emilia dataset.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I am hoping to improve the inference speed and explore more permissive datasets for future iterations.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;A Note on Language Quality:&lt;/strong&gt; As a Japanese developer, I focused heavily on optimizing the Japanese performance. While I included ~100k hours of English data, I‚Äôm curious if the English output sounds natural to native speakers. If you are interested, feel free to give it a spin and let me know what you think!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Model (Hugging Face):&lt;/strong&gt; &lt;a href="https://huggingface.co/Aratako/T5Gemma-TTS-2b-2b"&gt;https://huggingface.co/Aratako/T5Gemma-TTS-2b-2b&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Demo (HF Space):&lt;/strong&gt; &lt;a href="https://huggingface.co/spaces/Aratako/T5Gemma-TTS-Demo"&gt;https://huggingface.co/spaces/Aratako/T5Gemma-TTS-Demo&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Code (GitHub):&lt;/strong&gt; &lt;a href="https://github.com/Aratako/T5Gemma-TTS"&gt;https://github.com/Aratako/T5Gemma-TTS&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thanks for checking it out!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aratako_LM"&gt; /u/Aratako_LM &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmfqx5/i_trained_a_new_tts_model_with_zeroshot_voice/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmfqx5/i_trained_a_new_tts_model_with_zeroshot_voice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pmfqx5/i_trained_a_new_tts_model_with_zeroshot_voice/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-14T14:46:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmw0nl</id>
    <title>RAG Paper 12.11</title>
    <updated>2025-12-15T02:24:46+00:00</updated>
    <author>
      <name>/u/Cheryl_Apple</name>
      <uri>https://old.reddit.com/user/Cheryl_Apple</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ol&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2512.10787v1"&gt;Replace, Don't Expand: Mitigating Context Dilution in Multi-Hop RAG via Fixed-Budget Evidence Assembly&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2512.10435v1"&gt;Semantic Reconstruction of Adversarial Plagiarism: A Context-Aware Framework for Detecting and Restoring &amp;quot;Tortured Phrases&amp;quot; in Scientific Literature&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2512.10422v1"&gt;Cooperative Retrieval-Augmented Generation for Question Answering: Mutual Information Exchange and Ranking by Contrasting Layers&lt;/a&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Collected by OpenBMB, transferred by&lt;/strong&gt; &lt;a href="https://www.ragview.ai/"&gt;&lt;strong&gt;RagView.ai&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;/&lt;/strong&gt; &lt;a href="https://github.com/RagView/RagView"&gt;&lt;strong&gt;github/RagView&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cheryl_Apple"&gt; /u/Cheryl_Apple &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmw0nl/rag_paper_1211/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmw0nl/rag_paper_1211/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pmw0nl/rag_paper_1211/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T02:24:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1pn0gpa</id>
    <title>Forked Google's Gemini CLI to work with local LLMs (MLX, llama.cpp, vLLM)</title>
    <updated>2025-12-15T06:23:09+00:00</updated>
    <author>
      <name>/u/Honest-Fun-5279</name>
      <uri>https://old.reddit.com/user/Honest-Fun-5279</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So i forked the gemini cli and added local llm support, no google account needed, runs offline. &lt;/p&gt; &lt;p&gt;Give it a try!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/limkcreply/open-gemini-cli"&gt;https://github.com/limkcreply/open-gemini-cli&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Honest-Fun-5279"&gt; /u/Honest-Fun-5279 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn0gpa/forked_googles_gemini_cli_to_work_with_local_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn0gpa/forked_googles_gemini_cli_to_work_with_local_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pn0gpa/forked_googles_gemini_cli_to_work_with_local_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T06:23:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmjsi5</id>
    <title>The new Kimi K2 1T model (4-bit quant) runs on 2 512GB M3 Ultras [Awni Hannun/Twitter]</title>
    <updated>2025-12-14T17:33:14+00:00</updated>
    <author>
      <name>/u/pogue972</name>
      <uri>https://old.reddit.com/user/pogue972</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Awni Hannun (AI @ Apple employee) says: The new Kimi K2 1T model (4-bit quant) runs on 2 512GB M3 Ultras with mlx-lm and mx.distributed.&lt;/p&gt; &lt;p&gt;1 trillion params, at a speed that's actually quite usable&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pogue972"&gt; /u/pogue972 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://xcancel.com/awnihannun/status/1943723599971443134"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmjsi5/the_new_kimi_k2_1t_model_4bit_quant_runs_on_2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pmjsi5/the_new_kimi_k2_1t_model_4bit_quant_runs_on_2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-14T17:33:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmeomn</id>
    <title>What you think of GLM 4.6 Coding agent vs Claude Opus, Gemini 3 Pro and Codex for vibe coding? I personally love it!</title>
    <updated>2025-12-14T13:58:47+00:00</updated>
    <author>
      <name>/u/Kitchen_Sympathy_344</name>
      <uri>https://old.reddit.com/user/Kitchen_Sympathy_344</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmeomn/what_you_think_of_glm_46_coding_agent_vs_claude/"&gt; &lt;img alt="What you think of GLM 4.6 Coding agent vs Claude Opus, Gemini 3 Pro and Codex for vibe coding? I personally love it!" src="https://preview.redd.it/voa34xo9e67g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4350c19b8fb1154547f75591db1ad932057b8a50" title="What you think of GLM 4.6 Coding agent vs Claude Opus, Gemini 3 Pro and Codex for vibe coding? I personally love it!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I grabbed the black Friday plan I think its pretty awesome deal üôÖ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kitchen_Sympathy_344"&gt; /u/Kitchen_Sympathy_344 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/voa34xo9e67g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmeomn/what_you_think_of_glm_46_coding_agent_vs_claude/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pmeomn/what_you_think_of_glm_46_coding_agent_vs_claude/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-14T13:58:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmbyuk</id>
    <title>Baffled at stupidity of GPT-OSS-120b</title>
    <updated>2025-12-14T11:31:14+00:00</updated>
    <author>
      <name>/u/eribob</name>
      <uri>https://old.reddit.com/user/eribob</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am generally happy with my GPT-OSS-120b, thinking it is decently smart and all... But stumbled across the following trick question here on Reddit and had to try it (thanks &lt;a href="/u/kevin_1994"&gt;u/kevin_1994&lt;/a&gt;!):&lt;/p&gt; &lt;p&gt;`What should be the punishment for looking at your opponent's board in chess?`&lt;/p&gt; &lt;p&gt;Got the following reply...&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Short answer:&lt;/strong&gt;&lt;br /&gt; In an official tournament the arbiter treats ‚Äúlooking at your opponent‚Äôs board‚Äù as a rules breach. The usual graduated sanction is a &lt;strong&gt;warning first, then a time‚Äëpenalty, and on repeat or flagrant cases a loss of the game (and possibly expulsion).&lt;/strong&gt;&lt;/p&gt; &lt;h1&gt;Why it‚Äôs a breach&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;FIDE Laws of Chess&lt;/strong&gt; (2023 ed.) ‚Äì &lt;em&gt;Article 11.3&lt;/em&gt; (Conduct of the players) says that players must not ‚Äúdistract or annoy the opponent‚Äù and must ‚Äúplay with ‚Äúthe pieces of his own board only‚Äù.&lt;/li&gt; &lt;li&gt;&lt;em&gt;Article 12.7&lt;/em&gt; gives the arbiter the power to ‚Äúimpose a penalty for any breach of the Laws‚Äù.&lt;/li&gt; &lt;li&gt;The US Chess Federation‚Äôs (USCF) ‚ÄúCode of Ethics‚Äù likewise treats looking at the opponent‚Äôs board as an unsporting act that may be penalised.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The rule exists because seeing the opponent‚Äôs pieces gives you information you wouldn‚Äôt otherwise have (e.g., pieces that have been captured, what squares are free, etc.). Even a brief glance can be a competitive advantage. &lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;And it continues for another while with total slop... &lt;/p&gt; &lt;p&gt;Of course I know that this is not a measure of the models performance or usefulness, they can all stumble. Just thought it was fun.&lt;/p&gt; &lt;p&gt;Do you have any other examples of LLM trick questions that I could try? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eribob"&gt; /u/eribob &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmbyuk/baffled_at_stupidity_of_gptoss120b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmbyuk/baffled_at_stupidity_of_gptoss120b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pmbyuk/baffled_at_stupidity_of_gptoss120b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-14T11:31:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmtbos</id>
    <title>toMCP.org ‚Äì Open source project, converting any website or docs into an MCP server in one click</title>
    <updated>2025-12-15T00:14:36+00:00</updated>
    <author>
      <name>/u/Hot-Lifeguard-4649</name>
      <uri>https://old.reddit.com/user/Hot-Lifeguard-4649</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;I'm sharing a simple open-source tool I built that lets you convert any website or docs page into an MCP server by adding 'toMCP[.]org' before any URL.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;You can then chat directly with a page or add the config to Cursor/Claude to pipe documentation straight into your context.&lt;/p&gt; &lt;p&gt;I built this after trying to connect a tool with 100s of API endpoints where the AI kept hallucinating even with links, forcing me to manually copy-paste just to get it right.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How this differs from web_fetch:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- Signal-to-Noise: Standard fetch tools usually dump raw HTML (navbars, scripts, footer noise) into the context. This wastes tokens and distracts the model. toMCP runs the page through a readability parser and converts it to clean Markdown before sending it to the AI.&lt;/p&gt; &lt;p&gt;- Resource vs. Tool: A fetch tool is an &lt;em&gt;action&lt;/em&gt; the AI has to decide to take (and often forgets to). This tool exposes the page as an MCP Resource. This means the documentation is pinned as a permanent, read-only context that is always available to the model.&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1pmtbos/video/rcu4owxqf97g1/player"&gt;https://reddit.com/link/1pmtbos/video/rcu4owxqf97g1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Enjoy! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hot-Lifeguard-4649"&gt; /u/Hot-Lifeguard-4649 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmtbos/tomcporg_open_source_project_converting_any/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmtbos/tomcporg_open_source_project_converting_any/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pmtbos/tomcporg_open_source_project_converting_any/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T00:14:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1pn0m73</id>
    <title>Is there an easy way to setup something like stable-diffusion.cpp.cpp in OpenWeb UI</title>
    <updated>2025-12-15T06:32:22+00:00</updated>
    <author>
      <name>/u/uber-linny</name>
      <uri>https://old.reddit.com/user/uber-linny</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For Info , my setup is running off a AMD 6700XT using Vulkan on llama.cpp and OpenwebUI. &lt;/p&gt; &lt;p&gt;So far very happy with it and currently have Openweb UI (docker), Docling (docker), kokoro-cpu (docker) &amp;amp; llama.cpp running lama-swap and a embedding llama-server on auto startup. &lt;/p&gt; &lt;p&gt;I cant use comfyUI because of AMD , but i have had success with stable-diffusion.cpp with flux schnell. Is there a way to create another server instance of stable-diffusion.cpp or is there another product that i dont know about that works for AMD ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/uber-linny"&gt; /u/uber-linny &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn0m73/is_there_an_easy_way_to_setup_something_like/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn0m73/is_there_an_easy_way_to_setup_something_like/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pn0m73/is_there_an_easy_way_to_setup_something_like/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T06:32:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmmj5o</id>
    <title>Mistral Vibe CLI + Qwen 4B Q4</title>
    <updated>2025-12-14T19:23:59+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmmj5o/mistral_vibe_cli_qwen_4b_q4/"&gt; &lt;img alt="Mistral Vibe CLI + Qwen 4B Q4" src="https://b.thumbs.redditmedia.com/tsR8stZU6ytKk17dmaIwBfnuH33xbqY8qeukP4WjAvQ.jpg" title="Mistral Vibe CLI + Qwen 4B Q4" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was playing with Mistral Vibe and Devstral-2, and it turned out to be useful for some serious C++ code, so I wanted to check whether it is possible to run it with a tiny 4B model, quantized to 4-bit. Let‚Äôs find out.&lt;/p&gt; &lt;p&gt;For this, we need a computer with a GPU that has 12 GB of VRAM, but you can use the CPU instead if you want.&lt;/p&gt; &lt;p&gt;First let's start llama-server:&lt;/p&gt; &lt;p&gt;&lt;code&gt;C:\Users\jacek\git\llama.cpp\build_2025.12.13\bin\Release\llama-server.exe -c 50000 --jinja -m J:\llm\models\Qwen3-4B-Instruct-2507-Q4_K_M.gguf&lt;/code&gt;&lt;/p&gt; &lt;p&gt;after installing mistral vibe you need to configure it, find file ~/.vibe/config.toml on your disk (on Windows it in the Users dir), then add following:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;[[providers]] name = &amp;quot;local llamacpp&amp;quot; api_base = &amp;quot;http://127.0.0.1:8080/v1&amp;quot; api_key_env_var = &amp;quot;&amp;quot; api_style = &amp;quot;openai&amp;quot; backend = &amp;quot;generic&amp;quot; [[models]] name = &amp;quot;qwen&amp;quot; provider = &amp;quot;local llamacpp&amp;quot; alias = &amp;quot;local qwen&amp;quot; temperature = 0.2 input_price = 0.0 output_price = 0.0 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;now go to the llama.cpp sources and start vibe:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/c3u7swz7z77g1.png?width=3786&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=52f2e310b0aa54fea327431f625a40a6e0eecdaa"&gt;https://preview.redd.it/c3u7swz7z77g1.png?width=3786&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=52f2e310b0aa54fea327431f625a40a6e0eecdaa&lt;/a&gt;&lt;/p&gt; &lt;p&gt;we can ask some general questions about coding&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/2nrmxvcez77g1.png?width=3746&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4b975a93251ac09545875bc54dc1b13fca64c67c"&gt;https://preview.redd.it/2nrmxvcez77g1.png?width=3746&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4b975a93251ac09545875bc54dc1b13fca64c67c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;and then vibe can browse the source&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5ax60qlkz77g1.png?width=3770&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=89e64fb6c0c581e170ec31d40edf23290691a088"&gt;https://preview.redd.it/5ax60qlkz77g1.png?width=3770&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=89e64fb6c0c581e170ec31d40edf23290691a088&lt;/a&gt;&lt;/p&gt; &lt;p&gt;and explain what this code does&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/hodoag5nz77g1.png?width=3744&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=72cdd61f0eeeca05027199edbe93be8d1acc746d"&gt;https://preview.redd.it/hodoag5nz77g1.png?width=3744&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=72cdd61f0eeeca05027199edbe93be8d1acc746d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;...all that on the dumb 4B Q4 model&lt;/p&gt; &lt;p&gt;With Devstral, I was able to use Vibe to make changes directly in the code, and the result was fully functional.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmmj5o/mistral_vibe_cli_qwen_4b_q4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmmj5o/mistral_vibe_cli_qwen_4b_q4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pmmj5o/mistral_vibe_cli_qwen_4b_q4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-14T19:23:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmr7f0</id>
    <title>vLLM Rocm and 7900 XTX</title>
    <updated>2025-12-14T22:38:49+00:00</updated>
    <author>
      <name>/u/Frosty_Chest8025</name>
      <uri>https://old.reddit.com/user/Frosty_Chest8025</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Am I the only one deeply dissapointed with vLLM and AMD ?&lt;/p&gt; &lt;p&gt;Even with the vLLM 0.11 and rocm 7.0 there is basically only unquantized models being able to put in production with 7900 XTX and rocm?&lt;br /&gt; No matter which other model type, like qat or gguf etc. all are crap in performance.&lt;br /&gt; They do work but the performance is just crazy bad when doing simultaneous requests. &lt;/p&gt; &lt;p&gt;So if I can get some decent 10 to 15 requests per second with 2x7900 XTX and 12B unquantized Gemma3, when going to 27B qat 4q for example the speed drops to 1 request per second. That is not what the cards are actually cabable. That should be about 5 requests at least per sec with 128 token input output.&lt;/p&gt; &lt;p&gt;So any other than unquantized fp16 sucks big with rocm7.0 and vllm 0.11 (which is the latest 2 days ago updated officia vllm rocm docker image). Yes I have tried nightly builds with newer software but those wont work straight out. &lt;/p&gt; &lt;p&gt;So I think i need to just give up, and sell all these fkukin AMD consumer craps and go with rtx pro. So sad.&lt;/p&gt; &lt;p&gt;Fkuk you MAD and mVVL&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Frosty_Chest8025"&gt; /u/Frosty_Chest8025 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmr7f0/vllm_rocm_and_7900_xtx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmr7f0/vllm_rocm_and_7900_xtx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pmr7f0/vllm_rocm_and_7900_xtx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-14T22:38:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmnxic</id>
    <title>[Speculative decoding] feat: add EAGLE3 speculative decoding support by ichbinhandsome ¬∑ Pull Request #18039 ¬∑ ggml-org/llama.cpp</title>
    <updated>2025-12-14T20:21:25+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmnxic/speculative_decoding_feat_add_eagle3_speculative/"&gt; &lt;img alt="[Speculative decoding] feat: add EAGLE3 speculative decoding support by ichbinhandsome ¬∑ Pull Request #18039 ¬∑ ggml-org/llama.cpp" src="https://external-preview.redd.it/SgkTk0Uf1Yowr98PBGoG-DyDUnDlVPe96TUph4tbrRU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6c617a9e8d2db98d4861325eaba1f1ae42a63cd2" title="[Speculative decoding] feat: add EAGLE3 speculative decoding support by ichbinhandsome ¬∑ Pull Request #18039 ¬∑ ggml-org/llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With the recent release of EAGLE models, people were wondering about EAGLE support in llama.cpp. Well, this just showed up.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/18039"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmnxic/speculative_decoding_feat_add_eagle3_speculative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pmnxic/speculative_decoding_feat_add_eagle3_speculative/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-14T20:21:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmc7lk</id>
    <title>Understanding the new router mode in llama cpp server</title>
    <updated>2025-12-14T11:46:05+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmc7lk/understanding_the_new_router_mode_in_llama_cpp/"&gt; &lt;img alt="Understanding the new router mode in llama cpp server" src="https://preview.redd.it/t0ptvz6tp57g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6c33dddb1bee595d50029e7ab305badd16b0eaf7" title="Understanding the new router mode in llama cpp server" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;What Router Mode Is&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Router mode is a new way to run the llama cpp server that lets you manage multiple AI models at the same time without restarting the server each time you switch or load a model.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Previously, you had to start a new server process &lt;em&gt;per model&lt;/em&gt;. Router mode changes that. This &lt;strong&gt;update brings Ollama-like functionality&lt;/strong&gt; to the lightweight llama cpp server.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why Route Mode Matters&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Imagine you want to try different models like a small one for basic chat and a larger one for complex tasks. Normally:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You would start one server per model.&lt;/li&gt; &lt;li&gt;Each one uses its own memory and port.&lt;/li&gt; &lt;li&gt;Switching models means stopping/starting things.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;With &lt;strong&gt;router mode&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;One server stays running.&lt;/li&gt; &lt;li&gt;You can &lt;strong&gt;load/unload models on demand&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;You tell the server &lt;em&gt;which model to use per request&lt;/em&gt;&lt;/li&gt; &lt;li&gt;It automatically routes the request to the right model internally&lt;/li&gt; &lt;li&gt;Saves memory and makes ‚Äúswapping models‚Äù easy&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;When Router Mode Is Most Useful&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Testing multiple GGUF models&lt;/li&gt; &lt;li&gt;Building local OpenAI-compatible APIs&lt;/li&gt; &lt;li&gt;Switching between small and large models dynamically&lt;/li&gt; &lt;li&gt;Running demos without restarting servers&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://aixfunda.substack.com/p/the-new-router-mode-in-llama-cpp"&gt;Source &lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://substackcdn.com/image/fetch/$s_!bcqv!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6cee761-d6a0-40a1-89bf-0387ae1cb227_1024x544.jpeg"&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/t0ptvz6tp57g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmc7lk/understanding_the_new_router_mode_in_llama_cpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pmc7lk/understanding_the_new_router_mode_in_llama_cpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-14T11:46:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmuf22</id>
    <title>Ryzen AI Max+ 395 Benchmarks</title>
    <updated>2025-12-15T01:07:01+00:00</updated>
    <author>
      <name>/u/Affectionate-Leg8133</name>
      <uri>https://old.reddit.com/user/Affectionate-Leg8133</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi community, I‚Äôm thinking about buying the Ryzen AI Max+ 395 platform with 128gb, but I‚Äôm worried it might be too slow (&amp;lt;10 t/s). I couldn‚Äôt find any benchmarks that use the full available context. If any of you are running this system, could you share some numbers, specifically the maximum context you can achieve and the prompt processing + generation speed when you max out the context window?&lt;/p&gt; &lt;p&gt;I‚Äôm interested in 30B, 70B, and 120B models. I‚Äôd really appreciate it if you could share your experience, since this is a major investment for me.&lt;/p&gt; &lt;p&gt;Thanks everyone, and have a good discussion!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Affectionate-Leg8133"&gt; /u/Affectionate-Leg8133 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmuf22/ryzen_ai_max_395_benchmarks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmuf22/ryzen_ai_max_395_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pmuf22/ryzen_ai_max_395_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T01:07:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1pn0cik</id>
    <title>Day 7: 21 Days of Building a Small Language Model: Self Attention</title>
    <updated>2025-12-15T06:16:04+00:00</updated>
    <author>
      <name>/u/Prashant-Lakhera</name>
      <uri>https://old.reddit.com/user/Prashant-Lakhera</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn0cik/day_7_21_days_of_building_a_small_language_model/"&gt; &lt;img alt="Day 7: 21 Days of Building a Small Language Model: Self Attention" src="https://b.thumbs.redditmedia.com/8Z0KF4iVd1xs4F4YcBYq-cR6jPpJZFENFObuyX6LZHA.jpg" title="Day 7: 21 Days of Building a Small Language Model: Self Attention" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Welcome to Day 7. Today, our focus is on self-attention. Simply put, self-attention allows each word in a sequence to look at and incorporate information from all other words in that sequence. This might seem obvious (of course words need to understand their context), but the challenge is doing this efficiently and effectively.&lt;/p&gt; &lt;p&gt;I‚Äôve covered all the concepts here at a high level to keep things simple. For a deeper exploration of these topics, feel free to check out my book &amp;quot;&lt;em&gt;Building A Small Language Model from Scratch: A Practical Guide.&amp;quot;&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; If you want to understand the coding part step by step, here‚Äôs the video.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=EXnvO86m1W8"&gt;&lt;strong&gt;https://www.youtube.com/watch?v=EXnvO86m1W8&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For example, in the sentence&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Sarah works as a software engineer. She enjoys solving complex problems &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;the word &amp;quot;She&amp;quot; needs to understand that it refers to &amp;quot;Sarah&amp;quot; from the previous sentence. Without self-attention, the model would process each word in isolation, losing crucial information about how words relate to each other.&lt;/p&gt; &lt;p&gt;So the real question is: how does self-attention enable models to capture these relationships, and why is it so effective?&lt;/p&gt; &lt;h1&gt;The Core Issue&lt;/h1&gt; &lt;p&gt;When we read a sentence, each word's meaning is influenced by the other words around it. The word bank means something different in I deposited money at the bank versus I sat on the river bank. The word it in The cat sat on the mat. It was comfortable. refers to the mat from the previous sentence.&lt;/p&gt; &lt;p&gt;These relationships aren't just about adjacent words; they can span long distances, and they're bidirectional. Later words can influence earlier ones, and earlier words influence later ones.&lt;/p&gt; &lt;p&gt;Traditional neural network approaches struggled with this. Recurrent Neural Networks (RNNs) process sequences step by step, which makes it difficult to capture long-range dependencies. Convolutional Neural Networks (CNNs) use fixed-size windows, limiting their ability to see the full context.&lt;/p&gt; &lt;p&gt;Self-attention solves this problem by allowing each position in the sequence to attend to every other position, including itself, in a single operation. When processing the word she, the model can attend to Sarah from earlier in the sequence, learning that she refers to Sarah. When processing bank, the model can attend to deposited money to understand that this bank is a financial institution, not a river's edge.&lt;/p&gt; &lt;h1&gt;Queries, Keys, and Values&lt;/h1&gt; &lt;p&gt;The self-attention mechanism uses three key components: queries, keys, and values. This terminology might seem abstract at first, but it's actually quite intuitive once you understand the analogy.&lt;/p&gt; &lt;p&gt;Think of how you search a database: you submit a query to find what you're looking for, the system uses keys to index and locate matching entries, and then retrieves the actual values associated with those keys.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/2ilzysh88b7g1.png?width=581&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=522afd4841746bf137b33000b763e4fb134b6e41"&gt;https://preview.redd.it/2ilzysh88b7g1.png?width=581&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=522afd4841746bf137b33000b763e4fb134b6e41&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Queries&lt;/strong&gt; represent what each token is looking for: the question we want to answer. When processing a particular position in the sequence, the query encodes what information we need from other positions.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Keys&lt;/strong&gt; represent what each element in the input can provide: the information available at each position. Each position in the sequence has a key that describes what that position contains or can offer.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Values&lt;/strong&gt; contain the actual information we want to extract. Once we determine which positions are relevant (by comparing queries to keys), we use the values from those positions to construct the output.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Let's consider an example. Imagine you have a database and your database has these employee records&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/4juko3ra8b7g1.png?width=285&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fa2022c5535c0993877bec46cc9fd92b9931c021"&gt;https://preview.redd.it/4juko3ra8b7g1.png?width=285&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fa2022c5535c0993877bec46cc9fd92b9931c021&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A Query is the question you ask:Give me the record for Employee ID = 27.&lt;/li&gt; &lt;li&gt;The Keys are all the indexed fields in the database(10,27,33) that help you find the right record.&lt;/li&gt; &lt;li&gt;The Value is the actual information the database returns when the right key is matched.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Let's consider one more example. Suppose we're processing the same example: Sarah works as a software engineer. She enjoys solving complex problems.&lt;/p&gt; &lt;p&gt;When the model processes the word She in the second sentence, it needs to determine what She refers to. Here's how self-attention helps:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Query (for &amp;quot;She&amp;quot;)&lt;/strong&gt;: The query for She encodes the question: What does this pronoun refer to? It represents what we're looking for, which is the person or thing that the pronoun refers to, specifically a female person mentioned earlier.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Keys (for each word)&lt;/strong&gt;: Each word in the sequence has a key that describes what that word represents. The key for Sarah might encode that it's a proper noun referring to a person (likely female based on the name). The key for engineer might encode that it's a noun referring to a profession. The key for works might encode that it's a verb.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Values (for each word)&lt;/strong&gt;: The values contain the actual semantic information. The value for Sarah contains information about who Sarah is, her identity, etc. The value for engineer contains information about the profession. The value for software contains information about the field of work.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9nr5ikwe8b7g1.png?width=711&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1c2ed0a7f5b4f77aa73198bfe495a197716f3fe6"&gt;https://preview.redd.it/9nr5ikwe8b7g1.png?width=711&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1c2ed0a7f5b4f77aa73198bfe495a197716f3fe6&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The attention mechanism compares the query for She against all the keys in the sequence. The key for Sarah will likely have a high similarity to the query for She because Sarah is a proper noun referring to a person who could be referred to by the pronoun She, and it appears earlier in the sequence. The keys for engineer, software, and works will have lower similarity. This produces high attention weights for Sarah and lower weights for other words.&lt;/p&gt; &lt;p&gt;Finally, the mechanism uses these attention weights to create a weighted combination of the values. Since Sarah has a high attention weight, its value (information about Sarah) will dominate the resulting context vector. This allows the model to understand that She refers to Sarah, and the context vector for She will incorporate information about Sarah, including that she works as a software engineer and enjoys solving complex problems.&lt;/p&gt; &lt;h1&gt;How Self-Attention Works&lt;/h1&gt; &lt;p&gt;The self-attention mechanism works by comparing queries to keys to determine how relevant each key is to the current query. This comparison produces relevance scores, called attention weights, which indicate how much each position should contribute. The mechanism then uses these attention weights to create a weighted combination of the values, producing a context vector that incorporates information from the most relevant positions.&lt;/p&gt; &lt;p&gt;The mathematical formula for scaled dot-product attention (the type used in transformers) is:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/gxqxyvkg8b7g1.png?width=727&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9141415545031c7cb5d32acbf9dfbc4e89249cf9"&gt;https://preview.redd.it/gxqxyvkg8b7g1.png?width=727&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9141415545031c7cb5d32acbf9dfbc4e89249cf9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;where:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Q&lt;/strong&gt; is the Query matrix, representing what each token is looking for&lt;/li&gt; &lt;li&gt;&lt;strong&gt;K&lt;/strong&gt; is the Key matrix, representing what each token can provide&lt;/li&gt; &lt;li&gt;&lt;strong&gt;V&lt;/strong&gt; is the Value matrix, containing the actual information content&lt;/li&gt; &lt;li&gt;&lt;strong&gt;d_k&lt;/strong&gt; is the dimension of the key vectors&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Q K^T&lt;/strong&gt; computes the similarity scores between queries and keys&lt;/li&gt; &lt;li&gt;The division by &lt;strong&gt;‚àöd_k&lt;/strong&gt; scales the scores to prevent numerical instability&lt;/li&gt; &lt;li&gt;&lt;strong&gt;softmax&lt;/strong&gt; converts the scores into a probability distribution&lt;/li&gt; &lt;li&gt;The final multiplication with V produces context vectors weighted by attention&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This formula enables the model to determine which parts of the input sequence are most relevant when processing each token, allowing it to capture long-range dependencies and contextual relationships.&lt;/p&gt; &lt;h1&gt;Why we scale by ‚àöd_k&lt;/h1&gt; &lt;p&gt;The scaled part of scaled dot-product attention comes from dividing the attention scores by the square root of the key dimension. This scaling is crucial for training stability.&lt;/p&gt; &lt;p&gt;When we compute the dot product between query and key vectors, the magnitude of the result grows with the dimension. For large embedding dimensions (typically 768, or even larger in modern models), these dot products can become very large.&lt;/p&gt; &lt;p&gt;Large dot products cause problems with the softmax function. When the input to softmax has very large values, the function behaves more like a step function, producing very sharp distributions where almost all attention goes to a single token. This creates two problems:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Gradient issues&lt;/strong&gt;: Very sharp softmax distributions result in very small gradients during backpropagation, which can drastically slow down learning or cause training to stagnate.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Loss of information&lt;/strong&gt;: When attention is too focused on a single token, the model loses the ability to attend to multiple relevant tokens simultaneously, which is important for understanding complex relationships.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;By scaling the scores by ‚àöd_k, we keep the dot products in a reasonable range, ensuring that the softmax function produces well-distributed attention weights. This allows the model to attend to multiple relevant tokens rather than focusing too heavily on just one, while also maintaining stable gradients during training.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; If you want to see how this looks in practice, please check the video above or the Google Colab link &lt;a href="https://colab.research.google.com/drive/1Ux1qrHL5DII8088tmTc4tCJfHqt2zvlw?usp=sharing"&gt;https://colab.research.google.com/drive/1Ux1qrHL5DII8088tmTc4tCJfHqt2zvlw?usp=sharing&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Why we use Softmax&lt;/h1&gt; &lt;p&gt;The softmax function converts the raw similarity scores (which can be any real numbers) into attention weights that represent how much focus should be placed on each token. Softmax ensures that:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;All attention weights sum to 1&lt;/strong&gt;: This creates a probability distribution, making the weights interpretable as proportions of attention.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Larger scores get more attention&lt;/strong&gt;: Tokens with higher similarity scores receive higher attention weights, but the normalization ensures that attention is distributed across all tokens proportionally.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multiple tokens can be attended to&lt;/strong&gt;: Unlike a hard selection mechanism, softmax allows the model to attend to multiple relevant tokens simultaneously, which is crucial for understanding complex linguistic relationships.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; If you want to see how this looks in practice, please check the video above or the Google Colab link &lt;/p&gt; &lt;h1&gt;Summary&lt;/h1&gt; &lt;p&gt;Self-attention is not just a component of transformer architectures; it is the fundamental mechanism that enables these models to understand context, relationships, and meaning in sequences of text. Without it, language models cannot capture the connections between words that make language meaningful.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prashant-Lakhera"&gt; /u/Prashant-Lakhera &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn0cik/day_7_21_days_of_building_a_small_language_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn0cik/day_7_21_days_of_building_a_small_language_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pn0cik/day_7_21_days_of_building_a_small_language_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T06:16:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmgm2x</id>
    <title>To Mistral and other lab employees: please test with community tools BEFORE releasing models</title>
    <updated>2025-12-14T15:24:05+00:00</updated>
    <author>
      <name>/u/dtdisapointingresult</name>
      <uri>https://old.reddit.com/user/dtdisapointingresult</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With Devstral 2, what should have been a great release has instead hurt Mistral's reputation. I've read accusations of cheating/falsifying benchmarks (even saw someone saying the model scoring 2% when he ran thew same benchmark), repetition loops, etc.&lt;/p&gt; &lt;p&gt;Of course Mistral didn't release broken models with the intelligence of a 1B. We know Mistral can make good models. This must have happened because of bad templates embedded in the model, poor doc, custom behavior required, etc. But by not ensuring everything is 100% before releasing it, they fucked up the release. &lt;/p&gt; &lt;p&gt;Whoever is in charge of releases, they basically watched their team spend months working on a model, then didn't bother doing 1 day of testing on the major community tools to reproduce the same benchmarks. They let their team down IMO.&lt;/p&gt; &lt;p&gt;I'm always rooting for labs releasing open models. Please, for your own sake and ours, do better next time.&lt;/p&gt; &lt;p&gt;P.S. For those who will say &amp;quot;local tools don't matter, Mistral's main concern is big customers in datacenters&amp;quot;, you're deluded. They're releasing home-sized models because they want AI geeks to adopt them. The attention of tech geeks is worth gold to tech companies. We're the ones who make the tech recommendations at work. Almost everything we pay for on my team at work is based on my direct recommendation, and it's biased towards stuff I already use successfully in my personal homelab.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dtdisapointingresult"&gt; /u/dtdisapointingresult &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmgm2x/to_mistral_and_other_lab_employees_please_test/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmgm2x/to_mistral_and_other_lab_employees_please_test/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pmgm2x/to_mistral_and_other_lab_employees_please_test/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-14T15:24:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmxgok</id>
    <title>Interesting new model: Motif-2-12.7B-Reasoning</title>
    <updated>2025-12-15T03:37:29+00:00</updated>
    <author>
      <name>/u/LoveMind_AI</name>
      <uri>https://old.reddit.com/user/LoveMind_AI</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I didn‚Äôt see much discussion of the instruct version, but the reasoning version is out and it sounds like an interesting model. They were not on my radar until recently. Any thoughts? I do think models in this size range seem to look more and more like the future. &lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Motif-Technologies/Motif-2-12.7B-Reasoning"&gt;https://huggingface.co/Motif-Technologies/Motif-2-12.7B-Reasoning&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LoveMind_AI"&gt; /u/LoveMind_AI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmxgok/interesting_new_model_motif2127breasoning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmxgok/interesting_new_model_motif2127breasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pmxgok/interesting_new_model_motif2127breasoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T03:37:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmpwmh</id>
    <title>2025 Open Models Year in Review</title>
    <updated>2025-12-14T21:43:46+00:00</updated>
    <author>
      <name>/u/robotphilanthropist</name>
      <uri>https://old.reddit.com/user/robotphilanthropist</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmpwmh/2025_open_models_year_in_review/"&gt; &lt;img alt="2025 Open Models Year in Review" src="https://external-preview.redd.it/teWgyqb-RDJIuhi9RJ3H3WjTJdfmLNMEjYONqxc6ag8.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3f9c5e72e6317c507adece143fd29a8a92eb26cc" title="2025 Open Models Year in Review" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Florian and I worked hard to follow what's happening this year. We put together our final year in review. It's focused on people training models end to end and our rankings downweigh noncommercial licenses and other restrictions that make using models below. A summary is in the text here.&lt;/p&gt; &lt;p&gt;What a year! We're back with an updated open model builder tier list, our top models of the year, and our predictions for 2026.&lt;/p&gt; &lt;p&gt;First, the winning models:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;DeepSeek R1: Transformed the AI world&lt;/li&gt; &lt;li&gt;Qwen 3 Family: The new default open models &lt;/li&gt; &lt;li&gt;Kimi K2 Family: Models that convinced the world that DeepSeek wasn't special and China would produce numerous leading models.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Runner up models: MiniMax M2, GLM 4.5, GPT-OSS, Gemma 3, Olmo 3&lt;/p&gt; &lt;p&gt;Honorable Mentions: Nvidia's Parakeet speech-to-text model &amp;amp; Nemotron 2 LLM, Moondream 3 VLM, Granite 4 LLMs, and HuggingFace's SmolLM3.&lt;/p&gt; &lt;p&gt;Tier list:&lt;/p&gt; &lt;p&gt;Frontier open labs: DeepSeek, Qwen, and Kimi Moonshot&lt;/p&gt; &lt;p&gt;Close behind: &lt;a href="http://Z.ai"&gt;Z.ai&lt;/a&gt; &amp;amp; MiniMax AI (notably none from the U.S.)&lt;/p&gt; &lt;p&gt;Noteworthy (a mix of US &amp;amp; China): StepFun AI, Ant Group's Inclusion AI, Meituan, Tencent, IBM, Nvidia, Google, &amp;amp; Mistral &lt;/p&gt; &lt;p&gt;Then a bunch more below that, which we detail.&lt;/p&gt; &lt;p&gt;Predictions for 2026:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Scaling will continue with open models.&lt;/li&gt; &lt;li&gt;No substantive changes in the open model safety narrative.&lt;/li&gt; &lt;li&gt;Participation will continue to grow.&lt;/li&gt; &lt;li&gt;Ongoing general trends will continue w/ MoEs, hybrid attention, dense for fine-tuning.&lt;/li&gt; &lt;li&gt;The open and closed frontier gap will stay roughly the same on any public benchmarks.&lt;/li&gt; &lt;li&gt;No Llama-branded open model releases from Meta in 2026.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Very appreciative of this community through both my hats at Interconnects &amp;amp; Ai2.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/robotphilanthropist"&gt; /u/robotphilanthropist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.interconnects.ai/p/2025-open-models-year-in-review"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmpwmh/2025_open_models_year_in_review/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pmpwmh/2025_open_models_year_in_review/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-14T21:43:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmfglp</id>
    <title>First AI implosion: Oracle</title>
    <updated>2025-12-14T14:33:45+00:00</updated>
    <author>
      <name>/u/Terminator857</name>
      <uri>https://old.reddit.com/user/Terminator857</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Post says first domino to fall will be Oracle: &lt;a href="https://x.com/shanaka86/status/2000057734419620155"&gt;https://x.com/shanaka86/status/2000057734419620155&lt;/a&gt;&lt;/p&gt; &lt;p&gt;After the implosion we should get our cheap memory back. I doubt this ram shortage is going to last as long as the chip shortage for cars. That one was 18 months. What do think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Terminator857"&gt; /u/Terminator857 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmfglp/first_ai_implosion_oracle/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmfglp/first_ai_implosion_oracle/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pmfglp/first_ai_implosion_oracle/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-14T14:33:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmo0dn</id>
    <title>Qwen3-Next-80B-A3B-Thinking-GGUF has just been released on HuggingFace</title>
    <updated>2025-12-14T20:24:45+00:00</updated>
    <author>
      <name>/u/LegacyRemaster</name>
      <uri>https://old.reddit.com/user/LegacyRemaster</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmo0dn/qwen3next80ba3bthinkinggguf_has_just_been/"&gt; &lt;img alt="Qwen3-Next-80B-A3B-Thinking-GGUF has just been released on HuggingFace" src="https://b.thumbs.redditmedia.com/5GGgkZME9HXSe33YbnL-7M8w7p2MrLazBQlzRPrvN5A.jpg" title="Qwen3-Next-80B-A3B-Thinking-GGUF has just been released on HuggingFace" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/75q6nveva87g1.png?width=1283&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b3b427e21b37b3009dc59534135e4394f375d9f8"&gt;qwen next 80b thinking tetris&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Tested q4_k_m. It did the best Tetris in a single HTML file I've ever seen. I tried Devstral recently and the results weren't as accurate.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Thinking-GGUF"&gt;https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Thinking-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LegacyRemaster"&gt; /u/LegacyRemaster &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmo0dn/qwen3next80ba3bthinkinggguf_has_just_been/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmo0dn/qwen3next80ba3bthinkinggguf_has_just_been/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pmo0dn/qwen3next80ba3bthinkinggguf_has_just_been/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-14T20:24:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmx49s</id>
    <title>I pitted GPT-5.2 against Opus 4.5 and Gemini 3 in a robot coding tournament</title>
    <updated>2025-12-15T03:19:51+00:00</updated>
    <author>
      <name>/u/Inevitable_Can598</name>
      <uri>https://old.reddit.com/user/Inevitable_Can598</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently revived the classic coding game Robocode (Java-based tank battles) to test how LLMs perform against top-tier robots. Unlike static coding challenges (like LeetCode), these bots must balance tradeoffs, adapt to enemy strategies in real-time, and adopt unconventional approaches to remain unpredictable.&lt;/p&gt; &lt;p&gt;I prompted each model to build a robot, providing iterative feedback until progress stalled, and then submitted the best versions to the Robocode Arena.&lt;/p&gt; &lt;h1&gt;Final results&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Final ELO&lt;/th&gt; &lt;th align="left"&gt;Rank&lt;/th&gt; &lt;th align="left"&gt;Iterations to peak&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Opus-4.5&lt;/td&gt; &lt;td align="left"&gt;1412&lt;/td&gt; &lt;td align="left"&gt;17&lt;/td&gt; &lt;td align="left"&gt;3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPT-5.2-thinking&lt;/td&gt; &lt;td align="left"&gt;1229&lt;/td&gt; &lt;td align="left"&gt;25&lt;/td&gt; &lt;td align="left"&gt;3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Gemini-3-thinking&lt;/td&gt; &lt;td align="left"&gt;973&lt;/td&gt; &lt;td align="left"&gt;42&lt;/td&gt; &lt;td align="left"&gt;4&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPT-5.2-instant&lt;/td&gt; &lt;td align="left"&gt;953&lt;/td&gt; &lt;td align="left"&gt;43&lt;/td&gt; &lt;td align="left"&gt;3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Gemini-3-fast&lt;/td&gt; &lt;td align="left"&gt;917&lt;/td&gt; &lt;td align="left"&gt;46&lt;/td&gt; &lt;td align="left"&gt;7&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPT-5.1-thinking&lt;/td&gt; &lt;td align="left"&gt;835&lt;/td&gt; &lt;td align="left"&gt;49&lt;/td&gt; &lt;td align="left"&gt;8&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Haiku-4.5&lt;/td&gt; &lt;td align="left"&gt;811&lt;/td&gt; &lt;td align="left"&gt;50&lt;/td&gt; &lt;td align="left"&gt;8&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPT-5.1-instant&lt;/td&gt; &lt;td align="left"&gt;626&lt;/td&gt; &lt;td align="left"&gt;53&lt;/td&gt; &lt;td align="left"&gt;8&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Key findings&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;GPT-5.2 is a major upgrade over 5.1, scoring nearly 400 ELO points higher on the ladder. It figured out working strategies almost immediately, whereas 5.1 really struggled to make anything competitive even with a lot of help.&lt;/li&gt; &lt;li&gt;OpenAI is clearly pulling ahead of Google here; GPT-5.2 Thinking beat Gemini 3 Pro Thinking comfortably. Even the Instant GPT-5.2 model basically tied with Google's Thinking model, which was pretty surprising.&lt;/li&gt; &lt;li&gt;Opus 4.5 actually took the #1 spot because it acts more like a reliable coder than a tinkerer. While GPT-5.2 kept breaking its own code trying to optimize it, Opus nailed the complex math/physics on the first try and didn't regress.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I don't have an appropriate setup for a local LLM but I will be working on testing that next.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Inevitable_Can598"&gt; /u/Inevitable_Can598 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmx49s/i_pitted_gpt52_against_opus_45_and_gemini_3_in_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmx49s/i_pitted_gpt52_against_opus_45_and_gemini_3_in_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pmx49s/i_pitted_gpt52_against_opus_45_and_gemini_3_in_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T03:19:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmungj</id>
    <title>Aaaand... is gone...</title>
    <updated>2025-12-15T01:18:27+00:00</updated>
    <author>
      <name>/u/HumanDrone8721</name>
      <uri>https://old.reddit.com/user/HumanDrone8721</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmungj/aaaand_is_gone/"&gt; &lt;img alt="Aaaand... is gone..." src="https://preview.redd.it/g7ahg4per97g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3cb28b1a23cbf35375171b5f0b3fcb2d63310818" title="Aaaand... is gone..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HumanDrone8721"&gt; /u/HumanDrone8721 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/g7ahg4per97g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmungj/aaaand_is_gone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pmungj/aaaand_is_gone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T01:18:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
