<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-14T12:12:02+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1owte4t</id>
    <title>How can one train a LLM with custom reinforcement learning?</title>
    <updated>2025-11-14T10:54:16+00:00</updated>
    <author>
      <name>/u/Odd_Attention_9660</name>
      <uri>https://old.reddit.com/user/Odd_Attention_9660</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;for example, could I train a LLM and give it rewards if it succesfully completes a complex agentic action of my choice?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Odd_Attention_9660"&gt; /u/Odd_Attention_9660 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owte4t/how_can_one_train_a_llm_with_custom_reinforcement/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owte4t/how_can_one_train_a_llm_with_custom_reinforcement/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1owte4t/how_can_one_train_a_llm_with_custom_reinforcement/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-14T10:54:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1ow03a6</id>
    <title>Interesting to see an open-source model genuinely compete with frontier proprietary models for coding</title>
    <updated>2025-11-13T12:44:35+00:00</updated>
    <author>
      <name>/u/Technical_Gene4729</name>
      <uri>https://old.reddit.com/user/Technical_Gene4729</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ow03a6/interesting_to_see_an_opensource_model_genuinely/"&gt; &lt;img alt="Interesting to see an open-source model genuinely compete with frontier proprietary models for coding" src="https://preview.redd.it/l3lt0757s01g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7f3256b064baacc927a42b0d21aa1946bac509bb" title="Interesting to see an open-source model genuinely compete with frontier proprietary models for coding" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So Code Arena just dropped their new live coding benchmark, and the tier 1 results are sparking an interesting open vs proprietary debate.&lt;/p&gt; &lt;p&gt;GLM-4.6 is the only open-source model in the top tier. It's MIT licensed, the most permissive license possible. It's sitting at rank 1 (score: 1372) alongside Claude Opus and GPT-5.&lt;/p&gt; &lt;p&gt;What makes Code Arena different is that it's not static benchmarks. Real developers vote on actual functionality, code quality, and design. Models have to plan, scaffold, debug, and build working web apps step-by-step using tools just like human engineers.&lt;/p&gt; &lt;p&gt;The score gap among the tier 1 clusters is only ~2%. For context, every other model in ranks 6-10 is either proprietary or Apache 2.0 licensed, and they're 94-250 points behind.&lt;/p&gt; &lt;p&gt;This raises some questions. Are we reaching a point where open models can genuinely match frontier proprietary performance for specialized tasks? Or does this only hold for coding, where training data is more abundant?&lt;/p&gt; &lt;p&gt;The fact that it's MIT licensed (not just &amp;quot;open weights&amp;quot;) means you can actually build products with it, modify the architecture, deploy without restrictions, not just run it locally.&lt;/p&gt; &lt;p&gt;Community voting is still early (576-754 votes per model), but it's evaluating real-world functionality, not just benchmark gaming. You can watch the models work: reading files, debugging, iterating.&lt;/p&gt; &lt;p&gt;They're adding multi-file codebases and React support next, which will test architectural planning even more.&lt;/p&gt; &lt;p&gt;Do you think open models will close the gap across the board, or will proprietary labs always stay ahead? And does MIT vs Apache vs &amp;quot;weights only&amp;quot; licensing actually matter for your use cases?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Technical_Gene4729"&gt; /u/Technical_Gene4729 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/l3lt0757s01g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ow03a6/interesting_to_see_an_opensource_model_genuinely/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ow03a6/interesting_to_see_an_opensource_model_genuinely/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-13T12:44:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1owtnu6</id>
    <title>Can I get better performance out of my system for GLM 4.6?</title>
    <updated>2025-11-14T11:09:32+00:00</updated>
    <author>
      <name>/u/spaceman_</name>
      <uri>https://old.reddit.com/user/spaceman_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to run some larger models on my workstation, and since I really love GLM 4.5 Air on my Ryzen AI Max laptop, I tried GLM 4.6 at IQ4 quantization.&lt;/p&gt; &lt;p&gt;Here's what I have so far:&lt;/p&gt; &lt;h2&gt;My hardware:&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Intel Xeon Platinum 8368, 38-cores @ 3.3 GHz&lt;/li&gt; &lt;li&gt;8-channel DDR4, 256GB @ 3200MHz (~200GB/s memory bandwidth)&lt;/li&gt; &lt;li&gt;Radeon 7900 XTX (24GB VRAM)&lt;/li&gt; &lt;li&gt;Fedora 43&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Llama.cpp configuration:&lt;/h2&gt; &lt;p&gt;&lt;code&gt; cmake -B build -DGGML_VULKAN=ON -DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS -DGGML_RPC=O &lt;/code&gt;&lt;/p&gt; &lt;h2&gt;My llama.cpp command line:&lt;/h2&gt; &lt;p&gt;&lt;code&gt; llama-server --flash-attn on --cont-batching -hf unsloth/GLM-4.6-GGUF:IQ4_XS --jinja --ctx-size 0 -ctk q8_0 -ctv q8_0 --cpu-moe -ngl 30 &lt;/code&gt;&lt;/p&gt; &lt;h2&gt;My performance&lt;/h2&gt; &lt;p&gt;This gives me about &lt;strong&gt;4.4 tokens/s&lt;/strong&gt; on low context fill (~2000 tokens). I haven't run anything too long on it yet so can't speak to performance degradation yet.&lt;/p&gt; &lt;p&gt;GPU offloading doesn't seem to help very much, &lt;strong&gt;CPU-only inference gets me ~4.1t/s&lt;/strong&gt;. The number of layers for the GPU was chosen to get ~85% VRAM usage.&lt;/p&gt; &lt;p&gt;Is there anything I'm doing wrong, or that I could do to improve performance on my hardware? Or is this about as good as it gets on small-ish systems?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/spaceman_"&gt; /u/spaceman_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owtnu6/can_i_get_better_performance_out_of_my_system_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owtnu6/can_i_get_better_performance_out_of_my_system_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1owtnu6/can_i_get_better_performance_out_of_my_system_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-14T11:09:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1ow6eba</id>
    <title>What happened to bitnet models?</title>
    <updated>2025-11-13T16:58:49+00:00</updated>
    <author>
      <name>/u/GreenTreeAndBlueSky</name>
      <uri>https://old.reddit.com/user/GreenTreeAndBlueSky</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I thought they were supposed to be this hyper energy efficient solution with simplified matmuls all around but then never heard of them again&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GreenTreeAndBlueSky"&gt; /u/GreenTreeAndBlueSky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ow6eba/what_happened_to_bitnet_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ow6eba/what_happened_to_bitnet_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ow6eba/what_happened_to_bitnet_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-13T16:58:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1owlv9z</id>
    <title>Paper on how LLMs really think and how to leverage it for better results</title>
    <updated>2025-11-14T03:33:11+00:00</updated>
    <author>
      <name>/u/purealgo</name>
      <uri>https://old.reddit.com/user/purealgo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just read a new paper showing that LLMs technically have two ‚Äúmodes‚Äù under the hood:&lt;/p&gt; &lt;p&gt;- Broad, stable pathways ‚Üí used for reasoning, logic, structure&lt;/p&gt; &lt;p&gt;- Narrow, brittle pathways ‚Üí where verbatim memorization and fragile skills (like mathematics) live&lt;/p&gt; &lt;p&gt;Those brittle pathways are exactly where hallucinations, bad math, and wrong facts come from. Those skills literally ride on low curvature, weight directions.&lt;/p&gt; &lt;p&gt;You can exploit this knowledge without training the model. Here are some examples. (these maybe very obvious to you if you've used LLMs long enough)&lt;/p&gt; &lt;p&gt;- Improve accuracy by feeding it structure instead of facts.&lt;/p&gt; &lt;p&gt;Give it raw source material, snippets, or references, and let it reason over them. This pushes it into the stable pathway, which the paper shows barely degrades even when memorization is removed.&lt;/p&gt; &lt;p&gt;- Offload the fragile stuff strategically.&lt;/p&gt; &lt;p&gt;Math and pure recall sit in the wobbly directions, so use the model for multi-step logic but verify the final numbers or facts externally. (Which explains why the chain-of-thought is sometimes perfect and the final sum is not.)&lt;/p&gt; &lt;p&gt;- When the model slips, reframe the prompt.&lt;/p&gt; &lt;p&gt;If you ask for ‚Äúwhat‚Äôs the diet of the Andean fox?‚Äù you‚Äôre hitting brittle recall. But ‚Äúhere‚Äôs a wiki excerpt, synthesize this into a correct summary‚Äù jumps straight into the robust circuits.&lt;/p&gt; &lt;p&gt;‚Ä¢ Give the model micro lenses, not megaphones.&lt;/p&gt; &lt;p&gt;Rather than ‚ÄúTell me about X,‚Äù give it a few hand picked shards of context. The paper shows models behave dramatically better when they reason over snippets instead of trying to dredge them from memory.&lt;/p&gt; &lt;p&gt;The more you treat an LLM like a reasoning engine instead of a knowledge vault, the closer you get to its ‚Äútrue‚Äù strengths.&lt;/p&gt; &lt;p&gt;Here's the link to the paper:&lt;br /&gt; &lt;a href="https://arxiv.org/abs/2510.24256"&gt;https://arxiv.org/abs/2510.24256&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/purealgo"&gt; /u/purealgo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owlv9z/paper_on_how_llms_really_think_and_how_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owlv9z/paper_on_how_llms_really_think_and_how_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1owlv9z/paper_on_how_llms_really_think_and_how_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-14T03:33:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1owa4ag</id>
    <title>Muon Underfits, AdamW Overfits</title>
    <updated>2025-11-13T19:15:53+00:00</updated>
    <author>
      <name>/u/calculatedcontent</name>
      <uri>https://old.reddit.com/user/calculatedcontent</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owa4ag/muon_underfits_adamw_overfits/"&gt; &lt;img alt="Muon Underfits, AdamW Overfits" src="https://preview.redd.it/7294pt9aq21g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1fccde0d30f3e075592bb280616f07456cfeec2f" title="Muon Underfits, AdamW Overfits" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Recently, Muon has been getting some traction as a new and improved optimizer for LLMs and other AI models, a replacement for AdamW that accelerates convergence. What's really going on ?&lt;/p&gt; &lt;p&gt;Using the open-source weightwatcher tool, we can see how it compares to AdamW. Here, we see a typical layer (FC1) from a model (MLP3 on MNIST) trained with Muon (left) and (AdamW) to vert high test accuracy (99.3-99.4%).&lt;/p&gt; &lt;p&gt;On the left, for Muon, we can see that the layer empirical spectral density (ESD) tries to converge to a power law, with PL exponent Œ± ~ 2, as predicted by theory. But the layer has not fully converged, and there is a very pronounced random bulk region that distorts the fit. I suspect this results from the competition from the Muon whitening of the layer update and the NN training that wants to converge to a Power Law.&lt;/p&gt; &lt;p&gt;In contrast, on the right we see the same layer (from a 3-layer MLP), trained with AdamW. Here, AdamW overfits, forming a very heavy tailed PL, but with the weightwatcher Œ± &amp;lt;= 2, just below 2 and slightly overfit.&lt;/p&gt; &lt;p&gt;Both models have pretty good test accuracy, although AdamW is a little bit better than Muon here. And somewhere in between is the theoretically perfect model, with Œ±= 2 for every layer.&lt;/p&gt; &lt;p&gt;(Side note..the SETOL ERG condition is actually satisfied better for Muon than for AdamW, even though the AdamW PL fits look better. So some subtlety here. Stay tuned !)&lt;/p&gt; &lt;p&gt;Want to learn more ? Join us on the weightwatcher community Discord&lt;/p&gt; &lt;p&gt;&lt;a href="https://weightwatcher.ai/"&gt;https://weightwatcher.ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/submit/?source_id=t3_1ow97e0"&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/calculatedcontent"&gt; /u/calculatedcontent &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7294pt9aq21g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owa4ag/muon_underfits_adamw_overfits/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1owa4ag/muon_underfits_adamw_overfits/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-13T19:15:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1owqp8b</id>
    <title>Built a simple tool for long-form text-to-speech + multivoice narration (Kokoro Story)</title>
    <updated>2025-11-14T08:04:25+00:00</updated>
    <author>
      <name>/u/Xerophayze</name>
      <uri>https://old.reddit.com/user/Xerophayze</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been experimenting a lot with the Kokoro TTS model lately and ended up building a small project to make it easier for people to generate long text-to-speech audio and multi-voice narratives without having to piece everything together manually.&lt;/p&gt; &lt;p&gt;If you‚Äôve ever wanted to feed in long passages, stories, or scripts and have them automatically broken up, voiced, and exported, this might help. I put the code on GitHub here:&lt;/p&gt; &lt;p&gt;üîó &lt;a href="https://github.com/Xerophayze/Kokoro-Story"&gt;&lt;strong&gt;https://github.com/Xerophayze/Kokoro-Story&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It‚Äôs nothing fancy, but it solves a problem I kept running into, so I figured others might find it useful too. I really think Kokoro has a ton of potential and deserves more active development‚Äîit's one of the best-sounding non-cloud TTS systems I‚Äôve worked with, especially for multi-voice output.&lt;/p&gt; &lt;p&gt;If anyone wants to try it out, improve it, or suggest features, I‚Äôd love the feedback.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xerophayze"&gt; /u/Xerophayze &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owqp8b/built_a_simple_tool_for_longform_texttospeech/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owqp8b/built_a_simple_tool_for_longform_texttospeech/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1owqp8b/built_a_simple_tool_for_longform_texttospeech/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-14T08:04:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1owoz5x</id>
    <title>70% Price drop from Nous Research for Llama-3.1-405B</title>
    <updated>2025-11-14T06:17:51+00:00</updated>
    <author>
      <name>/u/Local_Youth_882</name>
      <uri>https://old.reddit.com/user/Local_Youth_882</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owoz5x/70_price_drop_from_nous_research_for_llama31405b/"&gt; &lt;img alt="70% Price drop from Nous Research for Llama-3.1-405B" src="https://b.thumbs.redditmedia.com/gQ3QxX9FikLBfwi-X6BnD61zzu3w83iahE6CMmbdoSo.jpg" title="70% Price drop from Nous Research for Llama-3.1-405B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/0k7446lzz51g1.png?width=857&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1de3c501d82998814dc6a1de89f4032bb57c40d6"&gt;Nous Research announcement on price drop&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/mghu34ph061g1.png?width=1292&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5eacc5bf51c0c22e8ae5a3dfde9eb9d0e909960c"&gt;Llama-3.1 405B providers on Openrouter&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Recently Nous Research announced a whopping 70% price drop in API of their Llama finetuned models. I am really surprised on how are they able to serve a 405B dense model at $0.37/1M output??&lt;br /&gt; Is this some software-hardware breakthrough or just some discount to attract users?&lt;br /&gt; If it is the first case, then how come other US providers are charging so much more? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Local_Youth_882"&gt; /u/Local_Youth_882 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owoz5x/70_price_drop_from_nous_research_for_llama31405b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owoz5x/70_price_drop_from_nous_research_for_llama31405b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1owoz5x/70_price_drop_from_nous_research_for_llama31405b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-14T06:17:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1ownirj</id>
    <title>MCP is great in theory, but it‚Äôs not always a blanket yes</title>
    <updated>2025-11-14T04:56:53+00:00</updated>
    <author>
      <name>/u/Miserable_Agent_9006</name>
      <uri>https://old.reddit.com/user/Miserable_Agent_9006</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been building agentic workflows in production lately and spent some time exploring MCP. It‚Äôs clean, standardized, and clearly the direction things are headed.&lt;/p&gt; &lt;p&gt;But I think when you're trying to move fast, it‚Äôs a bit heavy.&lt;/p&gt; &lt;p&gt;- another server to run and maintain&lt;/p&gt; &lt;p&gt;- extra network hops&lt;/p&gt; &lt;p&gt;- schema wrapping + versioning overhead&lt;/p&gt; &lt;p&gt;The lightweight ‚Äúhandshake‚Äù between agents and APIs works well enough for now. MCP makes sense when you‚Äôve got scale, multiple services, or teams to align.&lt;/p&gt; &lt;p&gt;I‚Äôm sure we‚Äôll adopt it eventually, but for now my team and I decided to skip it.&lt;/p&gt; &lt;p&gt;Anyone else taking a similar approach?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Miserable_Agent_9006"&gt; /u/Miserable_Agent_9006 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ownirj/mcp_is_great_in_theory_but_its_not_always_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ownirj/mcp_is_great_in_theory_but_its_not_always_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ownirj/mcp_is_great_in_theory_but_its_not_always_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-14T04:56:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1owanay</id>
    <title>Updated SWE-rebench Results: Sonnet 4.5, GPT-5-Codex, MiniMax M2, Qwen3-Coder, GLM and More on Fresh October 2025 Tasks</title>
    <updated>2025-11-13T19:36:02+00:00</updated>
    <author>
      <name>/u/CuriousPlatypus1881</name>
      <uri>https://old.reddit.com/user/CuriousPlatypus1881</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owanay/updated_swerebench_results_sonnet_45_gpt5codex/"&gt; &lt;img alt="Updated SWE-rebench Results: Sonnet 4.5, GPT-5-Codex, MiniMax M2, Qwen3-Coder, GLM and More on Fresh October 2025 Tasks" src="https://external-preview.redd.it/t4cNt5D638DSOJgsxl8f-7IwJhLpxHIh7HxK5GHcBJE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9b72b5025e78c2cc97de15c8fea348f262235ecb" title="Updated SWE-rebench Results: Sonnet 4.5, GPT-5-Codex, MiniMax M2, Qwen3-Coder, GLM and More on Fresh October 2025 Tasks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We‚Äôve updated the &lt;strong&gt;SWE-rebench&lt;/strong&gt; leaderboard with our October runs on &lt;strong&gt;51 fresh GitHub PR tasks&lt;/strong&gt; (last-month PR issues only).&lt;br /&gt; We‚Äôve also added a new set of &lt;strong&gt;Insights&lt;/strong&gt; highlighting the key findings from these latest evaluations.&lt;/p&gt; &lt;p&gt;Looking forward to your thoughts and suggestions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CuriousPlatypus1881"&gt; /u/CuriousPlatypus1881 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://swe-rebench.com/?insight=oct_2025"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owanay/updated_swerebench_results_sonnet_45_gpt5codex/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1owanay/updated_swerebench_results_sonnet_45_gpt5codex/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-13T19:36:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1ow0jj0</id>
    <title>Running a 1 Trillion Parameter Model on a PC with 128 GB RAM + 24 GB VRAM</title>
    <updated>2025-11-13T13:05:10+00:00</updated>
    <author>
      <name>/u/pulse77</name>
      <uri>https://old.reddit.com/user/pulse77</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi again, just wanted to share that this time I've successfully run &lt;strong&gt;Kimi K2 Thinking (1T parameters)&lt;/strong&gt; on &lt;strong&gt;llama.cpp&lt;/strong&gt; using my desktop setup:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;CPU:&lt;/strong&gt; Intel i9-13900KS&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RAM:&lt;/strong&gt; 128 GB DDR5 @ 4800 MT/s&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPU:&lt;/strong&gt; RTX 4090 (24 GB VRAM)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Storage:&lt;/strong&gt; 4TB NVMe SSD (7300 MB/s read)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm using &lt;strong&gt;Unsloth UD-Q3_K_XL (~3.5 bits)&lt;/strong&gt; from Hugging Face: &lt;a href="https://huggingface.co/unsloth/Kimi-K2-Thinking-GGUF"&gt;https://huggingface.co/unsloth/Kimi-K2-Thinking-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Performance (generation speed):&lt;/strong&gt; 0.42 tokens/sec&lt;/p&gt; &lt;p&gt;(I know, it's slow... but it runs! I'm just stress-testing what's possible on consumer hardware...)&lt;/p&gt; &lt;p&gt;I also tested other huge models - here is a full list with speeds for comparison:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Parameters&lt;/th&gt; &lt;th align="left"&gt;Quant&lt;/th&gt; &lt;th align="left"&gt;Context&lt;/th&gt; &lt;th align="left"&gt;Speed (t/s)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Kimi K2 Thinking&lt;/td&gt; &lt;td align="left"&gt;1T A32B&lt;/td&gt; &lt;td align="left"&gt;UD-Q3_K_XL&lt;/td&gt; &lt;td align="left"&gt;128K&lt;/td&gt; &lt;td align="left"&gt;0.42&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Kimi K2 Instruct 0905&lt;/td&gt; &lt;td align="left"&gt;1T A32B&lt;/td&gt; &lt;td align="left"&gt;UD-Q3_K_XL&lt;/td&gt; &lt;td align="left"&gt;128K&lt;/td&gt; &lt;td align="left"&gt;0.44&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;DeepSeek V3.1 Terminus&lt;/td&gt; &lt;td align="left"&gt;671B A37B&lt;/td&gt; &lt;td align="left"&gt;UD-Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;128K&lt;/td&gt; &lt;td align="left"&gt;0.34&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 Coder 480B Instruct&lt;/td&gt; &lt;td align="left"&gt;480B A35B&lt;/td&gt; &lt;td align="left"&gt;UD-Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;128K&lt;/td&gt; &lt;td align="left"&gt;1.0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GLM 4.6&lt;/td&gt; &lt;td align="left"&gt;355B A32B&lt;/td&gt; &lt;td align="left"&gt;UD-Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;128K&lt;/td&gt; &lt;td align="left"&gt;0.82&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 235B Thinking&lt;/td&gt; &lt;td align="left"&gt;235B A22B&lt;/td&gt; &lt;td align="left"&gt;UD-Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;128K&lt;/td&gt; &lt;td align="left"&gt;5.5&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 235B Instruct&lt;/td&gt; &lt;td align="left"&gt;235B A22B&lt;/td&gt; &lt;td align="left"&gt;UD-Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;128K&lt;/td&gt; &lt;td align="left"&gt;5.6&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MiniMax M2&lt;/td&gt; &lt;td align="left"&gt;230B A10B&lt;/td&gt; &lt;td align="left"&gt;UD-Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;128K&lt;/td&gt; &lt;td align="left"&gt;8.5&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GLM 4.5 Air&lt;/td&gt; &lt;td align="left"&gt;106B A12B&lt;/td&gt; &lt;td align="left"&gt;UD-Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;128K&lt;/td&gt; &lt;td align="left"&gt;11.2&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPT OSS 120B&lt;/td&gt; &lt;td align="left"&gt;120B A5.1B&lt;/td&gt; &lt;td align="left"&gt;MXFP4&lt;/td&gt; &lt;td align="left"&gt;128K&lt;/td&gt; &lt;td align="left"&gt;25.5&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IBM Granite 4.0 H Small&lt;/td&gt; &lt;td align="left"&gt;32B A9B&lt;/td&gt; &lt;td align="left"&gt;UD-Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;128K&lt;/td&gt; &lt;td align="left"&gt;72.2&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 30B Thinking&lt;/td&gt; &lt;td align="left"&gt;30B A3B&lt;/td&gt; &lt;td align="left"&gt;UD-Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;120K&lt;/td&gt; &lt;td align="left"&gt;197.2&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 30B Instruct&lt;/td&gt; &lt;td align="left"&gt;30B A3B&lt;/td&gt; &lt;td align="left"&gt;UD-Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;120K&lt;/td&gt; &lt;td align="left"&gt;218.8&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 30B Coder Instruct&lt;/td&gt; &lt;td align="left"&gt;30B A3B&lt;/td&gt; &lt;td align="left"&gt;UD-Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;120K&lt;/td&gt; &lt;td align="left"&gt;211.2&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPT OSS 20B&lt;/td&gt; &lt;td align="left"&gt;20B A3.6B&lt;/td&gt; &lt;td align="left"&gt;MXFP4&lt;/td&gt; &lt;td align="left"&gt;128K&lt;/td&gt; &lt;td align="left"&gt;223.3&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Command line used (llama.cpp):&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;llama-server --threads 32 --jinja --flash-attn on --cache-type-k q8_0 --cache-type-v q8_0 --model &amp;lt;PATH-TO-YOUR-MODEL&amp;gt; --ctx-size 131072 --n-cpu-moe 9999 --no-warmup&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Important:&lt;/strong&gt; Use &lt;em&gt;--no-warmup&lt;/em&gt; - otherwise, the process can crash before startup.&lt;/p&gt; &lt;p&gt;Notes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Memory mapping (mmap)&lt;/strong&gt; in llama.cpp lets it read model files far beyond RAM capacity.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;No swap/pagefile&lt;/strong&gt; - I disabled these to prevent SSD wear (no disk writes during inference).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Context size:&lt;/strong&gt; Reducing context length didn't improve speed for huge models (token/sec stayed roughly the same).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPU offload:&lt;/strong&gt; llama.cpp automatically uses GPU for all layers unless you limit it. I only use --n-cpu-moe 9999 to keep MoE layers on CPU.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Quantization:&lt;/strong&gt; Anything below ~4 bits noticeably reduces quality. Lowest meaningful quantization for me is UD-Q3_K_XL.&lt;/li&gt; &lt;li&gt;Tried &lt;strong&gt;UD-Q4_K_XL&lt;/strong&gt; for Kimi models, but it failed to start. UD-Q3_K_XL is the max stable setup on my rig.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Speed test method:&lt;/strong&gt; Each benchmark was done using the same prompt - &amp;quot;Explain quantum computing&amp;quot;. The measurement covers the entire generation process until the model finishes its response (so, true end-to-end inference speed).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;llama.cpp version:&lt;/strong&gt; b6963 ‚Äî all tests were run on this version.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;TL;DR - Yes&lt;/strong&gt;, it's possible to run (slowly) a &lt;strong&gt;1-trillion-parameter LLM&lt;/strong&gt; on a machine with &lt;strong&gt;128 GB RAM + 24 GB VRAM&lt;/strong&gt; - no cluster or cloud required. Mostly an experiment to see where the limits really are.&lt;/p&gt; &lt;p&gt;EDIT: Fixed info about IBM Granite model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pulse77"&gt; /u/pulse77 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ow0jj0/running_a_1_trillion_parameter_model_on_a_pc_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ow0jj0/running_a_1_trillion_parameter_model_on_a_pc_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ow0jj0/running_a_1_trillion_parameter_model_on_a_pc_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-13T13:05:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1owpxdd</id>
    <title>Anyone trying out Motif 2 13B?</title>
    <updated>2025-11-14T07:15:12+00:00</updated>
    <author>
      <name>/u/RobotRobotWhatDoUSee</name>
      <uri>https://old.reddit.com/user/RobotRobotWhatDoUSee</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just saw that a S Korean group released this model: &lt;a href="https://huggingface.co/collections/Motif-Technologies/motif-2-127b"&gt;Motif 2 12.7 B&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The benchmarks appear impressive for the size (whatever they are worth).&lt;/p&gt; &lt;p&gt;Has anyone tried this model yet?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RobotRobotWhatDoUSee"&gt; /u/RobotRobotWhatDoUSee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owpxdd/anyone_trying_out_motif_2_13b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owpxdd/anyone_trying_out_motif_2_13b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1owpxdd/anyone_trying_out_motif_2_13b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-14T07:15:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ow3anq</id>
    <title>Rejected for not using LangChain/LangGraph?</title>
    <updated>2025-11-13T15:00:14+00:00</updated>
    <author>
      <name>/u/dougeeai</name>
      <uri>https://old.reddit.com/user/dougeeai</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Today I got rejected after a job interview for not being &amp;quot;technical enough&amp;quot; because I use PyTorch/CUDA/GGUF directly with FastAPI microservices for multi-agent systems instead of LangChain/LangGraph in production.&lt;/p&gt; &lt;p&gt;They asked about 'efficient data movement in LangGraph' - I explained I work at a lower level with bare metal for better performance and control. Later it was revealed they mostly just use APIs to Claude/OpenAI/Bedrock.&lt;/p&gt; &lt;p&gt;I am legitimately asking - not venting - Am I missing something by not using LangChain? Is it becoming a required framework for AI engineering roles, or is this just framework bias?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Should I be adopting it even though I haven't seen performance benefits for my use cases?&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dougeeai"&gt; /u/dougeeai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ow3anq/rejected_for_not_using_langchainlanggraph/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ow3anq/rejected_for_not_using_langchainlanggraph/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ow3anq/rejected_for_not_using_langchainlanggraph/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-13T15:00:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1ovxksu</id>
    <title>Jan-v2-VL: 8B model for long-horizon tasks, improving Qwen3-VL-8B‚Äôs agentic capabilities almost 10x</title>
    <updated>2025-11-13T10:22:48+00:00</updated>
    <author>
      <name>/u/Delicious_Focus3465</name>
      <uri>https://old.reddit.com/user/Delicious_Focus3465</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovxksu/janv2vl_8b_model_for_longhorizon_tasks_improving/"&gt; &lt;img alt="Jan-v2-VL: 8B model for long-horizon tasks, improving Qwen3-VL-8B‚Äôs agentic capabilities almost 10x" src="https://external-preview.redd.it/bmthZnk4cjV4ejBnMYgdXr3Xr8K8l3LMKEIqfiXLStzaSkNnB6704_pmF3PX.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0626efa53bd219b2126a6e5fa2884ec700c482b3" title="Jan-v2-VL: 8B model for long-horizon tasks, improving Qwen3-VL-8B‚Äôs agentic capabilities almost 10x" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, this is Bach from the Jan team. We‚Äôre releasing Jan-v2-VL, an 8B vision‚Äìlanguage model aimed at long-horizon, multi-step tasks starting from browser use.&lt;/p&gt; &lt;p&gt;Jan-v2-VL-high executes 49 steps without failure on the Long-Horizon Execution benchmark, while the base model (Qwen3-VL-8B-Thinking) stops at 5 and other similar-scale VLMs stop between 1 and 2.&lt;/p&gt; &lt;p&gt;Across text and multimodal benchmarks, it matches or slightly improves on the base model, so you get higher long-horizon stability without giving up reasoning or vision quality.&lt;/p&gt; &lt;p&gt;We're releasing 3 variants:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Jan-v2-VL-low (efficiency-oriented)&lt;/li&gt; &lt;li&gt;Jan-v2-VL-med (balanced)&lt;/li&gt; &lt;li&gt;Jan-v2-VL-high (deeper reasoning and longer execution)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;How to run the model&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Download Jan-v2-VL from the Model Hub in Jan&lt;/li&gt; &lt;li&gt;Open the model‚Äôs settings and enable Tools and Vision&lt;/li&gt; &lt;li&gt;Enable BrowserUse MCP (or your preferred MCP setup for browser control)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can also run the model with vLLM or llama.cpp.&lt;/p&gt; &lt;p&gt;Recommended parameters&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;temperature: 1.0&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;top_p: 0.95&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;top_k: 20&lt;/code&gt;&lt;/li&gt; &lt;li&gt;repetition_penalty&lt;code&gt;: 1.0&lt;/code&gt;&lt;/li&gt; &lt;li&gt;presence_penalty&lt;code&gt;: 1.5&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/collections/janhq/jan-v2-vl"&gt;https://huggingface.co/collections/janhq/jan-v2-vl&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Jan app: &lt;a href="https://github.com/janhq/jan"&gt;https://github.com/janhq/jan&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We're also working on a browser extension to make model-driven browser automation faster and more reliable on top of this.&lt;/p&gt; &lt;p&gt;Credit to the Qwen team for the Qwen3-VL-8B-Thinking base model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Delicious_Focus3465"&gt; /u/Delicious_Focus3465 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/go4j38r5xz0g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovxksu/janv2vl_8b_model_for_longhorizon_tasks_improving/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ovxksu/janv2vl_8b_model_for_longhorizon_tasks_improving/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-13T10:22:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1owu2nn</id>
    <title>We built a framework for generating custom RAG evaluation datasets and released a D&amp;D-based one (open-source)</title>
    <updated>2025-11-14T11:31:58+00:00</updated>
    <author>
      <name>/u/mario_candela</name>
      <uri>https://old.reddit.com/user/mario_candela</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owu2nn/we_built_a_framework_for_generating_custom_rag/"&gt; &lt;img alt="We built a framework for generating custom RAG evaluation datasets and released a D&amp;amp;D-based one (open-source)" src="https://external-preview.redd.it/WM9coASZAPosxQnRrgryGmXh5OYtno3uUsf9XZYu3E8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=40dfbae3947d96d45263d794ae5426b261117e4f" title="We built a framework for generating custom RAG evaluation datasets and released a D&amp;amp;D-based one (open-source)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;üîó &lt;a href="https://datapizza.tech/it/blog/aij4r/"&gt;Blog post&lt;/a&gt;&lt;br /&gt; üîó &lt;a href="https://github.com/datapizza-labs/rag-dataset-builder"&gt;GitHub repo&lt;/a&gt;&lt;br /&gt; üîó &lt;a href="https://huggingface.co/datasets/datapizza-ai-lab/dnd5e-srd-qa"&gt;Dataset on Hugging Face&lt;/a&gt;&lt;br /&gt; Would love to hear your thoughts, feedback, or ideas on how to improve this! ‚ù§Ô∏è&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mario_candela"&gt; /u/mario_candela &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="http://datapizza.tech/it/blog/aij4r/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owu2nn/we_built_a_framework_for_generating_custom_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1owu2nn/we_built_a_framework_for_generating_custom_rag/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-14T11:31:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1ow9pdh</id>
    <title>new ops required by Qwen3 Next and Kimi Linear have been merged into llama.cpp</title>
    <updated>2025-11-13T19:00:34+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ow9pdh/new_ops_required_by_qwen3_next_and_kimi_linear/"&gt; &lt;img alt="new ops required by Qwen3 Next and Kimi Linear have been merged into llama.cpp" src="https://external-preview.redd.it/5ziszOa8NRon-ATgGFg5Bv3PXC9P_Gr-hIwXsD0snnU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=61860c512fcc2dad6ebe8431387929cdf3acb61d" title="new ops required by Qwen3 Next and Kimi Linear have been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen3 Next is still in progress &lt;a href="https://github.com/ggml-org/llama.cpp/pull/16095"&gt;https://github.com/ggml-org/llama.cpp/pull/16095&lt;/a&gt;&lt;/p&gt; &lt;p&gt;but this merge was needed to unblock it&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/17063"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ow9pdh/new_ops_required_by_qwen3_next_and_kimi_linear/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ow9pdh/new_ops_required_by_qwen3_next_and_kimi_linear/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-13T19:00:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1ow3kj3</id>
    <title>Qwen model coming soon üëÄ</title>
    <updated>2025-11-13T15:10:39+00:00</updated>
    <author>
      <name>/u/Odd-Ordinary-5922</name>
      <uri>https://old.reddit.com/user/Odd-Ordinary-5922</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ow3kj3/qwen_model_coming_soon/"&gt; &lt;img alt="Qwen model coming soon üëÄ" src="https://preview.redd.it/ibsrtr3ri11g1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=91dd786919de8cd49f495890d2b241fd22bf2f83" title="Qwen model coming soon üëÄ" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Odd-Ordinary-5922"&gt; /u/Odd-Ordinary-5922 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ibsrtr3ri11g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ow3kj3/qwen_model_coming_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ow3kj3/qwen_model_coming_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-13T15:10:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1ows6z3</id>
    <title>Kimi k2 thinking vs Claude Sonnet</title>
    <updated>2025-11-14T09:41:28+00:00</updated>
    <author>
      <name>/u/sebastianmicu24</name>
      <uri>https://old.reddit.com/user/sebastianmicu24</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I will add my personal experience with kimi k2 thinking for my usecase since I saw contrasting opinions. &lt;/p&gt; &lt;p&gt;I needed to cluster some cells from a csv file to see if it would be achievable with my data to do some unsupervised classification of tumor cell/healthy cell. &lt;/p&gt; &lt;p&gt;I tried with claude sonnet 4 and after 2$ in api calls and a bunch of prompts i got no result, it was clustering 99.9% of cells into one group and 0.1% into the other. It was also having difficulties into rendering the cells from the x y positions in the csv. &lt;/p&gt; &lt;p&gt;Kimi k2 thinking achieved a proper clustering in 2 prompts (one for preprocessing of csv data, and one for clustering, maybe it could have done the same in 1 prompt). Total cost 0.17$&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sebastianmicu24"&gt; /u/sebastianmicu24 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ows6z3/kimi_k2_thinking_vs_claude_sonnet/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ows6z3/kimi_k2_thinking_vs_claude_sonnet/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ows6z3/kimi_k2_thinking_vs_claude_sonnet/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-14T09:41:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1ow8j6d</id>
    <title>The return of the modded 4090 48GB</title>
    <updated>2025-11-13T18:17:12+00:00</updated>
    <author>
      <name>/u/king_priam_of_Troy</name>
      <uri>https://old.reddit.com/user/king_priam_of_Troy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ow8j6d/the_return_of_the_modded_4090_48gb/"&gt; &lt;img alt="The return of the modded 4090 48GB" src="https://b.thumbs.redditmedia.com/LgVvEhALpx4xDsp8AmNOoxzbQcsL_pEGUDe4iwXtJ-M.jpg" title="The return of the modded 4090 48GB" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Last month I bought a 4090 48GB in ShenZhen. I had to put this project on hold for a while but it's back.&lt;/p&gt; &lt;p&gt;The card is really fast even with my poor Gen3 4x PCIe connector. I can't put it inside as I can't find any compatible power cable.&lt;/p&gt; &lt;p&gt;I'm running at 150 tokens/second with GPT-OSS 20B from my first tests.&lt;/p&gt; &lt;p&gt;(This is a follow up of &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nifajh/i%5C_bought%5C_a%5C_modded%5C_4090%5C_48gb%5C_in%5C_shenzhen%5C_this%5C_is/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1nifajh/i\_bought\_a\_modded\_4090\_48gb\_in\_shenzhen\_this\_is/&lt;/a&gt;)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/king_priam_of_Troy"&gt; /u/king_priam_of_Troy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ow8j6d"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ow8j6d/the_return_of_the_modded_4090_48gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ow8j6d/the_return_of_the_modded_4090_48gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-13T18:17:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ow6a9i</id>
    <title>IBM's AI Researchers Patented a 200 yr old Math Technique by Rebranding as AI Interpretability</title>
    <updated>2025-11-13T16:54:38+00:00</updated>
    <author>
      <name>/u/DataBaeBee</name>
      <uri>https://old.reddit.com/user/DataBaeBee</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ow6a9i/ibms_ai_researchers_patented_a_200_yr_old_math/"&gt; &lt;img alt="IBM's AI Researchers Patented a 200 yr old Math Technique by Rebranding as AI Interpretability" src="https://external-preview.redd.it/bnA5cnNld3owMjFnMV58D9bda3Jb0zpLqYjHalvpbPpYKPrlCJRkL-iXGaPt.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fc24dc8d853c78013954d7d9f658801ad9c1a4e7" title="IBM's AI Researchers Patented a 200 yr old Math Technique by Rebranding as AI Interpretability" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;IBM AI researchers implemented a Continued Fraction class as linear layers in Pytorch and was awarded a patent for calling backward() on the computation graph. It's pretty bizarre.&lt;/p&gt; &lt;p&gt;Anyone who uses derivatives/power series to work with continued fractions is affected.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Mechanical engineers, Robotics and Industrialists - you can't use Pytorch to find the best number of teeth for your desired gear ratios lest you interfere with IBM's patent.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Pure Mathematicians and Math Educators - I learnt about the patent while investigating Continued Fractions and their relation to elliptic curves. I needed to find an approximate relationship and while I was writing in Torch I stumbled upon the patent.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Numerical programmers - continued fractions and their derivatives are used to approximate errors in algorithm design.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Here's the &lt;a href="https://leetarxiv.substack.com/p/ibm-patented-eulers-fractions"&gt;complete writeup with patent links&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DataBaeBee"&gt; /u/DataBaeBee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/nddv4ewz021g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ow6a9i/ibms_ai_researchers_patented_a_200_yr_old_math/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ow6a9i/ibms_ai_researchers_patented_a_200_yr_old_math/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-13T16:54:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1owmtkt</id>
    <title>I brought CUDA back to macOS. Not because it was useful ‚Äî because nobody else could.</title>
    <updated>2025-11-14T04:20:16+00:00</updated>
    <author>
      <name>/u/Adept_Tip8375</name>
      <uri>https://old.reddit.com/user/Adept_Tip8375</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owmtkt/i_brought_cuda_back_to_macos_not_because_it_was/"&gt; &lt;img alt="I brought CUDA back to macOS. Not because it was useful ‚Äî because nobody else could." src="https://external-preview.redd.it/YHp6xAwqBe8oZ_OrMdwTyJjRYCv9-wbk4V-lSqlUI3I.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6e28774fdaaba15ba19d667058a3967b4695ebc8" title="I brought CUDA back to macOS. Not because it was useful ‚Äî because nobody else could." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;just resurrected CUDA on High Sierra in 2025&lt;br /&gt; Apple killed it 2018, NVIDIA killed drivers 2021&lt;br /&gt; now my 1080 Ti is doing 11 TFLOPs under PyTorch again&lt;br /&gt; ‚Äúimpossible‚Äù they said&lt;br /&gt; &lt;a href="https://github.com/careunix/PyTorch-HighSierra-CUDA-Revival"&gt;https://github.com/careunix/PyTorch-HighSierra-CUDA-Revival&lt;/a&gt;&lt;br /&gt; who still runs 10.13 in 2025 üòÇ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Adept_Tip8375"&gt; /u/Adept_Tip8375 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owmtkt/i_brought_cuda_back_to_macos_not_because_it_was/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owmtkt/i_brought_cuda_back_to_macos_not_because_it_was/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1owmtkt/i_brought_cuda_back_to_macos_not_because_it_was/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-14T04:20:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1owskm6</id>
    <title>Windows llama.cpp is 20% faster</title>
    <updated>2025-11-14T10:05:21+00:00</updated>
    <author>
      <name>/u/johannes_bertens</name>
      <uri>https://old.reddit.com/user/johannes_bertens</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owskm6/windows_llamacpp_is_20_faster/"&gt; &lt;img alt="Windows llama.cpp is 20% faster" src="https://preview.redd.it/tfdcbkf6571g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6f97a0d3f3c6a2519462ab5e159f2045396e9409" title="Windows llama.cpp is 20% faster" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;But why?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Windows: 1000+ PP&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;llama-bench -m C:\Users\johan\.lmstudio\models\unsloth\Qwen3-VL-30B-A3B-Instruct-GGUF\Qwen3-VL-30B-A3B-Instruct-UD-Q8_K_XL.gguf -p 512,1024,2048,4096 -n 0 -fa 0 --mmap 0&lt;br /&gt; load_backend: loaded RPC backend from C:\Users\johan\Downloads\llama-b7032-bin-win-vulkan-x64\ggml-rpc.dll&lt;br /&gt; ggml_vulkan: Found 1 Vulkan devices:&lt;br /&gt; ggml_vulkan: 0 = AMD Radeon(TM) 8060S Graphics (AMD proprietary driver) | uma: 1 | fp16: 1 | bf16: 1 | warp size: 64 | shared memory: 32768 | int dot: 1 | matrix cores: KHR_coopmat&lt;br /&gt; load_backend: loaded Vulkan backend from C:\Users\johan\Downloads\llama-b7032-bin-win-vulkan-x64\ggml-vulkan.dll&lt;br /&gt; load_backend: loaded CPU backend from C:\Users\johan\Downloads\llama-b7032-bin-win-vulkan-x64\ggml-cpu-icelake.dll &lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;model &lt;/th&gt; &lt;th align="right"&gt; size&lt;/th&gt; &lt;th align="right"&gt; params&lt;/th&gt; &lt;th&gt;backend &lt;/th&gt; &lt;th align="right"&gt;ngl&lt;/th&gt; &lt;th align="right"&gt;mmap&lt;/th&gt; &lt;th align="right"&gt; test&lt;/th&gt; &lt;th align="right"&gt; t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;qwen3vlmoe 30B.A3B Q8_0 &lt;/td&gt; &lt;td align="right"&gt; 33.51 GiB&lt;/td&gt; &lt;td align="right"&gt; 30.53 B&lt;/td&gt; &lt;td&gt;Vulkan &lt;/td&gt; &lt;td align="right"&gt; 99&lt;/td&gt; &lt;td align="right"&gt; 0&lt;/td&gt; &lt;td align="right"&gt; pp512&lt;/td&gt; &lt;td align="right"&gt; 1079.12 ¬± 4.32&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3vlmoe 30B.A3B Q8_0 &lt;/td&gt; &lt;td align="right"&gt; 33.51 GiB&lt;/td&gt; &lt;td align="right"&gt; 30.53 B&lt;/td&gt; &lt;td&gt;Vulkan &lt;/td&gt; &lt;td align="right"&gt; 99&lt;/td&gt; &lt;td align="right"&gt; 0&lt;/td&gt; &lt;td align="right"&gt; pp1024&lt;/td&gt; &lt;td align="right"&gt; 975.04 ¬± 4.46&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3vlmoe 30B.A3B Q8_0 &lt;/td&gt; &lt;td align="right"&gt; 33.51 GiB&lt;/td&gt; &lt;td align="right"&gt; 30.53 B&lt;/td&gt; &lt;td&gt;Vulkan &lt;/td&gt; &lt;td align="right"&gt; 99&lt;/td&gt; &lt;td align="right"&gt; 0&lt;/td&gt; &lt;td align="right"&gt; pp2048&lt;/td&gt; &lt;td align="right"&gt; 892.94 ¬± 2.49&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3vlmoe 30B.A3B Q8_0 &lt;/td&gt; &lt;td align="right"&gt; 33.51 GiB&lt;/td&gt; &lt;td align="right"&gt; 30.53 B&lt;/td&gt; &lt;td&gt;Vulkan &lt;/td&gt; &lt;td align="right"&gt; 99&lt;/td&gt; &lt;td align="right"&gt; 0&lt;/td&gt; &lt;td align="right"&gt; pp4096&lt;/td&gt; &lt;td align="right"&gt; 806.84 ¬± 2.89&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Linux: 880 PP&lt;/strong&gt;&lt;/p&gt; &lt;p&gt; [johannes@toolbx ~]$ llama-bench -m models/Qwen3-VL-30B-A3B-Instruct-UD-Q8_K_XL.gguf -p 512,1024,2048,4096 -n 0 -fa 0 --mmap 0&lt;br /&gt; ggml_vulkan: Found 1 Vulkan devices:&lt;br /&gt; ggml_vulkan: 0 = Radeon 8060S Graphics (RADV GFX1151) (radv) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 1 | matrix cores: KHR_coopmat &lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;model &lt;/th&gt; &lt;th align="right"&gt; size&lt;/th&gt; &lt;th align="right"&gt; params&lt;/th&gt; &lt;th&gt;backend &lt;/th&gt; &lt;th align="right"&gt;ngl&lt;/th&gt; &lt;th align="right"&gt;mmap&lt;/th&gt; &lt;th align="right"&gt; test&lt;/th&gt; &lt;th align="right"&gt; t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;qwen3vlmoe 30B.A3B Q8_0 &lt;/td&gt; &lt;td align="right"&gt; 33.51 GiB&lt;/td&gt; &lt;td align="right"&gt; 30.53 B&lt;/td&gt; &lt;td&gt;Vulkan &lt;/td&gt; &lt;td align="right"&gt; 99&lt;/td&gt; &lt;td align="right"&gt; 0&lt;/td&gt; &lt;td align="right"&gt; pp512&lt;/td&gt; &lt;td align="right"&gt; 876.79 ¬± 4.76&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3vlmoe 30B.A3B Q8_0 &lt;/td&gt; &lt;td align="right"&gt; 33.51 GiB&lt;/td&gt; &lt;td align="right"&gt; 30.53 B&lt;/td&gt; &lt;td&gt;Vulkan &lt;/td&gt; &lt;td align="right"&gt; 99&lt;/td&gt; &lt;td align="right"&gt; 0&lt;/td&gt; &lt;td align="right"&gt; pp1024&lt;/td&gt; &lt;td align="right"&gt; 797.87 ¬± 1.56&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3vlmoe 30B.A3B Q8_0 &lt;/td&gt; &lt;td align="right"&gt; 33.51 GiB&lt;/td&gt; &lt;td align="right"&gt; 30.53 B&lt;/td&gt; &lt;td&gt;Vulkan &lt;/td&gt; &lt;td align="right"&gt; 99&lt;/td&gt; &lt;td align="right"&gt; 0&lt;/td&gt; &lt;td align="right"&gt; pp2048&lt;/td&gt; &lt;td align="right"&gt; 757.55 ¬± 2.10&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3vlmoe 30B.A3B Q8_0 &lt;/td&gt; &lt;td align="right"&gt; 33.51 GiB&lt;/td&gt; &lt;td align="right"&gt; 30.53 B&lt;/td&gt; &lt;td&gt;Vulkan &lt;/td&gt; &lt;td align="right"&gt; 99&lt;/td&gt; &lt;td align="right"&gt; 0&lt;/td&gt; &lt;td align="right"&gt; pp4096&lt;/td&gt; &lt;td align="right"&gt; 686.61 ¬± 0.89&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Obviously it's not 20% over the board, but still a very big difference. Is the &amp;quot;AMD proprietary driver&amp;quot; such a big deal?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/johannes_bertens"&gt; /u/johannes_bertens &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/tfdcbkf6571g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owskm6/windows_llamacpp_is_20_faster/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1owskm6/windows_llamacpp_is_20_faster/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-14T10:05:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1owocd2</id>
    <title>Is it normal to hear weird noises when running an LLM on 4√ó Pro 6000 Max-Q cards?</title>
    <updated>2025-11-14T05:41:14+00:00</updated>
    <author>
      <name>/u/PlusProfession9245</name>
      <uri>https://old.reddit.com/user/PlusProfession9245</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owocd2/is_it_normal_to_hear_weird_noises_when_running_an/"&gt; &lt;img alt="Is it normal to hear weird noises when running an LLM on 4√ó Pro 6000 Max-Q cards?" src="https://external-preview.redd.it/MnFzdzJ0b3l0NTFnMbphl7ifhldDVQJssqSE3uLNJKqrQJ4o9dG0SGtQf767.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3b2e8e94666721a90be11f2cea3b9f593dc28f21" title="Is it normal to hear weird noises when running an LLM on 4√ó Pro 6000 Max-Q cards?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It doesn‚Äôt sound like normal coil whine.&lt;br /&gt; In a Docker environment, when I run gpt-oss-120b across 4 GPUs, I hear a strange noise.&lt;br /&gt; The sound is also different depending on the model.&lt;br /&gt; Is this normal??&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PlusProfession9245"&gt; /u/PlusProfession9245 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9eez1soyt51g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owocd2/is_it_normal_to_hear_weird_noises_when_running_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1owocd2/is_it_normal_to_hear_weird_noises_when_running_an/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-14T05:41:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1oth5pw</id>
    <title>AMA With Moonshot AI, The Open-source Frontier Lab Behind Kimi K2 Thinking Model</title>
    <updated>2025-11-10T15:44:10+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/"&gt; &lt;img alt="AMA With Moonshot AI, The Open-source Frontier Lab Behind Kimi K2 Thinking Model" src="https://b.thumbs.redditmedia.com/yz9_FpdLcHNiCkaH5fLEIoXS2f5u5twNBr7SQ9Go3AI.jpg" title="AMA With Moonshot AI, The Open-source Frontier Lab Behind Kimi K2 Thinking Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Today we are having &lt;strong&gt;Moonshot AI&lt;/strong&gt;, the research lab behind the &lt;strong&gt;Kimi&lt;/strong&gt; &lt;strong&gt;models&lt;/strong&gt;. We‚Äôre excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="/u/ComfortableAsk4494"&gt;u/ComfortableAsk4494&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="/u/zxytim"&gt;u/zxytim&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="/u/ppwwyyxx"&gt;u/ppwwyyxx&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 8 AM ‚Äì 11 AM PST, with the Kimi team continuing to follow up on questions over the next 24 hours.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5yg0ncsn7g0g1.png?width=3525&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5318680204ef7502ad349aec148147d9e3398f87"&gt;https://preview.redd.it/5yg0ncsn7g0g1.png?width=3525&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5318680204ef7502ad349aec148147d9e3398f87&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Thanks everyone for joining our AMA. The live part has ended and the Kimi team will be following up with more answers sporadically over the next 24 hours.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T15:44:10+00:00</published>
  </entry>
</feed>
