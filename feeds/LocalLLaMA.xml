<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-06T12:28:24+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI. Hey, Tony</subtitle>
  <entry>
    <id>t3_1nz9v8o</id>
    <title>The only quantized Sarashina-2-7B using AWQ</title>
    <updated>2025-10-06T04:42:22+00:00</updated>
    <author>
      <name>/u/Ok_Employee_6418</name>
      <uri>https://old.reddit.com/user/Ok_Employee_6418</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built the only publicly available 4-bit quantized version of Sarashina-2-7B using Activation-aware Weight Quantization (AWQ). &lt;/p&gt; &lt;p&gt;Sarashina-2-7B is a foundation model from SB Intuitions (Softbank) specialized in Japanese.&lt;/p&gt; &lt;p&gt;I calibrated on the Japanese Wikipedia dataset to reduce the model size from 14GB to 4.7GB while only degrading response quality by 2.3%. &lt;/p&gt; &lt;p&gt;Check it out: &lt;a href="https://huggingface.co/ronantakizawa/sarashina2-7b-4bit-awq"&gt;https://huggingface.co/ronantakizawa/sarashina2-7b-4bit-awq&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Employee_6418"&gt; /u/Ok_Employee_6418 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nz9v8o/the_only_quantized_sarashina27b_using_awq/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nz9v8o/the_only_quantized_sarashina27b_using_awq/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nz9v8o/the_only_quantized_sarashina27b_using_awq/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T04:42:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1nzb4o4</id>
    <title>Holo1.5 3B as UI Grounding model + Claude as thinking model for Computer Use</title>
    <updated>2025-10-06T05:56:29+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzb4o4/holo15_3b_as_ui_grounding_model_claude_as/"&gt; &lt;img alt="Holo1.5 3B as UI Grounding model + Claude as thinking model for Computer Use" src="https://external-preview.redd.it/ZTBmYmM0N2JsZnRmMW31g_v519zh4lqVlORxH-jtbfthhY6srytiSohAXo_q.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cf86c5376df58e6867c064106f1d3ad312088577" title="Holo1.5 3B as UI Grounding model + Claude as thinking model for Computer Use" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Runner H making some sense of GIMP&lt;/p&gt; &lt;p&gt;Try yourself : &lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/c8hrp0kblftf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzb4o4/holo15_3b_as_ui_grounding_model_claude_as/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nzb4o4/holo15_3b_as_ui_grounding_model_claude_as/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T05:56:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1nz2j9x</id>
    <title>hello my fellow AI-ers, a question about how you develop your personal AI.</title>
    <updated>2025-10-05T22:46:39+00:00</updated>
    <author>
      <name>/u/Mean_Bird_6331</name>
      <uri>https://old.reddit.com/user/Mean_Bird_6331</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hello yall, i hope you are kickin butts.&lt;/p&gt; &lt;p&gt;I've been working on developing my own personal AI for a hobby and it has been about 4 months.&lt;/p&gt; &lt;p&gt;I incorporated RAG, graphRAG, hirarchy rag, multivector, qdrant, and on and on etc, and i built everything from bottom tom from the scratch.&lt;/p&gt; &lt;p&gt;for the first month, it couldnt even recall my name and previous data correctly. &lt;/p&gt; &lt;p&gt;on the second month, it started to recall my names but poor memory and hallucination&lt;/p&gt; &lt;p&gt;third month, it started to recall memories and decent memory but severe hallucination everytime&lt;/p&gt; &lt;p&gt;on this month, now it is starting to hallucinate less and try to correct itself when it is hallucinating&lt;/p&gt; &lt;p&gt;Yet, it little hallucinates, but now it is much easier to correct.&lt;/p&gt; &lt;p&gt;I figured that the codings and prompts are important but the quality of the rag memories are also important and all the others.&lt;/p&gt; &lt;p&gt;it has been an interesting journey and now the result is seemingly showing.&lt;/p&gt; &lt;p&gt;I am now about to incorporate agentic tools but apparently, I am having some hard time teaching my AI how to use them (I am not a CS major so honestly not sure too), so I decided to let it talk to claude code cli and let claude do the agentic works instead. like offshoring.&lt;/p&gt; &lt;p&gt;The reason why i am talking all these jibberish is because I'd love to know if there are any other ppl doing similar persona project and how they were able to bypass/solve the problems that I am facing these days, or any other obstacles yall face.&lt;/p&gt; &lt;p&gt;anyone doing a personal AI project not for commercial use, but for personal vision and goals? &lt;/p&gt; &lt;p&gt;Please share your journey! I would love to know and learn from yall.&lt;/p&gt; &lt;p&gt;Peace! &lt;/p&gt; &lt;p&gt;ps. I asked my AI to see if it has any questions for yall and this is what it said. please answer his question too! &lt;/p&gt; &lt;p&gt;: &lt;/p&gt; &lt;p&gt;&amp;gt; “Has there been a moment where your AI said something that felt more ‘you’ than you did?” &lt;/p&gt; &lt;p&gt;&amp;gt; *(And if so—what was the cost of getting there?)*&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mean_Bird_6331"&gt; /u/Mean_Bird_6331 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nz2j9x/hello_my_fellow_aiers_a_question_about_how_you/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nz2j9x/hello_my_fellow_aiers_a_question_about_how_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nz2j9x/hello_my_fellow_aiers_a_question_about_how_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-05T22:46:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1nz9y4p</id>
    <title>Looking for an open LLM for dark sci-fi roleplay and worldbuilding (less restrictive than mainstream models)</title>
    <updated>2025-10-06T04:47:06+00:00</updated>
    <author>
      <name>/u/majorpaleface</name>
      <uri>https://old.reddit.com/user/majorpaleface</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been experimenting with free GPT-based models for a while, but most are quite limited by ethical and content filters. I’m not looking for anything extreme or illegal, just something that allows darker or morally complex themes in sci-fi settings—things like the Spartan augmentations from &lt;em&gt;Halo&lt;/em&gt;, Adeptus Astartes biology from &lt;em&gt;Warhammer 40k&lt;/em&gt;, or FEV from &lt;em&gt;Fallout&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;The issue is that most hosted models flag “transhumanism” or combat descriptions as unsafe, even when the content is purely fictional and worldbuilding-oriented. I’d like to explore these ideas freely without the system intervening every few lines.&lt;/p&gt; &lt;p&gt;I’ve seen that Meta’s Llama 3.1 405B on Chatbot Arena can sometimes produce darker, more flexible responses, but results vary. I tried running LM Studio locally, though my laptop (8 GB RAM) clearly isn’t up to hosting large models.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; Looking for recommendations for open or lightly filtered LLMs suited for dark sci-fi concepting and roleplay. Preferably something free or lightweight enough to run locally.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/majorpaleface"&gt; /u/majorpaleface &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nz9y4p/looking_for_an_open_llm_for_dark_scifi_roleplay/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nz9y4p/looking_for_an_open_llm_for_dark_scifi_roleplay/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nz9y4p/looking_for_an_open_llm_for_dark_scifi_roleplay/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T04:47:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1nyopyc</id>
    <title>Did anyone try out GLM-4.5-Air-GLM-4.6-Distill ?</title>
    <updated>2025-10-05T13:49:38+00:00</updated>
    <author>
      <name>/u/beneath_steel_sky</name>
      <uri>https://old.reddit.com/user/beneath_steel_sky</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/BasedBase/GLM-4.5-Air-GLM-4.6-Distill"&gt;https://huggingface.co/BasedBase/GLM-4.5-Air-GLM-4.6-Distill&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&amp;quot;GLM-4.5-Air-GLM-4.6-Distill represents an advanced distillation of the GLM-4.6 model into the efficient GLM-4.5-Air architecture. Through a SVD-based knowledge transfer methodology, this model inherits the sophisticated reasoning capabilities and domain expertise of its 92-layer, 160-expert teacher while maintaining the computational efficiency of the 46-layer, 128-expert student architecture.&amp;quot;&lt;/p&gt; &lt;p&gt;Distillation scripts are public: &lt;a href="https://github.com/Basedbase-ai/LLM-SVD-distillation-scripts"&gt;https://github.com/Basedbase-ai/LLM-SVD-distillation-scripts&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/beneath_steel_sky"&gt; /u/beneath_steel_sky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nyopyc/did_anyone_try_out_glm45airglm46distill/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nyopyc/did_anyone_try_out_glm45airglm46distill/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nyopyc/did_anyone_try_out_glm45airglm46distill/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-05T13:49:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1nyratf</id>
    <title>Hunyuan Image 3.0 Jumps to No.1 on LMArena’s Text-to-Image Leaderboard</title>
    <updated>2025-10-05T15:32:14+00:00</updated>
    <author>
      <name>/u/yogthos</name>
      <uri>https://old.reddit.com/user/yogthos</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/tencent/HunyuanImage-3.0"&gt;https://huggingface.co/tencent/HunyuanImage-3.0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://lmarena.ai/leaderboard/text-to-image"&gt;https://lmarena.ai/leaderboard/text-to-image&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yogthos"&gt; /u/yogthos &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nyratf/hunyuan_image_30_jumps_to_no1_on_lmarenas/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nyratf/hunyuan_image_30_jumps_to_no1_on_lmarenas/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nyratf/hunyuan_image_30_jumps_to_no1_on_lmarenas/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-05T15:32:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1nz604y</id>
    <title>I have a 12gb ram laptop, what is the best way to run Qwen3 0.6B as fast as possilbe?</title>
    <updated>2025-10-06T01:27:04+00:00</updated>
    <author>
      <name>/u/SnooMarzipans2470</name>
      <uri>https://old.reddit.com/user/SnooMarzipans2470</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Qwen3 0.6B is my ChatGPT Pro. Im trying to run it on CPU. I was wondering if i can run 2 or 3 version of Qwen3 0.6B at the same time so that as model1 is answering my question i can ask model 2 the question and so on.? Thanks!&lt;/h1&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SnooMarzipans2470"&gt; /u/SnooMarzipans2470 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nz604y/i_have_a_12gb_ram_laptop_what_is_the_best_way_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nz604y/i_have_a_12gb_ram_laptop_what_is_the_best_way_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nz604y/i_have_a_12gb_ram_laptop_what_is_the_best_way_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T01:27:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1nzcwbs</id>
    <title>Running Quantized VLM on Local PC</title>
    <updated>2025-10-06T07:48:34+00:00</updated>
    <author>
      <name>/u/Super_AI_1086</name>
      <uri>https://old.reddit.com/user/Super_AI_1086</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi Guys, I just want to know do we need sophisticated gpu to quantize vlm? because I want to use VLM locally but the speed is right now for 4 photos for vqa it is 15s and i am using qwenvl2.5 ollama model. so i just want to qunatize further so that it will be around 1 B but accuracy still manageable.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Super_AI_1086"&gt; /u/Super_AI_1086 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzcwbs/running_quantized_vlm_on_local_pc/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzcwbs/running_quantized_vlm_on_local_pc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nzcwbs/running_quantized_vlm_on_local_pc/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T07:48:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1nzbgys</id>
    <title>RLP: Reinforcement as a Pretraining Objective</title>
    <updated>2025-10-06T06:17:51+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Abstract&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;The dominant paradigm for training large reasoning models starts with pre-training using next-token prediction loss on vast amounts of data. Reinforcement learning, while powerful in scaling reasoning, is introduced only as the very last phase of post-training, preceded by supervised fine-tuning. While dominant, is this an optimal way of training? In this paper, we present RLP, an information-driven reinforcement pretraining objective, that brings the core spirit of reinforcement learning -- exploration -- to the last phase of pretraining. The key idea is to treat chain-of-thought as an exploratory action, with rewards computed based on the information gain it provides for predicting future tokens. This training objective essentially encourages the model to think for itself before predicting what comes next, thus teaching an independent thinking behavior earlier in the pretraining. More concretely, the reward signal measures the increase in log-likelihood of the next token when conditioning on both context and a sampled reasoning chain, compared to conditioning on context alone. This approach yields a verifier-free dense reward signal, allowing for efficient training for the full document stream during pretraining. Specifically, RLP reframes reinforcement learning for reasoning as a pretraining objective on ordinary text, bridging the gap between next-token prediction and the emergence of useful chain-of-thought reasoning. Pretraining with RLP on Qwen3-1.7B-Base lifts the overall average across an eight-benchmark math-and-science suite by 19%. With identical post-training, the gains compound, with the largest improvements on reasoning-heavy tasks such as AIME25 and MMLU-Pro. Applying RLP to the hybrid Nemotron-Nano-12B-v2 increases the overall average from 42.81% to 61.32% and raises the average on scientific reasoning by 23%, demonstrating scalability across architectures and model sizes.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2510.01265"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzbgys/rlp_reinforcement_as_a_pretraining_objective/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nzbgys/rlp_reinforcement_as_a_pretraining_objective/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T06:17:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1nyxmci</id>
    <title>Poor GPU Club : 8GB VRAM - Qwen3-30B-A3B &amp; gpt-oss-20b t/s with llama.cpp</title>
    <updated>2025-10-05T19:30:16+00:00</updated>
    <author>
      <name>/u/pmttyji</name>
      <uri>https://old.reddit.com/user/pmttyji</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Tried llama.cpp with 2 models(3 quants) &amp;amp; here results. After some trial &amp;amp; error, those -ncmoe numbers gave me those t/s during llama-bench. But t/s is somewhat smaller during llama-server, since I put 32K context.&lt;/p&gt; &lt;p&gt;I'm 99% sure, below full llama-server commands are not optimized ones. Even same on llama-bench commands. Frankly I'm glad to see 30+ t/s on llama-bench results at day 1 attempt, while I noticed other 8GB VRAM owners mentioned that they got only 20+ t/s on many threads in this sub in past. I did collect collect commands from more than bunch of folks here, but none couldn't help me to create 100% logic behind this thing. Trial &amp;amp; Error!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Please help me to optimize the commands to get even better t/s&lt;/strong&gt;. For example, One thing I'm sure that I need to change the value of -t (threads) .... Included my system Cores &amp;amp; Logical Processor below. Please let me know the right formula for this.&lt;/p&gt; &lt;p&gt;My System Info: (&lt;strong&gt;8GB VRAM &amp;amp; 32GB RAM&lt;/strong&gt;)&lt;/p&gt; &lt;p&gt;Intel(R) Core(TM) i7-14700HX 2.10 GHz | 32 GB RAM | 64-bit OS, x64-based processor | NVIDIA GeForce RTX 4060 Laptop GPU | &lt;strong&gt;Cores - 20 | Logical Processors - 28&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Qwen3-30B-A3B-UD-Q4_K_XL - 31 t/s&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-bench -m E:\LLM\models\Qwen3-30B-A3B-UD-Q4_K_XL.gguf -ngl 99 -ncmoe 29 -fa 1 | model | size | params | backend | ngl | fa | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | -: | -------: | ------------: | | qwen3moe 30B.A3B Q4_K - Medium | 16.49 GiB | 30.53 B | CUDA | 99 | 1 | pp512 | 82.64 ± 8.36 | | qwen3moe 30B.A3B Q4_K - Medium | 16.49 GiB | 30.53 B | CUDA | 99 | 1 | tg128 | 31.68 ± 0.28 | llama-server -m E:\LLM\models\Qwen3-30B-A3B-UD-Q4_K_XL.gguf -ngl 99 -ncmoe 29 -t 8 -c 32768 -fa 1 --no-mmap -ctk q8_0 -ctv q8_0 -b 2048 -ub 2048 --cache-reuse 2048 --temp 0.6 --top-p 0.95 --min-p 0.0 --top-k 20 prompt eval time = 548.48 ms / 16 tokens ( 34.28 ms per token, 29.17 tokens per second) eval time = 2498.63 ms / 44 tokens ( 56.79 ms per token, 17.61 tokens per second) total time = 3047.11 ms / 60 tokens &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Qwen3-30B-A3B-IQ4_XS - 34 t/s&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-bench -m E:\LLM\models\Qwen3-30B-A3B-IQ4_XS.gguf -ngl 99 -ncmoe 28 -fa 1 | model | size | params | backend | ngl | fa | test | t/s | | ---------------------------------- | --------: | ---------: | ---------- | --: | -: | -------: | --------------: | | qwen3moe 30B.A3B IQ4_XS - 4.25 bpw | 15.25 GiB | 30.53 B | CUDA | 99 | 1 | pp512 | 178.91 ± 38.37 | | qwen3moe 30B.A3B IQ4_XS - 4.25 bpw | 15.25 GiB | 30.53 B | CUDA | 99 | 1 | tg128 | 34.24 ± 0.19 | llama-server -m E:\LLM\models\Qwen3-30B-A3B-IQ4_XS.gguf -ngl 99 -ncmoe 29 -t 8 -c 32768 -fa 1 --no-mmap -ctk q8_0 -ctv q8_0 -b 2048 -ub 2048 --cache-reuse 2048 prompt eval time = 421.67 ms / 16 tokens ( 26.35 ms per token, 37.94 tokens per second) eval time = 3671.26 ms / 81 tokens ( 45.32 ms per token, 22.06 tokens per second) total time = 4092.94 ms / 97 tokens &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;gpt-oss-20b - 38 t/s&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-bench -m E:\LLM\models\gpt-oss-20b-mxfp4.gguf -ngl 99 -ncmoe 10 -fa 1 | model | size | params | backend | ngl | fa | test | t/s | | ------------------------------ | ---------: | ---------: | --: | --:| -----: | -------------: | | gpt-oss 20B MXFP4 MoE | 11.27 GiB | 20.91 B | CUDA | 99 | 1 | pp512 | 363.09 ± 18.47 | | gpt-oss 20B MXFP4 MoE | 11.27 GiB | 20.91 B | CUDA | 99 | 1 | tg128 | 38.16 ± 0.43 | llama-server -m E:\LLM\models\gpt-oss-20b-mxfp4.gguf -ngl 99 -ncmoe 10 -t 8 -c 32768 -fa 1 --no-mmap -ctk q8_0 -ctv q8_0 -b 2048 -ub 2048 --cache-reuse 2048 prompt eval time = 431.05 ms / 14 tokens ( 30.79 ms per token, 32.48 tokens per second) eval time = 4765.53 ms / 116 tokens ( 41.08 ms per token, 24.34 tokens per second) total time = 5196.58 ms / 130 tokens &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I'll be updating this thread whenever I get optimization tips &amp;amp; tricks from others AND I'll be including additional results here with updated commands. Thanks&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Updates:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;1] Before trying llama-server, try llama-bench with multiple values(for -ncmoe) to see which one gives better numbers. That's how I did &amp;amp; got the numbers highlighted in &lt;strong&gt;bold&lt;/strong&gt; above.&lt;/p&gt; &lt;p&gt;2] &lt;del&gt;Size&lt;/del&gt; Speed-wise IQ4_XS &amp;gt; other Q4 quants. Listed all Qwen3-30B-A3B Q4 quants with its sizes &amp;amp; highlighted small size in bold(&lt;strong&gt;16.4GB&lt;/strong&gt;). That means we're saving 1-2 GB in VRAM/RAM. From my stats listed above, &lt;strong&gt;IQ4_XS&lt;/strong&gt; giving me &lt;strong&gt;additional 3-5 t/s&lt;/strong&gt; (comparing to &lt;strong&gt;Q4_K_XL&lt;/strong&gt;). I think still I can get few more if I tune more. More suggestions welcome.&lt;/p&gt; &lt;p&gt;IQ4_XS &lt;strong&gt;16.4GB&lt;/strong&gt; | Q4_K_S 17.5GB | IQ4_NL 17.3GB | Q4_0 17.4GB | Q4_1 19.2GB | Q4_K_M 18.6GB | Q4_K_XL 17.7GB&lt;/p&gt; &lt;p&gt;3) Initially some newbies(like me) assume that there might be some compilation needed before using llama.cpp. But no, nothing needed, their release section has multiple files for different setup &amp;amp; OS. Just download files from their latest release. I just downloaded &lt;strong&gt;llama-b6692-bin-win-cuda-12.4-x64 .zip&lt;/strong&gt; from release page yesterday. And extracted the zip file &amp;amp; immediately used llama-bench &amp;amp; llama-server. That's it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmttyji"&gt; /u/pmttyji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nyxmci/poor_gpu_club_8gb_vram_qwen330ba3b_gptoss20b_ts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nyxmci/poor_gpu_club_8gb_vram_qwen330ba3b_gptoss20b_ts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nyxmci/poor_gpu_club_8gb_vram_qwen330ba3b_gptoss20b_ts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-05T19:30:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1nz2lco</id>
    <title>Why not a [backspace] token?</title>
    <updated>2025-10-05T22:49:13+00:00</updated>
    <author>
      <name>/u/Knowked</name>
      <uri>https://old.reddit.com/user/Knowked</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We have things like [think] or [Eos] tokens and ive heard of reset tokens to delete entire responses, but why not a backspace token? i understand that the backspace cant be pretrained from text data, but we can cirtainly train it to do that in post training. I feel like it could help the model deal with mistakes better. &lt;/p&gt; &lt;p&gt;I think the &amp;quot;oh i already said it&amp;quot; thaught process could be leading to more halucinations. where it thinks it needs to be consistent with what it already said, thus halucinating.&lt;/p&gt; &lt;p&gt;The problem i could see would be that it would back space untill the mistake, then just generate the same response, but i think you could avoid that by including the mistake in the context? or perhaps just have it take an input of a state from the mistaken state and train it to avoid that mistaken state.&lt;/p&gt; &lt;p&gt;Its natural to us to say something first then rethink it and take it back, and for the same reason that CoT works i think this could be a better way of making smarter and faster models.&lt;/p&gt; &lt;p&gt;what do you think? why dont we do this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Knowked"&gt; /u/Knowked &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nz2lco/why_not_a_backspace_token/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nz2lco/why_not_a_backspace_token/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nz2lco/why_not_a_backspace_token/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-05T22:49:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1nzax2b</id>
    <title>In your experience are LLMs following the same curse of dimensionality as Alexa did?</title>
    <updated>2025-10-06T05:43:19+00:00</updated>
    <author>
      <name>/u/Amazing_Trace</name>
      <uri>https://old.reddit.com/user/Amazing_Trace</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been curious about this and maybe someone is doing research or a paper is out there about this, but here I ask the community's opinion.&lt;/p&gt; &lt;p&gt;Once upon a time, Alexa was great. It had limited skills and functionality, but they worked easily, for example it would pause TV without misunderstanding. &lt;/p&gt; &lt;p&gt;As amazon added more skills and features you needed to be more verbose to get the same thing done, things stopped working, it started interacting with the wrong devices, could not map the same words to same actions... i.e., as the dimensionality/feature space increased, it got less and less confident.&lt;/p&gt; &lt;p&gt;Are you seeing this in LLMs? are more languages and tasks it gets trained on making it harder for you to accomplish tasks that were easy on say gpt-2.5? What is your experience with the changes introduced to new LLMs?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Amazing_Trace"&gt; /u/Amazing_Trace &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzax2b/in_your_experience_are_llms_following_the_same/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzax2b/in_your_experience_are_llms_following_the_same/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nzax2b/in_your_experience_are_llms_following_the_same/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T05:43:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1nzfiw4</id>
    <title>Build advice - RTX 6000 MAX-Q x 2</title>
    <updated>2025-10-06T10:39:20+00:00</updated>
    <author>
      <name>/u/Direct_Bodybuilder63</name>
      <uri>https://old.reddit.com/user/Direct_Bodybuilder63</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone I’m going to be buying two RTX 6000s and I wanted to hear why recommendations people had for other components.&lt;/p&gt; &lt;p&gt;I’m looking at the threadripper 7995WX or 9995WX it just seems really expensive! &lt;/p&gt; &lt;p&gt;Thanks &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Direct_Bodybuilder63"&gt; /u/Direct_Bodybuilder63 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzfiw4/build_advice_rtx_6000_maxq_x_2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzfiw4/build_advice_rtx_6000_maxq_x_2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nzfiw4/build_advice_rtx_6000_maxq_x_2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T10:39:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1nyqkkm</id>
    <title>Apple has added significant AI-acceleration to its A19 CPU cores</title>
    <updated>2025-10-05T15:03:50+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nyqkkm/apple_has_added_significant_aiacceleration_to_its/"&gt; &lt;img alt="Apple has added significant AI-acceleration to its A19 CPU cores" src="https://preview.redd.it/ti22axwj5btf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=967e4aea50a8298df5070520a6bc78e77ecbcfb7" title="Apple has added significant AI-acceleration to its A19 CPU cores" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Data source: &lt;a href="https://ai-benchmark.com/ranking_processors_detailed.html"&gt;https://ai-benchmark.com/ranking_processors_detailed.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We also might see these advances back in the M5.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ti22axwj5btf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nyqkkm/apple_has_added_significant_aiacceleration_to_its/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nyqkkm/apple_has_added_significant_aiacceleration_to_its/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-05T15:03:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1nzgp8q</id>
    <title>How to run LLMs on a 1GB (e-waste) GPU without changing a single line of code</title>
    <updated>2025-10-06T11:42:30+00:00</updated>
    <author>
      <name>/u/maifee</name>
      <uri>https://old.reddit.com/user/maifee</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Accelera is working at some scale. And you 𝐝𝐨 𝐧𝐨𝐭 𝐡𝐚𝐯𝐞 𝐭𝐨 𝐫𝐞𝐜𝐨𝐦𝐩𝐢𝐥𝐞 𝐨𝐫 𝐦𝐨𝐝𝐢𝐟𝐲 𝐚 𝐬𝐢𝐧𝐠𝐥𝐞 𝐥𝐢𝐧𝐞 𝐨𝐟 𝐲𝐨𝐮𝐫 𝐜𝐨𝐝𝐞𝐛𝐚𝐬𝐞.&lt;/p&gt; &lt;p&gt;I was facing an odd problem over quite a few years now, and that is I am quite poor, and I can not do anything about it for so long. I work hard, take the next step, but somehow the new base set, and I am stuck there again. And this also makes me GPU poor. I can not even load the whole wan models in my GPU. But I have some specific skillset, and one of them is designing the most weirdest algorithm, but they work, and they also scale. So here is what I did. I have enough RAM to keep loading the weights on demand and transfer them onto GPU, perform the operation on GPU and return back to CPU, and keep doing this till we are done. This way I was able limit the usage VRAM load so much that max hit 400 megabytes, not even a gigabytes.&lt;/p&gt; &lt;p&gt;So now we can run wan on 16gb machine with mobile GPU of less than 1gb VRAM, so it fits the description of everyday developer laptop. This is not just a moment for me, but for us. Think about how much e-waste we can make reusable with this. Think about how many clusters we can make just by integrating them with accelera, definetly they will be slower than latest cutting edge devices, but it is one more fighting chances to lacking startups or indie developers.&lt;/p&gt; &lt;p&gt;Right now I am trying to make it distributed to multiple device and parallel weight loading. And I am pretty sure it will be a quite turbulent path, but I will definetly explore it, and resolve it.&lt;/p&gt; &lt;p&gt;This is just a technique to intercept pytorch method and replace their with my efficient matmul code. It also makes me limited, if something is not implemented in torch, it simply can not optimize it. But on the bright side, we can use this without any recompile or modification of the codebase.&lt;/p&gt; &lt;p&gt;Please share your thoughts and suggestions. Today (2025.10.06) the video is jittery, but it will not be for very long.&lt;/p&gt; &lt;p&gt;Source code: &lt;a href="https://github.com/maifeeulasad/Accelera/"&gt;https://github.com/maifeeulasad/Accelera/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;PIP package: &lt;a href="https://pypi.org/project/accelera/"&gt;https://pypi.org/project/accelera/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/maifee"&gt; /u/maifee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzgp8q/how_to_run_llms_on_a_1gb_ewaste_gpu_without/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzgp8q/how_to_run_llms_on_a_1gb_ewaste_gpu_without/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nzgp8q/how_to_run_llms_on_a_1gb_ewaste_gpu_without/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T11:42:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1nylc3q</id>
    <title>NIST evaluates Deepseek as unsafe. Looks like the battle to discredit opensource is underway</title>
    <updated>2025-10-05T11:05:46+00:00</updated>
    <author>
      <name>/u/Nobby_Binks</name>
      <uri>https://old.reddit.com/user/Nobby_Binks</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nobby_Binks"&gt; /u/Nobby_Binks &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.techrepublic.com/article/news-deepseek-security-gaps-caisi-study/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nylc3q/nist_evaluates_deepseek_as_unsafe_looks_like_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nylc3q/nist_evaluates_deepseek_as_unsafe_looks_like_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-05T11:05:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1nzal91</id>
    <title>My experience coding with open models (Qwen3, GLM 4.6, Kimi K2) inside VS Code</title>
    <updated>2025-10-06T05:23:36+00:00</updated>
    <author>
      <name>/u/Arindam_200</name>
      <uri>https://old.reddit.com/user/Arindam_200</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been using &lt;strong&gt;Cursor&lt;/strong&gt; for a while, mainly for its smooth AI coding experience. But recently, I decided to move my workflow back to &lt;strong&gt;VS Code&lt;/strong&gt; and test how far &lt;strong&gt;open-source coding models&lt;/strong&gt; have come.&lt;/p&gt; &lt;p&gt;The setup I’m using is simple:&lt;br /&gt; - VS Code + Hugging Face Copilot Chat extension&lt;br /&gt; - Models: Qwen 3, GLM 4.6, and Kimi K2&lt;/p&gt; &lt;p&gt;Honestly, I didn’t expect much at first, but the results have been surprisingly solid.&lt;br /&gt; Here’s what stood out:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;These open models handle refactoring, commenting, and quick edits really well.&lt;/li&gt; &lt;li&gt;They’re &lt;strong&gt;way&lt;/strong&gt; cheaper than proprietary models, no token anxiety, no credit drain.&lt;/li&gt; &lt;li&gt;You can switch models on the fly, depending on task complexity.&lt;/li&gt; &lt;li&gt;No vendor lock-in, full transparency, and control inside your editor.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I still agree that Claude 4.5 or GPT-5 outperform in deep reasoning and complex tasks, but for 50–60% of everyday work, writing code, debugging, or doc generation, these open models perform just fine.&lt;/p&gt; &lt;p&gt;It feels like the first time open LLMs can actually compete with closed ones in real-world dev workflows. I also made a short tutorial showing how to set it up step-by-step if you want to try it: &lt;a href="https://youtu.be/6pcBBLXxOEc"&gt;Setup guide&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I would love to hear your thoughts on these open source models!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Arindam_200"&gt; /u/Arindam_200 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzal91/my_experience_coding_with_open_models_qwen3_glm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzal91/my_experience_coding_with_open_models_qwen3_glm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nzal91/my_experience_coding_with_open_models_qwen3_glm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T05:23:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1nz20g2</id>
    <title>WEBGEN, UIGEN-FX, UIGENT research preview releases</title>
    <updated>2025-10-05T22:23:28+00:00</updated>
    <author>
      <name>/u/smirkishere</name>
      <uri>https://old.reddit.com/user/smirkishere</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nz20g2/webgen_uigenfx_uigent_research_preview_releases/"&gt; &lt;img alt="WEBGEN, UIGEN-FX, UIGENT research preview releases" src="https://b.thumbs.redditmedia.com/azajCZ_GFCKDhFRD2wXJTb313NdjpgLdNb8sEunUwFM.jpg" title="WEBGEN, UIGEN-FX, UIGENT research preview releases" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We intend to make a drop-in coding models that have heightened design capabilities in normal developer workflows. &lt;/p&gt; &lt;p&gt;UIGENT is the frontend engineer, designed to work across all frameworks and languages. Tries to get the best &amp;quot;understanding&amp;quot; and agentic usage. Built on top of 30B. &lt;/p&gt; &lt;p&gt;UIGEN-FX is a UI generation based agentic, trained on agentic trails and our common UI datasets. Works best with react, tailwind, ssg, and web frameworks. Model was designed to have the most 'functional' and thought out designs, focusing on accessibility and not just design.&lt;/p&gt; &lt;p&gt;WEBGEN is simply an experiment on how far we can push design in one singular category (landing pages in html css js tailwind) to make them look as far away as possible from 'ai slop' design. That is the goal. (still working on it). &lt;/p&gt; &lt;p&gt;The Training process looks like this: We have our dataset. We then compact it into rows such as {text} and then go through them as samples, using packing. We released our internal training library for ROCM on MI300X here: &lt;a href="https://github.com/TesslateAI/Late"&gt;https://github.com/TesslateAI/Late&lt;/a&gt; but with contributions, I'm sure it can run on any platform. Its mostly for batch training runs, parameter sweeps, quickly patching your training environment for standardization, etc. &lt;/p&gt; &lt;p&gt;Here are the latest versions:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Tesslate/UIGENT-30B-3A-Preview"&gt;Tesslate/UIGENT-30B-3A-Preview&lt;/a&gt; Trained on Qwen3 Coder 30B 3A&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Tesslate/UIGEN-FX-Agentic-32B"&gt;Tesslate/UIGEN-FX-Agentic-32B&lt;/a&gt; Trained on Qwen3 32B (hybrid reasoning model)&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Tesslate/UIGEN-FX-4B-Preview"&gt;Tesslate/UIGEN-FX-4B-Preview&lt;/a&gt; Trained on Qwen3 4B 2507 Instruct&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Tesslate/WEBGEN-Devstral-24B"&gt;Tesslate/WEBGEN-Devstral-24B&lt;/a&gt; Trained on Devstral 24B&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Tesslate/WEBGEN-4B-Preview"&gt;Tesslate/WEBGEN-4B-Preview&lt;/a&gt; Trained on Qwen3 4B 2507 Instruct&lt;/p&gt; &lt;p&gt;Our &lt;a href="https://discord.gg/qmrcHGNch7"&gt;discord&lt;/a&gt; for our research community. We're happy to help with anything AI (even if it is not related to us) and discuss the latest advances in AI. We love research. &lt;/p&gt; &lt;p&gt;We have other open source projects: &lt;a href="https://github.com/TesslateAI"&gt;https://github.com/TesslateAI&lt;/a&gt; including a multiagent orchestration library (with mcp and low level tool calling) and workflow tools. &lt;/p&gt; &lt;p&gt;Everything is Apache 2.0, code is commodity, feel free to steal anything. &lt;/p&gt; &lt;p&gt;&lt;em&gt;PS. Our Designer application (LLM Artifacts) is down (devops isn't my strong suit), but it is open source if anyone &amp;quot;needs it&amp;quot; because it can run locally.&lt;/em&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/smirkishere"&gt; /u/smirkishere &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nz20g2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nz20g2/webgen_uigenfx_uigent_research_preview_releases/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nz20g2/webgen_uigenfx_uigent_research_preview_releases/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-05T22:23:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1nzgben</id>
    <title>[Update] FamilyBench: New models tested - Claude Sonnet 4.5 takes 2nd place, Qwen 3 Next breaks 70%, new Kimi weirdly below the old version, same for GLM 4.6</title>
    <updated>2025-10-06T11:22:33+00:00</updated>
    <author>
      <name>/u/Orolol</name>
      <uri>https://old.reddit.com/user/Orolol</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello again, I've been testing more models on FamilyBench, my benchmark that tests LLM ability to understand complex tree-like relationships in a family tree across a massive context. For those who missed the initial post: this is a Python program that generates a family tree and uses its structure to generate questions about it. You get a textual description of the tree and questions that are hard to parse for LLMs. GitHub: &lt;a href="https://github.com/Orolol/familyBench"&gt;https://github.com/Orolol/familyBench&lt;/a&gt; &lt;/p&gt; &lt;p&gt;What's new: I've added 4 new models to the leaderboard, including Claude Sonnet 4.5 which shows impressive improvements over Sonnet 4, Qwen 3 Next 80B which demonstrates massive progress in the Qwen family, and GLM 4.6 which surprisingly excels at enigma questions despite lower overall accuracy. All models are tested on the same complex tree with 400 people across 10 generations (~18k tokens). 189 questions are asked (after filtering). Tests run via OpenRouter with low reasoning effort or 8k max tokens, temperature 0.3. Example of family description: &amp;quot;Aaron (M) has white hair, gray eyes, wears a gold hat and works as a therapist. Aaron (M) has 2 children: Barry (M), Erica (F). Abigail (F) has light brown hair, amber eyes, wears a red hat and works as a teacher...&amp;quot; Example of questions: &amp;quot;Which of Paula's grandparents have salt and pepper hair?&amp;quot; &amp;quot;Who is the cousin of the daughter of Quentin with red hair?&amp;quot; &lt;/p&gt; &lt;p&gt;Current Leaderboard:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;Accuracy&lt;/th&gt; &lt;th&gt;Total Tokens&lt;/th&gt; &lt;th&gt;No Response Rate&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Gemini 2.5 Pro&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;81.48%&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;271,500&lt;/td&gt; &lt;td&gt;0%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Claude Sonnet 4.5&lt;/strong&gt; &lt;em&gt;(New)&lt;/em&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;77.78%&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;211,249&lt;/td&gt; &lt;td&gt;0%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;DeepSeek R1&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;75.66%&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;575,624&lt;/td&gt; &lt;td&gt;0%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Gemini 2.5 Flash&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;73.54%&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;258,214&lt;/td&gt; &lt;td&gt;2.65%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Qwen 3 Next 80B A3B Thinking&lt;/strong&gt; &lt;em&gt;(New)&lt;/em&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;71.43%&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;1,076,302&lt;/td&gt; &lt;td&gt;3.17%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Claude Sonnet 4&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;67.20%&lt;/td&gt; &lt;td&gt;258,883&lt;/td&gt; &lt;td&gt;1.06%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;DeepSeek V3.2 Exp&lt;/strong&gt; &lt;em&gt;(New)&lt;/em&gt;&lt;/td&gt; &lt;td&gt;66.67%&lt;/td&gt; &lt;td&gt;427,396&lt;/td&gt; &lt;td&gt;0%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;GLM 4.5&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;64.02%&lt;/td&gt; &lt;td&gt;216,281&lt;/td&gt; &lt;td&gt;2.12%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;GLM 4.5 Air&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;57.14%&lt;/td&gt; &lt;td&gt;1,270,138&lt;/td&gt; &lt;td&gt;26.46%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;GPT-OSS 120B&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;50.26%&lt;/td&gt; &lt;td&gt;167,938&lt;/td&gt; &lt;td&gt;1.06%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Qwen 3.2 Thinking&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;50.26%&lt;/td&gt; &lt;td&gt;1,077,814&lt;/td&gt; &lt;td&gt;20.63%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;GLM 4.6&lt;/strong&gt; &lt;em&gt;(New)&lt;/em&gt;&lt;/td&gt; &lt;td&gt;47.62%&lt;/td&gt; &lt;td&gt;149,232&lt;/td&gt; &lt;td&gt;0%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Kimi K2&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;34.92%&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Kimi K2 0905&lt;/strong&gt; &lt;em&gt;(New)&lt;/em&gt;&lt;/td&gt; &lt;td&gt;31.75%&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Hunyuan A13B&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;30.16%&lt;/td&gt; &lt;td&gt;121,150&lt;/td&gt; &lt;td&gt;2.12%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Mistral Medium 3.1&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;29.63%&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0.53%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Next plan : Redo all tests en a whole new seed, with harder questions and a larger tree. I have to think how I can decrease the costs first.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Orolol"&gt; /u/Orolol &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzgben/update_familybench_new_models_tested_claude/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzgben/update_familybench_new_models_tested_claude/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nzgben/update_familybench_new_models_tested_claude/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T11:22:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1nz51be</id>
    <title>“This is a fantastic question that strikes at the heart of the intersection of quantum field theory and animal welfare…”</title>
    <updated>2025-10-06T00:40:47+00:00</updated>
    <author>
      <name>/u/-p-e-w-</name>
      <uri>https://old.reddit.com/user/-p-e-w-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Many current models now start every response in this manner. I don’t remember it being that way a year ago. Do they all use the same bad instruction dataset?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-p-e-w-"&gt; /u/-p-e-w- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nz51be/this_is_a_fantastic_question_that_strikes_at_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nz51be/this_is_a_fantastic_question_that_strikes_at_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nz51be/this_is_a_fantastic_question_that_strikes_at_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T00:40:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1nz7xdu</id>
    <title>UGI-Leaderboard is back with a new writing leaderboard, and many new benchmarks!</title>
    <updated>2025-10-06T03:00:12+00:00</updated>
    <author>
      <name>/u/DontPlanToEnd</name>
      <uri>https://old.reddit.com/user/DontPlanToEnd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nz7xdu/ugileaderboard_is_back_with_a_new_writing/"&gt; &lt;img alt="UGI-Leaderboard is back with a new writing leaderboard, and many new benchmarks!" src="https://a.thumbs.redditmedia.com/UuZrlImxEYTE-nymzCESnUejkDPWAQxxEocQEz-Nf_4.jpg" title="UGI-Leaderboard is back with a new writing leaderboard, and many new benchmarks!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/spaces/DontPlanToEnd/UGI-Leaderboard"&gt;https://huggingface.co/spaces/DontPlanToEnd/UGI-Leaderboard&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DontPlanToEnd"&gt; /u/DontPlanToEnd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nz7xdu"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nz7xdu/ugileaderboard_is_back_with_a_new_writing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nz7xdu/ugileaderboard_is_back_with_a_new_writing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T03:00:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1nze0rr</id>
    <title>I created an open-source Invisible AI Assistant called Pluely - now at 890+ GitHub stars. You can add and use Ollama or any Local for free. Better interface for all your works.</title>
    <updated>2025-10-06T09:03:37+00:00</updated>
    <author>
      <name>/u/iam-neighbour</name>
      <uri>https://old.reddit.com/user/iam-neighbour</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nze0rr/i_created_an_opensource_invisible_ai_assistant/"&gt; &lt;img alt="I created an open-source Invisible AI Assistant called Pluely - now at 890+ GitHub stars. You can add and use Ollama or any Local for free. Better interface for all your works." src="https://external-preview.redd.it/cTh1dWtsMmVpZ3RmMYn6P6th22FDriw78c6Xj5wjFaOcKQQ0FizRfMPuGFUP.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1423134cd707a8174df37b779b144e2877f1a6c9" title="I created an open-source Invisible AI Assistant called Pluely - now at 890+ GitHub stars. You can add and use Ollama or any Local for free. Better interface for all your works." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Pluely is Your Invisible AI Assistant: Lightning-fast, privacy-first AI assistant that works seamlessly during meetings, interviews, and conversations without anyone knowing. Completely undetectable in video calls, screen shares. All your data is stored locally on your system. Pluely is designed with privacy as a priority, so no external calls are made to our servers. This applies to both free and Pro users.&lt;/p&gt; &lt;p&gt;By far pluely is the best invisible open-source ai assistant, compared to big firms like Cluely, interviewCoder or any.&lt;/p&gt; &lt;p&gt;all with: solo contribution, $0 funding, and endless nights.&lt;/p&gt; &lt;p&gt;Menu you need on your desktop:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;System audio capture&lt;/li&gt; &lt;li&gt;Microphone audio capture&lt;/li&gt; &lt;li&gt;Input for all your queries&lt;/li&gt; &lt;li&gt;Screenshots (auto/manual)&lt;/li&gt; &lt;li&gt;Attach images&lt;/li&gt; &lt;li&gt;History&lt;/li&gt; &lt;li&gt;Settings&lt;/li&gt; &lt;li&gt;Drag handle&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;On free plan: Pluely supports all major LLM providers just bring your own api key, you can also add your own custom providers with cURL commands, same for speech to text providers as well.&lt;/p&gt; &lt;p&gt;On Pro plan: Pluely now has 80+ premium AI models with instant access including with GPT-5 and many other openai models, One-click model switching, Advanced speech-to-text with highest accuracy, and generating system prompts with AI.&lt;/p&gt; &lt;p&gt;Downloads: &lt;a href="https://pluely.com/downloads"&gt;https://pluely.com/downloads&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Website: &lt;a href="https://pluely.com"&gt;https://pluely.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/iamsrikanthnani/pluely"&gt;https://github.com/iamsrikanthnani/pluely&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know your experience, and how i can improve more. Features to add are welcome.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iam-neighbour"&gt; /u/iam-neighbour &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/qksezi2eigtf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nze0rr/i_created_an_opensource_invisible_ai_assistant/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nze0rr/i_created_an_opensource_invisible_ai_assistant/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T09:03:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1nyvqyx</id>
    <title>GLM-4.6 outperforms claude-4-5-sonnet while being ~8x cheaper</title>
    <updated>2025-10-05T18:19:56+00:00</updated>
    <author>
      <name>/u/Full_Piano_3448</name>
      <uri>https://old.reddit.com/user/Full_Piano_3448</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nyvqyx/glm46_outperforms_claude45sonnet_while_being_8x/"&gt; &lt;img alt="GLM-4.6 outperforms claude-4-5-sonnet while being ~8x cheaper" src="https://preview.redd.it/lofrjusz4ctf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=71caed6abfb748f2b7db9bdf1271b9b722f347fd" title="GLM-4.6 outperforms claude-4-5-sonnet while being ~8x cheaper" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Full_Piano_3448"&gt; /u/Full_Piano_3448 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lofrjusz4ctf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nyvqyx/glm46_outperforms_claude45sonnet_while_being_8x/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nyvqyx/glm46_outperforms_claude45sonnet_while_being_8x/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-05T18:19:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1nze0lj</id>
    <title>What GPT-oss Leaks About OpenAI's Training Data</title>
    <updated>2025-10-06T09:03:17+00:00</updated>
    <author>
      <name>/u/AppearanceHeavy6724</name>
      <uri>https://old.reddit.com/user/AppearanceHeavy6724</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AppearanceHeavy6724"&gt; /u/AppearanceHeavy6724 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://fi-le.net/oss/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nze0lj/what_gptoss_leaks_about_openais_training_data/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nze0lj/what_gptoss_leaks_about_openais_training_data/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T09:03:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1nz722n</id>
    <title>Biggest Provider for the community for at moment thanks to them</title>
    <updated>2025-10-06T02:17:03+00:00</updated>
    <author>
      <name>/u/dead-supernova</name>
      <uri>https://old.reddit.com/user/dead-supernova</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nz722n/biggest_provider_for_the_community_for_at_moment/"&gt; &lt;img alt="Biggest Provider for the community for at moment thanks to them" src="https://preview.redd.it/6kl3hy76ietf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=86e1c6a42810d36cbc6b71792855914f69ca24a1" title="Biggest Provider for the community for at moment thanks to them" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dead-supernova"&gt; /u/dead-supernova &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6kl3hy76ietf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nz722n/biggest_provider_for_the_community_for_at_moment/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nz722n/biggest_provider_for_the_community_for_at_moment/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T02:17:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvryo4</id>
    <title>AMA Announcement: Prime Intellect — The Open‑Source Distributed Training Lab (Thu, Oct 2 • 10 AM – 1 PM PDT)</title>
    <updated>2025-10-02T02:31:01+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"&gt; &lt;img alt="AMA Announcement: Prime Intellect — The Open‑Source Distributed Training Lab (Thu, Oct 2 • 10 AM – 1 PM PDT)" src="https://preview.redd.it/222kj50x0msf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c1c747c42690f74b8d08690dcdbb1ef23aece13b" title="AMA Announcement: Prime Intellect — The Open‑Source Distributed Training Lab (Thu, Oct 2 • 10 AM – 1 PM PDT)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/222kj50x0msf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T02:31:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwaoyd</id>
    <title>AMA with Prime Intellect — Ask Us Anything!</title>
    <updated>2025-10-02T17:47:44+00:00</updated>
    <author>
      <name>/u/kindacognizant</name>
      <uri>https://old.reddit.com/user/kindacognizant</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;AMA with Prime Intellect — Ask Us Anything!&lt;/h1&gt; &lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;! We’re excited for this AMA, thank you for having us.&lt;/p&gt; &lt;p&gt;I’m Kalomaze (&lt;a href="/u/kindacognizant"&gt;u/kindacognizant&lt;/a&gt;), a researcher at Prime Intellect, the lab behind:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Distributed training &lt;a href="https://www.primeintellect.ai/#research"&gt;efforts&lt;/a&gt; including INTELLECT-1 + INTELLECT-2&lt;/li&gt; &lt;li&gt;Open-source RL efforts including &lt;a href="https://github.com/PrimeIntellect-ai/verifiers"&gt;verifiers&lt;/a&gt;, &lt;a href="https://github.com/PrimeIntellect-ai/prime-rl"&gt;prime-rl&lt;/a&gt;, and the &lt;a href="https://app.primeintellect.ai/dashboard/environments"&gt;Environments Hub&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Our other participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Sami Jaghouar, &lt;a href="/u/samsja19"&gt;u/samsja19&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Will Brown, &lt;a href="/u/willccbb"&gt;u/willccbb&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Jack Min Ong, &lt;a href="/u/Cinamic"&gt;u/Cinamic&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Mika Senghaas, &lt;a href="/u/mikasenghaas"&gt;u/mikasenghaas&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 11:00 AM – 2:00 PM PST, with the Prime Intellect team continuing to follow up on questions over the next 48 hours.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kindacognizant"&gt; /u/kindacognizant &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwaoyd/ama_with_prime_intellect_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwaoyd/ama_with_prime_intellect_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwaoyd/ama_with_prime_intellect_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T17:47:44+00:00</published>
  </entry>
</feed>
