<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-21T20:06:36+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1nm6v83</id>
    <title>Qwen Next 80b q4 vs q8 vs GPT 120b vs Qwen Coder 30b</title>
    <updated>2025-09-20T19:14:37+00:00</updated>
    <author>
      <name>/u/Baldur-Norddahl</name>
      <uri>https://old.reddit.com/user/Baldur-Norddahl</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nm6v83/qwen_next_80b_q4_vs_q8_vs_gpt_120b_vs_qwen_coder/"&gt; &lt;img alt="Qwen Next 80b q4 vs q8 vs GPT 120b vs Qwen Coder 30b" src="https://b.thumbs.redditmedia.com/Ol0Pxbro6vazPUaSzrEtZA_JvTzjW_-by2F4t8qWnuU.jpg" title="Qwen Next 80b q4 vs q8 vs GPT 120b vs Qwen Coder 30b" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I ran this test on my M4 Max MacBook Pro 128 GB laptop. The interesting find is how prompt processing speed stays relatively flat as context grows. This is completely different behavior from Qwen3 Coder.&lt;/p&gt; &lt;p&gt;GPT 120b starts out faster but then becomes slower as context fills. However only the 4 bit quant of Qwen Next manages to overtake it when looking at total elapsed time. And that first happens at 80k context length. For most cases the GPT model stays the fastest then.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Baldur-Norddahl"&gt; /u/Baldur-Norddahl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nm6v83"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nm6v83/qwen_next_80b_q4_vs_q8_vs_gpt_120b_vs_qwen_coder/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nm6v83/qwen_next_80b_q4_vs_q8_vs_gpt_120b_vs_qwen_coder/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-20T19:14:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1nlyy6n</id>
    <title>Intel Arc Pro B60 24GB professional GPU listed at $599, in stock and shipping</title>
    <updated>2025-09-20T14:00:49+00:00</updated>
    <author>
      <name>/u/PhantomWolf83</name>
      <uri>https://old.reddit.com/user/PhantomWolf83</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nlyy6n/intel_arc_pro_b60_24gb_professional_gpu_listed_at/"&gt; &lt;img alt="Intel Arc Pro B60 24GB professional GPU listed at $599, in stock and shipping" src="https://external-preview.redd.it/942g63AteF3sF5KI6YzwLlHNUjooze5_uZcUA7PiVqQ.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=259bd6663f4a689dc50651317dca845a29e37f3f" title="Intel Arc Pro B60 24GB professional GPU listed at $599, in stock and shipping" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PhantomWolf83"&gt; /u/PhantomWolf83 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://videocardz.com/newz/intel-arc-pro-b60-24gb-professional-gpu-listed-at-599-in-stock-and-shipping"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nlyy6n/intel_arc_pro_b60_24gb_professional_gpu_listed_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nlyy6n/intel_arc_pro_b60_24gb_professional_gpu_listed_at/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-20T14:00:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1nmq7pw</id>
    <title>Best model for humour?</title>
    <updated>2025-09-21T12:06:12+00:00</updated>
    <author>
      <name>/u/ANONYMOUS_GAMER_07</name>
      <uri>https://old.reddit.com/user/ANONYMOUS_GAMER_07</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I made this post over an year ago... but I couldn't find any model that could actually make someone laugh or atleast smirk. I tried jailbreak system prompts, custom rp comedy conversations, tried local models finetuned for roleplay... but I am yet to see any such model.&lt;br /&gt; Maybe GPT-4o got close to that for many people, which we learnt after the 4o removal and reinstation debacle... but still I wouldn't really call it &amp;quot;humour&amp;quot;&lt;br /&gt; &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1f4yuh1/best_model_for_humour/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1f4yuh1/best_model_for_humour/&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Most of the LLMs I've used have very boring, synthetic, sounding Humour... and they don't generate anything new or original or creative. So, are there any models which can write jokes which don't sound like toddler-humour?&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Do we have anything now?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ANONYMOUS_GAMER_07"&gt; /u/ANONYMOUS_GAMER_07 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmq7pw/best_model_for_humour/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmq7pw/best_model_for_humour/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nmq7pw/best_model_for_humour/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T12:06:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1nmlluu</id>
    <title>Mini-PC Dilemma: 96GB vs 128GB. How Much RAM is it worth buying?</title>
    <updated>2025-09-21T07:27:46+00:00</updated>
    <author>
      <name>/u/Dull-Breadfruit-3241</name>
      <uri>https://old.reddit.com/user/Dull-Breadfruit-3241</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, I'm planning to pick up one of the new mini-PCs powered by the AMD Ryzen AI Max+ 395 CPU,specifically the Bosgame M5. The 96GB RAM model looks more cost-effective, but I'm weighing whether it's worth spending ~15% more for the 128GB version.&lt;/p&gt; &lt;p&gt;From what I understand, the 96GB config allows up to 64GB to be allocated to the integrated GPU, while the 128GB model can push that up to 96GB. That extra memory could make a difference on whether be able to run larger LLMs.&lt;/p&gt; &lt;p&gt;So here‚Äôs my question: will larger models that fit thanks to the extra memory actually run at decent speeds? Will I miss out on larger better models that would still run at decent speed on this machine by choosing the model that can allocate only 64GB of RAM to the GPU?&lt;/p&gt; &lt;p&gt;My goal is to experiment with LLMs and other AI projects locally, and I‚Äôd love to hear from anyone who‚Äôs tested similar setups or has insight into how well these systems scale with RAM.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dull-Breadfruit-3241"&gt; /u/Dull-Breadfruit-3241 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmlluu/minipc_dilemma_96gb_vs_128gb_how_much_ram_is_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmlluu/minipc_dilemma_96gb_vs_128gb_how_much_ram_is_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nmlluu/minipc_dilemma_96gb_vs_128gb_how_much_ram_is_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T07:27:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1nmiqjh</id>
    <title>OPEN WEIGHTS: Isaac 0.1. Perceptive-language model. 2B params. Matches or beats models significantly larger on core perception as claimed by Perceptron AI. Links to download in bodytext.</title>
    <updated>2025-09-21T04:34:19+00:00</updated>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmiqjh/open_weights_isaac_01_perceptivelanguage_model_2b/"&gt; &lt;img alt="OPEN WEIGHTS: Isaac 0.1. Perceptive-language model. 2B params. Matches or beats models significantly larger on core perception as claimed by Perceptron AI. Links to download in bodytext." src="https://b.thumbs.redditmedia.com/d6NOhInKANeH8JCDvtmFBeHE3iU2bdeRVBiujDCsTrI.jpg" title="OPEN WEIGHTS: Isaac 0.1. Perceptive-language model. 2B params. Matches or beats models significantly larger on core perception as claimed by Perceptron AI. Links to download in bodytext." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Blog: &lt;a href="https://www.perceptron.inc/blog/introducing-isaac-0-1"&gt;https://www.perceptron.inc/blog/introducing-isaac-0-1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Demo: &lt;a href="https://www.perceptron.inc/demo"&gt;https://www.perceptron.inc/demo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Download weights: &lt;a href="https://huggingface.co/PerceptronAI/Isaac-0.1"&gt;https://huggingface.co/PerceptronAI/Isaac-0.1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nmiqjh"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmiqjh/open_weights_isaac_01_perceptivelanguage_model_2b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nmiqjh/open_weights_isaac_01_perceptivelanguage_model_2b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T04:34:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1nmnrgw</id>
    <title>Raylight tensor split distributed GPU now can do LoRa for Wan, Flux and Qwen. Why by 5090 when you can buy 2x5060Tis</title>
    <updated>2025-09-21T09:42:48+00:00</updated>
    <author>
      <name>/u/Altruistic_Heat_9531</name>
      <uri>https://old.reddit.com/user/Altruistic_Heat_9531</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmnrgw/raylight_tensor_split_distributed_gpu_now_can_do/"&gt; &lt;img alt="Raylight tensor split distributed GPU now can do LoRa for Wan, Flux and Qwen. Why by 5090 when you can buy 2x5060Tis" src="https://b.thumbs.redditmedia.com/MFuCkMnKE4sjWkjqo3iTJG6fWRBCd4JYLtj-VDPofaM.jpg" title="Raylight tensor split distributed GPU now can do LoRa for Wan, Flux and Qwen. Why by 5090 when you can buy 2x5060Tis" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/komikndr/raylight"&gt;https://github.com/komikndr/raylight&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Just update for Raylight, some model still a bit unstable so you need to restart the ComfyUI&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You can now install it &lt;strong&gt;without&lt;/strong&gt; FlashAttention, so yey to Pascal(but i am not testing it yet).&lt;/li&gt; &lt;li&gt;Supported Attention : &lt;strong&gt;Sage, Flash, Torch&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Full &lt;strong&gt;LoRA&lt;/strong&gt; support&lt;/li&gt; &lt;li&gt;&lt;strong&gt;FSDP CPU offload,&lt;/strong&gt; analogous to block swap.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;AMD&lt;/strong&gt; User confirmed working on 8xMI300X using ROCm compiled PyTorch and Flash Attention&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Realtime Qwen on 2x RTX Ada 2000 , forgot to mute audio&lt;/p&gt; &lt;p&gt;&lt;a href="https://files.catbox.moe/a5rgon.mp4"&gt;https://files.catbox.moe/a5rgon.mp4&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Altruistic_Heat_9531"&gt; /u/Altruistic_Heat_9531 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nmnrgw"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmnrgw/raylight_tensor_split_distributed_gpu_now_can_do/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nmnrgw/raylight_tensor_split_distributed_gpu_now_can_do/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T09:42:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1nme5xy</id>
    <title>4x MI50 32GB reach 22 t/s with Qwen3 235B-A22B and 36 t/s with Qwen2.5 72B in vllm</title>
    <updated>2025-09-21T00:33:31+00:00</updated>
    <author>
      <name>/u/MLDataScientist</name>
      <uri>https://old.reddit.com/user/MLDataScientist</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;It is exciting to see AMD is finally fixing their software stack. I recently updated my MI50 GPU drivers and ROCm stack to 6.4.3. AMD officially deprecated support for MI50 (gfx906). But ROCm 6.4.3 works with one simple fix. You need to copy tensile library of MI50 from a package and paste it in rocm folder (details: &lt;a href="https://github.com/ROCm/ROCm/issues/4625#issuecomment-2899838977"&gt;https://github.com/ROCm/ROCm/issues/4625#issuecomment-2899838977&lt;/a&gt; ).&lt;/p&gt; &lt;p&gt;For performance tests, I used vllm backend - &lt;a href="https://github.com/nlzy/vllm-gfx906"&gt;https://github.com/nlzy/vllm-gfx906&lt;/a&gt; . Thank you &lt;a href="/u/NaLanZeYu"&gt;u/NaLanZeYu&lt;/a&gt; for supporting gfx906 in a separate vllm fork!&lt;/p&gt; &lt;p&gt;In my venv, I installed pytorch 2.8. I kept the original triton 3.3 but I earlier checked and triton 3.5 was also working with MI50. For single GPU, there were no package issues. For multi-GPU, there was an issue - rccl was compiled without gfx906 support. What I did was I compiled rccl with gfx906 support.&lt;/p&gt; &lt;p&gt;Downloaded rccl 2.22.3 (for ROCm 6.4.3) from &lt;a href="https://github.com/ROCm/rccl/releases/tag/rocm-6.4.3"&gt;https://github.com/ROCm/rccl/releases/tag/rocm-6.4.3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;extracted the zip file.&lt;/p&gt; &lt;p&gt;installed in ubuntu terminal:&lt;/p&gt; &lt;p&gt;```sudo ./install.sh --amdgpu_targets gfx906 -i -j 32 -p -r```&lt;/p&gt; &lt;p&gt;in vllmenv installation folder find &lt;a href="http://lbrccl.so"&gt;lbrccl.so&lt;/a&gt; and rename or delete it so that pytorch cannot use it. e.g. _librccl.so&lt;/p&gt; &lt;p&gt;in vllmenv, import the new rccl library location:&lt;/p&gt; &lt;p&gt;VLLM_NCCL_SO_PATH=/opt/rocm/lib&lt;/p&gt; &lt;p&gt;(or LD_LIBRARY_PATH=/opt/rocm/lib:$LD_LIBRARY_PATH)&lt;/p&gt; &lt;p&gt;now, vllm supports multi-GPU properly for MI50 with ROCm 6.4.3.&lt;/p&gt; &lt;p&gt;Some metrics:&lt;/p&gt; &lt;p&gt;single MI50 - single requests in vllm bench serve:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Llama-3.1-8B-AWQ-4bit - TG 93t/s; PP 945t/s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;four MI50 - single requests in vllm bench serve:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Qwen2.5 72B gptq int4 (TP 4) - TG 36/s; PP 500t/s&lt;/li&gt; &lt;li&gt;Qwen3-235B-A22B-AWQ (TP 4) - TG 22t/s; PP 290t/s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;All of them are connected to my MB with PCIE4.0 16x speed. CPU: AMD EPYC 7532 with 8x32GB DDR4 3200Mhz ECC RAM.&lt;/p&gt; &lt;p&gt;Overall, there is a great performance uplift (up to 25%) when we use ROCm 6.4.3 with gfx906.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MLDataScientist"&gt; /u/MLDataScientist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nme5xy/4x_mi50_32gb_reach_22_ts_with_qwen3_235ba22b_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nme5xy/4x_mi50_32gb_reach_22_ts_with_qwen3_235ba22b_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nme5xy/4x_mi50_32gb_reach_22_ts_with_qwen3_235ba22b_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T00:33:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1nn15rz</id>
    <title>How bad to have RTX Pro 6000 run at PCIE x8?</title>
    <updated>2025-09-21T19:27:46+00:00</updated>
    <author>
      <name>/u/kitgary</name>
      <uri>https://old.reddit.com/user/kitgary</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am building a dual RTX Pro 6000 workstation, buying the Threadripper is out of my budget as I already put 18k on the GPUs. My only option is to get the 9950x3D, I know there is not enough PCIE lanes, but how bad is it? I am using it for local LLM inference and fine tuning.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kitgary"&gt; /u/kitgary &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nn15rz/how_bad_to_have_rtx_pro_6000_run_at_pcie_x8/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nn15rz/how_bad_to_have_rtx_pro_6000_run_at_pcie_x8/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nn15rz/how_bad_to_have_rtx_pro_6000_run_at_pcie_x8/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T19:27:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1nn1elw</id>
    <title>Any recommended tools for best PDF extraction to prep data for an LLM?</title>
    <updated>2025-09-21T19:36:55+00:00</updated>
    <author>
      <name>/u/richardanaya</name>
      <uri>https://old.reddit.com/user/richardanaya</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm curious if anyone has any thoughts on tools that do an amazing job at pdf extraction? Thinking in particular about PDFs that have exotic elements like tables, random quote blocks, sidebars, etc.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/richardanaya"&gt; /u/richardanaya &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nn1elw/any_recommended_tools_for_best_pdf_extraction_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nn1elw/any_recommended_tools_for_best_pdf_extraction_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nn1elw/any_recommended_tools_for_best_pdf_extraction_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T19:36:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1nmp5jc</id>
    <title>What is the most creative open-weight model for story writing? Whether they are heavily aligned is irrelevant I am asking about pure prose and flavor of writing.</title>
    <updated>2025-09-21T11:07:34+00:00</updated>
    <author>
      <name>/u/Striking_Wedding_461</name>
      <uri>https://old.reddit.com/user/Striking_Wedding_461</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Kimi K2, DeepSeek, Qwen, GPT-oss (god help you pls don't), GLM etc.&lt;br /&gt; Non-thinking models are preferred, I really don't care if they're censored as jailbreaking is straight up a skill issue.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Striking_Wedding_461"&gt; /u/Striking_Wedding_461 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmp5jc/what_is_the_most_creative_openweight_model_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmp5jc/what_is_the_most_creative_openweight_model_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nmp5jc/what_is_the_most_creative_openweight_model_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T11:07:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1nmeu5s</id>
    <title>Qwen3-Omni, Qwen/Qwen3-Omni-7B spotted</title>
    <updated>2025-09-21T01:07:48+00:00</updated>
    <author>
      <name>/u/TKGaming_11</name>
      <uri>https://old.reddit.com/user/TKGaming_11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmeu5s/qwen3omni_qwenqwen3omni7b_spotted/"&gt; &lt;img alt="Qwen3-Omni, Qwen/Qwen3-Omni-7B spotted" src="https://external-preview.redd.it/VoGpbOIxrqAHEzxUbIOFVzMNSL9glnfyk27odhpB_Jk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8d87cfdb2cbad767672c45769d597618162abb80" title="Qwen3-Omni, Qwen/Qwen3-Omni-7B spotted" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TKGaming_11"&gt; /u/TKGaming_11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/huggingface/transformers/pull/41025"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmeu5s/qwen3omni_qwenqwen3omni7b_spotted/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nmeu5s/qwen3omni_qwenqwen3omni7b_spotted/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T01:07:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1nmvw7a</id>
    <title>Rolling Benchmarks - Evaluating AI Agents on Unseen GitHub Repos</title>
    <updated>2025-09-21T16:05:52+00:00</updated>
    <author>
      <name>/u/remyxai</name>
      <uri>https://old.reddit.com/user/remyxai</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently found Scale AI's new repo for benchmarking agent performance: &lt;a href="https://github.com/scaleapi/SWE-bench_Pro-os/"&gt;https://github.com/scaleapi/SWE-bench_Pro-os/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And since I'm building docker images for repos associated with arXiv papers each day: &lt;a href="https://hub.docker.com/u/remyxai"&gt;https://hub.docker.com/u/remyxai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I started thinking about a new direction for agent evaluation.&lt;/p&gt; &lt;p&gt;Static benchmarks are prone to leaderboard hacking and training data contamination, so how about a dynamic/rolling benchmark?&lt;/p&gt; &lt;p&gt;By limiting submissions to only freshly published code, we could evaluate based on consistency over time with rolling averages instead of finding agents overfit to a static benchmark.&lt;/p&gt; &lt;p&gt;Can rolling benchmarks bring us closer to evaluating agents in a way more closely aligned with their real-world applications?&lt;/p&gt; &lt;p&gt;Love to hear what you think about this.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/remyxai"&gt; /u/remyxai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmvw7a/rolling_benchmarks_evaluating_ai_agents_on_unseen/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmvw7a/rolling_benchmarks_evaluating_ai_agents_on_unseen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nmvw7a/rolling_benchmarks_evaluating_ai_agents_on_unseen/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T16:05:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1nmnmqh</id>
    <title>Wan 2.2 Animate : Open-Sourced model for character replacement and animation in videos</title>
    <updated>2025-09-21T09:34:38+00:00</updated>
    <author>
      <name>/u/Technical-Love-8479</name>
      <uri>https://old.reddit.com/user/Technical-Love-8479</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Wan 2.2 Animate 14B is released which can animate static pictures using reference videos with movement and expression replication Hugging Face : &lt;a href="https://huggingface.co/Wan-AI/Wan2.2-Animate-14B"&gt;https://huggingface.co/Wan-AI/Wan2.2-Animate-14B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Technical-Love-8479"&gt; /u/Technical-Love-8479 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmnmqh/wan_22_animate_opensourced_model_for_character/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmnmqh/wan_22_animate_opensourced_model_for_character/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nmnmqh/wan_22_animate_opensourced_model_for_character/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T09:34:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1nn1fqf</id>
    <title>Kimi K2, hallucinations/verification, and fine tuning</title>
    <updated>2025-09-21T19:38:04+00:00</updated>
    <author>
      <name>/u/ramendik</name>
      <uri>https://old.reddit.com/user/ramendik</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So in my previous Kimi K2 post I see that a good few people have this same &amp;quot;it would be so great if not for the hallucination/overconfidence&amp;quot; view of Kimi K2. Which kinda brings in an interesting question.&lt;/p&gt; &lt;p&gt;Might it be possible to assemble a team here to try and fine-tune the thing? It is NOT easy (1T+MoE) and it needs someone experienced in fine-tuning and knowing how to generate the data, as well as others willing to review the data, come up with suggestions, and importantly chip in for the GPU time or serverless training tokens. Then the resulting LoRA is just posted for everyone to have (including Moonshot of course).&lt;/p&gt; &lt;p&gt;I count myself among the latter group (review and chip in and also learn how people do the tuning thing).&lt;/p&gt; &lt;p&gt;There are quite a few things to iron out but first I want to see if this is even feasible in principle. (I would NOT want to touch any money on this, and would much prefer if that side was handled by some widely-trusted group; or failing that, if something like &lt;a href="http://Together.ai"&gt;Together.ai&lt;/a&gt; might maybe agree to have an account that is usable ONLY for fine-tuning that one model, then people including me just pay into that.)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ramendik"&gt; /u/ramendik &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nn1fqf/kimi_k2_hallucinationsverification_and_fine_tuning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nn1fqf/kimi_k2_hallucinationsverification_and_fine_tuning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nn1fqf/kimi_k2_hallucinationsverification_and_fine_tuning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T19:38:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1nmj3cr</id>
    <title>Lucy-Edit : 1st Open-sourced model for Video editing</title>
    <updated>2025-09-21T04:54:54+00:00</updated>
    <author>
      <name>/u/Technical-Love-8479</name>
      <uri>https://old.reddit.com/user/Technical-Love-8479</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Lucy-Edit-Dev, based on Wan2.2 5B is the first open-sourced AI model with video editing capabilities, calling itself the nano banana for video editing. It can change clothes, characters, backgrounds, object, etc.&lt;/p&gt; &lt;p&gt;Model weights : &lt;a href="https://huggingface.co/decart-ai/Lucy-Edit-Dev"&gt;https://huggingface.co/decart-ai/Lucy-Edit-Dev&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Technical-Love-8479"&gt; /u/Technical-Love-8479 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmj3cr/lucyedit_1st_opensourced_model_for_video_editing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmj3cr/lucyedit_1st_opensourced_model_for_video_editing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nmj3cr/lucyedit_1st_opensourced_model_for_video_editing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T04:54:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1nms85w</id>
    <title>Anyone got an iPhone 17 Pro to test prompt processing? I have an iPhone 16 Pro for comparison.</title>
    <updated>2025-09-21T13:39:12+00:00</updated>
    <author>
      <name>/u/auradragon1</name>
      <uri>https://old.reddit.com/user/auradragon1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nms85w/anyone_got_an_iphone_17_pro_to_test_prompt/"&gt; &lt;img alt="Anyone got an iPhone 17 Pro to test prompt processing? I have an iPhone 16 Pro for comparison." src="https://b.thumbs.redditmedia.com/1_bLYXMQp6ZTc5kwIU_vMMLR36v7_IjEMzxYOYbNOnA.jpg" title="Anyone got an iPhone 17 Pro to test prompt processing? I have an iPhone 16 Pro for comparison." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ol&gt; &lt;li&gt;&lt;p&gt;Download Pocket Pal from iOS app store&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Download and load model Gemma-2-2b-it (Q6_K)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Go to settings and enable Metal. Slide all the way to right. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Go to Benchmark mode (hamburger menu in top left)&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Post results here.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/auradragon1"&gt; /u/auradragon1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nms85w"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nms85w/anyone_got_an_iphone_17_pro_to_test_prompt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nms85w/anyone_got_an_iphone_17_pro_to_test_prompt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T13:39:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1nmr43i</id>
    <title>Is Qwen3 4B enough?</title>
    <updated>2025-09-21T12:50:03+00:00</updated>
    <author>
      <name>/u/Dreamingmathscience</name>
      <uri>https://old.reddit.com/user/Dreamingmathscience</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to run my coding agent locally so I am looking for a appropriate model. &lt;/p&gt; &lt;p&gt;I don't really need tool calling abilities. Instead I want better quality of the generated code. &lt;/p&gt; &lt;p&gt;I am finding 4B to 10B models and if they don't have dramatic code quality diff I prefer the small one. &lt;/p&gt; &lt;p&gt;Is Qwen3 enough for me? Is there any alternative? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dreamingmathscience"&gt; /u/Dreamingmathscience &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmr43i/is_qwen3_4b_enough/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmr43i/is_qwen3_4b_enough/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nmr43i/is_qwen3_4b_enough/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T12:50:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1nn0o62</id>
    <title>What GUI/interface do most people here use to run their models?</title>
    <updated>2025-09-21T19:09:10+00:00</updated>
    <author>
      <name>/u/tech4marco</name>
      <uri>https://old.reddit.com/user/tech4marco</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I used to be a big fan of &lt;a href="https://github.com/nomic-ai/gpt4all"&gt;https://github.com/nomic-ai/gpt4all&lt;/a&gt; but all development has stopped, which is a shame as this was quite lightweight and worked pretty well. &lt;/p&gt; &lt;p&gt;What do people here use to run models in GGUF format? &lt;/p&gt; &lt;p&gt;NOTE: I am not really up to date with everything in LLMA's and dont know what the latest bleeding edge model extension is or what must have applications run these things.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tech4marco"&gt; /u/tech4marco &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nn0o62/what_guiinterface_do_most_people_here_use_to_run/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nn0o62/what_guiinterface_do_most_people_here_use_to_run/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nn0o62/what_guiinterface_do_most_people_here_use_to_run/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T19:09:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1nmg185</id>
    <title>Qwen3Omni</title>
    <updated>2025-09-21T02:08:26+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmg185/qwen3omni/"&gt; &lt;img alt="Qwen3Omni" src="https://preview.redd.it/wcxu5ypyefqf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5b0e169e57d635253c780f31d6542861df594c98" title="Qwen3Omni" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wcxu5ypyefqf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmg185/qwen3omni/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nmg185/qwen3omni/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T02:08:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1nn0u8p</id>
    <title>Why is Hugging Face blocked in China when so many open‚Äëweight models are released by Chinese companies?</title>
    <updated>2025-09-21T19:15:37+00:00</updated>
    <author>
      <name>/u/zoxtech</name>
      <uri>https://old.reddit.com/user/zoxtech</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently learned that HF is inaccessible from mainland China. At the same time, a large share of the open‚Äëweight LLMs are published by Chinese firms.&lt;/p&gt; &lt;p&gt;Is this a legal prohibition on publishing Chinese models, or simply a network‚Äëlevel block that prevents users inside China from reaching the site?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zoxtech"&gt; /u/zoxtech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nn0u8p/why_is_hugging_face_blocked_in_china_when_so_many/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nn0u8p/why_is_hugging_face_blocked_in_china_when_so_many/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nn0u8p/why_is_hugging_face_blocked_in_china_when_so_many/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T19:15:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1nmkswn</id>
    <title>Just dropped: Qwen3-4B Function calling on just 6GB VRAM</title>
    <updated>2025-09-21T06:37:33+00:00</updated>
    <author>
      <name>/u/Honest-Debate-6863</name>
      <uri>https://old.reddit.com/user/Honest-Debate-6863</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just wanted to bring this to you if you are looking for a superior model for toolcalling to use with ollama for local Codex style personal coding assistant on terminal:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Manojb/Qwen3-4B-toolcalling-gguf-codex"&gt;https://huggingface.co/Manojb/Qwen3-4B-toolcalling-gguf-codex&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;‚úÖ &lt;strong&gt;Fine-tuned on 60K function calling examples&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;‚úÖ &lt;strong&gt;4B parameters&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;‚úÖ &lt;strong&gt;GGUF format&lt;/strong&gt; (optimized for CPU/GPU inference)&lt;/li&gt; &lt;li&gt;‚úÖ &lt;strong&gt;3.99GB download&lt;/strong&gt; (fits on any modern system)&lt;/li&gt; &lt;li&gt;‚úÖ &lt;strong&gt;Production-ready&lt;/strong&gt; with 0.518 training loss&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;this works with&lt;br /&gt; &lt;a href="https://github.com/ymichael/open-codex/"&gt;https://github.com/ymichael/open-codex/&lt;/a&gt;&lt;br /&gt; &lt;a href="https://github.com/8ankur8/anything-codex"&gt;https://github.com/8ankur8/anything-codex&lt;/a&gt;&lt;br /&gt; &lt;a href="https://github.com/dnakov/anon-codex"&gt;https://github.com/dnakov/anon-codex&lt;/a&gt;&lt;br /&gt; preferable: &lt;a href="https://github.com/search?q=repo%3Adnakov%2Fanon-codex%20ollama&amp;amp;type=code"&gt;https://github.com/search?q=repo%3Adnakov%2Fanon-codex%20ollama&amp;amp;type=code&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Enjoy!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Honest-Debate-6863"&gt; /u/Honest-Debate-6863 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmkswn/just_dropped_qwen34b_function_calling_on_just_6gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmkswn/just_dropped_qwen34b_function_calling_on_just_6gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nmkswn/just_dropped_qwen34b_function_calling_on_just_6gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T06:37:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1nn18k2</id>
    <title>Predicting the next "attention is all you need"</title>
    <updated>2025-09-21T19:30:36+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;NeurIPS 2025 &lt;a href="https://neurips.cc/Downloads/2025"&gt;accepted papers&lt;/a&gt; are out! If you didn't know, &amp;quot;Attention is all you Need&amp;quot; was published in &lt;a href="https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf"&gt;NeurIPS 2017&lt;/a&gt; and spawned the modern wave of Transformer-based large language models; but few would have predicted this back in 2017. Which NeurIPS 2025 paper do you think is the bext &amp;quot;Attention is all you Need&amp;quot;? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://neurips.cc/Downloads/2025"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nn18k2/predicting_the_next_attention_is_all_you_need/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nn18k2/predicting_the_next_attention_is_all_you_need/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T19:30:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1nmii5y</id>
    <title>Magistral 1.2 is incredible. Wife prefers it over Gemini 2.5 Pro.</title>
    <updated>2025-09-21T04:21:00+00:00</updated>
    <author>
      <name>/u/My_Unbiased_Opinion</name>
      <uri>https://old.reddit.com/user/My_Unbiased_Opinion</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TL:DR - AMAZING general use model. Y'all gotta try it. &lt;/p&gt; &lt;p&gt;Just wanna let y'all know that Magistral is worth trying. Currently running the UD Q3KXL quant from Unsloth on Ollama with Openwebui. &lt;/p&gt; &lt;p&gt;The model is incredible. It doesn't overthink and waste tokens unnecessarily in the reasoning chain. &lt;/p&gt; &lt;p&gt;The responses are focused, concise and to the point. No fluff, just tells you what you need to know. &lt;/p&gt; &lt;p&gt;The censorship is VERY minimal. My wife has been asking it medical-adjacent questions and it always gives you a solid answer. I am an ICU nurse by trade and am studying for advanced practice and can vouch for the advice magistral is giving is legit. &lt;/p&gt; &lt;p&gt;Before this, wife has been using Gemini 2.5 pro and hates the censorship and the way it talks to you like a child (let's break this down, etc). &lt;/p&gt; &lt;p&gt;The general knowledge in Magistral is already really good. Seems to know obscure stuff quite well. &lt;/p&gt; &lt;p&gt;Now, once you hook it up to a web search tool call is where this model I feel like can hit as hard as proprietary LLMs. The model really does wake up even more when hooked up to the web. &lt;/p&gt; &lt;p&gt;Model even supports image input. I have not tried that specifically but I loved image processing from Mistral 3.2 2506 so I expect no issues there.&lt;/p&gt; &lt;p&gt;Currently using with Openwebui with the recommended parameters. If you do use it with OWUI, be sure to set up the reasoning tokens in the model settings so thinking is kept separate from the model response. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/My_Unbiased_Opinion"&gt; /u/My_Unbiased_Opinion &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmii5y/magistral_12_is_incredible_wife_prefers_it_over/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmii5y/magistral_12_is_incredible_wife_prefers_it_over/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nmii5y/magistral_12_is_incredible_wife_prefers_it_over/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T04:21:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1nn01bj</id>
    <title>Qwen3-Coder-480B on the M3 Ultra 512GB Mac Studio is perfect for agentic coding</title>
    <updated>2025-09-21T18:45:29+00:00</updated>
    <author>
      <name>/u/ButThatsMyRamSlot</name>
      <uri>https://old.reddit.com/user/ButThatsMyRamSlot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen3-Coder-480b runs in MLX with 8bit quantization and just barely fits the full 256k context window within 512GB.&lt;/p&gt; &lt;p&gt;With Roo code/cline, Q3C works exceptionally well when working within an existing codebase.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;RAG (with Qwen3-Embed) retrieves API documentation and code samples which eliminates hallucinations.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;The long context length can handle entire source code files for additional details.&lt;/li&gt; &lt;li&gt;Prompt adherence is great, and the subtasks in Roo work very well to gather information without saturating the main context.&lt;/li&gt; &lt;li&gt;VSCode hints are read by Roo and provide feedback about the output code.&lt;/li&gt; &lt;li&gt;Console output is read back to identify compile time and runtime errors.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Green grass is more difficult, Q3C doesn‚Äôt do the best job at architecting a solution given a generic prompt. It‚Äôs much better to explicitly provide a design or at minimum design constraints rather than just ‚Äúimplement X using Y‚Äù.&lt;/p&gt; &lt;p&gt;Prompt processing, especially at full 256k context, can be quite slow. For an agentic workflow, this doesn‚Äôt matter much, since I‚Äôm running it in the background. I find Q3C difficult to use as a coding &lt;em&gt;assistant&lt;/em&gt;, at least the 480b version.&lt;/p&gt; &lt;p&gt;I was on the fence about this machine 6 months ago when I ordered it, but I‚Äôm quite happy with what it can do now. An alternative option I considered was to buy an RTX Pro 6000 for my 256GB threadripper system, but the throughout benefits are far outweighed by the ability to run larger models at higher precision in my use case.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ButThatsMyRamSlot"&gt; /u/ButThatsMyRamSlot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nn01bj/qwen3coder480b_on_the_m3_ultra_512gb_mac_studio/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nn01bj/qwen3coder480b_on_the_m3_ultra_512gb_mac_studio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nn01bj/qwen3coder480b_on_the_m3_ultra_512gb_mac_studio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T18:45:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1nmzio1</id>
    <title>LongCat-Flash-Thinking</title>
    <updated>2025-09-21T18:25:42+00:00</updated>
    <author>
      <name>/u/Xhehab_</name>
      <uri>https://old.reddit.com/user/Xhehab_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmzio1/longcatflashthinking/"&gt; &lt;img alt="LongCat-Flash-Thinking" src="https://preview.redd.it/l7o00pbb9kqf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3fb8a419e2a1961bdcf7234e4eb8e33897a2904f" title="LongCat-Flash-Thinking" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;üöÄ LongCat-Flash-Thinking: Smarter reasoning, leaner costs!&lt;/p&gt; &lt;p&gt;üèÜ Performance: SOTA open-source models on Logic/Math/Coding/Agent tasks&lt;/p&gt; &lt;p&gt;üìä Efficiency: 64.5% fewer tokens to hit top-tier accuracy on AIME25 with native tool use, agent-friendly&lt;/p&gt; &lt;p&gt;‚öôÔ∏è Infrastructure: Async RL achieves a 3x speedup over Sync frameworks&lt;/p&gt; &lt;p&gt;üîóModel: &lt;a href="https://huggingface.co/meituan-longcat/LongCat-Flash-Thinking"&gt;https://huggingface.co/meituan-longcat/LongCat-Flash-Thinking&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üíª Try Now: &lt;a href="http://longcat.ai"&gt;longcat.ai&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xhehab_"&gt; /u/Xhehab_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/l7o00pbb9kqf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmzio1/longcatflashthinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nmzio1/longcatflashthinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T18:25:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1njjz7j</id>
    <title>Our 4th AMA: The LMStudio Team! (Thursday, 11 AM-1 PM PDT)</title>
    <updated>2025-09-17T17:44:17+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njjz7j/our_4th_ama_the_lmstudio_team_thursday_11_am1_pm/"&gt; &lt;img alt="Our 4th AMA: The LMStudio Team! (Thursday, 11 AM-1 PM PDT)" src="https://preview.redd.it/4xt9enbairpf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1cc1ac16a9a2b96134934fcb2f81a9f3d4916b31" title="Our 4th AMA: The LMStudio Team! (Thursday, 11 AM-1 PM PDT)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4xt9enbairpf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njjz7j/our_4th_ama_the_lmstudio_team_thursday_11_am1_pm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1njjz7j/our_4th_ama_the_lmstudio_team_thursday_11_am1_pm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T17:44:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1nkft9l</id>
    <title>AMA with the LM Studio team</title>
    <updated>2025-09-18T18:12:24+00:00</updated>
    <author>
      <name>/u/yags-lms</name>
      <uri>https://old.reddit.com/user/yags-lms</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;! We're excited for this AMA. Thank you for having us here today. We got a full house from the LM Studio team: &lt;/p&gt; &lt;p&gt;- Yags &lt;a href="https://t.co/ERfA4NrR96"&gt;https://reddit.com/user/yags-lms/&lt;/a&gt; (founder)&lt;br /&gt; - Neil &lt;a href="https://t.co/KyiHVfv0QG"&gt;https://reddit.com/user/neilmehta24/&lt;/a&gt; (LLM engines and runtime)&lt;br /&gt; - Will &lt;a href="https://t.co/IjAZJL2JMK"&gt;https://reddit.com/user/will-lms/&lt;/a&gt; (LLM engines and runtime)&lt;br /&gt; - Matt &lt;a href="https://t.co/6MNkItPYnI"&gt;https://reddit.com/user/matt-lms/&lt;/a&gt; (LLM engines, runtime, and APIs)&lt;br /&gt; - Ryan &lt;a href="https://t.co/0snuNUPizo"&gt;https://reddit.com/user/ryan-lms/&lt;/a&gt; (Core system and APIs)&lt;br /&gt; - Rugved &lt;a href="https://t.co/xGtYHsJZI3"&gt;https://reddit.com/user/rugved_lms/&lt;/a&gt; (CLI and SDKs)&lt;br /&gt; - Alex &lt;a href="https://t.co/wtT2IFf0z6"&gt;https://reddit.com/user/alex-lms/&lt;/a&gt; (App)&lt;br /&gt; - Julian &lt;a href="https://www.reddit.com/user/julian-lms/"&gt;https://www.reddit.com/user/julian-lms/&lt;/a&gt; (Ops) &lt;/p&gt; &lt;p&gt;Excited to chat about: the latest local models, UX for local models, steering local models effectively, LM Studio SDK and APIs, how we support multiple LLM engines (llama.cpp, MLX, and more), privacy philosophy, why local AI matters, our open source projects (mlx-engine, lms, lmstudio-js, lmstudio-python, venvstacks), why ggerganov and Awni are the GOATs, where is TheBloke, and more. &lt;/p&gt; &lt;p&gt;Would love to hear about people's setup, which models you use, use cases that really work, how you got into local AI, what needs to improve in LM Studio and the ecosystem as a whole, how you use LM Studio, and anything in between!&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Everyone: it was awesome to see your questions here today and share replies! Thanks a lot for the welcoming AMA. We will continue to monitor this post for more questions over the next couple of days, but for now we're signing off to continue building üî®&lt;/p&gt; &lt;p&gt;We have several marquee features we've been working on for a loong time coming out later this month that we hope you'll love and find lots of value in. And don't worry, UI for n cpu moe is on the way too :)&lt;/p&gt; &lt;p&gt;Special shoutout and thanks to ggerganov, Awni Hannun, TheBloke, Hugging Face, and all the rest of the open source AI community!&lt;/p&gt; &lt;p&gt;Thank you and see you around! - Team LM Studio üëæ&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yags-lms"&gt; /u/yags-lms &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkft9l/ama_with_the_lm_studio_team/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkft9l/ama_with_the_lm_studio_team/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nkft9l/ama_with_the_lm_studio_team/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T18:12:24+00:00</published>
  </entry>
</feed>
