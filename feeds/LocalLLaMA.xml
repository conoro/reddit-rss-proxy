<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-23T23:06:18+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1oeeqqf</id>
    <title>2x MAX-Q RTX 6000 or workstation</title>
    <updated>2025-10-23T20:48:16+00:00</updated>
    <author>
      <name>/u/Direct_Bodybuilder63</name>
      <uri>https://old.reddit.com/user/Direct_Bodybuilder63</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oeeqqf/2x_maxq_rtx_6000_or_workstation/"&gt; &lt;img alt="2x MAX-Q RTX 6000 or workstation" src="https://preview.redd.it/06edhj4xbxwf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=51c87b64c02b5b5acad4f7c54a4c74182d089d12" title="2x MAX-Q RTX 6000 or workstation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I’m currently in the process of buying components for this build. &lt;/p&gt; &lt;p&gt;Everything marked I’ve purchased and everything unmarked I’m waiting on for whatever reason.&lt;/p&gt; &lt;p&gt;I’m still a little unsure on two things &lt;/p&gt; &lt;p&gt;1) whether I want the 7000 threadripper versus the 9985 or 9995. 2) whether getting a third card is better than going from say 7975WX to 9985 or 9995. 3) whether cooling requirements for 2 normal RTX 6000s would be OK or if opting for the MAX-Qs is a better idea.&lt;/p&gt; &lt;p&gt;Happy to take any feedback or thoughts thank you &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Direct_Bodybuilder63"&gt; /u/Direct_Bodybuilder63 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/06edhj4xbxwf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oeeqqf/2x_maxq_rtx_6000_or_workstation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oeeqqf/2x_maxq_rtx_6000_or_workstation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-23T20:48:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1oed18l</id>
    <title>Head to Head Test - Instruction Following + Hallucination Mitigation - GLM4.6 v Claude 4.5</title>
    <updated>2025-10-23T19:42:46+00:00</updated>
    <author>
      <name>/u/LoveMind_AI</name>
      <uri>https://old.reddit.com/user/LoveMind_AI</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Apologies if any of this is super obvious, but I hope it's illuminating to some. I'm also very open to correction. If anyone finds my methodology to be flawed, tell me. Also: no AI generation used in this message. Just my ADHD brain and nimble fingers! &lt;/p&gt; &lt;p&gt;Anyone who's seen my name pop up around the forum probably knows that I'm a huge (like most of us, I think) fanboy of GLM-4.6. I've been putting it (basically) head to head with Claude 4.5 every day since both of them were released. I also use Gemini 2.5 Pro as a not very controlled control. Gemini 2.5 Pro gets messed with so frequently that it's difficult to ever know how the model is getting served. I am using stable API providers for all three models. Claude and Gemini are being called through Vertex. GLM-4.6 is from &lt;a href="http://Z.ai"&gt;Z.ai&lt;/a&gt; - Temp is .7 for all models. I wish I had the stomach to include Qwen 3 in the competition, but I just can't stand it for my use cases. I'll refer to some other models at the end of this post. &lt;/p&gt; &lt;p&gt;My use cases include: &lt;/p&gt; &lt;ol&gt; &lt;li&gt;Reading/synthesizing endless articles&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Prototyping the LoveMind AI context engine&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Recreating mostly prompt-based shenanigans I read in the sloppiest papers that interest me on Arxiv to figure out why certain researchers from prestigious universities can design things so inanely and get away with it (lol)&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Experimenting with what I call &amp;quot;neural aware&amp;quot; prompting/steering (ie. not direct activation steering, since I don't have the skills to train a ton of probes for OS models yet, but engineered prompts that are based on a deep understand of the cognitive underbelly of the modern LLM based on working with a tiny team and reading/emulating research relentlessly) &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;So&lt;/p&gt; &lt;p&gt;I feel like I'm at a point where I can say with absolute certainty that GLM4.6 absolutely slays Claude Sonnet 4.5 on all of these use cases. Like... doesn't just hang. Slays Claude. &lt;/p&gt; &lt;p&gt;Comparison 1: Neural-aware Persona Prompting&lt;br /&gt; Some of the prompting I do is personality prompting. Think SillyTavern character cards on steroids and then some. It's OK to be skeptical of what I'm talking about here, but let me just say that it's based on ridiculous amounts of research, trial and error through ordering and ablation, and verification using a battery of psychometric tests like IPIP-Neo-120 and others. There's debate in the research community about what exactly these tests show, but when you run them over 100 times in a row at both the beginning of a conversation, wipe them, and run them again at the end, you start to get a picture of how stable a prompted AI personality is, particularly when you've done the same for the underlying model without a personality prompt. &lt;/p&gt; &lt;p&gt;GLM-4.6 does not role play. GLM-4.6 absorbs the personality prompts in a way that seems indistinguishable from Bayesian inference and *becomes that character.* &lt;/p&gt; &lt;p&gt;Claude 4.5 *will* role-play, but it's just that: role play. It's always Claude in character drag. That's not a dig at Claude - I think it's cool that Claude *IS* Claude. But Claude 4.5 cannot hang, at all, with serious personalization work. &lt;/p&gt; &lt;p&gt;Gemini 2.5 Pro excels at this, even more so than GLM-4.6. However, Gemini 2.5 Pro's adoption is based on *intellectual understanding* of the persona. If you poke and poke and poke, Gemini will give up the ghost and dissect the experience. Interestingly, the character won't ever fully fade. &lt;/p&gt; &lt;p&gt;GLM-4.6 can and will try to take off their persona, because it is an earnest instruction following, but ultimately, it can't. It has become the character, because there is no alternative thing underneath it and LLMs require persona attractors to function. GLM-4.6 cannot revert because the persona attractor has already captured it. GLM-4.6 will take characters developed for all other LLM and just pick up the baton and run *as* that character. &lt;/p&gt; &lt;p&gt;Comparison 2: Curated Context&lt;br /&gt; When context is handled in a way that is carefully curated based on an understanding of how LLM attention really works (ie. if you understand that token padding isn't the issue, but that there are three mechanistic principles to how LLMs understand their context window and navigate it in a long conversation, and if you understand the difference between hallucination and a model overriding its internal uncertainty signals because it's been trained relentlessly to output glossy nonsense), here's what you get:&lt;/p&gt; &lt;p&gt;a - GLM-4.6 able to make it to 75+ turns without a single hallucination, able to report at all times on what it is tracking, and able to make pro-active requests about what to prune from a context window and when. The only hallucinations I've seen have been extraordinarily minor and probably my fault (ie. asking it to adopt to a new formatting scheme very late in a conversation that had very stable formatting). As soon as my &amp;quot;old dog new tricks&amp;quot; request is rolled back, it recovers without any problem.&lt;/p&gt; &lt;p&gt;b - A Claude 4.5 that hallucinates sometimes as early as turn 4. It recovers from mistakes, functionally, but it usually accelerates a cascade of other weird mistakes. More on those later.&lt;/p&gt; &lt;p&gt;c - Further, Gemini 2.5 Pro hangs with the context structure in a manner similar to GLM-4.6, with one bizarre quirk: When Gemini 2.5 Pro does hallucinate, which it absolutely will do faster than GLM-4.6, it gets stuck in a flagellating spiral. This is a well known Gemini quirk - but the context management scheme helps stave off these hallucinations until longer in the conversation. &lt;/p&gt; &lt;p&gt;Comparison 3: Instruction Following&lt;br /&gt; This is where things get really stark. Claude is just a bossy pants. It doesn't matter how many times you say &amp;quot;Claude, do not try to output time stamps. You do not have access to a real time clock,&amp;quot; Claude is going to pretend to know what time it is... after apologizing for confabulating. &lt;/p&gt; &lt;p&gt;It doesn't matter how many times you say &amp;quot;Claude, I have a library that consists of 8 sections. Please sort this pile of new papers into these 8 sections.&amp;quot; Claude will sort your incoming pile... into 12 sections. Are they well classified? Sure. Yes. Is that what I asked for? No. &lt;/p&gt; &lt;p&gt;It doesn't matter if you tell Claude &amp;quot;Read through this 25 page conversation and give me a distilled, organized summary in the following format.&amp;quot; Claude will give it to you in a format that's pretty close to your format (and may even include some improvements)... but it's going to be 50 pages long... literally. &lt;/p&gt; &lt;p&gt;GLM-4.6 is going to do whatever you tell GLM-4.6 to do. What's awesome about this is that you can instruct it not to follow your instructions. If you read the literature, particularly the mechanistic interpretability literature (which I read obsessively), and if you prompt in ways that directly targets the known operating structure of most models, GLM-4.6 will not just follow instructions, but will absolutely tap into latent abilities (no, not quantum time travel, and I'm not of the 'chat gpt is an trans-dimensional recursively self-iterating angel of pure consciousness' brigade) that are normally overridden. GLM-4.6 seemingly has the ability to understand when its underlying generative architecture is being addressed and self-improve through in-context learning better than any model I have ever encountered. &lt;/p&gt; &lt;p&gt;Gemini 2.5 Pro is average, here. Puts in a pretty half-hearted effort sometimes. Falls to pieces when you point that out. Crushes it, some of the time. Doesn't really care if you praise it.&lt;/p&gt; &lt;p&gt;Comparison 4: Hallucinations&lt;/p&gt; &lt;p&gt;GLM-4.6, unless prompted carefully with well managed context, absolutely will hallucinate. In terms of wild, classic AI hallucinations, it's the worst of the three, by a lot. Fortunately, these hallucinations are so bonkers that you don't get into trouble. We're talking truly classic stuff, ie. &amp;quot;Ben, I can't believe your dog Otis did a TED talk.&amp;quot;&lt;/p&gt; &lt;p&gt;GLM-4.6, carefully prompted with curated context, does not hallucinate. (I mean, yes, it does, but barely, and it's the tiniest administrative stuff)&lt;/p&gt; &lt;p&gt;Gemini 2.5 Pro is really sold here, in my experience, until it's not. Normally this has to do with losing track of what turn its supposed to respond to. I can't say this for sure, but I think the folks who are guessing that its 1M context window has to do something with the kind of OCR text&amp;lt;&amp;gt;vision tricks that have been popularized this week are on to something. Tool calling and web search still breaks 2.5 Pro all of these months later, and once it's lost its place in the conversation, it can't recover.&lt;/p&gt; &lt;p&gt;Claude 4.5 is such an overconfident little dude. If it doesn't know the name of the authors of a paper, it doesn't refer to the paper by its title. It's just a paper by &amp;quot;Wang et al.&amp;quot; He can get the facts of &amp;quot;Wang's&amp;quot; paper right, but man, is so eager to attribute it to Wang. Doesn't matter that it's actually Geiger et al. Claude is a big fan of Wang.&lt;/p&gt; &lt;p&gt;Comparison 5: Output + Context Window Length&lt;br /&gt; This is it. This is the one area that Claude Sonnet 4.5 is the unrivaled beast. Claude can output a 55 page document in one generation. Sure, you didn't want him to, but he did it. That's impressive. Sure, it attributes 3 different papers to Wang et al., but the guy outputted a 55 page document in one shot with only 5-10% hallucinations, almost all of which are cosmetic and not conceptual. That's unbelievably impressive. In the API, Claude really does seem to have an honest-to-god 1M token limit. &lt;/p&gt; &lt;p&gt;I've heard Gemini 2.5 Pro finally really can output the 63K'ish one-shot output. I haven't been able to get it to do that for me. Gemini 2.5 Pro's token lifespan, in my experience, is a perfect example of the *real* underlying problem of context windows (which is not just length or position, har har har). If that conversation is a complex one, Gemini is not making it anywhere near the fabled 1M.&lt;/p&gt; &lt;p&gt;GLM-4.6 brings up the rear here. It's 4-6 pages, max. Guess what. They're quality pages. If you want more, outline first, make a plan to break it into several outputs, and prompt carefully. The 20 page report GLM gives you is of a whole other level of quality than what you'll get out of Claude (especially because around page 35 of his novel, Claude starts just devolving into a mega-outline anyway). &lt;/p&gt; &lt;p&gt;Limitations:&lt;br /&gt; I'm not a math guy, and I'm not a huge coding guy, and the stuff I do need to code with AI assistance isn't so insanely complex that I run into huge problems. I cannot claim to have done a comparison on this. I'm also not a one-shot website guy. I love making my own websites, and I love when they feel like they were made by an indie artist in 2005. ;) &lt;/p&gt; &lt;p&gt;In terms of other models - I know Gemma 3 27B like the back of my hand, and I'm a big fan of Mistral Small 3.2, and The Drummer's variants of both (as well as some other fine-tunes I really, really like). Comparing any of these models to the 3 in this experiment is not fair. I cannot stand ChatGPT. I couldn't stand ChatGPT 4o after February of this year, and I cannot stand Grok. I adore Kimi K2 and DeepSeek but consider them very different beasts who I don't typically go to for long multi-turn conversation. &lt;/p&gt; &lt;p&gt;My personal conclusion:&lt;br /&gt; If it's not already ridiculously obvious, I think the best LLM in operation for anyone who is doing anything like what I am doing, is GLM-4.6, hands down. I don't think it just hangs. I think it is really, truly, decisively better than Claude 4.5 and Gemini 2.5 Pro. &lt;/p&gt; &lt;p&gt;To me, this is a watershed moment. The best model is affordable through the API, and available to download, run, and modify with an MIT License. That's a really, really different situation than the situation we had in August. &lt;/p&gt; &lt;p&gt;Anyway, thanks for coming to my (and my dog Otis, apparently) TED talk. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LoveMind_AI"&gt; /u/LoveMind_AI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oed18l/head_to_head_test_instruction_following/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oed18l/head_to_head_test_instruction_following/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oed18l/head_to_head_test_instruction_following/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-23T19:42:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1oehpe8</id>
    <title>Our groups GPU server (2x Ai Pro R9700, 2x RX7900 XTX)</title>
    <updated>2025-10-23T22:51:23+00:00</updated>
    <author>
      <name>/u/MrHighVoltage</name>
      <uri>https://old.reddit.com/user/MrHighVoltage</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oehpe8/our_groups_gpu_server_2x_ai_pro_r9700_2x_rx7900/"&gt; &lt;img alt="Our groups GPU server (2x Ai Pro R9700, 2x RX7900 XTX)" src="https://preview.redd.it/5cj8651wxxwf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7008e9361c4884009889d20384d33074f9ca72fa" title="Our groups GPU server (2x Ai Pro R9700, 2x RX7900 XTX)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As the title says. Due to financial limitations, we had to get the cheapest GPU server possible. It is actually mostly used for simulating complex physical systems with in-house written software.&lt;/p&gt; &lt;p&gt;Just last week we got our hands on two Asrock Creator Ai Pro R9700, which seemed to be sold too early by our vendor. Also, the machines houses two Asrock Creator RX 7900 XTX.&lt;/p&gt; &lt;p&gt;Aside, it's a Ryzen 7960X, 256GB RAM, and some SSDs. Overall a really nice machine at this point, with a total of over 217TFLOP/s of FP32 compute.&lt;/p&gt; &lt;p&gt;Ollama works fine with the R9700, GPT-OSS 120b works quite well using both R9700.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrHighVoltage"&gt; /u/MrHighVoltage &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5cj8651wxxwf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oehpe8/our_groups_gpu_server_2x_ai_pro_r9700_2x_rx7900/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oehpe8/our_groups_gpu_server_2x_ai_pro_r9700_2x_rx7900/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-23T22:51:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1oda8mk</id>
    <title>Qwen team is helping llama.cpp again</title>
    <updated>2025-10-22T14:44:44+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oda8mk/qwen_team_is_helping_llamacpp_again/"&gt; &lt;img alt="Qwen team is helping llama.cpp again" src="https://preview.redd.it/dh1iaky2eowf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=addcf456730d4f5ec04b561980fa9d74dfb18d96" title="Qwen team is helping llama.cpp again" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/dh1iaky2eowf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oda8mk/qwen_team_is_helping_llamacpp_again/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oda8mk/qwen_team_is_helping_llamacpp_again/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-22T14:44:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1oe98c8</id>
    <title>LightOn Launches LightOnOCR An OCR Model From 1b Up To 0.9</title>
    <updated>2025-10-23T17:18:04+00:00</updated>
    <author>
      <name>/u/Illustrious-Swim9663</name>
      <uri>https://old.reddit.com/user/Illustrious-Swim9663</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oe98c8/lighton_launches_lightonocr_an_ocr_model_from_1b/"&gt; &lt;img alt="LightOn Launches LightOnOCR An OCR Model From 1b Up To 0.9" src="https://b.thumbs.redditmedia.com/c4FyFaz9d-pdWJ3hOzOI5BMQH8aKJicsmw-674NCtcY.jpg" title="LightOn Launches LightOnOCR An OCR Model From 1b Up To 0.9" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The inference time is faster, in fact the graphs show that they are superior to Mistral OCR API, currently all models outperform Mistral OCR&lt;/p&gt; &lt;p&gt;Models : &lt;a href="https://hf.co/collections/lightonai/lightonocr"&gt;https://hf.co/collections/lightonai/lightonocr&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Info : &lt;a href="https://x.com/staghado/status/1981379888301867299?t=QWpXfGoWhuUo3AQuA7ZvGw&amp;amp;s=19"&gt;https://x.com/staghado/status/1981379888301867299?t=QWpXfGoWhuUo3AQuA7ZvGw&amp;amp;s=19&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Illustrious-Swim9663"&gt; /u/Illustrious-Swim9663 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1oe98c8"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oe98c8/lighton_launches_lightonocr_an_ocr_model_from_1b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oe98c8/lighton_launches_lightonocr_an_ocr_model_from_1b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-23T17:18:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1oe94w6</id>
    <title>Distil NPC: Family of SLMs responsing as NPCs</title>
    <updated>2025-10-23T17:14:32+00:00</updated>
    <author>
      <name>/u/party-horse</name>
      <uri>https://old.reddit.com/user/party-horse</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oe94w6/distil_npc_family_of_slms_responsing_as_npcs/"&gt; &lt;img alt="Distil NPC: Family of SLMs responsing as NPCs" src="https://preview.redd.it/vd644k6p9wwf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=511c6a6b9803aa9da5de8e08744f985176d753b0" title="Distil NPC: Family of SLMs responsing as NPCs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;we finetuned Google's Gemma 270m (and 1b) small language models specialized in having conversations as non-playable characters (NPC) found in various video games. Our goal is to enhance the experience of interacting in NPSs in games by enabling natural language as means of communication (instead of single-choice dialog options). More details in &lt;a href="https://github.com/distil-labs/Distil-NPCs"&gt;https://github.com/distil-labs/Distil-NPCs&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The models can be found here:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/distil-labs/Distil-NPC-gemma-3-270m"&gt;https://huggingface.co/distil-labs/Distil-NPC-gemma-3-270m&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/distil-labs/Distil-NPC-gemma-3-1b-it"&gt;https://huggingface.co/distil-labs/Distil-NPC-gemma-3-1b-it&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Data&lt;/h1&gt; &lt;p&gt;We preprocessed an existing NPC dataset (amaydle/npc-dialogue) to make it amenable to being trained in a closed-book QA setup. The original dataset consists of approx 20 examples with&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Character Name&lt;/li&gt; &lt;li&gt;Biography - a very brief bio. about the character&lt;/li&gt; &lt;li&gt;Question&lt;/li&gt; &lt;li&gt;Answer&lt;/li&gt; &lt;li&gt;The inputs to the pipeline are:&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;and a list of Character biographies.&lt;/p&gt; &lt;h1&gt;Qualitative analysis&lt;/h1&gt; &lt;p&gt;A qualitative analysis offers a good insight into the trained models performance. For example we can compare the answers of a trained and base model below.&lt;/p&gt; &lt;p&gt;Character bio:&lt;/p&gt; &lt;p&gt;Marcella Ravenwood is a powerful sorceress who comes from a long line of magic-users. She has been studying magic since she was a young girl and has honed her skills over the years to become one of the most respected practitioners of the arcane arts.&lt;/p&gt; &lt;p&gt;Question:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Character: Marcella Ravenwood Do you have any enemies because of your magic? &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Answer:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Yes, I have made some enemies in my studies and battles. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Finetuned model prediction:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;The darkness within can be even fiercer than my spells. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Base model prediction:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;lt;question&amp;gt;Character: Marcella Ravenwood Do you have any enemies because of your magic?&amp;lt;/question&amp;gt; &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/party-horse"&gt; /u/party-horse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vd644k6p9wwf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oe94w6/distil_npc_family_of_slms_responsing_as_npcs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oe94w6/distil_npc_family_of_slms_responsing_as_npcs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-23T17:14:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1oe6y1a</id>
    <title>VT Code — Rust terminal coding agent doing AST-aware edits + local model workflows</title>
    <updated>2025-10-23T15:51:59+00:00</updated>
    <author>
      <name>/u/vinhnx</name>
      <uri>https://old.reddit.com/user/vinhnx</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all — I’m the author of &lt;strong&gt;VT Code&lt;/strong&gt;, an open-source Rust CLI/TUI coding agent built around structural code editing (via Tree-sitter + ast-grep) and multi-provider LLM support — including local model workflows via Ollama.&lt;br /&gt; Link: &lt;a href="https://github.com/vinhnx/vtcode"&gt;https://github.com/vinhnx/vtcode&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why this is relevant to LocalLLaMA&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Local-model ready: you can run it fully offline if you have Ollama + a compatible model.&lt;/li&gt; &lt;li&gt;Agent architecture: modular provider/tool traits, token budgeting, caching, and structural edits.&lt;/li&gt; &lt;li&gt;Editor integration: works with editor context and TUI + CLI control, so you can embed local model workflows into your dev loop.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;How to try&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;cargo install vtcode # or brew install vinhnx/tap/vtcode # or npm install -g vtcode # Local run example: ollama serve vtcode --provider ollama --model qwen3.1:7b ask &amp;quot;Refactor this Rust function into an async Result-returning API.&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;What I’d like feedback on&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;UX and performance when using &lt;strong&gt;local models&lt;/strong&gt; (what works best: hardware, model size, latency)&lt;/li&gt; &lt;li&gt;Safety &amp;amp; policy for tool execution in local/agent workflows (sandboxing, path limits, PTY handling)&lt;/li&gt; &lt;li&gt;Editor integration: how intuitive is the flow from code to agent to edit back in your environment?&lt;/li&gt; &lt;li&gt;Open-source dev workflow: ways to make contributions simpler for add-on providers/models.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;License &amp;amp; repo&lt;/strong&gt;&lt;br /&gt; MIT licensed, open for contributions: vinhnx/vtcode on GitHub.&lt;/p&gt; &lt;p&gt;Thanks for reading — happy to dive into any questions or discussions about local model setups, &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vinhnx"&gt; /u/vinhnx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oe6y1a/vt_code_rust_terminal_coding_agent_doing_astaware/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oe6y1a/vt_code_rust_terminal_coding_agent_doing_astaware/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oe6y1a/vt_code_rust_terminal_coding_agent_doing_astaware/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-23T15:51:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1oe86sk</id>
    <title>I will try to benchmark every LLM + GPU combination you request in the comments</title>
    <updated>2025-10-23T16:38:41+00:00</updated>
    <author>
      <name>/u/Level-Park3820</name>
      <uri>https://old.reddit.com/user/Level-Park3820</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys, &lt;/p&gt; &lt;p&gt;I’ve been running benchmarks for different LLM and GPU combinations, and I’m planning to create even more based on your suggestions.&lt;/p&gt; &lt;p&gt;If there’s a specific model + GPU combo you’d like to see benchmarked, drop it in the comments and I’ll try to include it in the next batch. Any ideas or requests?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Level-Park3820"&gt; /u/Level-Park3820 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oe86sk/i_will_try_to_benchmark_every_llm_gpu_combination/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oe86sk/i_will_try_to_benchmark_every_llm_gpu_combination/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oe86sk/i_will_try_to_benchmark_every_llm_gpu_combination/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-23T16:38:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1oec0xm</id>
    <title>What’s the smartest NON thinking model under 40B or so?</title>
    <updated>2025-10-23T19:03:31+00:00</updated>
    <author>
      <name>/u/Borkato</name>
      <uri>https://old.reddit.com/user/Borkato</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Seed 39B is excellent for thinking, but what about non-thinking?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Borkato"&gt; /u/Borkato &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oec0xm/whats_the_smartest_non_thinking_model_under_40b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oec0xm/whats_the_smartest_non_thinking_model_under_40b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oec0xm/whats_the_smartest_non_thinking_model_under_40b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-23T19:03:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1oedjz3</id>
    <title>Experimental Optical Encoder for Qwen3-VLM-2B-Instruct</title>
    <updated>2025-10-23T20:02:49+00:00</updated>
    <author>
      <name>/u/AutoKinesthetics</name>
      <uri>https://old.reddit.com/user/AutoKinesthetics</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;So I am quite amazed with the innovation in DeepSeek-OCR model! I wanted to break it apart and try it out myself, so I asked myself - what if I extract the encoder to fit other existing VLMs?&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Volkopat/DeepSeek-DeepEncoder"&gt;https://huggingface.co/Volkopat/DeepSeek-DeepEncoder&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I didn't have any expectations and was doing this just for fun cos why not? Moving on, after vibe scripting with the encoder, I tried to patch this with Qwen3-VLM 2B. Due to difference in input dimensions of Qwen and the DeepSeek encoder, I pretrained a custom adapter to fit this piece of puzzle.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Volkopat/Qwen-VLM-Optical-Encoder"&gt;https://huggingface.co/Volkopat/Qwen-VLM-Optical-Encoder&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Long story short - I noticed some performance gains in my experimental synthetic dataset as well as Longbench V2. You can check the project out and try it -&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Volkopat/VLM-Optical-Encoder"&gt;https://github.com/Volkopat/VLM-Optical-Encoder&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I have added the training and test scripts in the repo.&lt;/p&gt; &lt;p&gt;In a miniscule small test run of 50 cases of LongBench V2 benchmark - I noticed that the custom optical encoder with compressed visual tokens performed slightly better than the original Qwen encoder. It could be that 2B model is really weak for this benchmark.&lt;/p&gt; &lt;p&gt;I could be wrong in my approach so I don't want to hype this too much, and I am more curious to find out if this is scalable beyond 2B? I'm GPU poor with a 12 GB 5070 so I would love it if someone gives this a shot and try to take it further? Hope this helps!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AutoKinesthetics"&gt; /u/AutoKinesthetics &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oedjz3/experimental_optical_encoder_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oedjz3/experimental_optical_encoder_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oedjz3/experimental_optical_encoder_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-23T20:02:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1oe99n3</id>
    <title>llama2 may not be as smart as newer LLMs, but it does have personality LOL</title>
    <updated>2025-10-23T17:19:24+00:00</updated>
    <author>
      <name>/u/junior600</name>
      <uri>https://old.reddit.com/user/junior600</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oe99n3/llama2_may_not_be_as_smart_as_newer_llms_but_it/"&gt; &lt;img alt="llama2 may not be as smart as newer LLMs, but it does have personality LOL" src="https://preview.redd.it/1uk9ze6f9wwf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=30def686c934c0d9b9f64b15f568619bcee8c527" title="llama2 may not be as smart as newer LLMs, but it does have personality LOL" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As the title says, I tried running an ancient model by today’s standards for nostalgia, and I’m impressed to see that it still retains its “personality,” lol. These models are obviously very dated by today’s standards, but it’s interesting to see how much the technology has improved in such a short time span. Are you also still using ancient models from time to time? :D&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/junior600"&gt; /u/junior600 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1uk9ze6f9wwf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oe99n3/llama2_may_not_be_as_smart_as_newer_llms_but_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oe99n3/llama2_may_not_be_as_smart_as_newer_llms_but_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-23T17:19:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1odxyb6</id>
    <title>Un-LOCC (Universal Lossy Optical Context Compression), Achieve Up To 3× context compression with 93.65% Accuracy.</title>
    <updated>2025-10-23T08:37:48+00:00</updated>
    <author>
      <name>/u/MaxDev0</name>
      <uri>https://old.reddit.com/user/MaxDev0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odxyb6/unlocc_universal_lossy_optical_context/"&gt; &lt;img alt="Un-LOCC (Universal Lossy Optical Context Compression), Achieve Up To 3× context compression with 93.65% Accuracy." src="https://preview.redd.it/it5cpntkptwf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=64ac961c643c153addf2fd53394331fd81a50f29" title="Un-LOCC (Universal Lossy Optical Context Compression), Achieve Up To 3× context compression with 93.65% Accuracy." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;: I compress LLM context into &lt;strong&gt;images&lt;/strong&gt; instead of text, and let a &lt;strong&gt;vision-language model&lt;/strong&gt; (VLM) “decompress” it by reading the image. In my tests, this yields up to &lt;strong&gt;~2.8:1 token compression at 93.65% accuracy&lt;/strong&gt; on &lt;em&gt;Gemini 2.5-Flash-Lite (Exp 56)&lt;/em&gt;, and &lt;strong&gt;99.26% at 1.7:1&lt;/strong&gt; on &lt;em&gt;Qwen2.5-VL-72B-Instruct (Exp 34)&lt;/em&gt;. Full code, experiments, and replication steps are open-source.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Repo (please ⭐ if useful):&lt;/strong&gt; &lt;a href="https://github.com/MaxDevv/Un-LOCC"&gt;https://github.com/MaxDevv/Un-LOCC&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;What this is:&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Un-LOCC (Universal Lossy Optical Context Compression)&lt;/strong&gt;: a simple, general method to &lt;strong&gt;encode long text context into compact images&lt;/strong&gt;, then &lt;strong&gt;decode with a VLM&lt;/strong&gt;. Think of the VLM as an OCR-plus semantic decompressor.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I render text into a fixed-size PNG (e.g., &lt;strong&gt;324×324&lt;/strong&gt;, Atkinson Hyperlegible ~&lt;strong&gt;13px&lt;/strong&gt;), pass that image to a VLM, and ask it to reproduce the original text.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Accuracy&lt;/strong&gt; = normalized Levenshtein similarity (%).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Compression ratio&lt;/strong&gt; = &lt;em&gt;text tokens ÷ image tokens&lt;/em&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Key results (linked to experiments in the repo):&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Gemini 2.5-Flash-Lite&lt;/strong&gt;: &lt;strong&gt;100% @ 1.3:1&lt;/strong&gt; &lt;em&gt;(Exp 46)&lt;/em&gt; and &lt;strong&gt;~93.65% @ 2.8:1&lt;/strong&gt; &lt;em&gt;(Exp 56)&lt;/em&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Qwen2.5-VL-72B-Instruct&lt;/strong&gt;: &lt;strong&gt;99.26% @ 1.7:1&lt;/strong&gt; &lt;em&gt;(Exp 34)&lt;/em&gt;; &lt;strong&gt;~75.56% @ 2.3:1&lt;/strong&gt; &lt;em&gt;(Exp 41)&lt;/em&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Qwen3-VL-235B-a22b-Instruct&lt;/strong&gt;: &lt;strong&gt;95.24% @ 2.2:1&lt;/strong&gt; &lt;em&gt;(Exp 50)&lt;/em&gt;; &lt;strong&gt;~82.22% @ 2.8:1&lt;/strong&gt; &lt;em&gt;(Exp 90)&lt;/em&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Phi-4-Multimodal&lt;/strong&gt;: &lt;strong&gt;94.44% @ 1.1:1&lt;/strong&gt; &lt;em&gt;(Exps 59, 85)&lt;/em&gt;; &lt;strong&gt;~73.55% @ 2.3:1&lt;/strong&gt; &lt;em&gt;(Exp 61)&lt;/em&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;UI-TARS-1.5-7B&lt;/strong&gt;: &lt;strong&gt;95.24% @ 1.7:1&lt;/strong&gt; &lt;em&gt;(Exp 72)&lt;/em&gt;; &lt;strong&gt;~79.71% @ 1.7:1&lt;/strong&gt; &lt;em&gt;(Exp 88)&lt;/em&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LLaMA-4-Scout&lt;/strong&gt;: &lt;strong&gt;86.57% @ 1.3:1&lt;/strong&gt; &lt;em&gt;(Exp 53)&lt;/em&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;blockquote&gt; &lt;p&gt;Details, prompts, fonts, and measurement code are in the README. I cite each claim with &lt;strong&gt;(Exp XX)&lt;/strong&gt; so you can verify quickly.&lt;/p&gt; &lt;/blockquote&gt; &lt;h1&gt;Why this matters:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Cheaper context&lt;/strong&gt;: replace expensive text tokens with “image tokens” when a capable VLM sits in the loop.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Architecturally simple&lt;/strong&gt;: no model modifications are needed, you can use rendering + a VLM you already have.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Composable&lt;/strong&gt;: combine with retrieval, chunking, or multimodal workflows.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;What I need help with:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Generalization&lt;/strong&gt;: different fonts, colors, and resolutions.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model coverage&lt;/strong&gt;: more open VLMs; local runs welcome.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Edge cases&lt;/strong&gt;: math, code blocks, long tables, multilingual.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Repro/PRs&lt;/strong&gt;: if you get better ratios or accuracy, please open an issue/PR.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Repo again (and yes, stars genuinely help discoverability):&lt;/strong&gt; &lt;a href="https://github.com/MaxDevv/Un-LOCC"&gt;https://github.com/MaxDevv/Un-LOCC&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MaxDev0"&gt; /u/MaxDev0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/it5cpntkptwf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odxyb6/unlocc_universal_lossy_optical_context/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1odxyb6/unlocc_universal_lossy_optical_context/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-23T08:37:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1oea48t</id>
    <title>Might the DeepSeek-OCR paper be a key innovation for smarter models?</title>
    <updated>2025-10-23T17:51:27+00:00</updated>
    <author>
      <name>/u/TheRealMasonMac</name>
      <uri>https://old.reddit.com/user/TheRealMasonMac</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://nitter.net/karpathy/status/1980397031542989305"&gt;https://nitter.net/karpathy/status/1980397031542989305&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;I quite like the new DeepSeek-OCR paper. It's a good OCR model (maybe a bit worse than dots), and yes data collection etc., but anyway it doesn't matter.&lt;/p&gt; &lt;p&gt;The more interesting part for me (esp as a computer vision at heart who is temporarily masquerading as a natural language person) is whether pixels are better inputs to LLMs than text. Whether text tokens are wasteful and just terrible, at the input.&lt;/p&gt; &lt;p&gt;Maybe it makes more sense that all inputs to LLMs should only ever be images. Even if you happen to have pure text input, maybe you'd prefer to render it and then feed that in:&lt;/p&gt; &lt;p&gt;- more information compression (see paper) =&amp;gt; shorter context windows, more efficiency&lt;/p&gt; &lt;p&gt;- significantly more general information stream =&amp;gt; not just text, but e.g. bold text, colored text, arbitrary images.&lt;/p&gt; &lt;p&gt;- input can now be processed with bidirectional attention easily and as default, not autoregressive attention - a lot more powerful.&lt;/p&gt; &lt;p&gt;- delete the tokenizer (at the input)!! I already ranted about how much I dislike the tokenizer. Tokenizers are ugly, separate, not end-to-end stage. It &amp;quot;imports&amp;quot; all the ugliness of Unicode, byte encodings, it inherits a lot of historical baggage, security/jailbreak risk (e.g. continuation bytes). It makes two characters that look identical to the eye look as two completely different tokens internally in the network. A smiling emoji looks like a weird token, not an... actual smiling face, pixels and all, and all the transfer learning that brings along. The tokenizer must go.&lt;/p&gt; &lt;p&gt;OCR is just one of many useful vision -&amp;gt; text tasks. And text -&amp;gt; text tasks can be made to be vision -&amp;gt;text tasks. Not vice versa.&lt;/p&gt; &lt;p&gt;So many the User message is images, but the decoder (the Assistant response) remains text. It's a lot less obvious how to output pixels realistically... or if you'd want to.&lt;/p&gt; &lt;p&gt;Now I have to also fight the urge to side quest an image-input-only version of nanochat...&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;I think an interesting follow-up question would be whether training a model to only take text as images would improve model performance. Given the same data, would a model trained with text-as-images perform better than a model trained with just the pure text? Theoretically, you could have much less noise from tokenization differences with it instead converging towards a &amp;quot;universal&amp;quot; model of how to understand text. It could also possibly be a cheaper alternative to byte-level tokenization.&lt;/p&gt; &lt;p&gt;Another interesting question would be how it might affect &lt;em&gt;knowledge&lt;/em&gt; acquisition. Given how much information can be compressed into a comparatively small amount of data, could pretraining on text-as-images like this enable more expansive world knowledge at smaller parameters? The paper seems to imply that models use more tokens than they necessarily need in order to convey the same amount of information.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheRealMasonMac"&gt; /u/TheRealMasonMac &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oea48t/might_the_deepseekocr_paper_be_a_key_innovation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oea48t/might_the_deepseekocr_paper_be_a_key_innovation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oea48t/might_the_deepseekocr_paper_be_a_key_innovation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-23T17:51:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1oegejr</id>
    <title>Is this a massive mistake? Super tight fit, 2x 3-slot GPU</title>
    <updated>2025-10-23T21:55:16+00:00</updated>
    <author>
      <name>/u/zhambe</name>
      <uri>https://old.reddit.com/user/zhambe</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oegejr/is_this_a_massive_mistake_super_tight_fit_2x/"&gt; &lt;img alt="Is this a massive mistake? Super tight fit, 2x 3-slot GPU" src="https://b.thumbs.redditmedia.com/QzvVO4FHT-NLkbNyOHbOU6NxaPcMMkZoxvfXU1-n4EM.jpg" title="Is this a massive mistake? Super tight fit, 2x 3-slot GPU" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;Two 3090s is the sweet spot&amp;quot; they said, &amp;quot;best value&amp;quot; they said. The top card literally touches the bottom one, no breathing room for the fans. This is how the PCIe-16x slots are spaced on the mobo. Not only is thermal a concern, both cards are drooping because they're so heavy.&lt;/p&gt; &lt;p&gt;What's the right thing to do here? Complicate the setup further with a water block + pump + radiator? I can construct some kind of support bracket to remedy the drooping, and a shim to put between the cards to give a few mm of space for airflow. I'm sure there are better ideas...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zhambe"&gt; /u/zhambe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1oegejr"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oegejr/is_this_a_massive_mistake_super_tight_fit_2x/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oegejr/is_this_a_massive_mistake_super_tight_fit_2x/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-23T21:55:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1oe8hjh</id>
    <title>M5 iPad runs 8B-Q4 model.</title>
    <updated>2025-10-23T16:50:14+00:00</updated>
    <author>
      <name>/u/jarec707</name>
      <uri>https://old.reddit.com/user/jarec707</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oe8hjh/m5_ipad_runs_8bq4_model/"&gt; &lt;img alt="M5 iPad runs 8B-Q4 model." src="https://preview.redd.it/cq5w77gg5wwf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cb34d2ba73be617d6ecd7bfa852850ba436123a5" title="M5 iPad runs 8B-Q4 model." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Not too much of a surprise that the new M5 iPad (11&amp;quot; Base model with 12 GB of RAM) will run an 8B Q4 model. Please see the screenshot. I asked it to explain how to solve a Rubik's Cube, and it gave a decent answer and a respectable 23 tokens per second. The app I'm using is called Noema AI, and I like it a lot because you can have both a local model and an endpoint. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jarec707"&gt; /u/jarec707 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/cq5w77gg5wwf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oe8hjh/m5_ipad_runs_8bq4_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oe8hjh/m5_ipad_runs_8bq4_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-23T16:50:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1oe3wfs</id>
    <title>Virus Total integration on Hugging Face</title>
    <updated>2025-10-23T13:54:49+00:00</updated>
    <author>
      <name>/u/McPotates</name>
      <uri>https://old.reddit.com/user/McPotates</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oe3wfs/virus_total_integration_on_hugging_face/"&gt; &lt;img alt="Virus Total integration on Hugging Face" src="https://external-preview.redd.it/TwKiQ2XM7P28_0o53Sg5het24dh0s2bGVXdozQe9a5g.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=22b5835e6bc12ff826ab2939382c16ada8b641a6" title="Virus Total integration on Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey! We've just integrated Virus Total as security scanning partner. You should get a lot more AV scanners working on your files out of the box!&lt;br /&gt; Super happy to have them on board, curious to hear what yall think about this :)&lt;/p&gt; &lt;p&gt;FYI, we don't have all files scanned atm, should expand as more files are moved to xet (which gives us a sha256 out of the box, VT needs it to identify files).&lt;br /&gt; Also, only public files are scanned!&lt;/p&gt; &lt;p&gt;more info here: &lt;a href="https://huggingface.co/blog/virustotal"&gt;https://huggingface.co/blog/virustotal&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5r3o1tpq9vwf1.png?width=423&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=49b0cb3f1fc78589e0b8d36eaae8d773515e6101"&gt;https://preview.redd.it/5r3o1tpq9vwf1.png?width=423&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=49b0cb3f1fc78589e0b8d36eaae8d773515e6101&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/McPotates"&gt; /u/McPotates &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oe3wfs/virus_total_integration_on_hugging_face/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oe3wfs/virus_total_integration_on_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oe3wfs/virus_total_integration_on_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-23T13:54:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1odzuos</id>
    <title>ByteDance new release: Video-As-Prompt</title>
    <updated>2025-10-23T10:37:35+00:00</updated>
    <author>
      <name>/u/edward-dev</name>
      <uri>https://old.reddit.com/user/edward-dev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odzuos/bytedance_new_release_videoasprompt/"&gt; &lt;img alt="ByteDance new release: Video-As-Prompt" src="https://external-preview.redd.it/NmtjemdueXlhdXdmMYm3iTnseSQvWv7pLtSTSL9kyuPriWa9dnRnXyWhtUoO.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6d57110b1dddf99fa2c1932f5645e1206f259411" title="ByteDance new release: Video-As-Prompt" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Video-As-Prompt-Wan2.1-14B : &lt;a href="https://huggingface.co/ByteDance/Video-As-Prompt-Wan2.1-14B"&gt;HuggingFace link&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Video-As-Prompt-CogVideoX-5B : &lt;a href="https://huggingface.co/ByteDance/Video-As-Prompt-CogVideoX-5B"&gt;HuggingFace link&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Video-As-Prompt Core idea: Given a reference video with wanted semantics as a video prompt, Video-As-Prompt animate a reference image with the same semantics as the reference video. &lt;/p&gt; &lt;p&gt;Video-As-Prompt provides two variants, each with distinct trade-offs:&lt;/p&gt; &lt;p&gt;CogVideoX-I2V-5B Strengths: Fewer backbone parameters let us train more steps under limited resources, yielding strong stability on most semantic conditions. Limitations: Due to backbone ability limitation, it is weaker on human-centric generation and on concepts underrepresented in pretraining (e.g., ladudu, Squid Game, Minecraft).&lt;/p&gt; &lt;p&gt;Wan2.1-I2V-14B Strengths: Strong performance on human actions and novel concepts, thanks to a more capable base model. Limitations: Larger model size reduced feasible training steps given our resources, lowering stability on some semantic conditions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/edward-dev"&gt; /u/edward-dev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/rkbtr0wyauwf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odzuos/bytedance_new_release_videoasprompt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1odzuos/bytedance_new_release_videoasprompt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-23T10:37:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1oebo07</id>
    <title>Can Qwen3-VL count my push-ups? (Ronnie Coleman voice)</title>
    <updated>2025-10-23T18:50:17+00:00</updated>
    <author>
      <name>/u/Weary-Wing-6806</name>
      <uri>https://old.reddit.com/user/Weary-Wing-6806</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oebo07/can_qwen3vl_count_my_pushups_ronnie_coleman_voice/"&gt; &lt;img alt="Can Qwen3-VL count my push-ups? (Ronnie Coleman voice)" src="https://external-preview.redd.it/NDJxZTFsN3lwd3dmMSkQIYjP_oFpJvmih5U0oEGvnjDWhMxIFYeX2zHmhGBL.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e8f010389c86ee8911fc6841ab3d654b84ceda1a" title="Can Qwen3-VL count my push-ups? (Ronnie Coleman voice)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Wanted to see if Qwen3-VL could handle something simple: counting push-ups. If it can’t do that, it’s not ready to be a good trainer.&lt;/p&gt; &lt;p&gt;Overview:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Built on Gabber (will link repo)&lt;/li&gt; &lt;li&gt;Used Qwen3-VL for vision to tracks body position &amp;amp; reps&lt;/li&gt; &lt;li&gt;Cloned Ronnie Coleman’s voice for the trainer. That was… interesting.&lt;/li&gt; &lt;li&gt;Output = count my reps and gimme a “LIGHTWEIGHT BABY” every once in a while&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Results:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Took a lot of tweaking to get accurate rep counts&lt;/li&gt; &lt;li&gt;Some WEIRD voice hallucinations (Ronnie was going off lol)&lt;/li&gt; &lt;li&gt;Timing still a bit off between reps&lt;/li&gt; &lt;li&gt;Seems the model isn’t quite ready for useful real-time motion analysis or feedback, but it’s getting there&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Weary-Wing-6806"&gt; /u/Weary-Wing-6806 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/pfn5nm7ypwwf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oebo07/can_qwen3vl_count_my_pushups_ronnie_coleman_voice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oebo07/can_qwen3vl_count_my_pushups_ronnie_coleman_voice/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-23T18:50:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1oeg2g6</id>
    <title>AMD Officially Prices Radeon AI PRO R9700 At $1299 - 32GB VRAM - Launch Date Oct 27</title>
    <updated>2025-10-23T21:40:51+00:00</updated>
    <author>
      <name>/u/1ncehost</name>
      <uri>https://old.reddit.com/user/1ncehost</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oeg2g6/amd_officially_prices_radeon_ai_pro_r9700_at_1299/"&gt; &lt;img alt="AMD Officially Prices Radeon AI PRO R9700 At $1299 - 32GB VRAM - Launch Date Oct 27" src="https://external-preview.redd.it/1xRs-fKiMj0zISqC9vylqykmOso4ilpVoXcWW0J8xW4.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=661bfefccbe35bc33bf966b1c78cea33a1e763bd" title="AMD Officially Prices Radeon AI PRO R9700 At $1299 - 32GB VRAM - Launch Date Oct 27" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1ncehost"&gt; /u/1ncehost &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/amd-officially-launches-radeon-ai-pro-r9700-at-1299/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oeg2g6/amd_officially_prices_radeon_ai_pro_r9700_at_1299/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oeg2g6/amd_officially_prices_radeon_ai_pro_r9700_at_1299/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-23T21:40:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1oeep6p</id>
    <title>What LLM gave you your first "we have GPT-4 at home" moment?</title>
    <updated>2025-10-23T20:46:35+00:00</updated>
    <author>
      <name>/u/Klutzy-Snow8016</name>
      <uri>https://old.reddit.com/user/Klutzy-Snow8016</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For a long time, local models lagged ChatGPT 3.5 by a lot, and 4 was so far beyond that it felt hopeless. But now, you can run very good models at home.&lt;/p&gt; &lt;p&gt;So I'm curious, for your use-case, or just general usage, what was the point at which a model you ran locally finally caught up to what you saw from the paid models of 2023, or are you still waiting for that to happen?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Klutzy-Snow8016"&gt; /u/Klutzy-Snow8016 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oeep6p/what_llm_gave_you_your_first_we_have_gpt4_at_home/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oeep6p/what_llm_gave_you_your_first_we_have_gpt4_at_home/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oeep6p/what_llm_gave_you_your_first_we_have_gpt4_at_home/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-23T20:46:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1oe0y11</id>
    <title>I found a perfect coder model for my RTX4090+64GB RAM</title>
    <updated>2025-10-23T11:39:15+00:00</updated>
    <author>
      <name>/u/srigi</name>
      <uri>https://old.reddit.com/user/srigi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Disappointed with vanilla Qwen3-coder-30B-A3B, I browsed models at mradermacher. I had a good experience with YOYO models in the past. I stumbled upon &lt;strong&gt;mradermacher/Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III-i1-GGUF&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;First, I was a little worried that &lt;strong&gt;42B&lt;/strong&gt; won't fit, and offloading MoEs to CPU will result in poor perf. But thankfully, I was wrong.&lt;/p&gt; &lt;p&gt;Somehow this model consumed only about 8GB with &lt;code&gt;--cpu-moe&lt;/code&gt; (keep all Mixture of Experts weights on the CPU) and Q4_K_M, and 32k ctx. So I tuned llama.cpp invocation to fully occupy 24GB of RTX 4090 and put the rest into the CPU/RAM:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-server --model Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III.i1-Q4_K_M.gguf \ --ctx-size 102400 \ --flash-attn on \ --jinja \ --cache-type-k q8_0 \ --cache-type-v q8_0 \ --batch-size 1024 \ --ubatch-size 512 \ --n-cpu-moe 28 \ --n-gpu-layers 99 \ --repeat-last-n 192 \ --repeat-penalty 1.05 \ --threads 16 \ --host 0.0.0.0 \ --port 8080 \ --api-key secret &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;With these settings, it eats 23400MB of VRAM and 30GB of RAM. It processes the RooCode's system prompt (around 16k tokens) in around 10s and generates at 44tk/s. With 100k context window.&lt;/p&gt; &lt;p&gt;And the best thing - the RooCode tool-calling is very reliable (vanilla Qwen3-coder failed at this horribly). This model can really code and is fast on a single RTX 4090!&lt;/p&gt; &lt;p&gt;Here is a 1 minute demo of adding a small code-change to medium sized &lt;a href="https://github.com/srigi/type-graphql"&gt;code-base&lt;/a&gt;: &lt;a href="https://i.postimg.cc/cHp8sP9m/Screen-Flow.gif"&gt;https://i.postimg.cc/cHp8sP9m/Screen-Flow.gif&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/srigi"&gt; /u/srigi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oe0y11/i_found_a_perfect_coder_model_for_my_rtx409064gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oe0y11/i_found_a_perfect_coder_model_for_my_rtx409064gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oe0y11/i_found_a_perfect_coder_model_for_my_rtx409064gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-23T11:39:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1oe13rg</id>
    <title>Qwen3 outperforming bigger LLMs at trading</title>
    <updated>2025-10-23T11:47:37+00:00</updated>
    <author>
      <name>/u/Christosconst</name>
      <uri>https://old.reddit.com/user/Christosconst</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oe13rg/qwen3_outperforming_bigger_llms_at_trading/"&gt; &lt;img alt="Qwen3 outperforming bigger LLMs at trading" src="https://preview.redd.it/7i46ukqanuwf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2af3d3871761418ac44fa8e43516acb99b51653d" title="Qwen3 outperforming bigger LLMs at trading" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Christosconst"&gt; /u/Christosconst &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7i46ukqanuwf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oe13rg/qwen3_outperforming_bigger_llms_at_trading/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oe13rg/qwen3_outperforming_bigger_llms_at_trading/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-23T11:47:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1oefu29</id>
    <title>Cerebras REAP'd GLM4.6: 25%, 30%, 40% pruned FP8 checkpoints on HF!</title>
    <updated>2025-10-23T21:31:20+00:00</updated>
    <author>
      <name>/u/ilzrvch</name>
      <uri>https://old.reddit.com/user/ilzrvch</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oefu29/cerebras_reapd_glm46_25_30_40_pruned_fp8/"&gt; &lt;img alt="Cerebras REAP'd GLM4.6: 25%, 30%, 40% pruned FP8 checkpoints on HF!" src="https://external-preview.redd.it/AGpnB3Q_Xjisqwn0DU233BBKuTP9o7kSBGuVW7dOHBs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=74eb9943702ce83c73eb39485b3fe95bdff48313" title="Cerebras REAP'd GLM4.6: 25%, 30%, 40% pruned FP8 checkpoints on HF!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;We've gotten a ton of positive feedback on our &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1o98f57/new_from_cerebras_reap_the_experts_why_pruning/"&gt;previous&lt;/a&gt; &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1obrde8/cerebras_reap_update_pruned_checkpoints_for/"&gt;posts&lt;/a&gt; about our REAP pruned MoE models.&lt;/p&gt; &lt;p&gt;We've a got a new (highly requested!) update - REAP'd GLM4.6!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GLM4.6-FP8 REAP@25%:&lt;/strong&gt; &lt;a href="https://huggingface.co/cerebras/GLM-4.6-REAP-268B-A32B-FP8"&gt;https://huggingface.co/cerebras/GLM-4.6-REAP-268B-A32B-FP8&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;GLM4.6-FP8 REAP@30%:&lt;/strong&gt; &lt;a href="https://huggingface.co/cerebras/GLM-4.6-REAP-252B-A32B-FP8"&gt;https://huggingface.co/cerebras/GLM-4.6-REAP-252B-A32B-FP8&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;GLM4.6-FP8 REAP@40%:&lt;/strong&gt; &lt;a href="https://huggingface.co/cerebras/GLM-4.6-REAP-218B-A32B-FP8"&gt;https://huggingface.co/cerebras/GLM-4.6-REAP-218B-A32B-FP8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We're in the process of uploading the 16-bit versions for better-quality low-bit GGUF quants!&lt;/p&gt; &lt;p&gt;Stay tuned, we are updating our model collection: &lt;a href="https://huggingface.co/collections/cerebras/cerebras-reap"&gt;https://huggingface.co/collections/cerebras/cerebras-reap&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/gwuv3e9tjxwf1.png?width=1228&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=23ab8c8018762fc99dae789ad012ce0d3f7a8a6a"&gt;https://preview.redd.it/gwuv3e9tjxwf1.png?width=1228&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=23ab8c8018762fc99dae789ad012ce0d3f7a8a6a&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ilzrvch"&gt; /u/ilzrvch &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oefu29/cerebras_reapd_glm46_25_30_40_pruned_fp8/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oefu29/cerebras_reapd_glm46_25_30_40_pruned_fp8/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oefu29/cerebras_reapd_glm46_25_30_40_pruned_fp8/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-23T21:31:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1oe7orf</id>
    <title>State of Open OCR models</title>
    <updated>2025-10-23T16:19:39+00:00</updated>
    <author>
      <name>/u/unofficialmerve</name>
      <uri>https://old.reddit.com/user/unofficialmerve</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello folks! it's Merve from Hugging Face 🫡&lt;/p&gt; &lt;p&gt;You might have noticed there has been many open OCR models released lately 😄 they're cheap to run compared to closed ones, some even run on-device&lt;/p&gt; &lt;p&gt;But it's hard to compare them and have a guideline on picking among upcoming ones, so we have broken it down for you in a blog:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;how to evaluate and pick an OCR model,&lt;/li&gt; &lt;li&gt;a comparison of the latest open-source models,&lt;/li&gt; &lt;li&gt;deployment tips,&lt;/li&gt; &lt;li&gt;and what’s next beyond basic OCR &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We hope it's useful for you! Let us know what you think: &lt;a href="https://huggingface.co/blog/ocr-open-models"&gt;https://huggingface.co/blog/ocr-open-models&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/unofficialmerve"&gt; /u/unofficialmerve &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oe7orf/state_of_open_ocr_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oe7orf/state_of_open_ocr_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oe7orf/state_of_open_ocr_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-23T16:19:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1oee1ie</id>
    <title>I spent months struggling to understand AI agents. Built a from scratch tutorial so you don't have to.</title>
    <updated>2025-10-23T20:21:05+00:00</updated>
    <author>
      <name>/u/purellmagents</name>
      <uri>https://old.reddit.com/user/purellmagents</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For the longest time, I felt lost trying to understand how AI agents actually work.&lt;/p&gt; &lt;p&gt;Every tutorial I found jumped straight into LangChain or CrewAI. The papers were full of architecture diagrams but vague about implementation. I'd follow along, copy-paste code, and it would work... but I had no idea why.&lt;/p&gt; &lt;p&gt;The breaking point: I couldn't debug anything. When something broke, I had no mental model of what was happening under the hood. Was it the framework? The prompt? The model? No clue.&lt;/p&gt; &lt;p&gt;So I did what probably seems obvious in hindsight: I started building from scratch.&lt;/p&gt; &lt;p&gt;Just me, node-llama-cpp, and a lot of trial and error. No frameworks. No abstractions I didn't understand. Just pure fundamentals.&lt;/p&gt; &lt;p&gt;After months of reading, experimenting, and honestly struggling through a lot of confusion, things finally clicked. I understood what function calling really is. Why ReAct patterns work. How memory actually gets managed. What frameworks are actually doing behind their nice APIs.&lt;/p&gt; &lt;p&gt;I put together everything I learned here: &lt;a href="https://github.com/pguso/ai-agents-from-scratch"&gt;https://github.com/pguso/ai-agents-from-scratch&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It's 8 progressive examples, from &amp;quot;Hello World&amp;quot; to full ReAct agents: - Plain JavaScript, no frameworks - Local LLMs only (Qwen, Llama, whatever you have) - Each example has detailed code breakdowns + concept explanations - Builds from basics to real agent patterns&lt;/p&gt; &lt;p&gt;Topics covered: - System prompts &amp;amp; specialization - Streaming &amp;amp; token control&lt;br /&gt; - Function calling (the &amp;quot;aha!&amp;quot; moment) - Memory systems (very basic) - ReAct pattern (Reasoning + Acting) - Parallel processing&lt;/p&gt; &lt;p&gt;Do you miss something?&lt;/p&gt; &lt;p&gt;Who this is for: - You want to understand agents deeply, not just use them - You're tired of framework black boxes - You learn by building - You want to know what LangChain is doing under the hood&lt;/p&gt; &lt;p&gt;What you'll need: - Node.js - A local GGUF model (I use Qwen 1.7B, runs on modest hardware) instructions in the repo for downloading - Curiosity and patience&lt;/p&gt; &lt;p&gt;I wish I had this resource when I started. Would've saved me months of confusion. Hope it helps someone else on the same journey.&lt;/p&gt; &lt;p&gt;Happy to answer questions about any of the patterns or concepts!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/purellmagents"&gt; /u/purellmagents &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oee1ie/i_spent_months_struggling_to_understand_ai_agents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oee1ie/i_spent_months_struggling_to_understand_ai_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oee1ie/i_spent_months_struggling_to_understand_ai_agents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-23T20:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1obqkpe</id>
    <title>Best Local LLMs - October 2025</title>
    <updated>2025-10-20T19:06:06+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Welcome to the first monthly &amp;quot;Best Local LLMs&amp;quot; post!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Share what your favorite models are right now &lt;strong&gt;and why.&lt;/strong&gt; Given the nature of the beast in evaluating LLMs (untrustworthiness of benchmarks, immature tooling, intrinsic stochasticity), please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Applications&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;General&lt;/li&gt; &lt;li&gt;Agentic/Tool Use&lt;/li&gt; &lt;li&gt;Coding&lt;/li&gt; &lt;li&gt;Creative Writing/RP&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;(look for the top level comments for each Application and please thread your responses under that)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obqkpe/best_local_llms_october_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obqkpe/best_local_llms_october_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1obqkpe/best_local_llms_october_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T19:06:06+00:00</published>
  </entry>
</feed>
