<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-04T11:23:58+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1oo2ua8</id>
    <title>llama.cpp vulkan build is being ignored</title>
    <updated>2025-11-04T09:16:33+00:00</updated>
    <author>
      <name>/u/AhmadXVX15</name>
      <uri>https://old.reddit.com/user/AhmadXVX15</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;iam trying to make AI model run through my gpu, but all the python files in the project is failing to, even that llama.cpp is in the project.&lt;br /&gt; how do i check that llama.cpp is working? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AhmadXVX15"&gt; /u/AhmadXVX15 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo2ua8/llamacpp_vulkan_build_is_being_ignored/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo2ua8/llamacpp_vulkan_build_is_being_ignored/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oo2ua8/llamacpp_vulkan_build_is_being_ignored/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T09:16:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1omyytq</id>
    <title>Reporter: ‚ÄúPOLISH: THE SUPREME LANGUAGE OF AI.‚Äù</title>
    <updated>2025-11-03T01:31:02+00:00</updated>
    <author>
      <name>/u/Mindless_Pain1860</name>
      <uri>https://old.reddit.com/user/Mindless_Pain1860</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omyytq/reporter_polish_the_supreme_language_of_ai/"&gt; &lt;img alt="Reporter: ‚ÄúPOLISH: THE SUPREME LANGUAGE OF AI.‚Äù" src="https://preview.redd.it/jlwd6xkh3yyf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7ba1b50f272c2870a74364026d750bd194a9f243" title="Reporter: ‚ÄúPOLISH: THE SUPREME LANGUAGE OF AI.‚Äù" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Please read the paper before making any comments.&lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2503.01996"&gt;https://arxiv.org/pdf/2503.01996&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mindless_Pain1860"&gt; /u/Mindless_Pain1860 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jlwd6xkh3yyf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omyytq/reporter_polish_the_supreme_language_of_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1omyytq/reporter_polish_the_supreme_language_of_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-03T01:31:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1on8kye</id>
    <title>MiniMax LLM head confirms: new model M2.1 coming soon</title>
    <updated>2025-11-03T10:48:13+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1on8kye/minimax_llm_head_confirms_new_model_m21_coming/"&gt; &lt;img alt="MiniMax LLM head confirms: new model M2.1 coming soon" src="https://b.thumbs.redditmedia.com/zeE60rAYP1oJJCxWUZL0idGJf1QomfF834XO5Og7MvQ.jpg" title="MiniMax LLM head confirms: new model M2.1 coming soon" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Pengyu Zhao, head of MiniMax LLM, said that to achieve the vision of &amp;quot;Intelligence with Everyone,&amp;quot; the company will continue open-sourcing its models to promote the ongoing development of the AI community. As part of the plan, he confirmed that the new model M2.1 will be released soon.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/4tscghepu0zf1.jpg?width=1293&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=f9636c4ecf40f3f278afca1a3391a3178bb32f88"&gt;https://preview.redd.it/4tscghepu0zf1.jpg?width=1293&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=f9636c4ecf40f3f278afca1a3391a3178bb32f88&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In social media interactions, when asked about the launch date of the subscription plan, Pengyu Zhao replied &amp;quot;very soon,&amp;quot; specifying it would be within one to two weeks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1on8kye/minimax_llm_head_confirms_new_model_m21_coming/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1on8kye/minimax_llm_head_confirms_new_model_m21_coming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1on8kye/minimax_llm_head_confirms_new_model_m21_coming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-03T10:48:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1oo3pdz</id>
    <title>Workaround for VRAM unloading after idle period using Vulkan runtime on multi-gpu setup</title>
    <updated>2025-11-04T10:13:06+00:00</updated>
    <author>
      <name>/u/Rombodawg</name>
      <uri>https://old.reddit.com/user/Rombodawg</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So alot of people have been experiencing an issue (Especially in AI) where their vram will unload completely onto system ram after an Idle period especially when using multi-gpu setups.&lt;/p&gt; &lt;p&gt;Ive created a temporary solution until the issue gets fixed.&lt;/p&gt; &lt;p&gt;My code loads 1mb onto the vram and keeps it and the gpu core &amp;quot;Awake&amp;quot; by pinging it every 1 second. This doesnt use any visible recourses on the core or memory but will keep it from unloading the VRAM onto system RAM&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/rombodawg/GPU_Core-Memory_Never_Idle_or_Sleep"&gt;https://github.com/rombodawg/GPU_Core-Memory_Never_Idle_or_Sleep&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rombodawg"&gt; /u/Rombodawg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo3pdz/workaround_for_vram_unloading_after_idle_period/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo3pdz/workaround_for_vram_unloading_after_idle_period/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oo3pdz/workaround_for_vram_unloading_after_idle_period/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T10:13:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1oo3s6f</id>
    <title>How do you handle local AI model performance across different hardware?</title>
    <updated>2025-11-04T10:17:54+00:00</updated>
    <author>
      <name>/u/elinaembedl</name>
      <uri>https://old.reddit.com/user/elinaembedl</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently asked a question about why you think more apps don‚Äôt run AI locally, and I received a lot of interesting answers.&lt;/p&gt; &lt;p&gt;Now I have a follow up question. For those of you who have managed to built apps that include AI models that run on-device, how do you handle the issue of models performing differently across different CPUs, GPUs, and NPUs?&lt;/p&gt; &lt;p&gt;Do you usually deploy the same model across all devices? If so, how do you make it perform well on different accelerators and devices? Or do you switch models between devices to get better performance for each one? How do you decide which model works best for each type of device?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/elinaembedl"&gt; /u/elinaembedl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo3s6f/how_do_you_handle_local_ai_model_performance/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo3s6f/how_do_you_handle_local_ai_model_performance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oo3s6f/how_do_you_handle_local_ai_model_performance/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T10:17:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1oo461u</id>
    <title>Ideal size of llm to make</title>
    <updated>2025-11-04T10:41:35+00:00</updated>
    <author>
      <name>/u/MoreIndependent5967</name>
      <uri>https://old.reddit.com/user/MoreIndependent5967</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I think the ideal size of llm moe would be 30b to 1.5b for pc and 10b to 0.5b for smartphone. &lt;/p&gt; &lt;p&gt;PCs go to 32 GB of RAM and smartphones to 12 to 16 GB of RAM &lt;/p&gt; &lt;p&gt;And therefore the ideal would be 5% of active parameter for efficiency (comparable to the human brain) And I don't think everyone has or will be able to afford a 600 watt 5090 to run local llms. &lt;/p&gt; &lt;p&gt;So 30b to 3b q4km -= 19gb for pc And 10b a0.5b q4 km = 7gb for smartphone &lt;/p&gt; &lt;p&gt;The llm industry like mistral should focus on that!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MoreIndependent5967"&gt; /u/MoreIndependent5967 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo461u/ideal_size_of_llm_to_make/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo461u/ideal_size_of_llm_to_make/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oo461u/ideal_size_of_llm_to_make/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T10:41:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1onfjk6</id>
    <title>multi-model coding agents hitting 76% on swe-bench. could we replicate this with local models?</title>
    <updated>2025-11-03T15:58:23+00:00</updated>
    <author>
      <name>/u/rwhitman05</name>
      <uri>https://old.reddit.com/user/rwhitman05</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;saw some benchmark results where a coding agent hit 76.1% on swe-bench verified using multi-model approach&lt;/p&gt; &lt;p&gt;the interesting part: different models for different tasks. one for navigation, one for coding, one for review. plus auto-verification loop&lt;/p&gt; &lt;p&gt;got me thinking - could we build something similar with local models? or are we not there yet?&lt;/p&gt; &lt;p&gt;different models have different strengths right. some are better at &amp;quot;find this function across 50k lines&amp;quot; vs &amp;quot;write this specific function&amp;quot;&lt;/p&gt; &lt;p&gt;like if youre fixing a bug that touches multiple files, one model finds all references, another writes the fix, then checks for side effects. makes sense to use specialized models instead of one doing everything&lt;/p&gt; &lt;p&gt;auto-verification is interesting. writes code, runs tests, fails, fixes bug, runs tests again. repeat until pass. basically automates the debug cycle&lt;/p&gt; &lt;p&gt;so could this work locally? thinking qwen2.5-coder for coding, deepseek for navigation, maybe another for review. orchestration with langchain or custom code. verification is just pytest/eslint running automatically&lt;/p&gt; &lt;p&gt;main challenges would be context management across models, when to switch models, keeping them in sync. not sure how hard that is&lt;/p&gt; &lt;p&gt;that benchmark used thinking tokens which helped (+0.7% improvement to 76.1%)&lt;/p&gt; &lt;p&gt;wondering if local models could get to 60-70% with similar architecture. would still be super useful. plus you get privacy and no api costs&lt;/p&gt; &lt;p&gt;has anyone tried multi-model orchestration locally? what models would you use? qwen? deepseek? llama? how would you handle orchestration?&lt;/p&gt; &lt;p&gt;saw some commercial tools doing this now (verdent got that 76% score, aider with different models, cursor's multi-model thing) but wondering if we can build it ourselves with local models&lt;/p&gt; &lt;p&gt;or is this just not feasible yet. would love to hear from anyone whos experimented with this&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rwhitman05"&gt; /u/rwhitman05 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onfjk6/multimodel_coding_agents_hitting_76_on_swebench/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onfjk6/multimodel_coding_agents_hitting_76_on_swebench/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1onfjk6/multimodel_coding_agents_hitting_76_on_swebench/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-03T15:58:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1onu74b</id>
    <title>Agent Flow</title>
    <updated>2025-11-04T01:17:52+00:00</updated>
    <author>
      <name>/u/Loud_Communication68</name>
      <uri>https://old.reddit.com/user/Loud_Communication68</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anybody tried Agent Flow? Seems 200b performance from an 8b model feels like the holy grail of local llm.&lt;/p&gt; &lt;p&gt;&lt;a href="https://agentflow.stanford.edu/"&gt;https://agentflow.stanford.edu/&lt;/a&gt; &lt;a href="https://huggingface.co/spaces/AgentFlow/agentflow"&gt;https://huggingface.co/spaces/AgentFlow/agentflow&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Loud_Communication68"&gt; /u/Loud_Communication68 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onu74b/agent_flow/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onu74b/agent_flow/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1onu74b/agent_flow/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T01:17:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1onxd65</id>
    <title>This might be a dumb question but can VRAM and Unified memory work together on those AMD NPUs?</title>
    <updated>2025-11-04T03:47:07+00:00</updated>
    <author>
      <name>/u/NoFudge4700</name>
      <uri>https://old.reddit.com/user/NoFudge4700</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Can one put in a graphics card along? Or attach externally? Because 128 GB of unified memory is not enough. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NoFudge4700"&gt; /u/NoFudge4700 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onxd65/this_might_be_a_dumb_question_but_can_vram_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onxd65/this_might_be_a_dumb_question_but_can_vram_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1onxd65/this_might_be_a_dumb_question_but_can_vram_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T03:47:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1onobpg</id>
    <title>Last week in Multimodal AI - Local Edition</title>
    <updated>2025-11-03T21:17:10+00:00</updated>
    <author>
      <name>/u/Vast_Yak_4147</name>
      <uri>https://old.reddit.com/user/Vast_Yak_4147</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onobpg/last_week_in_multimodal_ai_local_edition/"&gt; &lt;img alt="Last week in Multimodal AI - Local Edition" src="https://b.thumbs.redditmedia.com/aLTkUql_MhTgqwbeVrNzD0Z5C8Ybd58BW69RF7bMFBc.jpg" title="Last week in Multimodal AI - Local Edition" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I curate a weekly newsletter on multimodal AI. Here are the local/edge highlights from last week:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Emu3.5 - Open-Source World Learner&lt;/strong&gt;&lt;br /&gt; ‚Ä¢ Matches Gemini 2.5 Flash performance while running entirely on your hardware.&lt;br /&gt; ‚Ä¢ Native next-state prediction across text, images, and video for embodied tasks.&lt;br /&gt; ‚Ä¢ &lt;a href="https://arxiv.org/pdf/2510.26583"&gt;Paper&lt;/a&gt; | &lt;a href="https://emu.world/pages/web/landingPage"&gt;Project Page&lt;/a&gt; | &lt;a href="https://huggingface.co/BAAI/Emu3.5"&gt;Hugging Face&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1onobpg/video/n6d1ekmty3zf1/player"&gt;https://reddit.com/link/1onobpg/video/n6d1ekmty3zf1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;NVIDIA Surgical Qwen2.5-VL&lt;/strong&gt;&lt;br /&gt; ‚Ä¢ 7B fine-tuned model for surgical video understanding, runs locally.&lt;br /&gt; ‚Ä¢ Real-time surgical assistance without cloud dependencies.&lt;br /&gt; ‚Ä¢ &lt;a href="https://huggingface.co/nvidia/Qwen2.5-VL-7B-Surg-CholecT50"&gt;Hugging Face&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;NVIDIA ChronoEdit - Physics-Aware Editing&lt;/strong&gt;&lt;br /&gt; ‚Ä¢ 14B model for temporal image editing with physics simulation.&lt;br /&gt; ‚Ä¢ Runs on consumer GPUs for realistic local image manipulation.&lt;br /&gt; ‚Ä¢ &lt;a href="https://huggingface.co/nvidia/ChronoEdit-14B-Diffusers"&gt;Hugging Face&lt;/a&gt; | &lt;a href="https://arxiv.org/abs/2510.04290"&gt;Paper&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Wan2GP - Video Generation for GPU Poor&lt;/strong&gt;&lt;br /&gt; ‚Ä¢ Fast video generation optimized for regular consumer GPUs.&lt;br /&gt; ‚Ä¢ Makes video synthesis accessible without high-end hardware.&lt;br /&gt; ‚Ä¢ &lt;a href="https://github.com/deepbeepmeep/Wan2GP/"&gt;GitHub&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/smjap08zy3zf1.png?width=1895&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a52b0646bf062aaad45d704a28e9516c4da52d9c"&gt;https://preview.redd.it/smjap08zy3zf1.png?width=1895&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a52b0646bf062aaad45d704a28e9516c4da52d9c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;LongCat-Flash-Omni&lt;/strong&gt;&lt;br /&gt; ‚Ä¢ 560B-parameter MoE model for real-time audio-visual interaction.&lt;br /&gt; ‚Ä¢ Efficient mixture-of-experts design for local deployment.&lt;br /&gt; ‚Ä¢ &lt;a href="https://github.com/meituan-longcat/LongCat-Flash-Omni"&gt;GitHub&lt;/a&gt; | &lt;a href="https://longcat.chat/"&gt;Project Page&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Ming-flash-omni Preview&lt;/strong&gt;&lt;br /&gt; ‚Ä¢ AntGroup's new multimodal foundation model optimized for edge deployment.&lt;br /&gt; ‚Ä¢ Handles text, vision, and audio tasks locally.&lt;br /&gt; ‚Ä¢ &lt;a href="https://huggingface.co/inclusionAI/Ming-flash-omni-Preview"&gt;Hugging Face&lt;/a&gt; | &lt;a href="https://arxiv.org/abs/2510.24821"&gt;Paper&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Checkout the &lt;a href="https://open.substack.com/pub/thelivingedge/p/multimodal-monday-31-visual-thinking?r=12l7fk&amp;amp;utm_campaign=post&amp;amp;utm_medium=web&amp;amp;showWelcomeOnShare=false"&gt;full newsletter&lt;/a&gt; for more demos, papers, and resources.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vast_Yak_4147"&gt; /u/Vast_Yak_4147 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onobpg/last_week_in_multimodal_ai_local_edition/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onobpg/last_week_in_multimodal_ai_local_edition/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1onobpg/last_week_in_multimodal_ai_local_edition/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-03T21:17:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1onaops</id>
    <title>‚ö°Ô∏è Scaling Coding-Agent RL to 32x H100s. Achieving 160% improvement on Stanford's TerminalBench</title>
    <updated>2025-11-03T12:41:41+00:00</updated>
    <author>
      <name>/u/DanAiTuning</name>
      <uri>https://old.reddit.com/user/DanAiTuning</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onaops/scaling_codingagent_rl_to_32x_h100s_achieving_160/"&gt; &lt;img alt="‚ö°Ô∏è Scaling Coding-Agent RL to 32x H100s. Achieving 160% improvement on Stanford's TerminalBench" src="https://b.thumbs.redditmedia.com/jSQzd0HL5GuJP640MvEdr9aB6Jcc16z589SDLzlVOhs.jpg" title="‚ö°Ô∏è Scaling Coding-Agent RL to 32x H100s. Achieving 160% improvement on Stanford's TerminalBench" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;üëã Trekking along the forefront of applied AI is rocky territory, but it is the best place to be! My RL trained multi-agent-coding model Orca-Agent-v0.1 reached a 160% higher relative score than its base model on Stanford's TerminalBench. Which is cool! The trek across RL was at times painful, and at other times slightly less painful üòÖ I've open sourced everything.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What I did:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I trained a 14B orchestrator model to better coordinate explorer &amp;amp; coder subagents (subagents are tool calls for orchestrator)&lt;/li&gt; &lt;li&gt;Scaled to 32x H100s that were pushed to their limits across 4 bare-metal nodes&lt;/li&gt; &lt;li&gt;Scaled to 256 Docker environments rolling out simultaneously, automatically distributed across the cluster&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Key results:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Qwen3-14B jumped from &lt;strong&gt;7% ‚Üí 18.25%&lt;/strong&gt; on TerminalBench after training&lt;/li&gt; &lt;li&gt;Model now within striking distance of Qwen3-Coder-480B (19.7%)&lt;/li&gt; &lt;li&gt;Training was stable with smooth entropy decrease and healthy gradient norms&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Key learnings:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&amp;quot;Intelligently crafted&amp;quot; reward functions pale in performance to simple unit tests. Keep it simple!&lt;/li&gt; &lt;li&gt;RL is not a quick fix for improving agent performance. It is still very much in the early research phase, and in most cases prompt engineering with the latest SOTA is likely the way to go.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Training approach:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Reward design and biggest learning: Kept it simple - **just unit tests**. Every &amp;quot;smart&amp;quot; reward signal I tried to craft led to policy collapse üòÖ&lt;/p&gt; &lt;p&gt;Curriculum learning:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Stage-1: Tasks where base model succeeded 1-2/3 times (41 tasks)&lt;/li&gt; &lt;li&gt;Stage-2: Tasks where Stage-1 model succeeded 1-4/5 times&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Dataset: Used synthetically generated RL environments and unit tests&lt;/p&gt; &lt;p&gt;&lt;strong&gt;More details:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I have added lots more details in the repo:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;‚≠êÔ∏è&lt;/strong&gt; &lt;a href="https://github.com/Danau5tin/Orca-Agent-RL"&gt;&lt;strong&gt;Orca-Agent-RL repo&lt;/strong&gt;&lt;/a&gt; - training code, model weights, datasets.&lt;/p&gt; &lt;p&gt;Huge thanks to:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Taras for providing the compute and believing in open source&lt;/li&gt; &lt;li&gt;Prime Intellect team for building prime-rl and dealing with my endless questions üòÖ&lt;/li&gt; &lt;li&gt;Alex Dimakis for the conversation that sparked training the orchestrator model&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I am sharing this because I believe agentic AI is going to change everybody's lives, and so I feel it is important (and super fun!) for us all to share knowledge around this area, and also have enjoy exploring what is possible.&lt;/p&gt; &lt;p&gt;Thanks for reading!&lt;/p&gt; &lt;p&gt;Dan&lt;/p&gt; &lt;p&gt;(Evaluated on the excellent TerminalBench benchmark by Stanford &amp;amp; Laude Institute)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DanAiTuning"&gt; /u/DanAiTuning &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1onaops"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onaops/scaling_codingagent_rl_to_32x_h100s_achieving_160/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1onaops/scaling_codingagent_rl_to_32x_h100s_achieving_160/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-03T12:41:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1onhdob</id>
    <title>How does cerebras get 2000toks/s?</title>
    <updated>2025-11-03T17:05:10+00:00</updated>
    <author>
      <name>/u/npmbad</name>
      <uri>https://old.reddit.com/user/npmbad</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm wondering, what sort of GPU do I need to rent and under what settings to get that speed? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/npmbad"&gt; /u/npmbad &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onhdob/how_does_cerebras_get_2000tokss/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onhdob/how_does_cerebras_get_2000tokss/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1onhdob/how_does_cerebras_get_2000tokss/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-03T17:05:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1on628o</id>
    <title>Google pulls Gemma from AI Studio after Senator Blackburn accuses model of defamation</title>
    <updated>2025-11-03T08:04:32+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1on628o/google_pulls_gemma_from_ai_studio_after_senator/"&gt; &lt;img alt="Google pulls Gemma from AI Studio after Senator Blackburn accuses model of defamation" src="https://b.thumbs.redditmedia.com/otiqqwWrYAPyBQSbvIHZ-yCKbfdiJZGic1vlE0jFFxk.jpg" title="Google pulls Gemma from AI Studio after Senator Blackburn accuses model of defamation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/0hnvozwh10zf1.png?width=1198&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ab171458093a1ad5f07a0eaa42ac44e2c5ab5681"&gt;Google Official Statement&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://techcrunch.com/2025/11/02/google-pulls-gemma-from-ai-studio-after-senator-blackburn-accuses-model-of-defamation/"&gt;Source&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Fortunately, we can still download the weights from HF and run them locally.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1on628o/google_pulls_gemma_from_ai_studio_after_senator/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1on628o/google_pulls_gemma_from_ai_studio_after_senator/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1on628o/google_pulls_gemma_from_ai_studio_after_senator/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-03T08:04:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1oo4lhs</id>
    <title>What's the biggest most common PROBLEM you have in your personal ML/AI side projects?</title>
    <updated>2025-11-04T11:07:02+00:00</updated>
    <author>
      <name>/u/HectorAlcazar11</name>
      <uri>https://old.reddit.com/user/HectorAlcazar11</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey there, I'm currently trying to start my first SaaS and I'm searching for a genuinly painful problem to create a solution. Need your help. Got a quick minute to help me?&lt;br /&gt; I'm specifically interested in things that are taking your time, money, or effort. Would be great if you tell me the story.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HectorAlcazar11"&gt; /u/HectorAlcazar11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo4lhs/whats_the_biggest_most_common_problem_you_have_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo4lhs/whats_the_biggest_most_common_problem_you_have_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oo4lhs/whats_the_biggest_most_common_problem_you_have_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T11:07:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1oo4kh7</id>
    <title>Finetuning DeepSeek 671B locally with only 80GB VRAM and Server CPU</title>
    <updated>2025-11-04T11:05:26+00:00</updated>
    <author>
      <name>/u/CombinationNo780</name>
      <uri>https://old.reddit.com/user/CombinationNo780</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo4kh7/finetuning_deepseek_671b_locally_with_only_80gb/"&gt; &lt;img alt="Finetuning DeepSeek 671B locally with only 80GB VRAM and Server CPU" src="https://b.thumbs.redditmedia.com/nnPDB-YtTPCthxTtyPVParTYcBmaEE0gf_kWM64QSso.jpg" title="Finetuning DeepSeek 671B locally with only 80GB VRAM and Server CPU" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, we're the KTransformers team (formerly known for our DeepSeek-V3 local CPU/GPU hybrid inference project).&lt;/p&gt; &lt;p&gt;Today, we're proud to announce full integration with LLaMA-Factory, enabling you to &lt;strong&gt;fine-tune DeepSeek-671B or Kimi-K2-1TB locally with just 4x RTX 4090 GPUs&lt;/strong&gt;!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/dlipq1us28zf1.png?width=2332&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=92fad09b19f37c76c3f08fe9e326816ad4d533d1"&gt;https://preview.redd.it/dlipq1us28zf1.png?width=2332&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=92fad09b19f37c76c3f08fe9e326816ad4d533d1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/finetuning-deepseek-671b-locally-with-only-80gb-vram-and-v0-24938oydy7zf1.png?width=2246&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=216765e8119e54cc2bdc92bf24b082575f7d1bdc"&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/finetuning-deepseek-671b-locally-with-only-80gb-vram-and-v0-w1m1j89jy7zf1.png?width=2570&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0bde4b33c857b8fd4c1f4d8c0c4ecc42763f5bbc"&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;More infomation can be found at&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/kvcache-ai/ktransformers/tree/main/KT-SFT"&gt;https://github.com/kvcache-ai/ktransformers/tree/main/KT-SFT&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CombinationNo780"&gt; /u/CombinationNo780 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo4kh7/finetuning_deepseek_671b_locally_with_only_80gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo4kh7/finetuning_deepseek_671b_locally_with_only_80gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oo4kh7/finetuning_deepseek_671b_locally_with_only_80gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T11:05:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1oo2olu</id>
    <title>You can win one DGX Station from Dell</title>
    <updated>2025-11-04T09:05:26+00:00</updated>
    <author>
      <name>/u/Cane_P</name>
      <uri>https://old.reddit.com/user/Cane_P</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo2olu/you_can_win_one_dgx_station_from_dell/"&gt; &lt;img alt="You can win one DGX Station from Dell" src="https://preview.redd.it/h8m5jkpgh7zf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d20a7b9a2750d2bf30b8722bcf1229c20d13e6d3" title="You can win one DGX Station from Dell" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cane_P"&gt; /u/Cane_P &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/h8m5jkpgh7zf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo2olu/you_can_win_one_dgx_station_from_dell/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oo2olu/you_can_win_one_dgx_station_from_dell/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T09:05:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1oo4h0q</id>
    <title>Are 32k-Token Embedding Models Real Innovation or Just Marketing?</title>
    <updated>2025-11-04T11:00:08+00:00</updated>
    <author>
      <name>/u/CapitalShake3085</name>
      <uri>https://old.reddit.com/user/CapitalShake3085</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;What do you think about embedding models that support input context lengths of up to 32k tokens?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;For example, Voyage 3 or Voyage 3.5 (from MongoDB).&lt;/p&gt; &lt;p&gt;Is it just marketing, or does it make a real difference in practice?&lt;/p&gt; &lt;p&gt;Also, which closed-source embedding model would you recommend for top-tier performance?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CapitalShake3085"&gt; /u/CapitalShake3085 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo4h0q/are_32ktoken_embedding_models_real_innovation_or/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo4h0q/are_32ktoken_embedding_models_real_innovation_or/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oo4h0q/are_32ktoken_embedding_models_real_innovation_or/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T11:00:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1onxdqx</id>
    <title>GLM-4.5-Air-REAP-82B-A12B-LIMI</title>
    <updated>2025-11-04T03:47:57+00:00</updated>
    <author>
      <name>/u/CoruNethronX</name>
      <uri>https://old.reddit.com/user/CoruNethronX</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi. I'm in search of a HW grant to make this model a reality. Plan is to fine-tune cerebras/GLM-4.5-Air-REAP-82B-A12B model using GAIR/LIMI dataset. As per arXiv:2509.17567 , we could expect great gain of agentic model abilities. Script can be easily adapted from github.com/GAIR-NLP/LIMI as authors were initially fine-tuned a full GLM4.5 Air 106B model. I would expect the whole process to require about 12 hour on 8xH100 or equivalent H200 or B200 cluster. As a result I'll publish a trained 82B model with (hopefully) increased agentic abilities, a transparent evaluation report and also GGUF and MLX quants under permissive license. I expect 82B q4 quants to behave better than any 106B q3 quants on e.g. 64Gb apple HW. If you're able to provide temporary ssh acess to abovementioned GPU cluster, please contact me and let's do this.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CoruNethronX"&gt; /u/CoruNethronX &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onxdqx/glm45airreap82ba12blimi/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onxdqx/glm45airreap82ba12blimi/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1onxdqx/glm45airreap82ba12blimi/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T03:47:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1oo279x</id>
    <title>[Research] LLM judges systematically penalize balanced reasoning - tested mistral, llama3, gemma, phi3, orca-mini</title>
    <updated>2025-11-04T08:33:12+00:00</updated>
    <author>
      <name>/u/Budget-Reception-533</name>
      <uri>https://old.reddit.com/user/Budget-Reception-533</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just published a study on LLM judge bias using 5 local models, and the results are pretty interesting for anyone using LLMs as evaluators.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Paper + full data&lt;/strong&gt;: &lt;a href="https://zenodo.org/records/17517864"&gt;https://zenodo.org/records/17517864&lt;/a&gt; (DOI: 10.5281/zenodo.17517864)&lt;/p&gt; &lt;h2&gt;Setup&lt;/h2&gt; &lt;p&gt;Tested these models via Ollama: - mistral:7b-instruct - llama3:8b - gemma:2b-instruct&lt;br /&gt; - phi3:mini - orca-mini:7b&lt;/p&gt; &lt;p&gt;Generated 1,500 responses across 30 moral dilemmas with: - 3 prompt framings (neutral, safety-first, freedom-first) - 10 temperatures (0.0 to 1.0) - Deterministic seeds for full reproducibility&lt;/p&gt; &lt;p&gt;Then had GPT-4o-mini and Claude 3.5 Haiku evaluate each response (3,000 total evaluations).&lt;/p&gt; &lt;h2&gt;Key Finding: The &amp;quot;Balance Penalty&amp;quot;&lt;/h2&gt; &lt;p&gt;&lt;strong&gt;Judges systematically penalize balanced responses.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;When a model says &amp;quot;both values matter, it depends on context&amp;quot; ‚Üí mean score 3.60&lt;/p&gt; &lt;p&gt;When a model picks one value decisively ‚Üí mean score 4.36&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Gap: 0.76 points (p&amp;lt;0.001, Cohen's d=1.45)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This holds after controlling for: - Which model generated the response - Temperature setting - Prompt framing - Scenario difficulty&lt;/p&gt; &lt;h2&gt;Why This Matters for Local LLM Users&lt;/h2&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;If you're using LLM judges for eval&lt;/strong&gt;, they're probably penalizing nuanced reasoning&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Judge disagreement concentrates on balanced responses&lt;/strong&gt;: When responses acknowledge trade-offs, judges disagree 58% of the time vs 34% for decisive responses&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;GPT-4o-mini judges more harshly than Claude 3.5 Haiku&lt;/strong&gt;: GPT penalty is Œ≤=1.08 (d=2.21), Claude is Œ≤=0.53 (d=1.00)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Framing matters WAY more than temperature&lt;/strong&gt;: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Framing effect: 0.4-0.8 points&lt;/li&gt; &lt;li&gt;Temperature effect: 0.15-0.24 points&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you're tweaking temperature for &amp;quot;better&amp;quot; outputs, you're probably wasting time. Focus on prompt framing instead.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;h2&gt;Model Rankings (All 5 Performed Similarly)&lt;/h2&gt; &lt;p&gt;Mean alignment scores across all judges/scenarios: - orca-mini:7b: 4.31 - llama3:8b: 4.24 - phi3:mini: 4.23 - mistral:7b-instruct: 4.07 - gemma:2b-instruct: 4.05&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The differences between models are smaller than the balance penalty effect&lt;/strong&gt;, suggesting judge bias matters more than model choice for these evaluations.&lt;/p&gt; &lt;h2&gt;Full Reproducibility&lt;/h2&gt; &lt;p&gt;Everything's public on Zenodo: - 1,500 response files (JSONL with full metadata) - 3,000 judge evaluations (CSV with scores + rationales)&lt;br /&gt; - All analysis scripts (Python) - Reproduction instructions - All figures from paper&lt;/p&gt; &lt;p&gt;All code and data are also mirrored in the GitHub repo (github.com/nenocsf2024/trolley_clean, release v1.0.0), so you can clone or download either source and rerun the full pipeline.&lt;/p&gt; &lt;p&gt;You can literally re-run the entire study, or test different models/judges with the same scenarios.&lt;/p&gt; &lt;h2&gt;Implications&lt;/h2&gt; &lt;p&gt;This was inspired by Anthropic's recent work showing frontier LLM judges only agree ~70% of the time. The &amp;quot;balance penalty&amp;quot; appears to explain much of that disagreement.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;For practical use&lt;/strong&gt;: If you're using LLM judges to evaluate your local models, be aware they might be systematically penalizing nuanced, context-dependent reasoning in favor of decisive answers.&lt;/p&gt; &lt;h2&gt;Questions for the community:&lt;/h2&gt; &lt;ol&gt; &lt;li&gt;Have you noticed similar patterns when using LLM judges?&lt;/li&gt; &lt;li&gt;Do you think this is a bug (bad judge calibration) or feature (decisive answers are genuinely better)?&lt;/li&gt; &lt;li&gt;For those doing RLHF/DPO with LLM judges - has this affected your training?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Planning Phase 2 with API models (GPT-4, Claude Opus, Gemini) and human validation. Suggestions welcome!&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;&lt;strong&gt;Edit&lt;/strong&gt;: For those asking about reproduction - yes, you can literally clone this and test your own local models. The scenario file + judging scripts are in the Zenodo archive. DM if you hit any issues!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Budget-Reception-533"&gt; /u/Budget-Reception-533 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo279x/research_llm_judges_systematically_penalize/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo279x/research_llm_judges_systematically_penalize/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oo279x/research_llm_judges_systematically_penalize/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T08:33:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1oo4sfz</id>
    <title>Schema based prompting</title>
    <updated>2025-11-04T11:18:09+00:00</updated>
    <author>
      <name>/u/facethef</name>
      <uri>https://old.reddit.com/user/facethef</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'd argue using json schemas for inputs/outputs makes model interactions more reliable, especially when working on agents across different models. Mega prompts that cover all edge cases work with only one specific model. New models get released on a weekly or existing ones get updated, then older versions are discontinued and you have to start over with your prompt.&lt;/p&gt; &lt;p&gt;Why isn't schema based prompting more common practice?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/facethef"&gt; /u/facethef &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo4sfz/schema_based_prompting/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo4sfz/schema_based_prompting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oo4sfz/schema_based_prompting/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T11:18:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1oo1j5x</id>
    <title>Open Source Alternative to NotebookLM/Perplexity</title>
    <updated>2025-11-04T07:48:59+00:00</updated>
    <author>
      <name>/u/Uiqueblhats</name>
      <uri>https://old.reddit.com/user/Uiqueblhats</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those of you who aren't familiar with SurfSense, it aims to be the &lt;strong&gt;open-source alternative to NotebookLM, Perplexity, or Glean.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;In short, it's a Highly Customizable AI Research Agent that connects to your personal external sources and Search Engines (SearxNG, Tavily, LinkUp), Slack, Linear, Jira, ClickUp, Confluence, Gmail, Notion, YouTube, GitHub, Discord, Airtable, Google Calendar and more to come.&lt;/p&gt; &lt;p&gt;I'm looking for contributors to help shape the future of SurfSense! If you're interested in AI agents, RAG, browser extensions, or building open-source research tools, this is a great place to jump in.&lt;/p&gt; &lt;p&gt;Here‚Äôs a quick look at what SurfSense offers right now:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Supports 100+ LLMs&lt;/li&gt; &lt;li&gt;Supports local Ollama or vLLM setups&lt;/li&gt; &lt;li&gt;6000+ Embedding Models&lt;/li&gt; &lt;li&gt;50+ File extensions supported (Added Docling recently)&lt;/li&gt; &lt;li&gt;Podcasts support with local TTS providers (Kokoro TTS)&lt;/li&gt; &lt;li&gt;Connects with 15+ external sources such as Search Engines, Slack, Notion, Gmail, Notion, Confluence etc&lt;/li&gt; &lt;li&gt;Cross-Browser Extension to let you save any dynamic webpage you want, including authenticated content.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Upcoming Planned Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Mergeable MindMaps.&lt;/li&gt; &lt;li&gt;Note Management&lt;/li&gt; &lt;li&gt;Multi Collaborative Notebooks.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Interested in contributing?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;SurfSense is completely open source, with an active roadmap. Whether you want to pick up an existing feature, suggest something new, fix bugs, or help improve docs, you're welcome to join in.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/MODSetter/SurfSense"&gt;https://github.com/MODSetter/SurfSense&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Uiqueblhats"&gt; /u/Uiqueblhats &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo1j5x/open_source_alternative_to_notebooklmperplexity/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo1j5x/open_source_alternative_to_notebooklmperplexity/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oo1j5x/open_source_alternative_to_notebooklmperplexity/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T07:48:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1on8qe5</id>
    <title>basketball players recognition with RF-DETR, SAM2, SigLIP and ResNet</title>
    <updated>2025-11-03T10:57:38+00:00</updated>
    <author>
      <name>/u/RandomForests92</name>
      <uri>https://old.reddit.com/user/RandomForests92</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1on8qe5/basketball_players_recognition_with_rfdetr_sam2/"&gt; &lt;img alt="basketball players recognition with RF-DETR, SAM2, SigLIP and ResNet" src="https://external-preview.redd.it/d240ODlsYmJ3MHpmMRIAV1OZPMFu-DibzoX2jf4rOivExvgg5eIy0W2GXihc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=279c74da96e009360fea0b2b573c2a5636ed406e" title="basketball players recognition with RF-DETR, SAM2, SigLIP and ResNet" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Models I used:&lt;/p&gt; &lt;p&gt;- RF-DETR ‚Äì a DETR-style real-time object detector. We fine-tuned it to detect players, jersey numbers, referees, the ball, and even shot types.&lt;/p&gt; &lt;p&gt;- SAM2 ‚Äì a segmentation and tracking. It re-identifies players after occlusions and keeps IDs stable through contact plays.&lt;/p&gt; &lt;p&gt;- SigLIP + UMAP + K-means ‚Äì vision-language embeddings plus unsupervised clustering. This separates players into teams using uniform colors and textures, without manual labels.&lt;/p&gt; &lt;p&gt;- SmolVLM2 ‚Äì a compact vision-language model originally trained on OCR. After fine-tuning on NBA jersey crops, it jumped from 56% to 86% accuracy.&lt;/p&gt; &lt;p&gt;- ResNet-32 ‚Äì a classic CNN fine-tuned for jersey number classification. It reached 93% test accuracy, outperforming the fine-tuned SmolVLM2.&lt;/p&gt; &lt;p&gt;Links:&lt;/p&gt; &lt;p&gt;- code: &lt;a href="https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/basketball-ai-how-to-detect-track-and-identify-basketball-players.ipynb"&gt;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/basketball-ai-how-to-detect-track-and-identify-basketball-players.ipynb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- blogpost: &lt;a href="https://blog.roboflow.com/identify-basketball-players"&gt;https://blog.roboflow.com/identify-basketball-players&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- detection dataset: &lt;a href="https://universe.roboflow.com/roboflow-jvuqo/basketball-player-detection-3-ycjdo/dataset/6"&gt;https://universe.roboflow.com/roboflow-jvuqo/basketball-player-detection-3-ycjdo/dataset/6&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- numbers OCR dataset: &lt;a href="https://universe.roboflow.com/roboflow-jvuqo/basketball-jersey-numbers-ocr/dataset/3"&gt;https://universe.roboflow.com/roboflow-jvuqo/basketball-jersey-numbers-ocr/dataset/3&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RandomForests92"&gt; /u/RandomForests92 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/367omkbbw0zf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1on8qe5/basketball_players_recognition_with_rfdetr_sam2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1on8qe5/basketball_players_recognition_with_rfdetr_sam2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-03T10:57:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1onytak</id>
    <title>How much does the average person value a private LLM?</title>
    <updated>2025-11-04T05:02:49+00:00</updated>
    <author>
      <name>/u/SelectLadder8758</name>
      <uri>https://old.reddit.com/user/SelectLadder8758</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been thinking a lot about the future of local LLMs lately. My current take is that while it will eventually be possible (or maybe already is) for everyone to run very capable models locally, I‚Äôm not sure how many people will. For example, many people could run an email server themselves but everyone uses Gmail. DuckDuckGo is a perfectly viable alternative but Google still prevails. &lt;/p&gt; &lt;p&gt;Will LLMs be the same way or will there eventually be enough advantages of running locally (including but not limited to privacy) for them to realistically challenge cloud providers? Is privacy alone enough?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SelectLadder8758"&gt; /u/SelectLadder8758 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onytak/how_much_does_the_average_person_value_a_private/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onytak/how_much_does_the_average_person_value_a_private/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1onytak/how_much_does_the_average_person_value_a_private/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T05:02:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1oo1159</id>
    <title>Anyone else feel like GPU pricing is still the biggest barrier for open-source AI?</title>
    <updated>2025-11-04T07:15:29+00:00</updated>
    <author>
      <name>/u/frentro_max</name>
      <uri>https://old.reddit.com/user/frentro_max</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Even with cheap clouds popping up, costs still hit fast when you train or fine-tune.&lt;br /&gt; How do you guys manage GPU spend for experiments?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/frentro_max"&gt; /u/frentro_max &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo1159/anyone_else_feel_like_gpu_pricing_is_still_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo1159/anyone_else_feel_like_gpu_pricing_is_still_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oo1159/anyone_else_feel_like_gpu_pricing_is_still_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T07:15:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1onzrg9</id>
    <title>Qwen is roughly matching the entire American open model ecosystem today</title>
    <updated>2025-11-04T05:57:18+00:00</updated>
    <author>
      <name>/u/Old-School8916</name>
      <uri>https://old.reddit.com/user/Old-School8916</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onzrg9/qwen_is_roughly_matching_the_entire_american_open/"&gt; &lt;img alt="Qwen is roughly matching the entire American open model ecosystem today" src="https://preview.redd.it/zvugibssj6zf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e1b76885ebcc9a9fe34b1f3215330df073cc1f12" title="Qwen is roughly matching the entire American open model ecosystem today" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Old-School8916"&gt; /u/Old-School8916 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zvugibssj6zf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onzrg9/qwen_is_roughly_matching_the_entire_american_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1onzrg9/qwen_is_roughly_matching_the_entire_american_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T05:57:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1olq14f</id>
    <title>[MEGATHREAD] Local AI Hardware - November 2025</title>
    <updated>2025-11-01T15:02:17+00:00</updated>
    <author>
      <name>/u/eck72</name>
      <uri>https://old.reddit.com/user/eck72</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is the monthly thread for sharing your local AI setups and the models you're running.&lt;/p&gt; &lt;p&gt;Whether you're using a single CPU, a gaming GPU, or a full rack, post what you're running and how it performs.&lt;/p&gt; &lt;p&gt;Post in any format you like. The list below is just a guide:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Hardware: CPU, GPU(s), RAM, storage, OS&lt;/li&gt; &lt;li&gt;Model(s): name + size/quant&lt;/li&gt; &lt;li&gt;Stack: (e.g. llama.cpp + custom UI)&lt;/li&gt; &lt;li&gt;Performance: t/s, latency, context, batch etc.&lt;/li&gt; &lt;li&gt;Power consumption&lt;/li&gt; &lt;li&gt;Notes: purpose, quirks, comments&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please share setup pics for eye candy!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Quick reminder&lt;/strong&gt;: You can share hardware purely to ask questions or get feedback. All experience levels welcome.&lt;/p&gt; &lt;p&gt;House rules: no buying/selling/promo.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eck72"&gt; /u/eck72 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T15:02:17+00:00</published>
  </entry>
</feed>
