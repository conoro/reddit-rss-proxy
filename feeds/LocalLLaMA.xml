<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-28T04:50:22+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1p88te7</id>
    <title>Local AI As a "Bubble-proof" Practice</title>
    <updated>2025-11-27T18:03:31+00:00</updated>
    <author>
      <name>/u/acornPersonal</name>
      <uri>https://old.reddit.com/user/acornPersonal</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've built a suite of off-line AI programs for macOS and iOS, with the central purpose of enabling everyday users, who are not tech savvy or up-to-date on the latest and greatest LLMs, etc., too have a private oasis from cloud based AI, data poisoning, and all that nasty data collection practices that the big box LLM companies are utilizing. Another thing that I've noticed about these signals like Peter Thiel's selling of massive amounts of stock in the AI sector says to me that they understand something that us in the local LLM community already intrinsically know, even if it hasn't always been set out loud, but the world Cannot support cloud based AI for every single human being, there's not enough energy or freshwater. We don't have enough planet for it. The only way for us to provide even some semblance or chance for intellectual equality and accessibility around the world is to put AI in peoples local devices. In its own way, the crisis that's occurring has a lot to do with the fact that it must be obvious to people at the top that buying power plants and building infrastructure to service the top 5 to 10% of the planet is just not a sustainable practice. What do you guys think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/acornPersonal"&gt; /u/acornPersonal &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p88te7/local_ai_as_a_bubbleproof_practice/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p88te7/local_ai_as_a_bubbleproof_practice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p88te7/local_ai_as_a_bubbleproof_practice/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-27T18:03:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1p85sj8</id>
    <title>deep dive article: nanochat is in transformers</title>
    <updated>2025-11-27T16:01:59+00:00</updated>
    <author>
      <name>/u/Zealousideal-Cut590</name>
      <uri>https://old.reddit.com/user/Zealousideal-Cut590</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p85sj8/deep_dive_article_nanochat_is_in_transformers/"&gt; &lt;img alt="deep dive article: nanochat is in transformers" src="https://external-preview.redd.it/RO15ENR7oKrHttCSn57xlmGq9x_giQbvpBUYn1LBd9w.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5d9b43465e74180124d46b5e6a62a228925f3e6d" title="deep dive article: nanochat is in transformers" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Finally, NanoChat has landed in transformers! üöÄ And we went wild on this deep dive blog post.&lt;/p&gt; &lt;p&gt;In this deep dive, I explore the lineage of the architecture, the integration process, and the powerful tools you can now use with it. It includes:&lt;/p&gt; &lt;p&gt;- detailed comparison of nanochat and canonical implementation.&lt;/p&gt; &lt;p&gt;- explainer on how and why transformers user modularity.&lt;/p&gt; &lt;p&gt;- deep dive examples on inference and training in torch, TRL, and vLLM.&lt;/p&gt; &lt;p&gt;It was a lot of fun working on this, so I hope folk enjoy the read.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Cut590"&gt; /u/Zealousideal-Cut590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/spaces/nanochat-students/transformers"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p85sj8/deep_dive_article_nanochat_is_in_transformers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p85sj8/deep_dive_article_nanochat_is_in_transformers/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-27T16:01:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1p7rr0g</id>
    <title>Intellect-3: Post-trained GLM 4.5 Air</title>
    <updated>2025-11-27T03:25:16+00:00</updated>
    <author>
      <name>/u/Cute-Sprinkles4911</name>
      <uri>https://old.reddit.com/user/Cute-Sprinkles4911</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;106B (A12B) parameter Mixture-of-Experts reasoning model&lt;/p&gt; &lt;p&gt;NGL the reported stats are sick:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/PrimeIntellect/INTELLECT-3"&gt;https://huggingface.co/PrimeIntellect/INTELLECT-3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;BF16 version can run on 2x H200s, with FP8 on 1x H200&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cute-Sprinkles4911"&gt; /u/Cute-Sprinkles4911 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7rr0g/intellect3_posttrained_glm_45_air/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7rr0g/intellect3_posttrained_glm_45_air/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p7rr0g/intellect3_posttrained_glm_45_air/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-27T03:25:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1p80bw7</id>
    <title>Paper page - NVIDIA Nemotron Parse 1.1</title>
    <updated>2025-11-27T11:50:47+00:00</updated>
    <author>
      <name>/u/ab2377</name>
      <uri>https://old.reddit.com/user/ab2377</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p80bw7/paper_page_nvidia_nemotron_parse_11/"&gt; &lt;img alt="Paper page - NVIDIA Nemotron Parse 1.1" src="https://external-preview.redd.it/2qpOcYaS2j2UQIHcV7jQDC__U_Q0iB2Xn13ee-Dp1dU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5420a7b266ae9ecbe02f1c320631b2ab2fec3c4b" title="Paper page - NVIDIA Nemotron Parse 1.1" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;More OCR!&lt;/p&gt; &lt;p&gt;&amp;quot;We introduce Nemotron-Parse-1.1, a lightweight document parsing and OCR model that advances the capabilities of its predecessor, Nemoretriever-Parse-1.0. Nemotron-Parse-1.1 delivers improved capabilities across general OCR, markdown formatting, structured table parsing, and text extraction from pictures, charts, and diagrams. It also supports a longer output sequence length for visually dense documents. As with its predecessor, it extracts bounding boxes of text segments, as well as corresponding semantic classes. Nemotron-Parse-1.1 follows an encoder-decoder architecture with 885M parameters, including a compact 256M-parameter language decoder. It achieves competitive accuracy on public benchmarks making it a strong lightweight OCR solution. We release the model weights publicly on Huggingface, as well as an optimized NIM container, along with a subset of the training data as part of the broader Nemotron-VLM-v2 dataset. Additionally, we release Nemotron-Parse-1.1-TC which operates on a reduced vision token length, offering a 20% speed improvement with minimal quality degradation.&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ab2377"&gt; /u/ab2377 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/papers/2511.20478"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p80bw7/paper_page_nvidia_nemotron_parse_11/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p80bw7/paper_page_nvidia_nemotron_parse_11/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-27T11:50:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1p88mwr</id>
    <title>llama.cpp now supports online repacking for Q4_K quants on ARM CPUs with dotprod.</title>
    <updated>2025-11-27T17:56:33+00:00</updated>
    <author>
      <name>/u/PurpleWinterDawn</name>
      <uri>https://old.reddit.com/user/PurpleWinterDawn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;First of all, a massive thank you to the llama.cpp team and contributors!&lt;/p&gt; &lt;p&gt;This is huge for ARM-based systems using better quality quants such as Q4_K_M (compared to Q4_0 or IQ4_NL).&lt;/p&gt; &lt;p&gt;On my phone:&lt;/p&gt; &lt;p&gt;LFM2-8B-A1B-Q4_K_M went from 32 pp and 15 tg, to 85 pp and 35 tg. It's still short of 35 pp compared to Q4_0 (I'm getting 125 pp 40 tg), but it's more usable.&lt;/p&gt; &lt;p&gt;The older Ministral-8B-Instruct-2410-Q4_K_M runs 21 pp and 10 tg, up from 10 pp and 6 tg (off the top of my head).&lt;/p&gt; &lt;p&gt;I don't have an ARM-based Mac to test it on, but those numbers look promising for them!&lt;/p&gt; &lt;p&gt;Edit: KoboldCpp also merged the llama.cpp Q4_K repack.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PurpleWinterDawn"&gt; /u/PurpleWinterDawn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p88mwr/llamacpp_now_supports_online_repacking_for_q4_k/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p88mwr/llamacpp_now_supports_online_repacking_for_q4_k/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p88mwr/llamacpp_now_supports_online_repacking_for_q4_k/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-27T17:56:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1p85tiw</id>
    <title>An update to "why multimodal API calls to vLLM server have worse outputs than using Open WebUI"</title>
    <updated>2025-11-27T16:03:03+00:00</updated>
    <author>
      <name>/u/Majesticeuphoria</name>
      <uri>https://old.reddit.com/user/Majesticeuphoria</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;About two weeks ago, I asked this question: &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ouft9q/need_help_figuring_out_why_multimodal_api_calls/"&gt;https://old.reddit.com/r/LocalLLaMA/comments/1ouft9q/need_help_figuring_out_why_multimodal_api_calls/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Finally figured out after extensive testing that the difference was due to usage of qwen-vl-utils to preprocess images. The output is quite different with vs without utils. Just thought this would help anyone else facing similar issues.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Majesticeuphoria"&gt; /u/Majesticeuphoria &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p85tiw/an_update_to_why_multimodal_api_calls_to_vllm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p85tiw/an_update_to_why_multimodal_api_calls_to_vllm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p85tiw/an_update_to_why_multimodal_api_calls_to_vllm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-27T16:03:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1p83rp1</id>
    <title>Qwen3-VL-32B-Thinking EXL3 3.5bpw ‚Äì first working 32B VL quant on single 4090 (16-17 t/s)</title>
    <updated>2025-11-27T14:38:14+00:00</updated>
    <author>
      <name>/u/Nox1793</name>
      <uri>https://old.reddit.com/user/Nox1793</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p83rp1/qwen3vl32bthinking_exl3_35bpw_first_working_32b/"&gt; &lt;img alt="Qwen3-VL-32B-Thinking EXL3 3.5bpw ‚Äì first working 32B VL quant on single 4090 (16-17 t/s)" src="https://b.thumbs.redditmedia.com/vQeSl8GUqQ72s6vWhDU66LdC8eTO6If0EKTqSxBTMhM.jpg" title="Qwen3-VL-32B-Thinking EXL3 3.5bpw ‚Äì first working 32B VL quant on single 4090 (16-17 t/s)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just released the first usable EXL3 quant of the brand-new Qwen3-VL-32B-Thinking (the 32B reasoning + vision beast that dropped 3 days ago).&lt;/p&gt; &lt;ul&gt; &lt;li&gt;3.5 bpw HQ (hb6 / cc4096)&lt;/li&gt; &lt;li&gt;~18-20 GB VRAM ‚Üí fits and runs smooth on single 4090&lt;/li&gt; &lt;li&gt;Vision + &amp;lt;think&amp;gt; chain-of-thought fully preserved&lt;/li&gt; &lt;li&gt;16-17 t/s real-world (see Garfield getting the lasagna meme below üòπ)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;HF: &lt;a href="https://huggingface.co/nullrunner/Qwen3-VL-32B-Thinking-EXL3-3.5bpw"&gt;https://huggingface.co/nullrunner/Qwen3-VL-32B-Thinking-EXL3-3.5bpw&lt;/a&gt;&lt;/p&gt; &lt;p&gt;4bpw HQ baking right now, Instruct version next.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/tsb6uri79t3g1.jpg?width=880&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=6ea9ff51d98a761c4c3f923efd8bfc260ab67689"&gt;Test Image&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5s3w7cwa9t3g1.png?width=1125&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6ea70a9defada2b3ec829f6c33abf7fb5228ea1f"&gt;Output and Metrics&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&amp;quot;convert.py&amp;quot; was broken, vision tower misaligned, LDLQ crashes on layer 37, constant OoM ‚Üí 4 hours of pain + A100 + Claude Code to make it actually work.&lt;/p&gt; &lt;p&gt;Hope someone finds it usefulüî•&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nox1793"&gt; /u/Nox1793 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p83rp1/qwen3vl32bthinking_exl3_35bpw_first_working_32b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p83rp1/qwen3vl32bthinking_exl3_35bpw_first_working_32b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p83rp1/qwen3vl32bthinking_exl3_35bpw_first_working_32b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-27T14:38:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1p8f0jz</id>
    <title>Trying to find the best AI note taking app that isn‚Äôt a bot in my meetings</title>
    <updated>2025-11-27T22:38:43+00:00</updated>
    <author>
      <name>/u/lebron8</name>
      <uri>https://old.reddit.com/user/lebron8</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been bouncing between different ‚ÄúAI note‚Äù tools, and honestly most of them are kind of annoying, either a bot joins the call, or everything gets shipped off to the cloud. Not great if you‚Äôre on sensitive or client calls.&lt;/p&gt; &lt;p&gt;I tried Bluedot recently because it records on your device without joining the meeting, which feels way less weird....but it made me wonder if there‚Äôs a fully local setup people here use.&lt;/p&gt; &lt;p&gt;Anyone hacked together a Whisper + LLaMA combo for meeting transcriptions/summaries? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lebron8"&gt; /u/lebron8 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8f0jz/trying_to_find_the_best_ai_note_taking_app_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8f0jz/trying_to_find_the_best_ai_note_taking_app_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p8f0jz/trying_to_find_the_best_ai_note_taking_app_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-27T22:38:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1p8khfu</id>
    <title>Opencode Mobile / Web</title>
    <updated>2025-11-28T03:20:33+00:00</updated>
    <author>
      <name>/u/getfitdotus</name>
      <uri>https://old.reddit.com/user/getfitdotus</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8khfu/opencode_mobile_web/"&gt; &lt;img alt="Opencode Mobile / Web" src="https://a.thumbs.redditmedia.com/zdbtZsgeLmnqdpgBM6hy97r7Ua23cY63qTXbxWn50s4.jpg" title="Opencode Mobile / Web" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Mobile-first web interface for OpenCode AI assistant. Run, control, and code with OpenCode from any device - your phone, tablet, or desktop. Features Git integration, file management, and real-time chat in a responsive PWA. Deploy with Docker for instant setup.&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/23wobsrr1x3g1.gif"&gt;https://i.redd.it/23wobsrr1x3g1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/mkln8blj1x3g1.gif"&gt;https://i.redd.it/mkln8blj1x3g1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/chriswritescode-dev/opencode-web"&gt;https://github.com/chriswritescode-dev/opencode-web&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/getfitdotus"&gt; /u/getfitdotus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8khfu/opencode_mobile_web/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8khfu/opencode_mobile_web/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p8khfu/opencode_mobile_web/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-28T03:20:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1p80png</id>
    <title>DeepSeekMath-V2: Towards Self-Verifiable Mathematical Reasoning</title>
    <updated>2025-11-27T12:12:01+00:00</updated>
    <author>
      <name>/u/hedgehog0</name>
      <uri>https://old.reddit.com/user/hedgehog0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p80png/deepseekmathv2_towards_selfverifiable/"&gt; &lt;img alt="DeepSeekMath-V2: Towards Self-Verifiable Mathematical Reasoning" src="https://external-preview.redd.it/NNRX5IH0bPXI-mJ26LQk19NWgnKHeMgBlqbRSXbbGFk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=91ce1e1d706821a2296ebf8e620933cc5aa91b2f" title="DeepSeekMath-V2: Towards Self-Verifiable Mathematical Reasoning" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hedgehog0"&gt; /u/hedgehog0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-Math-V2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p80png/deepseekmathv2_towards_selfverifiable/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p80png/deepseekmathv2_towards_selfverifiable/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-27T12:12:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1p82mg3</id>
    <title>Never been a better time, to learn to write a good rhyme!</title>
    <updated>2025-11-27T13:48:02+00:00</updated>
    <author>
      <name>/u/lakySK</name>
      <uri>https://old.reddit.com/user/lakySK</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p82mg3/never_been_a_better_time_to_learn_to_write_a_good/"&gt; &lt;img alt="Never been a better time, to learn to write a good rhyme!" src="https://preview.redd.it/g6nc02nc0t3g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d98ddd988232c35646210a0f68c0b2836986a310" title="Never been a better time, to learn to write a good rhyme!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Adversarial Poetry as a Universal Single-Turn Jailbreak Mechanism in Large Language Models &lt;a href="https://arxiv.org/abs/2511.15304"&gt;https://arxiv.org/abs/2511.15304&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lakySK"&gt; /u/lakySK &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/g6nc02nc0t3g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p82mg3/never_been_a_better_time_to_learn_to_write_a_good/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p82mg3/never_been_a_better_time_to_learn_to_write_a_good/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-27T13:48:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1p8ld27</id>
    <title>Smaller 32B models at Q8 or GLM 4.5 Air at Q3?</title>
    <updated>2025-11-28T04:08:26+00:00</updated>
    <author>
      <name>/u/InfinityZeroFive</name>
      <uri>https://old.reddit.com/user/InfinityZeroFive</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Title. I have an M4 Max Macbook with 64 GB unified memory. At this weight class I can comfortably run Qwen 3 VL 32B, Qwen 3 30B A3B, Gemma 3 27B at Q8, but I can also fit in GLM 4.5 Air at Q3 and below (using the Cerebras REAP variant: &lt;a href="https://huggingface.co/cerebras/GLM-4.5-Air-REAP-82B-A12B"&gt;https://huggingface.co/cerebras/GLM-4.5-Air-REAP-82B-A12B&lt;/a&gt;), however not sure about the performance difference with these quants. My use case is primarily instruction following, machine learning, scientific coding, and math.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InfinityZeroFive"&gt; /u/InfinityZeroFive &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8ld27/smaller_32b_models_at_q8_or_glm_45_air_at_q3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8ld27/smaller_32b_models_at_q8_or_glm_45_air_at_q3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p8ld27/smaller_32b_models_at_q8_or_glm_45_air_at_q3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-28T04:08:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1p8ilde</id>
    <title>Agentic coding with 16GB VRAM and 64GB RAM: can I do locally?</title>
    <updated>2025-11-28T01:41:09+00:00</updated>
    <author>
      <name>/u/esamueb32</name>
      <uri>https://old.reddit.com/user/esamueb32</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi!&lt;/p&gt; &lt;p&gt;I'm a software engineer, and at work I use the company provided cursor agent which works well enough for our uses.&lt;/p&gt; &lt;p&gt;I want to have something similar for personal projects. Is there any model that I can run with my machine that's actually good enough for general coding tasks, or should I just use online models? Which local or online models would you suggest?&lt;/p&gt; &lt;p&gt;Thank you&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/esamueb32"&gt; /u/esamueb32 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8ilde/agentic_coding_with_16gb_vram_and_64gb_ram_can_i/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8ilde/agentic_coding_with_16gb_vram_and_64gb_ram_can_i/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p8ilde/agentic_coding_with_16gb_vram_and_64gb_ram_can_i/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-28T01:41:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1p7o83p</id>
    <title>Where did the Epstein emails dataset go</title>
    <updated>2025-11-27T00:27:19+00:00</updated>
    <author>
      <name>/u/egomarker</name>
      <uri>https://old.reddit.com/user/egomarker</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Removed from Hugging Face (&lt;a href="https://huggingface.co/datasets/tensonaut/EPSTEIN_FILES_20K"&gt;link&lt;/a&gt;)&lt;br /&gt; Removed from GitHub (&lt;a href="https://github.com/EF20K/"&gt;link&lt;/a&gt;)&lt;br /&gt; Reddit account deleted (&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1p683yz/thank_you_all_for_your_contribution_with_tools/"&gt;last post&lt;/a&gt;)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/egomarker"&gt; /u/egomarker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7o83p/where_did_the_epstein_emails_dataset_go/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7o83p/where_did_the_epstein_emails_dataset_go/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p7o83p/where_did_the_epstein_emails_dataset_go/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-27T00:27:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1p8irtu</id>
    <title>What LocalLlama Black Friday deals should I go for?</title>
    <updated>2025-11-28T01:50:15+00:00</updated>
    <author>
      <name>/u/RobotRobotWhatDoUSee</name>
      <uri>https://old.reddit.com/user/RobotRobotWhatDoUSee</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Only answers that will get me in trouble with significant other please.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RobotRobotWhatDoUSee"&gt; /u/RobotRobotWhatDoUSee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8irtu/what_localllama_black_friday_deals_should_i_go_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8irtu/what_localllama_black_friday_deals_should_i_go_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p8irtu/what_localllama_black_friday_deals_should_i_go_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-28T01:50:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1p8d0xn</id>
    <title>If you were wondering about how Tenstorrent's Blackhole chips perform, now we know</title>
    <updated>2025-11-27T21:04:38+00:00</updated>
    <author>
      <name>/u/Tyme4Trouble</name>
      <uri>https://old.reddit.com/user/Tyme4Trouble</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's a pretty dense read but the TLDR is that that Tenstorrent's P150 has a lot of &lt;em&gt;potential&lt;/em&gt; particularly if you string a bunch of them together. &lt;/p&gt; &lt;p&gt;&lt;em&gt;Potential&lt;/em&gt; being the key word here because the software just isn't there yet and won't be until someone writes new kernels for the chips rather than rerunning ones written for Wormhole.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tyme4Trouble"&gt; /u/Tyme4Trouble &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.theregister.com/2025/11/27/tenstorrent_quietbox_review/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8d0xn/if_you_were_wondering_about_how_tenstorrents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p8d0xn/if_you_were_wondering_about_how_tenstorrents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-27T21:04:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1p7siuu</id>
    <title>Anthropic just showed how to make AI agents work on long projects without falling apart</title>
    <updated>2025-11-27T04:05:59+00:00</updated>
    <author>
      <name>/u/purealgo</name>
      <uri>https://old.reddit.com/user/purealgo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Most AI agents forget everything between sessions, which means they completely lose track of long tasks. Anthropic‚Äôs new article shows a surprisingly practical fix. Instead of giving an agent one giant goal like ‚Äúbuild a web app,‚Äù they wrap it in a simple harness that forces structure, memory, and accountability.&lt;/p&gt; &lt;p&gt;First, an initializer agent sets up the project. It creates a full feature list, marks everything as failing, initializes git, and writes a progress log. Then each later session uses a coding agent that reads the log and git history, picks exactly one unfinished feature, implements it, tests it, commits the changes, and updates the log. No guessing, no drift, no forgetting.&lt;/p&gt; &lt;p&gt;The result is an AI that can stop, restart, and keep improving a project across many independent runs. It behaves more like a disciplined engineer than a clever autocomplete. It also shows that the real unlock for long-running agents may not be smarter models, but better scaffolding.&lt;/p&gt; &lt;p&gt;Read the article here:&lt;br /&gt; &lt;a href="https://www.anthropic.com/engineering/effective-harnesses-for-long-running-agents"&gt;https://www.anthropic.com/engineering/effective-harnesses-for-long-running-agents&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/purealgo"&gt; /u/purealgo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7siuu/anthropic_just_showed_how_to_make_ai_agents_work/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7siuu/anthropic_just_showed_how_to_make_ai_agents_work/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p7siuu/anthropic_just_showed_how_to_make_ai_agents_work/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-27T04:05:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1p8fhii</id>
    <title>Seeing 5060 Ti 16GB going for $370; worth it?</title>
    <updated>2025-11-27T23:02:01+00:00</updated>
    <author>
      <name>/u/Careful_Breath_1108</name>
      <uri>https://old.reddit.com/user/Careful_Breath_1108</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Thinking of using two of these together for a total of 32GB VRAM for a beginner home setup to explore inference, fine tuning, and training. Would this be considered viable and cost effective? Or is a single 3090 still way more worth it&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Careful_Breath_1108"&gt; /u/Careful_Breath_1108 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8fhii/seeing_5060_ti_16gb_going_for_370_worth_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8fhii/seeing_5060_ti_16gb_going_for_370_worth_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p8fhii/seeing_5060_ti_16gb_going_for_370_worth_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-27T23:02:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1p7z9g1</id>
    <title>deepseek-ai/DeepSeek-Math-V2 ¬∑ Hugging Face</title>
    <updated>2025-11-27T10:47:09+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7z9g1/deepseekaideepseekmathv2_hugging_face/"&gt; &lt;img alt="deepseek-ai/DeepSeek-Math-V2 ¬∑ Hugging Face" src="https://external-preview.redd.it/NNRX5IH0bPXI-mJ26LQk19NWgnKHeMgBlqbRSXbbGFk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=91ce1e1d706821a2296ebf8e620933cc5aa91b2f" title="deepseek-ai/DeepSeek-Math-V2 ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-Math-V2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7z9g1/deepseekaideepseekmathv2_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p7z9g1/deepseekaideepseekmathv2_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-27T10:47:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1p89j2t</id>
    <title>Today I learned that DDR5 can throttle itself at high temps. It affects inference speed.</title>
    <updated>2025-11-27T18:33:00+00:00</updated>
    <author>
      <name>/u/__JockY__</name>
      <uri>https://old.reddit.com/user/__JockY__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been moving the rig over to a proper frame from the $50 Amazon mining frame and taking the opportunity to do airflow properly. I measured the temps of the 6400 MT/s DDR5 RDIMMs using ipmitool and found they were hitting 95C and above while compiling vLLM from source.&lt;/p&gt; &lt;p&gt;Ouch. That‚Äôs very near the top of their operating envelope.&lt;/p&gt; &lt;p&gt;After 3D printing some RAM shrouds and adding a pair of 92mm Noctua Chromax the DDR5 stays under 60C during compiling and even during CPU inference.&lt;/p&gt; &lt;p&gt;And it runs approx 10% faster at inference even for GPU-only models. &lt;/p&gt; &lt;p&gt;Check your RAM temps!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/__JockY__"&gt; /u/__JockY__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p89j2t/today_i_learned_that_ddr5_can_throttle_itself_at/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p89j2t/today_i_learned_that_ddr5_can_throttle_itself_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p89j2t/today_i_learned_that_ddr5_can_throttle_itself_at/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-27T18:33:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1p8ahy8</id>
    <title>Prime Intellect Introduces INTELLECT-3: A 100B+ MoE Trained With Large-scale RL That Achieves State-Of-The-Art Performance For Its Size, Taking The Lead Amongst Open-Sourced Models Across Math, Code, Science &amp; Reasoning Benchmarks. (Link to Chat with the Model provided)</title>
    <updated>2025-11-27T19:13:50+00:00</updated>
    <author>
      <name>/u/44th--Hokage</name>
      <uri>https://old.reddit.com/user/44th--Hokage</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8ahy8/prime_intellect_introduces_intellect3_a_100b_moe/"&gt; &lt;img alt="Prime Intellect Introduces INTELLECT-3: A 100B+ MoE Trained With Large-scale RL That Achieves State-Of-The-Art Performance For Its Size, Taking The Lead Amongst Open-Sourced Models Across Math, Code, Science &amp;amp; Reasoning Benchmarks. (Link to Chat with the Model provided)" src="https://a.thumbs.redditmedia.com/tQWpy1j22HMExtYqdGvlA_Lo8sIubygJf4xso2VwSj0.jpg" title="Prime Intellect Introduces INTELLECT-3: A 100B+ MoE Trained With Large-scale RL That Achieves State-Of-The-Art Performance For Its Size, Taking The Lead Amongst Open-Sourced Models Across Math, Code, Science &amp;amp; Reasoning Benchmarks. (Link to Chat with the Model provided)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h2&gt;From the Official Announcement:&lt;/h2&gt; &lt;blockquote&gt; &lt;p&gt;Today, we release INTELLECT-3, a 100B+ parameter Mixture-of-Experts model trained on our RL stack, achieving state-of-the-art performance for its size across math, code, science and reasoning benchmarks, outperforming many larger frontier models.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Our complete recipe ‚Äî from the model weights and training frameworks, to our datasets, RL environments, and evaluations ‚Äî has been open-sourced, with the goal of encouraging more open research on large scale reinforcement learning.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;INTELLECT-3 is trained on the same software and infrastructure that we‚Äôre open-sourcing and making available on our platform at Prime Intellect, giving everyone the tools to post-train their own state-of-the-art models, and moving us towards a future where every company can be an AI company.&lt;/p&gt; &lt;p&gt;The sharpest distinction between Prime-RL and many other RL trainers is that it is async-only ‚Äî we recognized fairly early (for our previous INTELLECT-2 model) that the future of RL is async; i.e. always a few steps off-policy. Async training is simply the only practical way to efficiently scale RL to long-horizon agentic rollouts without incurring bottlenecks based on the slowest rollouts per step.&lt;/p&gt; &lt;/blockquote&gt; &lt;hr /&gt; &lt;h2&gt;Architecture:&lt;/h2&gt; &lt;p&gt;Three main abstractions facilitate RL training: the orchestrator, the trainer, and the inference service. A RL training run involves the coordination of a trainer, orchestrator and an inference service. The FSDP trainer and vLLM inference run disaggregated, and can be individually deployed across multiple nodes.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Orchestrator:&lt;/strong&gt; - The orchestrator is a lightweight CPU process that handles the core data flow and scheduling logic, serving as an intermediary between the trainer and inference service with bidirectional relays. In one direction, it collects rollouts from the inference server, assembles them into packed batches, and dispatches them to the trainer; in the other direction, it relays updated model weights from the trainer to the inference service. The orchestrator utilizes verifiers environments to abstract multi-turn rollout generation and scoring, allowing any environment on the Environments Hub to plug into the training loop.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Trainer:&lt;/strong&gt; - The trainer is responsible for producing an updated policy model given rollouts and advantages. We use FSDP 2 as the backend with compatibility for any HuggingFace model. FSDP shards model parameters, gradients, and optimizer states, allowing training large models with data parallelism and minimal GPU memory footprint. The trainer is inspired by torchtitan and relies on native PyTorch features to implement advanced parallelism techniques, such as tensor, context, and expert parallelism, and leverages grouped matrix multiplication kernels for efficient MoE training.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Inference:&lt;/strong&gt; - The inference pool consists of standard OpenAI-compatible servers with a vLLM backend. The API specification is extended with custom endpoints to enable updating the server with the latest policy: &lt;code&gt;/update_weights&lt;/code&gt; is used to update the policy, and &lt;code&gt;/reload_weights&lt;/code&gt; is used to reset the weights to the base model in between experiments. We rely on vLLM's optimized kernels, parallelism strategies, and scheduling for fast rollout generation. Given the disaggregated nature of the service architecture, it can be directly extended to include multiple engines with a shared request pool, allowing operation across multiple clusters and straightforward integration of alternative inference engines.&lt;/p&gt; &lt;hr /&gt; &lt;h4&gt;Link to the Official Announcement: &lt;a href="https://www.primeintellect.ai/blog/intellect-3"&gt;https://www.primeintellect.ai/blog/intellect-3&lt;/a&gt;&lt;/h4&gt; &lt;hr /&gt; &lt;h4&gt;Link to the Technical Report: &lt;a href="https://storage.googleapis.com/intellect-3-paper/INTELLECT_3_Technical_Report.pdf"&gt;https://storage.googleapis.com/intellect-3-paper/INTELLECT_3_Technical_Report.pdf&lt;/a&gt;&lt;/h4&gt; &lt;hr /&gt; &lt;h4&gt;Link to the Open-Sourced Prime-RL GitHub: &lt;a href="https://github.com/PrimeIntellect-ai/prime-rl"&gt;https://github.com/PrimeIntellect-ai/prime-rl&lt;/a&gt;&lt;/h4&gt; &lt;hr /&gt; &lt;h4&gt;Link to the Open-Sourced Model Weights: &lt;a href="https://huggingface.co/PrimeIntellect/INTELLECT-3"&gt;https://huggingface.co/PrimeIntellect/INTELLECT-3&lt;/a&gt;&lt;/h4&gt; &lt;hr /&gt; &lt;h4&gt;Chat with the Model Here: &lt;a href="https://chat.primeintellect.ai/"&gt;https://chat.primeintellect.ai/&lt;/a&gt;&lt;/h4&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/44th--Hokage"&gt; /u/44th--Hokage &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1p8ahy8"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8ahy8/prime_intellect_introduces_intellect3_a_100b_moe/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p8ahy8/prime_intellect_introduces_intellect3_a_100b_moe/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-27T19:13:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1p81k2z</id>
    <title>Yes it is possible to uncensor gpt-oss-20b - ArliAI/gpt-oss-20b-Derestricted</title>
    <updated>2025-11-27T12:56:59+00:00</updated>
    <author>
      <name>/u/Arli_AI</name>
      <uri>https://old.reddit.com/user/Arli_AI</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p81k2z/yes_it_is_possible_to_uncensor_gptoss20b/"&gt; &lt;img alt="Yes it is possible to uncensor gpt-oss-20b - ArliAI/gpt-oss-20b-Derestricted" src="https://external-preview.redd.it/8n5MhbkzXEcl9NlvZMbb8GGre-k1VjQ0kDAKe7qQtQM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9334318d3d29cfd953050dfdf981bc10db9cc00b" title="Yes it is possible to uncensor gpt-oss-20b - ArliAI/gpt-oss-20b-Derestricted" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Original discussion on the initial &lt;strong&gt;&lt;a href="https://www.arliai.com"&gt;Arli AI&lt;/a&gt;&lt;/strong&gt; created GLM-4.5-Air-Derestricted model that was ablated using &lt;a href="/u/grimjim"&gt;u/grimjim&lt;/a&gt;'s new ablation method is here: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1p5epot/the_most_objectively_correct_way_to_abliterate_so/"&gt;The most objectively correct way to abliterate so far - ArliAI/GLM-4.5-Air-Derestricted &lt;/a&gt;&lt;/p&gt; &lt;p&gt;(Note: Derestricted is a name given to models created by Arli AI using this method, but the method officially is just called &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1oypwa7/a_more_surgical_approach_to_abliteration/"&gt;Norm-Preserving Biprojected Abliteration&lt;/a&gt; by &lt;a href="/u/grimjim"&gt;u/grimjim&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;Hey everyone, Owen here from &lt;strong&gt;&lt;a href="https://www.arliai.com"&gt;Arli AI&lt;/a&gt;&lt;/strong&gt; again. In my previous post, I got a lot of requests to attempt this derestricting on OpenAI's gpt-oss models as they are models that are intelligent but was infamous for being very...restricted.&lt;/p&gt; &lt;p&gt;I thought that it would be a big challenge and be interesting to try and attempt as well, and so that was the next model I decided to try and derestrict next. The 120b version is more unwieldy to transfer around and load in/out of VRAM/RAM as I was experimenting, so I started with the 20b version first but I will get to the 120b next which should be super interesting.&lt;/p&gt; &lt;p&gt;As for the 20b model here, it seems to have worked! The model now can respond to questions that OpenAI never would have approved of answering (lol!). It also seems to have cut down its wasteful looping around of deciding whether it can or cannot answer a question based on a non existent policy in it's reasoning, although this isn't completely removed yet. I suspect a more customized harmful/harmless dataset to specifically target this behavior might be useful for this, so that will be what I need to work on.&lt;/p&gt; &lt;p&gt;Otherwise I think this is just an outright improved model over the original as it is much more useful now than it's original behavior. Where it would usually flag a lot of false positives and be absolutely useless in certain situations just because of &amp;quot;safety&amp;quot;.&lt;/p&gt; &lt;p&gt;In order to work on modifying the weights of the model, I also had to use a BF16 converted version to start with as the model as you all might know was released in MXFP4 format, but then attempting the ablation on the BF16 converted model seems to work well. I think that this proves that this new method of essentially &amp;quot;direction-based&amp;quot; abliteration is really flexible and works super well for probably any models.&lt;/p&gt; &lt;p&gt;As for quants, I'm not one to worry about making GGUFs myself because I'm sure the GGUF makers will get to it pretty fast and do a better job than I can. Also, there are no FP8 or INT8 quants now because its pretty small and those that run FP8 or INT8 quants usually have a substantial GPU setup anyways.&lt;/p&gt; &lt;p&gt;Try it out and have fun! This time it's really for &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; because we don't even run this model on our Arli AI API service.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Arli_AI"&gt; /u/Arli_AI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/ArliAI/gpt-oss-20b-Derestricted"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p81k2z/yes_it_is_possible_to_uncensor_gptoss20b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p81k2z/yes_it_is_possible_to_uncensor_gptoss20b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-27T12:56:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1p8hqq4</id>
    <title>Apparently Asus is working with Nvidia on a 784GB "Coherent" Memory desktop PC with 20 PFLOPS AI Performance</title>
    <updated>2025-11-28T00:56:49+00:00</updated>
    <author>
      <name>/u/waiting_for_zban</name>
      <uri>https://old.reddit.com/user/waiting_for_zban</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Somehow the announcement went under the radar, but back in May, along side the Ascent GX10, Asus announced the &lt;a href="https://www.asus.com/displays-desktops/workstations/performance/expertcenter-pro-et900n-g3/"&gt;ExpertCenter Pro ET900N G3&lt;/a&gt;, with GB300 Blackwell. They don't really say what's a &amp;quot;Coherent&amp;quot; memory, but my guess it's another term of saying unified memory like Apple and AMD. &lt;/p&gt; &lt;p&gt;The announcement and the specs are very dry on details, but given the GB300, we might get a very decent memory bandwidth, without &lt;a href="https://i.imgur.com/pNaKzWb.png"&gt;looking like a hideous frankestein monster&lt;/a&gt;. &lt;/p&gt; &lt;p&gt;This might be &lt;a href="/r/Localllama"&gt;r/Localllama&lt;/a&gt; wet dream. If they manage to price it well, and fix that memory bandwidth (that plagued Spark), they have my money.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/waiting_for_zban"&gt; /u/waiting_for_zban &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8hqq4/apparently_asus_is_working_with_nvidia_on_a_784gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8hqq4/apparently_asus_is_working_with_nvidia_on_a_784gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p8hqq4/apparently_asus_is_working_with_nvidia_on_a_784gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-28T00:56:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1b550</id>
    <title>AMA with MiniMax ‚Äî Ask Us Anything!</title>
    <updated>2025-11-19T15:46:38+00:00</updated>
    <author>
      <name>/u/OccasionNo6699</name>
      <uri>https://old.reddit.com/user/OccasionNo6699</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;! We‚Äôre really excited to be here, thanks for having us.&lt;/p&gt; &lt;p&gt;I‚Äôm Skyler (&lt;a href="https://www.reddit.com/user/OccasionNo6699/"&gt;u/OccasionNo6699&lt;/a&gt;), head of engineering at MiniMax, the lab behind:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://x.com/MiniMax__AI/status/1982674798649160175?s=20"&gt;MiniMax-M2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983382728343994414"&gt;Hailuo 2.3&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983661667872600296"&gt;MiniMax Speech 2.6&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983964920493568296"&gt;MiniMax Music 2.0&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Joining me today are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Pengyu Zhao, &lt;a href="https://www.reddit.com/user/Wise_Evidence9973/"&gt;u/Wise_Evidence9973&lt;/a&gt; ‚Äî Head of LLM Research&lt;/li&gt; &lt;li&gt;Jade Cai, &lt;a href="https://www.reddit.com/user/srtng/"&gt;u/srtng&lt;/a&gt; ‚Äî Head of Developer Community&lt;/li&gt; &lt;li&gt;midnight_compile , &lt;a href="/u/Top_Cattle_2098"&gt;u/Top_Cattle_2098&lt;/a&gt; ‚Äî LLM Researcher&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 8AM-11AM PST with our core MiniMax tech team continuing to follow up on questions over the next 48 hours.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OccasionNo6699"&gt; /u/OccasionNo6699 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T15:46:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5retd</id>
    <title>Best Local VLMs - November 2025</title>
    <updated>2025-11-24T20:00:04+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Share what your favorite models are right now and &lt;strong&gt;&lt;em&gt;why&lt;/em&gt;&lt;/strong&gt;. Given the nature of the beast in evaluating VLMs (untrustworthiness of benchmarks, immature tooling, intrinsic stochasticity), please be as detailed as possible in describing your setup, nature of your usage (what applications, how much, personal/professional use), tools/frameworks/prompts etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5retd/best_local_vlms_november_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5retd/best_local_vlms_november_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p5retd/best_local_vlms_november_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T20:00:04+00:00</published>
  </entry>
</feed>
