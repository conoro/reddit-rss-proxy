<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-18T18:09:00+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qfpomi</id>
    <title>[GamersNexus] Creating a 48GB NVIDIA RTX 4090 GPU</title>
    <updated>2026-01-17T21:39:36+00:00</updated>
    <author>
      <name>/u/ThisGonBHard</name>
      <uri>https://old.reddit.com/user/ThisGonBHard</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfpomi/gamersnexus_creating_a_48gb_nvidia_rtx_4090_gpu/"&gt; &lt;img alt="[GamersNexus] Creating a 48GB NVIDIA RTX 4090 GPU" src="https://external-preview.redd.it/BkpkFFoxQzTdVFBwygr_NjC6jb0CW1UxI49hdIPceBg.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f2ae4d7ac52eeeba792fbdb62506b06e57290b67" title="[GamersNexus] Creating a 48GB NVIDIA RTX 4090 GPU" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This seems quite interesting, in getting the 48 GB cards.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThisGonBHard"&gt; /u/ThisGonBHard &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/TcRGBeOENLg?si=2CKaZR7Dj0x89MMU"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfpomi/gamersnexus_creating_a_48gb_nvidia_rtx_4090_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qfpomi/gamersnexus_creating_a_48gb_nvidia_rtx_4090_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T21:39:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfmc05</id>
    <title>China's AGI-NEXT Conference (Qwen, Kimi, Zhipu, Tencent)</title>
    <updated>2026-01-17T19:25:24+00:00</updated>
    <author>
      <name>/u/nuclearbananana</name>
      <uri>https://old.reddit.com/user/nuclearbananana</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfmc05/chinas_aginext_conference_qwen_kimi_zhipu_tencent/"&gt; &lt;img alt="China's AGI-NEXT Conference (Qwen, Kimi, Zhipu, Tencent)" src="https://external-preview.redd.it/TpKYg79IWzebupDqkzAodJruBP4N0VFsDaZESasEpKQ.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ea8968e021234c9b599b354059d32de716c52bed" title="China's AGI-NEXT Conference (Qwen, Kimi, Zhipu, Tencent)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Someone else posted about this, but never posted a transcript, so I found one online.&lt;/p&gt; &lt;p&gt;Lot of interesting stuff about China vs US, paths to AGI, compute, marketing etc.&lt;/p&gt; &lt;p&gt;Unfortunately Moonshot seems to have a very short section. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nuclearbananana"&gt; /u/nuclearbananana &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.chinatalk.media/p/the-all-star-chinese-ai-conversation"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfmc05/chinas_aginext_conference_qwen_kimi_zhipu_tencent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qfmc05/chinas_aginext_conference_qwen_kimi_zhipu_tencent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T19:25:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1qg64fe</id>
    <title>My findings with "Slow, warm cafe song generator" [HeartMula]</title>
    <updated>2026-01-18T11:19:01+00:00</updated>
    <author>
      <name>/u/Rheumi</name>
      <uri>https://old.reddit.com/user/Rheumi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qg64fe/my_findings_with_slow_warm_cafe_song_generator/"&gt; &lt;img alt="My findings with &amp;quot;Slow, warm cafe song generator&amp;quot; [HeartMula]" src="https://a.thumbs.redditmedia.com/54gzNzF_MoRW9nFwP-aVUuX7WjixnN56FqpouNtu020.jpg" title="My findings with &amp;quot;Slow, warm cafe song generator&amp;quot; [HeartMula]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I wanted to share my impressions I got after generating a few songs locally with HeartMula. I got wind of this tool watching a Youtube video by the channel AI Search.&lt;/p&gt; &lt;p&gt;For the people who did not heard of it yet, just a short introction. It's a local song generator. &lt;a href="https://heartmula.github.io"&gt;https://heartmula.github.io&lt;/a&gt; They just released their 3B parameter model that they compare with Udio, Suno and other local models. According to their charts they claim to be on par with Suno 4.5. We'll come back later if the claims are true (mostly not! ;-) ).&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/vntpzhovb3eg1.png?width=5360&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a8ba604f7f5429a2a229cbb2d30b852485ba66b1"&gt;https://preview.redd.it/vntpzhovb3eg1.png?width=5360&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a8ba604f7f5429a2a229cbb2d30b852485ba66b1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;My local Setup is a Ryzen 5950X, 128GB DDR4 3333 RAM and a RTX 3090 24GB. I am NOT an expert in Suno Song creation and I also do not claim to be an expert in LocalAI usage. If I'll install Models, that do not have ComfyUI support I'll always use ChatGPT to help me with troubleshooting.&lt;/p&gt; &lt;p&gt;However installation was really fast. Only some Python incompabilities which were solved very quickly.&lt;/p&gt; &lt;p&gt;There is &amp;quot;NO&amp;quot; graphic interface for easier local usage in your browser yet. You have to use powershell to generate songs and tweak variables like CFG. Your lyrics and stlye tags are stored in .txt files, so it makes sense to make backups if you dont want to overwrite them.&lt;/p&gt; &lt;p&gt;For testing I used some examples which I copy pasted from my SUNO song library. Regarding the style tags its a very basic format: one or to words maximum for each style tag. No sentences! VRAM was filled at ~21.7 GB. So I dont know if you can use it with 16GB cards. Generation time with my 3090 for a ~2.5 minute song takes about 3 minutes. So it's nearly real time. Of course there is no preview while generation like in Suno. You can play the output.mp3 stored locally in your folder when its generated completely.&lt;/p&gt; &lt;p&gt;I tested about 10 songs with a variety of differents styles. Some more basic with just 3 tags, some more complicated. The results were pretty underwhelming and did not match the expectations their charts and demos on their website promised. It was good at following the lyrics and expect one error, where in mid of the song the volume suddenly changed, it was a somehow coherent song structure.&lt;/p&gt; &lt;p&gt;The quality of the audio generation is all over the place. If you keep your style very close to their cherrypicked Demo songs, the quality good. Clear voice. nice piano music. Not Suno 4.5 but like V3 quality. But when you want to deviate into styles other than &amp;quot;slow pace,warm,cafe,female voice&amp;quot; region, it will get worse quickly. Like a Suno 1.5.&lt;/p&gt; &lt;p&gt;It really depends on the style - which is the next critique. It will ignore styles it does not know and these are A LOT!! Congratulations if you generated something that resembles a rock song. But its not good at electric guitars and fast paced drums. It sounds like half the instruments are missing and are replaced with MIDI files. Electronic generation is also really basic. Metal and other harder styles are non existent.&lt;/p&gt; &lt;p&gt;However it does also generate German lyrics even though they are not advertised on their demo page.&lt;/p&gt; &lt;p&gt;Overall I think it is a nice, clunky to use, Proof of concept that gives me the impression that its trained with only a handful of songs. It has potential as the demo songs show but It's biggest problem is variaty and style following. My negative tone comes from beeing disappointed that I feel a bit deceived because their charts show something that it only somewhat promised to deliver when its used in a very narrow style corridor.&lt;/p&gt; &lt;p&gt;Thats all I needed to share with you so you dont have high expectations. If a HeartMula delevoper reads this, please do not feel disappointed or offended by this text. As I said, potential is there and I look forward into improvements in usability and style variation for a Version 2! :)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TLDR; Do not use if you intend to generate music other than whats showed on the demo page.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;PS: I you have any other questions regarding my impressions, please ask!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rheumi"&gt; /u/Rheumi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qg64fe/my_findings_with_slow_warm_cafe_song_generator/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qg64fe/my_findings_with_slow_warm_cafe_song_generator/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qg64fe/my_findings_with_slow_warm_cafe_song_generator/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-18T11:19:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1qgf4q7</id>
    <title>Need help and suggestions for gguf models</title>
    <updated>2026-01-18T17:48:07+00:00</updated>
    <author>
      <name>/u/cmdrmcgarrett</name>
      <uri>https://old.reddit.com/user/cmdrmcgarrett</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am running Qwen2.5-14B-Instruct-abliterated-v2.Q6_K and not getting decent responses as I am in Gemini (my online go-to)&lt;/p&gt; &lt;p&gt;I have 16gb vram 5060ti&lt;/p&gt; &lt;p&gt;Are there any other possible LLMs? I use it for general searches, computer help, all over questioning over various subjects. No health questions&lt;/p&gt; &lt;p&gt;I have also tried Mistral-Nemo-12B-ArliAI-RPMax-v1.2-q8_0 to same effect.&lt;/p&gt; &lt;p&gt;How can I get Gemini-type answers other than using Gemini online. I would like abliterated versions/&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cmdrmcgarrett"&gt; /u/cmdrmcgarrett &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgf4q7/need_help_and_suggestions_for_gguf_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgf4q7/need_help_and_suggestions_for_gguf_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qgf4q7/need_help_and_suggestions_for_gguf_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-18T17:48:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1qg55aa</id>
    <title>Self-improving coding workflow experiment: AI generates tests, fixes bugs autonomously, mixed results</title>
    <updated>2026-01-18T10:21:30+00:00</updated>
    <author>
      <name>/u/Independent_Plum_489</name>
      <uri>https://old.reddit.com/user/Independent_Plum_489</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been experimenting with a workflow where the AI writes code, generates test cases, runs them, then fixes failures without me intervening. Inspired by some research on self-play training but applied to actual coding tasks.&lt;/p&gt; &lt;p&gt;Basic setup: gave it a loose spec for a JSON parser, let it write the implementation, generate edge case tests, then iterate on failures. Used GPT and Claude through Verdent to compare approaches.&lt;/p&gt; &lt;p&gt;Some things worked well. Simple functions with clear success criteria like parsers, validators, formatters. It caught edge cases I wouldn't have thought of. Iteration speed was fast once the loop started.&lt;/p&gt; &lt;p&gt;Other things didn't work. Complex architecture decisions had it refactoring in circles. No sense of &amp;quot;good enough&amp;quot; so it would optimize forever if I let it. Generated tests were sometimes too narrow or too broad. Broke a working auth flow trying to &amp;quot;improve&amp;quot; it, had to rollback.&lt;/p&gt; &lt;p&gt;The verification problem is real. For math or parsing you can check correctness objectively. For UI or business logic there's no automatic way to verify &amp;quot;this is what the user wanted.&amp;quot;&lt;/p&gt; &lt;p&gt;Cost was around $22 in tokens for about 2 hours of iterations. Faster than writing tests myself but the code quality was inconsistent. Some functions were clean, others were over-engineered.&lt;/p&gt; &lt;p&gt;Not sure this is actually better than traditional TDD. You still need to review everything and the AI doesn't understand tradeoffs. It'll optimize for test coverage over readability.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent_Plum_489"&gt; /u/Independent_Plum_489 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qg55aa/selfimproving_coding_workflow_experiment_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qg55aa/selfimproving_coding_workflow_experiment_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qg55aa/selfimproving_coding_workflow_experiment_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-18T10:21:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1qftdr4</id>
    <title>AI insiders seek to poison the data that feeds them</title>
    <updated>2026-01-18T00:14:54+00:00</updated>
    <author>
      <name>/u/HumanDrone8721</name>
      <uri>https://old.reddit.com/user/HumanDrone8721</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qftdr4/ai_insiders_seek_to_poison_the_data_that_feeds/"&gt; &lt;img alt="AI insiders seek to poison the data that feeds them" src="https://external-preview.redd.it/oZsMR98JWtXvCHBS_WeIPqnrR9GLtUCvLvVVcRNn-mI.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=176b3305413cf55f808df5dcf6836f7c0b89e27e" title="AI insiders seek to poison the data that feeds them" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HumanDrone8721"&gt; /u/HumanDrone8721 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.theregister.com/2026/01/11/industry_insiders_seek_to_poison/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qftdr4/ai_insiders_seek_to_poison_the_data_that_feeds/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qftdr4/ai_insiders_seek_to_poison_the_data_that_feeds/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-18T00:14:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1qgcm6x</id>
    <title>Demo for the latest PersonaPlex model from nvidia (speech-to-speech model that is controllable through system prompt)</title>
    <updated>2026-01-18T16:13:05+00:00</updated>
    <author>
      <name>/u/Severe-Awareness829</name>
      <uri>https://old.reddit.com/user/Severe-Awareness829</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgcm6x/demo_for_the_latest_personaplex_model_from_nvidia/"&gt; &lt;img alt="Demo for the latest PersonaPlex model from nvidia (speech-to-speech model that is controllable through system prompt)" src="https://external-preview.redd.it/3-UW-zJ-uCRgO1BeO_woYq-yx09bN2AqMjYYa3dJ998.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c42872e666813b68b457c113320540644e72db91" title="Demo for the latest PersonaPlex model from nvidia (speech-to-speech model that is controllable through system prompt)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hope this can be helpful for someone&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Severe-Awareness829"&gt; /u/Severe-Awareness829 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/spaces/MohamedRashad/PersonaPlex"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgcm6x/demo_for_the_latest_personaplex_model_from_nvidia/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qgcm6x/demo_for_the_latest_personaplex_model_from_nvidia/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-18T16:13:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1qgbx2s</id>
    <title>I built a fully autonomous "Infinite Podcast" rig running entirely on my RTX 5060 Ti. No OpenAI, No ElevenLabs. Just Python + Local Models</title>
    <updated>2026-01-18T15:47:08+00:00</updated>
    <author>
      <name>/u/Legion10008</name>
      <uri>https://old.reddit.com/user/Legion10008</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgbx2s/i_built_a_fully_autonomous_infinite_podcast_rig/"&gt; &lt;img alt="I built a fully autonomous &amp;quot;Infinite Podcast&amp;quot; rig running entirely on my RTX 5060 Ti. No OpenAI, No ElevenLabs. Just Python + Local Models" src="https://external-preview.redd.it/YW96OXJobnRvNGVnMdq6M9CsXAgOG_Rn-16_4_EyJoCn-JyF57xCOwtYmHZ6.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2c08de7629bef3d66726b9e4e747705137d83215" title="I built a fully autonomous &amp;quot;Infinite Podcast&amp;quot; rig running entirely on my RTX 5060 Ti. No OpenAI, No ElevenLabs. Just Python + Local Models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Legion10008"&gt; /u/Legion10008 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/jwx2d5nto4eg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgbx2s/i_built_a_fully_autonomous_infinite_podcast_rig/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qgbx2s/i_built_a_fully_autonomous_infinite_podcast_rig/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-18T15:47:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1qgdb1i</id>
    <title>4x AMD R9700 (128GB VRAM) + Threadripper 9955WX Build</title>
    <updated>2026-01-18T16:39:31+00:00</updated>
    <author>
      <name>/u/NunzeCs</name>
      <uri>https://old.reddit.com/user/NunzeCs</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgdb1i/4x_amd_r9700_128gb_vram_threadripper_9955wx_build/"&gt; &lt;img alt="4x AMD R9700 (128GB VRAM) + Threadripper 9955WX Build" src="https://b.thumbs.redditmedia.com/tH0O7nrjhLN0eZEvYxWWxQfxMWqH9KP_g0wYgBP6sEQ.jpg" title="4x AMD R9700 (128GB VRAM) + Threadripper 9955WX Build" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Disclaimer: I am from Germany and my English is not perfect, so I used an LLM to help me structure and write this post.&lt;/p&gt; &lt;p&gt;Context &amp;amp; Motivation: I built this system for my small company. The main reason for all new hardware is that I received a 50% subsidy/refund from my local municipality for digitalization investments. To qualify for this funding, I had to buy new hardware and build a proper &amp;quot;server-grade&amp;quot; system.&lt;/p&gt; &lt;p&gt;My goal was to run large models (120B+) locally for data privacy. With the subsidy in mind, I had a budget of around 10,000€ (pre-refund). I initially considered NVIDIA, but I wanted to maximize VRAM. I decided to go with 4x AMD RDNA4 cards (ASRock R9700) to get 128GB VRAM total and used the rest of the budget for a solid Threadripper platform.&lt;/p&gt; &lt;p&gt;Hardware Specs:&lt;/p&gt; &lt;p&gt;Total Cost: ~9,800€ (I get ~50% back, so effectively ~4,900€ for me).&lt;/p&gt; &lt;p&gt;CPU: AMD Ryzen Threadripper PRO 9955WX (16 Cores) Mainboard: ASRock WRX90 WS EVO RAM: 128GB DDR5 5600MHz GPU: 4x ASRock Radeon AI PRO R9700 32GB (Total 128GB VRAM) Configuration: All cards running at full PCIe 5.0 x16 bandwidth. Storage: 2x 2TB PCIe 4.0 SSD PSU: Seasonic 2200W Cooling: Alphacool Eisbaer Pro Aurora 360 CPU AIO&lt;/p&gt; &lt;p&gt;Benchmark Results&lt;/p&gt; &lt;p&gt;I tested various models ranging from 8B to 230B parameters.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Llama.cpp (Focus: Single User Latency) Settings: Flash Attention ON, Batch 2048&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Model Size Quant Mode Prompt t/s Gen t/s Meta-Llama-3.1-8B-Instruct 8B Q4_K_M GPU-Full 3169.16 81.01 Qwen2.5-32B-Instruct 32B Q4_K_M GPU-Full 848.68 25.14 Meta-Llama-3.1-70B-Instruct 70B Q4_K_M GPU-Full 399.03 12.66 gpt-oss-120b 120B Q4_K_M GPU-Full 2977.83 97.47 GLM-4.7-REAP-218B 218B Q3_K_M GPU-Full 504.15 17.48 MiniMax-M2.1 ~230B Q4_K_M Hybrid 938.89 32.12&lt;/p&gt; &lt;p&gt;Side note: I found that with PCIe 5.0, standard Pipeline Parallelism (Layer Split) is significantly faster (~97 t/s) than Tensor Parallelism/Row Split (~67 t/s) for a single user on this setup.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;vLLM (Focus: Throughput) Model: GPT-OSS-120B (bfloat16), TP=4, test for 20 requests&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Total Throughput: ~314 tokens/s (Generation) Prompt Processing: ~5339 tokens/s Single user throughput 50 tokens/s&lt;/p&gt; &lt;p&gt;I used rocm 7.1.1 for llama.cpp also testet Vulkan but it was worse&lt;/p&gt; &lt;p&gt;If I could do it again, I would have used the budget to buy a single NVIDIA RTX Pro 6000 Blackwell (96GB). Maybe I will, if local AI is going well for my use case, I swap the R9700 with Pro 6000 in the future.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NunzeCs"&gt; /u/NunzeCs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qgdb1i"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgdb1i/4x_amd_r9700_128gb_vram_threadripper_9955wx_build/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qgdb1i/4x_amd_r9700_128gb_vram_threadripper_9955wx_build/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-18T16:39:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfkn3a</id>
    <title>Best "End of world" model that will run on 24gb VRAM</title>
    <updated>2026-01-17T18:21:20+00:00</updated>
    <author>
      <name>/u/gggghhhhiiiijklmnop</name>
      <uri>https://old.reddit.com/user/gggghhhhiiiijklmnop</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey peeps, I'm feeling in a bit of a omg the world is ending mood and have been amusing myself by downloading and hoarding a bunch of data - think wikipedia, wiktionary, wikiversity, khan academy, etc etc&lt;/p&gt; &lt;p&gt;What's your take on the smartest / best model(s) to download and store - they need to fit and run on my 24gb VRAM / 64gb RAM PC.? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gggghhhhiiiijklmnop"&gt; /u/gggghhhhiiiijklmnop &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfkn3a/best_end_of_world_model_that_will_run_on_24gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfkn3a/best_end_of_world_model_that_will_run_on_24gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qfkn3a/best_end_of_world_model_that_will_run_on_24gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T18:21:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1qg9uvi</id>
    <title>Just built an app using llama.cpp</title>
    <updated>2026-01-18T14:23:58+00:00</updated>
    <author>
      <name>/u/Useful_Advisor920</name>
      <uri>https://old.reddit.com/user/Useful_Advisor920</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qg9uvi/just_built_an_app_using_llamacpp/"&gt; &lt;img alt="Just built an app using llama.cpp" src="https://external-preview.redd.it/Z3NhMDNlYW5hNGVnMUNUtL3ojV8mRZI5MC7PHWnku738D1U5XxnZmpt8UVdx.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4e4b5997d9b5255e4545c0ce9d1829e43cf2e31c" title="Just built an app using llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Run deepseek ocr locally on your phone&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Useful_Advisor920"&gt; /u/Useful_Advisor920 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/lbhci3ana4eg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qg9uvi/just_built_an_app_using_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qg9uvi/just_built_an_app_using_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-18T14:23:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1qg5io6</id>
    <title>Is it feasible for a Team to replace Claude Code with one of the "local" alternatives?</title>
    <updated>2026-01-18T10:44:03+00:00</updated>
    <author>
      <name>/u/nunodonato</name>
      <uri>https://old.reddit.com/user/nunodonato</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So yes, I've read countless posts in this sub about replacing Claude Code with local models.&lt;/p&gt; &lt;p&gt;My question is slightly different. I'm talking about finding a replacement that would be able to serve a small team of developers.&lt;/p&gt; &lt;p&gt;We are currently spending around 2k/mo on Claude. And that can go a long way on cloud GPUs. However, I'm not sure if it would be good enough to support a few concurrent requests.&lt;/p&gt; &lt;p&gt;I've read a lot of praise for Deepseek Coder and a few of the newer models, but would they still perform okay-ish with Q8?&lt;/p&gt; &lt;p&gt;Any advice? recommendations?&lt;/p&gt; &lt;p&gt;thanks in advance&lt;/p&gt; &lt;p&gt;Edit: I plan to keep Claude Code (the app), but switch the models. I know that Claude Code is responsible for the high success rate, regardless of the model. The tools and prompt are very good. So I think even with a worse model, we would get reasonable results when using it via claude code&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nunodonato"&gt; /u/nunodonato &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qg5io6/is_it_feasible_for_a_team_to_replace_claude_code/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qg5io6/is_it_feasible_for_a_team_to_replace_claude_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qg5io6/is_it_feasible_for_a_team_to_replace_claude_code/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-18T10:44:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1qgc15w</id>
    <title>ROCm+Linux on AMD Strix Halo: January 2026 Stable Configurations</title>
    <updated>2026-01-18T15:51:22+00:00</updated>
    <author>
      <name>/u/Intrepid_Rub_3566</name>
      <uri>https://old.reddit.com/user/Intrepid_Rub_3566</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgc15w/rocmlinux_on_amd_strix_halo_january_2026_stable/"&gt; &lt;img alt="ROCm+Linux on AMD Strix Halo: January 2026 Stable Configurations" src="https://b.thumbs.redditmedia.com/4HB_kRub0ByrHTDKoq1IizVqH5bAkGmUVpks54w-dLU.jpg" title="ROCm+Linux on AMD Strix Halo: January 2026 Stable Configurations" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;New video on ROCm+Linux support for AMD Strix Halo, documenting working/stable configurations in January 2026 and what caused the original issues.&lt;/p&gt; &lt;p&gt;&lt;a href="https://youtu.be/Hdg7zL3pcIs"&gt;https://youtu.be/Hdg7zL3pcIs&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Copying the table here for reference (&lt;a href="https://github.com/kyuz0/amd-strix-halo-gfx1151-toolboxes"&gt;https://github.com/kyuz0/amd-strix-halo-gfx1151-toolboxes&lt;/a&gt;):&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ygn7zad4r4eg1.png?width=2538&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5291169682acb6fb54cf25d21118877d926ede3a"&gt;https://preview.redd.it/ygn7zad4r4eg1.png?width=2538&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5291169682acb6fb54cf25d21118877d926ede3a&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Intrepid_Rub_3566"&gt; /u/Intrepid_Rub_3566 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgc15w/rocmlinux_on_amd_strix_halo_january_2026_stable/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgc15w/rocmlinux_on_amd_strix_halo_january_2026_stable/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qgc15w/rocmlinux_on_amd_strix_halo_january_2026_stable/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-18T15:51:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1qgcj8b</id>
    <title>RLVR with GRPO from scratch code notebook</title>
    <updated>2026-01-18T16:10:02+00:00</updated>
    <author>
      <name>/u/seraschka</name>
      <uri>https://old.reddit.com/user/seraschka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgcj8b/rlvr_with_grpo_from_scratch_code_notebook/"&gt; &lt;img alt="RLVR with GRPO from scratch code notebook" src="https://external-preview.redd.it/1zp6Ys_kCzKo-Gqi6ZfsuCLpMOxYXSdKyJbi6hC7oDk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=80606d0ff718fb311decc9610424061c1fa2743b" title="RLVR with GRPO from scratch code notebook" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/seraschka"&gt; /u/seraschka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/rasbt/reasoning-from-scratch/blob/main/ch06/01_main-chapter-code/ch06_main.ipynb"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgcj8b/rlvr_with_grpo_from_scratch_code_notebook/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qgcj8b/rlvr_with_grpo_from_scratch_code_notebook/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-18T16:10:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfq9ez</id>
    <title>The Search for Uncensored AI (That Isn’t Adult-Oriented)</title>
    <updated>2026-01-17T22:03:23+00:00</updated>
    <author>
      <name>/u/Fun-Situation-4358</name>
      <uri>https://old.reddit.com/user/Fun-Situation-4358</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been trying to find an AI that’s genuinely unfiltered &lt;em&gt;and&lt;/em&gt; technically advanced, uncensored something that can reason freely without guardrails killing every interesting response.&lt;/p&gt; &lt;p&gt;Instead, almost everything I run into is marketed as “uncensored,” but it turns out to be optimized for low-effort adult use rather than actual intelligence or depth.&lt;/p&gt; &lt;p&gt;It feels like the space between heavily restricted corporate AI and shallow adult-focused models is strangely empty, and I’m curious why that gap still exists...&lt;/p&gt; &lt;p&gt;Is there any &lt;strong&gt;uncensored or lightly filtered AI&lt;/strong&gt; that focuses on reasoning, creativity,uncensored technology or serious problem-solving instead? I’m open to self-hosted models, open-source projects, or lesser-known platforms. Suggestions appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fun-Situation-4358"&gt; /u/Fun-Situation-4358 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfq9ez/the_search_for_uncensored_ai_that_isnt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfq9ez/the_search_for_uncensored_ai_that_isnt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qfq9ez/the_search_for_uncensored_ai_that_isnt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T22:03:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1qgbkcd</id>
    <title>Running language models where they don't belong</title>
    <updated>2026-01-18T15:33:03+00:00</updated>
    <author>
      <name>/u/Brief_Argument8155</name>
      <uri>https://old.reddit.com/user/Brief_Argument8155</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We have seen a cool counter-trend recently to the typical scaleup narrative (see Smol/Phi and ZIT most notably). I've been on a mission to push this to the limit (mainly for fun), moving LMs into environments where they have no business existing.&lt;/p&gt; &lt;p&gt;My thesis is that even the most primitive environments can host generative capabilities if you bake them in correctly.&lt;/p&gt; &lt;p&gt;So here goes:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. The NES LM (inference on 1983 hardware)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I started by writing a char-level bigram model in straight 6502 asm for the original Nintendo Entertainment System.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;2KB of RAM and a CPU with no multiplication opcode, let alone float math.&lt;/li&gt; &lt;li&gt;The model compresses a name space of 18 million possibilities into a footprint smaller than a Final Fantasy black mage sprite (729 bytes of weights).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For extra fun I packaged it into a romhack for Final Fantasy I and Dragon Warrior to generate fantasy names at game time, on original hardware.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Code:&lt;/strong&gt; &lt;a href="https://github.com/erodola/bigram-nes"&gt;https://github.com/erodola/bigram-nes&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. The Compile-Time LM (inference while compiling, duh)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Then I realized that even the NES was too much runtime. Why even wait for the code to run at all? I built a model that does inference entirely at compile-time using C++ template metaprogramming. &lt;/p&gt; &lt;p&gt;Because the compiler itself is Turing complete you know. You could run Doom in it.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The C++ compiler acts as the inference engine. It performs the multinomial sampling and Markov chain transitions &lt;em&gt;while&lt;/em&gt; you are building the project.&lt;/li&gt; &lt;li&gt;Since compilers are deterministic, I hashed &lt;strong&gt;TIME&lt;/strong&gt; into an FNV-1a seed to power a constexpr Xorshift32 RNG.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;When the binary finally runs, the CPU does zero math. The generated text is already there, baked into the data segment as a constant string.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Code:&lt;/strong&gt; &lt;a href="https://github.com/erodola/bigram-metacpp"&gt;https://github.com/erodola/bigram-metacpp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Next up is ofc attempting to scale this toward TinyStories-style models. Or speech synthesis, or OCR. I wont stop until my build logs are more sentient than the code they're actually producing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Brief_Argument8155"&gt; /u/Brief_Argument8155 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgbkcd/running_language_models_where_they_dont_belong/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgbkcd/running_language_models_where_they_dont_belong/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qgbkcd/running_language_models_where_they_dont_belong/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-18T15:33:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1qg6dr6</id>
    <title>Ministral 3 Reasoning Heretic and GGUFs</title>
    <updated>2026-01-18T11:34:20+00:00</updated>
    <author>
      <name>/u/coder3101</name>
      <uri>https://old.reddit.com/user/coder3101</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks,&lt;/p&gt; &lt;p&gt;Back with another series of abilitered (uncensored) models, this time Ministral 3 with Vision capability. These models lost all their refusal with minimal damage. &lt;/p&gt; &lt;p&gt;As bonus, this time I also quantized them instead of waiting for community.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/coder3101/ministral-3-reasoning-heretic"&gt;https://huggingface.co/collections/coder3101/ministral-3-reasoning-heretic&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Series contains:&lt;/p&gt; &lt;p&gt;- Ministral 3 4B Reasoning&lt;/p&gt; &lt;p&gt;- Ministral 3 8B Reasoning&lt;/p&gt; &lt;p&gt;- Ministral 3 14B Reasoning&lt;/p&gt; &lt;p&gt;All with Q4, Q5, Q8, BF16 quantization with MMPROJ for Vision capabilities.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/coder3101"&gt; /u/coder3101 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qg6dr6/ministral_3_reasoning_heretic_and_ggufs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qg6dr6/ministral_3_reasoning_heretic_and_ggufs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qg6dr6/ministral_3_reasoning_heretic_and_ggufs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-18T11:34:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1qg4d4t</id>
    <title>What we learned processing 1M+ emails for context engineering</title>
    <updated>2026-01-18T09:35:07+00:00</updated>
    <author>
      <name>/u/EnoughNinja</name>
      <uri>https://old.reddit.com/user/EnoughNinja</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We spent the last year building systems to turn email into structured context for AI agents. Processed over a million emails to figure out what actually works.&lt;/p&gt; &lt;p&gt;Some things that weren't obvious going in:&lt;/p&gt; &lt;p&gt;Thread reconstruction is way harder than I thought. You've got replies, forwards, people joining mid-conversation, decisions getting revised three emails later. Most systems just concatenate text in chronological order and hope the LLM figures it out, but that falls apart fast because you lose who said what and why it matters.&lt;/p&gt; &lt;p&gt;Attachments are half the conversation. PDFs, contracts, invoices, they're not just metadata, they're actual content that drives decisions. We had to build OCR and structure parsing so the system can actually read them, not just know they exist as file names.&lt;/p&gt; &lt;p&gt;Multilingual threads are more common than you'd think. People switch languages mid-conversation all the time, especially in global teams. Semantic search that works well in English completely breaks down when you need cross-language understanding.&lt;/p&gt; &lt;p&gt;Zero data retention is non-negotiable if you want enterprise customers. We discard every prompt after processing. Memory gets reconstructed on demand from the original sources, nothing stored. Took us way longer to build but there's no other way to get past compliance teams.&lt;/p&gt; &lt;p&gt;Performance-wise we're hitting around 200ms for retrieval and about 3 seconds to first token even on massive inboxes. &lt;/p&gt; &lt;p&gt;Most of the time is in the reasoning step, not the search.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EnoughNinja"&gt; /u/EnoughNinja &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qg4d4t/what_we_learned_processing_1m_emails_for_context/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qg4d4t/what_we_learned_processing_1m_emails_for_context/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qg4d4t/what_we_learned_processing_1m_emails_for_context/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-18T09:35:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1qg823q</id>
    <title>Kind of Rant: My local server order got cancelled after a 3-month wait because they wanted to over triple the price. Anybody got in similar situation?</title>
    <updated>2026-01-18T13:04:04+00:00</updated>
    <author>
      <name>/u/SomeRandomGuuuuuuy</name>
      <uri>https://old.reddit.com/user/SomeRandomGuuuuuuy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, &lt;/p&gt; &lt;p&gt;I never post stuff like this, but need to vent as I can't stop thinking about it and it piss me of so much.&lt;/p&gt; &lt;p&gt;Since I was young I couldn't afford hardware or do much, heck I needed to wait till 11 pm each day to watch youtube video as network in my region was so shitty (less than 100 kbps 90% of day). There were also no other provider. I was like scripting downloads of movies youtube video or some courses at night at specific hours at night and closing pc as it was working like a jet engine.&lt;/p&gt; &lt;p&gt;I’m a young dev who finally saved up enough money to upgrade from my old laptop to a real rig for AI training, video editing and optimization tests of local inference. I spent months researching parts and found a company willing to build a custom server with 500GB RAM and room for GPU expansion. I paid about €5k and was told it would arrive by December.&lt;/p&gt; &lt;p&gt;Long story short: &lt;strong&gt;One day before Christmas&lt;/strong&gt;, they tell me that because RAM prices increased, I need to pay an &lt;strong&gt;extra €10k&lt;/strong&gt; on top of what I already paid plus tax. I tried fighting it, but since it was a B2B/private mix purchase, EU consumer laws are making it hard, and lawyers are too expensive. They forced a refund on me to wash their hands of it that I don't even accept.&lt;/p&gt; &lt;p&gt;I have &lt;strong&gt;RTX 5090&lt;/strong&gt; that has been sitting in a box for a year (I bought it early, planning for this build).&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I have nothing to put it in. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I play around models and projects like vLLM, SGLang, and Dynamo for work and hobby. Also do some smart home stuff assistance. I am left with old laptop that crash regularly so I am thinking at least of M5 Pro Macbook to abuse battery and go around to cafes as I loved doing it in Uni. &lt;/p&gt; &lt;p&gt;I could have chance to go with my company to China or the USA later this year so maybe I could buy some parts. I technically have some resources at job agreed on playing but not much and it could bite my ass maybe later.&lt;/p&gt; &lt;p&gt;Anybody have similar story ? What you guys plan to do ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SomeRandomGuuuuuuy"&gt; /u/SomeRandomGuuuuuuy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qg823q/kind_of_rant_my_local_server_order_got_cancelled/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qg823q/kind_of_rant_my_local_server_order_got_cancelled/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qg823q/kind_of_rant_my_local_server_order_got_cancelled/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-18T13:04:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfscp5</id>
    <title>128GB VRAM quad R9700 server</title>
    <updated>2026-01-17T23:30:26+00:00</updated>
    <author>
      <name>/u/Ulterior-Motive_</name>
      <uri>https://old.reddit.com/user/Ulterior-Motive_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfscp5/128gb_vram_quad_r9700_server/"&gt; &lt;img alt="128GB VRAM quad R9700 server" src="https://b.thumbs.redditmedia.com/SbBMg1b6qTh913lUa8uWDuyYZrIwJ_ECUuUVvuWh_qA.jpg" title="128GB VRAM quad R9700 server" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is a sequel to my &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1fqwrvg/64gb_vram_dual_mi100_server/"&gt;previous thread&lt;/a&gt; from 2024.&lt;/p&gt; &lt;p&gt;I originally planned to pick up another pair of MI100s and an Infinity Fabric Bridge, and I picked up a lot of hardware upgrades over the course of 2025 in preparation for this. Notably, faster, double capacity memory (last February, well before the current price jump), another motherboard, higher capacity PSU, etc. But then I saw benchmarks for the R9700, particularly in the &lt;a href="https://github.com/ggml-org/llama.cpp/discussions/15021"&gt;llama.cpp ROCm thread&lt;/a&gt;, and saw the much better prompt processing performance for a small token generation loss. The MI100 also went up in price to about $1000, so factoring in the cost of a bridge, it'd come to about the same price. So I sold the MI100s, picked up 4 R9700s and called it a day.&lt;/p&gt; &lt;p&gt;Here's the specs and BOM. Note that the CPU and SSD were taken from the previous build, and the internal fans came bundled with the PSU as part of a deal:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Component&lt;/th&gt; &lt;th align="left"&gt;Description&lt;/th&gt; &lt;th align="left"&gt;Number&lt;/th&gt; &lt;th align="left"&gt;Unit Price&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;CPU&lt;/td&gt; &lt;td align="left"&gt;AMD Ryzen 7 5700X&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;$160.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;RAM&lt;/td&gt; &lt;td align="left"&gt;Corsair Vengance LPX 64GB (2 x 32GB) DDR4 3600MHz C18&lt;/td&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;$105.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPU&lt;/td&gt; &lt;td align="left"&gt;PowerColor AMD Radeon AI PRO R9700 32GB&lt;/td&gt; &lt;td align="left"&gt;4&lt;/td&gt; &lt;td align="left"&gt;$1,300.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Motherboard&lt;/td&gt; &lt;td align="left"&gt;MSI MEG X570 GODLIKE Motherboard&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;$490.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Storage&lt;/td&gt; &lt;td align="left"&gt;Inland Performance 1TB NVMe SSD&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;$100.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;PSU&lt;/td&gt; &lt;td align="left"&gt;Super Flower Leadex Titanium 1600W 80+ Titanium&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;$440.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Internal Fans&lt;/td&gt; &lt;td align="left"&gt;Super Flower MEGACOOL 120mm fan, Triple-Pack&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;$0.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Case Fans&lt;/td&gt; &lt;td align="left"&gt;Noctua NF-A14 iPPC-3000 PWM&lt;/td&gt; &lt;td align="left"&gt;6&lt;/td&gt; &lt;td align="left"&gt;$30.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;CPU Heatsink&lt;/td&gt; &lt;td align="left"&gt;AMD Wraith Prism aRGB CPU Cooler&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;$20.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Fan Hub&lt;/td&gt; &lt;td align="left"&gt;Noctua NA-FH1&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;$45.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Case&lt;/td&gt; &lt;td align="left"&gt;Phanteks Enthoo Pro 2 Server Edition&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;$190.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Total&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;$7,035.00&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;128GB VRAM, 128GB RAM for offloading, all for less than the price of a RTX 6000 Blackwell.&lt;/p&gt; &lt;p&gt;Some benchmarks:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;n_batch&lt;/th&gt; &lt;th align="left"&gt;n_ubatch&lt;/th&gt; &lt;th align="left"&gt;fa&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 7B Q4_0&lt;/td&gt; &lt;td align="left"&gt;3.56 GiB&lt;/td&gt; &lt;td align="left"&gt;6.74 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp8192&lt;/td&gt; &lt;td align="left"&gt;6524.91 ± 11.30&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 7B Q4_0&lt;/td&gt; &lt;td align="left"&gt;3.56 GiB&lt;/td&gt; &lt;td align="left"&gt;6.74 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;90.89 ± 0.41&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q8_0&lt;/td&gt; &lt;td align="left"&gt;33.51 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp8192&lt;/td&gt; &lt;td align="left"&gt;2113.82 ± 2.88&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q8_0&lt;/td&gt; &lt;td align="left"&gt;33.51 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;72.51 ± 0.27&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3vl 32B Q8_0&lt;/td&gt; &lt;td align="left"&gt;36.76 GiB&lt;/td&gt; &lt;td align="left"&gt;32.76 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp8192&lt;/td&gt; &lt;td align="left"&gt;1725.46 ± 5.93&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3vl 32B Q8_0&lt;/td&gt; &lt;td align="left"&gt;36.76 GiB&lt;/td&gt; &lt;td align="left"&gt;32.76 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;14.75 ± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 70B IQ4_XS - 4.25 bpw&lt;/td&gt; &lt;td align="left"&gt;35.29 GiB&lt;/td&gt; &lt;td align="left"&gt;70.55 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp8192&lt;/td&gt; &lt;td align="left"&gt;1110.02 ± 3.49&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 70B IQ4_XS - 4.25 bpw&lt;/td&gt; &lt;td align="left"&gt;35.29 GiB&lt;/td&gt; &lt;td align="left"&gt;70.55 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;14.53 ± 0.03&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3next 80B.A3B IQ4_XS - 4.25 bpw&lt;/td&gt; &lt;td align="left"&gt;39.71 GiB&lt;/td&gt; &lt;td align="left"&gt;79.67 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp8192&lt;/td&gt; &lt;td align="left"&gt;821.10 ± 0.27&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3next 80B.A3B IQ4_XS - 4.25 bpw&lt;/td&gt; &lt;td align="left"&gt;39.71 GiB&lt;/td&gt; &lt;td align="left"&gt;79.67 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;38.88 ± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;glm4moe ?B IQ4_XS - 4.25 bpw&lt;/td&gt; &lt;td align="left"&gt;54.33 GiB&lt;/td&gt; &lt;td align="left"&gt;106.85 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp8192&lt;/td&gt; &lt;td align="left"&gt;1928.45 ± 3.74&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;glm4moe ?B IQ4_XS - 4.25 bpw&lt;/td&gt; &lt;td align="left"&gt;54.33 GiB&lt;/td&gt; &lt;td align="left"&gt;106.85 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;48.09 ± 0.16&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;minimax-m2 230B.A10B IQ4_XS - 4.25 bpw&lt;/td&gt; &lt;td align="left"&gt;113.52 GiB&lt;/td&gt; &lt;td align="left"&gt;228.69 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp8192&lt;/td&gt; &lt;td align="left"&gt;2082.04 ± 4.49&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;minimax-m2 230B.A10B IQ4_XS - 4.25 bpw&lt;/td&gt; &lt;td align="left"&gt;113.52 GiB&lt;/td&gt; &lt;td align="left"&gt;228.69 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;48.78 ± 0.06&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;minimax-m2 230B.A10B Q8_0&lt;/td&gt; &lt;td align="left"&gt;226.43 GiB&lt;/td&gt; &lt;td align="left"&gt;228.69 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;30&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp8192&lt;/td&gt; &lt;td align="left"&gt;42.62 ± 7.96&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;minimax-m2 230B.A10B Q8_0&lt;/td&gt; &lt;td align="left"&gt;226.43 GiB&lt;/td&gt; &lt;td align="left"&gt;228.69 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;30&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;6.58 ± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;A few final observations:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;glm4 moe and minimax-m2 are actually GLM-4.6V and MiniMax-M2.1, respectively.&lt;/li&gt; &lt;li&gt;There's an open issue for Qwen3-Next at the moment; recent optimizations caused some pretty hefty prompt processing regressions. The numbers here are pre #18683, in case the exact issue gets resolved.&lt;/li&gt; &lt;li&gt;A word on the Q8 quant of MiniMax-M2.1; &lt;code&gt;--fit on&lt;/code&gt; isn't supported on llama-bench, so I can't give an apples to apples comparison to simply reducing the number of gpu layers, but it's also extremely unreliable for me in llama-server, giving me HIP error 906 on the first generation. Out of a dozen or so attempts, I've gotten it to work once, with a TG around 8.5 t/s, but take that with a grain of salt. Otherwise, maybe the quality jump is worth letting it run overnight? You be the judge. It also takes 2 hours to load, but that could be because I'm loading it off external storage.&lt;/li&gt; &lt;li&gt;The internal fan mount on the case only has screws on one side; in the intended configuration, the holes for power cables are on the opposite side of where the GPU power sockets are, meaning the power cables will block airflow from the fans. How they didn't see this, I have no idea. Thankfully, it stays in place from a friction fit if you flip it 180 like I did. Really, I probably could have gone without it, it was mostly a consideration for when I was still going with MI100s, but the fans were free anyway.&lt;/li&gt; &lt;li&gt;I really, really wanted to go AM5 for this, but there just isn't a board out there with 4 full sized PCIe slots spaced for 2 slot GPUs. At best you can fit 3 and then cover up one of them. But if you need a bazillion m.2 slots you're golden /s. You might then ask why I didn't go for Threadripper/Epyc, and that's because I was worried about power consumption and heat. I didn't want to mess with risers and open rigs, so I found the one AM4 board that could do this, even if it comes at the cost of RAM speeds/channels and slower PCIe speeds.&lt;/li&gt; &lt;li&gt;The MI100s and R9700s didn't play nice for the brief period of time I had 2 of both. I didn't bother troubleshooting, just shrugged and sold them off, so it may have been a simple fix but FYI.&lt;/li&gt; &lt;li&gt;Going with a 1 TB SSD in my original build was a mistake, even 2 would have made a world of difference. Between LLMs, image generation, TTS, ect. I'm having trouble actually taking advantage of the extra VRAM with less quantized models due to storage constraints, which is why my benchmarks still have a lot of 4-bit quants despite being able to easily do 8-bit ones.&lt;/li&gt; &lt;li&gt;I don't know how to control the little LCD display on the board. I'm not sure there is a way on Linux. A shame.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ulterior-Motive_"&gt; /u/Ulterior-Motive_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qfscp5"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfscp5/128gb_vram_quad_r9700_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qfscp5/128gb_vram_quad_r9700_server/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T23:30:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1qg48z8</id>
    <title>Newelle 1.2 released</title>
    <updated>2026-01-18T09:28:09+00:00</updated>
    <author>
      <name>/u/iTzSilver_YT</name>
      <uri>https://old.reddit.com/user/iTzSilver_YT</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qg48z8/newelle_12_released/"&gt; &lt;img alt="Newelle 1.2 released" src="https://b.thumbs.redditmedia.com/LkuhPhoU8yX94dk4Ih1TJttPbdGMxJp3kQLOX0r9qXA.jpg" title="Newelle 1.2 released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Newelle, AI assistant for Linux, has been updated to 1.2! You can download it from &lt;a href="https://flathub.org/en/apps/io.github.qwersyk.Newelle"&gt;FlatHub&lt;/a&gt;&lt;/p&gt; &lt;p&gt;⚡️ Add llama.cpp, with options to recompile it with any backend&lt;br /&gt; 📖 Implement a new model library for ollama / llama.cpp&lt;br /&gt; 🔎 Implement hybrid search, improving document reading&lt;/p&gt; &lt;p&gt;💻 Add command execution tool&lt;br /&gt; 🗂 Add tool groups&lt;br /&gt; 🔗 Improve MCP server adding, supporting also STDIO for non flatpak&lt;br /&gt; 📝 Add semantic memory handler&lt;br /&gt; 📤 Add ability to import/export chats&lt;br /&gt; 📁 Add custom folders to the RAG index&lt;br /&gt; ℹ️ Improved message information menu, showing the token count and token speed&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iTzSilver_YT"&gt; /u/iTzSilver_YT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qg48z8"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qg48z8/newelle_12_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qg48z8/newelle_12_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-18T09:28:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfv1ms</id>
    <title>Qwen 4 might be a long way off !? Lead Dev says they are "slowing down" to focus on quality.</title>
    <updated>2026-01-18T01:28:57+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfv1ms/qwen_4_might_be_a_long_way_off_lead_dev_says_they/"&gt; &lt;img alt="Qwen 4 might be a long way off !? Lead Dev says they are &amp;quot;slowing down&amp;quot; to focus on quality." src="https://preview.redd.it/ylsevy04f0eg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bf47eb2c12055fdb3e08f36d4d3746a234d630ff" title="Qwen 4 might be a long way off !? Lead Dev says they are &amp;quot;slowing down&amp;quot; to focus on quality." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ylsevy04f0eg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfv1ms/qwen_4_might_be_a_long_way_off_lead_dev_says_they/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qfv1ms/qwen_4_might_be_a_long_way_off_lead_dev_says_they/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-18T01:28:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1qg8yoh</id>
    <title>The sad state of the GPU market in Germany and EU, some of them are not even available</title>
    <updated>2026-01-18T13:45:52+00:00</updated>
    <author>
      <name>/u/HumanDrone8721</name>
      <uri>https://old.reddit.com/user/HumanDrone8721</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qg8yoh/the_sad_state_of_the_gpu_market_in_germany_and_eu/"&gt; &lt;img alt="The sad state of the GPU market in Germany and EU, some of them are not even available" src="https://preview.redd.it/9mmc603p34eg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7fe5b9d5a452e5ddb136ff1be2c60fdc5b0ed2c5" title="The sad state of the GPU market in Germany and EU, some of them are not even available" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HumanDrone8721"&gt; /u/HumanDrone8721 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9mmc603p34eg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qg8yoh/the_sad_state_of_the_gpu_market_in_germany_and_eu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qg8yoh/the_sad_state_of_the_gpu_market_in_germany_and_eu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-18T13:45:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1qgdb7f</id>
    <title>4x AMD R9700 (128GB VRAM) + Threadripper 9955WX Build</title>
    <updated>2026-01-18T16:39:42+00:00</updated>
    <author>
      <name>/u/NunzeCs</name>
      <uri>https://old.reddit.com/user/NunzeCs</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgdb7f/4x_amd_r9700_128gb_vram_threadripper_9955wx_build/"&gt; &lt;img alt="4x AMD R9700 (128GB VRAM) + Threadripper 9955WX Build" src="https://b.thumbs.redditmedia.com/bQ4SRK8dHDz2IGShLwX64vLIVj0fWUigDqG_dO43P-U.jpg" title="4x AMD R9700 (128GB VRAM) + Threadripper 9955WX Build" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Disclaimer: I am from Germany and my English is not perfect, so I used an LLM to help me structure and write this post.&lt;/p&gt; &lt;p&gt;Context &amp;amp; Motivation: I built this system for my small company. The main reason for all new hardware is that I received a 50% subsidy/refund from my local municipality for digitalization investments. To qualify for this funding, I had to buy new hardware and build a proper &amp;quot;server-grade&amp;quot; system.&lt;/p&gt; &lt;p&gt;My goal was to run large models (120B+) locally for data privacy. With the subsidy in mind, I had a budget of around 10,000€ (pre-refund). I initially considered NVIDIA, but I wanted to maximize VRAM. I decided to go with 4x AMD RDNA4 cards (ASRock R9700) to get 128GB VRAM total and used the rest of the budget for a solid Threadripper platform.&lt;/p&gt; &lt;p&gt;Hardware Specs:&lt;/p&gt; &lt;p&gt;Total Cost: ~9,800€ (I get ~50% back, so effectively ~4,900€ for me).&lt;/p&gt; &lt;p&gt;CPU: AMD Ryzen Threadripper PRO 9955WX (16 Cores) Mainboard: ASRock WRX90 WS EVO RAM: 128GB DDR5 5600MHz GPU: 4x ASRock Radeon AI PRO R9700 32GB (Total 128GB VRAM) Configuration: All cards running at full PCIe 5.0 x16 bandwidth. Storage: 2x 2TB PCIe 4.0 SSD PSU: Seasonic 2200W Cooling: Alphacool Eisbaer Pro Aurora 360 CPU AIO&lt;/p&gt; &lt;p&gt;Benchmark Results&lt;/p&gt; &lt;p&gt;I tested various models ranging from 8B to 230B parameters.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Llama.cpp (Focus: Single User Latency) Settings: Flash Attention ON, Batch 2048&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Model Size Quant Mode Prompt t/s Gen t/s Meta-Llama-3.1-8B-Instruct 8B Q4_K_M GPU-Full 3169.16 81.01 Qwen2.5-32B-Instruct 32B Q4_K_M GPU-Full 848.68 25.14 Meta-Llama-3.1-70B-Instruct 70B Q4_K_M GPU-Full 399.03 12.66 gpt-oss-120b 120B Q4_K_M GPU-Full 2977.83 97.47 GLM-4.7-REAP-218B 218B Q3_K_M GPU-Full 504.15 17.48 MiniMax-M2.1 ~230B Q4_K_M Hybrid 938.89 32.12&lt;/p&gt; &lt;p&gt;Side note: I found that with PCIe 5.0, standard Pipeline Parallelism (Layer Split) is significantly faster (~97 t/s) than Tensor Parallelism/Row Split (~67 t/s) for a single user on this setup.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;vLLM (Focus: Throughput) Model: GPT-OSS-120B (bfloat16), TP=4, test for 20 requests&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Total Throughput: ~314 tokens/s (Generation) Prompt Processing: ~5339 tokens/s Single user throughput 50 tokens/s&lt;/p&gt; &lt;p&gt;I used rocm 7.1.1 for llama.cpp also testet Vulkan but it was worse&lt;/p&gt; &lt;p&gt;If I could do it again, I would have used the budget to buy a single NVIDIA RTX Pro 6000 Blackwell (96GB). Maybe I will, if local AI is going well for my use case, I swap the R9700 with Pro 6000 in the future.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NunzeCs"&gt; /u/NunzeCs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qgdb7f"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgdb7f/4x_amd_r9700_128gb_vram_threadripper_9955wx_build/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qgdb7f/4x_amd_r9700_128gb_vram_threadripper_9955wx_build/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-18T16:39:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
