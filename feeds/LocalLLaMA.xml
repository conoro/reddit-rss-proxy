<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-14T17:06:40+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ow0jj0</id>
    <title>Running a 1 Trillion Parameter Model on a PC with 128 GB RAM + 24 GB VRAM</title>
    <updated>2025-11-13T13:05:10+00:00</updated>
    <author>
      <name>/u/pulse77</name>
      <uri>https://old.reddit.com/user/pulse77</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi again, just wanted to share that this time I've successfully run &lt;strong&gt;Kimi K2 Thinking (1T parameters)&lt;/strong&gt; on &lt;strong&gt;llama.cpp&lt;/strong&gt; using my desktop setup:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;CPU:&lt;/strong&gt; Intel i9-13900KS&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RAM:&lt;/strong&gt; 128 GB DDR5 @ 4800 MT/s&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPU:&lt;/strong&gt; RTX 4090 (24 GB VRAM)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Storage:&lt;/strong&gt; 4TB NVMe SSD (7300 MB/s read)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm using &lt;strong&gt;Unsloth UD-Q3_K_XL (~3.5 bits)&lt;/strong&gt; from Hugging Face: &lt;a href="https://huggingface.co/unsloth/Kimi-K2-Thinking-GGUF"&gt;https://huggingface.co/unsloth/Kimi-K2-Thinking-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Performance (generation speed):&lt;/strong&gt; 0.42 tokens/sec&lt;/p&gt; &lt;p&gt;(I know, it's slow... but it runs! I'm just stress-testing what's possible on consumer hardware...)&lt;/p&gt; &lt;p&gt;I also tested other huge models - here is a full list with speeds for comparison:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Parameters&lt;/th&gt; &lt;th align="left"&gt;Quant&lt;/th&gt; &lt;th align="left"&gt;Context&lt;/th&gt; &lt;th align="left"&gt;Speed (t/s)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Kimi K2 Thinking&lt;/td&gt; &lt;td align="left"&gt;1T A32B&lt;/td&gt; &lt;td align="left"&gt;UD-Q3_K_XL&lt;/td&gt; &lt;td align="left"&gt;128K&lt;/td&gt; &lt;td align="left"&gt;0.42&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Kimi K2 Instruct 0905&lt;/td&gt; &lt;td align="left"&gt;1T A32B&lt;/td&gt; &lt;td align="left"&gt;UD-Q3_K_XL&lt;/td&gt; &lt;td align="left"&gt;128K&lt;/td&gt; &lt;td align="left"&gt;0.44&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;DeepSeek V3.1 Terminus&lt;/td&gt; &lt;td align="left"&gt;671B A37B&lt;/td&gt; &lt;td align="left"&gt;UD-Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;128K&lt;/td&gt; &lt;td align="left"&gt;0.34&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 Coder 480B Instruct&lt;/td&gt; &lt;td align="left"&gt;480B A35B&lt;/td&gt; &lt;td align="left"&gt;UD-Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;128K&lt;/td&gt; &lt;td align="left"&gt;1.0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GLM 4.6&lt;/td&gt; &lt;td align="left"&gt;355B A32B&lt;/td&gt; &lt;td align="left"&gt;UD-Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;128K&lt;/td&gt; &lt;td align="left"&gt;0.82&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 235B Thinking&lt;/td&gt; &lt;td align="left"&gt;235B A22B&lt;/td&gt; &lt;td align="left"&gt;UD-Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;128K&lt;/td&gt; &lt;td align="left"&gt;5.5&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 235B Instruct&lt;/td&gt; &lt;td align="left"&gt;235B A22B&lt;/td&gt; &lt;td align="left"&gt;UD-Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;128K&lt;/td&gt; &lt;td align="left"&gt;5.6&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MiniMax M2&lt;/td&gt; &lt;td align="left"&gt;230B A10B&lt;/td&gt; &lt;td align="left"&gt;UD-Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;128K&lt;/td&gt; &lt;td align="left"&gt;8.5&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GLM 4.5 Air&lt;/td&gt; &lt;td align="left"&gt;106B A12B&lt;/td&gt; &lt;td align="left"&gt;UD-Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;128K&lt;/td&gt; &lt;td align="left"&gt;11.2&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPT OSS 120B&lt;/td&gt; &lt;td align="left"&gt;120B A5.1B&lt;/td&gt; &lt;td align="left"&gt;MXFP4&lt;/td&gt; &lt;td align="left"&gt;128K&lt;/td&gt; &lt;td align="left"&gt;25.5&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IBM Granite 4.0 H Small&lt;/td&gt; &lt;td align="left"&gt;32B A9B&lt;/td&gt; &lt;td align="left"&gt;UD-Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;128K&lt;/td&gt; &lt;td align="left"&gt;72.2&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 30B Thinking&lt;/td&gt; &lt;td align="left"&gt;30B A3B&lt;/td&gt; &lt;td align="left"&gt;UD-Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;120K&lt;/td&gt; &lt;td align="left"&gt;197.2&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 30B Instruct&lt;/td&gt; &lt;td align="left"&gt;30B A3B&lt;/td&gt; &lt;td align="left"&gt;UD-Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;120K&lt;/td&gt; &lt;td align="left"&gt;218.8&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 30B Coder Instruct&lt;/td&gt; &lt;td align="left"&gt;30B A3B&lt;/td&gt; &lt;td align="left"&gt;UD-Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;120K&lt;/td&gt; &lt;td align="left"&gt;211.2&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPT OSS 20B&lt;/td&gt; &lt;td align="left"&gt;20B A3.6B&lt;/td&gt; &lt;td align="left"&gt;MXFP4&lt;/td&gt; &lt;td align="left"&gt;128K&lt;/td&gt; &lt;td align="left"&gt;223.3&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Command line used (llama.cpp):&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;llama-server --threads 32 --jinja --flash-attn on --cache-type-k q8_0 --cache-type-v q8_0 --model &amp;lt;PATH-TO-YOUR-MODEL&amp;gt; --ctx-size 131072 --n-cpu-moe 9999 --no-warmup&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Important:&lt;/strong&gt; Use &lt;em&gt;--no-warmup&lt;/em&gt; - otherwise, the process can crash before startup.&lt;/p&gt; &lt;p&gt;Notes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Memory mapping (mmap)&lt;/strong&gt; in llama.cpp lets it read model files far beyond RAM capacity.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;No swap/pagefile&lt;/strong&gt; - I disabled these to prevent SSD wear (no disk writes during inference).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Context size:&lt;/strong&gt; Reducing context length didn't improve speed for huge models (token/sec stayed roughly the same).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPU offload:&lt;/strong&gt; llama.cpp automatically uses GPU for all layers unless you limit it. I only use --n-cpu-moe 9999 to keep MoE layers on CPU.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Quantization:&lt;/strong&gt; Anything below ~4 bits noticeably reduces quality. Lowest meaningful quantization for me is UD-Q3_K_XL.&lt;/li&gt; &lt;li&gt;Tried &lt;strong&gt;UD-Q4_K_XL&lt;/strong&gt; for Kimi models, but it failed to start. UD-Q3_K_XL is the max stable setup on my rig.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Speed test method:&lt;/strong&gt; Each benchmark was done using the same prompt - &amp;quot;Explain quantum computing&amp;quot;. The measurement covers the entire generation process until the model finishes its response (so, true end-to-end inference speed).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;llama.cpp version:&lt;/strong&gt; b6963 ‚Äî all tests were run on this version.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;TL;DR - Yes&lt;/strong&gt;, it's possible to run (slowly) a &lt;strong&gt;1-trillion-parameter LLM&lt;/strong&gt; on a machine with &lt;strong&gt;128 GB RAM + 24 GB VRAM&lt;/strong&gt; - no cluster or cloud required. Mostly an experiment to see where the limits really are.&lt;/p&gt; &lt;p&gt;EDIT: Fixed info about IBM Granite model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pulse77"&gt; /u/pulse77 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ow0jj0/running_a_1_trillion_parameter_model_on_a_pc_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ow0jj0/running_a_1_trillion_parameter_model_on_a_pc_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ow0jj0/running_a_1_trillion_parameter_model_on_a_pc_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-13T13:05:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1owqp8b</id>
    <title>Built a simple tool for long-form text-to-speech + multivoice narration (Kokoro Story)</title>
    <updated>2025-11-14T08:04:25+00:00</updated>
    <author>
      <name>/u/Xerophayze</name>
      <uri>https://old.reddit.com/user/Xerophayze</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been experimenting a lot with the Kokoro TTS model lately and ended up building a small project to make it easier for people to generate long text-to-speech audio and multi-voice narratives without having to piece everything together manually.&lt;/p&gt; &lt;p&gt;If you‚Äôve ever wanted to feed in long passages, stories, or scripts and have them automatically broken up, voiced, and exported, this might help. I put the code on GitHub here:&lt;/p&gt; &lt;p&gt;üîó &lt;a href="https://github.com/Xerophayze/Kokoro-Story"&gt;&lt;strong&gt;https://github.com/Xerophayze/Kokoro-Story&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It‚Äôs nothing fancy, but it solves a problem I kept running into, so I figured others might find it useful too. I really think Kokoro has a ton of potential and deserves more active development‚Äîit's one of the best-sounding non-cloud TTS systems I‚Äôve worked with, especially for multi-voice output.&lt;/p&gt; &lt;p&gt;If anyone wants to try it out, improve it, or suggest features, I‚Äôd love the feedback.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xerophayze"&gt; /u/Xerophayze &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owqp8b/built_a_simple_tool_for_longform_texttospeech/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owqp8b/built_a_simple_tool_for_longform_texttospeech/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1owqp8b/built_a_simple_tool_for_longform_texttospeech/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-14T08:04:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1ox12cj</id>
    <title>Open-source local Claude-Code alternative for DevOps - looking for beta testers</title>
    <updated>2025-11-14T16:24:49+00:00</updated>
    <author>
      <name>/u/apinference</name>
      <uri>https://old.reddit.com/user/apinference</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been working on a small open-source project - a local Claude-Code-style assistant built with ollama.&lt;/p&gt; &lt;p&gt;It runs entirely offline, uses a locally trained model optimised for speed, and can handle practical DevOps tasks like reading/writing files, running shell commands, and checking env vars.&lt;/p&gt; &lt;p&gt;Core ideas:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Local model (Ollama), uses only ~1.1 GB RAM (kept small for DevOps use)&lt;/li&gt; &lt;li&gt;Speed optimised - after initial load it responds in about 7‚Äì10 seconds&lt;/li&gt; &lt;li&gt;No data leaking, no APIs, no telemetry, no subscriptions&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/ubermorgenland/devops-agent"&gt;https://github.com/ubermorgenland/devops-agent&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It‚Äôs early-stage, but working - would love a few beta testers to try it locally and share feedback or ideas for new tools.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/apinference"&gt; /u/apinference &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ox12cj/opensource_local_claudecode_alternative_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ox12cj/opensource_local_claudecode_alternative_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ox12cj/opensource_local_claudecode_alternative_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-14T16:24:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ow3anq</id>
    <title>Rejected for not using LangChain/LangGraph?</title>
    <updated>2025-11-13T15:00:14+00:00</updated>
    <author>
      <name>/u/dougeeai</name>
      <uri>https://old.reddit.com/user/dougeeai</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Today I got rejected after a job interview for not being &amp;quot;technical enough&amp;quot; because I use PyTorch/CUDA/GGUF directly with FastAPI microservices for multi-agent systems instead of LangChain/LangGraph in production.&lt;/p&gt; &lt;p&gt;They asked about 'efficient data movement in LangGraph' - I explained I work at a lower level with bare metal for better performance and control. Later it was revealed they mostly just use APIs to Claude/OpenAI/Bedrock.&lt;/p&gt; &lt;p&gt;I am legitimately asking - not venting - Am I missing something by not using LangChain? Is it becoming a required framework for AI engineering roles, or is this just framework bias?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Should I be adopting it even though I haven't seen performance benefits for my use cases?&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dougeeai"&gt; /u/dougeeai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ow3anq/rejected_for_not_using_langchainlanggraph/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ow3anq/rejected_for_not_using_langchainlanggraph/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ow3anq/rejected_for_not_using_langchainlanggraph/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-13T15:00:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1ovxksu</id>
    <title>Jan-v2-VL: 8B model for long-horizon tasks, improving Qwen3-VL-8B‚Äôs agentic capabilities almost 10x</title>
    <updated>2025-11-13T10:22:48+00:00</updated>
    <author>
      <name>/u/Delicious_Focus3465</name>
      <uri>https://old.reddit.com/user/Delicious_Focus3465</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovxksu/janv2vl_8b_model_for_longhorizon_tasks_improving/"&gt; &lt;img alt="Jan-v2-VL: 8B model for long-horizon tasks, improving Qwen3-VL-8B‚Äôs agentic capabilities almost 10x" src="https://external-preview.redd.it/bmthZnk4cjV4ejBnMYgdXr3Xr8K8l3LMKEIqfiXLStzaSkNnB6704_pmF3PX.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0626efa53bd219b2126a6e5fa2884ec700c482b3" title="Jan-v2-VL: 8B model for long-horizon tasks, improving Qwen3-VL-8B‚Äôs agentic capabilities almost 10x" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, this is Bach from the Jan team. We‚Äôre releasing Jan-v2-VL, an 8B vision‚Äìlanguage model aimed at long-horizon, multi-step tasks starting from browser use.&lt;/p&gt; &lt;p&gt;Jan-v2-VL-high executes 49 steps without failure on the Long-Horizon Execution benchmark, while the base model (Qwen3-VL-8B-Thinking) stops at 5 and other similar-scale VLMs stop between 1 and 2.&lt;/p&gt; &lt;p&gt;Across text and multimodal benchmarks, it matches or slightly improves on the base model, so you get higher long-horizon stability without giving up reasoning or vision quality.&lt;/p&gt; &lt;p&gt;We're releasing 3 variants:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Jan-v2-VL-low (efficiency-oriented)&lt;/li&gt; &lt;li&gt;Jan-v2-VL-med (balanced)&lt;/li&gt; &lt;li&gt;Jan-v2-VL-high (deeper reasoning and longer execution)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;How to run the model&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Download Jan-v2-VL from the Model Hub in Jan&lt;/li&gt; &lt;li&gt;Open the model‚Äôs settings and enable Tools and Vision&lt;/li&gt; &lt;li&gt;Enable BrowserUse MCP (or your preferred MCP setup for browser control)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can also run the model with vLLM or llama.cpp.&lt;/p&gt; &lt;p&gt;Recommended parameters&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;temperature: 1.0&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;top_p: 0.95&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;top_k: 20&lt;/code&gt;&lt;/li&gt; &lt;li&gt;repetition_penalty&lt;code&gt;: 1.0&lt;/code&gt;&lt;/li&gt; &lt;li&gt;presence_penalty&lt;code&gt;: 1.5&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/collections/janhq/jan-v2-vl"&gt;https://huggingface.co/collections/janhq/jan-v2-vl&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Jan app: &lt;a href="https://github.com/janhq/jan"&gt;https://github.com/janhq/jan&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We're also working on a browser extension to make model-driven browser automation faster and more reliable on top of this.&lt;/p&gt; &lt;p&gt;Credit to the Qwen team for the Qwen3-VL-8B-Thinking base model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Delicious_Focus3465"&gt; /u/Delicious_Focus3465 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/go4j38r5xz0g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovxksu/janv2vl_8b_model_for_longhorizon_tasks_improving/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ovxksu/janv2vl_8b_model_for_longhorizon_tasks_improving/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-13T10:22:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1ow9pdh</id>
    <title>new ops required by Qwen3 Next and Kimi Linear have been merged into llama.cpp</title>
    <updated>2025-11-13T19:00:34+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ow9pdh/new_ops_required_by_qwen3_next_and_kimi_linear/"&gt; &lt;img alt="new ops required by Qwen3 Next and Kimi Linear have been merged into llama.cpp" src="https://external-preview.redd.it/5ziszOa8NRon-ATgGFg5Bv3PXC9P_Gr-hIwXsD0snnU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=61860c512fcc2dad6ebe8431387929cdf3acb61d" title="new ops required by Qwen3 Next and Kimi Linear have been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen3 Next is still in progress &lt;a href="https://github.com/ggml-org/llama.cpp/pull/16095"&gt;https://github.com/ggml-org/llama.cpp/pull/16095&lt;/a&gt;&lt;/p&gt; &lt;p&gt;but this merge was needed to unblock it&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/17063"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ow9pdh/new_ops_required_by_qwen3_next_and_kimi_linear/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ow9pdh/new_ops_required_by_qwen3_next_and_kimi_linear/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-13T19:00:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1ow3kj3</id>
    <title>Qwen model coming soon üëÄ</title>
    <updated>2025-11-13T15:10:39+00:00</updated>
    <author>
      <name>/u/Odd-Ordinary-5922</name>
      <uri>https://old.reddit.com/user/Odd-Ordinary-5922</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ow3kj3/qwen_model_coming_soon/"&gt; &lt;img alt="Qwen model coming soon üëÄ" src="https://preview.redd.it/ibsrtr3ri11g1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=91dd786919de8cd49f495890d2b241fd22bf2f83" title="Qwen model coming soon üëÄ" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Odd-Ordinary-5922"&gt; /u/Odd-Ordinary-5922 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ibsrtr3ri11g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ow3kj3/qwen_model_coming_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ow3kj3/qwen_model_coming_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-13T15:10:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1owpxdd</id>
    <title>Anyone trying out Motif 2 13B?</title>
    <updated>2025-11-14T07:15:12+00:00</updated>
    <author>
      <name>/u/RobotRobotWhatDoUSee</name>
      <uri>https://old.reddit.com/user/RobotRobotWhatDoUSee</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just saw that a S Korean group released this model: &lt;a href="https://huggingface.co/collections/Motif-Technologies/motif-2-127b"&gt;Motif 2 12.7 B&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The benchmarks appear impressive for the size (whatever they are worth).&lt;/p&gt; &lt;p&gt;Has anyone tried this model yet?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RobotRobotWhatDoUSee"&gt; /u/RobotRobotWhatDoUSee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owpxdd/anyone_trying_out_motif_2_13b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owpxdd/anyone_trying_out_motif_2_13b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1owpxdd/anyone_trying_out_motif_2_13b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-14T07:15:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ownirj</id>
    <title>MCP is great in theory, but it‚Äôs not always a blanket yes</title>
    <updated>2025-11-14T04:56:53+00:00</updated>
    <author>
      <name>/u/Miserable_Agent_9006</name>
      <uri>https://old.reddit.com/user/Miserable_Agent_9006</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been building agentic workflows in production lately and spent some time exploring MCP. It‚Äôs clean, standardized, and clearly the direction things are headed.&lt;/p&gt; &lt;p&gt;But I think when you're trying to move fast, it‚Äôs a bit heavy.&lt;/p&gt; &lt;p&gt;- another server to run and maintain&lt;/p&gt; &lt;p&gt;- extra network hops&lt;/p&gt; &lt;p&gt;- schema wrapping + versioning overhead&lt;/p&gt; &lt;p&gt;The lightweight ‚Äúhandshake‚Äù between agents and APIs works well enough for now. MCP makes sense when you‚Äôve got scale, multiple services, or teams to align.&lt;/p&gt; &lt;p&gt;I‚Äôm sure we‚Äôll adopt it eventually, but for now my team and I decided to skip it.&lt;/p&gt; &lt;p&gt;Anyone else taking a similar approach?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Miserable_Agent_9006"&gt; /u/Miserable_Agent_9006 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ownirj/mcp_is_great_in_theory_but_its_not_always_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ownirj/mcp_is_great_in_theory_but_its_not_always_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ownirj/mcp_is_great_in_theory_but_its_not_always_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-14T04:56:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1ox1icc</id>
    <title>LLMs from emacs</title>
    <updated>2025-11-14T16:40:47+00:00</updated>
    <author>
      <name>/u/GregariousWolf</name>
      <uri>https://old.reddit.com/user/GregariousWolf</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ox1icc/llms_from_emacs/"&gt; &lt;img alt="LLMs from emacs" src="https://b.thumbs.redditmedia.com/SNSWRjSxujhbT9r7PCLy1iybEubGT5fYirJKDo0GAjY.jpg" title="LLMs from emacs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/b3l97npl391g1.jpg?width=2560&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=4d005ea34a7167bbfc909c90188f299159d792cd"&gt;https://preview.redd.it/b3l97npl391g1.jpg?width=2560&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=4d005ea34a7167bbfc909c90188f299159d792cd&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I've been working on the home lab doing Linux stuff and testing out my LLM orchestration tool. It's not really meant to be used like this. What you see is a utility view to see all the buffers that are open. What it really looks like is emacs because you're editing and compiling and debugging. It started as a convenient way to get a buffer to and fro. Here I can connect them with a pipe, broadcast to multiple models at once, send two outputs to a third for comparison.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GregariousWolf"&gt; /u/GregariousWolf &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ox1icc/llms_from_emacs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ox1icc/llms_from_emacs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ox1icc/llms_from_emacs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-14T16:40:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ow8j6d</id>
    <title>The return of the modded 4090 48GB</title>
    <updated>2025-11-13T18:17:12+00:00</updated>
    <author>
      <name>/u/king_priam_of_Troy</name>
      <uri>https://old.reddit.com/user/king_priam_of_Troy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ow8j6d/the_return_of_the_modded_4090_48gb/"&gt; &lt;img alt="The return of the modded 4090 48GB" src="https://b.thumbs.redditmedia.com/LgVvEhALpx4xDsp8AmNOoxzbQcsL_pEGUDe4iwXtJ-M.jpg" title="The return of the modded 4090 48GB" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Last month I bought a 4090 48GB in ShenZhen. I had to put this project on hold for a while but it's back.&lt;/p&gt; &lt;p&gt;The card is really fast even with my poor Gen3 4x PCIe connector. I can't put it inside as I can't find any compatible power cable.&lt;/p&gt; &lt;p&gt;I'm running at 150 tokens/second with GPT-OSS 20B from my first tests.&lt;/p&gt; &lt;p&gt;(This is a follow up of &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nifajh/i%5C_bought%5C_a%5C_modded%5C_4090%5C_48gb%5C_in%5C_shenzhen%5C_this%5C_is/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1nifajh/i\_bought\_a\_modded\_4090\_48gb\_in\_shenzhen\_this\_is/&lt;/a&gt;)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/king_priam_of_Troy"&gt; /u/king_priam_of_Troy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ow8j6d"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ow8j6d/the_return_of_the_modded_4090_48gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ow8j6d/the_return_of_the_modded_4090_48gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-13T18:17:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1owxob9</id>
    <title>Why aren't there cheap NVLink adapters for RTX 3090s?</title>
    <updated>2025-11-14T14:17:43+00:00</updated>
    <author>
      <name>/u/alex_bit_</name>
      <uri>https://old.reddit.com/user/alex_bit_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is the NVLink only a wire jumper linking both cards together?&lt;/p&gt; &lt;p&gt;Can I make my own homemade connections?&lt;/p&gt; &lt;p&gt;Or are there some chips or other things inside the bridge?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/alex_bit_"&gt; /u/alex_bit_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owxob9/why_arent_there_cheap_nvlink_adapters_for_rtx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owxob9/why_arent_there_cheap_nvlink_adapters_for_rtx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1owxob9/why_arent_there_cheap_nvlink_adapters_for_rtx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-14T14:17:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1owulxd</id>
    <title>Kimi k2 thinking + kilo code really not bad</title>
    <updated>2025-11-14T12:00:43+00:00</updated>
    <author>
      <name>/u/Federal_Spend2412</name>
      <uri>https://old.reddit.com/user/Federal_Spend2412</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm genuinely impressed. Once your AGENTS.md and rules.md are clear enough, kimi k2 thinking + kilo code really seems to be just as capable as Claude 4.0 sonnet, especially when it comes to programming and debugging. It‚Äôs a surprisingly powerful combination.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Federal_Spend2412"&gt; /u/Federal_Spend2412 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owulxd/kimi_k2_thinking_kilo_code_really_not_bad/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owulxd/kimi_k2_thinking_kilo_code_really_not_bad/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1owulxd/kimi_k2_thinking_kilo_code_really_not_bad/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-14T12:00:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1owu2nn</id>
    <title>We built a framework for generating custom RAG evaluation datasets and released a D&amp;D-based one (open-source)</title>
    <updated>2025-11-14T11:31:58+00:00</updated>
    <author>
      <name>/u/mario_candela</name>
      <uri>https://old.reddit.com/user/mario_candela</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owu2nn/we_built_a_framework_for_generating_custom_rag/"&gt; &lt;img alt="We built a framework for generating custom RAG evaluation datasets and released a D&amp;amp;D-based one (open-source)" src="https://external-preview.redd.it/WM9coASZAPosxQnRrgryGmXh5OYtno3uUsf9XZYu3E8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=40dfbae3947d96d45263d794ae5426b261117e4f" title="We built a framework for generating custom RAG evaluation datasets and released a D&amp;amp;D-based one (open-source)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;üîó &lt;a href="https://datapizza.tech/it/blog/aij4r/"&gt;Blog post&lt;/a&gt;&lt;br /&gt; üîó &lt;a href="https://github.com/datapizza-labs/rag-dataset-builder"&gt;GitHub repo&lt;/a&gt;&lt;br /&gt; üîó &lt;a href="https://huggingface.co/datasets/datapizza-ai-lab/dnd5e-srd-qa"&gt;Dataset on Hugging Face&lt;/a&gt;&lt;br /&gt; Would love to hear your thoughts, feedback, or ideas on how to improve this! ‚ù§Ô∏è&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mario_candela"&gt; /u/mario_candela &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="http://datapizza.tech/it/blog/aij4r/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owu2nn/we_built_a_framework_for_generating_custom_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1owu2nn/we_built_a_framework_for_generating_custom_rag/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-14T11:31:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1owszii</id>
    <title>Hits different now</title>
    <updated>2025-11-14T10:30:03+00:00</updated>
    <author>
      <name>/u/lfiction</name>
      <uri>https://old.reddit.com/user/lfiction</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owszii/hits_different_now/"&gt; &lt;img alt="Hits different now" src="https://preview.redd.it/t0yq5cko971g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d7faccd0b39cc19dfb7a62aecffe57e0c87dc7d3" title="Hits different now" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hadn‚Äôt seen this in ages.. I don‚Äôt have opinions on AGI either way at this point, but this scene sure hits a lot harder now than it did back then! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lfiction"&gt; /u/lfiction &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/t0yq5cko971g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owszii/hits_different_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1owszii/hits_different_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-14T10:30:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1ow6a9i</id>
    <title>IBM's AI Researchers Patented a 200 yr old Math Technique by Rebranding as AI Interpretability</title>
    <updated>2025-11-13T16:54:38+00:00</updated>
    <author>
      <name>/u/DataBaeBee</name>
      <uri>https://old.reddit.com/user/DataBaeBee</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ow6a9i/ibms_ai_researchers_patented_a_200_yr_old_math/"&gt; &lt;img alt="IBM's AI Researchers Patented a 200 yr old Math Technique by Rebranding as AI Interpretability" src="https://external-preview.redd.it/bnA5cnNld3owMjFnMV58D9bda3Jb0zpLqYjHalvpbPpYKPrlCJRkL-iXGaPt.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fc24dc8d853c78013954d7d9f658801ad9c1a4e7" title="IBM's AI Researchers Patented a 200 yr old Math Technique by Rebranding as AI Interpretability" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;IBM AI researchers implemented a Continued Fraction class as linear layers in Pytorch and was awarded a patent for calling backward() on the computation graph. It's pretty bizarre.&lt;/p&gt; &lt;p&gt;Anyone who uses derivatives/power series to work with continued fractions is affected.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Mechanical engineers, Robotics and Industrialists - you can't use Pytorch to find the best number of teeth for your desired gear ratios lest you interfere with IBM's patent.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Pure Mathematicians and Math Educators - I learnt about the patent while investigating Continued Fractions and their relation to elliptic curves. I needed to find an approximate relationship and while I was writing in Torch I stumbled upon the patent.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Numerical programmers - continued fractions and their derivatives are used to approximate errors in algorithm design.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Here's the &lt;a href="https://leetarxiv.substack.com/p/ibm-patented-eulers-fractions"&gt;complete writeup with patent links&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DataBaeBee"&gt; /u/DataBaeBee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/nddv4ewz021g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ow6a9i/ibms_ai_researchers_patented_a_200_yr_old_math/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ow6a9i/ibms_ai_researchers_patented_a_200_yr_old_math/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-13T16:54:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1owyk52</id>
    <title>distil-localdoc.py - SLM assistant for writing Python documentation</title>
    <updated>2025-11-14T14:52:32+00:00</updated>
    <author>
      <name>/u/party-horse</name>
      <uri>https://old.reddit.com/user/party-horse</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owyk52/distillocaldocpy_slm_assistant_for_writing_python/"&gt; &lt;img alt="distil-localdoc.py - SLM assistant for writing Python documentation" src="https://preview.redd.it/phzq8yxnj81g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=07e950f1db9905480e03d620bbe8e37210ea60dc" title="distil-localdoc.py - SLM assistant for writing Python documentation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We built an SLM assistant for automatic Python documentation - a Qwen3 0.6B parameter model that generates complete, properly formatted docstrings for your code in Google style. Run it locally, keeping your proprietary code secure! Find it at &lt;a href="https://github.com/distil-labs/distil-localdoc.py"&gt;https://github.com/distil-labs/distil-localdoc.py&lt;/a&gt;&lt;/p&gt; &lt;h2&gt;Usage&lt;/h2&gt; &lt;p&gt;We load the model and your Python file. By default we load the downloaded Qwen3 0.6B model and generate Google-style docstrings.&lt;/p&gt; &lt;p&gt;```bash python localdoc.py --file your_script.py&lt;/p&gt; &lt;h1&gt;optionally, specify model and docstring style&lt;/h1&gt; &lt;p&gt;python localdoc.py --file your_script.py --model localdoc_qwen3 --style google ```&lt;/p&gt; &lt;p&gt;The tool will generate an updated file with &lt;code&gt;_documented&lt;/code&gt; suffix (e.g., &lt;code&gt;your_script_documented.py&lt;/code&gt;).&lt;/p&gt; &lt;h2&gt;Features&lt;/h2&gt; &lt;p&gt;The assistant can generate docstrings for: - &lt;strong&gt;Functions&lt;/strong&gt;: Complete parameter descriptions, return values, and raised exceptions - &lt;strong&gt;Methods&lt;/strong&gt;: Instance and class method documentation with proper formatting. The tool skips double underscore (dunder: __xxx) methods.&lt;/p&gt; &lt;h2&gt;Examples&lt;/h2&gt; &lt;p&gt;Feel free to run them yourself using the files in [examples](examples)&lt;/p&gt; &lt;h3&gt;Before:&lt;/h3&gt; &lt;p&gt;&lt;code&gt;python def calculate_total(items, tax_rate=0.08, discount=None): subtotal = sum(item['price'] * item['quantity'] for item in items) if discount: subtotal *= (1 - discount) return subtotal * (1 + tax_rate) &lt;/code&gt;&lt;/p&gt; &lt;h3&gt;After (Google style):&lt;/h3&gt; &lt;p&gt;```python def calculate_total(items, tax_rate=0.08, discount=None): &amp;quot;&amp;quot;&amp;quot; Calculate the total cost of items, applying a tax rate and optionally a discount.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Args: items: List of item objects with price and quantity tax_rate: Tax rate expressed as a decimal (default 0.08) discount: Discount rate expressed as a decimal; if provided, the subtotal is multiplied by (1 - discount) Returns: Total amount after applying the tax Example: &amp;gt;&amp;gt;&amp;gt; items = [{'price': 10, 'quantity': 2}, {'price': 5, 'quantity': 1}] &amp;gt;&amp;gt;&amp;gt; calculate_total(items, tax_rate=0.1, discount=0.05) 22.5 &amp;quot;&amp;quot;&amp;quot; subtotal = sum(item['price'] * item['quantity'] for item in items) if discount: subtotal *= (1 - discount) return subtotal * (1 + tax_rate) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;```&lt;/p&gt; &lt;h2&gt;FAQ&lt;/h2&gt; &lt;p&gt;&lt;strong&gt;Q: Why don't we just use GPT-4/Claude API for this?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Because your proprietary code shouldn't leave your infrastructure. Cloud APIs create security risks, compliance issues, and ongoing costs. Our models run locally with comparable quality.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Q: Can I document existing docstrings or update them?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Currently, the tool only adds missing docstrings. Updating existing documentation is planned for future releases. For now, you can manually remove docstrings you want regenerated.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Q: Which docstring style can I use?&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Google&lt;/strong&gt;: Most readable, great for general Python projects&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Q: The model does not work as expected&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;A: The tool calling on our platform is in active development! &lt;a href="https://www.linkedin.com/company/distil-labs/"&gt;Follow us on LinkedIn&lt;/a&gt; for updates, or &lt;a href="https://join.slack.com/t/distil-labs-community/shared_invite/zt-36zqj87le-i3quWUn2bjErRq22xoE58g"&gt;join our community&lt;/a&gt;. You can also manually refine any generated docstrings.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Q: Can you train a model for my company's documentation standards?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;A: Visit our &lt;a href="https://www.distillabs.ai"&gt;website&lt;/a&gt; and reach out to us, we offer custom solutions tailored to your coding standards and domain-specific requirements.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Q: Does this support type hints or other Python documentation tools?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;A: Type hints are parsed and incorporated into docstrings. Integration with tools like pydoc, Sphinx, and MkDocs is on our roadmap.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/party-horse"&gt; /u/party-horse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/phzq8yxnj81g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owyk52/distillocaldocpy_slm_assistant_for_writing_python/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1owyk52/distillocaldocpy_slm_assistant_for_writing_python/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-14T14:52:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1ows6z3</id>
    <title>Kimi k2 thinking vs Claude Sonnet</title>
    <updated>2025-11-14T09:41:28+00:00</updated>
    <author>
      <name>/u/sebastianmicu24</name>
      <uri>https://old.reddit.com/user/sebastianmicu24</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I will add my personal experience with kimi k2 thinking for my usecase since I saw contrasting opinions. &lt;/p&gt; &lt;p&gt;I needed to cluster some cells from a csv file to see if it would be achievable with my data to do some unsupervised classification of tumor cell/healthy cell. &lt;/p&gt; &lt;p&gt;I tried with claude sonnet 4 and after 2$ in api calls and a bunch of prompts i got no result, it was clustering 99.9% of cells into one group and 0.1% into the other. It was also having difficulties into rendering the cells from the x y positions in the csv. &lt;/p&gt; &lt;p&gt;Kimi k2 thinking achieved a proper clustering in 2 prompts (one for preprocessing of csv data, and one for clustering, maybe it could have done the same in 1 prompt). Total cost 0.17$&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sebastianmicu24"&gt; /u/sebastianmicu24 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ows6z3/kimi_k2_thinking_vs_claude_sonnet/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ows6z3/kimi_k2_thinking_vs_claude_sonnet/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ows6z3/kimi_k2_thinking_vs_claude_sonnet/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-14T09:41:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1owmtkt</id>
    <title>I brought CUDA back to macOS. Not because it was useful ‚Äî because nobody else could.</title>
    <updated>2025-11-14T04:20:16+00:00</updated>
    <author>
      <name>/u/Adept_Tip8375</name>
      <uri>https://old.reddit.com/user/Adept_Tip8375</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owmtkt/i_brought_cuda_back_to_macos_not_because_it_was/"&gt; &lt;img alt="I brought CUDA back to macOS. Not because it was useful ‚Äî because nobody else could." src="https://external-preview.redd.it/YHp6xAwqBe8oZ_OrMdwTyJjRYCv9-wbk4V-lSqlUI3I.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6e28774fdaaba15ba19d667058a3967b4695ebc8" title="I brought CUDA back to macOS. Not because it was useful ‚Äî because nobody else could." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;just resurrected CUDA on High Sierra in 2025&lt;br /&gt; Apple killed it 2018, NVIDIA killed drivers 2021&lt;br /&gt; now my 1080 Ti is doing 11 TFLOPs under PyTorch again&lt;br /&gt; ‚Äúimpossible‚Äù they said&lt;br /&gt; &lt;a href="https://github.com/careunix/PyTorch-HighSierra-CUDA-Revival"&gt;https://github.com/careunix/PyTorch-HighSierra-CUDA-Revival&lt;/a&gt;&lt;br /&gt; who still runs 10.13 in 2025 üòÇ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Adept_Tip8375"&gt; /u/Adept_Tip8375 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owmtkt/i_brought_cuda_back_to_macos_not_because_it_was/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owmtkt/i_brought_cuda_back_to_macos_not_because_it_was/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1owmtkt/i_brought_cuda_back_to_macos_not_because_it_was/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-14T04:20:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1owx1nh</id>
    <title>The company gmktec made a comparison of the EVO-X2 that has a Ryzen AI Max+ 395 processor vs NVIDIA DGX SPARK</title>
    <updated>2025-11-14T13:52:15+00:00</updated>
    <author>
      <name>/u/Illustrious-Swim9663</name>
      <uri>https://old.reddit.com/user/Illustrious-Swim9663</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owx1nh/the_company_gmktec_made_a_comparison_of_the_evox2/"&gt; &lt;img alt="The company gmktec made a comparison of the EVO-X2 that has a Ryzen AI Max+ 395 processor vs NVIDIA DGX SPARK" src="https://preview.redd.it/pl1lqj8r981g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1855485e8cb6f5d5b69639209615733627982830" title="The company gmktec made a comparison of the EVO-X2 that has a Ryzen AI Max+ 395 processor vs NVIDIA DGX SPARK" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My point is that they should make comparisons with small models that have come out lately because they are enough for most people and because the inference is also faster&lt;/p&gt; &lt;p&gt;Info :&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.gmktec.com/blog/evo-x2-vs-nvidia-dgx-spark-redefining-local-ai-performance"&gt;https://www.gmktec.com/blog/evo-x2-vs-nvidia-dgx-spark-redefining-local-ai-performance&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Illustrious-Swim9663"&gt; /u/Illustrious-Swim9663 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/pl1lqj8r981g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owx1nh/the_company_gmktec_made_a_comparison_of_the_evox2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1owx1nh/the_company_gmktec_made_a_comparison_of_the_evox2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-14T13:52:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1owyp8q</id>
    <title>The Big LLM Architecture Comparison: From DeepSeek-V3 to Kimi K2 Thinking</title>
    <updated>2025-11-14T14:58:02+00:00</updated>
    <author>
      <name>/u/seraschka</name>
      <uri>https://old.reddit.com/user/seraschka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owyp8q/the_big_llm_architecture_comparison_from/"&gt; &lt;img alt="The Big LLM Architecture Comparison: From DeepSeek-V3 to Kimi K2 Thinking" src="https://external-preview.redd.it/5N8z_mXiAneWfY6B3hrkRbiDD5IkgsvFJWMT1AAURS8.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c88626d25ad555fca14567f2825f2de4449a35ff" title="The Big LLM Architecture Comparison: From DeepSeek-V3 to Kimi K2 Thinking" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/seraschka"&gt; /u/seraschka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://sebastianraschka.com/blog/2025/the-big-llm-architecture-comparison.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owyp8q/the_big_llm_architecture_comparison_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1owyp8q/the_big_llm_architecture_comparison_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-14T14:58:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1owskm6</id>
    <title>Windows llama.cpp is 20% faster</title>
    <updated>2025-11-14T10:05:21+00:00</updated>
    <author>
      <name>/u/johannes_bertens</name>
      <uri>https://old.reddit.com/user/johannes_bertens</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owskm6/windows_llamacpp_is_20_faster/"&gt; &lt;img alt="Windows llama.cpp is 20% faster" src="https://preview.redd.it/tfdcbkf6571g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6f97a0d3f3c6a2519462ab5e159f2045396e9409" title="Windows llama.cpp is 20% faster" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;But why?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Windows: 1000+ PP&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;llama-bench -m C:\Users\johan\.lmstudio\models\unsloth\Qwen3-VL-30B-A3B-Instruct-GGUF\Qwen3-VL-30B-A3B-Instruct-UD-Q8_K_XL.gguf -p 512,1024,2048,4096 -n 0 -fa 0 --mmap 0&lt;br /&gt; load_backend: loaded RPC backend from C:\Users\johan\Downloads\llama-b7032-bin-win-vulkan-x64\ggml-rpc.dll&lt;br /&gt; ggml_vulkan: Found 1 Vulkan devices:&lt;br /&gt; ggml_vulkan: 0 = AMD Radeon(TM) 8060S Graphics (AMD proprietary driver) | uma: 1 | fp16: 1 | bf16: 1 | warp size: 64 | shared memory: 32768 | int dot: 1 | matrix cores: KHR_coopmat&lt;br /&gt; load_backend: loaded Vulkan backend from C:\Users\johan\Downloads\llama-b7032-bin-win-vulkan-x64\ggml-vulkan.dll&lt;br /&gt; load_backend: loaded CPU backend from C:\Users\johan\Downloads\llama-b7032-bin-win-vulkan-x64\ggml-cpu-icelake.dll &lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;model &lt;/th&gt; &lt;th align="right"&gt; size&lt;/th&gt; &lt;th align="right"&gt; params&lt;/th&gt; &lt;th&gt;backend &lt;/th&gt; &lt;th align="right"&gt;ngl&lt;/th&gt; &lt;th align="right"&gt;mmap&lt;/th&gt; &lt;th align="right"&gt; test&lt;/th&gt; &lt;th align="right"&gt; t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;qwen3vlmoe 30B.A3B Q8_0 &lt;/td&gt; &lt;td align="right"&gt; 33.51 GiB&lt;/td&gt; &lt;td align="right"&gt; 30.53 B&lt;/td&gt; &lt;td&gt;Vulkan &lt;/td&gt; &lt;td align="right"&gt; 99&lt;/td&gt; &lt;td align="right"&gt; 0&lt;/td&gt; &lt;td align="right"&gt; pp512&lt;/td&gt; &lt;td align="right"&gt; 1079.12 ¬± 4.32&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3vlmoe 30B.A3B Q8_0 &lt;/td&gt; &lt;td align="right"&gt; 33.51 GiB&lt;/td&gt; &lt;td align="right"&gt; 30.53 B&lt;/td&gt; &lt;td&gt;Vulkan &lt;/td&gt; &lt;td align="right"&gt; 99&lt;/td&gt; &lt;td align="right"&gt; 0&lt;/td&gt; &lt;td align="right"&gt; pp1024&lt;/td&gt; &lt;td align="right"&gt; 975.04 ¬± 4.46&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3vlmoe 30B.A3B Q8_0 &lt;/td&gt; &lt;td align="right"&gt; 33.51 GiB&lt;/td&gt; &lt;td align="right"&gt; 30.53 B&lt;/td&gt; &lt;td&gt;Vulkan &lt;/td&gt; &lt;td align="right"&gt; 99&lt;/td&gt; &lt;td align="right"&gt; 0&lt;/td&gt; &lt;td align="right"&gt; pp2048&lt;/td&gt; &lt;td align="right"&gt; 892.94 ¬± 2.49&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3vlmoe 30B.A3B Q8_0 &lt;/td&gt; &lt;td align="right"&gt; 33.51 GiB&lt;/td&gt; &lt;td align="right"&gt; 30.53 B&lt;/td&gt; &lt;td&gt;Vulkan &lt;/td&gt; &lt;td align="right"&gt; 99&lt;/td&gt; &lt;td align="right"&gt; 0&lt;/td&gt; &lt;td align="right"&gt; pp4096&lt;/td&gt; &lt;td align="right"&gt; 806.84 ¬± 2.89&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Linux: 880 PP&lt;/strong&gt;&lt;/p&gt; &lt;p&gt; [johannes@toolbx ~]$ llama-bench -m models/Qwen3-VL-30B-A3B-Instruct-UD-Q8_K_XL.gguf -p 512,1024,2048,4096 -n 0 -fa 0 --mmap 0&lt;br /&gt; ggml_vulkan: Found 1 Vulkan devices:&lt;br /&gt; ggml_vulkan: 0 = Radeon 8060S Graphics (RADV GFX1151) (radv) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 1 | matrix cores: KHR_coopmat &lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;model &lt;/th&gt; &lt;th align="right"&gt; size&lt;/th&gt; &lt;th align="right"&gt; params&lt;/th&gt; &lt;th&gt;backend &lt;/th&gt; &lt;th align="right"&gt;ngl&lt;/th&gt; &lt;th align="right"&gt;mmap&lt;/th&gt; &lt;th align="right"&gt; test&lt;/th&gt; &lt;th align="right"&gt; t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;qwen3vlmoe 30B.A3B Q8_0 &lt;/td&gt; &lt;td align="right"&gt; 33.51 GiB&lt;/td&gt; &lt;td align="right"&gt; 30.53 B&lt;/td&gt; &lt;td&gt;Vulkan &lt;/td&gt; &lt;td align="right"&gt; 99&lt;/td&gt; &lt;td align="right"&gt; 0&lt;/td&gt; &lt;td align="right"&gt; pp512&lt;/td&gt; &lt;td align="right"&gt; 876.79 ¬± 4.76&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3vlmoe 30B.A3B Q8_0 &lt;/td&gt; &lt;td align="right"&gt; 33.51 GiB&lt;/td&gt; &lt;td align="right"&gt; 30.53 B&lt;/td&gt; &lt;td&gt;Vulkan &lt;/td&gt; &lt;td align="right"&gt; 99&lt;/td&gt; &lt;td align="right"&gt; 0&lt;/td&gt; &lt;td align="right"&gt; pp1024&lt;/td&gt; &lt;td align="right"&gt; 797.87 ¬± 1.56&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3vlmoe 30B.A3B Q8_0 &lt;/td&gt; &lt;td align="right"&gt; 33.51 GiB&lt;/td&gt; &lt;td align="right"&gt; 30.53 B&lt;/td&gt; &lt;td&gt;Vulkan &lt;/td&gt; &lt;td align="right"&gt; 99&lt;/td&gt; &lt;td align="right"&gt; 0&lt;/td&gt; &lt;td align="right"&gt; pp2048&lt;/td&gt; &lt;td align="right"&gt; 757.55 ¬± 2.10&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3vlmoe 30B.A3B Q8_0 &lt;/td&gt; &lt;td align="right"&gt; 33.51 GiB&lt;/td&gt; &lt;td align="right"&gt; 30.53 B&lt;/td&gt; &lt;td&gt;Vulkan &lt;/td&gt; &lt;td align="right"&gt; 99&lt;/td&gt; &lt;td align="right"&gt; 0&lt;/td&gt; &lt;td align="right"&gt; pp4096&lt;/td&gt; &lt;td align="right"&gt; 686.61 ¬± 0.89&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Obviously it's not 20% over the board, but still a very big difference. Is the &amp;quot;AMD proprietary driver&amp;quot; such a big deal?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/johannes_bertens"&gt; /u/johannes_bertens &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/tfdcbkf6571g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owskm6/windows_llamacpp_is_20_faster/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1owskm6/windows_llamacpp_is_20_faster/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-14T10:05:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1owocd2</id>
    <title>Is it normal to hear weird noises when running an LLM on 4√ó Pro 6000 Max-Q cards?</title>
    <updated>2025-11-14T05:41:14+00:00</updated>
    <author>
      <name>/u/PlusProfession9245</name>
      <uri>https://old.reddit.com/user/PlusProfession9245</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owocd2/is_it_normal_to_hear_weird_noises_when_running_an/"&gt; &lt;img alt="Is it normal to hear weird noises when running an LLM on 4√ó Pro 6000 Max-Q cards?" src="https://external-preview.redd.it/MnFzdzJ0b3l0NTFnMbphl7ifhldDVQJssqSE3uLNJKqrQJ4o9dG0SGtQf767.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3b2e8e94666721a90be11f2cea3b9f593dc28f21" title="Is it normal to hear weird noises when running an LLM on 4√ó Pro 6000 Max-Q cards?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It doesn‚Äôt sound like normal coil whine.&lt;br /&gt; In a Docker environment, when I run gpt-oss-120b across 4 GPUs, I hear a strange noise.&lt;br /&gt; The sound is also different depending on the model.&lt;br /&gt; Is this normal??&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PlusProfession9245"&gt; /u/PlusProfession9245 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9eez1soyt51g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owocd2/is_it_normal_to_hear_weird_noises_when_running_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1owocd2/is_it_normal_to_hear_weird_noises_when_running_an/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-14T05:41:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1oth5pw</id>
    <title>AMA With Moonshot AI, The Open-source Frontier Lab Behind Kimi K2 Thinking Model</title>
    <updated>2025-11-10T15:44:10+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/"&gt; &lt;img alt="AMA With Moonshot AI, The Open-source Frontier Lab Behind Kimi K2 Thinking Model" src="https://b.thumbs.redditmedia.com/yz9_FpdLcHNiCkaH5fLEIoXS2f5u5twNBr7SQ9Go3AI.jpg" title="AMA With Moonshot AI, The Open-source Frontier Lab Behind Kimi K2 Thinking Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Today we are having &lt;strong&gt;Moonshot AI&lt;/strong&gt;, the research lab behind the &lt;strong&gt;Kimi&lt;/strong&gt; &lt;strong&gt;models&lt;/strong&gt;. We‚Äôre excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="/u/ComfortableAsk4494"&gt;u/ComfortableAsk4494&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="/u/zxytim"&gt;u/zxytim&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="/u/ppwwyyxx"&gt;u/ppwwyyxx&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 8 AM ‚Äì 11 AM PST, with the Kimi team continuing to follow up on questions over the next 24 hours.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5yg0ncsn7g0g1.png?width=3525&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5318680204ef7502ad349aec148147d9e3398f87"&gt;https://preview.redd.it/5yg0ncsn7g0g1.png?width=3525&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5318680204ef7502ad349aec148147d9e3398f87&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Thanks everyone for joining our AMA. The live part has ended and the Kimi team will be following up with more answers sporadically over the next 24 hours.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T15:44:10+00:00</published>
  </entry>
</feed>
