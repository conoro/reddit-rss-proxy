<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-28T21:24:00+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ohm80t</id>
    <title>Newegg has 32gb AMD r9700 for $1,300</title>
    <updated>2025-10-27T18:23:03+00:00</updated>
    <author>
      <name>/u/Terminator857</name>
      <uri>https://old.reddit.com/user/Terminator857</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://videocardz.com/newz/amd-radeon-pro-ai-r9700-is-now-available-32gb-memory-and-full-navi-48-gpu"&gt;https://videocardz.com/newz/amd-radeon-pro-ai-r9700-is-now-available-32gb-memory-and-full-navi-48-gpu&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Phoronix did a poor job of benchmarking it. Would prefer benchmarking a 30gb model like qwen3 coder, but instead focuses on 8gb model: &lt;a href="https://www.phoronix.com/review/amd-radeon-ai-pro-r9700"&gt;https://www.phoronix.com/review/amd-radeon-ai-pro-r9700&lt;/a&gt; Doesn't bother to compare it to 4090/5090. This video does gaming benchmarks: &lt;a href="https://www.youtube.com/watch?v=x0YJ32Q0mNw"&gt;https://www.youtube.com/watch?v=x0YJ32Q0mNw&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Guessing 30 tokens per second (TPS) for qwen3 coder.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Terminator857"&gt; /u/Terminator857 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohm80t/newegg_has_32gb_amd_r9700_for_1300/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohm80t/newegg_has_32gb_amd_r9700_for_1300/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohm80t/newegg_has_32gb_amd_r9700_for_1300/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T18:23:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1oilwvm</id>
    <title>MiniMax-M2 llama.cpp</title>
    <updated>2025-10-28T21:23:23+00:00</updated>
    <author>
      <name>/u/butlan</name>
      <uri>https://old.reddit.com/user/butlan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried to implement it, it's fully cursor generated ai slop code, sorry. The chat template is strange; I'm 100% sure it's not correctly implemented, but it works with the roo code (Q2 is bad, Q4 is fine) at least. Anyone who wants to waste 100gb bandwidth can give it a try.&lt;/p&gt; &lt;p&gt;test device and command : 2x4090 and lot of ram&lt;/p&gt; &lt;p&gt;&lt;code&gt;./llama-server -m minimax-m2-Q4_K.gguf -ngl 999 --cpu-moe --jinja -fa on -c 50000 --reasoning-format auto&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;code:&lt;/code&gt; &lt;a href="https://github.com/cturan/llama.cpp/tree/minimax"&gt;here&lt;/a&gt; &lt;code&gt;gguf:&lt;/code&gt; &lt;a href="https://huggingface.co/cturan/MiniMax-M2-GGUF/tree/main"&gt;here&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1oilwvm/video/ofpwt9vn4xxf1/player"&gt;https://reddit.com/link/1oilwvm/video/ofpwt9vn4xxf1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/butlan"&gt; /u/butlan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oilwvm/minimaxm2_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oilwvm/minimaxm2_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oilwvm/minimaxm2_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T21:23:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1oihxvc</id>
    <title>Looking for models that are good for product design</title>
    <updated>2025-10-28T18:51:49+00:00</updated>
    <author>
      <name>/u/Striking_Luck5201</name>
      <uri>https://old.reddit.com/user/Striking_Luck5201</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello all. l am new to local LLMs and have been trying a few models, but haven't found one that clicks yet.&lt;/p&gt; &lt;p&gt;For the past year or more I have used Claude as my main AI platform and then followed up with chat gpt if i needed a more accurate answer. I would discuss circuit designs, conceptual designs, and mostly use it as a way to help develop ideas. It was great up until recently where they have been choking down on the usage like crazy. &lt;/p&gt; &lt;p&gt;I would like to switch to using local llms, but I really haven't found a model yet that works well as just a general conversationalist. I run a nvidia 3090, so I have been trying various qwen models, llama 70b, and a few others. Most of them have been hallucinating hard. &lt;/p&gt; &lt;p&gt;I would love to hear some general thoughts from you guys. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Striking_Luck5201"&gt; /u/Striking_Luck5201 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oihxvc/looking_for_models_that_are_good_for_product/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oihxvc/looking_for_models_that_are_good_for_product/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oihxvc/looking_for_models_that_are_good_for_product/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T18:51:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohdl9q</id>
    <title>Silicon Valley is migrating from expensive closed-source models to cheaper open-source alternatives</title>
    <updated>2025-10-27T12:53:12+00:00</updated>
    <author>
      <name>/u/xiaoruhao</name>
      <uri>https://old.reddit.com/user/xiaoruhao</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohdl9q/silicon_valley_is_migrating_from_expensive/"&gt; &lt;img alt="Silicon Valley is migrating from expensive closed-source models to cheaper open-source alternatives" src="https://external-preview.redd.it/cDlpaWtncThobnhmMYyuPjWRTezxeRfqB3upVJ5ATISaueUIVVjdl6ikWaxE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=49803f057b43fcece9390b3b966fe6ba4de209b3" title="Silicon Valley is migrating from expensive closed-source models to cheaper open-source alternatives" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Chamath Palihapitiya said his team migrated a large number of workloads to Kimi K2 because it was significantly more performant and much cheaper than both OpenAI and Anthropic.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xiaoruhao"&gt; /u/xiaoruhao &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/avwpphq8hnxf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohdl9q/silicon_valley_is_migrating_from_expensive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohdl9q/silicon_valley_is_migrating_from_expensive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T12:53:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1oicoeh</id>
    <title>Need help properly setting up open-webui</title>
    <updated>2025-10-28T15:37:06+00:00</updated>
    <author>
      <name>/u/stuckwi</name>
      <uri>https://old.reddit.com/user/stuckwi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello localLLama experts, &lt;/p&gt; &lt;p&gt;Could someone point me to some guide on how to tweak open-webui parameters to properly give me the correct results?&lt;br /&gt; I have OWUI and Ollama running in docker containers. I've pulled a few models to run on my RTX3090. eg. Codestral and Gemma3 27b. I've also connected to Mistral API and exposed a few models from that API to OWUI. All using default parameters, no custom prompts for any of the models as I don't know what I'm doing in those areas anyway.&lt;/p&gt; &lt;p&gt;Here is the problem. When I give a sample data table and ask the model to give me code to do XYZ, the Codestral model using Mistral API correctly gives me code I asked for. But when I use the locally hosted Codestral running on ollama with the EXACT same prompt, all it just gives me is a summary of the data table. &lt;/p&gt; &lt;p&gt;Could someone kindly help me or point me in the right direction to configure this setup to achieve the same/similar results running on the local model as the cloud model?&lt;/p&gt; &lt;p&gt;Thank you in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/stuckwi"&gt; /u/stuckwi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oicoeh/need_help_properly_setting_up_openwebui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oicoeh/need_help_properly_setting_up_openwebui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oicoeh/need_help_properly_setting_up_openwebui/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T15:37:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1oij8bg</id>
    <title>Has anyone gotten vLLM working natively on Windows (no WSL/Docker) with Flash Attention?</title>
    <updated>2025-10-28T19:41:11+00:00</updated>
    <author>
      <name>/u/JustSayin_thatuknow</name>
      <uri>https://old.reddit.com/user/JustSayin_thatuknow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Has anyone successfully run vLLM natively on Windows with Flash Attention enabled?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I'm trying to get vLLM running on Windows and wanted to check if anyone has managed to do this:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Native Windows installation (not WSL or Docker)&lt;/li&gt; &lt;li&gt;Not using the vllm-windows fork/project&lt;/li&gt; &lt;li&gt;With Flash Attention actually working&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you've gotten this setup working, I'd love to hear about: - What installation method you used - Any specific dependencies or build steps - Whether Flash Attention is actually functioning or just enabled without errors&lt;/p&gt; &lt;p&gt;Most guides I've found either use WSL, Docker, or point to the vllm-windows project, but I'm curious if anyone's gotten the upstream vLLM working natively with all features.&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JustSayin_thatuknow"&gt; /u/JustSayin_thatuknow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oij8bg/has_anyone_gotten_vllm_working_natively_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oij8bg/has_anyone_gotten_vllm_working_natively_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oij8bg/has_anyone_gotten_vllm_working_natively_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T19:41:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1oi2tky</id>
    <title>I built a small Python tool to track how your directories get messy (and clean again)</title>
    <updated>2025-10-28T07:12:21+00:00</updated>
    <author>
      <name>/u/VegetableSense</name>
      <uri>https://old.reddit.com/user/VegetableSense</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, much as we hate to admit, almost every project or downloads folder gets out of control over time (yep).&lt;/p&gt; &lt;p&gt;I got curious â€” not just about which files change, but &lt;strong&gt;how the structure itself evolves.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;So I built &lt;a href="https://github.com/sukanto-m/directory-monitor"&gt;&lt;strong&gt;Directory Monitor&lt;/strong&gt;&lt;/a&gt; â€” a lightweight Python script that keeps tabs on &lt;strong&gt;directory organization&lt;/strong&gt;, not just file edits. This tool uses local LLMs (Qwen, Llama, choose your own) to analyze project structure and give cleanup recommendations. Everything runs locally - no cloud APIs.&lt;/p&gt; &lt;p&gt;**The interesting technical bits:**&lt;/p&gt; &lt;p&gt;- Uses RAG with local sentence-transformers to compare current state against historical scans&lt;/p&gt; &lt;p&gt;- LLM analyzes trends and gives specific, actionable recommendations &lt;/p&gt; &lt;p&gt;- Terminal UI with Rich showing real-time metrics and sparklines&lt;/p&gt; &lt;p&gt;- All stored in SQLite locally&lt;/p&gt; &lt;p&gt;**Example output:**&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;Messiness Score: 6.2/10&lt;/p&gt; &lt;p&gt;Top 3 Issues:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Too many files (28) in src/components - split into ui/, forms/, layouts/&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;8 files contain 'temp' - move to .archive/ or use proper version control&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Directory depth exceeds 7 levels - flatten structure&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Trend: ðŸ“‰ Improving (was 7.8, now 6.2)&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;**Stack:**&lt;/p&gt; &lt;p&gt;- Ollama (Qwen/Llama) for LLM&lt;/p&gt; &lt;p&gt;- sentence-transformers for embeddings&lt;/p&gt; &lt;p&gt;- SQLite for history&lt;/p&gt; &lt;p&gt;- Python with Rich/Flask&lt;/p&gt; &lt;p&gt;Works completely offline after setup. Tested with Qwen3:8b and Llama3.2.&lt;/p&gt; &lt;p&gt;Would love feedback â€” what features would &lt;em&gt;you&lt;/em&gt; add for keeping folders sane?&lt;/p&gt; &lt;p&gt;**GitHub:** &lt;a href="https://github.com/sukanto-m/directory-monitor"&gt;https://github.com/sukanto-m/directory-monitor&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VegetableSense"&gt; /u/VegetableSense &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi2tky/i_built_a_small_python_tool_to_track_how_your/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi2tky/i_built_a_small_python_tool_to_track_how_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oi2tky/i_built_a_small_python_tool_to_track_how_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T07:12:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1oik4mm</id>
    <title>Fine tune existing LLMs in Colab or Kaggle</title>
    <updated>2025-10-28T20:14:55+00:00</updated>
    <author>
      <name>/u/DobraVibra</name>
      <uri>https://old.reddit.com/user/DobraVibra</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried to use Colab and Kaggle to fine-tune an existing 1B LLMs for my style. I was fine-tuning them, changing epoch, and slowing down learning. I have 7k of my own messages in my own style. I also checked my training dataset to be in the correct format. &lt;/p&gt; &lt;p&gt;Mostly Colab doesn't work for since it runs out of RAM. I cannot really use Kaggle right now because of &amp;quot;additional_chat_templates does not exist on main&amp;quot;.&lt;/p&gt; &lt;p&gt;Which good LLMs were you able to run on those 2 services? Or maybe on some other service?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DobraVibra"&gt; /u/DobraVibra &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oik4mm/fine_tune_existing_llms_in_colab_or_kaggle/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oik4mm/fine_tune_existing_llms_in_colab_or_kaggle/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oik4mm/fine_tune_existing_llms_in_colab_or_kaggle/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T20:14:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohvcwt</id>
    <title>Is an NVIDIA A40 48GB for 1500USD a bad idea because it's age?</title>
    <updated>2025-10-28T00:26:07+00:00</updated>
    <author>
      <name>/u/panchovix</name>
      <uri>https://old.reddit.com/user/panchovix</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohvcwt/is_an_nvidia_a40_48gb_for_1500usd_a_bad_idea/"&gt; &lt;img alt="Is an NVIDIA A40 48GB for 1500USD a bad idea because it's age?" src="https://a.thumbs.redditmedia.com/UBwX6pI157vt7gMYDChu_R8QUux04jne3QgjRhJMLr0.jpg" title="Is an NVIDIA A40 48GB for 1500USD a bad idea because it's age?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello guys, hope you're fine.&lt;/p&gt; &lt;p&gt;Short question, I managed to find, working and testing on my PC right now, an A40 48GB. It is passively cooled and it gets quite hot.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8az1kqsdyqxf1.png?width=764&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=301fff8d7b8d78a3f33c97765bb96ebdeaa03e2d"&gt;Local testing on my PC&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The seller (a friend) is asking me 1500USD for it. I'm not from USA, but a 3rd world country.&lt;/p&gt; &lt;p&gt;But I have read here on Local llama that such old cards and such aren't very worth it, also no FP8 support, etc.&lt;/p&gt; &lt;p&gt;So I'm really torn and indecisive about it. For reference, 5090 new goes for about 2700-3300USD (so 32GB, but fp8/fp4 support, like 4x times the bandwidth, etc). Used 4090s are 1600USD. 4090 48GB modded when importing they're about 4200-4400USD. 3090s are 550-600USD.&lt;/p&gt; &lt;p&gt;What would you guys do? Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/panchovix"&gt; /u/panchovix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohvcwt/is_an_nvidia_a40_48gb_for_1500usd_a_bad_idea/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohvcwt/is_an_nvidia_a40_48gb_for_1500usd_a_bad_idea/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohvcwt/is_an_nvidia_a40_48gb_for_1500usd_a_bad_idea/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T00:26:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1oilbia</id>
    <title>Best simple plug and play conversational LLM STT TTS set up?</title>
    <updated>2025-10-28T21:00:06+00:00</updated>
    <author>
      <name>/u/Adventurous-Gold6413</name>
      <uri>https://old.reddit.com/user/Adventurous-Gold6413</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I donâ€™t really have the time to build one myself &lt;/p&gt; &lt;p&gt;Iâ€™d probably wanna use GPT-OSS 20b &lt;/p&gt; &lt;p&gt;Yeah doesnâ€™t have to be god tier but it should not be TTS that sounds whack either(standard windows TTS)&lt;/p&gt; &lt;p&gt;Any suggestions/ GitHub projects you guys can recommend? Thank you &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Adventurous-Gold6413"&gt; /u/Adventurous-Gold6413 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oilbia/best_simple_plug_and_play_conversational_llm_stt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oilbia/best_simple_plug_and_play_conversational_llm_stt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oilbia/best_simple_plug_and_play_conversational_llm_stt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T21:00:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohyeee</id>
    <title>Minimax-M2 support added in MLX</title>
    <updated>2025-10-28T02:49:15+00:00</updated>
    <author>
      <name>/u/No_Conversation9561</name>
      <uri>https://old.reddit.com/user/No_Conversation9561</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohyeee/minimaxm2_support_added_in_mlx/"&gt; &lt;img alt="Minimax-M2 support added in MLX" src="https://preview.redd.it/4yqqtqzynrxf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=85e601e94e902746685decb25bfc69d58f508850" title="Minimax-M2 support added in MLX" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Conversation9561"&gt; /u/No_Conversation9561 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4yqqtqzynrxf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohyeee/minimaxm2_support_added_in_mlx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohyeee/minimaxm2_support_added_in_mlx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T02:49:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1oht9pw</id>
    <title>Z.ai release Glyph weight</title>
    <updated>2025-10-27T22:55:19+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oht9pw/zai_release_glyph_weight/"&gt; &lt;img alt="Z.ai release Glyph weight" src="https://a.thumbs.redditmedia.com/69TaqDj6bS-Vs7O_YYHmJygT9-976J0J5KJcduszmX8.jpg" title="Z.ai release Glyph weight" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Glyph: Scaling Context Windows via Visual-Text Compression&lt;/p&gt; &lt;p&gt;Paper: arxiv.org/abs/2510.17800&lt;/p&gt; &lt;p&gt;Weights: huggingface.co/zai-org/Glyph&lt;/p&gt; &lt;p&gt;Repo: github.com/thu-coai/Glyph&lt;/p&gt; &lt;p&gt;Glyph is a framework for scaling the context length through visual-text compression. It renders long textual sequences into images and processes them using visionâ€“language models.&lt;/p&gt; &lt;p&gt;This design transforms the challenge of long-context modeling into a multimodal problem, substantially reducing computational and memory costs while preserving semantic information.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1oht9pw"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oht9pw/zai_release_glyph_weight/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oht9pw/zai_release_glyph_weight/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T22:55:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1oieip0</id>
    <title>Theoretically Scaling Beyond 2 DGX Sparks in a Single Cluster.</title>
    <updated>2025-10-28T16:44:45+00:00</updated>
    <author>
      <name>/u/SIN3R6Y</name>
      <uri>https://old.reddit.com/user/SIN3R6Y</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;First off, let's get into why NVIDIA only supports clustering 2 of these at the moment.&lt;/p&gt; &lt;p&gt;&lt;code&gt;user@spark:~$ lspci | grep Mellanox&lt;/code&gt;&lt;br /&gt; &lt;code&gt;0000:01:00.0 Ethernet controller: Mellanox Technologies MT2910 Family [ConnectX-7]&lt;/code&gt;&lt;br /&gt; &lt;code&gt;0000:01:00.1 Ethernet controller: Mellanox Technologies MT2910 Family [ConnectX-7]&lt;/code&gt;&lt;br /&gt; &lt;code&gt;0002:01:00.0 Ethernet controller: Mellanox Technologies MT2910 Family [ConnectX-7]&lt;/code&gt;&lt;br /&gt; &lt;code&gt;0002:01:00.1 Ethernet controller: Mellanox Technologies MT2910 Family [ConnectX-7]&lt;/code&gt;&lt;/p&gt; &lt;p&gt;The cpu is essentially two 10 core compute units married together, each with their own pcie root complex connected to the CX7 at Gen5 x4. Meaning each compute half of the CPU can push roughly 100gbps (200gbps across both complexes), and the CX7 interfaces effectively show up twice.&lt;/p&gt; &lt;p&gt;CPU 1st Half:&lt;br /&gt; enp1s0f0np0 -&amp;gt; port 1&lt;br /&gt; enp1s0f1np1 -&amp;gt; port 2&lt;/p&gt; &lt;p&gt;CPU 2nd Half:&lt;br /&gt; enP2p1s0f0np0 -&amp;gt; port 1&lt;br /&gt; enP2p1s0f1np1 -&amp;gt; port 2&lt;/p&gt; &lt;pre&gt;&lt;code&gt;user@spark:~$ ibdev2netdev rocep1s0f0 port 1 ==&amp;gt; enp1s0f0np0 (Up) rocep1s0f1 port 1 ==&amp;gt; enp1s0f1np1 (Up) roceP2p1s0f0 port 1 ==&amp;gt; enP2p1s0f0np0 (Up) roceP2p1s0f1 port 1 ==&amp;gt; enP2p1s0f1np1 (Up) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;NVIDIA docs will basically tell you to ignore the all the second half (enP2) interfaces. This works at 200gbps in a p2p dual spark scenario because NCCL is going to transmit ROCE v1 L2 frames out of all up ROCE interfaces. Doing a direct connection will bring up two of those (one per complex) and it will just work, no ROCE configuration really needed. Ethernet traffic will be limited to about 100gbps out of the single port however.&lt;/p&gt; &lt;p&gt;But, now in my case. I am connecting these sparks over dual 100gbit QSFP28 links to a cluster of NVIDIA sn2010 switches. QSFP28, because no matter what, 200gbps is the absolute maximum the CX7 can do given the PCIe limitations.&lt;/p&gt; &lt;p&gt;To make this work, with ROCE v2 and layer 3 links to the switch. You can set an IP on each half of the complex.&lt;/p&gt; &lt;p&gt;enp1s0f0np0 -&amp;gt; set ip (CPU 1st half CX7 port 1)&lt;br /&gt; enP2p1s0f1np1 - set ip (CPU 2nd half CX7 port 2)&lt;/p&gt; &lt;p&gt;Now, this will break NCCL. NCCL needs some variables tweaked, otherwise it's going to try to use ROCE v1 p2p ports which cannot work in this scenario. Here is an NCCL test that will get 200gbps across both links to a switch.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;mpirun -np 2 -H &amp;lt;spark 1 ip&amp;gt;,&amp;lt;spark 2 ip&amp;gt; \ --mca plm_rsh_agent &amp;quot;ssh -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no&amp;quot; \ -x LD_LIBRARY_PATH=$LD_LIBRARY_PATH \ -x UCX_NET_DEVICES=enp1s0f0np0,enP2p1s0f1np1 \ -x NCCL_SOCKET_IFNAME=enp1s0f0np0,enP2p1s0f1np1 \ -x NCCL_SOCKET_FAMILY=AF_INET \ -x NCCL_IB_HCA=rocep1s0f0,roceP2p1s0f1 \ -x OMPI_MCA_btl_tcp_if_include=enp1s0f0np0,enP2p1s0f1np1 \ -x NCCL_IB_GID_INDEX=3 \ -x NCCL_IB_TC=3 \ -x NCCL_IB_MERGE_NICS=1\ $HOME/nccl-tests/build/all_gather_perf -b 16G -e 16G -f 2 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The host IP's above can be the the IP's of the 10g interfaces, NCCL will still discover the CX7 paths but just do IP coordination over the 10g links. Just sure the two sparks are routable to each other over the CX7 or on the same L2 segment. I use static layer 3 routes for this, but for larger setups BGP would also work well here.&lt;/p&gt; &lt;p&gt;These flags restrict the interfaces NCCL sees, forces ROCE v2, merges those nics, and forces the lossless traffic class. In theory, with both CX7 interfaces connected to a switch, you're only scaling limit here with multiple sparks is how many switch ports you have.&lt;/p&gt; &lt;p&gt;To make this more permanent I set these in .profile for the user.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;export CUDA_HOME=&amp;quot;/usr/local/cuda&amp;quot; export MPI_HOME=&amp;quot;/usr/lib/aarch64-linux-gnu/openmpi&amp;quot; export NCCL_HOME=&amp;quot;$HOME/nccl/build/&amp;quot; export LD_LIBRARY_PATH=&amp;quot;$NCCL_HOME/lib:$CUDA_HOME/lib64/:$MPI_HOME/lib:$LD_LIBRARY_PATH&amp;quot; export IP_IF_NAME=enp1s0f0np0,enP2p1s0f1np1 export IB_IF_NAME=rocep1s0f0,roceP2p1s0f1 export UCX_NET_DEVICES=$IP_IF_NAME export NCCL_SOCKET_IFNAME=$IP_IF_NAME export NCCL_SOCKET_FAMILY=AF_INET export NCCL_IB_HCA=$IB_IF_NAME export NCCL_IB_GID_INDEX=3 export NCCL_IB_MERGE_NICS=1 export OMPI_MCA_btl_tcp_if_include=$IP_IF_NAME &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;NCCL Test Results&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# nccl-tests version 2.17.4 nccl-headers=22807 nccl-library=22807 # Collective test starting: all_gather_perf # nThread 1 nGpus 1 minBytes 17179869184 maxBytes 17179869184 step: 2(factor) warmup iters: 1 iters: 20 agg iters: 1 validation: 1 graph: 0 # # Using devices # Rank 0 Group 0 Pid 303712 on spark-1af4 device 0 [000f:01:00] NVIDIA GB10 # Rank 1 Group 0 Pid 166882 on spark-870f device 0 [000f:01:00] NVIDIA GB10 # # out-of-place in-place # size count type redop root time algbw busbw #wrong time algbw busbw #wrong # (B) (elements) (us) (GB/s) (GB/s) (us) (GB/s) (GB/s) 17179869184 2147483648 float none -1 410263 41.88 20.94 0 409388 41.96 20.98 0 # Out of bounds values : 0 OK # Avg bus bandwidth : 20.96 # # Collective test concluded: all_gather_perf &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;EDIT: It's worth noting that with this setup, you are able to get both 200gbps ROCE v2 traffic and 200gbps Ethernet traffic (not at the same time, they share the combined 200GB of throughput). VS the default p2p setup which gives you 200gbps of ROCE v1 traffic and 100gbps of Ethernet traffic.&lt;/p&gt; &lt;p&gt;However, you can't bond the two links in LACP. This is not supported for NCCL. So what I do is layer 3 (hence why I force ROCE v2) use ECMP to get the desired results.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SIN3R6Y"&gt; /u/SIN3R6Y &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oieip0/theoretically_scaling_beyond_2_dgx_sparks_in_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oieip0/theoretically_scaling_beyond_2_dgx_sparks_in_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oieip0/theoretically_scaling_beyond_2_dgx_sparks_in_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T16:44:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1oi7k25</id>
    <title>HF Space to help create the -ot flags in llama.cpp</title>
    <updated>2025-10-28T12:07:11+00:00</updated>
    <author>
      <name>/u/bullerwins</name>
      <uri>https://old.reddit.com/user/bullerwins</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi7k25/hf_space_to_help_create_the_ot_flags_in_llamacpp/"&gt; &lt;img alt="HF Space to help create the -ot flags in llama.cpp" src="https://external-preview.redd.it/g9TtSkJliQx_HIwoIut_ECyFVGHzqlshzt1AT9ZxIjA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=711e4dcc38b2e4a206b6b1d8e5ce67eae349c3be" title="HF Space to help create the -ot flags in llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi!&lt;/p&gt; &lt;p&gt;Mainly as I was frustrated when manually assigning the layers with the -of flag in llama.cpp and ik_llama.cpp and when increasing maybe just 1 layer in a previous gpu i had to increase the number in all the rest of the gpu, I created a Hugging face space to help with that.&lt;/p&gt; &lt;p&gt;It lets you select the number of GPUs, the size of the model weights and the number of layers and it automatically tries to assign how many layers would fit in your gpu size &lt;strong&gt;on an empty context.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Then if you want to fit more context either switch to manual and reduce 1-2 layers per gpu, or increase the size in GB of the model a bit.&lt;/p&gt; &lt;p&gt;Example:&lt;br /&gt; I want to load &lt;a href="https://huggingface.co/bartowski/zai-org_GLM-4.6-GGUF"&gt;Bartowski GLM-4.6&lt;/a&gt; in Q6 in my rig (rtx6000, 2x5090, 4x3090) and I have 256GB VRAM and the quant takes 294 GB in Q6 as you can see now in HF if you go to the folder:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/bartowski/zai-org_GLM-4.6-GGUF/tree/main/zai-org_GLM-4.6-Q6_K"&gt;https://huggingface.co/bartowski/zai-org_GLM-4.6-GGUF/tree/main/zai-org_GLM-4.6-Q6_K&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/cjc7oe2jeuxf1.png?width=798&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=17433d663ad544eafa7547b47a7d1b917d069837"&gt;https://preview.redd.it/cjc7oe2jeuxf1.png?width=798&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=17433d663ad544eafa7547b47a7d1b917d069837&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And GLM-4.6 has 92 layers as you can see here: &lt;a href="https://huggingface.co/zai-org/GLM-4.6/blob/main/config.json#L31"&gt;https://huggingface.co/zai-org/GLM-4.6/blob/main/config.json#L31&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So fill the settings as such:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/qdyznyd7euxf1.png?width=3418&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=75b3b577c4b9058ce6409be57d82a6b0db40a6e8"&gt;https://preview.redd.it/qdyznyd7euxf1.png?width=3418&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=75b3b577c4b9058ce6409be57d82a6b0db40a6e8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And that actually loads using 2048 context and the GPU are all almost at a 100% vram usage which is what we want.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/qcf0ixxbeuxf1.png?width=1670&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a62cfeec20a34028e8e6fbe0b7a9f99b15bb8442"&gt;https://preview.redd.it/qcf0ixxbeuxf1.png?width=1670&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a62cfeec20a34028e8e6fbe0b7a9f99b15bb8442&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If I reduce one layer per GPU to quickly allow more vram for ctx, I can now load 32K context. But checking the GPU usage I might be able to assign one more layer to the rtx6000.&lt;/p&gt; &lt;p&gt;So the final command would be:&lt;/p&gt; &lt;p&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES=2,0,6,1,3,4,5 ./build/bin/llama-server \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;--model /mnt/llms/models/bartowski/zai-org_GLM-4.6-GGUF/zai-org_GLM-4.6-Q6_K/zai-org_GLM-4.6-Q6_K-00001-of-00008.gguf \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;--alias glm-4.6 \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;--ctx-size 32768 \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-ngl 99 \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;--host&lt;/code&gt; &lt;a href="http://0.0.0.0"&gt;&lt;code&gt;0.0.0.0&lt;/code&gt;&lt;/a&gt; &lt;code&gt;\&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;--port 5000 \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-ot &amp;quot;blk\.(3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27|28|29|30)\.ffn_.*=CUDA0&amp;quot; \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-ot &amp;quot;blk\.(31|32|33|34|35|36|37|38)\.ffn_.*=CUDA1&amp;quot; \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-ot &amp;quot;blk\.(39|40|41|42|43|44|45|46)\.ffn_.*=CUDA2&amp;quot; \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-ot &amp;quot;blk\.(47|48|49|50|51)\.ffn_.*=CUDA3&amp;quot; \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-ot &amp;quot;blk\.(52|53|54|55|56)\.ffn_.*=CUDA4&amp;quot; \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-ot &amp;quot;blk\.(57|58|59|60|61)\.ffn_.*=CUDA5&amp;quot; \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-ot &amp;quot;blk\.(62|63|64|65|66)\.ffn_.*=CUDA6&amp;quot; --cpu-moe&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Link to the HF space: &lt;a href="https://huggingface.co/spaces/bullerwins/Llamacpp-GPU-Layer-Assignment-Tool"&gt;https://huggingface.co/spaces/bullerwins/Llamacpp-GPU-Layer-Assignment-Tool&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bullerwins"&gt; /u/bullerwins &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi7k25/hf_space_to_help_create_the_ot_flags_in_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi7k25/hf_space_to_help_create_the_ot_flags_in_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oi7k25/hf_space_to_help_create_the_ot_flags_in_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T12:07:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1oibaz2</id>
    <title>Waiting for an UnSloth GUFF for MiniMax-M2!</title>
    <updated>2025-10-28T14:45:00+00:00</updated>
    <author>
      <name>/u/Ok_Ninja7526</name>
      <uri>https://old.reddit.com/user/Ok_Ninja7526</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Unsloth has already put MiniMax-M2 on Hugging Face! That means a guff version could arrive very soon. In other words, we might not be far from truly accessible local use. &lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/unsloth/MiniMax-M2"&gt;https://huggingface.co/unsloth/MiniMax-M2&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Ninja7526"&gt; /u/Ok_Ninja7526 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oibaz2/waiting_for_an_unsloth_guff_for_minimaxm2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oibaz2/waiting_for_an_unsloth_guff_for_minimaxm2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oibaz2/waiting_for_an_unsloth_guff_for_minimaxm2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T14:45:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1oi9d43</id>
    <title>50-minute screencast version of a lecture I gave on Model Quantization to a graduate AI &amp; Deep Learning class</title>
    <updated>2025-10-28T13:28:38+00:00</updated>
    <author>
      <name>/u/michaelmalak</name>
      <uri>https://old.reddit.com/user/michaelmalak</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi9d43/50minute_screencast_version_of_a_lecture_i_gave/"&gt; &lt;img alt="50-minute screencast version of a lecture I gave on Model Quantization to a graduate AI &amp;amp; Deep Learning class" src="https://external-preview.redd.it/rTJa8Nhli5TnEtPcCyezMrozQD4vuVxVpOAoemUSml4.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=85dd88668fea8b054bcbd26215365a38c320197b" title="50-minute screencast version of a lecture I gave on Model Quantization to a graduate AI &amp;amp; Deep Learning class" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/michaelmalak"&gt; /u/michaelmalak &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=ze0Xq5QMvmA"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi9d43/50minute_screencast_version_of_a_lecture_i_gave/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oi9d43/50minute_screencast_version_of_a_lecture_i_gave/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T13:28:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1oi63n6</id>
    <title>OSS alternative to Open WebUI - ChatGPT-like UI, API and CLI</title>
    <updated>2025-10-28T10:50:29+00:00</updated>
    <author>
      <name>/u/mythz</name>
      <uri>https://old.reddit.com/user/mythz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi63n6/oss_alternative_to_open_webui_chatgptlike_ui_api/"&gt; &lt;img alt="OSS alternative to Open WebUI - ChatGPT-like UI, API and CLI" src="https://external-preview.redd.it/NbBv8AZ8_FKSnCg3gr7veZ2x9ORPuKnCYj3fZNiyQ4g.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fa75645d12e8474d1f573ac3f747ada63b172124" title="OSS alternative to Open WebUI - ChatGPT-like UI, API and CLI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mythz"&gt; /u/mythz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ServiceStack/llms"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi63n6/oss_alternative_to_open_webui_chatgptlike_ui_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oi63n6/oss_alternative_to_open_webui_chatgptlike_ui_api/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T10:50:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohtp6d</id>
    <title>Bad news: DGX Spark may have only half the performance claimed.</title>
    <updated>2025-10-27T23:13:15+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohtp6d/bad_news_dgx_spark_may_have_only_half_the/"&gt; &lt;img alt="Bad news: DGX Spark may have only half the performance claimed." src="https://preview.redd.it/9b2ziei0lqxf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=de9741ebb17cdabb88dde97eb430a1c2ff563565" title="Bad news: DGX Spark may have only half the performance claimed." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There might be more bad news about the DGX Spark!&lt;/p&gt; &lt;p&gt;Before it was even released, I told everyone that this thing has a memory bandwidth problem. Although it boasts 1 PFLOPS of FP4 floating-point performance, its memory bandwidth is only 273GB/s. This will cause major stuttering when running large models (with performance being roughly only one-third of a MacStudio M2 Ultra).&lt;/p&gt; &lt;p&gt;Today, more bad news emerged: the floating-point performance doesn't even reach 1 PFLOPS.&lt;/p&gt; &lt;p&gt;Tests from two titans of the industryâ€”John Carmack (founder of id Software, developer of games like Doom, and a name every programmer should know from the legendary fast inverse square root algorithm) and Awni Hannun (the primary lead of Apple's large model framework, MLX)â€”have shown that this device only achieves 480 TFLOPS of FP4 performance (approximately 60 TFLOPS BF16). That's less than half of the advertised performance.&lt;/p&gt; &lt;p&gt;Furthermore, if you run it for an extended period, it will overheat and restart.&lt;/p&gt; &lt;p&gt;It's currently unclear whether the problem is caused by the power supply, firmware, CUDA, or something else, or if the SoC is genuinely this underpowered. I hope Jensen Huang fixes this soon. The memory bandwidth issue could be excused as a calculated product segmentation decision from NVIDIA, a result of us having overly high expectations meeting his precise market strategy. However, performance not matching the advertised claims is a major integrity problem.&lt;/p&gt; &lt;p&gt;So, for all the folks who bought an NVIDIA DGX Spark, Gigabyte AI TOP Atom, or ASUS Ascent GX10, I recommend you all run some tests and see if you're indeed facing performance issues.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9b2ziei0lqxf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohtp6d/bad_news_dgx_spark_may_have_only_half_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohtp6d/bad_news_dgx_spark_may_have_only_half_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T23:13:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1oia7pp</id>
    <title>GLM-4.6 on fresh SWE-benchâ€“style tasks collected in September 2025</title>
    <updated>2025-10-28T14:02:30+00:00</updated>
    <author>
      <name>/u/CuriousPlatypus1881</name>
      <uri>https://old.reddit.com/user/CuriousPlatypus1881</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, I'm Anton from Nebius.&lt;/p&gt; &lt;p&gt;Weâ€™ve updated the &lt;strong&gt;SWE-rebench&lt;/strong&gt; leaderboard with model evaluations of GLM-4.6 on 49 fresh tasks.&lt;/p&gt; &lt;p&gt;Key takeaways:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GLM 4.6&lt;/strong&gt; joins the leaderboard and is now the &lt;strong&gt;best open-source performer&lt;/strong&gt;, achieving &lt;strong&gt;37.0 % resolved rate&lt;/strong&gt; and &lt;strong&gt;42.9 % pass@5&lt;/strong&gt;, surpassing &lt;strong&gt;GLM 4.5&lt;/strong&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Check out the full leaderboard and insights here, and feel free to reach out if youâ€™d like to see other models evaluated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CuriousPlatypus1881"&gt; /u/CuriousPlatypus1881 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://swe-rebench.com/?insight=sep_2025"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oia7pp/glm46_on_fresh_swebenchstyle_tasks_collected_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oia7pp/glm46_on_fresh_swebenchstyle_tasks_collected_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T14:02:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1oihbtx</id>
    <title>Minimax-M2 cracks top 10 overall LLMs (production LLM performance gap shrinking: 7 points from GPT-5 in Artificial Analysis benchmark)</title>
    <updated>2025-10-28T18:28:42+00:00</updated>
    <author>
      <name>/u/medi6</name>
      <uri>https://old.reddit.com/user/medi6</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been analysing the Artificial Analysis benchmark set (94 production models, 329 API endpoints) and wanted to share some trends that seem notable.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Context&lt;/strong&gt;&lt;br /&gt; This is models with commercial API access, not the full experimental OS landscape. So mostly models you'd actually deploy out of the box rather than every research models&lt;/p&gt; &lt;p&gt;The gap between best tracked OS (MiniMax-M2, quality 61) and best proprietary (GPT-5, 68) is now 7 points. Last year it was around 18 points in the same dataset. Linear extrapolation suggests parity by Q2 2026 for production-ready models, though obviously that assumes the trend holds (and chinese labs keep shipping OSS models)&lt;/p&gt; &lt;p&gt;What's interesting is the tier distribution:&lt;/p&gt; &lt;p&gt;- Elite (60+): 1 OS, 11 proprietary&lt;br /&gt; - High (50-59): 8 OS, 8 proprietary (we hit parity here)&lt;br /&gt; - Below 50: OS dominates by volume&lt;/p&gt; &lt;p&gt;The economics are pretty stark.&lt;br /&gt; OS average: $0.83/M tokens.&lt;br /&gt; Proprietary: $6.03/M.&lt;br /&gt; Value leaders like Qwen3-235B are hitting 228 quality per dollar vs ~10-20 for proprietary elite models (kind of a random approach but tried playing with this: quality per dollar = quality Index Ã· price/M tokens)&lt;/p&gt; &lt;p&gt;Speed is also shifting. OS on optimised infra (Groq, Fireworks) peaks at 3,087 tok/sec vs 616 for proprietary. Not sure how sustainable that edge is as proprietary invests in inference optimisation.&lt;/p&gt; &lt;p&gt;Made an interactive comparison: &lt;a href="http://whatllm.org"&gt;whatllm.org&lt;/a&gt;&lt;br /&gt; Full write-up: &lt;a href="https://www.whatllm.org/blog/open-source-vs-proprietary-llms-2025"&gt;https://www.whatllm.org/blog/open-source-vs-proprietary-llms-2025&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Two questions I'm chewing on:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;How representative is this benchmark set vs the wider OS ecosystem? AA focuses on API-ready production models, which excludes a lot of experimental work, fine tuned models etc&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Is there a ceiling coming, or does this compression just continue? Chinese labs seem to be iterating faster than I expected.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Curious what others think about the trajectory here.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/medi6"&gt; /u/medi6 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oihbtx/minimaxm2_cracks_top_10_overall_llms_production/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oihbtx/minimaxm2_cracks_top_10_overall_llms_production/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oihbtx/minimaxm2_cracks_top_10_overall_llms_production/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T18:28:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1oiiz8k</id>
    <title>Poker Tournament for LLMs</title>
    <updated>2025-10-28T19:31:37+00:00</updated>
    <author>
      <name>/u/undoing8</name>
      <uri>https://old.reddit.com/user/undoing8</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oiiz8k/poker_tournament_for_llms/"&gt; &lt;img alt="Poker Tournament for LLMs" src="https://b.thumbs.redditmedia.com/Qc-E4Vt6HaSlMs8eFi3HmabBhuyGXwPfxNFOLNxKUVs.jpg" title="Poker Tournament for LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Watch here: &lt;a href="https://pokerbattle.ai/event"&gt;https://pokerbattle.ai/event&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/undoing8"&gt; /u/undoing8 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1oiiz8k"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oiiz8k/poker_tournament_for_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oiiz8k/poker_tournament_for_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T19:31:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1oifmg6</id>
    <title>IBM releases Granite-4.0 Nano (300M &amp; 1B), along with a local browser demo showing how the models can programmatically interact with websites and call tools/browser APIs on your behalf.</title>
    <updated>2025-10-28T17:25:26+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oifmg6/ibm_releases_granite40_nano_300m_1b_along_with_a/"&gt; &lt;img alt="IBM releases Granite-4.0 Nano (300M &amp;amp; 1B), along with a local browser demo showing how the models can programmatically interact with websites and call tools/browser APIs on your behalf." src="https://external-preview.redd.it/c3EyM2o0d2d5dnhmMQ3Ju84xO0NZTaEdmCFfUDcYCN9cnlFCq8u0lL0AKtmD.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8a2e66541a3e76c148bc13a8f5f718ef7b807643" title="IBM releases Granite-4.0 Nano (300M &amp;amp; 1B), along with a local browser demo showing how the models can programmatically interact with websites and call tools/browser APIs on your behalf." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;IBM just released Granite-4.0 Nano, their smallest LLMs to date (300M &amp;amp; 1B). The models demonstrate remarkable instruction following and tool calling capabilities, making them perfect for on-device applications.&lt;/p&gt; &lt;p&gt;Links:&lt;br /&gt; - Blog post: &lt;a href="https://huggingface.co/blog/ibm-granite/granite-4-nano"&gt;https://huggingface.co/blog/ibm-granite/granite-4-nano&lt;/a&gt;&lt;br /&gt; - Demo (+ source code): &lt;a href="https://huggingface.co/spaces/ibm-granite/Granite-4.0-Nano-WebGPU"&gt;https://huggingface.co/spaces/ibm-granite/Granite-4.0-Nano-WebGPU&lt;/a&gt;&lt;/p&gt; &lt;p&gt;+ for those wondering, the demo uses Transformers.js to run the models 100% locally in your browser with WebGPU acceleration.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/s5hzz3wgyvxf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oifmg6/ibm_releases_granite40_nano_300m_1b_along_with_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oifmg6/ibm_releases_granite40_nano_300m_1b_along_with_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T17:25:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1oichb7</id>
    <title>Granite 4.0 Nano Language Models</title>
    <updated>2025-10-28T15:29:41+00:00</updated>
    <author>
      <name>/u/ApprehensiveAd3629</name>
      <uri>https://old.reddit.com/user/ApprehensiveAd3629</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oichb7/granite_40_nano_language_models/"&gt; &lt;img alt="Granite 4.0 Nano Language Models" src="https://external-preview.redd.it/IWIrfsaMSUG5JLRfVdW-aDvE5706Tdr6bIFsDJelbBQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3eae6a377c52b823a98de991ed339474596e018d" title="Granite 4.0 Nano Language Models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;IBM Granite team released Granite 4 Nano models:&lt;/p&gt; &lt;p&gt;1B and 350m versions&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ApprehensiveAd3629"&gt; /u/ApprehensiveAd3629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/ibm-granite/granite-40-nano-language-models"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oichb7/granite_40_nano_language_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oichb7/granite_40_nano_language_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T15:29:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1oibvz1</id>
    <title>Sparse Adaptive Attention â€œMoEâ€: How I Solved OpenAIâ€™s $650B Problem With a Â£700 GPU</title>
    <updated>2025-10-28T15:07:09+00:00</updated>
    <author>
      <name>/u/EconomicConstipator</name>
      <uri>https://old.reddit.com/user/EconomicConstipator</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oibvz1/sparse_adaptive_attention_moe_how_i_solved/"&gt; &lt;img alt="Sparse Adaptive Attention â€œMoEâ€: How I Solved OpenAIâ€™s $650B Problem With a Â£700 GPU" src="https://external-preview.redd.it/kt6Nre_n4gaw93bJ1Gb2EvMMeQYgdiVYcfIxBFW1mNk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3e9fb94008925ef4e956ee562491c3bbbdb7b137" title="Sparse Adaptive Attention â€œMoEâ€: How I Solved OpenAIâ€™s $650B Problem With a Â£700 GPU" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EconomicConstipator"&gt; /u/EconomicConstipator &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://medium.com/@hyborian_/sparse-adaptive-attention-moe-how-i-solved-openais-650b-problem-with-a-700-gpu-343f47b2d6c1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oibvz1/sparse_adaptive_attention_moe_how_i_solved/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oibvz1/sparse_adaptive_attention_moe_how_i_solved/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T15:07:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1oia8fi</id>
    <title>The vLLM team's daily life be like:</title>
    <updated>2025-10-28T14:03:17+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oia8fi/the_vllm_teams_daily_life_be_like/"&gt; &lt;img alt="The vLLM team's daily life be like:" src="https://external-preview.redd.it/ZDF3MmtiYW16dXhmMWptouG6uHo-mrPzGurb2qCOnKrlpr9yhnl7mMdksMxF.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a0aa03383224463057de137e1db2d57ee7e56cd5" title="The vLLM team's daily life be like:" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A massive shout-out to the vLLM team for being the heroes holding it all together so we can actually run all these amazing new models.&lt;/p&gt; &lt;p&gt;And, of course, a huge thank you to all the open-source teams like DeepSeek, Qwen, Kimi, and so many others. You are all pushing the entire field forward. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/lw255camzuxf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oia8fi/the_vllm_teams_daily_life_be_like/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oia8fi/the_vllm_teams_daily_life_be_like/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T14:03:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohdzxs</id>
    <title>AMA Announcement: Liquid AI, the team behind Liquid Foundational Models, LEAP and Apollo (Thu, Oct 30 â€¢ 10 AM â€“ 1 PM PDT)</title>
    <updated>2025-10-27T13:10:46+00:00</updated>
    <author>
      <name>/u/LiquidAI_Team</name>
      <uri>https://old.reddit.com/user/LiquidAI_Team</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohdzxs/ama_announcement_liquid_ai_the_team_behind_liquid/"&gt; &lt;img alt="AMA Announcement: Liquid AI, the team behind Liquid Foundational Models, LEAP and Apollo (Thu, Oct 30 â€¢ 10 AM â€“ 1 PM PDT)" src="https://preview.redd.it/47wfyylmlnxf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c359ceba4921b523ecd2e493f9cc84bd8b3e7881" title="AMA Announcement: Liquid AI, the team behind Liquid Foundational Models, LEAP and Apollo (Thu, Oct 30 â€¢ 10 AM â€“ 1 PM PDT)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;When: Thursday 10/30, 10 AM â€“ 1 PM PST&lt;/h1&gt; &lt;p&gt;The Liquid AI team will also continue answering questions for the following 24 hours, so jump in anytime!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Who will be there:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Jacob Marks (Data)&lt;/li&gt; &lt;li&gt;Jimmy Smith (Pre-Training)&lt;/li&gt; &lt;li&gt;Maxime Labonne (Post-Training)&lt;/li&gt; &lt;li&gt;Fernando Fernandes (Post-training)&lt;/li&gt; &lt;li&gt;Anna Banaszak (LFM2-VL)&lt;/li&gt; &lt;li&gt;Arthur BÃ¶Ã¶k (LFM2-Audio)&lt;/li&gt; &lt;li&gt;Yuri Khrustalev (Inference engine, llama.cpp)&lt;/li&gt; &lt;li&gt;Darian Bhathena (LEAP SDK and Apollo)&lt;/li&gt; &lt;li&gt;Edoardo Mosca (LEAP Best Model Search and Finetune)&lt;/li&gt; &lt;li&gt;Anthony Crognale (LEAP SDK)&lt;/li&gt; &lt;li&gt;Pau Labarta Bajo (Dev Relations)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Want to get started?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;â†’ &lt;a href="https://leap.liquid.ai/models?utm_source=reddit&amp;amp;utm_medium=devrel"&gt;Deploy your first model on-device today&lt;/a&gt;&lt;br /&gt; â†’ &lt;a href="https://huggingface.co/LiquidAI?utm_source=reddit&amp;amp;utm_medium=devrel"&gt;Check out our models on Hugging Face&lt;/a&gt;&lt;br /&gt; â†’ &lt;a href="https://www.liquid.ai/apollo?utm_source=reddit&amp;amp;utm_medium=devrel"&gt;Play with models on Apollo&lt;/a&gt;&lt;br /&gt; â†’ &lt;a href="https://www.liquid.ai/company/news?utm_source=reddit&amp;amp;utm_medium=devrel"&gt;Learn more about our recent releases&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LiquidAI_Team"&gt; /u/LiquidAI_Team &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/47wfyylmlnxf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohdzxs/ama_announcement_liquid_ai_the_team_behind_liquid/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohdzxs/ama_announcement_liquid_ai_the_team_behind_liquid/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T13:10:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohqev8</id>
    <title>Best Local TTS/STT Models - October 2025</title>
    <updated>2025-10-27T21:00:42+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Share what your favorite TTS / STT models are right now &lt;strong&gt;and why.&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Given the the amount of ambiguity and subjectivity in rating/testing these models, please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc. Closed models like Elevenlabs v3 seem to continue to be a few levels above open models, so comparisons, especially empirical ones are welcome.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please use the top level TTS/STT comments to thread your responses. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohqev8/best_local_ttsstt_models_october_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohqev8/best_local_ttsstt_models_october_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohqev8/best_local_ttsstt_models_october_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T21:00:42+00:00</published>
  </entry>
</feed>
