<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-03T04:56:21+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1q2iedp</id>
    <title>I'm new at local AI, I have a question regarding Mini PCs vs Super AI Computers.</title>
    <updated>2026-01-03T02:29:45+00:00</updated>
    <author>
      <name>/u/Fast-Cheetah9944</name>
      <uri>https://old.reddit.com/user/Fast-Cheetah9944</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I see that you can make a Mega-PC with a lot of Nvidia GPUs as pewdiepie did (to give an example), but I also see these mini PCs with shared RAM between the system and the integrated graphics. The thing is that with these mini PCs you can run insanely large models due to the amount of vram you can give to the GPU, so, why would I want to make a super computer with many GPUs if i already get the same result (of being able to run large models) from a cheaper mini PC?&lt;/p&gt; &lt;p&gt;I'm clearly very lost on this so I would really appreciate any explanation at all, and if you are willing to give explanations of this or the difference between Nvidia and AMD GPUs for AI specifically, I would really appreciate it, since that's is the other big doubt I have.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fast-Cheetah9944"&gt; /u/Fast-Cheetah9944 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2iedp/im_new_at_local_ai_i_have_a_question_regarding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2iedp/im_new_at_local_ai_i_have_a_question_regarding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2iedp/im_new_at_local_ai_i_have_a_question_regarding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T02:29:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1q25nh7</id>
    <title>Opensource NMT from Tencent - how good is it?</title>
    <updated>2026-01-02T18:01:11+00:00</updated>
    <author>
      <name>/u/Aware_Self2205</name>
      <uri>https://old.reddit.com/user/Aware_Self2205</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi folks, just stumbled upon &lt;a href="https://github.com/Tencent-Hunyuan/HY-MT"&gt;https://github.com/Tencent-Hunyuan/HY-MT&lt;/a&gt; which claims to be an opensource NMT performing better than many models and commercial translation APIs like Google Cloud translation API. Has anyone tested it already? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aware_Self2205"&gt; /u/Aware_Self2205 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q25nh7/opensource_nmt_from_tencent_how_good_is_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q25nh7/opensource_nmt_from_tencent_how_good_is_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q25nh7/opensource_nmt_from_tencent_how_good_is_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-02T18:01:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1q1ntkh</id>
    <title>I built a simple Web UI for training and running LLM experiments on your local computer! Inspired by minGPT project.</title>
    <updated>2026-01-02T03:27:06+00:00</updated>
    <author>
      <name>/u/Maxwell10206</name>
      <uri>https://old.reddit.com/user/Maxwell10206</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1ntkh/i_built_a_simple_web_ui_for_training_and_running/"&gt; &lt;img alt="I built a simple Web UI for training and running LLM experiments on your local computer! Inspired by minGPT project." src="https://b.thumbs.redditmedia.com/NiQ7WKE58qMIxYT2YbL_iLhmCuWAxahmtcA-k3N_GMw.jpg" title="I built a simple Web UI for training and running LLM experiments on your local computer! Inspired by minGPT project." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was playing around with the open source project called minGPT. And started to build a ton of scripts and running many different training experiments using different datasets I was either download or generating. It became a huge mess quickly and lost track of a lot of things. So I got inspired to build my own local web ui for building datasets, configuration files, running training experiments and inspecting the outputs of LLMs. Thought I would share it here to see what everyone thought or if anything similar exists already xD&lt;/p&gt; &lt;p&gt;You can find it on GitHub here &lt;a href="https://github.com/MaxHastings/llm-madness"&gt;https://github.com/MaxHastings/llm-madness&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Maxwell10206"&gt; /u/Maxwell10206 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1q1ntkh"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1ntkh/i_built_a_simple_web_ui_for_training_and_running/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q1ntkh/i_built_a_simple_web_ui_for_training_and_running/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-02T03:27:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2jdkr</id>
    <title>For those of you who bought DGX OS hardware (e.g. Spark) for local LLM, did all of you flash Ubuntu (or some distro) into it to replace DGX OS to remove the telemetry among other bloats?</title>
    <updated>2026-01-03T03:13:15+00:00</updated>
    <author>
      <name>/u/jinnyjuice</name>
      <uri>https://old.reddit.com/user/jinnyjuice</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2jdkr/for_those_of_you_who_bought_dgx_os_hardware_eg/"&gt; &lt;img alt="For those of you who bought DGX OS hardware (e.g. Spark) for local LLM, did all of you flash Ubuntu (or some distro) into it to replace DGX OS to remove the telemetry among other bloats?" src="https://b.thumbs.redditmedia.com/cjsfvoW7CcuomhDRZNJNHONL9DsXuUxv3kiFVeoUs7g.jpg" title="For those of you who bought DGX OS hardware (e.g. Spark) for local LLM, did all of you flash Ubuntu (or some distro) into it to replace DGX OS to remove the telemetry among other bloats?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For a while, Spark and similar hardware have been talk of the town around YouTube, reddit, Hackernews, etc., or at least I've been exposed to it (non-ads) a lot for local solution. (I understand that there are other solutions out there, but Spark-like solutions came with convenience, performance, specs, among other quantitative and qualitative measures that matched certain thresholds)&lt;/p&gt; &lt;p&gt;However, I should have been more thorough. So many things about it is not very 'local' with telemetry pre-installs, forcing you to connect to Wi-Fi, and other Internet-required bloats. Another factor for the recommendation was lean, but it comes with quite a few unnecessary Nvidia installs. So I've been wondering if others are flashing Ubuntu into it or something along those lines, since I came across such a comment at least once, so now I'm wondering if it's the norm.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;&lt;code&gt;Rant start&lt;/code&gt;&lt;/p&gt; &lt;p&gt;The initial screen from DGX OS for connecting to Wi-Fi definitely belongs in &lt;a href="/r/assholedesign"&gt;/r/assholedesign&lt;/a&gt;. You can't do anything until you actually connect to a Wi-Fi, and I couldn't find any solution online or in the documentation for this. So I thought of connecting my phone's hotspot without data, but I couldn't even find my phone on the AP list. There is no search. There are almost 2000 APs around me, so I have scroll the whole time, and the scrolling is very, very sluggish. Mental.&lt;/p&gt; &lt;p&gt;I finally found it and connected it, but because it doesn't have data, it refused to connect to it. Then I connected my satellite mobile modem to it. Refused again. I tried to search for an answer, but with a help of my friend, we narrowed it down to the mobile modem's DNS. I put adblocking DNS on the modem. Ugh, I guess it comes with telemetry. That's not a very nice 'local' recommendation, is it?&lt;/p&gt; &lt;p&gt;Finally, I connected to my friend's hotspot then immediately disconnected. It rebooted itself automatically. I logged in. Worked fine. I check on Terminal, immediately &lt;code&gt;apt list | grep &amp;quot;telemetry&amp;quot;&lt;/code&gt; among others (see pics). It seems that apt repos updated during the hotspot connection, but that seemed to be about it.&lt;/p&gt; &lt;p&gt;&lt;code&gt;Rant end&lt;/code&gt;&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;And for those of you who didn't flash a different distro on it, what did you do to delete the telemetry bloat? What else did you delete? (Bonus question -- can I delete Nvidia AI Bench and everything else in the pic?)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jinnyjuice"&gt; /u/jinnyjuice &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1q2jdkr"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2jdkr/for_those_of_you_who_bought_dgx_os_hardware_eg/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2jdkr/for_those_of_you_who_bought_dgx_os_hardware_eg/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T03:13:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2f8c9</id>
    <title>Are there any* frontends that allow you to view top token probabilities?</title>
    <updated>2026-01-03T00:11:42+00:00</updated>
    <author>
      <name>/u/Velocita84</name>
      <uri>https://old.reddit.com/user/Velocita84</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;*other than mikupad and sillytavern&lt;/p&gt; &lt;p&gt;I'm using Qwen3 vl 8b with llama.cpp to OCR text from japanese artwork, it's the most accurate model for this that i've tried, but it still sometimes gets a character wrong or omits it entirely. I'm sure the correct prediction is somewhere in the top tokens, so if i had access to them i could easily correct my outputs.&lt;/p&gt; &lt;p&gt;I'd also like to know if any popular frontends (e.g. OpenWebUI) that don't usually support logprobs have extensions or similar (i'm not familiar with any of them) that implement it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Velocita84"&gt; /u/Velocita84 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2f8c9/are_there_any_frontends_that_allow_you_to_view/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2f8c9/are_there_any_frontends_that_allow_you_to_view/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2f8c9/are_there_any_frontends_that_allow_you_to_view/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T00:11:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2c520</id>
    <title>DGX Spark Rack Setup and Cooling Solution</title>
    <updated>2026-01-02T22:05:10+00:00</updated>
    <author>
      <name>/u/MLisdabomb</name>
      <uri>https://old.reddit.com/user/MLisdabomb</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2c520/dgx_spark_rack_setup_and_cooling_solution/"&gt; &lt;img alt="DGX Spark Rack Setup and Cooling Solution" src="https://b.thumbs.redditmedia.com/djKIM-yp7wuHiZ6G9KBCek_HXRLkXJe9z8_svrMHKMw.jpg" title="DGX Spark Rack Setup and Cooling Solution" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you own a DGX Spark you know that it can get pretty toasty during training runs. I built a DeskPI Rack and hooked up an automated temperature controller that controls the fan speed based on the case temperature. At below 30C the fans are off and at 35C the fans are on full blast. With this setup I am able to keep the max temps hovering around 72C during training.&lt;/p&gt; &lt;p&gt;Posting for informational purposes in case this helps someone figure out their setup.&lt;/p&gt; &lt;p&gt;Temp Monitoring Code: &lt;a href="https://github.com/cgpadwick/system-temp-monitor"&gt;https://github.com/cgpadwick/system-temp-monitor&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Parts List:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Deskpi Rackmate T2&lt;/li&gt; &lt;li&gt;Noctua Fan 80mm x 2&lt;/li&gt; &lt;li&gt;Heavy duty shelfs from Geeekpi&lt;/li&gt; &lt;li&gt;Vented front panel from Geeekpi&lt;/li&gt; &lt;li&gt;NVIDIA Spark DGX&lt;/li&gt; &lt;li&gt;PDU Elecvoztile&lt;/li&gt; &lt;li&gt;Patch panel Geeekpi&lt;/li&gt; &lt;li&gt;KCEVE KVM Switch&lt;/li&gt; &lt;li&gt;Netgear 5-port switch&lt;/li&gt; &lt;li&gt;ICSTATION DC 12V PWM 4-Wire Fan Speed Controller Module with Temperature probe&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/y5iuwrped0bg1.jpg?width=316&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=5b27bbd9d3c96fa765c8c1d2660198990b766933"&gt;https://preview.redd.it/y5iuwrped0bg1.jpg?width=316&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=5b27bbd9d3c96fa765c8c1d2660198990b766933&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/2aqzcqggd0bg1.png?width=960&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=090f8385174e82b5ba165871f158a9fb88b9ebc3"&gt;https://preview.redd.it/2aqzcqggd0bg1.png?width=960&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=090f8385174e82b5ba165871f158a9fb88b9ebc3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7a81llgid0bg1.png?width=1972&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a543e8280910103cbb6df837795605b31dd981c2"&gt;https://preview.redd.it/7a81llgid0bg1.png?width=1972&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a543e8280910103cbb6df837795605b31dd981c2&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MLisdabomb"&gt; /u/MLisdabomb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2c520/dgx_spark_rack_setup_and_cooling_solution/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2c520/dgx_spark_rack_setup_and_cooling_solution/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2c520/dgx_spark_rack_setup_and_cooling_solution/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-02T22:05:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1q1lgb7</id>
    <title>TIL you can allocate 128 GB of unified memory to normal AMD iGPUs on Linux via GTT</title>
    <updated>2026-01-02T01:37:11+00:00</updated>
    <author>
      <name>/u/1ncehost</name>
      <uri>https://old.reddit.com/user/1ncehost</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I am training a 1B model right now on my 7900 XTX with some custom kernels I wrote, and while it is training I wanted to optimize the kernels at the same time. However, my VRAM is nearly maxed doing training, so its not ideal.&lt;/p&gt; &lt;p&gt;Then I realized maybe my 2 CU Raphael iGPU might be able to help since I only need to run some limited samples and the speed isn't as important for optimization as it is for training. After doing some research, it turned out that not only does ROCm recognize the iGPU, but a Linux feature called Graphics Translation Table (GTT) for AMD iGPUs can use up to 128 GB of system memory as VRAM. It even allocates it dynamically, so it isn't removed from your CPU's memory pool until it is allocated. I think a lot of people running Strix Halo are probably using the bios setting, but if you are running Linux you should check to see if GTT works for you since its dynamically allocated.&lt;/p&gt; &lt;p&gt;This isn't very useful for most people:&lt;/p&gt; &lt;p&gt;1) It isn't going to be good for inference because iGPUs are very very slow, and usually the CPU itself is faster for inference.&lt;/p&gt; &lt;p&gt;2) I'm accessing ROCm directly via C++ / HIP kernels, so I can avoid all the support issues ROCm has for iGPUs in the python stack&lt;/p&gt; &lt;p&gt;However, for development it is actually pretty awesome. I allocated 24 GB of GTT so now the iGPU can load a full training run that my main GPU can run so I can profile it. Meanwhile my main GPU is doing long term loss convergence tests in parallel. Since RDNA iGPUs have been around for a while now, this enables big memory AMD GPU kernel development for cheap.&lt;/p&gt; &lt;p&gt;Also it might be interesting for developing hybrid CPU/GPU architectures. The MI300A does exist which has unified HBM tied to a CPU and giant iGPU. A standard ryzen laptop could kind of sort of simulate it for cheap. Stuff like vector indexing on the CPU into big GEMMs on the GPU could be done without PCIE overhead.&lt;/p&gt; &lt;p&gt;I thought it was cool enough to post. Probably a &amp;quot;Cool story bro&amp;quot; moment for most of you though haha.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1ncehost"&gt; /u/1ncehost &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1lgb7/til_you_can_allocate_128_gb_of_unified_memory_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1lgb7/til_you_can_allocate_128_gb_of_unified_memory_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q1lgb7/til_you_can_allocate_128_gb_of_unified_memory_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-02T01:37:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1q27ogv</id>
    <title>I built a CLI tool for forensic analysis because Llama 3 kept hallucinating comparisons.</title>
    <updated>2026-01-02T19:14:18+00:00</updated>
    <author>
      <name>/u/PaperTraditional7784</name>
      <uri>https://old.reddit.com/user/PaperTraditional7784</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q27ogv/i_built_a_cli_tool_for_forensic_analysis_because/"&gt; &lt;img alt="I built a CLI tool for forensic analysis because Llama 3 kept hallucinating comparisons." src="https://preview.redd.it/41g6asvpjzag1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=89b9b4028110ccede945943e4a04f8dc2aa57601" title="I built a CLI tool for forensic analysis because Llama 3 kept hallucinating comparisons." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I‚Äôve been working on &lt;strong&gt;LLM-Cerebroscope&lt;/strong&gt;, a Python CLI tool that uses local LLMs (Ollama + Llama 3) to detect contradictions between documents (e.g., Invoice vs. Delivery Report).&lt;/p&gt; &lt;p&gt;I hit a wall recently: when two conflicting documents had the exact same reliability score (e.g., 75/100), the model would often hallucinate a &amp;quot;winner&amp;quot; or make up math just to provide a verdict.&lt;/p&gt; &lt;p&gt;I implemented a strict &amp;quot;Logic Engine&amp;quot; in the system prompt that forces a deterministic tie-breaker based on timestamps. Now, instead of guessing, it outputs: &lt;em&gt;&amp;quot;Trust X because it is more recent (reliability scores are tied).&amp;quot;&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The tool features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Local Inference: 100% offline using Ollama.&lt;/li&gt; &lt;li&gt;Conflict Detection: Doesn't just summarize; it looks for logical mismatches.&lt;/li&gt; &lt;li&gt;UI: Built with Rich for a terminal-based dashboard feel.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;I‚Äôm looking for feedback on the architecture and the prompt engineering part. Has anyone else struggled with LLMs failing basic comparison logic in RAG?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Repo:&lt;/strong&gt; &lt;a href="https://github.com/oskarbrzycki/llm-cerebroscope"&gt;https://github.com/oskarbrzycki/llm-cerebroscope&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PaperTraditional7784"&gt; /u/PaperTraditional7784 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/41g6asvpjzag1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q27ogv/i_built_a_cli_tool_for_forensic_analysis_because/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q27ogv/i_built_a_cli_tool_for_forensic_analysis_because/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-02T19:14:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1q1w2tg</id>
    <title>88% vs 76%: Multimodal outperforms text embeddings on visual docs in RAG</title>
    <updated>2026-01-02T11:16:15+00:00</updated>
    <author>
      <name>/u/midamurat</name>
      <uri>https://old.reddit.com/user/midamurat</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Building a RAG system for docs with mixed content: text, tables, charts. I wanted to know if multimodal embeddings are worth it or if text would be just fine.&lt;/p&gt; &lt;p&gt;Decided to test it out. I had two approaches:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Convert everything to text, use text embeddings&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Keep images as images, use multimodal embeddings&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;After running 150 queries on identical setups across DocVQA (text + tables), ChartQA (charts), and AI2D (diagrams):&lt;/p&gt; &lt;p&gt;Results of Recall@1:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Tables = multimodal 88%, text 76% (12-point gap)&lt;/li&gt; &lt;li&gt;Charts = multimodal 92%, text 90% (small edge)&lt;/li&gt; &lt;li&gt; Pure text = text 96%, multimodal 92% (text wins)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Takeaway: for dealing with visual docs, multimodal seem to be the better default. But for pure text, text embeddings would be enough. &lt;/p&gt; &lt;p&gt;(posted a write-up of full breakdown here: &lt;a href="https://agentset.ai/blog/multimodal-vs-text-embeddings"&gt;https://agentset.ai/blog/multimodal-vs-text-embeddings&lt;/a&gt; )&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/midamurat"&gt; /u/midamurat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1w2tg/88_vs_76_multimodal_outperforms_text_embeddings/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1w2tg/88_vs_76_multimodal_outperforms_text_embeddings/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q1w2tg/88_vs_76_multimodal_outperforms_text_embeddings/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-02T11:16:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1q261ht</id>
    <title>anyone else externalizing context to survive the memory wipe?</title>
    <updated>2026-01-02T18:15:12+00:00</updated>
    <author>
      <name>/u/Massive-Ballbag</name>
      <uri>https://old.reddit.com/user/Massive-Ballbag</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;been running multiple projects with claude/gpt/local models and the context reset every session was killing me. started dumping everything to github - project state, decision logs, what to pick up next - parsing and loading it back in on every new chat&lt;/p&gt; &lt;p&gt;basically turned it into a boot sequence. load the project file, load the last session log, keep going&lt;/p&gt; &lt;p&gt;feels hacky but it works. curious if anyone else is doing something similar or if there's a better approach I'm missing&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Massive-Ballbag"&gt; /u/Massive-Ballbag &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q261ht/anyone_else_externalizing_context_to_survive_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q261ht/anyone_else_externalizing_context_to_survive_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q261ht/anyone_else_externalizing_context_to_survive_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-02T18:15:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2afpr</id>
    <title>üç≥ Cook High Quality Custom GGUF Dynamic Quants ‚Äî right from your web browser</title>
    <updated>2026-01-02T20:59:07+00:00</updated>
    <author>
      <name>/u/Thireus</name>
      <uri>https://old.reddit.com/user/Thireus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've just published a web front-end that wraps the GGUF Tool Suite's &lt;code&gt;quant_assign.py&lt;/code&gt; so you can produce high-quality dynamic GGUF quants without touching the command line. Everything is integrated in the browser: upload or pick calibration/deg CSVs, tune advanced options in a friendly UI, and export a &lt;code&gt;.recipe&lt;/code&gt; tuned to your hardware in seconds.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why this exists&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Making GGUF quantization accessible: no more wrestling with terminals, dependency hell or manual piping. If you want precise, automated, system-tuned GGUF dynamic quant production ‚Äî but prefer a web-first experience ‚Äî this is for you.&lt;/p&gt; &lt;hr /&gt; &lt;h3&gt;üî• Cook High Quality Custom GGUF Dynamic Quants in 3 Steps&lt;/h3&gt; &lt;p&gt;&lt;em&gt;‚ú® Target exact VRAM/RAM sizes. Mix quant types. Done in minutes!&lt;/em&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;üç≥ &lt;strong&gt;Step 1 ‚Äî Generate a GGUF recipe&lt;/strong&gt;: open &lt;code&gt;quant_assign.html&lt;/code&gt; and let the UI size a recipe for your hardware.&lt;br /&gt; &lt;a href="https://gguf.thireus.com/quant_assign.html"&gt;https://gguf.thireus.com/quant_assign.html&lt;/a&gt;&lt;/li&gt; &lt;li&gt;‚òÅÔ∏è &lt;strong&gt;Step 2 ‚Äî Download GGUF files&lt;/strong&gt;: feed the recipe into &lt;code&gt;quant_downloader.html&lt;/code&gt; and grab the GGUFs.&lt;br /&gt; &lt;a href="https://gguf.thireus.com/quant_downloader.html"&gt;https://gguf.thireus.com/quant_downloader.html&lt;/a&gt;&lt;/li&gt; &lt;li&gt;üöÄ &lt;strong&gt;Step 3 ‚Äî Run anywhere&lt;/strong&gt;: use &lt;code&gt;llama.cpp&lt;/code&gt;, &lt;code&gt;ik_llama.cpp&lt;/code&gt;, or any GGUF-compatible runtime.&lt;/li&gt; &lt;/ol&gt; &lt;hr /&gt; &lt;p&gt;&lt;strong&gt;A few notes&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;GLM-4.7 calibration data is coming soon ‚Äî subscribe to this issue for updates: &lt;a href="https://github.com/Thireus/GGUF-Tool-Suite/issues/50"&gt;https://github.com/Thireus/GGUF-Tool-Suite/issues/50&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thireus"&gt; /u/Thireus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2afpr/cook_high_quality_custom_gguf_dynamic_quants/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2afpr/cook_high_quality_custom_gguf_dynamic_quants/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2afpr/cook_high_quality_custom_gguf_dynamic_quants/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-02T20:59:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1q1ura1</id>
    <title>[IQuestLab/IQuest-Coder-V1] SWE-bench score is compromised because environment setup was wrong</title>
    <updated>2026-01-02T09:57:34+00:00</updated>
    <author>
      <name>/u/nullmove</name>
      <uri>https://old.reddit.com/user/nullmove</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TL;DR is that they didn't clean the repo (.git/ folder), model just reward hacked its way to look up future commits with fixes. Credit goes to everyone in this thread for solving this: &lt;a href="https://xcancel.com/xeophon/status/2006969664346501589"&gt;https://xcancel.com/xeophon/status/2006969664346501589&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(given that IQuestLab published their SWE-Bench Verified trajectory data, I want to be charitable and assume genuine oversight rather than &amp;quot;benchmaxxing&amp;quot;, probably an easy to miss thing if you are new to benchmarking)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nullmove"&gt; /u/nullmove &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1ura1/iquestlabiquestcoderv1_swebench_score_is/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1ura1/iquestlabiquestcoderv1_swebench_score_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q1ura1/iquestlabiquestcoderv1_swebench_score_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-02T09:57:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2et07</id>
    <title>I got frustrated dealing with massive responses from many MCPs and threw something together over the last couple days... it might help you too. Or not!</title>
    <updated>2026-01-02T23:53:31+00:00</updated>
    <author>
      <name>/u/steezy13312</name>
      <uri>https://old.reddit.com/user/steezy13312</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLlama"&gt;/r/LocalLlama&lt;/a&gt;, I spent the last couple of days working on a little personal project and figured I‚Äôd share. &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/samteezy/mcp-context-proxy/"&gt;https://github.com/samteezy/mcp-context-proxy/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Background: As a relatively low-investment homelabber, I'm always battling context size and chasing optimal prompt processing/token generation speeds.&lt;/p&gt; &lt;p&gt;I don‚Äôt mean to pick on this one in particular, but a MCP that really got me frustrated was an otherwise very &lt;a href="https://github.com/sirkirby/unifi-network-mcp"&gt;well built MCP&lt;/a&gt; that allows you to extract data from your UniFi network devices. I was working with it to build documentation of my home network, and I was finding it was giving me response payloads from the UniFi API that had a ton of extra data which started just filling up my context and taking &lt;em&gt;forever&lt;/em&gt; for gpt-oss-120b to process. I don't blame the author - this is just a fundamental failing in current MCP implementation; MCPs are meant to help give instruction but there's no special solution to optimizing number of tokens returned (there's no free lunch).&lt;/p&gt; &lt;p&gt;I love small models like those from Qwen and Liquid AI, and I have &lt;code&gt;llama-swap&lt;/code&gt; configured to always have a small task model in the background for tools like Karakeep and Open WebUI to use... so what if I could use this for basically compressing any MCP response?&lt;/p&gt; &lt;p&gt;So I decided to turn Claude Code onto the problem and create a little tool that we have here. It is an MCP which acts a transparent proxy, oriented towards the home lab/context poor user with the following features/benefits:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Transparently presents MCP tools to the client, but allows you to preprocess the MCP's response before sending it back to the client LLM (ideally you use a locally hosted LLM, but could also make remote callouts to the cloud to a super inexpensive or free API via something like OpenRouter)&lt;/li&gt; &lt;li&gt;Uses a simple in-memory cache for caching responses for identical requests&lt;/li&gt; &lt;li&gt;Allows disabling individual tools or overwriting the upstream tool descriptions to better control context size and tool selection accuracy when launching an agent&lt;/li&gt; &lt;li&gt;Adds capability to intercept outgoing tool calls and incoming MCP responses for things like PII masking or prompt injection (future)&lt;/li&gt; &lt;li&gt;One proxy for managing multiple MCPs; great for if you're playing with multiple AI tools/coding assistants and hate having to reconfigure MCPs for each one&lt;/li&gt; &lt;li&gt;Very configurable options to override behavior globally or different tools via a single JSON file, plus a UI for management and visibility&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I've been testing with a high-quant Qwen3-0.6b and LFM2-1.2b and it's doing very well for me. For example, I have it use web search and fetch for URLs and instead of having the larger model process the entire pages, the tiny model reads the page up to 10x faster, and just gives the large model the answers it needs, also keeping context lower. It's made using tools like search and fetch worthwhile. YMMV.&lt;/p&gt; &lt;p&gt;It is not:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Being monetized or going to make you a ton of money&lt;/li&gt; &lt;li&gt;Guaranteed to work in a high-stress environment (not that it's emotionally sensitive, just that I don't know where its performance limits are)&lt;/li&gt; &lt;li&gt;Completely revolutionary&lt;/li&gt; &lt;li&gt;Going to solve all of MCP flaws and failings&lt;/li&gt; &lt;li&gt;Going to make your ex take you back&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;And yes, it is vibe coded... so of course take it with a grain of salt, but I use these tools professionally and understand how to use AI as a coding assistant rather than an expert. Don't like that? Fork it and have AI inspect it yourself. Or write your own. Or do whatever, &lt;a href="https://tenor.com/view/youre-not-my-supervisor-youre-not-my-boss-gif-12971403"&gt;I'm not your supervisor&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm planning on adding in optional prompt injection review (curious about some of IBM's and others' models out there more to understand how they work) and seeing how well I can get the masking side working. I haven't tested that a ton yet. I'm also playing around with the idea of adding an optional override for the client LLM to bypass content summarization, but I feel like that risks defeating the purpose.&lt;/p&gt; &lt;p&gt;Hope this helps you get more value out of the hardware and setup you currently have.&lt;/p&gt; &lt;p&gt;Note, I'm not super familiar with publishing npm packages and open source projects, so I might not be doing versioning or other things by-the-book... open to any constructive criticism on how you see things set up and structured so far.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/steezy13312"&gt; /u/steezy13312 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2et07/i_got_frustrated_dealing_with_massive_responses/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2et07/i_got_frustrated_dealing_with_massive_responses/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2et07/i_got_frustrated_dealing_with_massive_responses/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-02T23:53:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1q1p5q5</id>
    <title>Getting ready to train in Intel arc</title>
    <updated>2026-01-02T04:33:19+00:00</updated>
    <author>
      <name>/u/hasanismail_</name>
      <uri>https://old.reddit.com/user/hasanismail_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1p5q5/getting_ready_to_train_in_intel_arc/"&gt; &lt;img alt="Getting ready to train in Intel arc" src="https://a.thumbs.redditmedia.com/iosPXZzRPv_Wd4_Y3UYcEyKFQKwEaHWvSBwjyGk3uo8.jpg" title="Getting ready to train in Intel arc" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just waiting on pcie risers can't wait to start training on Intel arc I'm not sure in anyone else is attempting the same thing yet so I though I would share &lt;/p&gt; &lt;p&gt;PS. I am not causing a GPU shortage pls dont comment about this I am not open ai or google believe me there would have been signs on my other posts gamers would say sh*t like this so before u comment please educate yourselves&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hasanismail_"&gt; /u/hasanismail_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1q1p5q5"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1p5q5/getting_ready_to_train_in_intel_arc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q1p5q5/getting_ready_to_train_in_intel_arc/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-02T04:33:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1q1uyf6</id>
    <title>New Models from South Korea's Sovereign AI Foundation Model Project</title>
    <updated>2026-01-02T10:09:28+00:00</updated>
    <author>
      <name>/u/chibop1</name>
      <uri>https://old.reddit.com/user/chibop1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've seen posts with individual models here and there, but not together in one post. Also I'm including some English articles I found about the project.&lt;/p&gt; &lt;p&gt;It's bit old news, but the South Korean government funded the Sovereign AI Foundation Model Project, and the five selected teams released their initial models and presented on December 30, 2025.&lt;/p&gt; &lt;p&gt;Below are the repos I was able to track down on Huggingface, but please let me know if I missed or included wrong repo.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Naver Cloud: &lt;a href="https://huggingface.co/naver-hyperclovax/HyperCLOVAX-SEED-Omni-8B"&gt;HyperCLOVAX-SEED-Omni-8B &lt;/a&gt;, &lt;a href="https://huggingface.co/naver-hyperclovax/HyperCLOVAX-SEED-Think-32B"&gt;HyperCLOVAX-SEED-Think-32B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Upstage: &lt;a href="https://huggingface.co/upstage/Solar-Open-100B"&gt;Solar-Open-102B-A12B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;SK Telecom: &lt;a href="https://huggingface.co/skt/A.X-K1"&gt;A.X-K1-519B-A33B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;LG AI Research: &lt;a href="https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B"&gt;K-EXAONE-236B-A23B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;NC AI: &lt;a href="https://huggingface.co/NC-AI-consortium-VAETKI/VAETKI"&gt;VAETKI-112B-A10B&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;South Koreas current president ran with AI as one of his prominent campaign themes, and the government &lt;a href="https://www.chosun.com/english/market-money-en/2025/12/11/25QXCJ2Q4NCINOC5EXP6SR3QYQ/"&gt;pledged to invest&lt;/a&gt; 30T KRW (20.8B USD) in the AI sector over five years , roughly 0.23% of GDP per year, as part of National Growth Fund.&lt;/p&gt; &lt;p&gt;It looks like MSIT is backing the project with funding, GPUs, and datasets. Teams will be evaluated and eliminated through 2026 and into mid 2027 until two finalists.&lt;/p&gt; &lt;p&gt;Also it said all 5 teams &amp;quot;presented robust open-source policies so that foundation models they develop and release can also be used commercially by other companies, thereby contributing in many ways to expansion of the domestic AI ecosystem, to the acceleration of diverse AI services, and to improved public access to AI.&amp;quot;&lt;/p&gt; &lt;p&gt;You can read more about the project below:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.msit.go.kr/eng/bbs/view.do?bbsSeqNo=42&amp;amp;mId=4&amp;amp;nttSeqNo=1152&amp;amp;sCode=eng"&gt;https://www.msit.go.kr/eng/bbs/view.do?bbsSeqNo=42&amp;amp;mId=4&amp;amp;nttSeqNo=1152&amp;amp;sCode=eng&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.upi.com/Top_News/World-News/2025/12/30/ai-model-national-project/7441767133090/"&gt;https://www.upi.com/Top_News/World-News/2025/12/30/ai-model-national-project/7441767133090/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.koreatimes.co.kr/business/tech-science/20251230/consortia-unveil-models-for-national-ai-project"&gt;https://www.koreatimes.co.kr/business/tech-science/20251230/consortia-unveil-models-for-national-ai-project&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chibop1"&gt; /u/chibop1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1uyf6/new_models_from_south_koreas_sovereign_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1uyf6/new_models_from_south_koreas_sovereign_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q1uyf6/new_models_from_south_koreas_sovereign_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-02T10:09:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2eau3</id>
    <title>My New Year's resolution was to add Docker support. Only 2 days late. Audiobook Maker v1.1.0</title>
    <updated>2026-01-02T23:32:12+00:00</updated>
    <author>
      <name>/u/DigiJoe79</name>
      <uri>https://old.reddit.com/user/DigiJoe79</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2eau3/my_new_years_resolution_was_to_add_docker_support/"&gt; &lt;img alt="My New Year's resolution was to add Docker support. Only 2 days late. Audiobook Maker v1.1.0" src="https://a.thumbs.redditmedia.com/TNdBuFemSJ4jRJ66xw8XJAlZ-H0HvTXr_8DHB2mjLq8.jpg" title="My New Year's resolution was to add Docker support. Only 2 days late. Audiobook Maker v1.1.0" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;!&lt;/p&gt; &lt;p&gt;About three weeks ago I shared my passion project here - an app to create audiobooks from text using local TTS engines like XTTS and Chatterbox. &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1piduwm/i_wanted_audiobooks_of_stories_that_dont_exist_so/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1piduwm/i_wanted_audiobooks_of_stories_that_dont_exist_so/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The response was amazing and motivated me to keep going. Special shoutout to &lt;a href="https://github.com/codesterribly"&gt;https://github.com/codesterribly&lt;/a&gt; who pushed me to tackle Docker support - you were right, it was worth it!&lt;/p&gt; &lt;p&gt;So here's my slightly-late New Year's gift to the community: v1.1.0 üéÅ&lt;/p&gt; &lt;p&gt;What's New?&lt;/p&gt; &lt;p&gt;Docker-First Architecture&lt;/p&gt; &lt;ul&gt; &lt;li&gt;No more Python environment hell! Engines come as prebuilt Docker images&lt;/li&gt; &lt;li&gt;One-click installation from the online catalog&lt;/li&gt; &lt;li&gt;Works on Windows, Linux, and partially with macOS (Apple Silicon)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Remote GPU Offloading&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Got a beefy GPU server in your closet? Run VibeVoice 7B there via SSH&lt;/li&gt; &lt;li&gt;Your laptop stays cool while the server does the heavy lifting&lt;/li&gt; &lt;li&gt;Built-in SSH key wizard - no manual config needed&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;New TTS Engine: VibeVoice&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Microsoft's long-form multi-speaker TTS&lt;/li&gt; &lt;li&gt;Great for podcasts and dialogues&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Quick Start&lt;/p&gt; &lt;h1&gt;Pull the backend&lt;/h1&gt; &lt;p&gt;docker pull ghcr.io/digijoe79/audiobook-maker/backend:latest&lt;/p&gt; &lt;h1&gt;Run it&lt;/h1&gt; &lt;p&gt;&lt;code&gt;docker run -d --name audiobook-maker-backend \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-p 8765:8765 \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;--add-host=host.docker.internal:host-gateway \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-e DOCKER_ENGINE_HOST=host.docker.internal \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-v /var/run/docker.sock:/var/run/docker.sock \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-v audiobook-data-path:/app/data \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-v audiobook-media-path:/app/media \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;ghcr.io/digijoe79/audiobook-maker/backend:latest&lt;/code&gt; &lt;/p&gt; &lt;p&gt;Then grab the desktop app, connect, and install engines from the catalog. That's it!&lt;/p&gt; &lt;p&gt;Links&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/DigiJoe79/audiobook-maker"&gt;https://github.com/DigiJoe79/audiobook-maker&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/DigiJoe79/audiobook-maker/releases/tag/v1.1.0"&gt;https://github.com/DigiJoe79/audiobook-maker/releases/tag/v1.1.0&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/DigiJoe79/audiobook-maker/tree/main/docs/samples"&gt;https://github.com/DigiJoe79/audiobook-maker/tree/main/docs/samples&lt;/a&gt; (Moby Dick previews)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What's Next?&lt;/p&gt; &lt;p&gt;Already thinking about v1.2.0 - better batch processing, more for Apple Silicon. Open to suggestions!&lt;/p&gt; &lt;p&gt;Thanks again for all the feedback on the original post. This community is awesome. üôè&lt;/p&gt; &lt;p&gt;Happy (belated) New Year, and happy listening!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DigiJoe79"&gt; /u/DigiJoe79 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1q2eau3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2eau3/my_new_years_resolution_was_to_add_docker_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2eau3/my_new_years_resolution_was_to_add_docker_support/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-02T23:32:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2jazd</id>
    <title>Part 4 (Finale): Building LLMs from Scratch ‚Äì Evaluation &amp; Deployment [Follow-up to Parts 1, thru 3]</title>
    <updated>2026-01-03T03:10:03+00:00</updated>
    <author>
      <name>/u/amitbahree</name>
      <uri>https://old.reddit.com/user/amitbahree</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Happy new year! I‚Äôm excited to share &lt;strong&gt;Part 4&lt;/strong&gt; (and the final part) of my series on building an LLM from scratch.&lt;/p&gt; &lt;p&gt;This installment covers the ‚Äúokay, but does it &lt;em&gt;work&lt;/em&gt;?‚Äù phase: evaluation, testing, and deployment - taking the trained models from Part 3 and turning them into something you can validate, iterate on, and actually share/use (including publishing to HF).&lt;/p&gt; &lt;p&gt;What you‚Äôll find inside:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A practical evaluation framework (quick vs comprehensive) for historical language models (not just perplexity).&lt;/li&gt; &lt;li&gt;Tests and validation patterns: historical accuracy checks, linguistic checks, temporal consistency, and basic performance sanity checks.&lt;/li&gt; &lt;li&gt;Deployment paths: &lt;ul&gt; &lt;li&gt;local inference from PyTorch checkpoints&lt;/li&gt; &lt;li&gt;Hugging Face Hub publishing + model cards&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;CI-ish smoke checks you can run on CPU to catch obvious regressions.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Why it matters?&lt;br /&gt; Training is only half the battle. Without evaluation + tests + a repeatable publishing workflow, you can easily end up with a model that ‚Äútrains fine‚Äù but is unreliable, inconsistent, or impossible for others to reproduce/use. This post focuses on making the last mile boring (in the best way).&lt;/p&gt; &lt;p&gt;Resources:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;üîó Blog post (Part 4) - &lt;a href="https://blog.desigeek.com/post/2026/01/building-llm-from-scratch-part4-evaluation-deployment/"&gt;Evaluations and Deployment&lt;/a&gt;&lt;/li&gt; &lt;li&gt;üîó GitHub repo: &lt;a href="https://github.com/bahree/helloLondon"&gt;https://github.com/bahree/helloLondon&lt;/a&gt;&lt;/li&gt; &lt;li&gt;üîó Hugging Face: &lt;a href="https://huggingface.co/bahree"&gt;https://huggingface.co/bahree&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In case you are interested in the previous parts&lt;/p&gt; &lt;ul&gt; &lt;li&gt;üîó Part 3 - &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1oluay3/part_3_building_llms_from_scratch_model/"&gt;Model Architecture &amp;amp; GPU Training&lt;/a&gt;&lt;/li&gt; &lt;li&gt;üîó Part 2 - &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1o562l3/part_2_building_llms_from_scratch_data_collection/"&gt;Data Collection &amp;amp; Custom Tokenizers&lt;/a&gt;&lt;/li&gt; &lt;li&gt;üîó Part 1 - &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1npzstw/a_step_by_step_guide_on_how_to_build_a_llm_from/"&gt;Quick Start &amp;amp; Overview&lt;/a&gt;&lt;/li&gt; &lt;li&gt;üîó LinkedIn &lt;a href="https://www.linkedin.com/posts/amitbahree_building-llms-from-scratch-part-4-evaluation-activity-7413050136974700544-0OwB/"&gt;post&lt;/a&gt; (if that is your thing).&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/amitbahree"&gt; /u/amitbahree &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2jazd/part_4_finale_building_llms_from_scratch/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2jazd/part_4_finale_building_llms_from_scratch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2jazd/part_4_finale_building_llms_from_scratch/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T03:10:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2jwsn</id>
    <title>How is Cloud Inference so cheap</title>
    <updated>2026-01-03T03:37:13+00:00</updated>
    <author>
      <name>/u/VolkoTheWorst</name>
      <uri>https://old.reddit.com/user/VolkoTheWorst</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How do cloud inference companies like DeepInfra, Together, Chutes, Novita etc manage to be in profit regarding to the price of the GPUs/electricity and the fact that I guess it's difficult to have always someone to serve ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VolkoTheWorst"&gt; /u/VolkoTheWorst &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2jwsn/how_is_cloud_inference_so_cheap/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2jwsn/how_is_cloud_inference_so_cheap/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2jwsn/how_is_cloud_inference_so_cheap/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T03:37:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1q21zql</id>
    <title>Industry Update: Supermicro Policy on Standalone Motherboards Sales Discontinued ‚Äî Spectrum Sourcing</title>
    <updated>2026-01-02T15:47:29+00:00</updated>
    <author>
      <name>/u/FullstackSensei</name>
      <uri>https://old.reddit.com/user/FullstackSensei</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q21zql/industry_update_supermicro_policy_on_standalone/"&gt; &lt;img alt="Industry Update: Supermicro Policy on Standalone Motherboards Sales Discontinued ‚Äî Spectrum Sourcing" src="https://external-preview.redd.it/hL9IlSGjDxXkUJI7Ss74xpCqhzcKCtv4f-p8hzWkQ_U.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=704790efb261d2a51abf0f339a4ace74d00ec32c" title="Industry Update: Supermicro Policy on Standalone Motherboards Sales Discontinued ‚Äî Spectrum Sourcing" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This isn't new, but somehow I missed it, and figure many in this community might also not be aware of this.&lt;/p&gt; &lt;p&gt;The TLDR, as the title says: Supermicro is stopping standalone motherboard sales and now selling only entire servers. As if things weren't already bad enough...&lt;/p&gt; &lt;p&gt;I had noticed an uptick in used board prices on ebay, local ads, and tech forums but didn't have an explanation for it. This explains why.&lt;/p&gt; &lt;p&gt;While most discussions in this community center around consumer boards, workstation and server boards offer so many more features and functionality, and used to be much cheaper than their desktop counterparts.&lt;/p&gt; &lt;p&gt;Supermicro was arguably the largest supplier of such boards, and with them stopping motherboard sales, all workstation and server boards in standard industry form-factor (EATX, ATX, MATX, IT, and SSE variants) will have a sharp drop in availability in the foreseeable future.&lt;/p&gt; &lt;p&gt;Add to that the sharp increase in RAM prices, and you can see why many businesses will be hesitant to move to newer DDR5 server platforms and instead choose to stock to DDR4 platforms to reuse their existing memory. I suspect many will consolidate their existing DDR4 based Xeon and early Epyc (Naples) to Epyc Milan servers using existing market supply of servers and boards.&lt;/p&gt; &lt;p&gt;We're barely in 2026, but it's looking like this year will squeeze us, consumer, even more than 2025 has.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullstackSensei"&gt; /u/FullstackSensei &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.spectrumsourcing.com/spectrum-news-feed/industry-update-supermicro-policy-on-standalone-motherboards-sales-discontinued"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q21zql/industry_update_supermicro_policy_on_standalone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q21zql/industry_update_supermicro_policy_on_standalone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-02T15:47:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1q1w1qj</id>
    <title>Most optimal vram/performance per price and advice for Shenzhen GPU market</title>
    <updated>2026-01-02T11:14:30+00:00</updated>
    <author>
      <name>/u/notafakename10</name>
      <uri>https://old.reddit.com/user/notafakename10</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1w1qj/most_optimal_vramperformance_per_price_and_advice/"&gt; &lt;img alt="Most optimal vram/performance per price and advice for Shenzhen GPU market" src="https://preview.redd.it/4nfcarq96xag1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=34f4fbc2fba6317c3d5435a92332540815eb3714" title="Most optimal vram/performance per price and advice for Shenzhen GPU market" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm in Shanghai at the moment and heading to Shenzhen soon - I‚Äôve got around $1500-3000 USD to get the most optimal setup possible. The people I am with are great at negotiating (natives, speak the language) I just need to figure out what I want‚Ä¶ &lt;/p&gt; &lt;p&gt;I main use local models I would want at least 48gb vram, ideally closer to 96gb an at least some grunt for the odd PyTorch model training run. I‚Äôm open to modded cards (one of my current front runners is 4x 3080 20gb cards) open to both AMD and domestic / enterprise cards. &lt;/p&gt; &lt;p&gt;Prices are best estimates from deep seek - could be wildly wrong, anyone had experience navigating the GPU markets? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/notafakename10"&gt; /u/notafakename10 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4nfcarq96xag1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1w1qj/most_optimal_vramperformance_per_price_and_advice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q1w1qj/most_optimal_vramperformance_per_price_and_advice/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-02T11:14:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1q21wqw</id>
    <title>A deep dive in DeepSeek's mHC: They improved things everyone else thought didn‚Äôt need improving</title>
    <updated>2026-01-02T15:44:21+00:00</updated>
    <author>
      <name>/u/InternationalAsk1490</name>
      <uri>https://old.reddit.com/user/InternationalAsk1490</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;The Context&lt;/h1&gt; &lt;p&gt;Since ResNet (2015), the Residual Connection (x_{l+1} = x_l + F(x_l)) has been the untouchable backbone of deep learning (from CNN to Transformer, from BERT to GPT). It solves the vanishing gradient problem by providing an &amp;quot;identity mapping&amp;quot; fast lane. For 10 years, almost no one questioned it.&lt;/p&gt; &lt;h1&gt;The Problem&lt;/h1&gt; &lt;p&gt;However, this standard design forces a rigid 1:1 ratio between the input and the new computation, preventing the model from dynamically adjusting how much it relies on past layers versus new information.&lt;/p&gt; &lt;h1&gt;The Innovation&lt;/h1&gt; &lt;p&gt;ByteDace tried to break this rule with &amp;quot;Hyper-Connections&amp;quot; (HC), allowing the model to learn the connection weights instead of using a fixed ratio.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;The potential:&lt;/strong&gt; Faster convergence and better performance due to flexible information routing.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The issue:&lt;/strong&gt; It was incredibly unstable. Without constraints, signals were amplified by &lt;strong&gt;3000x&lt;/strong&gt; in deep networks, leading to exploding gradients.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;The Solution: Manifold-Constrained Hyper-Connections (mHC)&lt;/h1&gt; &lt;p&gt;In their new paper, DeepSeek solved the instability by constraining the learnable matrices to be &amp;quot;Double Stochastic&amp;quot; (all elements ‚âß 0, rows/cols sum to 1).&lt;/p&gt; &lt;p&gt;Mathematically, this forces the operation to act as a weighted average (convex combination). It guarantees that signals are never amplified beyond control, regardless of network depth.&lt;/p&gt; &lt;h1&gt;The Results&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Stability:&lt;/strong&gt; Max gain magnitude dropped from &lt;strong&gt;3000 to 1.6&lt;/strong&gt; (3 orders of magnitude improvement).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Performance:&lt;/strong&gt; mHC beats both the standard baseline and the unstable HC on benchmarks like GSM8K and DROP.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cost:&lt;/strong&gt; Only adds ~6% to training time due to heavy optimization (kernel fusion).&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Why it matters&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ybux3x1wgyag1.png?width=1206&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=daafe17d3a61d387adf952ad756eb70af3bc445f"&gt;https://preview.redd.it/ybux3x1wgyag1.png?width=1206&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=daafe17d3a61d387adf952ad756eb70af3bc445f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;As hinted in the attached tweet, we are seeing a fascinating split in the AI world. While the industry frenzy focuses on commercialization and AI Agents‚Äîexemplified by Meta spending $2 Billion to acquire Manus‚Äîlabs like DeepSeek and Moonshot (Kimi) are playing a different game.&lt;/p&gt; &lt;p&gt;Despite resource constraints, they are digging into the deepest levels of macro-architecture and optimization. They have the audacity to question what we took for granted: &lt;strong&gt;Residual Connections&lt;/strong&gt; (challenged by DeepSeek's mHC) and &lt;strong&gt;AdamW&lt;/strong&gt; (challenged by Kimi's Muon). Just because these have been the standard for 10 years doesn't mean they are the optimal solution.&lt;/p&gt; &lt;p&gt;Crucially, instead of locking these secrets behind closed doors for commercial dominance, they are &lt;strong&gt;open-sourcing&lt;/strong&gt; these findings for the advancement of humanity. This spirit of relentless self-doubt and fundamental reinvention is exactly how we evolve.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InternationalAsk1490"&gt; /u/InternationalAsk1490 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q21wqw/a_deep_dive_in_deepseeks_mhc_they_improved_things/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q21wqw/a_deep_dive_in_deepseeks_mhc_they_improved_things/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q21wqw/a_deep_dive_in_deepseeks_mhc_they_improved_things/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-02T15:44:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2dcje</id>
    <title>ASUS officially announces price hikes from January 5, right before CES 2026</title>
    <updated>2026-01-02T22:53:19+00:00</updated>
    <author>
      <name>/u/HumanDrone8721</name>
      <uri>https://old.reddit.com/user/HumanDrone8721</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2dcje/asus_officially_announces_price_hikes_from/"&gt; &lt;img alt="ASUS officially announces price hikes from January 5, right before CES 2026" src="https://external-preview.redd.it/e0KouFF887lm3A9dV6mq_44cYZmQvCh1I3h_7LTZz8c.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2c1b6efd1e001ac6907d2df9de1ac4165e4b086a" title="ASUS officially announces price hikes from January 5, right before CES 2026" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HumanDrone8721"&gt; /u/HumanDrone8721 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://videocardz.com/newz/asus-officially-announces-price-hikes-from-january-5-right-before-ces-2026"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2dcje/asus_officially_announces_price_hikes_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2dcje/asus_officially_announces_price_hikes_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-02T22:53:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1q25070</id>
    <title>LeCun Says Llama 4 results "were fudged a little bit"</title>
    <updated>2026-01-02T17:38:01+00:00</updated>
    <author>
      <name>/u/MrPecunius</name>
      <uri>https://old.reddit.com/user/MrPecunius</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There was speculation in this sub about suspicious Llama 4 benchmarks some time back, and now LeCun confirms it on his way out. Best I can do is a Slashdot link since the FT article is paywalled:&lt;/p&gt; &lt;p&gt;&lt;a href="https://tech.slashdot.org/story/26/01/02/1449227/results-were-fudged-departing-meta-ai-chief-confirms-llama-4-benchmark-manipulation"&gt;'Results Were Fudged': Departing Meta AI Chief Confirms Llama 4 Benchmark Manipulation &lt;/a&gt;&lt;/p&gt; &lt;p&gt;This bit jumped out at me:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Zuckerberg subsequently &amp;quot;sidelined the entire GenAI organisation,&amp;quot; according to LeCun. &amp;quot;A lot of people have left, a lot of people who haven't yet left will leave.&amp;quot;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;This explains a lot, if true: we never saw the promised huge Llama 4 model, and there hasn't been any followup since the other releases.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrPecunius"&gt; /u/MrPecunius &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q25070/lecun_says_llama_4_results_were_fudged_a_little/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q25070/lecun_says_llama_4_results_were_fudged_a_little/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q25070/lecun_says_llama_4_results_were_fudged_a_little/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-02T17:38:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptxm3x</id>
    <title>AMA With Z.AI, The Lab Behind GLM-4.7</title>
    <updated>2025-12-23T16:04:23+00:00</updated>
    <author>
      <name>/u/zixuanlimit</name>
      <uri>https://old.reddit.com/user/zixuanlimit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi r/LocalLLaMA&lt;/p&gt; &lt;p&gt;Today we are having &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt;, the research lab behind the GLM 4.7. We‚Äôre excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Yuxuan Zhang, &lt;a href="/u/YuxuanZhangzR"&gt;u/YuxuanZhangzR&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Qinkai Zheng, &lt;a href="/u/QinkaiZheng"&gt;u/QinkaiZheng&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Aohan Zeng, &lt;a href="/u/Sengxian"&gt;u/Sengxian&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Zhenyu Hou, &lt;a href="/u/ZhenyuHou"&gt;u/ZhenyuHou&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Xin Lv, &lt;a href="/u/davidlvxin"&gt;u/davidlvxin&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The AMA will run from 8 AM ‚Äì 11 AM PST, with the &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt; team continuing to follow up on questions over the next 48 hours.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zixuanlimit"&gt; /u/zixuanlimit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T16:04:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwh0q9</id>
    <title>Best Local LLMs - 2025</title>
    <updated>2025-12-26T22:31:28+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Year end thread for the best LLMs of 2025!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;2025 is almost done! Its been &lt;strong&gt;a wonderful year&lt;/strong&gt; for us Open/Local AI enthusiasts. And its looking like Xmas time brought some great gifts in the shape of Minimax M2.1 and GLM4.7 that are touting frontier model performance. Are we there already? are we at parity with proprietary models?!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The standard spiel:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Share what your favorite models are right now &lt;strong&gt;and why.&lt;/strong&gt; Given the nature of the beast in evaluating LLMs (untrustworthiness of benchmarks, immature tooling, intrinsic stochasticity), please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Only open weights models&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;em&gt;Please thread your responses in the top level comments for each Application below to enable readability&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Applications&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;General&lt;/strong&gt;: Includes practical guidance, how to, encyclopedic QnA, search engine replacement/augmentation&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agentic/Agentic Coding/Tool Use/Coding&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Creative Writing/RP&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Speciality&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;If a category is missing, please create a top level comment under the Speciality comment&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Notes&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Useful breakdown of how folk are using LLMs: &lt;a href="https://preview.redd.it/i8td7u8vcewf1.png?width=1090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d"&gt;https://preview.redd.it/i8td7u8vcewf1.png?width=1090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d&lt;/a&gt; &lt;/p&gt; &lt;p&gt;A good suggestion for last time, breakdown/classify your recommendation by model memory footprint: (you can and should be using multiple models in each size range for different tasks)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Unlimited: &amp;gt;128GB VRAM &lt;/li&gt; &lt;li&gt;Medium: 8 to 128GB VRAM&lt;/li&gt; &lt;li&gt;Small: &amp;lt;8GB VRAM&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T22:31:28+00:00</published>
  </entry>
</feed>
