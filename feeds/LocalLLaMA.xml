<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-19T08:11:52+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qg5io6</id>
    <title>Is it feasible for a Team to replace Claude Code with one of the "local" alternatives?</title>
    <updated>2026-01-18T10:44:03+00:00</updated>
    <author>
      <name>/u/nunodonato</name>
      <uri>https://old.reddit.com/user/nunodonato</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So yes, I've read countless posts in this sub about replacing Claude Code with local models.&lt;/p&gt; &lt;p&gt;My question is slightly different. I'm talking about finding a replacement that would be able to serve a small team of developers.&lt;/p&gt; &lt;p&gt;We are currently spending around 2k/mo on Claude. And that can go a long way on cloud GPUs. However, I'm not sure if it would be good enough to support a few concurrent requests.&lt;/p&gt; &lt;p&gt;I've read a lot of praise for Deepseek Coder and a few of the newer models, but would they still perform okay-ish with Q8?&lt;/p&gt; &lt;p&gt;Any advice? recommendations?&lt;/p&gt; &lt;p&gt;thanks in advance&lt;/p&gt; &lt;p&gt;Edit: I plan to keep Claude Code (the app), but switch the models. I know that Claude Code is responsible for the high success rate, regardless of the model. The tools and prompt are very good. So I think even with a worse model, we would get reasonable results when using it via claude code&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nunodonato"&gt; /u/nunodonato &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qg5io6/is_it_feasible_for_a_team_to_replace_claude_code/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qg5io6/is_it_feasible_for_a_team_to_replace_claude_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qg5io6/is_it_feasible_for_a_team_to_replace_claude_code/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-18T10:44:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1qgcj8b</id>
    <title>RLVR with GRPO from scratch code notebook</title>
    <updated>2026-01-18T16:10:02+00:00</updated>
    <author>
      <name>/u/seraschka</name>
      <uri>https://old.reddit.com/user/seraschka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgcj8b/rlvr_with_grpo_from_scratch_code_notebook/"&gt; &lt;img alt="RLVR with GRPO from scratch code notebook" src="https://external-preview.redd.it/1zp6Ys_kCzKo-Gqi6ZfsuCLpMOxYXSdKyJbi6hC7oDk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=80606d0ff718fb311decc9610424061c1fa2743b" title="RLVR with GRPO from scratch code notebook" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/seraschka"&gt; /u/seraschka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/rasbt/reasoning-from-scratch/blob/main/ch06/01_main-chapter-code/ch06_main.ipynb"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgcj8b/rlvr_with_grpo_from_scratch_code_notebook/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qgcj8b/rlvr_with_grpo_from_scratch_code_notebook/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-18T16:10:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1qgc15w</id>
    <title>ROCm+Linux on AMD Strix Halo: January 2026 Stable Configurations</title>
    <updated>2026-01-18T15:51:22+00:00</updated>
    <author>
      <name>/u/Intrepid_Rub_3566</name>
      <uri>https://old.reddit.com/user/Intrepid_Rub_3566</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgc15w/rocmlinux_on_amd_strix_halo_january_2026_stable/"&gt; &lt;img alt="ROCm+Linux on AMD Strix Halo: January 2026 Stable Configurations" src="https://b.thumbs.redditmedia.com/4HB_kRub0ByrHTDKoq1IizVqH5bAkGmUVpks54w-dLU.jpg" title="ROCm+Linux on AMD Strix Halo: January 2026 Stable Configurations" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;New video on ROCm+Linux support for AMD Strix Halo, documenting working/stable configurations in January 2026 and what caused the original issues.&lt;/p&gt; &lt;p&gt;&lt;a href="https://youtu.be/Hdg7zL3pcIs"&gt;https://youtu.be/Hdg7zL3pcIs&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Copying the table here for reference (&lt;a href="https://github.com/kyuz0/amd-strix-halo-gfx1151-toolboxes"&gt;https://github.com/kyuz0/amd-strix-halo-gfx1151-toolboxes&lt;/a&gt;):&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ygn7zad4r4eg1.png?width=2538&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5291169682acb6fb54cf25d21118877d926ede3a"&gt;https://preview.redd.it/ygn7zad4r4eg1.png?width=2538&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5291169682acb6fb54cf25d21118877d926ede3a&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Intrepid_Rub_3566"&gt; /u/Intrepid_Rub_3566 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgc15w/rocmlinux_on_amd_strix_halo_january_2026_stable/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgc15w/rocmlinux_on_amd_strix_halo_january_2026_stable/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qgc15w/rocmlinux_on_amd_strix_halo_january_2026_stable/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-18T15:51:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1qg4d4t</id>
    <title>What we learned processing 1M+ emails for context engineering</title>
    <updated>2026-01-18T09:35:07+00:00</updated>
    <author>
      <name>/u/EnoughNinja</name>
      <uri>https://old.reddit.com/user/EnoughNinja</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We spent the last year building systems to turn email into structured context for AI agents. Processed over a million emails to figure out what actually works.&lt;/p&gt; &lt;p&gt;Some things that weren't obvious going in:&lt;/p&gt; &lt;p&gt;Thread reconstruction is way harder than I thought. You've got replies, forwards, people joining mid-conversation, decisions getting revised three emails later. Most systems just concatenate text in chronological order and hope the LLM figures it out, but that falls apart fast because you lose who said what and why it matters.&lt;/p&gt; &lt;p&gt;Attachments are half the conversation. PDFs, contracts, invoices, they're not just metadata, they're actual content that drives decisions. We had to build OCR and structure parsing so the system can actually read them, not just know they exist as file names.&lt;/p&gt; &lt;p&gt;Multilingual threads are more common than you'd think. People switch languages mid-conversation all the time, especially in global teams. Semantic search that works well in English completely breaks down when you need cross-language understanding.&lt;/p&gt; &lt;p&gt;Zero data retention is non-negotiable if you want enterprise customers. We discard every prompt after processing. Memory gets reconstructed on demand from the original sources, nothing stored. Took us way longer to build but there's no other way to get past compliance teams.&lt;/p&gt; &lt;p&gt;Performance-wise we're hitting around 200ms for retrieval and about 3 seconds to first token even on massive inboxes. &lt;/p&gt; &lt;p&gt;Most of the time is in the reasoning step, not the search.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EnoughNinja"&gt; /u/EnoughNinja &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qg4d4t/what_we_learned_processing_1m_emails_for_context/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qg4d4t/what_we_learned_processing_1m_emails_for_context/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qg4d4t/what_we_learned_processing_1m_emails_for_context/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-18T09:35:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfscp5</id>
    <title>128GB VRAM quad R9700 server</title>
    <updated>2026-01-17T23:30:26+00:00</updated>
    <author>
      <name>/u/Ulterior-Motive_</name>
      <uri>https://old.reddit.com/user/Ulterior-Motive_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfscp5/128gb_vram_quad_r9700_server/"&gt; &lt;img alt="128GB VRAM quad R9700 server" src="https://b.thumbs.redditmedia.com/SbBMg1b6qTh913lUa8uWDuyYZrIwJ_ECUuUVvuWh_qA.jpg" title="128GB VRAM quad R9700 server" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is a sequel to my &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1fqwrvg/64gb_vram_dual_mi100_server/"&gt;previous thread&lt;/a&gt; from 2024.&lt;/p&gt; &lt;p&gt;I originally planned to pick up another pair of MI100s and an Infinity Fabric Bridge, and I picked up a lot of hardware upgrades over the course of 2025 in preparation for this. Notably, faster, double capacity memory (last February, well before the current price jump), another motherboard, higher capacity PSU, etc. But then I saw benchmarks for the R9700, particularly in the &lt;a href="https://github.com/ggml-org/llama.cpp/discussions/15021"&gt;llama.cpp ROCm thread&lt;/a&gt;, and saw the much better prompt processing performance for a small token generation loss. The MI100 also went up in price to about $1000, so factoring in the cost of a bridge, it'd come to about the same price. So I sold the MI100s, picked up 4 R9700s and called it a day.&lt;/p&gt; &lt;p&gt;Here's the specs and BOM. Note that the CPU and SSD were taken from the previous build, and the internal fans came bundled with the PSU as part of a deal:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Component&lt;/th&gt; &lt;th align="left"&gt;Description&lt;/th&gt; &lt;th align="left"&gt;Number&lt;/th&gt; &lt;th align="left"&gt;Unit Price&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;CPU&lt;/td&gt; &lt;td align="left"&gt;AMD Ryzen 7 5700X&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;$160.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;RAM&lt;/td&gt; &lt;td align="left"&gt;Corsair Vengance LPX 64GB (2 x 32GB) DDR4 3600MHz C18&lt;/td&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;$105.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPU&lt;/td&gt; &lt;td align="left"&gt;PowerColor AMD Radeon AI PRO R9700 32GB&lt;/td&gt; &lt;td align="left"&gt;4&lt;/td&gt; &lt;td align="left"&gt;$1,300.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Motherboard&lt;/td&gt; &lt;td align="left"&gt;MSI MEG X570 GODLIKE Motherboard&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;$490.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Storage&lt;/td&gt; &lt;td align="left"&gt;Inland Performance 1TB NVMe SSD&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;$100.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;PSU&lt;/td&gt; &lt;td align="left"&gt;Super Flower Leadex Titanium 1600W 80+ Titanium&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;$440.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Internal Fans&lt;/td&gt; &lt;td align="left"&gt;Super Flower MEGACOOL 120mm fan, Triple-Pack&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;$0.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Case Fans&lt;/td&gt; &lt;td align="left"&gt;Noctua NF-A14 iPPC-3000 PWM&lt;/td&gt; &lt;td align="left"&gt;6&lt;/td&gt; &lt;td align="left"&gt;$30.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;CPU Heatsink&lt;/td&gt; &lt;td align="left"&gt;AMD Wraith Prism aRGB CPU Cooler&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;$20.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Fan Hub&lt;/td&gt; &lt;td align="left"&gt;Noctua NA-FH1&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;$45.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Case&lt;/td&gt; &lt;td align="left"&gt;Phanteks Enthoo Pro 2 Server Edition&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;$190.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Total&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;$7,035.00&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;128GB VRAM, 128GB RAM for offloading, all for less than the price of a RTX 6000 Blackwell.&lt;/p&gt; &lt;p&gt;Some benchmarks:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;n_batch&lt;/th&gt; &lt;th align="left"&gt;n_ubatch&lt;/th&gt; &lt;th align="left"&gt;fa&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 7B Q4_0&lt;/td&gt; &lt;td align="left"&gt;3.56 GiB&lt;/td&gt; &lt;td align="left"&gt;6.74 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp8192&lt;/td&gt; &lt;td align="left"&gt;6524.91 ¬± 11.30&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 7B Q4_0&lt;/td&gt; &lt;td align="left"&gt;3.56 GiB&lt;/td&gt; &lt;td align="left"&gt;6.74 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;90.89 ¬± 0.41&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q8_0&lt;/td&gt; &lt;td align="left"&gt;33.51 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp8192&lt;/td&gt; &lt;td align="left"&gt;2113.82 ¬± 2.88&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q8_0&lt;/td&gt; &lt;td align="left"&gt;33.51 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;72.51 ¬± 0.27&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3vl 32B Q8_0&lt;/td&gt; &lt;td align="left"&gt;36.76 GiB&lt;/td&gt; &lt;td align="left"&gt;32.76 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp8192&lt;/td&gt; &lt;td align="left"&gt;1725.46 ¬± 5.93&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3vl 32B Q8_0&lt;/td&gt; &lt;td align="left"&gt;36.76 GiB&lt;/td&gt; &lt;td align="left"&gt;32.76 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;14.75 ¬± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 70B IQ4_XS - 4.25 bpw&lt;/td&gt; &lt;td align="left"&gt;35.29 GiB&lt;/td&gt; &lt;td align="left"&gt;70.55 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp8192&lt;/td&gt; &lt;td align="left"&gt;1110.02 ¬± 3.49&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 70B IQ4_XS - 4.25 bpw&lt;/td&gt; &lt;td align="left"&gt;35.29 GiB&lt;/td&gt; &lt;td align="left"&gt;70.55 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;14.53 ¬± 0.03&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3next 80B.A3B IQ4_XS - 4.25 bpw&lt;/td&gt; &lt;td align="left"&gt;39.71 GiB&lt;/td&gt; &lt;td align="left"&gt;79.67 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp8192&lt;/td&gt; &lt;td align="left"&gt;821.10 ¬± 0.27&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3next 80B.A3B IQ4_XS - 4.25 bpw&lt;/td&gt; &lt;td align="left"&gt;39.71 GiB&lt;/td&gt; &lt;td align="left"&gt;79.67 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;38.88 ¬± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;glm4moe ?B IQ4_XS - 4.25 bpw&lt;/td&gt; &lt;td align="left"&gt;54.33 GiB&lt;/td&gt; &lt;td align="left"&gt;106.85 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp8192&lt;/td&gt; &lt;td align="left"&gt;1928.45 ¬± 3.74&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;glm4moe ?B IQ4_XS - 4.25 bpw&lt;/td&gt; &lt;td align="left"&gt;54.33 GiB&lt;/td&gt; &lt;td align="left"&gt;106.85 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;48.09 ¬± 0.16&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;minimax-m2 230B.A10B IQ4_XS - 4.25 bpw&lt;/td&gt; &lt;td align="left"&gt;113.52 GiB&lt;/td&gt; &lt;td align="left"&gt;228.69 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp8192&lt;/td&gt; &lt;td align="left"&gt;2082.04 ¬± 4.49&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;minimax-m2 230B.A10B IQ4_XS - 4.25 bpw&lt;/td&gt; &lt;td align="left"&gt;113.52 GiB&lt;/td&gt; &lt;td align="left"&gt;228.69 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;48.78 ¬± 0.06&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;minimax-m2 230B.A10B Q8_0&lt;/td&gt; &lt;td align="left"&gt;226.43 GiB&lt;/td&gt; &lt;td align="left"&gt;228.69 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;30&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp8192&lt;/td&gt; &lt;td align="left"&gt;42.62 ¬± 7.96&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;minimax-m2 230B.A10B Q8_0&lt;/td&gt; &lt;td align="left"&gt;226.43 GiB&lt;/td&gt; &lt;td align="left"&gt;228.69 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;30&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;6.58 ¬± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;A few final observations:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;glm4 moe and minimax-m2 are actually GLM-4.6V and MiniMax-M2.1, respectively.&lt;/li&gt; &lt;li&gt;There's an open issue for Qwen3-Next at the moment; recent optimizations caused some pretty hefty prompt processing regressions. The numbers here are pre #18683, in case the exact issue gets resolved.&lt;/li&gt; &lt;li&gt;A word on the Q8 quant of MiniMax-M2.1; &lt;code&gt;--fit on&lt;/code&gt; isn't supported on llama-bench, so I can't give an apples to apples comparison to simply reducing the number of gpu layers, but it's also extremely unreliable for me in llama-server, giving me HIP error 906 on the first generation. Out of a dozen or so attempts, I've gotten it to work once, with a TG around 8.5 t/s, but take that with a grain of salt. Otherwise, maybe the quality jump is worth letting it run overnight? You be the judge. It also takes 2 hours to load, but that could be because I'm loading it off external storage.&lt;/li&gt; &lt;li&gt;The internal fan mount on the case only has screws on one side; in the intended configuration, the holes for power cables are on the opposite side of where the GPU power sockets are, meaning the power cables will block airflow from the fans. How they didn't see this, I have no idea. Thankfully, it stays in place from a friction fit if you flip it 180 like I did. Really, I probably could have gone without it, it was mostly a consideration for when I was still going with MI100s, but the fans were free anyway.&lt;/li&gt; &lt;li&gt;I really, really wanted to go AM5 for this, but there just isn't a board out there with 4 full sized PCIe slots spaced for 2 slot GPUs. At best you can fit 3 and then cover up one of them. But if you need a bazillion m.2 slots you're golden /s. You might then ask why I didn't go for Threadripper/Epyc, and that's because I was worried about power consumption and heat. I didn't want to mess with risers and open rigs, so I found the one AM4 board that could do this, even if it comes at the cost of RAM speeds/channels and slower PCIe speeds.&lt;/li&gt; &lt;li&gt;The MI100s and R9700s didn't play nice for the brief period of time I had 2 of both. I didn't bother troubleshooting, just shrugged and sold them off, so it may have been a simple fix but FYI.&lt;/li&gt; &lt;li&gt;Going with a 1 TB SSD in my original build was a mistake, even 2 would have made a world of difference. Between LLMs, image generation, TTS, ect. I'm having trouble actually taking advantage of the extra VRAM with less quantized models due to storage constraints, which is why my benchmarks still have a lot of 4-bit quants despite being able to easily do 8-bit ones.&lt;/li&gt; &lt;li&gt;I don't know how to control the little LCD display on the board. I'm not sure there is a way on Linux. A shame.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ulterior-Motive_"&gt; /u/Ulterior-Motive_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qfscp5"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfscp5/128gb_vram_quad_r9700_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qfscp5/128gb_vram_quad_r9700_server/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T23:30:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1qgluze</id>
    <title>Anybody run Minimax 2.1 q4 on pure RAM (CPU) ?</title>
    <updated>2026-01-18T22:10:36+00:00</updated>
    <author>
      <name>/u/xSNYPSx777</name>
      <uri>https://old.reddit.com/user/xSNYPSx777</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Does anybody runs Minimax 2.1 q4 on pure RAM (CPU) ?&lt;/p&gt; &lt;p&gt;I mean DDR5 (~6000) how much t/s ?&lt;/p&gt; &lt;p&gt;Any other quants ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xSNYPSx777"&gt; /u/xSNYPSx777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgluze/anybody_run_minimax_21_q4_on_pure_ram_cpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgluze/anybody_run_minimax_21_q4_on_pure_ram_cpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qgluze/anybody_run_minimax_21_q4_on_pure_ram_cpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-18T22:10:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1qg6dr6</id>
    <title>Ministral 3 Reasoning Heretic and GGUFs</title>
    <updated>2026-01-18T11:34:20+00:00</updated>
    <author>
      <name>/u/coder3101</name>
      <uri>https://old.reddit.com/user/coder3101</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks,&lt;/p&gt; &lt;p&gt;Back with another series of abilitered (uncensored) models, this time Ministral 3 with Vision capability. These models lost all their refusal with minimal damage. &lt;/p&gt; &lt;p&gt;As bonus, this time I also quantized them instead of waiting for community.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/coder3101/ministral-3-reasoning-heretic"&gt;https://huggingface.co/collections/coder3101/ministral-3-reasoning-heretic&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Series contains:&lt;/p&gt; &lt;p&gt;- Ministral 3 4B Reasoning&lt;/p&gt; &lt;p&gt;- Ministral 3 8B Reasoning&lt;/p&gt; &lt;p&gt;- Ministral 3 14B Reasoning&lt;/p&gt; &lt;p&gt;All with Q4, Q5, Q8, BF16 quantization with MMPROJ for Vision capabilities.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/coder3101"&gt; /u/coder3101 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qg6dr6/ministral_3_reasoning_heretic_and_ggufs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qg6dr6/ministral_3_reasoning_heretic_and_ggufs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qg6dr6/ministral_3_reasoning_heretic_and_ggufs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-18T11:34:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfv1ms</id>
    <title>Qwen 4 might be a long way off !? Lead Dev says they are "slowing down" to focus on quality.</title>
    <updated>2026-01-18T01:28:57+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfv1ms/qwen_4_might_be_a_long_way_off_lead_dev_says_they/"&gt; &lt;img alt="Qwen 4 might be a long way off !? Lead Dev says they are &amp;quot;slowing down&amp;quot; to focus on quality." src="https://preview.redd.it/ylsevy04f0eg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bf47eb2c12055fdb3e08f36d4d3746a234d630ff" title="Qwen 4 might be a long way off !? Lead Dev says they are &amp;quot;slowing down&amp;quot; to focus on quality." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ylsevy04f0eg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfv1ms/qwen_4_might_be_a_long_way_off_lead_dev_says_they/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qfv1ms/qwen_4_might_be_a_long_way_off_lead_dev_says_they/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-18T01:28:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1qg823q</id>
    <title>Kind of Rant: My local server order got cancelled after a 3-month wait because they wanted to over triple the price. Anybody got in similar situation?</title>
    <updated>2026-01-18T13:04:04+00:00</updated>
    <author>
      <name>/u/SomeRandomGuuuuuuy</name>
      <uri>https://old.reddit.com/user/SomeRandomGuuuuuuy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, &lt;/p&gt; &lt;p&gt;I never post stuff like this, but need to vent as I can't stop thinking about it and it piss me of so much.&lt;/p&gt; &lt;p&gt;Since I was young I couldn't afford hardware or do much, heck I needed to wait till 11 pm each day to watch youtube video as network in my region was so shitty (less than 100 kbps 90% of day). There were also no other provider. I was like scripting downloads of movies youtube video or some courses at night at specific hours at night and closing pc as it was working like a jet engine.&lt;/p&gt; &lt;p&gt;I‚Äôm a young dev who finally saved up enough money to upgrade from my old laptop to a real rig for AI training, video editing and optimization tests of local inference. I spent months researching parts and found a company willing to build a custom server with 500GB RAM and room for GPU expansion. I paid about ‚Ç¨5k and was told it would arrive by December.&lt;/p&gt; &lt;p&gt;Long story short: &lt;strong&gt;One day before Christmas&lt;/strong&gt;, they tell me that because RAM prices increased, I need to pay an &lt;strong&gt;extra ‚Ç¨10k&lt;/strong&gt; on top of what I already paid plus tax. I tried fighting it, but since it was a B2B/private mix purchase, EU consumer laws are making it hard, and lawyers are too expensive. They forced a refund on me to wash their hands of it that I don't even accept.&lt;/p&gt; &lt;p&gt;I have &lt;strong&gt;RTX 5090&lt;/strong&gt; that has been sitting in a box for a year (I bought it early, planning for this build).&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I have nothing to put it in. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I play around models and projects like vLLM, SGLang, and Dynamo for work and hobby. Also do some smart home stuff assistance. I am left with old laptop that crash regularly so I am thinking at least of M5 Pro Macbook to abuse battery and go around to cafes as I loved doing it in Uni. &lt;/p&gt; &lt;p&gt;I could have chance to go with my company to China or the USA later this year so maybe I could buy some parts. I technically have some resources at job agreed on playing but not much and it could bite my ass maybe later.&lt;/p&gt; &lt;p&gt;Anybody have similar story ? What you guys plan to do ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SomeRandomGuuuuuuy"&gt; /u/SomeRandomGuuuuuuy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qg823q/kind_of_rant_my_local_server_order_got_cancelled/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qg823q/kind_of_rant_my_local_server_order_got_cancelled/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qg823q/kind_of_rant_my_local_server_order_got_cancelled/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-18T13:04:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1qg48z8</id>
    <title>Newelle 1.2 released</title>
    <updated>2026-01-18T09:28:09+00:00</updated>
    <author>
      <name>/u/iTzSilver_YT</name>
      <uri>https://old.reddit.com/user/iTzSilver_YT</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qg48z8/newelle_12_released/"&gt; &lt;img alt="Newelle 1.2 released" src="https://b.thumbs.redditmedia.com/LkuhPhoU8yX94dk4Ih1TJttPbdGMxJp3kQLOX0r9qXA.jpg" title="Newelle 1.2 released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Newelle, AI assistant for Linux, has been updated to 1.2! You can download it from &lt;a href="https://flathub.org/en/apps/io.github.qwersyk.Newelle"&gt;FlatHub&lt;/a&gt;&lt;/p&gt; &lt;p&gt;‚ö°Ô∏è Add llama.cpp, with options to recompile it with any backend&lt;br /&gt; üìñ Implement a new model library for ollama / llama.cpp&lt;br /&gt; üîé Implement hybrid search, improving document reading&lt;/p&gt; &lt;p&gt;üíª Add command execution tool&lt;br /&gt; üóÇ Add tool groups&lt;br /&gt; üîó Improve MCP server adding, supporting also STDIO for non flatpak&lt;br /&gt; üìù Add semantic memory handler&lt;br /&gt; üì§ Add ability to import/export chats&lt;br /&gt; üìÅ Add custom folders to the RAG index&lt;br /&gt; ‚ÑπÔ∏è Improved message information menu, showing the token count and token speed&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iTzSilver_YT"&gt; /u/iTzSilver_YT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qg48z8"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qg48z8/newelle_12_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qg48z8/newelle_12_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-18T09:28:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1qgxgns</id>
    <title>built a (free) photo based nutrition tracker for iOS, with local LLM support</title>
    <updated>2026-01-19T07:13:26+00:00</updated>
    <author>
      <name>/u/Agusx1211</name>
      <uri>https://old.reddit.com/user/Agusx1211</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgxgns/built_a_free_photo_based_nutrition_tracker_for/"&gt; &lt;img alt="built a (free) photo based nutrition tracker for iOS, with local LLM support" src="https://a.thumbs.redditmedia.com/XMo038VJinSiM5XwZcMNBgiITaVwM1pBgX9oQVUFR58.jpg" title="built a (free) photo based nutrition tracker for iOS, with local LLM support" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/u7heybr4a9eg1.png?width=3342&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a64a6e55169e346bae43751c34393d89afb90c39"&gt;https://preview.redd.it/u7heybr4a9eg1.png?width=3342&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a64a6e55169e346bae43751c34393d89afb90c39&lt;/a&gt;&lt;/p&gt; &lt;p&gt;During my Christmas break (and weekends), I started working on this project. It is mostly vibe coded, but it works quite well. I built it for myself really, but since you have to pay Apple 100 bucks even if you want an app only for yourself, I decided to go through publishing it.&lt;/p&gt; &lt;p&gt;It is a nutrition tracker; it integrates with Apple Health and uses LLMs to estimate portions, nutrients, etc. The flow is quite simple: take a picture and click send. In my (biased) opinion, it is a lot simpler than Lose It and similar apps imho.&lt;/p&gt; &lt;p&gt;It supports local LLMs, even running them on device. The on-device models are not great, but you can connect the app to LM Studio and run more powerful models there. The app works best with Gemini 3.0 Flash, but for that you need to add an OpenRouter key. I am also running a free &amp;quot;cloud&amp;quot; service for quick testing, but please don't hammer it to death.&lt;/p&gt; &lt;p&gt;The app is free, and I have no plans on monetizing it. The data never leaves your device if you use a local LLM (or LM Studio). It doesn't even have analytics, so if you try it, give me a ping, because I won't be able to tell!&lt;/p&gt; &lt;p&gt;Link: &lt;a href="https://apps.apple.com/us/app/flog-ai-food-tracker/id6756525727"&gt;https://apps.apple.com/us/app/flog-ai-food-tracker/id6756525727&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Agusx1211"&gt; /u/Agusx1211 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgxgns/built_a_free_photo_based_nutrition_tracker_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgxgns/built_a_free_photo_based_nutrition_tracker_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qgxgns/built_a_free_photo_based_nutrition_tracker_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T07:13:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1qgv0vk</id>
    <title>Has anyone quantized VibeVoice-Realtime-0.5B (Stream) for edge devices yet?</title>
    <updated>2026-01-19T05:02:09+00:00</updated>
    <author>
      <name>/u/New_Source_6765</name>
      <uri>https://old.reddit.com/user/New_Source_6765</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking for a quantized version (GGUF or ONNX) of the &lt;strong&gt;Microsoft VibeVoice-Realtime-0.5B&lt;/strong&gt; model to run on an SBC (Orange Pi).&lt;/p&gt; &lt;p&gt;I've seen some repos for the 7B version, but I specifically need the lightweight 0.5B stream version for an edge project. Has anyone successfully converted this, or can point me to a guide on how to quantize this specific architecture?&lt;/p&gt; &lt;p&gt;thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/New_Source_6765"&gt; /u/New_Source_6765 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgv0vk/has_anyone_quantized_vibevoicerealtime05b_stream/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgv0vk/has_anyone_quantized_vibevoicerealtime05b_stream/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qgv0vk/has_anyone_quantized_vibevoicerealtime05b_stream/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T05:02:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1qgv2ey</id>
    <title>BFL FLUX.2 Klein tutorial and some optimizations - under 1s latency on an A100</title>
    <updated>2026-01-19T05:04:18+00:00</updated>
    <author>
      <name>/u/LayerHot</name>
      <uri>https://old.reddit.com/user/LayerHot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A quick tutorial on running FLUX.2 Klein (the new BFL model from last week). Here's what we're seeing on A100:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;4B distilled&lt;/strong&gt;: ~0.9s per image (1024x1024, 4 steps) with torch.compile + fused QKV&lt;/li&gt; &lt;li&gt;&lt;strong&gt;9B distilled&lt;/strong&gt;: ~1.8s per image with same optimizations&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;These models are pretty good and fast for basic image generation (the 4B model sometimes messes up the image structure, but works quite well for it's size)&lt;/p&gt; &lt;p&gt;We put together Gradio and FastAPI scripts with the optimizations: &lt;a href="https://docs.jarvislabs.ai/tutorials/running-flux2-klein"&gt;https://docs.jarvislabs.ai/tutorials/running-flux2-klein&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LayerHot"&gt; /u/LayerHot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgv2ey/bfl_flux2_klein_tutorial_and_some_optimizations/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgv2ey/bfl_flux2_klein_tutorial_and_some_optimizations/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qgv2ey/bfl_flux2_klein_tutorial_and_some_optimizations/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T05:04:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1qg8yoh</id>
    <title>The sad state of the GPU market in Germany and EU, some of them are not even available</title>
    <updated>2026-01-18T13:45:52+00:00</updated>
    <author>
      <name>/u/HumanDrone8721</name>
      <uri>https://old.reddit.com/user/HumanDrone8721</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qg8yoh/the_sad_state_of_the_gpu_market_in_germany_and_eu/"&gt; &lt;img alt="The sad state of the GPU market in Germany and EU, some of them are not even available" src="https://preview.redd.it/9mmc603p34eg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7fe5b9d5a452e5ddb136ff1be2c60fdc5b0ed2c5" title="The sad state of the GPU market in Germany and EU, some of them are not even available" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HumanDrone8721"&gt; /u/HumanDrone8721 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9mmc603p34eg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qg8yoh/the_sad_state_of_the_gpu_market_in_germany_and_eu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qg8yoh/the_sad_state_of_the_gpu_market_in_germany_and_eu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-18T13:45:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1qgnbx8</id>
    <title>Textual game world generation Instructor pipeline</title>
    <updated>2026-01-18T23:09:08+00:00</updated>
    <author>
      <name>/u/JEs4</name>
      <uri>https://old.reddit.com/user/JEs4</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgnbx8/textual_game_world_generation_instructor_pipeline/"&gt; &lt;img alt="Textual game world generation Instructor pipeline" src="https://b.thumbs.redditmedia.com/USiUjlxVILJBRKiTUNitW_RRdgq53FnRT-0iGK8J3CE.jpg" title="Textual game world generation Instructor pipeline" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I threw together an instructor/pydantic pipeline for generating interconnected RPG world content using a local LM.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/jwest33/lm_world_gen"&gt;https://github.com/jwest33/lm_world_gen&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It starts from a high concept you define in a yaml file, and it iteratively generates regions, factions, characters, and branching dialog trees that all reference each other consistently using an in-memory (sqlite) fact registry.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Generates structured JSON content using Pydantic schemas + Instructor&lt;/li&gt; &lt;li&gt;Two-phase generation (skeletons first, then expansion) to ensure variety &lt;ul&gt; &lt;li&gt;This was pretty key as trying to generate complete branches resulted in far too little variety despite efforts to alter context dynamically (seeds, temp walking, context filling etc)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;SQLite (in-memory) fact registry prevents contradictions across generations&lt;/li&gt; &lt;li&gt;Saves progress incrementally so you can resume interrupted runs&lt;/li&gt; &lt;li&gt;Web-based viewer/editor for browsing and regenerating content&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It should work with any OpenAI-compatible API but I only used llama.cpp.&lt;/p&gt; &lt;p&gt;The example below (full json is in the repo with the config file too) was generated using off-the-shelf gemma-27b-it in a single pass. It is has 5 regions, 8 factions, 50 characters, 50 dialogs, and 1395 canonical facts.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/i8hs04swv6eg1.jpg?width=1248&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=186f9f17ff1a81e4ad8ca02b4bfcf8bbbc01bac6"&gt;https://preview.redd.it/i8hs04swv6eg1.jpg?width=1248&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=186f9f17ff1a81e4ad8ca02b4bfcf8bbbc01bac6&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/r0wktvjyv6eg1.jpg?width=2079&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=121a2a29605c726ab518e2af2d066e9291241d26"&gt;https://preview.redd.it/r0wktvjyv6eg1.jpg?width=2079&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=121a2a29605c726ab518e2af2d066e9291241d26&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/sal25j9zv6eg1.jpg?width=2067&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=ca980f560e16b86ed13691b6338f6e02bacc2cd4"&gt;https://preview.redd.it/sal25j9zv6eg1.jpg?width=2067&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=ca980f560e16b86ed13691b6338f6e02bacc2cd4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/w7kjv4uzv6eg1.jpg?width=2104&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=516f7ae120f463a9b98527fdd6d1938bb8e7afc8"&gt;https://preview.redd.it/w7kjv4uzv6eg1.jpg?width=2104&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=516f7ae120f463a9b98527fdd6d1938bb8e7afc8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ci700n60w6eg1.jpg?width=2104&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=fb6b7537ac9c6681744638a365d716fac64a4ac2"&gt;https://preview.redd.it/ci700n60w6eg1.jpg?width=2104&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=fb6b7537ac9c6681744638a365d716fac64a4ac2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Anyway, I didn‚Äôt spend any time optimizing since I‚Äôm just using it for a game I‚Äôm building so it‚Äôs a bit slow, but while it‚Äôs not perfect, I found it to be much more useful then I expected so I figured I‚Äôd share.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JEs4"&gt; /u/JEs4 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgnbx8/textual_game_world_generation_instructor_pipeline/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgnbx8/textual_game_world_generation_instructor_pipeline/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qgnbx8/textual_game_world_generation_instructor_pipeline/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-18T23:09:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1qgxsgc</id>
    <title>JARVIS progress report</title>
    <updated>2026-01-19T07:32:35+00:00</updated>
    <author>
      <name>/u/Dougy27</name>
      <uri>https://old.reddit.com/user/Dougy27</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgxsgc/jarvis_progress_report/"&gt; &lt;img alt="JARVIS progress report" src="https://external-preview.redd.it/YTE2dWNqYzNlOWVnMTqK9dRy4-8Cdu5j5I4RjgQTCdrM-bwVN9HuzETcliQ8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=871ca28c0f7a9afc042974ccfaaac3166ab2becc" title="JARVIS progress report" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dougy27"&gt; /u/Dougy27 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/y98v2hc3e9eg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgxsgc/jarvis_progress_report/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qgxsgc/jarvis_progress_report/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T07:32:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1qgbkcd</id>
    <title>Running language models where they don't belong</title>
    <updated>2026-01-18T15:33:03+00:00</updated>
    <author>
      <name>/u/Brief_Argument8155</name>
      <uri>https://old.reddit.com/user/Brief_Argument8155</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We have seen a cool counter-trend recently to the typical scaleup narrative (see Smol/Phi and ZIT most notably). I've been on a mission to push this to the limit (mainly for fun), moving LMs into environments where they have no business existing.&lt;/p&gt; &lt;p&gt;My thesis is that even the most primitive environments can host generative capabilities if you bake them in correctly.&lt;/p&gt; &lt;p&gt;So here goes:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. The NES LM (inference on 1983 hardware)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I started by writing a char-level bigram model in straight 6502 asm for the original Nintendo Entertainment System.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;2KB of RAM and a CPU with no multiplication opcode, let alone float math.&lt;/li&gt; &lt;li&gt;The model compresses a name space of 18 million possibilities into a footprint smaller than a Final Fantasy black mage sprite (729 bytes of weights).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For extra fun I packaged it into a romhack for Final Fantasy I and Dragon Warrior to generate fantasy names at game time, on original hardware.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Code:&lt;/strong&gt; &lt;a href="https://github.com/erodola/bigram-nes"&gt;https://github.com/erodola/bigram-nes&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. The Compile-Time LM (inference while compiling, duh)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Then I realized that even the NES was too much runtime. Why even wait for the code to run at all? I built a model that does inference entirely at compile-time using C++ template metaprogramming. &lt;/p&gt; &lt;p&gt;Because the compiler itself is Turing complete you know. You could run Doom in it.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The C++ compiler acts as the inference engine. It performs the multinomial sampling and Markov chain transitions &lt;em&gt;while&lt;/em&gt; you are building the project.&lt;/li&gt; &lt;li&gt;Since compilers are deterministic, I hashed &lt;strong&gt;TIME&lt;/strong&gt; into an FNV-1a seed to power a constexpr Xorshift32 RNG.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;When the binary finally runs, the CPU does zero math. The generated text is already there, baked into the data segment as a constant string.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Code:&lt;/strong&gt; &lt;a href="https://github.com/erodola/bigram-metacpp"&gt;https://github.com/erodola/bigram-metacpp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Next up is ofc attempting to scale this toward TinyStories-style models. Or speech synthesis, or OCR. I wont stop until my build logs are more sentient than the code they're actually producing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Brief_Argument8155"&gt; /u/Brief_Argument8155 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgbkcd/running_language_models_where_they_dont_belong/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgbkcd/running_language_models_where_they_dont_belong/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qgbkcd/running_language_models_where_they_dont_belong/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-18T15:33:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1qgwup8</id>
    <title>Is Local Coding even worth setting up</title>
    <updated>2026-01-19T06:38:37+00:00</updated>
    <author>
      <name>/u/Interesting-Fish6494</name>
      <uri>https://old.reddit.com/user/Interesting-Fish6494</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi I am new to Local LLM but have been having a lot of issues setting up a local LLM coding environment so wanted some suggestions from people.I have a 5070 ti (16gb vram).&lt;/p&gt; &lt;p&gt;I have tried to use Kilo code with qwen 2.5 coder 7B running through ollama but the context size feels so low that it finishes the context within a single file of my project.&lt;/p&gt; &lt;p&gt;How are other people with a 16gb GPU dealing with local llm?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Interesting-Fish6494"&gt; /u/Interesting-Fish6494 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgwup8/is_local_coding_even_worth_setting_up/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgwup8/is_local_coding_even_worth_setting_up/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qgwup8/is_local_coding_even_worth_setting_up/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T06:38:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1qgksrm</id>
    <title>Roast my build</title>
    <updated>2026-01-18T21:28:13+00:00</updated>
    <author>
      <name>/u/RoboDogRush</name>
      <uri>https://old.reddit.com/user/RoboDogRush</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgksrm/roast_my_build/"&gt; &lt;img alt="Roast my build" src="https://b.thumbs.redditmedia.com/b9pW-_QRs8a6Yq5-9Nf1d_lQSgXCDYpLuPpj2b0jIXM.jpg" title="Roast my build" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This started as an Optiplex 990 with a 2nd gen i5 as a home server. Someone gave me a 3060, I started running Ollama with Gemma 7B to help manage my Home Assistant, and it became addicting.&lt;/p&gt; &lt;p&gt;The upgrades outgrew the SFF case, PSU and GPU spilling out the side, and it slowly grew into this beast. Around the time I bought the open frame, my wife said it's gotta move out of sight, so I got banished to the unfinished basement, next to the sewage pump. Honestly, better for me, got to plug directly into the network and get off wifi.&lt;/p&gt; &lt;p&gt;6 months of bargain hunting, eBay alerts at 2am, Facebook Marketplace meetups in parking lots, explaining what VRAM is for the 47th time. The result:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;6x RTX 3090 (24GB each)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;1x RTX 5090 (32GB), $1,700 open box Microcenter&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;ROMED8-2T + EPYC 7282&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;2x ASRock 1600W PSUs (both open box)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;32GB A-Tech DDR4 ECC RDIMM&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;$10 Phanteks 300mm PCIe 4.0 riser cables (too long for the lower rack, but costs more to replace with shorter ones)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;176GB total VRAM, ~$6,500 all-in&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;First motherboard crapped out, but got a warranty replacement right before they went out of stock.&lt;/p&gt; &lt;p&gt;Currently running Unsloth's GPT-OSS 120B MXFP4 GGUF. Also been doing Ralph Wiggum loops with Devstral-2 Q8_0 via Mistral Vibe, which yes, I know is unlimited free and full precision in the cloud. But the cloud can't hear my sewage pump.&lt;/p&gt; &lt;p&gt;I think I'm finally done adding on. I desperately needed this. Now I'm not sure what to do with it.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Edit: Fixed the GPT-OSS precision claim. It's natively MXFP4, not F16. The model was trained that way. Thanks to the commenters who caught it.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RoboDogRush"&gt; /u/RoboDogRush &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qgksrm"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgksrm/roast_my_build/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qgksrm/roast_my_build/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-18T21:28:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1qgx83t</id>
    <title>3x3090 + 3060 in a mid tower case</title>
    <updated>2026-01-19T06:59:39+00:00</updated>
    <author>
      <name>/u/liviuberechet</name>
      <uri>https://old.reddit.com/user/liviuberechet</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgx83t/3x3090_3060_in_a_mid_tower_case/"&gt; &lt;img alt="3x3090 + 3060 in a mid tower case" src="https://b.thumbs.redditmedia.com/isvewN3PNijf6_OZxoF82ROhZEAD0nfG7ddsr3ghkYU.jpg" title="3x3090 + 3060 in a mid tower case" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Decided to go all out and max out this desktop. I was lucky to find 3090 cards for around 600 usd, over a period of 3 months and decided to go for it. &lt;/p&gt; &lt;p&gt;The RAM was a bit more expensive, but I had 64 bought before the price spiked.&lt;/p&gt; &lt;p&gt;I didn‚Äôt want to change the case, because I through it‚Äôs a high quality case and it would be a shame to toss it. So made the most out of it!&lt;/p&gt; &lt;p&gt;Specs: * Fractal Define 7 Mid Tower * 3x3090 + 1x3060 (86gb total, but 72gb VRAM main) * 128GB DDR4 (Corsair 4x32) * Corsair HX1500i 1500w (has 7 PCIe power cables) * Vertical mounts are all cheap from AliExpress * ASUS Maximus XII Hero ‚Äî has only 3x PCIe16x, had to deactivate the 2nd NVMe to use the 3rd PCIe16x in 4x, the 4th GPU (the 3060) is on a riser from a PCIe1x. * For drives, only one NVMe of 1TB works, I also bought 2x2TB SSDs that I tried in RAID but the performance was terrible (and they are limited to 500mb from the SATA interface, which I didn‚Äôt know‚Ä¶) so I keep them as 2 drives.&lt;/p&gt; &lt;p&gt;Temperatures are holding surprisingly well. The gap between the cards is about the size of an empty PCIe slot, maybe a bit more. &lt;/p&gt; &lt;p&gt;Temperature was a big improvement compared to having just 2x3090 stacked without any space between them ‚Äî the way the motherboard is designed to use them.&lt;/p&gt; &lt;p&gt;In terms of performance 3x3090 is great! There are great options in the 60-65gb range with the extra space to 72gb VRAM used for context.&lt;/p&gt; &lt;p&gt;I am not using the RAM for anything other than to load models, and the speed is amazing when everything is loaded in VRAM! &lt;/p&gt; &lt;p&gt;Models I started using a lot: * gpt-oss-120b in MXFP4 with 60k context * glm-4.5-air in IQ4_NL with 46k context * qwen3-vl-235b in TQ1_0 (surprisingly good!) * minimax-M2-REAP-139B in Q3_K_S with 40k context&lt;/p&gt; &lt;p&gt;But still return a lot to old models for context and speed: * devstral-small-2-24 in Q8_0 with 200k context * qwen3-coder in Q8 with 1M (!!) context (using RAM) * qwen3-next-80b in Q6_K with 60k context ‚Äî still my favourite for general chat, and the Q6 makes me trust it more than Q3-Q4 models&lt;/p&gt; &lt;p&gt;The 3060 on the riser from PCIe1x is very slow at loading the models, however, once it‚Äôs loaded it works great! I am using it for image generation and TTS audio generation mostly (for Open WebUI).&lt;/p&gt; &lt;p&gt;Also did a lot of testing on using 2x3090 via normal PCIe, with a 3rd card via riser ‚Äî it works same as normal PCIe! But the loading takes forever (sometimes over 2-3 minutes) and you simply can‚Äôt use the RAM for context because of how slow it is ‚Äî so I am considering the current setup to be ‚Äúmaxed out‚Äù because I don‚Äôt think adding a 4th 3090 will be useful. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/liviuberechet"&gt; /u/liviuberechet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qgx83t"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgx83t/3x3090_3060_in_a_mid_tower_case/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qgx83t/3x3090_3060_in_a_mid_tower_case/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T06:59:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1qglyqz</id>
    <title>how do you pronounce ‚Äúgguf‚Äù?</title>
    <updated>2026-01-18T22:14:35+00:00</updated>
    <author>
      <name>/u/Hamfistbumhole</name>
      <uri>https://old.reddit.com/user/Hamfistbumhole</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;is it ‚Äújee - guff‚Äù? ‚Äúgiguff‚Äù? or the full ‚Äújee jee you eff‚Äù? others???&lt;/p&gt; &lt;p&gt;discuss.&lt;/p&gt; &lt;p&gt;and sorry for not using proper international phonetic alphabet symbol things&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hamfistbumhole"&gt; /u/Hamfistbumhole &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qglyqz/how_do_you_pronounce_gguf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qglyqz/how_do_you_pronounce_gguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qglyqz/how_do_you_pronounce_gguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-18T22:14:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1qgj2n9</id>
    <title>Are most major agents really just markdown todo list processors?</title>
    <updated>2026-01-18T20:15:25+00:00</updated>
    <author>
      <name>/u/TheDigitalRhino</name>
      <uri>https://old.reddit.com/user/TheDigitalRhino</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been poking around different code bases and scrutixzing logs from the majors LLM providers, and it seems like every agent just decomposes task to a todo list and process them one by one.&lt;/p&gt; &lt;p&gt;Has anyone found a different approach?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheDigitalRhino"&gt; /u/TheDigitalRhino &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgj2n9/are_most_major_agents_really_just_markdown_todo/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgj2n9/are_most_major_agents_really_just_markdown_todo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qgj2n9/are_most_major_agents_really_just_markdown_todo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-18T20:15:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1qgrw3d</id>
    <title>Just put together my new setup(3x v620 for 96gb vram)</title>
    <updated>2026-01-19T02:31:42+00:00</updated>
    <author>
      <name>/u/PraxisOG</name>
      <uri>https://old.reddit.com/user/PraxisOG</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgrw3d/just_put_together_my_new_setup3x_v620_for_96gb/"&gt; &lt;img alt="Just put together my new setup(3x v620 for 96gb vram)" src="https://b.thumbs.redditmedia.com/mJVsjBo7IHRjbuffjsuNrdAMt7krK8sWOZUchd5L7tE.jpg" title="Just put together my new setup(3x v620 for 96gb vram)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PraxisOG"&gt; /u/PraxisOG &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qgrw3d"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgrw3d/just_put_together_my_new_setup3x_v620_for_96gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qgrw3d/just_put_together_my_new_setup3x_v620_for_96gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T02:31:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1qgdb7f</id>
    <title>4x AMD R9700 (128GB VRAM) + Threadripper 9955WX Build</title>
    <updated>2026-01-18T16:39:42+00:00</updated>
    <author>
      <name>/u/NunzeCs</name>
      <uri>https://old.reddit.com/user/NunzeCs</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgdb7f/4x_amd_r9700_128gb_vram_threadripper_9955wx_build/"&gt; &lt;img alt="4x AMD R9700 (128GB VRAM) + Threadripper 9955WX Build" src="https://b.thumbs.redditmedia.com/bQ4SRK8dHDz2IGShLwX64vLIVj0fWUigDqG_dO43P-U.jpg" title="4x AMD R9700 (128GB VRAM) + Threadripper 9955WX Build" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Disclaimer: I am from Germany and my English is not perfect, so I used an LLM to help me structure and write this post.&lt;/p&gt; &lt;p&gt;Context &amp;amp; Motivation: I built this system for my small company. The main reason for all new hardware is that I received a 50% subsidy/refund from my local municipality for digitalization investments. To qualify for this funding, I had to buy new hardware and build a proper &amp;quot;server-grade&amp;quot; system.&lt;/p&gt; &lt;p&gt;My goal was to run large models (120B+) locally for data privacy. With the subsidy in mind, I had a budget of around 10,000‚Ç¨ (pre-refund). I initially considered NVIDIA, but I wanted to maximize VRAM. I decided to go with 4x AMD RDNA4 cards (ASRock R9700) to get 128GB VRAM total and used the rest of the budget for a solid Threadripper platform.&lt;/p&gt; &lt;p&gt;Hardware Specs:&lt;/p&gt; &lt;p&gt;Total Cost: ~9,800‚Ç¨ (I get ~50% back, so effectively ~4,900‚Ç¨ for me).&lt;/p&gt; &lt;p&gt;CPU: AMD Ryzen Threadripper PRO 9955WX (16 Cores) Mainboard: ASRock WRX90 WS EVO RAM: 128GB DDR5 5600MHz GPU: 4x ASRock Radeon AI PRO R9700 32GB (Total 128GB VRAM) Configuration: All cards running at full PCIe 5.0 x16 bandwidth. Storage: 2x 2TB PCIe 4.0 SSD PSU: Seasonic 2200W Cooling: Alphacool Eisbaer Pro Aurora 360 CPU AIO&lt;/p&gt; &lt;p&gt;Benchmark Results&lt;/p&gt; &lt;p&gt;I tested various models ranging from 8B to 230B parameters.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Llama.cpp (Focus: Single User Latency) Settings: Flash Attention ON, Batch 2048&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Model Size Quant Mode Prompt t/s Gen t/s Meta-Llama-3.1-8B-Instruct 8B Q4_K_M GPU-Full 3169.16 81.01 Qwen2.5-32B-Instruct 32B Q4_K_M GPU-Full 848.68 25.14 Meta-Llama-3.1-70B-Instruct 70B Q4_K_M GPU-Full 399.03 12.66 gpt-oss-120b 120B Q4_K_M GPU-Full 2977.83 97.47 GLM-4.7-REAP-218B 218B Q3_K_M GPU-Full 504.15 17.48 MiniMax-M2.1 ~230B Q4_K_M Hybrid 938.89 32.12&lt;/p&gt; &lt;p&gt;Side note: I found that with PCIe 5.0, standard Pipeline Parallelism (Layer Split) is significantly faster (~97 t/s) than Tensor Parallelism/Row Split (~67 t/s) for a single user on this setup.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;vLLM (Focus: Throughput) Model: GPT-OSS-120B (bfloat16), TP=4, test for 20 requests&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Total Throughput: ~314 tokens/s (Generation) Prompt Processing: ~5339 tokens/s Single user throughput 50 tokens/s&lt;/p&gt; &lt;p&gt;I used rocm 7.1.1 for llama.cpp also testet Vulkan but it was worse&lt;/p&gt; &lt;p&gt;If I could do it again, I would have used the budget to buy a single NVIDIA RTX Pro 6000 Blackwell (96GB). Maybe I will, if local AI is going well for my use case, I swap the R9700 with Pro 6000 in the future.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NunzeCs"&gt; /u/NunzeCs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qgdb7f"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgdb7f/4x_amd_r9700_128gb_vram_threadripper_9955wx_build/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qgdb7f/4x_amd_r9700_128gb_vram_threadripper_9955wx_build/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-18T16:39:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
