<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-30T17:45:19+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qqidxp</id>
    <title>Why are small models (32b) scoring close to frontier models?</title>
    <updated>2026-01-29T19:27:44+00:00</updated>
    <author>
      <name>/u/Financial-Cap-8711</name>
      <uri>https://old.reddit.com/user/Financial-Cap-8711</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I keep seeing benchmark results where models like Qwen-32B or GLM-4.x Flash score surprisingly good as per their size than larger models like DeepSeek V3, Kimi K2.5 (1T), or GPT-5.x.&lt;/p&gt; &lt;p&gt;Given the huge gap in model size and training compute, I‚Äôd expect a bigger difference.&lt;/p&gt; &lt;p&gt;So what‚Äôs going on?&lt;/p&gt; &lt;p&gt;Are benchmarks basically saturated?&lt;/p&gt; &lt;p&gt;Is this distillation / contamination / inference-time tricks?&lt;/p&gt; &lt;p&gt;Do small models break down on long-horizon or real-world tasks that benchmarks don‚Äôt test?&lt;/p&gt; &lt;p&gt;Curious where people actually see the gap show up in practice.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Financial-Cap-8711"&gt; /u/Financial-Cap-8711 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qqidxp/why_are_small_models_32b_scoring_close_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qqidxp/why_are_small_models_32b_scoring_close_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qqidxp/why_are_small_models_32b_scoring_close_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-29T19:27:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1qrbzs8</id>
    <title>Update: OCTAVE MCP v1.0.0 - a semantic shorthand for LLM communication (turns out 40 tokens is all they need to learn it)</title>
    <updated>2026-01-30T17:31:07+00:00</updated>
    <author>
      <name>/u/sbuswell</name>
      <uri>https://old.reddit.com/user/sbuswell</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Quick update on OCTAVE (&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ptukgi/created_a_dslcontrol_layer_for_multiagent/"&gt;the semantic shorthand for LLM communication I posted about a month ago&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What's new:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Hit v1.0.0. 1610 tests passing, 90% coverage. I'd say it's production-grade now but welcome to feedback on this.&lt;/p&gt; &lt;p&gt;The more interesting finding though: &lt;strong&gt;40 tokens is all any LLM needs to become OCTAVE-literate and work this language.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Last time I said agents need a 458-token &amp;quot;literacy&amp;quot; skill. We ran a proper test - Claude, o3, and Gemini all producing valid OCTAVE after just the 40-token primer. The barrier was never capability, just invocation.&lt;/p&gt; &lt;p&gt;So now the README has the primer embedded directly. Any LLM that reads the README becomes OCTAVE-literate with zero configuration.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why bother with another format?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The MCP server does the heavy lifting:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;octave_write&lt;/code&gt; &lt;strong&gt;is like Prettier for docs&lt;/strong&gt; - LLMs don't need to memorize syntax rules. They write rough OCTAVE, the tool normalizes it to canonical form.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Self-validating documents&lt;/strong&gt; - v6 added &amp;quot;Holographic Contracts&amp;quot;: documents carry their own validation rules in the META block. The parser reads META first, compiles it to a grammar, then validates the document against its own rules.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;54-68% smaller than JSON&lt;/strong&gt; - not compression, just denser semantics. Mythology as a &amp;quot;semantic zip file&amp;quot; (SISYPHEAN encodes &amp;quot;repetitive + frustrating + endless + cyclical&amp;quot; in one word).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The insight:&lt;/strong&gt; &amp;quot;Change the water, not the pipe.&amp;quot; OCTAVE tunnels through JSON/MCP - you don't need native protocol support. The LLM outputs OCTAVE, MCP wraps it, receiver unwraps and validates.&lt;/p&gt; &lt;p&gt;Still useful in my own agentic setup. Still open to suggestions.&lt;/p&gt; &lt;p&gt;I would really love for folks to try this, as it's a real token saver from my perspective.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/elevanaltd/octave-mcp"&gt;https://github.com/elevanaltd/octave-mcp&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sbuswell"&gt; /u/sbuswell &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrbzs8/update_octave_mcp_v100_a_semantic_shorthand_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrbzs8/update_octave_mcp_v100_a_semantic_shorthand_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qrbzs8/update_octave_mcp_v100_a_semantic_shorthand_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-30T17:31:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1qrasty</id>
    <title>70B models</title>
    <updated>2026-01-30T16:49:44+00:00</updated>
    <author>
      <name>/u/Weak-Shelter-1698</name>
      <uri>https://old.reddit.com/user/Weak-Shelter-1698</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey 70B users. I need a little help/suggestion on finding a good 70B model. Can you guys tell me which one does roleplaying better and is creative?&lt;/p&gt; &lt;p&gt;- Steelskull/L3.3-San-Mai-R1-70b&lt;br /&gt; - BruhzWater/Apocrypha-L3.3-70b-0.4a&lt;br /&gt; - TheDrummer/Anubis-70B-v1.1&lt;br /&gt; - Strawberrylemonade-L3-70B-v1.2 (Used v1.1, it was unhinged but sometimes dumb)&lt;br /&gt; - Steelskull/L3.3-MS-Nevoria-70b (Used this one i liked it, but not sure).&lt;br /&gt; - I'd love any other 70B suggestion.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Weak-Shelter-1698"&gt; /u/Weak-Shelter-1698 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrasty/70b_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrasty/70b_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qrasty/70b_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-30T16:49:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1qqvb79</id>
    <title>GitHub - TrevorS/qwen3-tts-rs: Pure Rust implementation of Qwen3-TTS speech synthesis</title>
    <updated>2026-01-30T04:18:03+00:00</updated>
    <author>
      <name>/u/adefa</name>
      <uri>https://old.reddit.com/user/adefa</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qqvb79/github_trevorsqwen3ttsrs_pure_rust_implementation/"&gt; &lt;img alt="GitHub - TrevorS/qwen3-tts-rs: Pure Rust implementation of Qwen3-TTS speech synthesis" src="https://external-preview.redd.it/-L5uxiQVcL6ROhDnA4nw0i8GDaCVuNfbMGVzfQpr0OA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=61bb9f2e5549f6fb0e684d0b9a7d8fa88472f0b3" title="GitHub - TrevorS/qwen3-tts-rs: Pure Rust implementation of Qwen3-TTS speech synthesis" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I love pushing these coding platforms to their (my? our?) limits!&lt;/p&gt; &lt;p&gt;This time I ported the new Qwen 3 TTS model to Rust using Candle: &lt;a href="https://github.com/TrevorS/qwen3-tts-rs"&gt;https://github.com/TrevorS/qwen3-tts-rs&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It took a few days to get the first intelligible audio, but eventually voice cloning and voice design were working as well. I was never able to get in context learning (ICL) to work, neither with the original Python code, or with this library.&lt;/p&gt; &lt;p&gt;I've tested that CPU, CUDA, and Metal are all working. Check it out, peek at the code, let me know what you think!&lt;/p&gt; &lt;p&gt;P.S. -- new (to me) Claude Code trick: when working on a TTS speech model, write a skill to run the output through speech to text to verify the results. :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/adefa"&gt; /u/adefa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/TrevorS/qwen3-tts-rs"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qqvb79/github_trevorsqwen3ttsrs_pure_rust_implementation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qqvb79/github_trevorsqwen3ttsrs_pure_rust_implementation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-30T04:18:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1qr8q78</id>
    <title>Which program do you use for local llms? I keep having issues</title>
    <updated>2026-01-30T15:35:25+00:00</updated>
    <author>
      <name>/u/Raven-002</name>
      <uri>https://old.reddit.com/user/Raven-002</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For context, I have rtx4070 ti super 16GB and r9 9900x, 64GB ram (before it was expensive)&lt;/p&gt; &lt;p&gt;I have tried running models both with ollama and llamacpp (compiled from master pulled everytime to see if things are fixed)&lt;/p&gt; &lt;p&gt;Im always having problems with either tool calls, response format, reasoning and content, or just the parser not working and failing&lt;/p&gt; &lt;p&gt;Most problems are with llamacpp, but ollama also gave me problems, and it is also a lot slower&lt;/p&gt; &lt;p&gt;Im trying to get glm-4.7-flash, gpt-oss-20b and qwen3 coder 30b a3b&lt;/p&gt; &lt;p&gt;Im using unsloth UD-Q4 (or regular q4) for all of them&lt;/p&gt; &lt;p&gt;I tried to debug it with the help for Gemini, it couldn't help solve everything and each solution caused other errors...&lt;/p&gt; &lt;p&gt;Any suggestions for how to get them working? If i need a different GGUF, if there are presets that solve the issues, or just to use a different program to run it...&lt;/p&gt; &lt;p&gt;If anyone is interested in performance using llamacpp (when screen locked, otherwise about 10% slower): - gpt-oss-20b: ~200 tk/s (entirely on gpu) - glm-4.7-flash and qwen coder: ~80tk/s&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Raven-002"&gt; /u/Raven-002 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qr8q78/which_program_do_you_use_for_local_llms_i_keep/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qr8q78/which_program_do_you_use_for_local_llms_i_keep/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qr8q78/which_program_do_you_use_for_local_llms_i_keep/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-30T15:35:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1qr5hij</id>
    <title>PaddleOCR-VL 1.5</title>
    <updated>2026-01-30T13:29:38+00:00</updated>
    <author>
      <name>/u/iLaurens</name>
      <uri>https://old.reddit.com/user/iLaurens</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;PaddleOCR-VL 1.5 seems to have been released yesterday but hasn't been mentioned in this sub yet. Looks like an excellent update!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iLaurens"&gt; /u/iLaurens &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.paddleocr.ai/latest/en/index.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qr5hij/paddleocrvl_15/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qr5hij/paddleocrvl_15/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-30T13:29:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1qqfe1k</id>
    <title>Kimi AI team sent me this appreciation mail</title>
    <updated>2026-01-29T17:42:26+00:00</updated>
    <author>
      <name>/u/mehulgupta7991</name>
      <uri>https://old.reddit.com/user/mehulgupta7991</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qqfe1k/kimi_ai_team_sent_me_this_appreciation_mail/"&gt; &lt;img alt="Kimi AI team sent me this appreciation mail" src="https://preview.redd.it/0ztj2mk3sbgg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0cb8fb8a83de79c13b7ca310a250afa85f95fe79" title="Kimi AI team sent me this appreciation mail" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I covered Kimi K2.5 on my YT channel and the team sent me this mail with a premium access to agent swarm&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mehulgupta7991"&gt; /u/mehulgupta7991 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0ztj2mk3sbgg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qqfe1k/kimi_ai_team_sent_me_this_appreciation_mail/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qqfe1k/kimi_ai_team_sent_me_this_appreciation_mail/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-29T17:42:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1qr5sp3</id>
    <title>My local LLM usecase</title>
    <updated>2026-01-30T13:42:47+00:00</updated>
    <author>
      <name>/u/TheProtector0034</name>
      <uri>https://old.reddit.com/user/TheProtector0034</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;No matter how much you spent on hardware you simply cant get the same performance as the SOTA models at home. I am not only talking about the quality of the output but also PP and TG. I use LLM‚Äôs for vibe coding, as a oracle for asking technical questions in my field (system administrator/devops) and tagging bookmarks in Karakeep. For the ‚Äúoracle‚Äù usecase I noticed the GPT-OSS 20b does a decent job and for tagging bookmarks Gemma 4b works also great. I run these models on a MBP M4 Pro with 24GB RAM. For vibecoding I use Claude Pro Subscription for 20 euro a month in combination with GLM 4.7 Code Subscription for when I reach my limits from the Claude subscription.&lt;/p&gt; &lt;p&gt;Now I wait for the M5 Mac Mini which should show great improvement with PP and settle with gemma 4b and GPT-OSS 20b. A current M4 Mac Mini with 256GB SSD and 32GB RAM costs around 1200 euro and as I work in the education sector I can also get some discount from Apple. I expect that the same configuration when the M5 is released will be more or less at the same price level (yes I know the situation with RAM prices etc but I can imagine Apple buys this in bulk and can keep the prices ‚Äúlow‚Äù). I think 256GB SSD is enough as the biggest size you can run as a model is around 30GB in theory and around 25GB in more practical uses.&lt;/p&gt; &lt;p&gt;So when the new Mac Mini is out I finally will get a dedicated LLM machine with M5, 32GB RAM and 256GB for around 1200 euros which fits nicely in my mini rack. What do do you guys think about this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheProtector0034"&gt; /u/TheProtector0034 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qr5sp3/my_local_llm_usecase/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qr5sp3/my_local_llm_usecase/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qr5sp3/my_local_llm_usecase/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-30T13:42:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1qr0ubh</id>
    <title>Beginner in RAG, Need help.</title>
    <updated>2026-01-30T09:28:13+00:00</updated>
    <author>
      <name>/u/whatshouldidotoknow</name>
      <uri>https://old.reddit.com/user/whatshouldidotoknow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I have a 400-500 page unstructured PDF document with selectable text filled with Tables. I have been provided Nvidia L40S GPU for a week. I need help in parsing such PDf's to be able to run RAG on this. My task is to make RAG possible on such documents which span anywhere betwee 400 to 1000 pages. I work in pharma so i cant use any paid API's to parse this.&lt;br /&gt; I have tried Camelot - didnt work well,&lt;br /&gt; Tried Docling, works well but takes forever to parse 500 pages.&lt;br /&gt; I thought of converting the PDF to Json, that didnt work so well either. I am new to all this, please help me with some idea on how to go forward.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/whatshouldidotoknow"&gt; /u/whatshouldidotoknow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qr0ubh/beginner_in_rag_need_help/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qr0ubh/beginner_in_rag_need_help/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qr0ubh/beginner_in_rag_need_help/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-30T09:28:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1qr6b63</id>
    <title>Am I the only one who thinks limiting ROCm support for local Finetunes just to these cards makes no sense? Why rx 7700 is supported but 7600 is not? Or RDNA2? Does anyone have an idea how to use QLoRA on RX6600? Official or not.</title>
    <updated>2026-01-30T14:03:36+00:00</updated>
    <author>
      <name>/u/hackiv</name>
      <uri>https://old.reddit.com/user/hackiv</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qr6b63/am_i_the_only_one_who_thinks_limiting_rocm/"&gt; &lt;img alt="Am I the only one who thinks limiting ROCm support for local Finetunes just to these cards makes no sense? Why rx 7700 is supported but 7600 is not? Or RDNA2? Does anyone have an idea how to use QLoRA on RX6600? Official or not." src="https://preview.redd.it/slm4vnwythgg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=99abf56715bce700c62130252037a6777b8b03e9" title="Am I the only one who thinks limiting ROCm support for local Finetunes just to these cards makes no sense? Why rx 7700 is supported but 7600 is not? Or RDNA2? Does anyone have an idea how to use QLoRA on RX6600? Official or not." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html"&gt;https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://rocm.docs.amd.com/projects/ai-developer-hub/en/v5.1/notebooks/fine_tune/QLoRA_Llama-3.1.html"&gt;https://rocm.docs.amd.com/projects/ai-developer-hub/en/v5.1/notebooks/fine_tune/QLoRA_Llama-3.1.html&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hackiv"&gt; /u/hackiv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/slm4vnwythgg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qr6b63/am_i_the_only_one_who_thinks_limiting_rocm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qr6b63/am_i_the_only_one_who_thinks_limiting_rocm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-30T14:03:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1qrazir</id>
    <title>Do you think we support enough open source/weights?</title>
    <updated>2026-01-30T16:56:21+00:00</updated>
    <author>
      <name>/u/Leflakk</name>
      <uri>https://old.reddit.com/user/Leflakk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We mainly rely on chinese models because the more AI becomes smart &amp;amp; usefull the more labs or companies tend to close (especially US big techs). So probably (my opinion) in the futur US will do their best limit access to chinese stuff.&lt;/p&gt; &lt;p&gt;But being part of this community, I feel a bit guilty not to support enough the all these labs that keep doing efforts to create and open stuff. &lt;/p&gt; &lt;p&gt;So to change that, I will try to test more models (even those which are not my favourites) and provide more real world usage feedback. Could we have a flair dedicated to feebacks so things may be more readable??&lt;/p&gt; &lt;p&gt;Do you have others ideas?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Leflakk"&gt; /u/Leflakk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrazir/do_you_think_we_support_enough_open_sourceweights/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrazir/do_you_think_we_support_enough_open_sourceweights/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qrazir/do_you_think_we_support_enough_open_sourceweights/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-30T16:56:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1qrbel2</id>
    <title>Qwen3 ASR 1.7B vs Whisper v3 Large</title>
    <updated>2026-01-30T17:10:38+00:00</updated>
    <author>
      <name>/u/OGScottingham</name>
      <uri>https://old.reddit.com/user/OGScottingham</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi!&lt;/p&gt; &lt;p&gt;Has anybody had the chance to try out the new transcription model from the Qwen team? It just came out yesterday and I haven't seen much talk about it here.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/QwenLM/Qwen3-ASR?tab=readme-ov-file"&gt;https://github.com/QwenLM/Qwen3-ASR?tab=readme-ov-file&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Their intro from the github:&lt;br /&gt; &lt;a href="https://camo.githubusercontent.com/0f65d4213247aa283f23cc3e2c5e5e51542670d4942123430ada7a58587d6c66/68747470733a2f2f7169616e77656e2d7265732e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f5177656e332d4153522d5265706f2f7177656e335f6173725f696e74726f64756374696f6e2e706e67"&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The Qwen3-ASR family includes Qwen3-ASR-1.7B and Qwen3-ASR-0.6B, which support language identification and ASR for 52 languages and dialects. Both leverage large-scale speech training data and the strong audio understanding capability of their foundation model, Qwen3-Omni. Experiments show that the 1.7B version achieves state-of-the-art performance among open-source ASR models and is competitive with the strongest proprietary commercial APIs. Here are the main features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;All-in-one&lt;/strong&gt;: Qwen3-ASR-1.7B and Qwen3-ASR-0.6B support language identification and speech recognition for 30 languages and 22 Chinese dialects, so as to English accents from multiple countries and regions.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Excellent and Fast&lt;/strong&gt;: The Qwen3-ASR family ASR models maintains high-quality and robust recognition under complex acoustic environments and challenging text patterns. Qwen3-ASR-1.7B achieves strong performance on both open-sourced and internal benchmarks. While the 0.6B version achieves accuracy-efficient trade-off, it reaches 2000 times throughput at a concurrency of 128. They both achieve streaming / offline unified inference with single model and support transcribe long audio.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Novel and strong forced alignment Solution&lt;/strong&gt;: We introduce Qwen3-ForcedAligner-0.6B, which supports timestamp prediction for arbitrary units within up to 5 minutes of speech in 11 languages. Evaluations show its timestamp accuracy surpasses E2E based forced-alignment models.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Comprehensive inference toolkit&lt;/strong&gt;: In addition to open-sourcing the architectures and weights of the Qwen3-ASR series, we also release a powerful, full-featured inference framework that supports vLLM-based batch inference, asynchronous serving, streaming inference, timestamp prediction, and more.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OGScottingham"&gt; /u/OGScottingham &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrbel2/qwen3_asr_17b_vs_whisper_v3_large/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrbel2/qwen3_asr_17b_vs_whisper_v3_large/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qrbel2/qwen3_asr_17b_vs_whisper_v3_large/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-30T17:10:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1qqj51h</id>
    <title>LingBot-World outperforms Genie 3 in dynamic simulation and is fully Open Source</title>
    <updated>2026-01-29T19:54:56+00:00</updated>
    <author>
      <name>/u/Electrical-Shape-266</name>
      <uri>https://old.reddit.com/user/Electrical-Shape-266</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qqj51h/lingbotworld_outperforms_genie_3_in_dynamic/"&gt; &lt;img alt="LingBot-World outperforms Genie 3 in dynamic simulation and is fully Open Source" src="https://external-preview.redd.it/NWM3YmNxOGtlY2dnMazYtBKu82jdVCrAUapl0f29ZcySaNJ_OhsJC51jAkVT.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ec16c92dbc441bee1bbd5f8a0850b37011370867" title="LingBot-World outperforms Genie 3 in dynamic simulation and is fully Open Source" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The newly released LingBot-World framework offers the first high capability world model that is fully open source, directly contrasting with proprietary systems like Genie 3. The technical report highlights that while both models achieve real-time interactivity, LingBot-World surpasses Genie 3 in dynamic degree, meaning it handles complex physics and scene transitions with greater fidelity. It achieves 16 frames per second and features emergent spatial memory where objects remain consistent even after leaving the field of view for 60 seconds. This release effectively breaks the monopoly on interactive world simulation by providing the community with full access to the code and model weights.&lt;/p&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/collections/robbyant/lingbot-world"&gt;https://huggingface.co/collections/robbyant/lingbot-world&lt;/a&gt;&lt;/p&gt; &lt;p&gt;AGI will be very near. Let's talk about it!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Electrical-Shape-266"&gt; /u/Electrical-Shape-266 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/fjyoor8kecgg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qqj51h/lingbotworld_outperforms_genie_3_in_dynamic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qqj51h/lingbotworld_outperforms_genie_3_in_dynamic/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-29T19:54:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1qrbfez</id>
    <title>spec : add ngram-mod by ggerganov ¬∑ Pull Request #19164 ¬∑ ggml-org/llama.cpp</title>
    <updated>2026-01-30T17:11:26+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrbfez/spec_add_ngrammod_by_ggerganov_pull_request_19164/"&gt; &lt;img alt="spec : add ngram-mod by ggerganov ¬∑ Pull Request #19164 ¬∑ ggml-org/llama.cpp" src="https://external-preview.redd.it/g3Kl7EuA7uN68kx8-95HOYqEV6uFaejZ8ghgYxWQDJQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d663e7e68ebfca599cda3a0ba19c677f3a8b64c8" title="spec : add ngram-mod by ggerganov ¬∑ Pull Request #19164 ¬∑ ggml-org/llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;watch the video&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/19164"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrbfez/spec_add_ngrammod_by_ggerganov_pull_request_19164/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qrbfez/spec_add_ngrammod_by_ggerganov_pull_request_19164/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-30T17:11:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1qr5v9d</id>
    <title>Why we went desktop and local-first for agents 6 months ago</title>
    <updated>2026-01-30T13:45:45+00:00</updated>
    <author>
      <name>/u/Farajizx</name>
      <uri>https://old.reddit.com/user/Farajizx</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We‚Äôve been thinking a lot about first principles when building agent project, and one conclusion we keep coming back to is this:&lt;/p&gt; &lt;p&gt;The first thing you should optimize for is the agent‚Äôs capability ceiling.&lt;/p&gt; &lt;p&gt;From that perspective, a desktop-first agent architecture makes a lot of sense. A few reasons why:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Context access&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;If you want agents to be genuinely useful, they need real user context. On desktop, an agent can natively and seamlessly access local files, folders, running apps, logs, configs, and other artifacts that are either impossible or extremely awkward to reach from a purely web-based agent.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Permissions equal intelligence&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Powerful agents need powerful permissions. Desktop agents can read and write the local file system, control native software like IDEs, terminals, browsers, or design tools, and make system-level calls or interact with hardware. This isn‚Äôt about being invasive, but about enabling workflows that simply don‚Äôt fit inside a web sandbox.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Web parity without web limitations&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;A desktop agent can still do everything a web agent can do, whether through an embedded Chromium environment or via browser-extension-style control. The reverse is not true: web agents can‚Äôt escape their sandbox.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Cost structure&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;An often overlooked point is that desktop agents run on user-owned compute. Browsers, terminals, and local tools all execute locally, which significantly reduces backend costs and makes high-frequency, long-running agents much more viable.&lt;/p&gt; &lt;p&gt;This line of thinking is what led us to build Eigent, the opensource alternative to cowork&lt;/p&gt; &lt;p&gt;Curious how others here think about:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Desktop-first vs web-first agents&lt;/li&gt; &lt;li&gt;Capability vs security trade-offs&lt;/li&gt; &lt;li&gt;Whether ‚Äúagent OS‚Äù is a real emerging category or just hype&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Would love to hear thoughts from people building or running local agents!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Farajizx"&gt; /u/Farajizx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qr5v9d/why_we_went_desktop_and_localfirst_for_agents_6/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qr5v9d/why_we_went_desktop_and_localfirst_for_agents_6/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qr5v9d/why_we_went_desktop_and_localfirst_for_agents_6/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-30T13:45:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1qqhhtx</id>
    <title>Mistral CEO Arthur Mensch: ‚ÄúIf you treat intelligence as electricity, then you just want to make sure that your access to intelligence cannot be throttled.‚Äù</title>
    <updated>2026-01-29T18:56:08+00:00</updated>
    <author>
      <name>/u/Wonderful-Excuse4922</name>
      <uri>https://old.reddit.com/user/Wonderful-Excuse4922</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qqhhtx/mistral_ceo_arthur_mensch_if_you_treat/"&gt; &lt;img alt="Mistral CEO Arthur Mensch: ‚ÄúIf you treat intelligence as electricity, then you just want to make sure that your access to intelligence cannot be throttled.‚Äù" src="https://external-preview.redd.it/NW03ZGMyazI1Y2dnMWh2gxSpyeR6q2IEmV4jHAJM791DDo_e5MvHim0gQe4g.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=43d352ce0d8764709982770e551c498fa8279ecc" title="Mistral CEO Arthur Mensch: ‚ÄúIf you treat intelligence as electricity, then you just want to make sure that your access to intelligence cannot be throttled.‚Äù" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wonderful-Excuse4922"&gt; /u/Wonderful-Excuse4922 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/wd12dl725cgg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qqhhtx/mistral_ceo_arthur_mensch_if_you_treat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qqhhtx/mistral_ceo_arthur_mensch_if_you_treat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-29T18:56:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1qr5vdu</id>
    <title>LM Studio doesn't let continue generating a message anymore</title>
    <updated>2026-01-30T13:45:54+00:00</updated>
    <author>
      <name>/u/PhyrexianSpaghetti</name>
      <uri>https://old.reddit.com/user/PhyrexianSpaghetti</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I used LM studio for a long time and always liked it. Since my computer isn't nasa-level, I have to use quantized llms, and this means that often, to make them understand what I want, I needed to edit their answer with something along the lines of &amp;quot;Oh I see, you need me to...&amp;quot; and then click on the button that forced it to continue the generation based on the start I fed it.&lt;br /&gt; After the latest update, I can't find the button to make the model continue an edited answer, for some reason they seem to have removed the most important feature of running models locally.&lt;/p&gt; &lt;p&gt;Did they move it or is it gone? Is there another similarly well curated and easy to use software to do that without complex setup?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PhyrexianSpaghetti"&gt; /u/PhyrexianSpaghetti &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qr5vdu/lm_studio_doesnt_let_continue_generating_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qr5vdu/lm_studio_doesnt_let_continue_generating_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qr5vdu/lm_studio_doesnt_let_continue_generating_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-30T13:45:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1qqw3ov</id>
    <title>GLM 4.7 Flash 30B PRISM + Web Search: Very solid.</title>
    <updated>2026-01-30T04:56:57+00:00</updated>
    <author>
      <name>/u/My_Unbiased_Opinion</name>
      <uri>https://old.reddit.com/user/My_Unbiased_Opinion</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just got this set up yesterday. I have been messing around with it and I am extremely impressed. I find that it is very efficient in reasoning compared to Qwen models. The model is quite uncensored so I'm able to research any topics, it is quite thorough. &lt;/p&gt; &lt;p&gt;The knowledge is definitely less than 120B Derestricted, but once Web Search RAG is involved, I'm finding the 30B model generally superior with far less soft refusals. Since the model has web access, I feel the base knowledge deficit is mitigated. &lt;/p&gt; &lt;p&gt;Running it in the latest LMstudio beta + OpenwebUI. Y'all gotta try it. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/My_Unbiased_Opinion"&gt; /u/My_Unbiased_Opinion &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qqw3ov/glm_47_flash_30b_prism_web_search_very_solid/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qqw3ov/glm_47_flash_30b_prism_web_search_very_solid/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qqw3ov/glm_47_flash_30b_prism_web_search_very_solid/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-30T04:56:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1qqpon2</id>
    <title>OpenCode + llama.cpp + GLM-4.7 Flash: Claude Code at home</title>
    <updated>2026-01-30T00:07:33+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qqpon2/opencode_llamacpp_glm47_flash_claude_code_at_home/"&gt; &lt;img alt="OpenCode + llama.cpp + GLM-4.7 Flash: Claude Code at home" src="https://b.thumbs.redditmedia.com/7H9fabOO-Ob3KECLa7f5HGWJVRXrjNM3C7iqKkauNZU.jpg" title="OpenCode + llama.cpp + GLM-4.7 Flash: Claude Code at home" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;command I use (may be suboptimal but it works for me now):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES=0,1,2 llama-server --jinja --host 0.0.0.0 -m /mnt/models1/GLM/GLM-4.7-Flash-Q8_0.gguf --ctx-size 200000 --parallel 1 --batch-size 2048 --ubatch-size 1024 --flash-attn on --cache-ram 61440 --context-shift &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;potential additional speedup has been merged into llama.cpp: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1qrbfez/comment/o2mzb1q/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1qrbfez/comment/o2mzb1q/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qqpon2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qqpon2/opencode_llamacpp_glm47_flash_claude_code_at_home/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qqpon2/opencode_llamacpp_glm47_flash_claude_code_at_home/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-30T00:07:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1qrazyy</id>
    <title>Cline team got absorbed by OpenAI. Kilo is going full source available in response.</title>
    <updated>2026-01-30T16:56:49+00:00</updated>
    <author>
      <name>/u/demon_bhaiya</name>
      <uri>https://old.reddit.com/user/demon_bhaiya</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrazyy/cline_team_got_absorbed_by_openai_kilo_is_going/"&gt; &lt;img alt="Cline team got absorbed by OpenAI. Kilo is going full source available in response." src="https://external-preview.redd.it/OJiv7stnybHLdn8-mzf6t_NZ9C8xS7VIYLhMSJsX0d8.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=969735c0073c014c64189d4c6b79a9e599fe2c52" title="Cline team got absorbed by OpenAI. Kilo is going full source available in response." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those who used Cline with local models, heads up that the core team appears to have joined OpenAI's Codex group based on their LinkedIn profiles. No official announcement yet, but we have seen how these acqui-hires usually play out.&lt;/p&gt; &lt;p&gt;Kilo Code (which forked from Cline and Roo Code) just responded by announcing they are making their backend source available by Feb 6. The VS Code extension, JetBrains plugin, and CLI stay Apache 2.0(Open source). Their gateway supports 500+ models including Qwen, DeepSeek, and Mistral.&lt;/p&gt; &lt;p&gt;They're offering $100 credits to anyone who contributed to Cline, and $150 per merged PR in February. If you want to keep building on an open codebase instead of watching another project disappear into a walled garden, might be worth checking out.&lt;/p&gt; &lt;p&gt;The agentic coding space needs alternatives that work with local and open weight models. Would suck to see all the decent tools end up controlled by the big labs.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/demon_bhaiya"&gt; /u/demon_bhaiya &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://blog.kilo.ai/p/cline-just-acqui-hired"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrazyy/cline_team_got_absorbed_by_openai_kilo_is_going/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qrazyy/cline_team_got_absorbed_by_openai_kilo_is_going/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-30T16:56:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1qr7cbh</id>
    <title>Kimi-k2.5 reaches gemini 2.5 Pro-like performance in long context!</title>
    <updated>2026-01-30T14:44:01+00:00</updated>
    <author>
      <name>/u/fictionlive</name>
      <uri>https://old.reddit.com/user/fictionlive</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qr7cbh/kimik25_reaches_gemini_25_prolike_performance_in/"&gt; &lt;img alt="Kimi-k2.5 reaches gemini 2.5 Pro-like performance in long context!" src="https://preview.redd.it/on28koqz0igg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=011c51e6852eeaee308ab92b0cd9e4852da4bfe0" title="Kimi-k2.5 reaches gemini 2.5 Pro-like performance in long context!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fictionlive"&gt; /u/fictionlive &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/on28koqz0igg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qr7cbh/kimik25_reaches_gemini_25_prolike_performance_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qr7cbh/kimik25_reaches_gemini_25_prolike_performance_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-30T14:44:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1qr7ncz</id>
    <title>Design Arena is now dominated by an open model</title>
    <updated>2026-01-30T14:55:35+00:00</updated>
    <author>
      <name>/u/moks4tda</name>
      <uri>https://old.reddit.com/user/moks4tda</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qr7ncz/design_arena_is_now_dominated_by_an_open_model/"&gt; &lt;img alt="Design Arena is now dominated by an open model" src="https://a.thumbs.redditmedia.com/IOzZQkj-NN9LpvwuZen1AYFWFys9dnrIFBwpaZCd7D0.jpg" title="Design Arena is now dominated by an open model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The first month of 2026 is already this wild, I can't even imagine what's coming next!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/moks4tda"&gt; /u/moks4tda &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qr7ncz"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qr7ncz/design_arena_is_now_dominated_by_an_open_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qr7ncz/design_arena_is_now_dominated_by_an_open_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-30T14:55:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1qr4p4x</id>
    <title>Yann LeCun says the best open models are not coming from the West. Researchers across the field are using Chinese models. Openness drove AI progress. Close access, and the West risks slowing itself.</title>
    <updated>2026-01-30T12:55:38+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qr4p4x/yann_lecun_says_the_best_open_models_are_not/"&gt; &lt;img alt="Yann LeCun says the best open models are not coming from the West. Researchers across the field are using Chinese models. Openness drove AI progress. Close access, and the West risks slowing itself." src="https://external-preview.redd.it/MnNnNHZ6eGNoaGdnMcC0w-E97YmQ2Bn80LEN79By6gOnSLJ7DXbqces3JuUE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=189015efd04b82b6e73fa3d8be460d38d65659e4" title="Yann LeCun says the best open models are not coming from the West. Researchers across the field are using Chinese models. Openness drove AI progress. Close access, and the West risks slowing itself." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From Forbes on YouTube: Yann LeCun Gives Unfiltered Take On The Future Of AI In Davos: &lt;a href="https://www.youtube.com/watch?v=MWMe7yjPYpE"&gt;https://www.youtube.com/watch?v=MWMe7yjPYpE&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Video by vitrupo on ùïè: &lt;a href="https://x.com/vitrupo/status/2017218170273313033"&gt;https://x.com/vitrupo/status/2017218170273313033&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/n31pvrxchhgg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qr4p4x/yann_lecun_says_the_best_open_models_are_not/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qr4p4x/yann_lecun_says_the_best_open_models_are_not/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-30T12:55:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1qp46za</id>
    <title>AMA Announcement: Moonshot AI, The Opensource Frontier Lab Behind Kimi K2.5 SoTA Model (Wednesday, 8AM-11AM PST)</title>
    <updated>2026-01-28T06:54:28+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qp46za/ama_announcement_moonshot_ai_the_opensource/"&gt; &lt;img alt="AMA Announcement: Moonshot AI, The Opensource Frontier Lab Behind Kimi K2.5 SoTA Model (Wednesday, 8AM-11AM PST)" src="https://preview.redd.it/y2qj7ancf1gg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7bb1df11c9d46ca94be0db3438449dc28e2dd48e" title="AMA Announcement: Moonshot AI, The Opensource Frontier Lab Behind Kimi K2.5 SoTA Model (Wednesday, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; üëã&lt;/p&gt; &lt;p&gt;We're excited for Wednesday's guests, &lt;strong&gt;The Moonshot AI Lab Team!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Kicking things off Wednesday, Jan. 28th, 8 AM‚Äì11 AM PST&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;‚ö†Ô∏è &lt;strong&gt;Note:&lt;/strong&gt; The AMA itself will be hosted in a &lt;strong&gt;separate thread,&lt;/strong&gt; please don‚Äôt post questions here.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/y2qj7ancf1gg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qp46za/ama_announcement_moonshot_ai_the_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qp46za/ama_announcement_moonshot_ai_the_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T06:54:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpewj7</id>
    <title>AMA With Kimi, The Open-source Frontier Lab Behind Kimi K2.5 Model</title>
    <updated>2026-01-28T15:46:40+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"&gt; &lt;img alt="AMA With Kimi, The Open-source Frontier Lab Behind Kimi K2.5 Model" src="https://b.thumbs.redditmedia.com/Tdp5mXDQmwe_e0sKIEY1hjWX6FrN679odChZp8YL2Rk.jpg" title="AMA With Kimi, The Open-source Frontier Lab Behind Kimi K2.5 Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Today we are having &lt;strong&gt;Kimi&lt;/strong&gt;, the research lab behind the &lt;strong&gt;Kimi&lt;/strong&gt; &lt;strong&gt;K2.5&lt;/strong&gt;. We‚Äôre excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/ComfortableAsk4494/"&gt;u/ComfortableAsk4494&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/zxytim/"&gt;u/zxytim&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/ppwwyyxx/"&gt;u/ppwwyyxx&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 8 AM ‚Äì 11 AM PST, with the Kimi team continuing to follow up on questions over the next 24 hours.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3yq8msvp24gg1.png?width=2000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=98c89b5d86ee1197799532fead6a84da2223b389"&gt;https://preview.redd.it/3yq8msvp24gg1.png?width=2000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=98c89b5d86ee1197799532fead6a84da2223b389&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Thanks everyone for joining our AMA. The live part has ended and the Kimi team will be following up with more answers sporadically over the next 24 hours.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T15:46:40+00:00</published>
  </entry>
</feed>
