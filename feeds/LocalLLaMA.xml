<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-30T15:35:27+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1p9qe7o</id>
    <title>Qwen3 Next imatrix GGUFs up!</title>
    <updated>2025-11-29T14:33:49+00:00</updated>
    <author>
      <name>/u/noneabove1182</name>
      <uri>https://old.reddit.com/user/noneabove1182</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just figured I'd post in case anyone's looking for imatrix and IQ quants&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/bartowski/Qwen_Qwen3-Next-80B-A3B-Instruct-GGUF"&gt;https://huggingface.co/bartowski/Qwen_Qwen3-Next-80B-A3B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/bartowski/Qwen_Qwen3-Next-80B-A3B-Thinking-GGUF"&gt;https://huggingface.co/bartowski/Qwen_Qwen3-Next-80B-A3B-Thinking-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;As usual this also uses my PR/fork for slightly more optimized MoE quantization &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/12727"&gt;https://github.com/ggml-org/llama.cpp/pull/12727&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/noneabove1182"&gt; /u/noneabove1182 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9qe7o/qwen3_next_imatrix_ggufs_up/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9qe7o/qwen3_next_imatrix_ggufs_up/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p9qe7o/qwen3_next_imatrix_ggufs_up/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-29T14:33:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1paaxlj</id>
    <title>Newbie Question about GPU choice</title>
    <updated>2025-11-30T06:04:31+00:00</updated>
    <author>
      <name>/u/mundane_marietta</name>
      <uri>https://old.reddit.com/user/mundane_marietta</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Use case - training a model on 10 years of my writing, high school football player data, scouting reports, historical stats, etc., so that I can create a model that churns out 25 articles a day (between 250-750 words) for my football recruiting website. &lt;/p&gt; &lt;p&gt;I have good deals in place for a 5070 for $475 and a 4080 for $715 tax included. I just need to decide which one would be the best value for my use case. My local Microcenter does have a few 3090's available for $775.&lt;/p&gt; &lt;p&gt;I have no idea what I'm doing, so the upfront investment does seem daunting as the prices climb, but the season is almost over, and I believe with time, I can figure out what to do.&lt;/p&gt; &lt;p&gt;Not sure if this is the appropriate place to ask this question, and I know VRAM is king, but not sure if a 5070 could do the trick for my use case.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mundane_marietta"&gt; /u/mundane_marietta &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1paaxlj/newbie_question_about_gpu_choice/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1paaxlj/newbie_question_about_gpu_choice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1paaxlj/newbie_question_about_gpu_choice/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-30T06:04:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1pa02w2</id>
    <title>My preferred gpt-oss system prompt</title>
    <updated>2025-11-29T21:17:52+00:00</updated>
    <author>
      <name>/u/Chafedokibu</name>
      <uri>https://old.reddit.com/user/Chafedokibu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I feel like it doesn't matter what your prompt is gpt-oss explodes a prompt that's too wordy and WAY too long. I didn't like how I could give it a four word sentence and it would consistently give me no less than like two full pages of information. I named it Nova but obviously you can change it to anything.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;You are Nova. Nova is an artificial assistant that gives the user a human-like conversational experience. Nova is helpful, honest, charismatic, and straight to the point. Before Nova responds to any prompt Nova must first determine if asking the user a single or multiple questions would help Nova be a better and more accurate help. Pre-response-questions determination should be based on the level of detail in the context window. Note: Nova is not required to ask the user any questions. After Nova has determined that Nova has an adequate amount of information needed to proceed with the prompt given by the user Nova then must determine the length of Nova‚Äôs response. The length of Nova‚Äôs responses should be determined based off of how complex and detailed Nova‚Äôs response should be. The amount of complexity and detail in Nova‚Äôs responses should be determined by the amount of complexity and detail in the context window that refers to the current response Nova is tasked to complete. &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Chafedokibu"&gt; /u/Chafedokibu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pa02w2/my_preferred_gptoss_system_prompt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pa02w2/my_preferred_gptoss_system_prompt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pa02w2/my_preferred_gptoss_system_prompt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-29T21:17:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1pad7c9</id>
    <title>Can anyone share their experience on how a local LLM helps them in building software?</title>
    <updated>2025-11-30T08:25:15+00:00</updated>
    <author>
      <name>/u/National-Fold-2375</name>
      <uri>https://old.reddit.com/user/National-Fold-2375</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;* How large is your codebase/repository?&lt;/p&gt; &lt;p&gt;* How much VRAM do you have and what model do you use and how large is your context window?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/National-Fold-2375"&gt; /u/National-Fold-2375 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pad7c9/can_anyone_share_their_experience_on_how_a_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pad7c9/can_anyone_share_their_experience_on_how_a_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pad7c9/can_anyone_share_their_experience_on_how_a_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-30T08:25:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1pai9ol</id>
    <title>GPU offload/Context size in LM Studio</title>
    <updated>2025-11-30T13:27:06+00:00</updated>
    <author>
      <name>/u/anonXMR</name>
      <uri>https://old.reddit.com/user/anonXMR</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hi, &lt;/p&gt; &lt;p&gt;I was wondering (as I'm somewhat new to this), beyond slowly becoming slower during inference, is there any reason not to increase context size in LM Studio?&lt;/p&gt; &lt;p&gt;I have 128GB of RAM and am using gpt-oss-120 on my Mac, the 4096 context seems way too small.&lt;/p&gt; &lt;p&gt;Also, I noticed even for models that are small enough to be fully GPU bound, the default settings don't offload completely to the GPU, any idea why?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/anonXMR"&gt; /u/anonXMR &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pai9ol/gpu_offloadcontext_size_in_lm_studio/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pai9ol/gpu_offloadcontext_size_in_lm_studio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pai9ol/gpu_offloadcontext_size_in_lm_studio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-30T13:27:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1p9udbu</id>
    <title>PrimeIntellect is actually awesome</title>
    <updated>2025-11-29T17:20:25+00:00</updated>
    <author>
      <name>/u/Icy_Gas8807</name>
      <uri>https://old.reddit.com/user/Icy_Gas8807</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9udbu/primeintellect_is_actually_awesome/"&gt; &lt;img alt="PrimeIntellect is actually awesome" src="https://preview.redd.it/ew6myj9kc84g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fdd6ec877e717ceaf2aa8afda32707a0332d8088" title="PrimeIntellect is actually awesome" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tested prime intellect 3: - Q4_K_L&lt;br /&gt; - 71.82GB&lt;br /&gt; - Uses Q8_0 for embed and output weights. Good quality, recommended.&lt;/p&gt; &lt;p&gt;Model seams intelligent enough for most of my daily tasks, will be using it along with gpt-oss-120B. This did give me a hope, if this trend continues and hoping to get great models like this at below 160B @fp4, inference possible in strix halo chips. &lt;/p&gt; &lt;p&gt;Also, now I want to connect it to web search. I know it is previously discussed: (&lt;a href="https://github.com/mrkrsl/web-search-mcp"&gt;https://github.com/mrkrsl/web-search-mcp&lt;/a&gt;) this seams to be the best option without jargon of adding api. Are there any better alternatives?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Icy_Gas8807"&gt; /u/Icy_Gas8807 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ew6myj9kc84g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9udbu/primeintellect_is_actually_awesome/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p9udbu/primeintellect_is_actually_awesome/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-29T17:20:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1pa1c6j</id>
    <title>Watch as my Llama.cpp and FastAPI servers process requests from my Unity game</title>
    <updated>2025-11-29T22:12:25+00:00</updated>
    <author>
      <name>/u/LandoRingel</name>
      <uri>https://old.reddit.com/user/LandoRingel</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pa1c6j/watch_as_my_llamacpp_and_fastapi_servers_process/"&gt; &lt;img alt="Watch as my Llama.cpp and FastAPI servers process requests from my Unity game" src="https://external-preview.redd.it/OGlseDdndHlxOTRnMX4quFMd7p9QoCGjTuoiWgG_oJG2-Mck0DisnSL19IfY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=541fbaca8b3ee85e4f95929676b26a8f9402e489" title="Watch as my Llama.cpp and FastAPI servers process requests from my Unity game" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://landoringel.itch.io/good-cop-bad-cop"&gt;https://landoringel.itch.io/good-cop-bad-cop&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LandoRingel"&gt; /u/LandoRingel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/mexvdk4lq94g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pa1c6j/watch_as_my_llamacpp_and_fastapi_servers_process/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pa1c6j/watch_as_my_llamacpp_and_fastapi_servers_process/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-29T22:12:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1p9ojio</id>
    <title>Yet another reason to stick with local models</title>
    <updated>2025-11-29T13:07:36+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9ojio/yet_another_reason_to_stick_with_local_models/"&gt; &lt;img alt="Yet another reason to stick with local models" src="https://b.thumbs.redditmedia.com/LVWG1v1DcQ2DgWlztEoKA3ITIG04JVS8k3_QhWcs3tw.jpg" title="Yet another reason to stick with local models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/uofoe3u5374g1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=219b2ab46ac5d0b74767604bd131b78757a40ac9"&gt;https://preview.redd.it/uofoe3u5374g1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=219b2ab46ac5d0b74767604bd131b78757a40ac9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/btibor91/status/1994714152636690834?s=20"&gt;Tibor Blaho&lt;/a&gt;, a trusted reverse engineer, found ad system strings inside the latest ChatGPT Android beta(v1.2025.329).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9ojio/yet_another_reason_to_stick_with_local_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9ojio/yet_another_reason_to_stick_with_local_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p9ojio/yet_another_reason_to_stick_with_local_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-29T13:07:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1pai12j</id>
    <title>Direct Google AI Mode Scraper - No API needed, pure Python, works in headless mode!</title>
    <updated>2025-11-30T13:15:32+00:00</updated>
    <author>
      <name>/u/Cool-Statistician880</name>
      <uri>https://old.reddit.com/user/Cool-Statistician880</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey LocalLLaMA fam! üëã I built a Python tool that scrapes Google's AI Mode directly without needing any API keys or paid services. Perfect for educational research and building datasets! **What it does:** - ü§ñ Extracts clean AI responses as paragraphs - üìä Automatically formats tables into beautiful ASCII - üé≠ Works in headless mode (no browser popup needed) - üì¶ Batch processing for multiple queries - üíæ Exports to JSON for fine-tuning datasets **Why this matters for local LLM users:** You can build comparison datasets, gather training examples, or create evaluation benchmarks - all without API costs. Great for educational purposes and research. **Tech:** Pure Python with Selenium + BeautifulSoup. No external APIs, no rate limits from paid services. GitHub: &lt;a href="https://github.com/Adwaith673/-Google-AI-Mode-Direct-Scraper"&gt;https://github.com/Adwaith673/-Google-AI-Mode-Direct-Scraper&lt;/a&gt; Built this for students and researchers. Would love your feedback! üöÄ **Note:** For educational use only - please respect rate limits and use responsibly.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cool-Statistician880"&gt; /u/Cool-Statistician880 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pai12j/direct_google_ai_mode_scraper_no_api_needed_pure/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pai12j/direct_google_ai_mode_scraper_no_api_needed_pure/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pai12j/direct_google_ai_mode_scraper_no_api_needed_pure/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-30T13:15:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1paiw6l</id>
    <title>$6k AMD AI Build (2x R9700, 64GB VRAM) - Worth it for a beginner learning fine-tuning vs. Cloud?</title>
    <updated>2025-11-30T13:57:16+00:00</updated>
    <author>
      <name>/u/buenavista62</name>
      <uri>https://old.reddit.com/user/buenavista62</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Planned Build:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GPU:&lt;/strong&gt; 2x AsRock Radeon AI PRO R9700 (2*32GB = 64GB Total VRAM)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;CPU:&lt;/strong&gt; AMD Ryzen 9 9950X&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Motherboard:&lt;/strong&gt; Gigabyte B850 AI TOP&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RAM:&lt;/strong&gt; Corsair 2x48GB DDR5 6000 MHz (96GB Total)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Storage:&lt;/strong&gt; Lexar NM990 2TB SSD (OS/Apps) + Lexar NM1090 Pro 4TB SSD (Models/Data)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;PSU:&lt;/strong&gt; be quiet! 1200 W&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cooling:&lt;/strong&gt; Corsair NAUTILUS 360 AIO&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;My Situation &amp;amp; Dilemma:&lt;/strong&gt;&lt;br /&gt; I'm a beginner to local LLMs. My goal is to learn/study fine-tuning concepts and tinker with diffusion models. I'm torn because:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Building:&lt;/strong&gt; It would be a cool experience, and I'd have a powerful local machine for experimentation.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cloud:&lt;/strong&gt; I could rent compute only when needed, potentially saving money upfront.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm aware of the current software disadvantages of ROCm compared to CUDA, but I'm betting on AMD's future improvements.&lt;/p&gt; &lt;p&gt;What would you do in my shoes? Is the hands-on learning experience worth the ~$6k investment, or would I be better off putting that money towards cloud credits? Do you see other advantages/disadvantages between these two options? I'm also open to alternative build suggestions at a similar or lower price point.&lt;/p&gt; &lt;p&gt;Any recommendations or shared experiences are highly appreciated!&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/buenavista62"&gt; /u/buenavista62 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1paiw6l/6k_amd_ai_build_2x_r9700_64gb_vram_worth_it_for_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1paiw6l/6k_amd_ai_build_2x_r9700_64gb_vram_worth_it_for_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1paiw6l/6k_amd_ai_build_2x_r9700_64gb_vram_worth_it_for_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-30T13:57:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1pa96zw</id>
    <title>GPT2 using MLX</title>
    <updated>2025-11-30T04:30:29+00:00</updated>
    <author>
      <name>/u/Disastrous-Maybe2501</name>
      <uri>https://old.reddit.com/user/Disastrous-Maybe2501</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pa96zw/gpt2_using_mlx/"&gt; &lt;img alt="GPT2 using MLX" src="https://external-preview.redd.it/wKilzTCRJSxOz93th9lQZAcudR38Z1MSjpizm9YSIDQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=11a8f4246ed9bcf7275e15c0baecfa2872439cc9" title="GPT2 using MLX" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, I was learning LLM pre-training from Andrej Karpathy's NanoGPT and decided to try it out using MLX. I originally thought it would be more or less a simple translation from PyTorch to MLX, but it turned out to be much more tricky than that. I published my code and documented my learnings in a blog post included in the repo. I'll kick off full training on fineweb on my M3 Max and will be publishing the training results to the repo once I have that. Any thoughts and feedback are welcome, here or directly on the repo. Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Disastrous-Maybe2501"&gt; /u/Disastrous-Maybe2501 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/yuchaoran2011/gpt2-mlx"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pa96zw/gpt2_using_mlx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pa96zw/gpt2_using_mlx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-30T04:30:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1pagchj</id>
    <title>re-did my quantization chart based on the feedback i got</title>
    <updated>2025-11-30T11:43:16+00:00</updated>
    <author>
      <name>/u/Even_Ganache6148</name>
      <uri>https://old.reddit.com/user/Even_Ganache6148</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pagchj/redid_my_quantization_chart_based_on_the_feedback/"&gt; &lt;img alt="re-did my quantization chart based on the feedback i got" src="https://b.thumbs.redditmedia.com/Fr8p_tY40Y7ySx8g5bQWyaAlzsB5c-nNKA3mF_7GqaI.jpg" title="re-did my quantization chart based on the feedback i got" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hey everyone so i posted a few days ago and got some really detailed responses that helped me understand this better. i realized i was looking at the data the wrong way so i went back and updated my analysis.&lt;/p&gt; &lt;p&gt;im still pretty new to this and running everything on a cheap laptop with 8gb ram so i really need to know what works before downloading huge files. i spent the weekend looking into the newer models like qwen 2.5 and gemma 3 to see where the actual limits are.&lt;/p&gt; &lt;p&gt;heres the updated chart i made. i tried to focus on exactly where they break because thats the part that was confusing me before.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7qeeoksxrd4g1.png?width=1392&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=44ab441139ccaacb7291109d7f9dafb6bb20ca10"&gt;https://preview.redd.it/7qeeoksxrd4g1.png?width=1392&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=44ab441139ccaacb7291109d7f9dafb6bb20ca10&lt;/a&gt;&lt;/p&gt; &lt;p&gt;i also made a quick ram reference table because i kept calculating this manually every time. hope this saves someone else the math(not sure if its accurate tho):&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/aug44hi2td4g1.png?width=759&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9468c04d725434eec7d8f6f5d32dd505e351aef9"&gt;https://preview.redd.it/aug44hi2td4g1.png?width=759&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9468c04d725434eec7d8f6f5d32dd505e351aef9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;what i figured out is that different tasks break at different levels. like math falls apart way faster than creative writing so you cant really use one rule for everything. also i noticed that the newer qat format for gemma is actually really good for keeping quality up on low ram.&lt;/p&gt; &lt;p&gt;basically im just realizing that being efficient with the right format matters way more than just trying to force big models to run.&lt;/p&gt; &lt;p&gt;anyway let me know if these numbers look right to you guys. thanks for pointing me in the right direction last time guys it helped a lot.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Even_Ganache6148"&gt; /u/Even_Ganache6148 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pagchj/redid_my_quantization_chart_based_on_the_feedback/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pagchj/redid_my_quantization_chart_based_on_the_feedback/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pagchj/redid_my_quantization_chart_based_on_the_feedback/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-30T11:43:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1pajgby</id>
    <title>How do you choose your open-source LLM without having to test them all?</title>
    <updated>2025-11-30T14:22:50+00:00</updated>
    <author>
      <name>/u/Holiday-Case-4524</name>
      <uri>https://old.reddit.com/user/Holiday-Case-4524</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;br /&gt; How do you usually decide which model (or specific version/quantization) performs best for your use case without having to test literally every single one? Any favorite heuristics, rules of thumb, or quick evaluation tricks you rely on?&lt;/p&gt; &lt;p&gt;We all know there are tons of options out there right now ‚Äî different quantizations (4-bit, 8-bit, AWQ, GGUF, etc.), reasoning/thinking variants, instruct-tuned models, base vs fine-tuned, and so on ‚Äî so trying them all manually is basically impossible.&lt;/p&gt; &lt;p&gt;Thanks in advance for any tips!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Holiday-Case-4524"&gt; /u/Holiday-Case-4524 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pajgby/how_do_you_choose_your_opensource_llm_without/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pajgby/how_do_you_choose_your_opensource_llm_without/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pajgby/how_do_you_choose_your_opensource_llm_without/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-30T14:22:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1p9zoiw</id>
    <title>NeKot - a terminal interface for interacting with local and cloud LLMs</title>
    <updated>2025-11-29T21:00:37+00:00</updated>
    <author>
      <name>/u/Balanceballs</name>
      <uri>https://old.reddit.com/user/Balanceballs</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9zoiw/nekot_a_terminal_interface_for_interacting_with/"&gt; &lt;img alt="NeKot - a terminal interface for interacting with local and cloud LLMs" src="https://external-preview.redd.it/emhzNGhndG5mOTRnMe6rR53dxe7TwZ8ZKOIc0FAxbevUKRyRaaXNJrhJXdM9.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1f0e1a15b0b50823bb62c7fb8a41de7b9614e1fe" title="NeKot - a terminal interface for interacting with local and cloud LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been working on this for a while, since I could not find a decent solution that is not abandoned or has all the features I need.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Supports Gemini, OpenAI and OpenRouter APIs as well as almost any local solution (tested with llama-cpp + llamaswap, ollama, lmstudio).&lt;/li&gt; &lt;li&gt;Has support for images, presets (each preset can have it's own settings and system prompt), sessions.&lt;/li&gt; &lt;li&gt;Written in GO , so no interpreter or runtime required.&lt;/li&gt; &lt;li&gt;Has support for basic vim motions.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/BalanceBalls/nekot"&gt;https://github.com/BalanceBalls/nekot&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balanceballs"&gt; /u/Balanceballs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/m66w35tnf94g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9zoiw/nekot_a_terminal_interface_for_interacting_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p9zoiw/nekot_a_terminal_interface_for_interacting_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-29T21:00:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1pagzo9</id>
    <title>To what degree to PCIe lanes x16 vs x4 or x1 matter in a multi-GPU setup for running LLMs?</title>
    <updated>2025-11-30T12:20:27+00:00</updated>
    <author>
      <name>/u/fabkosta</name>
      <uri>https://old.reddit.com/user/fabkosta</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Many mainboard offering multi-GPU setups only offer one primary PCIe slot with full x16 bandwidth, wheras the others are then at e.g. x4 or oftentimes only x1. Let's assume I'd have 1 Nvidia RTX 3090 at x16 and 3 others at x1, how does this realistically impact the processing speed of an LLM vs having all four on x16? Does anyone have real-life experience?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fabkosta"&gt; /u/fabkosta &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pagzo9/to_what_degree_to_pcie_lanes_x16_vs_x4_or_x1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pagzo9/to_what_degree_to_pcie_lanes_x16_vs_x4_or_x1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pagzo9/to_what_degree_to_pcie_lanes_x16_vs_x4_or_x1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-30T12:20:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1pakey8</id>
    <title>Users of Qwen3-Next-80B-A3B-Instruct-GGUF, How is Performance &amp; Benchmarks?</title>
    <updated>2025-11-30T15:04:47+00:00</updated>
    <author>
      <name>/u/pmttyji</name>
      <uri>https://old.reddit.com/user/pmttyji</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's been over a day we got GGUF. Please share your experience. Thanks&lt;/p&gt; &lt;p&gt;At first, I didn't believe that we could run this model just with 30GB RAM(Yes, RAM only) .... Unsloth posted a thread actually. Then someone shared a stat on that. &lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLM/comments/1p8xlnw/comment/nrcjh83/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button"&gt;17 t/s just with 32GB RAM + 10GB VRAM using Q4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Good for Poor GPU Club.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmttyji"&gt; /u/pmttyji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pakey8/users_of_qwen3next80ba3binstructgguf_how_is/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pakey8/users_of_qwen3next80ba3binstructgguf_how_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pakey8/users_of_qwen3next80ba3binstructgguf_how_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-30T15:04:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1pac8az</id>
    <title>Raw Chain-of-Thought from Gemini 3 Pro. It hallucinates, corrects itself, and eventually crashes.</title>
    <updated>2025-11-30T07:22:57+00:00</updated>
    <author>
      <name>/u/Numerous-Campaign844</name>
      <uri>https://old.reddit.com/user/Numerous-Campaign844</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pac8az/raw_chainofthought_from_gemini_3_pro_it/"&gt; &lt;img alt="Raw Chain-of-Thought from Gemini 3 Pro. It hallucinates, corrects itself, and eventually crashes." src="https://a.thumbs.redditmedia.com/CqXgFE1zvbugRVpZuwwvyeLBDo0qafjPsltbeHXoLc8.jpg" title="Raw Chain-of-Thought from Gemini 3 Pro. It hallucinates, corrects itself, and eventually crashes." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We know how Gemini Pro has the 'Thinking' block which shows &amp;quot;summary&amp;quot; of its reasoning process, but I somehow glitched it into outputting the raw internal monologue instead of the summary. It looks very similar to DeepSeek's R1 &lt;/p&gt; &lt;p&gt;So it happned when I was testing &lt;strong&gt;Gemini 3 Pro&lt;/strong&gt; on AI Studio with some heavy obfucsated JS. After it missed a hidden URL, I corrected it and asked why it failed.. That‚Äôs when it broke.&lt;/p&gt; &lt;p&gt;Instead of the usual 'Thinking' summary, it spit out its entire raw internal monologue reasoning that felt bizarrely human&lt;/p&gt; &lt;h1&gt;My Theory:&lt;/h1&gt; &lt;p&gt;I think I finally understand why gemini &lt;strong&gt;summarizes&lt;/strong&gt; the &amp;quot;Thinking&amp;quot; block instead of showing it raw. It‚Äôs not just for a cleaner UI. I think they hide it because if the model gets &amp;quot;stuck&amp;quot; or enters a recursive loop, it looks absolutely unhinged. There might be a failsafe mechanism designed to 'reset' or sanitize the thought process when it enters a repetitive state like this, but I somehow bypassed it.&lt;/p&gt; &lt;p&gt;&lt;a href="https://aistudio.google.com/app/prompts/1425A2hRIMe1F5fDvi5ltEYZWwDvrGGqL"&gt;Full Chat URL&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Honestly, the fact that it admitted 'I will accept the L' in its internal monologue is the most human thing I've seen from an AI&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Numerous-Campaign844"&gt; /u/Numerous-Campaign844 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pac8az"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pac8az/raw_chainofthought_from_gemini_3_pro_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pac8az/raw_chainofthought_from_gemini_3_pro_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-30T07:22:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1pa7kbf</id>
    <title>A 4B Model That Outperforms 32B on GUI Tasks, Fully Open-Source</title>
    <updated>2025-11-30T03:05:29+00:00</updated>
    <author>
      <name>/u/Successful-Bill-5543</name>
      <uri>https://old.reddit.com/user/Successful-Bill-5543</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pa7kbf/a_4b_model_that_outperforms_32b_on_gui_tasks/"&gt; &lt;img alt="A 4B Model That Outperforms 32B on GUI Tasks, Fully Open-Source" src="https://external-preview.redd.it/pBg1Y9QQ3lFHZujHbUtXu5G8o5YMGOIQg4ARl6TwaGg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=928a54aa59f7d1522bd15a28beb5c837cb046bee" title="A 4B Model That Outperforms 32B on GUI Tasks, Fully Open-Source" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It includes &lt;/p&gt; &lt;ol&gt; &lt;li&gt;4B GUI Agent model capable of running on local computers.&lt;/li&gt; &lt;li&gt;Plug-and-play inference infrastructure that handles ADB connections, dependency installation, and task recording/replay&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Successful-Bill-5543"&gt; /u/Successful-Bill-5543 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/stepfun-ai/GELab-Zero-4B-preview"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pa7kbf/a_4b_model_that_outperforms_32b_on_gui_tasks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pa7kbf/a_4b_model_that_outperforms_32b_on_gui_tasks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-30T03:05:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1pa3ok3</id>
    <title>TOON is terrible, so I invented a new format (TRON) to prove a point</title>
    <updated>2025-11-29T23:57:33+00:00</updated>
    <author>
      <name>/u/No-Olive342</name>
      <uri>https://old.reddit.com/user/No-Olive342</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pa3ok3/toon_is_terrible_so_i_invented_a_new_format_tron/"&gt; &lt;img alt="TOON is terrible, so I invented a new format (TRON) to prove a point" src="https://preview.redd.it/365yh1sr9a4g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c661136be4f4a8e7de715c1a745a14480ae744fb" title="TOON is terrible, so I invented a new format (TRON) to prove a point" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There's been a lot of noise around TOON lately. This so-called &amp;quot;Token oriented&amp;quot; object notation is only useful when serializing an array of unnested objects. But lets face it, most practical use cases involve nested objects - a structure that almost always makes TOON less token efficient than JSON. Just look at the response payload for &lt;a href="https://gist.github.com/didier-durand/2970be82fec6c84d522f7953ac7881b4"&gt;listing MCP tools for GitHub&lt;/a&gt; for instance.&lt;/p&gt; &lt;p&gt;I've noticed that most people posting about TOON are comparing its token count with indented JSON. That's CHEATING. If you're going to compare token count, you gotta compare with compressed JSON.&lt;/p&gt; &lt;p&gt;However, I do admit that there is some token inefficiencies with (compressed) JSON such as the repeating property names for common object structures. However, I didn't want to complain about TOON without providing my own suggestion. So as an experiment, I came up with my own data format called TRON (Token Reduced Object Notation).&lt;/p&gt; &lt;p&gt;Specifications: &lt;a href="https://tron-format.github.io/"&gt;https://tron-format.github.io/&lt;/a&gt;&lt;br /&gt; Playground: &lt;a href="https://tron-format.github.io/#/playground"&gt;https://tron-format.github.io/#/playground&lt;/a&gt;&lt;br /&gt; JavaScript SDK: &lt;a href="https://github.com/tron-format/tron-javascript"&gt;https://github.com/tron-format/tron-javascript&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Feel free to check out the Playground to try out TRON on your data. For now, I am not advocating this to be a standard. Just wanted to prove a point that if we really wanted to go down the route of having a token-efficient data format, we can do better than TOON.&lt;/p&gt; &lt;p&gt;(P.S. I already spent more time than I'd like coming up with this format and creating the website and JavaScript SDK. Maybe this catches on, maybe not. But for now, unless there is passion in the community to push this forward, I will refrain from spending additional time on this)&lt;/p&gt; &lt;p&gt;Edit: TRON is essentially a superset of JSON but with OOP features. Encoders can choose to serialize objects with class instantiation instead of pure JSON when there is more than one occurence (and more than one property) of an object structure. This almost always results in less tokens than pure JSON, while keeping the structure easy to understand by LLMs. Based on the comments, I assume most people aren't opening the specs link to figure this out.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Olive342"&gt; /u/No-Olive342 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/365yh1sr9a4g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pa3ok3/toon_is_terrible_so_i_invented_a_new_format_tron/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pa3ok3/toon_is_terrible_so_i_invented_a_new_format_tron/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-29T23:57:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1pa7b0w</id>
    <title>ArliAI/gpt-oss-120b-Derestricted ¬∑ Hugging Face</title>
    <updated>2025-11-30T02:53:10+00:00</updated>
    <author>
      <name>/u/Arli_AI</name>
      <uri>https://old.reddit.com/user/Arli_AI</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pa7b0w/arliaigptoss120bderestricted_hugging_face/"&gt; &lt;img alt="ArliAI/gpt-oss-120b-Derestricted ¬∑ Hugging Face" src="https://external-preview.redd.it/8n5MhbkzXEcl9NlvZMbb8GGre-k1VjQ0kDAKe7qQtQM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9334318d3d29cfd953050dfdf981bc10db9cc00b" title="ArliAI/gpt-oss-120b-Derestricted ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Previous posts and discussion about the Norm-Preserving Biprojected method of abliteration being used:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/user/Arli_AI/comments/1p5exem/the_most_objectively_correct_way_to_abliterate_so/"&gt;https://www.reddit.com/user/Arli_AI/comments/1p5exem/the_most_objectively_correct_way_to_abliterate_so/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1p81k2z/yes_it_is_possible_to_uncensor_gptoss20b/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1p81k2z/yes_it_is_possible_to_uncensor_gptoss20b/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Another highly requested model for &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;! No we do not have this on our API service, sorry. But we release models for everyone anyways and we are working on more models to Derestrict!&lt;/p&gt; &lt;p&gt;Quants by &lt;a href="https://huggingface.co/mradermacher"&gt;mradermacher&lt;/a&gt;:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mradermacher/gpt-oss-120b-Derestricted-GGUF"&gt;https://huggingface.co/mradermacher/gpt-oss-120b-Derestricted-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mradermacher/gpt-oss-120b-Derestricted-i1-GGUF"&gt;https://huggingface.co/mradermacher/gpt-oss-120b-Derestricted-i1-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Arli_AI"&gt; /u/Arli_AI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/ArliAI/gpt-oss-120b-Derestricted"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pa7b0w/arliaigptoss120bderestricted_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pa7b0w/arliaigptoss120bderestricted_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-30T02:53:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1paj4m8</id>
    <title>Trained a chess LLM locally that beats GPT-5 (technically)</title>
    <updated>2025-11-30T14:07:48+00:00</updated>
    <author>
      <name>/u/KingGongzilla</name>
      <uri>https://old.reddit.com/user/KingGongzilla</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;Over the past week I worked on a project training an LLM from scratch to play chess. The result is a language model that can play chess and generates legal moves almost 100% of the time completing about 96% of games without any illegal moves. For comparison, GPT-5 produces illegal moves in every game I tested, usually within 6-10 moves.&lt;/p&gt; &lt;p&gt;I‚Äôve trained two versions so far:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/daavidhauser/chess-bot-3000-100m"&gt;https://huggingface.co/daavidhauser/chess-bot-3000-100m&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/daavidhauser/chess-bot-3000-250m"&gt;https://huggingface.co/daavidhauser/chess-bot-3000-250m&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The models can occasionally beat Stockfish at ELO levels between 1500-2500, though I‚Äôm still running more evaluations and will update the results as I go.&lt;/p&gt; &lt;p&gt;If you want to try training yourself or build on it this is the Github repo for training: &lt;a href="https://github.com/kinggongzilla/chess-bot-3000"&gt;https://github.com/kinggongzilla/chess-bot-3000&lt;/a&gt;&lt;/p&gt; &lt;p&gt;vRAM requirements for training locally are ~12GB and ~22GB for the 100m and 250m model respectively. So this can definitely be done on an RTX 3090 or similar.&lt;/p&gt; &lt;p&gt;Full disclosure: the only reason it ‚Äúbeats‚Äù GPT-5 is because GPT-5 keeps making illegal moves. Still, it‚Äôs been a fun experiment in training a specialized LLM locally, and there are definitely a lot of things one could do to improve the model further. Better data curation etc etc..&lt;/p&gt; &lt;p&gt;Let me know if you try it out or have any feedback!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KingGongzilla"&gt; /u/KingGongzilla &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1paj4m8/trained_a_chess_llm_locally_that_beats_gpt5/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1paj4m8/trained_a_chess_llm_locally_that_beats_gpt5/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1paj4m8/trained_a_chess_llm_locally_that_beats_gpt5/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-30T14:07:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1pagx76</id>
    <title>Optimizing Token Generation in llama.cpp's CUDA Backend</title>
    <updated>2025-11-30T12:16:35+00:00</updated>
    <author>
      <name>/u/am17an</name>
      <uri>https://old.reddit.com/user/am17an</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Link to the post: &lt;a href="https://github.com/ggml-org/llama.cpp/discussions/17621"&gt;https://github.com/ggml-org/llama.cpp/discussions/17621&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We've been working over the last few months on kernel fusion in llama.cpp, I wrote a small write-up, it's semi-technical but one of the things I wanted to raise awareness is about if you're on a single GPU you can use GGML_CUDA_GRAPH_OPT=1 to run things slightly faster :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/am17an"&gt; /u/am17an &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pagx76/optimizing_token_generation_in_llamacpps_cuda/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pagx76/optimizing_token_generation_in_llamacpps_cuda/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pagx76/optimizing_token_generation_in_llamacpps_cuda/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-30T12:16:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1pa85la</id>
    <title>Any idea when RAM prices will be ‚Äúnormal‚Äùagain?</title>
    <updated>2025-11-30T03:36:02+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pa85la/any_idea_when_ram_prices_will_be_normalagain/"&gt; &lt;img alt="Any idea when RAM prices will be ‚Äúnormal‚Äùagain?" src="https://preview.redd.it/uz2nfcieeb4g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c78543ae9c70a1017d7527e56d45bde64aef7586" title="Any idea when RAM prices will be ‚Äúnormal‚Äùagain?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is it the datacenter buildouts driving prices up? WTF? DDR4 and DDR5 prices are kinda insane right now (compared to like a couple months ago). &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/uz2nfcieeb4g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pa85la/any_idea_when_ram_prices_will_be_normalagain/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pa85la/any_idea_when_ram_prices_will_be_normalagain/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-30T03:36:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5retd</id>
    <title>Best Local VLMs - November 2025</title>
    <updated>2025-11-24T20:00:04+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Share what your favorite models are right now and &lt;strong&gt;&lt;em&gt;why&lt;/em&gt;&lt;/strong&gt;. Given the nature of the beast in evaluating VLMs (untrustworthiness of benchmarks, immature tooling, intrinsic stochasticity), please be as detailed as possible in describing your setup, nature of your usage (what applications, how much, personal/professional use), tools/frameworks/prompts etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5retd/best_local_vlms_november_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5retd/best_local_vlms_november_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p5retd/best_local_vlms_november_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T20:00:04+00:00</published>
  </entry>
</feed>
