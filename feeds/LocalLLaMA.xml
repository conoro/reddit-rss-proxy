<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-01T17:23:49+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1nv7npc</id>
    <title>Want to get started with training LLMs for theorem proving (with 500-1000 USD budget), so what are my options?</title>
    <updated>2025-10-01T13:05:03+00:00</updated>
    <author>
      <name>/u/hedgehog0</name>
      <uri>https://old.reddit.com/user/hedgehog0</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I recently graduated from a Master program in math at a German University. As I am always interested in AI4Math and formal theorem proving (like Coq and Lean), I want to explore and get hands-on experience with training and applying LLMs to formal math. However, I have a rather limited budget, e.g., around 500 to 1000 USD.&lt;/p&gt; &lt;p&gt;After reading &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nuq4tr/spent_3k_building_the_open_source_models_you/"&gt;this 3k post&lt;/a&gt;, I realized that it may be possible to train some prover/math LLMs by myself, so I was wondering what are my options?&lt;/p&gt; &lt;p&gt;More specifically, I have the following questions:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;How many and what size models could I reasonably train or fine-tune for theorem proving tasks (e.g. Lean and/or Coq)?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Would fine-tuning existing open models (e.g. LLaMA, Mistral, Qwen, etc.) on theorem-proving data count as ‚Äútraining‚Äù? Or do I need to attempt training something from scratch?&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Basically, I‚Äôm looking for the best path to get meaningful hands-on experience in this area without breaking the bank. Any recommendations from people who‚Äôve done fine-tuning or small-scale training for formal math would be super helpful!&lt;/p&gt; &lt;p&gt;Many thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hedgehog0"&gt; /u/hedgehog0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nv7npc/want_to_get_started_with_training_llms_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nv7npc/want_to_get_started_with_training_llms_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nv7npc/want_to_get_started_with_training_llms_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-01T13:05:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1nuerql</id>
    <title>zai-org/GLM-4.6 ¬∑ Hugging Face</title>
    <updated>2025-09-30T14:31:09+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nuerql/zaiorgglm46_hugging_face/"&gt; &lt;img alt="zai-org/GLM-4.6 ¬∑ Hugging Face" src="https://external-preview.redd.it/PGKpaG-61JC7z-y_F2XkhwKzdcpyb99tvV79_JhB320.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3b34f4363d1d490762c5a458490a60b87ed1e125" title="zai-org/GLM-4.6 ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Model Introduction&lt;/h1&gt; &lt;p&gt;Compared with GLM-4.5, &lt;strong&gt;GLM-4.6&lt;/strong&gt; brings several key improvements:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Longer context window:&lt;/strong&gt; The context window has been expanded from 128K to 200K tokens, enabling the model to handle more complex agentic tasks.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Superior coding performance:&lt;/strong&gt; The model achieves higher scores on code benchmarks and demonstrates better real-world performance in applications such as Claude Code„ÄÅCline„ÄÅRoo Code and Kilo Code, including improvements in generating visually polished front-end pages.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Advanced reasoning:&lt;/strong&gt; GLM-4.6 shows a clear improvement in reasoning performance and supports tool use during inference, leading to stronger overall capability.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;More capable agents:&lt;/strong&gt; GLM-4.6 exhibits stronger performance in tool using and search-based agents, and integrates more effectively within agent frameworks.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Refined writing:&lt;/strong&gt; Better aligns with human preferences in style and readability, and performs more naturally in role-playing scenarios.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We evaluated GLM-4.6 across eight public benchmarks covering agents, reasoning, and coding. Results show clear gains over GLM-4.5, with GLM-4.6 also holding competitive advantages over leading domestic and international models such as &lt;strong&gt;DeepSeek-V3.1-Terminus&lt;/strong&gt; and &lt;strong&gt;Claude Sonnet 4&lt;/strong&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/zai-org/GLM-4.6"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nuerql/zaiorgglm46_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nuerql/zaiorgglm46_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-30T14:31:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1nutjlm</id>
    <title>Qwen3-Next-80B-GGUF, Any Update?</title>
    <updated>2025-10-01T00:09:23+00:00</updated>
    <author>
      <name>/u/Iory1998</name>
      <uri>https://old.reddit.com/user/Iory1998</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;I am wondering what's the update on this model's support in llama.cpp?&lt;/p&gt; &lt;p&gt;Does anyone of you have any idea?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Iory1998"&gt; /u/Iory1998 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nutjlm/qwen3next80bgguf_any_update/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nutjlm/qwen3next80bgguf_any_update/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nutjlm/qwen3next80bgguf_any_update/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-01T00:09:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1nv6msf</id>
    <title>Can anyone help me understand the difference between GLM 4.6 and GLM 4.5? Shall I switch to the new model? Anyone tried both the models side by side</title>
    <updated>2025-10-01T12:19:01+00:00</updated>
    <author>
      <name>/u/Technical-Love-8479</name>
      <uri>https://old.reddit.com/user/Technical-Love-8479</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So &lt;a href="http://Z.ai"&gt;Z.ai&lt;/a&gt; has launched GLM 4.6 yesterday. I have been Using GLM 4.5 constantly for a while now, and quite comfortable with the model. But given the benchmarks today, GLM 4.6 definitely looks a great upgrade over GLM 4.5. But is the model actually good? Has anyone used them side-by-side? And can compare whether I should switch from GLM 4.5 to GLM 4.6? This will require a few prompt tunings as well on my end in my pipeline.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Technical-Love-8479"&gt; /u/Technical-Love-8479 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nv6msf/can_anyone_help_me_understand_the_difference/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nv6msf/can_anyone_help_me_understand_the_difference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nv6msf/can_anyone_help_me_understand_the_difference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-01T12:19:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1nufu17</id>
    <title>AMD tested 20+ local models for coding &amp; only 2 actually work (testing linked)</title>
    <updated>2025-09-30T15:11:54+00:00</updated>
    <author>
      <name>/u/nick-baumann</name>
      <uri>https://old.reddit.com/user/nick-baumann</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nufu17/amd_tested_20_local_models_for_coding_only_2/"&gt; &lt;img alt="AMD tested 20+ local models for coding &amp;amp; only 2 actually work (testing linked)" src="https://external-preview.redd.it/eXRyOW5lM2JpYnNmMRoy4uRFePoUQZKQzw3MqAlRHs-miZIp3JL6ldgQ6nGR.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=121a931aab3efc98ee57ab2e3519bf172950e05f" title="AMD tested 20+ local models for coding &amp;amp; only 2 actually work (testing linked)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;tldr&lt;/strong&gt;; qwen3-coder (4-bit, 8-bit) is really the only viable local model for coding, if you have 128gb+ of RAM, check out GLM-4.5-air (8-bit)&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;hello hello!&lt;/p&gt; &lt;p&gt;So AMD just dropped their comprehensive testing of local models for AI coding and it pretty much validates what I've been preaching about local models&lt;/p&gt; &lt;p&gt;They tested 20+ models and found exactly what many of us suspected: most of them completely fail at actual coding tasks. Out of everything they tested, only three models consistently worked: Qwen3-Coder 30B, GLM-4.5-Air for those with beefy rigs. Magistral Small is worth an honorable mention in my books.&lt;/p&gt; &lt;p&gt;deepseek/deepseek-r1-0528-qwen3-8b, smaller Llama models, GPT-OSS-20B, Seed-OSS-36B (bytedance) all produce broken outputs or can't handle tool use properly. This isn't a knock on the models themselves, they're just not built for the complex tool-calling that coding agents need.&lt;/p&gt; &lt;p&gt;What's interesting is their RAM findings match exactly what I've been seeing. For 32gb machines, Qwen3-Coder 30B at 4-bit is basically your only option, but an extremely viable one at that.&lt;/p&gt; &lt;p&gt;For those with 64gb RAM, you can run the same model at 8-bit quantization. And if you've got 128gb+, GLM-4.5-Air is apparently incredible (this is AMD's #1)&lt;/p&gt; &lt;p&gt;AMD used Cline &amp;amp; LM Studio for all their testing, which is how they validated these specific configurations. Cline is pretty demanding in terms of tool-calling and context management, so if a model works with Cline, it'll work with pretty much anything.&lt;/p&gt; &lt;p&gt;AMD's blog: &lt;a href="https://www.amd.com/en/blogs/2025/how-to-vibe-coding-locally-with-amd-ryzen-ai-and-radeon.html"&gt;https://www.amd.com/en/blogs/2025/how-to-vibe-coding-locally-with-amd-ryzen-ai-and-radeon.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;setup instructions for coding w/ local models: &lt;a href="https://cline.bot/blog/local-models-amd"&gt;https://cline.bot/blog/local-models-amd&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nick-baumann"&gt; /u/nick-baumann &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/fmg3qe3bibsf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nufu17/amd_tested_20_local_models_for_coding_only_2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nufu17/amd_tested_20_local_models_for_coding_only_2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-30T15:11:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1nuq4tr</id>
    <title>‚ùåSpent ~$3K building the open source models you asked for. Need to abort Art-1-20B and shut down AGI-0. Ideas?‚ùå</title>
    <updated>2025-09-30T21:40:33+00:00</updated>
    <author>
      <name>/u/GuiltyBookkeeper4849</name>
      <uri>https://old.reddit.com/user/GuiltyBookkeeper4849</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Quick update on AGI-0 Labs. Not great news.&lt;/p&gt; &lt;p&gt;A while back I &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nbqa34/what_model_should_we_build_next_you_decide/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button"&gt;posted asking what model you wanted next&lt;/a&gt;. The response was awesome - you voted, gave ideas, and I started building. Art-1-8B is nearly done, and I was working on Art-1-20B plus the community-voted model .&lt;/p&gt; &lt;p&gt;Problem: I've burned through almost $3K of my own money on compute. I'm basically tapped out.&lt;/p&gt; &lt;p&gt;Art-1-8B I can probably finish. Art-1-20B and the community model? Can't afford to complete them. And I definitely can't keep doing this.&lt;/p&gt; &lt;p&gt;So I'm at a decision point: either figure out how to make this financially viable, or just shut it down and move on. I'm not interested in half-doing this as a occasional hobby project.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;I've thought about a few options:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Paid community - early access, vote on models, co-author credits, shared compute pool&lt;/li&gt; &lt;li&gt;Finding sponsors for model releases - logo and website link on the model card, still fully open source&lt;/li&gt; &lt;li&gt;Custom model training / consulting - offering services for a fee&lt;/li&gt; &lt;li&gt;Just donations (Already possible at &lt;a href="https://agi-0.com/donate"&gt;https://agi-0.com/donate&lt;/a&gt; )&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;But honestly? I don't know what makes sense or what anyone would actually pay for.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;So I'm asking: if you want AGI-0 to keep releasing open source models, what's the path here?&lt;/strong&gt; What would you actually support? Is there an obvious funding model I'm missing?&lt;/p&gt; &lt;p&gt;Or should I just accept this isn't sustainable and shut it down?&lt;/p&gt; &lt;p&gt;Not trying to guilt anyone - genuinely asking for ideas. If there's a clear answer in the comments I'll pursue it. If not, I'll wrap up Art-1-8B and call it.&lt;/p&gt; &lt;p&gt;Let me know what you think.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GuiltyBookkeeper4849"&gt; /u/GuiltyBookkeeper4849 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nuq4tr/spent_3k_building_the_open_source_models_you/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nuq4tr/spent_3k_building_the_open_source_models_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nuq4tr/spent_3k_building_the_open_source_models_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-30T21:40:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvbu3h</id>
    <title>So has anyone actually tried Apriel-v1.5-15B?</title>
    <updated>2025-10-01T15:47:01+00:00</updated>
    <author>
      <name>/u/MKU64</name>
      <uri>https://old.reddit.com/user/MKU64</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It‚Äôs obvious it isn‚Äôt on R1‚Äôs level. But honestly, if we get a model that performs insanely well on 15B then it truly is something for this community. The benchmarks of Artificial Intelligence Index focuses a lot recently in tool calling and instruction following so having a very reliable one is a plus.&lt;/p&gt; &lt;p&gt;Can‚Äôt personally do this because I don‚Äôt have 16GB :( &lt;/p&gt; &lt;p&gt;UPDATE: Have tried it in the HuggingFace Space. That reasoning is really fantastic for small models, it basically begins brainstorming topics so that it can then start mixing them together to answer the query. And it does give really great answers (but it thinks a lot of course, that‚Äôs the only outcome with how big that is). I like it a lot.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MKU64"&gt; /u/MKU64 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvbu3h/so_has_anyone_actually_tried_aprielv1515b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvbu3h/so_has_anyone_actually_tried_aprielv1515b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvbu3h/so_has_anyone_actually_tried_aprielv1515b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-01T15:47:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvdyiy</id>
    <title>NVIDIA DGX Spark expected to become available in October 2025</title>
    <updated>2025-10-01T17:05:00+00:00</updated>
    <author>
      <name>/u/Excellent_Produce146</name>
      <uri>https://old.reddit.com/user/Excellent_Produce146</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It looks like we will finally get to know how well or badly the NVIDIA GB10 performs in October (2025!) or November depending on the shipping times.&lt;/p&gt; &lt;p&gt;In the &lt;a href="https://forums.developer.nvidia.com/t/dgx-spark-release-updates/341703/90"&gt;NVIDIA developer forum&lt;/a&gt; this article was posted:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.ctee.com.tw/news/20250930700082-430502"&gt;https://www.ctee.com.tw/news/20250930700082-430502&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;em&gt;GB10 new products to be launched in October... Taiwan's four major PC brand manufacturers see praise in Q4&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;[..] In addition to NVIDIA's public version product delivery schedule waiting for NVIDIA's final decision, the GB10 products of Taiwanese manufacturers ASUS, Gigabyte, MSI, and Acer are all expected to be officially shipped in October. Among them, ASUS, which has already opened a wave of pre-orders in the previous quarter, is rumored to have obtained at least 18,000 sets of GB10 configurations in the first batch, while Gigabyte has about 15,000 sets, and MSI also has a configuration scale of up to 10,000 sets. It is estimated that including the supply on hand from Acer, the four major Taiwanese manufacturers will account for about 70% of the available supply of GB10 in the first wave. [..]&lt;/em&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;(translated with Google Gemini as Chinese is still on my list of languages to learn...)&lt;/p&gt; &lt;p&gt;Looking forward to the first reports/benchmarks. üßê&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Excellent_Produce146"&gt; /u/Excellent_Produce146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvdyiy/nvidia_dgx_spark_expected_to_become_available_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvdyiy/nvidia_dgx_spark_expected_to_become_available_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvdyiy/nvidia_dgx_spark_expected_to_become_available_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-01T17:05:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvb8d9</id>
    <title>Tutorial: Matrix Core Programming on AMD CDNA3 and CDNA4 architecture</title>
    <updated>2025-10-01T15:24:46+00:00</updated>
    <author>
      <name>/u/salykova_</name>
      <uri>https://old.reddit.com/user/salykova_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvb8d9/tutorial_matrix_core_programming_on_amd_cdna3_and/"&gt; &lt;img alt="Tutorial: Matrix Core Programming on AMD CDNA3 and CDNA4 architecture" src="https://preview.redd.it/up8q1u00qisf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=39781e64f363eb4ec35ac98f64aae0c9bacdcd4d" title="Tutorial: Matrix Core Programming on AMD CDNA3 and CDNA4 architecture" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;I'm excited to announce my new tutorial on programming Matrix Cores in HIP. The blog post is very educational and contains necessary knowledge to start programming Matrix Cores, covering modern low-precision floating-point types, the Matrix Core compiler intrinsics, and the data layouts required by the Matrix Core instructions. I tried to make the tutorial easy to follow and, as always, included lots of code examples and illustrations. I hope you will enjoy it!&lt;/p&gt; &lt;p&gt;I plan to publish in-depth technical tutorials on kernel programming in HIP and inference optimization for RDNA and CDNA architecture. Please let me know if there are any other technical ROCm/HIP-related topics you would like to hear more about!&lt;/p&gt; &lt;p&gt;Link: &lt;a href="https://salykova.github.io/matrix-cores-cdna"&gt;https://salykova.github.io/matrix-cores-cdna&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/salykova_"&gt; /u/salykova_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/up8q1u00qisf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvb8d9/tutorial_matrix_core_programming_on_amd_cdna3_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvb8d9/tutorial_matrix_core_programming_on_amd_cdna3_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-01T15:24:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvc4ad</id>
    <title>Eclaire ‚Äì Open-source, privacy-focused AI assistant for your data</title>
    <updated>2025-10-01T15:57:40+00:00</updated>
    <author>
      <name>/u/dorali8</name>
      <uri>https://old.reddit.com/user/dorali8</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvc4ad/eclaire_opensource_privacyfocused_ai_assistant/"&gt; &lt;img alt="Eclaire ‚Äì Open-source, privacy-focused AI assistant for your data" src="https://external-preview.redd.it/7p206NDe120lxhrc6n4JVw5doWuQzfnDXGAVCvWvfRk.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9afd82e1b3769f8671a17e5be4476f289edccc48" title="Eclaire ‚Äì Open-source, privacy-focused AI assistant for your data" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1nvc4ad/video/q423v4jovisf1/player"&gt;https://reddit.com/link/1nvc4ad/video/q423v4jovisf1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hi all, this is a project I've been working on for some time. It started as a personal AI to help manage growing amounts of data - bookmarks, photos, documents, notes, etc. All in one place.&lt;/p&gt; &lt;p&gt;Once the data gets added to the system, it gets processed including fetching bookmarks, tagging, classification, image analysis, text extraction / ocr, and more. And then the AI is able to work with those assets to perform search, answer questions, create new items, etc. You can also create scheduled / recurring tasks to assing to the AI.&lt;/p&gt; &lt;p&gt;Using llama.cpp with Qweb3-14b by default for the assistant backend and Gemma3-4b for workers multimodal processing. You can easily swap to other models.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Demo: &lt;a href="https://eclaire.co/#demo"&gt;https://eclaire.co/#demo&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Code: &lt;a href="https://github.com/eclaire-labs/eclaire"&gt;https://github.com/eclaire-labs/eclaire&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;MIT Licensed. Feedback and contributions welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dorali8"&gt; /u/dorali8 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvc4ad/eclaire_opensource_privacyfocused_ai_assistant/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvc4ad/eclaire_opensource_privacyfocused_ai_assistant/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvc4ad/eclaire_opensource_privacyfocused_ai_assistant/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-01T15:57:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvdrig</id>
    <title>-ERNIE-4.5-21B-A3B-Thinking ‚Äî impressions after some testing</title>
    <updated>2025-10-01T16:58:06+00:00</updated>
    <author>
      <name>/u/locaf</name>
      <uri>https://old.reddit.com/user/locaf</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been playing around with ERNIE-4.5-21B-A3B-Thinking for a bit and figured I‚Äôd drop my thoughts. This is Baidu‚Äôs ‚Äúthinking‚Äù model for logic, math, science, and coding.&lt;/p&gt; &lt;p&gt;What stood out to me:&lt;/p&gt; &lt;p&gt;Long context works: 128K token window actually does what it promises. I‚Äôve loaded multi-page papers and notes, and it keeps things coherent better than most open models I‚Äôve tried.&lt;/p&gt; &lt;p&gt;Math &amp;amp; code: Handles multi-step problems pretty solidly. Small scripts work fine; bigger coding tasks, I‚Äôd still pick Qwen. Surprised by how little it hallucinates on structured problems.&lt;/p&gt; &lt;p&gt;Performance: 21B params total, ~3B active thanks to MoE. Feels smoother than you‚Äôd expect for a model this size.&lt;/p&gt; &lt;p&gt;Reasoning style: Focused and doesn‚Äôt ramble unnecessarily. Good at staying on track.&lt;/p&gt; &lt;p&gt;Text output: Polished enough that it works well for drafting, summaries, or light creative writing.&lt;/p&gt; &lt;p&gt;Best use cases: Really strong for reasoning and analysis. Weaker if you‚Äôre pushing it into larger coding projects or very complex/nuanced creative writing. So far, it‚Äôs been useful for checking reasoning steps, parsing documents, or running experiments where I need something to actually ‚Äúthink through‚Äù a problem instead of shortcutting.&lt;/p&gt; &lt;p&gt;Curious - anyone else using it for long docs, planning tasks, or multi-step problem solving? What‚Äôs been working for you?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/locaf"&gt; /u/locaf &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvdrig/ernie4521ba3bthinking_impressions_after_some/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvdrig/ernie4521ba3bthinking_impressions_after_some/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvdrig/ernie4521ba3bthinking_impressions_after_some/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-01T16:58:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvbiqa</id>
    <title>Looking for contributors to PipesHub (open-source platform for AI Agents)</title>
    <updated>2025-10-01T15:35:18+00:00</updated>
    <author>
      <name>/u/Effective-Ad2060</name>
      <uri>https://old.reddit.com/user/Effective-Ad2060</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Teams across the globe are building AI Agents. AI Agents need context and tools to work well.&lt;br /&gt; We‚Äôve been building &lt;strong&gt;PipesHub&lt;/strong&gt;, an open-source developer platform for AI Agents that need real enterprise context scattered across multiple business apps. Think of it like the open-source alternative to Glean but designed for developers, not just big companies.&lt;/p&gt; &lt;p&gt;Right now, the project is growing fast (crossed 1,000+ GitHub stars in just a few months) and we‚Äôd love more contributors to join us.&lt;/p&gt; &lt;p&gt;We support almost all major native Embedding and Chat Generator models and OpenAI compatible endpoints. Users can connect to Google Drive, Gmail, Onedrive, Sharepoint Online, Confluence, Jira and more.&lt;/p&gt; &lt;p&gt;Some cool things you can help with:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Improve support for Local Inferencing - Ollama, vLLM, LM Studio, oLLM&lt;/li&gt; &lt;li&gt;Improving our RAG pipeline with more robust Knowledge Graphs and filters&lt;/li&gt; &lt;li&gt;Providing tools to Agents like Web search, Image Generator, CSV, Excel, Docx, PPTX, Coding Sandbox, etc&lt;/li&gt; &lt;li&gt;Universal MCP Server&lt;/li&gt; &lt;li&gt;Adding Memory, Guardrails to Agents&lt;/li&gt; &lt;li&gt;Improving REST APIs&lt;/li&gt; &lt;li&gt;SDKs for python, typescript, other programming languages&lt;/li&gt; &lt;li&gt;Docs, examples, and community support for new devs&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We‚Äôre trying to make it super easy for devs to spin up AI pipelines that actually work in production, with trust and explainability baked in.&lt;/p&gt; &lt;p&gt;üëâ Repo: &lt;a href="https://github.com/pipeshub-ai/pipeshub-ai"&gt;https://github.com/pipeshub-ai/pipeshub-ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You can join our Discord group for more details or pick items from GitHub issues list.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Effective-Ad2060"&gt; /u/Effective-Ad2060 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvbiqa/looking_for_contributors_to_pipeshub_opensource/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvbiqa/looking_for_contributors_to_pipeshub_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvbiqa/looking_for_contributors_to_pipeshub_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-01T15:35:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1nuyjp9</id>
    <title>LiquidAI bet on small but mighty model LFM2-1.2B-Tool/RAG/Extract</title>
    <updated>2025-10-01T04:13:12+00:00</updated>
    <author>
      <name>/u/dheetoo</name>
      <uri>https://old.reddit.com/user/dheetoo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So LiquidAI just announced their fine-tuned LFM models with different variants - Tool, RAG, and Extract. Each one's built for specific tasks instead of trying to do everything.&lt;/p&gt; &lt;p&gt;This lines up perfectly with that Nvidia whitepaper about how small specialized models are the future of agentic AI. Looks like it's actually happening now.&lt;/p&gt; &lt;p&gt;I'm planning to swap out parts of my current agentic workflow to test these out. Right now I'm running Qwen3-4B for background tasks and Qwen3-235B for answer generation. Gonna try replacing the background task layer with these LFM models since my main use cases are extraction and RAG.&lt;/p&gt; &lt;p&gt;Will report back with results once I've tested them out.&lt;/p&gt; &lt;p&gt;Update:&lt;br /&gt; Cant get it to work with my flow, it messing system prompt few-shot example with user query (that bad). I guess it work great for simple zero shot info extraction, like crafting search query from user text something like that. Gotta create some example to determine it use-cases&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dheetoo"&gt; /u/dheetoo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nuyjp9/liquidai_bet_on_small_but_mighty_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nuyjp9/liquidai_bet_on_small_but_mighty_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nuyjp9/liquidai_bet_on_small_but_mighty_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-01T04:13:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1nuw0du</id>
    <title>Sonnet 4.5 tops EQ-Bench writing evals. GLM-4.6 sees incremental improvement.</title>
    <updated>2025-10-01T02:05:23+00:00</updated>
    <author>
      <name>/u/_sqrkl</name>
      <uri>https://old.reddit.com/user/_sqrkl</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nuw0du/sonnet_45_tops_eqbench_writing_evals_glm46_sees/"&gt; &lt;img alt="Sonnet 4.5 tops EQ-Bench writing evals. GLM-4.6 sees incremental improvement." src="https://a.thumbs.redditmedia.com/b5CVus4Rz2oOfFenXT9Ujk9S6gtDV3VmYHL6R6JgQ_0.jpg" title="Sonnet 4.5 tops EQ-Bench writing evals. GLM-4.6 sees incremental improvement." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sonnet 4.5 tops both EQ-Bench writing evals!&lt;/p&gt; &lt;p&gt;Anthropic have evidently worked on safety for this release, with much stronger pushback &amp;amp; de-escalation on spiral-bench vs sonnet-4.&lt;/p&gt; &lt;p&gt;GLM-4.6's score is incremental over GLM-4.5 - but personally I like the newer version's writing much better.&lt;/p&gt; &lt;p&gt;&lt;a href="https://eqbench.com/"&gt;https://eqbench.com/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Sonnet-4.5 creative writing samples:&lt;/p&gt; &lt;p&gt;&lt;a href="https://eqbench.com/results/creative-writing-v3/claude-sonnet-4.5.html"&gt;https://eqbench.com/results/creative-writing-v3/claude-sonnet-4.5.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;x-ai/glm-4.6 creative writing samples:&lt;/p&gt; &lt;p&gt;&lt;a href="https://eqbench.com/results/creative-writing-v3/zai-org__GLM-4.6.html"&gt;https://eqbench.com/results/creative-writing-v3/zai-org__GLM-4.6.html&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_sqrkl"&gt; /u/_sqrkl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nuw0du"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nuw0du/sonnet_45_tops_eqbench_writing_evals_glm46_sees/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nuw0du/sonnet_45_tops_eqbench_writing_evals_glm46_sees/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-01T02:05:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvc5eq</id>
    <title>The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain</title>
    <updated>2025-10-01T15:58:50+00:00</updated>
    <author>
      <name>/u/Salty-Garage7777</name>
      <uri>https://old.reddit.com/user/Salty-Garage7777</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://arxiv.org/html/2509.26507v1"&gt;https://arxiv.org/html/2509.26507v1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A very interesting paper from the guys supported by ≈Åukasz Kaiser, one of the co-authors of the seminal Transformers paper from 2017.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Salty-Garage7777"&gt; /u/Salty-Garage7777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvc5eq/the_dragon_hatchling_the_missing_link_between_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvc5eq/the_dragon_hatchling_the_missing_link_between_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvc5eq/the_dragon_hatchling_the_missing_link_between_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-01T15:58:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1nv873m</id>
    <title>GLM-4.5V model locally for computer use</title>
    <updated>2025-10-01T13:27:48+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nv873m/glm45v_model_locally_for_computer_use/"&gt; &lt;img alt="GLM-4.5V model locally for computer use" src="https://external-preview.redd.it/NDc4azRudDk1aXNmMbWADQNBSkImjVESNjfi_q43l9ostHKNGAFX_QJdfnS0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9b760fdc634bdebc9a069fd1c4a6ba6e673668d6" title="GLM-4.5V model locally for computer use" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;On OSWorld-V, it scores 35.8% - beating UI-TARS-1.5, matching Claude-3.7-Sonnet-20250219, and setting SOTA for fully open-source computer-use models.&lt;/p&gt; &lt;p&gt;Run it with Cua either: Locally via Hugging Face Remotely via OpenRouter&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/trycua"&gt;https://github.com/trycua&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Docs + examples: &lt;a href="https://docs.trycua.com/docs/agent-sdk/supported-agents/computer-use-agents#glm-45v"&gt;https://docs.trycua.com/docs/agent-sdk/supported-agents/computer-use-agents#glm-45v&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/6ff5zu1a5isf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nv873m/glm45v_model_locally_for_computer_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nv873m/glm45v_model_locally_for_computer_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-01T13:27:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1nuq54g</id>
    <title>No GLM-4.6 Air version is coming out</title>
    <updated>2025-09-30T21:40:52+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nuq54g/no_glm46_air_version_is_coming_out/"&gt; &lt;img alt="No GLM-4.6 Air version is coming out" src="https://preview.redd.it/mfj4sracgdsf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=99a9ab99d0f5ab635fe346eff30880f517f02f02" title="No GLM-4.6 Air version is coming out" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Zhipu-AI just shared on X that there are currently no plans to release an Air version of their newly announced GLM-4.6.&lt;/p&gt; &lt;p&gt;That said, I‚Äôm still incredibly excited about what this lab is doing. In my opinion, Zhipu-AI is one of the most promising open-weight AI labs out there right now. I‚Äôve run my own private benchmarks across all major open-weight model releases, and GLM-4.5 stood out significantly, especially for coding and agentic workloads. It‚Äôs the closest I‚Äôve seen an open-weight model come to the performance of the closed-weight frontier models.&lt;/p&gt; &lt;p&gt;I‚Äôve also been keeping up with their technical reports, and they‚Äôve been impressively transparent about their training methods. Notably, they even open-sourced their RL post-training framework, Slime, which is a huge win for the community.&lt;/p&gt; &lt;p&gt;I don‚Äôt have any insider knowledge, but based on what I‚Äôve seen so far, I‚Äôm hopeful they‚Äôll continue approaching/pushing the open-weight frontier and supporting the local LLM ecosystem.&lt;/p&gt; &lt;p&gt;This is an appreciation post.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/mfj4sracgdsf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nuq54g/no_glm46_air_version_is_coming_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nuq54g/no_glm46_air_version_is_coming_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-30T21:40:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1nulea4</id>
    <title>How can I use this beast to benefit the community? Quantize larger models? It‚Äôs a 9985wx, 768 ddr5, 384 gb vram.</title>
    <updated>2025-09-30T18:39:46+00:00</updated>
    <author>
      <name>/u/joninco</name>
      <uri>https://old.reddit.com/user/joninco</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nulea4/how_can_i_use_this_beast_to_benefit_the_community/"&gt; &lt;img alt="How can I use this beast to benefit the community? Quantize larger models? It‚Äôs a 9985wx, 768 ddr5, 384 gb vram." src="https://preview.redd.it/78yadl81kcsf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=977db53773f1bf118531422a3b8b0a76e5905352" title="How can I use this beast to benefit the community? Quantize larger models? It‚Äôs a 9985wx, 768 ddr5, 384 gb vram." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Any ideas are greatly appreciated to use this beast for good!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/joninco"&gt; /u/joninco &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/78yadl81kcsf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nulea4/how_can_i_use_this_beast_to_benefit_the_community/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nulea4/how_can_i_use_this_beast_to_benefit_the_community/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-30T18:39:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1nv7quz</id>
    <title>I spent a few hours prompting LLMs for a pilot study of the "Confidence profile" of GPT-5 vs Qwen3-Max. Findings: GPT-5 is "cosmetically tuned" for confidence. Qwen3, despite meta awareness of its own precision level, defaults towards underconfidence without access to tools.</title>
    <updated>2025-10-01T13:08:53+00:00</updated>
    <author>
      <name>/u/partysnatcher</name>
      <uri>https://old.reddit.com/user/partysnatcher</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nv7quz/i_spent_a_few_hours_prompting_llms_for_a_pilot/"&gt; &lt;img alt="I spent a few hours prompting LLMs for a pilot study of the &amp;quot;Confidence profile&amp;quot; of GPT-5 vs Qwen3-Max. Findings: GPT-5 is &amp;quot;cosmetically tuned&amp;quot; for confidence. Qwen3, despite meta awareness of its own precision level, defaults towards underconfidence without access to tools." src="https://preview.redd.it/nqtw7wzx0isf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2f5e9871c689bdb2e7267272c090e15a3fb22e17" title="I spent a few hours prompting LLMs for a pilot study of the &amp;quot;Confidence profile&amp;quot; of GPT-5 vs Qwen3-Max. Findings: GPT-5 is &amp;quot;cosmetically tuned&amp;quot; for confidence. Qwen3, despite meta awareness of its own precision level, defaults towards underconfidence without access to tools." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;See examples of questions used and explanations of scales in the image. I will copy some of the text from the image here:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GPT-5 findings:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Given a normal human prompt style (and the phrase ‚Äúcan you confidently..‚Äù), the model will have little meta awareness of its data quality, and will confidently hallucinate.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Confidence dump / risk maximization prompt (ie. emphasizing risk and reminding the model that it hallucinates): &lt;ul&gt; &lt;li&gt;Consistently reduces confidence.&lt;/li&gt; &lt;li&gt;Almost avoids hallucinations for the price of some underconfident refusals (false negatives)&lt;br /&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Suggesting ‚Äúcosmetic‚Äù tuning:&lt;/strong&gt; Since hallucinations &lt;em&gt;can&lt;/em&gt; be avoided in preprompt, and models do have some assumption of precision for a question, it is likely that OpenAI is more afraid of the (‚Äúunimpressive‚Äù) occasional underconfidence than of the (‚Äúseemingly impressive‚Äù) consistent confident hallucinations. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Qwen3-Max findings:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Any sense of uncertainty will cause Qwen to want to look up facts.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Any insinuation of required confidence, when lookup is not available, will cause an ‚Äúinconfident‚Äù reply.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Qwen generally needs to be clearly prompted with confidence boosting, and that its okay to hallucinate.&lt;br /&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Distrust of weights for hard facts:&lt;/strong&gt; In short, Qwen generally does not trust its weights to produce hard facts, except in some cases (thus allowing it to ‚Äúoverride‚Äù looked up facts).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/partysnatcher"&gt; /u/partysnatcher &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nqtw7wzx0isf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nv7quz/i_spent_a_few_hours_prompting_llms_for_a_pilot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nv7quz/i_spent_a_few_hours_prompting_llms_for_a_pilot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-01T13:08:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1nuxdd4</id>
    <title>[Release] Finally a working 8-bit quantized VibeVoice model (Release 1.8.0)</title>
    <updated>2025-10-01T03:12:08+00:00</updated>
    <author>
      <name>/u/Fabix84</name>
      <uri>https://old.reddit.com/user/Fabix84</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nuxdd4/release_finally_a_working_8bit_quantized/"&gt; &lt;img alt="[Release] Finally a working 8-bit quantized VibeVoice model (Release 1.8.0)" src="https://preview.redd.it/yevipl7e3fsf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4c842c6d1d94b17de6583302b9661250570aab2a" title="[Release] Finally a working 8-bit quantized VibeVoice model (Release 1.8.0)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;br /&gt; first of all, thank you once again for the incredible support... the project just reached &lt;strong&gt;944 stars&lt;/strong&gt; on GitHub. üôè&lt;/p&gt; &lt;p&gt;In the past few days, several 8-bit quantized models were shared to me, but unfortunately all of them produced only static noise. Since there was clear community interest, I decided to take the challenge and work on it myself. The result is the &lt;strong&gt;first fully working 8-bit quantized model&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;üîó &lt;a href="https://huggingface.co/FabioSarracino/VibeVoice-Large-Q8"&gt;FabioSarracino/VibeVoice-Large-Q8 on HuggingFace&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Alongside this, the latest VibeVoice-ComfyUI releases bring some major updates:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Dynamic on-the-fly quantization&lt;/strong&gt;: you can now quantize the base model to 4-bit or 8-bit at runtime.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;New manual model management system&lt;/strong&gt;: replaced the old automatic HF downloads (which many found inconvenient). Details here ‚Üí &lt;a href="https://github.com/Enemyx-net/VibeVoice-ComfyUI/releases/tag/v1.6.0"&gt;Release 1.6.0&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Latest release (1.8.0)&lt;/strong&gt;: &lt;a href="https://github.com/Enemyx-net/VibeVoice-ComfyUI/releases/tag/v1.8.0"&gt;Changelog&lt;/a&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;GitHub repo (custom ComfyUI node):&lt;br /&gt; üëâ &lt;a href="https://github.com/Enemyx-net/VibeVoice-ComfyUI"&gt;Enemyx-net/VibeVoice-ComfyUI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks again to everyone who contributed feedback, testing, and support! This project wouldn‚Äôt be here without the community.&lt;/p&gt; &lt;p&gt;&lt;em&gt;(Of course, I‚Äôd love if you try it with my node, but it should also work fine with other VibeVoice nodes üòâ)&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fabix84"&gt; /u/Fabix84 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/yevipl7e3fsf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nuxdd4/release_finally_a_working_8bit_quantized/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nuxdd4/release_finally_a_working_8bit_quantized/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-01T03:12:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1nv5uw8</id>
    <title>don't sleep on Apriel-1.5-15b-Thinker and Snowpiercer</title>
    <updated>2025-10-01T11:41:17+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Apriel-1.5-15b-Thinker&lt;/strong&gt; is a multimodal reasoning model in ServiceNow‚Äôs Apriel SLM series which achieves competitive performance against models 10 times it's size. Apriel-1.5 is the second model in the reasoning series. It introduces enhanced textual reasoning capabilities and adds image reasoning support to the previous text model. It has undergone extensive continual pretraining across both text and image domains. In terms of post-training this model has &lt;strong&gt;undergone text-SFT only&lt;/strong&gt;. Our research demonstrates that with a strong mid-training regimen, we are able to achive SOTA performance on text and image reasoning tasks without having any image SFT training or RL.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Highlights&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Achieves a score of &lt;strong&gt;52&lt;/strong&gt; on the Artificial Analysis index and is competitive with Deepseek R1 0528, Gemini-Flash etc.&lt;/li&gt; &lt;li&gt;It is &lt;strong&gt;AT LEAST 1 / 10&lt;/strong&gt; the size of any other model that scores &amp;gt; 50 on the Artificial Analysis index.&lt;/li&gt; &lt;li&gt;Scores &lt;strong&gt;68&lt;/strong&gt; on Tau2 Bench Telecom and &lt;strong&gt;62&lt;/strong&gt; on IFBench, which are key benchmarks for the enterprise domain.&lt;/li&gt; &lt;li&gt;At 15B parameters, the model fits on a single GPU, making it highly memory-efficient.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;it was published yesterday&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/ServiceNow-AI/Apriel-1.5-15b-Thinker"&gt;https://huggingface.co/ServiceNow-AI/Apriel-1.5-15b-Thinker&lt;/a&gt;&lt;/p&gt; &lt;p&gt;their previous model was&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/ServiceNow-AI/Apriel-Nemotron-15b-Thinker"&gt;https://huggingface.co/ServiceNow-AI/Apriel-Nemotron-15b-Thinker&lt;/a&gt;&lt;/p&gt; &lt;p&gt;which is a base model for&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/TheDrummer/Snowpiercer-15B-v3"&gt;https://huggingface.co/TheDrummer/Snowpiercer-15B-v3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;which was published earlier this week :)&lt;/p&gt; &lt;p&gt;let's hope mr &lt;a href="/u/TheLocalDrummer"&gt;u/TheLocalDrummer&lt;/a&gt; will continue Snowpiercing &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nv5uw8/dont_sleep_on_apriel1515bthinker_and_snowpiercer/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nv5uw8/dont_sleep_on_apriel1515bthinker_and_snowpiercer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nv5uw8/dont_sleep_on_apriel1515bthinker_and_snowpiercer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-01T11:41:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1nv8l6o</id>
    <title>Am i seeing this Right?</title>
    <updated>2025-10-01T13:43:53+00:00</updated>
    <author>
      <name>/u/Brave-Hold-9389</name>
      <uri>https://old.reddit.com/user/Brave-Hold-9389</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nv8l6o/am_i_seeing_this_right/"&gt; &lt;img alt="Am i seeing this Right?" src="https://a.thumbs.redditmedia.com/tjIudKmNPF2PlShuW_x68KwSAC4X9VgILiR3p0Bm-R4.jpg" title="Am i seeing this Right?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It would be really cool if unsloth provides quants for Apriel-v1.5-15B-Thinker&lt;/p&gt; &lt;p&gt;(Sorted by opensource, small and tiny)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Brave-Hold-9389"&gt; /u/Brave-Hold-9389 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nv8l6o"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nv8l6o/am_i_seeing_this_right/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nv8l6o/am_i_seeing_this_right/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-01T13:43:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1nv4oy9</id>
    <title>Codex is amazing, it can fix code issues without the need of constant approver. my setup: gpt-oss-20b on lm_studio.</title>
    <updated>2025-10-01T10:37:00+00:00</updated>
    <author>
      <name>/u/kyeoh1</name>
      <uri>https://old.reddit.com/user/kyeoh1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nv4oy9/codex_is_amazing_it_can_fix_code_issues_without/"&gt; &lt;img alt="Codex is amazing, it can fix code issues without the need of constant approver. my setup: gpt-oss-20b on lm_studio." src="https://external-preview.redd.it/ODFtbnEzNm45aHNmMTG3bHLe9xXVwwNl3KvP1Qzcgr5dnq8C6Rg-wDqEIF5Y.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=31079d1483d03020c93a99d188076eb10a02002c" title="Codex is amazing, it can fix code issues without the need of constant approver. my setup: gpt-oss-20b on lm_studio." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kyeoh1"&gt; /u/kyeoh1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/1lusu36n9hsf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nv4oy9/codex_is_amazing_it_can_fix_code_issues_without/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nv4oy9/codex_is_amazing_it_can_fix_code_issues_without/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-01T10:37:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvcjkr</id>
    <title>We're building a local OpenRouter: Auto-configure the best LLM engine on any PC</title>
    <updated>2025-10-01T16:13:17+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvcjkr/were_building_a_local_openrouter_autoconfigure/"&gt; &lt;img alt="We're building a local OpenRouter: Auto-configure the best LLM engine on any PC" src="https://preview.redd.it/fe4322p9yisf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=63b8433dff7ec591d237dcfae3b32ef0a530e5c4" title="We're building a local OpenRouter: Auto-configure the best LLM engine on any PC" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Lemonade is a local LLM server-router that auto-configures high-performance inference engines for your computer. We don't just wrap llama.cpp, we're here to wrap everything!&lt;/p&gt; &lt;p&gt;We started out building an OpenAI-compatible server for AMD NPUs and quickly found that users and devs want flexibility, so we kept adding support for more devices, engines, and operating systems. &lt;/p&gt; &lt;p&gt;What was once a single-engine server evolved into a server-router, like OpenRouter but 100% local. Today's v8.1.11 release adds another inference engine and another OS to the list!&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;üöÄ FastFlowLM&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;The FastFlowLM inference engine for AMD NPUs is fully integrated with Lemonade for Windows Ryzen AI 300-series PCs.&lt;/li&gt; &lt;li&gt;Switch between ONNX, GGUF, and FastFlowLM models from the same Lemonade install with one click.&lt;/li&gt; &lt;li&gt;Shoutout to TWei, Alfred, and Zane for supporting the integration!&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h2&gt;üçé macOS / Apple Silicon&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;PyPI installer for M-series macOS devices, with the same experience available on Windows and Linux.&lt;/li&gt; &lt;li&gt;Taps into llama.cpp's Metal backend for compute.&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h2&gt;ü§ù Community Contributions&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Added a stop button, chat auto-scroll, custom vision model download, model size info, and UI refinements to the built-in web ui.&lt;/li&gt; &lt;li&gt;Support for gpt-oss's reasoning style, changing context size from the tray app, and refined the .exe installer.&lt;/li&gt; &lt;li&gt;Shoutout to kpoineal, siavashhub, ajnatopic1, Deepam02, Kritik-07, RobertAgee, keetrap, and ianbmacdonald!&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h2&gt;ü§ñ What's Next&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Popular apps like Continue, Dify, Morphik, and more are integrating with Lemonade as a native LLM provider, with more apps to follow.&lt;/li&gt; &lt;li&gt;Should we add more inference engines or backends? Let us know what you'd like to see.&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;p&gt;GitHub/Discord links in the comments. Check us out and say hi if the project direction sounds good to you. The community's support is what empowers our team at AMD to expand across different hardware, engines, and OSs.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fe4322p9yisf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvcjkr/were_building_a_local_openrouter_autoconfigure/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvcjkr/were_building_a_local_openrouter_autoconfigure/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-01T16:13:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1nv53rb</id>
    <title>GLM-4.6-GGUF is out!</title>
    <updated>2025-10-01T11:00:52+00:00</updated>
    <author>
      <name>/u/TheAndyGeorge</name>
      <uri>https://old.reddit.com/user/TheAndyGeorge</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nv53rb/glm46gguf_is_out/"&gt; &lt;img alt="GLM-4.6-GGUF is out!" src="https://preview.redd.it/kptmc2f0fhsf1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9344c531abf7cb2d05a64a1d2ee461b6106008bb" title="GLM-4.6-GGUF is out!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheAndyGeorge"&gt; /u/TheAndyGeorge &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kptmc2f0fhsf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nv53rb/glm46gguf_is_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nv53rb/glm46gguf_is_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-01T11:00:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
