<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-01T16:56:08+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1mejkcu</id>
    <title>[P] Tri-70B-preview-SFT: New 70B Model (Research Preview, SFT-only)</title>
    <updated>2025-08-01T01:31:25+00:00</updated>
    <author>
      <name>/u/jshin49</name>
      <uri>https://old.reddit.com/user/jshin49</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;We're a scrappy startup at Trillion Labs and just released &lt;a href="https://huggingface.co/trillionlabs/Tri-70B-preview-SFT"&gt;Tri-70B-preview-SFT&lt;/a&gt;, our largest language model yet (70B params!), trained from scratch on ~1.5T tokens. We unexpectedly ran short on compute, so this is a pure supervised fine-tuning (SFT) release‚Äîzero RLHF.&lt;/p&gt; &lt;h1&gt;TL;DR:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;70B parameters&lt;/strong&gt;; pure supervised fine-tuning (&lt;strong&gt;no RLHF&lt;/strong&gt; yet!)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;32K token context window&lt;/strong&gt; (perfect for experimenting with Yarn, if you're bold!)&lt;/li&gt; &lt;li&gt;Optimized primarily for &lt;strong&gt;English and Korean&lt;/strong&gt;, with decent Japanese performance&lt;/li&gt; &lt;li&gt;Tried some new tricks (&lt;strong&gt;FP8 mixed precision, Scalable Softmax, iRoPE attention&lt;/strong&gt;)&lt;/li&gt; &lt;li&gt;Benchmarked roughly around &lt;strong&gt;Qwen-2.5-72B and LLaMA-3.1-70B&lt;/strong&gt;, but it's noticeably raw and needs alignment tweaks.&lt;/li&gt; &lt;li&gt;Model and tokenizer fully open on ü§ó HuggingFace under a permissive license (&lt;strong&gt;auto-approved&lt;/strong&gt; conditional commercial usage allowed, but it‚Äôs definitely experimental!).&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Why release it raw?&lt;/h1&gt; &lt;p&gt;We think releasing Tri-70B in its current form might spur unique research‚Äîespecially for those into RLHF, RLVR, GRPO, CISPO, GSPO, etc. It‚Äôs a perfect baseline for alignment experimentation. Frankly, we know it‚Äôs not perfectly aligned, and we'd love your help to identify weak spots.&lt;/p&gt; &lt;p&gt;Give it a spin and see what it can (and can‚Äôt) do. We‚Äôre particularly curious about your experiences with alignment, context handling, and multilingual use.&lt;/p&gt; &lt;p&gt;**üëâ **&lt;a href="https://huggingface.co/trillionlabs/Tri-70B-preview-SFT"&gt;&lt;strong&gt;Check out the repo and model card here!&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Questions, thoughts, criticisms warmly welcomed‚Äîhit us up below!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jshin49"&gt; /u/jshin49 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mejkcu/p_tri70bpreviewsft_new_70b_model_research_preview/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mejkcu/p_tri70bpreviewsft_new_70b_model_research_preview/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mejkcu/p_tri70bpreviewsft_new_70b_model_research_preview/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-01T01:31:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1mf0mw2</id>
    <title>Open-source architectures that aren't Llama 3 knock offs?</title>
    <updated>2025-08-01T16:10:44+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just got through Raschka's model architecture series. Seems like everything is a tweak of Llama 3.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mf0mw2/opensource_architectures_that_arent_llama_3_knock/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mf0mw2/opensource_architectures_that_arent_llama_3_knock/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mf0mw2/opensource_architectures_that_arent_llama_3_knock/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-01T16:10:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1me4i2h</id>
    <title>I made a comparison chart for Qwen3-Coder-30B-A3B vs. Qwen3-Coder-480B-A35B</title>
    <updated>2025-07-31T15:23:42+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1me4i2h/i_made_a_comparison_chart_for_qwen3coder30ba3b_vs/"&gt; &lt;img alt="I made a comparison chart for Qwen3-Coder-30B-A3B vs. Qwen3-Coder-480B-A35B" src="https://preview.redd.it/l6547uel88gf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=40dafbd4c67e845ff8ce7c141e92d59fdfd342fe" title="I made a comparison chart for Qwen3-Coder-30B-A3B vs. Qwen3-Coder-480B-A35B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As you can see from the radar chart, the scores on the left for the two Agent capability tests, mind2web and BFCL-v3, are very close. This suggests that the Agent capabilities of Qwen3-Coder-FLash should be quite strong. &lt;/p&gt; &lt;p&gt;However, there is still a significant gap in the Aider-Polyglot and SWE Multilingual tests, which implies that its programming capabilities are indeed quite different from those of Qwen3-Coder-480B.&lt;/p&gt; &lt;p&gt;Has anyone started using it yet? What's the actual user experience like?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/l6547uel88gf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1me4i2h/i_made_a_comparison_chart_for_qwen3coder30ba3b_vs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1me4i2h/i_made_a_comparison_chart_for_qwen3coder30ba3b_vs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-31T15:23:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1mf0i54</id>
    <title>Qwen 30b a3b 2507 instruct as good as Gemma 3 27B!?</title>
    <updated>2025-08-01T16:05:49+00:00</updated>
    <author>
      <name>/u/Hanthunius</name>
      <uri>https://old.reddit.com/user/Hanthunius</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What an awesome model. Everything I throw at it I get comparable results to Gemma 3, but 4.5x faster.&lt;/p&gt; &lt;p&gt;Great at general knowledge, but also follows instructions very well.&lt;/p&gt; &lt;p&gt;Please let me know your experiences with it!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hanthunius"&gt; /u/Hanthunius &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mf0i54/qwen_30b_a3b_2507_instruct_as_good_as_gemma_3_27b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mf0i54/qwen_30b_a3b_2507_instruct_as_good_as_gemma_3_27b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mf0i54/qwen_30b_a3b_2507_instruct_as_good_as_gemma_3_27b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-01T16:05:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1meyvir</id>
    <title>Best NSFW friendly writing models(editing only)</title>
    <updated>2025-08-01T15:04:09+00:00</updated>
    <author>
      <name>/u/LordOfHeavenWill</name>
      <uri>https://old.reddit.com/user/LordOfHeavenWill</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm searching for a good and reliable LLM that can correct my grammar mistakes. It does not need to have the ability to 'write' or create something new. I used Grammarly till now, but it's kinda slow and I need to choose every correction myself. It's mostly for spelling mistakes, as I'm not a native English speaker, and only roughly know how to spell each word. I tried ChatGPT, and it's actually kinda nice. It recognized the words I meant and corrects them. I re-edit the sentences it changes afterward back, but I want an alternative locally.&lt;/p&gt; &lt;p&gt;Edit: Mistral AI is even better, as it does not change your sentences, but is not NSFW friendly(the online version the least)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LordOfHeavenWill"&gt; /u/LordOfHeavenWill &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1meyvir/best_nsfw_friendly_writing_modelsediting_only/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1meyvir/best_nsfw_friendly_writing_modelsediting_only/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1meyvir/best_nsfw_friendly_writing_modelsediting_only/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-01T15:04:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1meep6o</id>
    <title>The Great Deception of "Low Prices" in LLM APIs</title>
    <updated>2025-07-31T21:54:06+00:00</updated>
    <author>
      <name>/u/Current-Stop7806</name>
      <uri>https://old.reddit.com/user/Current-Stop7806</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1meep6o/the_great_deception_of_low_prices_in_llm_apis/"&gt; &lt;img alt="The Great Deception of &amp;quot;Low Prices&amp;quot; in LLM APIs" src="https://preview.redd.it/f8vv4t837agf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b1305c708b1fbe4bb7166cf9808a29640f750a67" title="The Great Deception of &amp;quot;Low Prices&amp;quot; in LLM APIs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;( Or... The adventures of a newbie )&lt;/p&gt; &lt;p&gt;Today I learned something really important ‚Äî and honestly, I had no idea how using API-hosted LLMs can quietly become a black hole for your wallet.üí∏üí∞&lt;/p&gt; &lt;p&gt;At first glance, the pricing seems super appealing. You see those spicy ‚Äúlow‚Äù prices from big US companies ‚Äî something like $0.002 per 1,000 tokens, and you think, &amp;quot;Wow, that‚Äôs cheap!&amp;quot;&lt;/p&gt; &lt;p&gt;But‚Ä¶ let‚Äôs do the math.&lt;/p&gt; &lt;p&gt;You start using a 128k context model on a platform like OpenRouter, and you don‚Äôt realize that with every new interaction, your entire chat history is being resent to the API. That‚Äôs the only way the model can &amp;quot;remember&amp;quot; the conversation. So after just a few minutes, each message you're sending might carry along 10k tokens ‚Äî or even more.&lt;/p&gt; &lt;p&gt;Now imagine you‚Äôre chatting for hours. Every tiny reply ‚Äî even a simple ‚Äúok‚Äù ‚Äî could trigger a payload of 50,000 or 100,000 tokens being sent again and again. It‚Äôs like buying an entire book just to read the next letter.&lt;/p&gt; &lt;p&gt;In just a few hours, you may have burned through $5 to $10, just for a basic conversation. And now think monthly... or worse ‚Äî imagine you‚Äôre editing a software file with 800 lines of code. Every time you tweak a line and hit send, it could cost you $1 or $2 per second.&lt;/p&gt; &lt;p&gt;I mean... what?!&lt;/p&gt; &lt;p&gt;I now understand the almost desperate effort some people make to run LLMs locally on their own machines ‚Äî because something that looks insanely cheap at first glance‚Ä¶ can turn out to be violently expensive.&lt;/p&gt; &lt;p&gt;This is insane. Maybe everyone else already knew this ‚Äî but I didn‚Äôt! üòØüòØüòØ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Current-Stop7806"&gt; /u/Current-Stop7806 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/f8vv4t837agf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1meep6o/the_great_deception_of_low_prices_in_llm_apis/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1meep6o/the_great_deception_of_low_prices_in_llm_apis/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-31T21:54:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1me2zc6</id>
    <title>Qwen3-Coder-30B-A3B released!</title>
    <updated>2025-07-31T14:24:40+00:00</updated>
    <author>
      <name>/u/glowcialist</name>
      <uri>https://old.reddit.com/user/glowcialist</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1me2zc6/qwen3coder30ba3b_released/"&gt; &lt;img alt="Qwen3-Coder-30B-A3B released!" src="https://external-preview.redd.it/IAGFmaGszKcqSKR_8qg0oES6OBfFDCBNvzr72pbVe7o.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9d2b1429fc14f5ca152608718fd3ef6d50119778" title="Qwen3-Coder-30B-A3B released!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/glowcialist"&gt; /u/glowcialist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1me2zc6/qwen3coder30ba3b_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1me2zc6/qwen3coder30ba3b_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-31T14:24:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1mf08e5</id>
    <title>SVDQuant does INT4 quantization of text-to-image models without losing quality. Can't the same technique be used in LLMs?</title>
    <updated>2025-08-01T15:56:02+00:00</updated>
    <author>
      <name>/u/we_are_mammals</name>
      <uri>https://old.reddit.com/user/we_are_mammals</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mf08e5/svdquant_does_int4_quantization_of_texttoimage/"&gt; &lt;img alt="SVDQuant does INT4 quantization of text-to-image models without losing quality. Can't the same technique be used in LLMs?" src="https://preview.redd.it/0cq321qc1fgf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=162d6f061d05ab906b370e0b7ce08f4a7f85014d" title="SVDQuant does INT4 quantization of text-to-image models without losing quality. Can't the same technique be used in LLMs?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/we_are_mammals"&gt; /u/we_are_mammals &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0cq321qc1fgf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mf08e5/svdquant_does_int4_quantization_of_texttoimage/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mf08e5/svdquant_does_int4_quantization_of_texttoimage/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-01T15:56:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1men28l</id>
    <title>[Guide] The *SIMPLE* Self-Hosted AI Coding That Just Works feat. Qwen3-Coder-Flash</title>
    <updated>2025-08-01T04:28:12+00:00</updated>
    <author>
      <name>/u/xrailgun</name>
      <uri>https://old.reddit.com/user/xrailgun</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;, This guide outlines a method to create a fully local AI coding assistant with RAG capabilities. The entire backend runs through LM Studio, which handles model downloading, options, serving, and tool integration, avoiding the need for Docker or separate Python environments. Heavily based on the previous guide by &lt;a href="/u/send_me_a_ticket"&gt;u/send_me_a_ticket&lt;/a&gt; (thanks!), just further simplified.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I know some of you wizards want to run things directly through CLI and llama.cpp etc, this guide is not for you.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Core Components&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Engine:&lt;/strong&gt; &lt;strong&gt;LM Studio.&lt;/strong&gt; Used for downloading models, serving them via a local API, and running the tool server.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Tool Server (RAG):&lt;/strong&gt; &lt;a href="https://github.com/arabold/docs-mcp-server"&gt;&lt;strong&gt;docs-mcp-server&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;.&lt;/strong&gt; Runs as a plugin directly inside LM Studio to scrape and index documentation for the LLM to use.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Frontend:&lt;/strong&gt; &lt;strong&gt;VS Code +&lt;/strong&gt; &lt;a href="https://marketplace.visualstudio.com/items?itemName=hitbunt.roo-code"&gt;&lt;strong&gt;Roo Code&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;.&lt;/strong&gt; The editor extension that connects to the local model server.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Advantages of this Approach&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Straightforward Setup:&lt;/strong&gt; Uses the LM Studio GUI for most of the configuration.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;100% Local &amp;amp; Private:&lt;/strong&gt; Code and prompts are not sent to external services.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;VRAM-Friendly:&lt;/strong&gt; Optimized for running quantized GGUF models on consumer hardware.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Part 1: Configuring LM Studio&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;1. Install LM Studio&lt;/strong&gt; Download and install the latest version from the &lt;a href="https://lmstudio.ai/"&gt;LM Studio website&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. Download Your Models&lt;/strong&gt; In the LM Studio main window (Search tab, magnifying glass icon), search for and download two models:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;A Coder LLM:&lt;/strong&gt; Example: &lt;code&gt;qwen/qwen3-coder-30b&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;An Embedding Model:&lt;/strong&gt; Example: &lt;code&gt;Qwen/Qwen3-Embedding-0.6B-GGUF&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;3. Tune Model Settings&lt;/strong&gt; Navigate to the &amp;quot;My Models&amp;quot; tab (folder icon on the left). For both your LLM and your embedding model, you can click on them to tune settings like context length, GPU offload, and enable options like Flash Attention/QV Caching according to your model/hardware.&lt;/p&gt; &lt;p&gt;Qwen3 doesn't seem to like quantized QV Caching, resulting in Exit code: 18446744072635812000, so leave that off/default at f16.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;4. Configure the&lt;/strong&gt; &lt;code&gt;docs-mcp-server&lt;/code&gt; &lt;strong&gt;Plugin&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Click the &amp;quot;Chat&amp;quot; tab (yellow chat bubble icon on top left).&lt;/li&gt; &lt;li&gt;Click on Program on the right.&lt;/li&gt; &lt;li&gt;Click on Install, select `Edit mcp.json', and replace its entire contents with this:&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&amp;#8203;&lt;/p&gt; &lt;pre&gt;&lt;code&gt; { &amp;quot;mcpServers&amp;quot;: { &amp;quot;docs-mcp-server&amp;quot;: { &amp;quot;command&amp;quot;: &amp;quot;npx&amp;quot;, &amp;quot;args&amp;quot;: [ &amp;quot;@arabold/docs-mcp-server@latest&amp;quot; ], &amp;quot;env&amp;quot;: { &amp;quot;OPENAI_API_KEY&amp;quot;: &amp;quot;lmstudio&amp;quot;, &amp;quot;OPENAI_API_BASE&amp;quot;: &amp;quot;http://localhost:1234/v1&amp;quot;, &amp;quot;DOCS_MCP_EMBEDDING_MODEL&amp;quot;: &amp;quot;text-embedding-qwen3-embedding-0.6b&amp;quot; } } } } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;Note: Your&lt;/em&gt; &lt;code&gt;DOCS_MCP_EMBEDDING_MODEL&lt;/code&gt; &lt;em&gt;value must match the API Model Name shown on the Server tab once the model is loaded. If yours is different, you'll need to update it here.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;If it's correct, &lt;code&gt;the mcp/docs-mcp-server&lt;/code&gt; tab will show things like &lt;code&gt;Tools&lt;/code&gt;, &lt;code&gt;scrape_docs&lt;/code&gt;, &lt;code&gt;search_docs&lt;/code&gt;, ... etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;5. Start the Server&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Navigate to the Local Server tab (&lt;code&gt;&amp;gt;_&lt;/code&gt; icon on the left).&lt;/li&gt; &lt;li&gt;In the top slot, load your coder LLM (e.g., Qwen3-Coder).&lt;/li&gt; &lt;li&gt;In the second slot, load your embedding model (e.g., Qwen3-Embeddings).&lt;/li&gt; &lt;li&gt;Click &lt;strong&gt;Start Server&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Check the server logs at the bottom to verify that the server is running and the &lt;code&gt;docs-mcp-server&lt;/code&gt; plugin has loaded correctly.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Part 2: Configuring VS Code &amp;amp; Roo Code&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;1. Install VS Code and Roo Code&lt;/strong&gt; Install &lt;a href="https://code.visualstudio.com/"&gt;Visual Studio Code&lt;/a&gt;. Then, inside VS Code, go to the Extensions tab and search for and install &lt;strong&gt;Roo Code&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. Connect Roo Code to LM Studio&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;In VS Code, click the Roo Code icon in the sidebar.&lt;/li&gt; &lt;li&gt;At the bottom, click the gear icon next to your profile name to open the settings.&lt;/li&gt; &lt;li&gt;Click &lt;strong&gt;Add Profile&lt;/strong&gt;, give it a name (e.g., &amp;quot;LM Studio&amp;quot;), and configure it:&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LM Provider:&lt;/strong&gt; Select &lt;code&gt;LM Studio&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Base URL:&lt;/strong&gt; &lt;a href="http://127.0.0.1:1234"&gt;&lt;code&gt;http://127.0.0.1:1234&lt;/code&gt;&lt;/a&gt; (or your server address)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model:&lt;/strong&gt; Select your coder model's ID (e.g., &lt;code&gt;qwen/qwen3-coder-30b&lt;/code&gt;, it should appear automatically) .&lt;/li&gt; &lt;li&gt;While in the settings, you can go through the other tabs (like &amp;quot;Auto-Approve&amp;quot;) and toggle preferences to fit your workflow.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;3. Connect Roo Code to the Tool Server&lt;/strong&gt; Finally, we have to expose the mcp server to Roo.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;In the Roo Code settings panel, click the 3 horizontal dots (top right), select &amp;quot;MCP Servers&amp;quot; from the drop-down menu.&lt;/li&gt; &lt;li&gt;Ensure the &lt;strong&gt;&amp;quot;Enable MCP Servers&amp;quot;&lt;/strong&gt; checkbox is &lt;strong&gt;ENABLED&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Scroll down and click &amp;quot;Edit Global MCP&amp;quot;, and replace the contents (if any) with this:&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&amp;#8203;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;{ &amp;quot;mcpServers&amp;quot;: { &amp;quot;docs-mcp-server&amp;quot;: { &amp;quot;command&amp;quot;: &amp;quot;npx&amp;quot;, &amp;quot;args&amp;quot;: [ &amp;quot;@arabold/docs-mcp-server@latest&amp;quot; ], &amp;quot;env&amp;quot;: { &amp;quot;OPENAI_API_KEY&amp;quot;: &amp;quot;lmstudio&amp;quot;, &amp;quot;OPENAI_API_BASE&amp;quot;: &amp;quot;http://localhost:1234/v1&amp;quot;, &amp;quot;DOCS_MCP_EMBEDDING_MODEL&amp;quot;: &amp;quot;text-embedding-qwen3-embedding-0.6b&amp;quot; }, &amp;quot;alwaysAllow&amp;quot;: [ &amp;quot;fetch_url&amp;quot;, &amp;quot;remove_docs&amp;quot;, &amp;quot;scrape_docs&amp;quot;, &amp;quot;search_docs&amp;quot;, &amp;quot;list_libraries&amp;quot;, &amp;quot;find_version&amp;quot;, &amp;quot;list_jobs&amp;quot;, &amp;quot;get_job_info&amp;quot;, &amp;quot;cancel_job&amp;quot; ], &amp;quot;disabled&amp;quot;: false } } } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;Note: I'm not exactly sure how this part works. This is functional, but maybe contains redundancies. Hopefully someone with more knowledge can optimize this in the comments.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Then you can toggle it on and see a green circle if there's no issues.&lt;/p&gt; &lt;p&gt;Your setup is now complete. You have a local coding assistant that can use the &lt;code&gt;docs-mcp-server&lt;/code&gt; to perform RAG against documentation you provide.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xrailgun"&gt; /u/xrailgun &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1men28l/guide_the_simple_selfhosted_ai_coding_that_just/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1men28l/guide_the_simple_selfhosted_ai_coding_that_just/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1men28l/guide_the_simple_selfhosted_ai_coding_that_just/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-01T04:28:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1mepr5q</id>
    <title>How to run Qwen3 Coder 30B-A3B the fastest?</title>
    <updated>2025-08-01T07:09:04+00:00</updated>
    <author>
      <name>/u/R46H4V</name>
      <uri>https://old.reddit.com/user/R46H4V</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to switch from using claude code to running this model locally via cline or other similar extensions.&lt;/p&gt; &lt;p&gt;My Laptop's specs are: i5-11400H with 32GB DDR4 RAM at 2666Mhz. RTX 3060 Laptop GPU with 6GB GDDR6 VRAM.&lt;/p&gt; &lt;p&gt;I got confused as there are a lot of inference engines available such as Ollama, LM studio, llama.cpp, vLLM, sglang, ik_llama.cpp etc. i dont know why there are som many of these and what are their pros and cons. So i wanted to ask here. I need the absolute fastest responses possible, i don't mind installing niche software or other things. &lt;/p&gt; &lt;p&gt;Thank you in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/R46H4V"&gt; /u/R46H4V &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mepr5q/how_to_run_qwen3_coder_30ba3b_the_fastest/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mepr5q/how_to_run_qwen3_coder_30ba3b_the_fastest/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mepr5q/how_to_run_qwen3_coder_30ba3b_the_fastest/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-01T07:09:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1meeyee</id>
    <title>Ollama's new GUI is closed source?</title>
    <updated>2025-07-31T22:04:17+00:00</updated>
    <author>
      <name>/u/Sea_Night_2572</name>
      <uri>https://old.reddit.com/user/Sea_Night_2572</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1meeyee/ollamas_new_gui_is_closed_source/"&gt; &lt;img alt="Ollama's new GUI is closed source?" src="https://b.thumbs.redditmedia.com/zLVPiHg9ufyqhvp5Basb43POL8O8dmXli04dBAOzdrw.jpg" title="Ollama's new GUI is closed source?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Brothers and sisters, we're being taken for fools.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/d1iudzju8agf1.png?width=922&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c7d5d1e1b891425817fab581afae0149aec26b6b"&gt;https://preview.redd.it/d1iudzju8agf1.png?width=922&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c7d5d1e1b891425817fab581afae0149aec26b6b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Did anyone check if it's phoning home?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sea_Night_2572"&gt; /u/Sea_Night_2572 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1meeyee/ollamas_new_gui_is_closed_source/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1meeyee/ollamas_new_gui_is_closed_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1meeyee/ollamas_new_gui_is_closed_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-31T22:04:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1mewq1v</id>
    <title>Hugging Face space for anyone who want to try the new Dots OCR</title>
    <updated>2025-08-01T13:37:45+00:00</updated>
    <author>
      <name>/u/Severe-Awareness829</name>
      <uri>https://old.reddit.com/user/Severe-Awareness829</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mewq1v/hugging_face_space_for_anyone_who_want_to_try_the/"&gt; &lt;img alt="Hugging Face space for anyone who want to try the new Dots OCR" src="https://external-preview.redd.it/jdWLa4yLFu9Ou5jfhrAqJqpvnaA7jgwPFox4bV2vTWM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b249c1a963b7199d36b85e3948a0475db9194b46" title="Hugging Face space for anyone who want to try the new Dots OCR" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My initial experiments with the model is very positive, i hope the space is useful for anyone who want to try the model&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Severe-Awareness829"&gt; /u/Severe-Awareness829 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/spaces/MohamedRashad/Dots-OCR"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mewq1v/hugging_face_space_for_anyone_who_want_to_try_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mewq1v/hugging_face_space_for_anyone_who_want_to_try_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-01T13:37:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1mf0hou</id>
    <title>support for the upcoming hunyuan dense models has been merged into llama.cpp</title>
    <updated>2025-08-01T16:05:23+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mf0hou/support_for_the_upcoming_hunyuan_dense_models_has/"&gt; &lt;img alt="support for the upcoming hunyuan dense models has been merged into llama.cpp" src="https://external-preview.redd.it/pRyHe6l3-qrKqD2qUrrGwmAgS3GhMUUKtd4TRQbJKGc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f81f256091726e20730924e97a225729e6c971ec" title="support for the upcoming hunyuan dense models has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In the source code, we see a link to Hunyuan-4B-Instruct, but I think we‚Äôll see much larger models :)&lt;/p&gt; &lt;p&gt;bonus: fix hunyuan_moe chat template&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/14878"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mf0hou/support_for_the_upcoming_hunyuan_dense_models_has/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mf0hou/support_for_the_upcoming_hunyuan_dense_models_has/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-01T16:05:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1meqnn1</id>
    <title>More supposed info about OpenAI's open-weight model</title>
    <updated>2025-08-01T08:07:40+00:00</updated>
    <author>
      <name>/u/CheekyBastard55</name>
      <uri>https://old.reddit.com/user/CheekyBastard55</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CheekyBastard55"&gt; /u/CheekyBastard55 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/apples_jimmy/status/1951192085119508860"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1meqnn1/more_supposed_info_about_openais_openweight_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1meqnn1/more_supposed_info_about_openais_openweight_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-01T08:07:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1me31d8</id>
    <title>üöÄ Qwen3-Coder-Flash released!</title>
    <updated>2025-07-31T14:26:52+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ü¶• Qwen3-Coder-Flash: Qwen3-Coder-30B-A3B-Instruct&lt;/p&gt; &lt;p&gt;üíö Just lightning-fast, accurate code generation.&lt;/p&gt; &lt;p&gt;‚úÖ Native 256K context (supports up to 1M tokens with YaRN)&lt;/p&gt; &lt;p&gt;‚úÖ Optimized for platforms like Qwen Code, Cline, Roo Code, Kilo Code, etc.&lt;/p&gt; &lt;p&gt;‚úÖ Seamless function calling &amp;amp; agent workflows&lt;/p&gt; &lt;p&gt;üí¨ Chat: &lt;a href="https://chat.qwen.ai/"&gt;https://chat.qwen.ai/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ü§ó Hugging Face: &lt;a href="https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct"&gt;https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ü§ñ ModelScope: &lt;a href="https://modelscope.cn/models/Qwen/Qwen3-Coder-30B-A3B-Instruct"&gt;https://modelscope.cn/models/Qwen/Qwen3-Coder-30B-A3B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/p7fpia2bz7gf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1me31d8/qwen3coderflash_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1me31d8/qwen3coderflash_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-31T14:26:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1meucvo</id>
    <title>Unsloth GGUFs Perplexity Score Comparison | Qwen3-Coder-30B-A3B-Instruct</title>
    <updated>2025-08-01T11:49:43+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1meucvo/unsloth_ggufs_perplexity_score_comparison/"&gt; &lt;img alt="Unsloth GGUFs Perplexity Score Comparison | Qwen3-Coder-30B-A3B-Instruct" src="https://b.thumbs.redditmedia.com/VNoO8rEJvUZhkVA7_FI5gY826jnCUY53ZbZFN6EhWxs.jpg" title="Unsloth GGUFs Perplexity Score Comparison | Qwen3-Coder-30B-A3B-Instruct" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/8edgr1plcegf1.png?width=888&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8a4d35aab872fa2e1ca113eb8f8510212b54d68b"&gt;https://preview.redd.it/8edgr1plcegf1.png?width=888&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8a4d35aab872fa2e1ca113eb8f8510212b54d68b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Lower PPL = Better&lt;/p&gt; &lt;p&gt;I didn't test q6 and q8 because they can't fit in my 24gb card&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-perplexity.exe --model &amp;quot;&amp;quot; --threads 15 --ctx-size 8000 -f wiki.test.raw --flash-attn --cache-type-k q8_0 --cache-type-v q8_0 --n-gpu-layers 99 --mlock --parallel 8 --seed 7894 --temp 0.7 --top-k 20 --top-p 0.8 --min-p 0 --repeat-penalty 1.05 --presence-penalty 1.5 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;IQ4_XS&lt;br /&gt; 7 experts PPL = 7.6844&lt;br /&gt; default 8 experts PPL = 7.6741&lt;br /&gt; 9 experts PPL = 7.6890&lt;br /&gt; 10 experts PPL = 7.7343&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1meucvo/unsloth_ggufs_perplexity_score_comparison/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1meucvo/unsloth_ggufs_perplexity_score_comparison/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1meucvo/unsloth_ggufs_perplexity_score_comparison/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-01T11:49:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1mes7rc</id>
    <title>Quantize your own GGUFs the same way as your fav Unsloth Dynamic GGUFs</title>
    <updated>2025-08-01T09:48:04+00:00</updated>
    <author>
      <name>/u/terminoid_</name>
      <uri>https://old.reddit.com/user/terminoid_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/electroglyph/quant_clone"&gt;https://github.com/electroglyph/quant_clone&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is a tiny little app which will create a llama-quantize command based on how a target GGUF is quantized. I wanted it so that I can quantize my finetunes the same way Unsloth does.&lt;/p&gt; &lt;p&gt;For instance, if you run quant_clone gemma-3-1b-it-UD-IQ1_S.gguf&lt;/p&gt; &lt;p&gt;you get:&lt;/p&gt; &lt;p&gt;llama-quantize --imatrix &amp;lt;imatrix_unsloth.dat&amp;gt; --tensor-type token_embd.weight=Q5_1 --tensor-type blk.0.attn_k.weight=IQ4_NL --tensor-type blk.0.attn_output.weight=IQ2_XXS --tensor-type blk.0.attn_q.weight=IQ4_NL --tensor-type blk.0.attn_v.weight=Q5_0 --tensor-type blk.0.ffn_down.weight=IQ3_S --tensor-type blk.0.ffn_gate.weight=IQ4_NL --tensor-type blk.0.ffn_up.weight=IQ4_NL --tensor-type blk.1.attn_k.weight=IQ4_NL --tensor-type blk.1.attn_output.weight=IQ2_XXS --tensor-type blk.1.attn_q.weight=IQ4_NL --tensor-type blk.1.attn_v.weight=Q5_0 --tensor-type blk.1.ffn_down.weight=Q2_K --tensor-type blk.1.ffn_gate.weight=IQ4_NL --tensor-type blk.1.ffn_up.weight=IQ4_NL --tensor-type blk.2.attn_k.weight=IQ4_NL --tensor-type blk.2.attn_output.weight=IQ2_XXS --tensor-type blk.2.attn_q.weight=IQ4_NL --tensor-type blk.2.attn_v.weight=Q5_0 --tensor-type blk.2.ffn_down.weight=IQ3_S --tensor-type blk.2.ffn_gate.weight=IQ4_NL --tensor-type blk.2.ffn_up.weight=IQ4_NL --tensor-type blk.3.attn_k.weight=IQ4_NL --tensor-type blk.3.attn_output.weight=IQ2_XXS --tensor-type blk.3.attn_q.weight=IQ4_NL --tensor-type blk.3.attn_v.weight=Q5_0 --tensor-type blk.3.ffn_down.weight=IQ3_S --tensor-type blk.3.ffn_gate.weight=IQ4_NL --tensor-type blk.3.ffn_up.weight=IQ4_NL --tensor-type blk.4.attn_k.weight=IQ4_NL --tensor-type blk.4.attn_output.weight=IQ2_XXS --tensor-type blk.4.attn_q.weight=IQ4_NL --tensor-type blk.4.attn_v.weight=Q5_0 --tensor-type blk.4.ffn_down.weight=IQ3_S --tensor-type blk.4.ffn_gate.weight=IQ4_NL --tensor-type blk.4.ffn_up.weight=IQ4_NL --tensor-type blk.5.attn_k.weight=IQ4_NL --tensor-type blk.5.attn_output.weight=IQ2_XXS --tensor-type blk.5.attn_q.weight=IQ4_NL --tensor-type blk.5.attn_v.weight=Q5_0 --tensor-type blk.5.ffn_down.weight=IQ1_S --tensor-type blk.5.ffn_gate.weight=IQ4_NL --tensor-type blk.5.ffn_up.weight=IQ4_NL --tensor-type blk.6.attn_k.weight=IQ4_NL --tensor-type blk.6.attn_output.weight=IQ2_XXS --tensor-type blk.6.attn_q.weight=IQ4_NL --tensor-type blk.6.attn_v.weight=Q5_0 --tensor-type blk.6.ffn_down.weight=IQ1_S --tensor-type blk.6.ffn_gate.weight=IQ4_NL --tensor-type blk.6.ffn_up.weight=IQ4_NL --tensor-type blk.7.attn_k.weight=IQ4_NL --tensor-type blk.7.attn_output.weight=IQ2_XXS --tensor-type blk.7.attn_q.weight=IQ4_NL --tensor-type blk.7.attn_v.weight=Q5_0 --tensor-type blk.7.ffn_down.weight=IQ1_S --tensor-type blk.7.ffn_gate.weight=IQ4_NL --tensor-type blk.7.ffn_up.weight=IQ4_NL --tensor-type blk.8.attn_k.weight=IQ4_NL --tensor-type blk.8.attn_output.weight=IQ2_XXS --tensor-type blk.8.attn_q.weight=IQ4_NL --tensor-type blk.8.attn_v.weight=Q5_0 --tensor-type blk.8.ffn_down.weight=IQ1_S --tensor-type blk.8.ffn_gate.weight=IQ4_NL --tensor-type blk.8.ffn_up.weight=IQ4_NL --tensor-type blk.9.attn_k.weight=IQ4_NL --tensor-type blk.9.attn_output.weight=IQ2_XXS --tensor-type blk.9.attn_q.weight=IQ4_NL --tensor-type blk.9.attn_v.weight=Q5_0 --tensor-type blk.9.ffn_down.weight=IQ1_S --tensor-type blk.9.ffn_gate.weight=IQ4_NL --tensor-type blk.9.ffn_up.weight=IQ4_NL --tensor-type blk.10.attn_k.weight=IQ4_NL --tensor-type blk.10.attn_output.weight=IQ2_XXS --tensor-type blk.10.attn_q.weight=IQ4_NL --tensor-type blk.10.attn_v.weight=Q5_0 --tensor-type blk.10.ffn_down.weight=IQ1_S --tensor-type blk.10.ffn_gate.weight=IQ4_NL --tensor-type blk.10.ffn_up.weight=IQ4_NL --tensor-type blk.11.attn_k.weight=IQ4_NL --tensor-type blk.11.attn_output.weight=IQ2_XXS --tensor-type blk.11.attn_q.weight=IQ4_NL --tensor-type blk.11.attn_v.weight=Q5_0 --tensor-type blk.11.ffn_down.weight=IQ2_S --tensor-type blk.11.ffn_gate.weight=IQ4_NL --tensor-type blk.11.ffn_up.weight=IQ4_NL --tensor-type blk.12.attn_k.weight=IQ4_NL --tensor-type blk.12.attn_output.weight=IQ2_XXS --tensor-type blk.12.attn_q.weight=IQ4_NL --tensor-type blk.12.attn_v.weight=Q5_0 --tensor-type blk.12.ffn_down.weight=IQ2_S --tensor-type blk.12.ffn_gate.weight=IQ4_NL --tensor-type blk.12.ffn_up.weight=IQ4_NL --tensor-type blk.13.attn_k.weight=IQ4_NL --tensor-type blk.13.attn_output.weight=IQ2_XXS --tensor-type blk.13.attn_q.weight=IQ4_NL --tensor-type blk.13.attn_v.weight=Q5_0 --tensor-type blk.13.ffn_down.weight=IQ2_S --tensor-type blk.13.ffn_gate.weight=IQ4_NL --tensor-type blk.13.ffn_up.weight=IQ4_NL --tensor-type blk.14.attn_k.weight=IQ4_NL --tensor-type blk.14.attn_output.weight=IQ2_XXS --tensor-type blk.14.attn_q.weight=IQ4_NL --tensor-type blk.14.attn_v.weight=Q5_0 --tensor-type blk.14.ffn_down.weight=IQ2_S --tensor-type blk.14.ffn_gate.weight=IQ4_NL --tensor-type blk.14.ffn_up.weight=IQ4_NL --tensor-type blk.15.attn_k.weight=IQ4_NL --tensor-type blk.15.attn_output.weight=IQ2_XXS --tensor-type blk.15.attn_q.weight=IQ4_NL --tensor-type blk.15.attn_v.weight=Q5_0 --tensor-type blk.15.ffn_down.weight=IQ2_S --tensor-type blk.15.ffn_gate.weight=IQ4_NL --tensor-type blk.15.ffn_up.weight=IQ4_NL --tensor-type blk.16.attn_k.weight=IQ4_NL --tensor-type blk.16.attn_output.weight=IQ2_XXS --tensor-type blk.16.attn_q.weight=IQ4_NL --tensor-type blk.16.attn_v.weight=Q5_0 --tensor-type blk.16.ffn_down.weight=IQ1_S --tensor-type blk.16.ffn_gate.weight=IQ4_NL --tensor-type blk.16.ffn_up.weight=IQ4_NL --tensor-type blk.17.attn_k.weight=IQ4_NL --tensor-type blk.17.attn_output.weight=IQ2_XXS --tensor-type blk.17.attn_q.weight=IQ4_NL --tensor-type blk.17.attn_v.weight=Q5_0 --tensor-type blk.17.ffn_down.weight=IQ1_S --tensor-type blk.17.ffn_gate.weight=IQ4_NL --tensor-type blk.17.ffn_up.weight=IQ4_NL --tensor-type blk.18.attn_k.weight=IQ4_NL --tensor-type blk.18.attn_output.weight=IQ2_XXS --tensor-type blk.18.attn_q.weight=IQ4_NL --tensor-type blk.18.attn_v.weight=Q5_0 --tensor-type blk.18.ffn_down.weight=IQ1_S --tensor-type blk.18.ffn_gate.weight=IQ4_NL --tensor-type blk.18.ffn_up.weight=IQ4_NL --tensor-type blk.19.attn_k.weight=IQ4_NL --tensor-type blk.19.attn_output.weight=IQ2_XXS --tensor-type blk.19.attn_q.weight=IQ4_NL --tensor-type blk.19.attn_v.weight=Q5_0 --tensor-type blk.19.ffn_down.weight=IQ1_S --tensor-type blk.19.ffn_gate.weight=IQ4_NL --tensor-type blk.19.ffn_up.weight=IQ4_NL --tensor-type blk.20.attn_k.weight=IQ4_NL --tensor-type blk.20.attn_output.weight=IQ2_XXS --tensor-type blk.20.attn_q.weight=IQ4_NL --tensor-type blk.20.attn_v.weight=Q5_0 --tensor-type blk.20.ffn_down.weight=IQ1_S --tensor-type blk.20.ffn_gate.weight=IQ4_NL --tensor-type blk.20.ffn_up.weight=IQ4_NL --tensor-type blk.21.attn_k.weight=IQ4_NL --tensor-type blk.21.attn_output.weight=IQ2_XXS --tensor-type blk.21.attn_q.weight=IQ4_NL --tensor-type blk.21.attn_v.weight=Q5_0 --tensor-type blk.21.ffn_down.weight=IQ1_S --tensor-type blk.21.ffn_gate.weight=IQ4_NL --tensor-type blk.21.ffn_up.weight=IQ4_NL --tensor-type blk.22.attn_k.weight=IQ4_NL --tensor-type blk.22.attn_output.weight=IQ2_XXS --tensor-type blk.22.attn_q.weight=IQ4_NL --tensor-type blk.22.attn_v.weight=Q5_0 --tensor-type blk.22.ffn_down.weight=IQ1_S --tensor-type blk.22.ffn_gate.weight=IQ4_NL --tensor-type blk.22.ffn_up.weight=IQ4_NL --tensor-type blk.23.attn_k.weight=IQ4_NL --tensor-type blk.23.attn_output.weight=IQ2_XXS --tensor-type blk.23.attn_q.weight=IQ4_NL --tensor-type blk.23.attn_v.weight=Q5_0 --tensor-type blk.23.ffn_down.weight=IQ1_S --tensor-type blk.23.ffn_gate.weight=IQ4_NL --tensor-type blk.23.ffn_up.weight=IQ4_NL --tensor-type blk.24.attn_k.weight=IQ4_NL --tensor-type blk.24.attn_output.weight=IQ2_XXS --tensor-type blk.24.attn_q.weight=IQ4_NL --tensor-type blk.24.attn_v.weight=Q5_0 --tensor-type blk.24.ffn_down.weight=IQ1_S --tensor-type blk.24.ffn_gate.weight=IQ4_NL --tensor-type blk.24.ffn_up.weight=IQ4_NL --tensor-type blk.25.attn_k.weight=IQ4_NL --tensor-type blk.25.attn_output.weight=IQ2_XXS --tensor-type blk.25.attn_q.weight=IQ4_NL --tensor-type blk.25.attn_v.weight=Q5_0 --tensor-type blk.25.ffn_down.weight=IQ3_S --tensor-type blk.25.ffn_gate.weight=IQ4_NL --tensor-type blk.25.ffn_up.weight=IQ4_NL &amp;lt;input.gguf&amp;gt; &amp;lt;output.gguf&amp;gt; Q8_0&lt;/p&gt; &lt;p&gt;note that the Q8_0 at the end is just to get llama-quantize to do it's thing (F16/F32/COPY doesn't run quantization). all the tensors will be overridden with the actual --tensor-type params&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/terminoid_"&gt; /u/terminoid_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mes7rc/quantize_your_own_ggufs_the_same_way_as_your_fav/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mes7rc/quantize_your_own_ggufs_the_same_way_as_your_fav/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mes7rc/quantize_your_own_ggufs_the_same_way_as_your_fav/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-01T09:48:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1mexa2g</id>
    <title>Heads up to those that downloaded Qwen3 Coder 480B before yesterday</title>
    <updated>2025-08-01T14:00:47+00:00</updated>
    <author>
      <name>/u/VegetaTheGrump</name>
      <uri>https://old.reddit.com/user/VegetaTheGrump</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Mentioned in the new, Qwen3 30B download announcement was that 480B's tool calling was fixed and it &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1me31d8/qwen3coderflash_released/#:%7E:text=We%20also%20fixed%20tool%20calling%20for%20the%20480B%20and%20this%20model%20and%20fixed%2030B%20thinking%2C%20so%20please%20redownload%20the%20first%20shard"&gt;needed to be re-downloaded&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm just posting it so that no one misses it. I'm using LMStudio and it just showed as &amp;quot;downloaded&amp;quot;. It didn't seem to know there was a change.&lt;/p&gt; &lt;p&gt;EDIT: Yes, this only refers to the unsloth versions of 480B. Thank you &lt;a href="/u/MikeRoz"&gt;u/MikeRoz&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VegetaTheGrump"&gt; /u/VegetaTheGrump &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mexa2g/heads_up_to_those_that_downloaded_qwen3_coder/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mexa2g/heads_up_to_those_that_downloaded_qwen3_coder/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mexa2g/heads_up_to_those_that_downloaded_qwen3_coder/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-01T14:00:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1mesi2s</id>
    <title>GLM-4.5-Air running on 64GB Mac Studio(M4)</title>
    <updated>2025-08-01T10:05:19+00:00</updated>
    <author>
      <name>/u/riwritingreddit</name>
      <uri>https://old.reddit.com/user/riwritingreddit</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mesi2s/glm45air_running_on_64gb_mac_studiom4/"&gt; &lt;img alt="GLM-4.5-Air running on 64GB Mac Studio(M4)" src="https://preview.redd.it/87ng5bmisdgf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=54f42f44d09cb4df95a9f6ed8ad3cf70c2cc96bf" title="GLM-4.5-Air running on 64GB Mac Studio(M4)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I allocated more RAM and took the guard rail off. when loading the model the Activity monitor showed a brief red memory warning for 2-3 seconds but loads fine. The is 4bit version.Runs around 25-27 tokens/sec.When running inference memory pressure intermittently increases and it does use swap memory a around 1-12 GB in my case, but never showed red warning after loading it in memory.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/riwritingreddit"&gt; /u/riwritingreddit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/87ng5bmisdgf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mesi2s/glm45air_running_on_64gb_mac_studiom4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mesi2s/glm45air_running_on_64gb_mac_studiom4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-01T10:05:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1mepr38</id>
    <title>DocStrange - Open Source Document Data Extractor</title>
    <updated>2025-08-01T07:08:55+00:00</updated>
    <author>
      <name>/u/LostAmbassador6872</name>
      <uri>https://old.reddit.com/user/LostAmbassador6872</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mepr38/docstrange_open_source_document_data_extractor/"&gt; &lt;img alt="DocStrange - Open Source Document Data Extractor" src="https://preview.redd.it/vghke2r1ycgf1.gif?width=640&amp;amp;crop=smart&amp;amp;s=12643bc505cd05a85286b55a7fff556b82b4872a" title="DocStrange - Open Source Document Data Extractor" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sharing &lt;strong&gt;DocStrange&lt;/strong&gt;, an open-source Python library that makes document data extraction easy.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Universal Input&lt;/strong&gt;: PDFs, Images, Word docs, PowerPoint, Excel&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multiple Outputs&lt;/strong&gt;: Clean Markdown, structured JSON, CSV tables, formatted HTML&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Smart Extraction&lt;/strong&gt;: Specify exact fields you want (e.g., &amp;quot;invoice_number&amp;quot;, &amp;quot;total_amount&amp;quot;)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Schema Support&lt;/strong&gt;: Define JSON schemas for consistent structured output&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Quick start:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from docstrange import DocumentExtractor extractor = DocumentExtractor() result = extractor.extract(&amp;quot;research_paper.pdf&amp;quot;) # Get clean markdown for LLM training markdown = result.extract_markdown() &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;CLI&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pip install docstrange docstrange document.pdf --output json --extract-fields title author date &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Data Processing Options&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Cloud Mode&lt;/strong&gt;: Fast and free processing with minimal setup&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Local Mode&lt;/strong&gt;: Complete privacy - all processing happens on your machine, no data sent anywhere, works on both cpu and gpu&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;PyPI: &lt;a href="https://pypi.org/project/docstrange/"&gt;https://pypi.org/project/docstrange/&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LostAmbassador6872"&gt; /u/LostAmbassador6872 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vghke2r1ycgf1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mepr38/docstrange_open_source_document_data_extractor/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mepr38/docstrange_open_source_document_data_extractor/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-01T07:08:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1mf0qlf</id>
    <title>Qwen3-235B-A22B-2507 is the top open weights model on lmarena</title>
    <updated>2025-08-01T16:14:40+00:00</updated>
    <author>
      <name>/u/tarruda</name>
      <uri>https://old.reddit.com/user/tarruda</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tarruda"&gt; /u/tarruda &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/lmarena_ai/status/1951308670375174457"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mf0qlf/qwen3235ba22b2507_is_the_top_open_weights_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mf0qlf/qwen3235ba22b2507_is_the_top_open_weights_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-01T16:14:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1mepz8z</id>
    <title>OpenAI OS model info leaked - 120B &amp; 20B will be available</title>
    <updated>2025-08-01T07:23:36+00:00</updated>
    <author>
      <name>/u/ShreckAndDonkey123</name>
      <uri>https://old.reddit.com/user/ShreckAndDonkey123</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mepz8z/openai_os_model_info_leaked_120b_20b_will_be/"&gt; &lt;img alt="OpenAI OS model info leaked - 120B &amp;amp; 20B will be available" src="https://preview.redd.it/08m94pio0dgf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c8e423b8c1c16726ef958bbd8725e985cc58bc68" title="OpenAI OS model info leaked - 120B &amp;amp; 20B will be available" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ShreckAndDonkey123"&gt; /u/ShreckAndDonkey123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/08m94pio0dgf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mepz8z/openai_os_model_info_leaked_120b_20b_will_be/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mepz8z/openai_os_model_info_leaked_120b_20b_will_be/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-01T07:23:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1meu3jn</id>
    <title>Gemini 2.5 Deep Think mode benchmarks!</title>
    <updated>2025-08-01T11:36:06+00:00</updated>
    <author>
      <name>/u/Beautiful-Essay1945</name>
      <uri>https://old.reddit.com/user/Beautiful-Essay1945</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1meu3jn/gemini_25_deep_think_mode_benchmarks/"&gt; &lt;img alt="Gemini 2.5 Deep Think mode benchmarks!" src="https://preview.redd.it/8wnv6pme9egf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=557a01b879fc1bdbf0cc88dc3a91d0b4a7b1c10c" title="Gemini 2.5 Deep Think mode benchmarks!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Beautiful-Essay1945"&gt; /u/Beautiful-Essay1945 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8wnv6pme9egf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1meu3jn/gemini_25_deep_think_mode_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1meu3jn/gemini_25_deep_think_mode_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-01T11:36:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1mepeqh</id>
    <title>The OpenAI Open weight model might be 120B</title>
    <updated>2025-08-01T06:47:42+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mepeqh/the_openai_open_weight_model_might_be_120b/"&gt; &lt;img alt="The OpenAI Open weight model might be 120B" src="https://b.thumbs.redditmedia.com/uvWDYtCBC32T5YD2glI0V4HTGmyDJzZTUERWQkmJQoE.jpg" title="The OpenAI Open weight model might be 120B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The person who &amp;quot;leaked&amp;quot; this model is from the openai (HF) organization &lt;/p&gt; &lt;p&gt;So as expected, it's not gonna be something you can easily run locally, it won't hurt the chatgpt subscription business, you will need a dedicated LLM machine for that model &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mepeqh"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mepeqh/the_openai_open_weight_model_might_be_120b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mepeqh/the_openai_open_weight_model_might_be_120b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-01T06:47:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1meydpz</id>
    <title>The "Leaked" 120B OpenAI Model Is Trained In FP4</title>
    <updated>2025-08-01T14:44:49+00:00</updated>
    <author>
      <name>/u/Few_Painter_5588</name>
      <uri>https://old.reddit.com/user/Few_Painter_5588</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1meydpz/the_leaked_120b_openai_model_is_trained_in_fp4/"&gt; &lt;img alt="The &amp;quot;Leaked&amp;quot; 120B OpenAI Model Is Trained In FP4" src="https://preview.redd.it/vkstrhkm6fgf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3c6ba69aa1ef749b09c18c209f914b7320fb7a4e" title="The &amp;quot;Leaked&amp;quot; 120B OpenAI Model Is Trained In FP4" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Apparently someone on twitter managed to obtain the leaked weights, but has not been able to get the model to work. Since then, this repo has been privated&lt;/p&gt; &lt;p&gt;So the rough memory usage would be around 80GB of memory. If the 20B model is also trained at FP4, then that one would use about 10-14GB of memory when considering context.&lt;/p&gt; &lt;p&gt;If this model is truly Horizon-Alpha on OpenRouter, then OpenAI would be releasing a seriously powerful model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Few_Painter_5588"&gt; /u/Few_Painter_5588 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vkstrhkm6fgf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1meydpz/the_leaked_120b_openai_model_is_trained_in_fp4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1meydpz/the_leaked_120b_openai_model_is_trained_in_fp4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-01T14:44:49+00:00</published>
  </entry>
</feed>
