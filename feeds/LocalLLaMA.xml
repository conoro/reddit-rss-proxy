<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-01T17:06:38+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1loxw8f</id>
    <title>What is night forge?</title>
    <updated>2025-07-01T10:12:17+00:00</updated>
    <author>
      <name>/u/Professional-Ad-4376</name>
      <uri>https://old.reddit.com/user/Professional-Ad-4376</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1loxw8f/what_is_night_forge/"&gt; &lt;img alt="What is night forge?" src="https://a.thumbs.redditmedia.com/93U5v10Vycvd1XA2AyyAUDnfoGNgsP5NzRsHUeD4F_0.jpg" title="What is night forge?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/l4xe14k6m8af1.png?width=2920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c4e263c0717a28e4d5a85e0664b5e7bc8d144aec"&gt;https://preview.redd.it/l4xe14k6m8af1.png?width=2920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c4e263c0717a28e4d5a85e0664b5e7bc8d144aec&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I did a webdev arena, and one was very distinct in its style but I preferred it.&lt;/p&gt; &lt;p&gt;after voting for it, it said it was nightforge? I tried googling but couldn't find anything. Am I on the moon or whats going on?&lt;/p&gt; &lt;p&gt;Does anyone know what this is?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Professional-Ad-4376"&gt; /u/Professional-Ad-4376 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1loxw8f/what_is_night_forge/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1loxw8f/what_is_night_forge/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1loxw8f/what_is_night_forge/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T10:12:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1loza95</id>
    <title>Dual RX580 2048SP (16GB) llama.cpp(vulkan)</title>
    <updated>2025-07-01T11:33:27+00:00</updated>
    <author>
      <name>/u/IVequalsW</name>
      <uri>https://old.reddit.com/user/IVequalsW</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all! I have a server in my house with dual rx580 (16gb) in it, running llama.cpp via Vulkan. it runs the Qwen-3-32B-q5 (28GB total) at about 4.5 - 4.8 t/s. &lt;/p&gt; &lt;p&gt;does anyone want me to test any other ggufs? I could test it with 1 or both of the GPUs. &lt;/p&gt; &lt;p&gt;they work relatively well and are really cheap for a large amount of vram. Memory bus speed is about 256GB/s. &lt;/p&gt; &lt;p&gt;Give ideas in the comments&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IVequalsW"&gt; /u/IVequalsW &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1loza95/dual_rx580_2048sp_16gb_llamacppvulkan/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1loza95/dual_rx580_2048sp_16gb_llamacppvulkan/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1loza95/dual_rx580_2048sp_16gb_llamacppvulkan/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T11:33:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1lp4h7t</id>
    <title>I Designed an LLM Shorthand Based on Language Attributes, Math and Python</title>
    <updated>2025-07-01T15:22:31+00:00</updated>
    <author>
      <name>/u/cddelgado</name>
      <uri>https://old.reddit.com/user/cddelgado</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp4h7t/i_designed_an_llm_shorthand_based_on_language/"&gt; &lt;img alt="I Designed an LLM Shorthand Based on Language Attributes, Math and Python" src="https://external-preview.redd.it/S9o0IA1U3pGH6QHTYM7xe67F2sJ2lrkOFphUXXN9Wf0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=03423193e5f99a7b14385a26b16c8e15c8211b1a" title="I Designed an LLM Shorthand Based on Language Attributes, Math and Python" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From the Repo:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Fact-RAR is a symbolic mini-language for writing declarative knowledge in an &lt;strong&gt;LLM-friendly&lt;/strong&gt;, &lt;strong&gt;token-efficient&lt;/strong&gt;, and &lt;strong&gt;human-readable&lt;/strong&gt; format. (Some humans may find it tedious or dense.) It is a mini-language which was inspired by Japanese grammar, low-resource syntax, and programming idioms and syntax.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;I hope you find benefit from compressing your knowledge in a token-efficient format that LLMs apparently understand without prior knowledge of the spec.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cddelgado"&gt; /u/cddelgado &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/sidewaysthought/fact-rar"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp4h7t/i_designed_an_llm_shorthand_based_on_language/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lp4h7t/i_designed_an_llm_shorthand_based_on_language/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T15:22:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1lodmc6</id>
    <title>ERNIE 4.5 Collection from Baidu</title>
    <updated>2025-06-30T17:27:55+00:00</updated>
    <author>
      <name>/u/AppearanceHeavy6724</name>
      <uri>https://old.reddit.com/user/AppearanceHeavy6724</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AppearanceHeavy6724"&gt; /u/AppearanceHeavy6724 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ernie.baidu.com/blog/posts/ernie4.5/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lodmc6/ernie_45_collection_from_baidu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lodmc6/ernie_45_collection_from_baidu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T17:27:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1lol3na</id>
    <title>[Dataset] 4,000 hours of full-body, in-person, human face-to-face interaction videos</title>
    <updated>2025-06-30T22:20:44+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Dataset on Huggingface: &lt;a href="https://huggingface.co/datasets/facebook/seamless-interaction"&gt;https://huggingface.co/datasets/facebook/seamless-interaction&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.aidemos.meta.com/seamless_interaction_dataset"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lol3na/dataset_4000_hours_of_fullbody_inperson_human/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lol3na/dataset_4000_hours_of_fullbody_inperson_human/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T22:20:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1loo2u3</id>
    <title>Struggling with vLLM. The instructions make it sound so simple to run, but it‚Äôs like my Kryptonite. I give up.</title>
    <updated>2025-07-01T00:35:22+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm normally the guy they call in to fix the IT stuff nobody else can fix. I‚Äôll laser focus on whatever it is and figure it out probably 99% of the time. I‚Äôve been in IT for over 28+ years. I‚Äôve been messing with AI stuff for nearly 2 years now. Getting my Masters in AI right now. All that being said, I‚Äôve never encountered a more difficult software package to run than trying to get vLLM working in Docker. I can run nearly anything else in Docker except for vLLM. I feel like I‚Äôm really close, but every time I think it‚Äôs going to run, BAM! some new error that i find very little information on. - I‚Äôm running Ubuntu 24.04 - I have a 4090, 3090, and 64GB of RAM on AERO-D TRX50 motherboard. - Yes I have the Nvidia runtime container working - Yes I have the hugginface token generated is there an easy button somewhere that I‚Äôm missing? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1loo2u3/struggling_with_vllm_the_instructions_make_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1loo2u3/struggling_with_vllm_the_instructions_make_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1loo2u3/struggling_with_vllm_the_instructions_make_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T00:35:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1lp4cht</id>
    <title>Help on prompt memory and personas - what to do?</title>
    <updated>2025-07-01T15:17:37+00:00</updated>
    <author>
      <name>/u/TheRealKevinChrist</name>
      <uri>https://old.reddit.com/user/TheRealKevinChrist</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I need some recommendations on what to do to implement prompt/persona memory across my local setup. I've read up on vector databases and levels to set, but am looking for a step by step on which compoments to implement. I would love to have the solution self-hosted and local, and I am a full time AI user with 40% of my day job leveraging this day-to-day.&lt;/p&gt; &lt;p&gt;Currently running an NVIDIA P40 with 24GB of vRAM in an Ubuntu 24.04 server with Docker (64GB memory, AMD 5800X). I currently use Big-AGI as my front end with Ollama (willing to change this up). I have a GGUF for Gemma 32B to allow for large token sets, but again, willing to change that.&lt;/p&gt; &lt;p&gt;Any suggestions to implement prompt/persona memory across this? Thanks!&lt;/p&gt; &lt;p&gt;Edit 1: I am looking at &lt;a href="https://github.com/n8n-io"&gt;https://github.com/n8n-io&lt;/a&gt; which seems to provide a lot of this, but would love some suggestions here.&lt;/p&gt; &lt;p&gt;Edit 2: Further context on my desired state: I currently prompt-based RAG per prompt 'chain', where I add my private documents to a thread for context. This becomes cumbersome &lt;em&gt;across&lt;/em&gt; prompts, and I need more of a persona that can learn across common threads.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheRealKevinChrist"&gt; /u/TheRealKevinChrist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp4cht/help_on_prompt_memory_and_personas_what_to_do/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp4cht/help_on_prompt_memory_and_personas_what_to_do/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lp4cht/help_on_prompt_memory_and_personas_what_to_do/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T15:17:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1lococc</id>
    <title>Open Source AI Editor: First Milestone</title>
    <updated>2025-06-30T16:52:52+00:00</updated>
    <author>
      <name>/u/isidor_n</name>
      <uri>https://old.reddit.com/user/isidor_n</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lococc/open_source_ai_editor_first_milestone/"&gt; &lt;img alt="Open Source AI Editor: First Milestone" src="https://external-preview.redd.it/7W6FU5na7gC1vKxg3pWb3QkD0a8T5GyzeaLh8U3roNc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d188c22d72aa036de764ff96aa9d951cba5ae6b3" title="Open Source AI Editor: First Milestone" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Let me know if you have any questions about open sourcing. Happy to answer. &lt;/p&gt; &lt;p&gt;vscode pm here&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/isidor_n"&gt; /u/isidor_n &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://code.visualstudio.com/blogs/2025/06/30/openSourceAIEditorFirstMilestone"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lococc/open_source_ai_editor_first_milestone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lococc/open_source_ai_editor_first_milestone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T16:52:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1lp0j7f</id>
    <title>Best open source Arabic tts</title>
    <updated>2025-07-01T12:36:32+00:00</updated>
    <author>
      <name>/u/Spiritual_Button827</name>
      <uri>https://old.reddit.com/user/Spiritual_Button827</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I‚Äôve been trying to find the best TTS options to fine tune for Arabic and I‚Äôve kinda hit a wall with Fish audio after their release of the new S1 model, as they‚Äôve removed the fine tuning code for older models like v1.5.&lt;/p&gt; &lt;p&gt;I tried coqui‚Äôs XTTS fork by Idap: &lt;a href="https://github.com/idiap/coqui-ai-TTS"&gt;https://github.com/idiap/coqui-ai-TTS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And got good results, but I would like to try other good options.&lt;/p&gt; &lt;p&gt;I looked at &lt;a href="https://huggingface.co/spaces/TTS-AGI/TTS-Arena"&gt;https://huggingface.co/spaces/TTS-AGI/TTS-Arena&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And I see that not many options support Arabic.&lt;/p&gt; &lt;p&gt;My use case is: real time inference of Arabic text for an interactive chatbot&lt;/p&gt; &lt;p&gt;I‚Äôm kinda new to TTS and would appreciate any help/advice.&lt;/p&gt; &lt;p&gt;I have a good server in hand with lots of compute to test anything so any open source model with fine tuning code available and can support Arabic is welcome&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Spiritual_Button827"&gt; /u/Spiritual_Button827 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp0j7f/best_open_source_arabic_tts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp0j7f/best_open_source_arabic_tts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lp0j7f/best_open_source_arabic_tts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T12:36:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1lp6def</id>
    <title>Anyone experimenting with local multi-modal LLaMA or RAG pipelines? Curious about integration strategies.</title>
    <updated>2025-07-01T16:34:44+00:00</updated>
    <author>
      <name>/u/No_Edge2098</name>
      <uri>https://old.reddit.com/user/No_Edge2098</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In order to achieve a fully offline, multi-modal solution, I'm constructing a local RAG pipeline using LLaMA (7B/13B) and integrating it with vector DBs such as Faiss/Chroma for domain-specific document QA. &lt;/p&gt; &lt;p&gt;Seeking to gain knowledge from those who are trying with:Multimodal input (using CLIP/BLIP to add photos and PDFs) &lt;/p&gt; &lt;p&gt;Fine-tuning LoRA on retrieved chunks (in contrast to the entire corpus)Prior to LLaMA inference, intelligent chunking and compression &lt;/p&gt; &lt;p&gt;Effective loaders (llama.cpp, exllama, and vLLM)Motivating tactics for multi-modal and structured contexts &lt;/p&gt; &lt;p&gt;Contextual restrictions, modality drift, and hallucinations from vaguely related retrievals are the main obstacles. &lt;/p&gt; &lt;p&gt;If you're creating comparable setups locally, let's exchange notes. üöÄ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Edge2098"&gt; /u/No_Edge2098 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp6def/anyone_experimenting_with_local_multimodal_llama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp6def/anyone_experimenting_with_local_multimodal_llama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lp6def/anyone_experimenting_with_local_multimodal_llama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T16:34:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1lp5obe</id>
    <title>Using llama.cpp in an enterprise?</title>
    <updated>2025-07-01T16:08:21+00:00</updated>
    <author>
      <name>/u/Careless-Car_</name>
      <uri>https://old.reddit.com/user/Careless-Car_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Pretty much the title!&lt;/p&gt; &lt;p&gt;Does anyone have examples of llama.cpp being used in a form of enterprise/business context successfully?&lt;/p&gt; &lt;p&gt;I see vLLM used at scale everywhere, so it would be cool to see any use cases that leverage laptops/lower-end hardware towards their benefit!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Careless-Car_"&gt; /u/Careless-Car_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp5obe/using_llamacpp_in_an_enterprise/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp5obe/using_llamacpp_in_an_enterprise/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lp5obe/using_llamacpp_in_an_enterprise/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T16:08:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1lp5pt0</id>
    <title>Day 7/50: Building a Small Language Model from Scratch ‚Äì Coding Positional Embeddings</title>
    <updated>2025-07-01T16:09:54+00:00</updated>
    <author>
      <name>/u/Prashant-Lakhera</name>
      <uri>https://old.reddit.com/user/Prashant-Lakhera</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Yesterday, we discussed &lt;em&gt;what&lt;/em&gt; &lt;a href="https://www.ideaweaver.ai/blog/day6.html"&gt;positional embeddings &lt;/a&gt;are and &lt;em&gt;why&lt;/em&gt; they‚Äôre essential in Transformer models. Today, let‚Äôs jump into the code and see exactly how they're implemented.&lt;/p&gt; &lt;p&gt;The reference implementation comes from an open-source GPT-style model I‚Äôve been experimenting with &lt;a href="https://github.com/ideaweaver-ai/Tiny-Children-Stories-30M-model"&gt;Tiny Children Stories 30M&lt;/a&gt;. It's designed to generate short children's stories and offers a clean, minimal setup perfect for understanding the internals.&lt;/p&gt; &lt;h1&gt;Quick Recap: Why Transformers Need Positional Embeddings&lt;/h1&gt; &lt;p&gt;Transformer models process all tokens in parallel (unlike RNNs), so they don‚Äôt naturally understand word order. For example:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;quot;The cat sat on the mat&amp;quot; &amp;quot;The mat sat on the cat&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To a transformer without positional embeddings, those look identical, same tokens, shuffled order, same representation. That‚Äôs a problem.&lt;/p&gt; &lt;h1&gt;What Are Positional Embeddings?&lt;/h1&gt; &lt;p&gt;They‚Äôre additional vectors that encode the &lt;em&gt;position&lt;/em&gt; of each token in the sequence. These are added to token embeddings so that the model knows what the token is and where it is located.&lt;/p&gt; &lt;h1&gt;&lt;a href="https://github.com/ideaweaver-ai/Tiny-Children-Stories-30M-model/blob/main/src/model/gpt.py"&gt;Step-by-Step Code Walkthrough&lt;/a&gt;&lt;/h1&gt; &lt;h1&gt;1. Model Config&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;u/dataclass class GPTConfig: vocab_size: int = 50257 block_size: int = 1024 n_layer: int = 6 n_head: int = 8 n_embd: int = 512 dropout: float = 0.1 bias: bool = True &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;code&gt;block_size&lt;/code&gt; defines the maximum sequence length and thus the number of positional embeddings needed.&lt;/p&gt; &lt;h1&gt;2. Defining the Embedding Layers&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;self.transformer = nn.ModuleDict(dict( wte=nn.Embedding(config.vocab_size, config.n_embd), # token embeddings wpe=nn.Embedding(config.block_size, config.n_embd), # positional embeddings ... )) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Both embeddings are of shape &lt;code&gt;(sequence_length, embedding_dim)&lt;/code&gt;, so they can be added together.&lt;/p&gt; &lt;h1&gt;3. Forward Pass&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;pos = torch.arange(0, t, dtype=torch.long, device=device) tok_emb = self.transformer.wte(idx) pos_emb = self.transformer.wpe(pos) x = self.transformer.drop(tok_emb + pos_emb) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This does:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Generate position indices &lt;code&gt;[0, 1, 2, ..., t-1]&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Look up token and position embeddings&lt;/li&gt; &lt;li&gt;Add them&lt;/li&gt; &lt;li&gt;Apply dropout&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Example&lt;/h1&gt; &lt;p&gt;Input: &lt;code&gt;&amp;quot;The cat sat&amp;quot;&lt;/code&gt;&lt;br /&gt; Token IDs: &lt;code&gt;[464, 2368, 3290]&lt;/code&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Token&lt;/th&gt; &lt;th align="left"&gt;Token Embedding&lt;/th&gt; &lt;th align="left"&gt;Positional Embedding&lt;/th&gt; &lt;th align="left"&gt;Combined Embedding&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;The&lt;/td&gt; &lt;td align="left"&gt;&lt;code&gt;[0.1, -0.3, ‚Ä¶]&lt;/code&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;code&gt;[0.0, 0.1, ‚Ä¶]&lt;/code&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;code&gt;[0.1, -0.2, ‚Ä¶]&lt;/code&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;cat&lt;/td&gt; &lt;td align="left"&gt;&lt;code&gt;[0.5, 0.2, ‚Ä¶]&lt;/code&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;code&gt;[0.1, 0.0, ‚Ä¶]&lt;/code&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;code&gt;[0.6, 0.2, ‚Ä¶]&lt;/code&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;sat&lt;/td&gt; &lt;td align="left"&gt;&lt;code&gt;[-0.2, 0.8, ‚Ä¶]&lt;/code&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;code&gt;[0.2, -0.1, ‚Ä¶]&lt;/code&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;code&gt;[0.0, 0.7, ‚Ä¶]&lt;/code&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Now the model knows both the identity and the order of the tokens.&lt;/p&gt; &lt;h1&gt;Now the question is why This Matters&lt;/h1&gt; &lt;p&gt;By adding token + position, the model learns:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Semantics&lt;/strong&gt; (what the word is)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Context&lt;/strong&gt; (where the word is)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This is crucial in generation tasks like storytelling, where position changes meaning.&lt;/p&gt; &lt;h1&gt;Limitations&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Fixed length&lt;/strong&gt;: Can‚Äôt handle sequences longer than &lt;code&gt;block_size&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;No relative awareness&lt;/strong&gt;: Doesn't know how far two tokens are apart.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Sparse training&lt;/strong&gt;: If you never train on long sequences, performance drops.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Alternatives&lt;/h1&gt; &lt;h1&gt;Sinusoidal Positional Embeddings&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;def get_sinusoidal_embeddings(seq_len, embed_dim): pos = torch.arange(seq_len).unsqueeze(1) div_term = torch.exp(torch.arange(0, embed_dim, 2) * -(math.log(10000.0) / embed_dim)) pe = torch.zeros(seq_len, embed_dim) pe[:, 0::2] = torch.sin(pos * div_term) pe[:, 1::2] = torch.cos(pos * div_term) return pe &lt;/code&gt;&lt;/pre&gt; &lt;ul&gt; &lt;li&gt;Infinite length&lt;/li&gt; &lt;li&gt;No learned parameters&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Relative Positional Embeddings&lt;/h1&gt; &lt;p&gt;Rather than saying &amp;quot;this is position 5&amp;quot;, you tell the model &amp;quot;this token is 3 positions to the left of that one.&amp;quot;&lt;/p&gt; &lt;p&gt;Great for:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Reasoning&lt;/li&gt; &lt;li&gt;Long document understanding&lt;/li&gt; &lt;li&gt;Question answering&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Tips&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Don‚Äôt overextend &lt;code&gt;block_size,&lt;/code&gt; it increases memory consumption fast.&lt;/li&gt; &lt;li&gt;Ensure your training data has diverse sequence lengths.&lt;/li&gt; &lt;li&gt;For long inputs, check out RoPE or relative embeddings.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Final Thoughts&lt;/h1&gt; &lt;p&gt;Positional embeddings are the quiet workhorses of transformer models. Just by combining two vectors (token + position), we enable the model to process ordered text meaningfully.&lt;/p&gt; &lt;p&gt;Without this, a model wouldn't know if ‚ÄúThe End‚Äù belongs at the start or the finish of your story.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Coming Up Next:&lt;/strong&gt;&lt;br /&gt; Tomorrow we‚Äôll dive into Rotary Positional Embeddings (RoPE), a more scalable and elegant solution to position encoding.&lt;/p&gt; &lt;p&gt;If you're following this series, feel free to share or &lt;a href="https://www.linkedin.com/in/prashant-lakhera-696119b/"&gt;connect&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prashant-Lakhera"&gt; /u/Prashant-Lakhera &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp5pt0/day_750_building_a_small_language_model_from/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp5pt0/day_750_building_a_small_language_model_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lp5pt0/day_750_building_a_small_language_model_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T16:09:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1loswvr</id>
    <title>New to the scene. Yesterday, got 4 t/s on R1 671b q4. Today, I'm getting about 0.15 t/s... What did I break lol</title>
    <updated>2025-07-01T04:46:12+00:00</updated>
    <author>
      <name>/u/sourpatchgrownadults</name>
      <uri>https://old.reddit.com/user/sourpatchgrownadults</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;5975wx, 512gb DDR4 3200, dual 3090s. Ollama + OpenWebUI. Running on LMDE.&lt;/p&gt; &lt;p&gt;Idk what went wrong now but I'm struggling to get it back to 4 t/s... I can work with 4 t/s, but 0.15 t/s is just terrible.&lt;/p&gt; &lt;p&gt;Any ideas? Happy to provide information upon request.&lt;/p&gt; &lt;p&gt;Total noob here, just built this a few days ago and very little terminal experience lol but have an open mind and a will to learn.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sourpatchgrownadults"&gt; /u/sourpatchgrownadults &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1loswvr/new_to_the_scene_yesterday_got_4_ts_on_r1_671b_q4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1loswvr/new_to_the_scene_yesterday_got_4_ts_on_r1_671b_q4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1loswvr/new_to_the_scene_yesterday_got_4_ts_on_r1_671b_q4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T04:46:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1lok3r2</id>
    <title>[News] Datacenter GPUs May Have an Astonishingly Short Lifespan of Only 1 to 3 Years | TrendForce News</title>
    <updated>2025-06-30T21:40:05+00:00</updated>
    <author>
      <name>/u/EasternBeyond</name>
      <uri>https://old.reddit.com/user/EasternBeyond</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lok3r2/news_datacenter_gpus_may_have_an_astonishingly/"&gt; &lt;img alt="[News] Datacenter GPUs May Have an Astonishingly Short Lifespan of Only 1 to 3 Years | TrendForce News" src="https://external-preview.redd.it/7cRnC2dFTB8VTd7qs9tim3BVul_HOXlhVu97BYC8mXw.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cfea0e06944005f53398ccc99f53814a8c4923f4" title="[News] Datacenter GPUs May Have an Astonishingly Short Lifespan of Only 1 to 3 Years | TrendForce News" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EasternBeyond"&gt; /u/EasternBeyond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.trendforce.com/news/2024/10/31/news-datacenter-gpus-may-have-an-astonishingly-short-lifespan-of-only-1-to-3-years/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lok3r2/news_datacenter_gpus_may_have_an_astonishingly/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lok3r2/news_datacenter_gpus_may_have_an_astonishingly/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T21:40:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1lokp88</id>
    <title>Intel Arc Pro B60 Dual 48G Turbo Maxsun GPU Pricing Revealed</title>
    <updated>2025-06-30T22:04:32+00:00</updated>
    <author>
      <name>/u/Airwalker19</name>
      <uri>https://old.reddit.com/user/Airwalker19</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Like many others, I was hyped for the dual GPU Intel Arc Pro B60, so I emailed Maxsun for a quote. Their US distributor hit me back with $5k per unit for 3 GPUs, or $4.5k each for 5+.&lt;/p&gt; &lt;p&gt;Sure, dual GPUs should cost more, but this is &lt;em&gt;10x&lt;/em&gt; the rumored MSRP of the 24GB card. Space savings are nice, but not &lt;em&gt;that&lt;/em&gt; nice.&lt;/p&gt; &lt;p&gt;RIP my hopes for an (affordable) AI desktop win.&lt;/p&gt; &lt;p&gt;Anyone else think this pricing is delusional, or just me?&lt;/p&gt; &lt;p&gt;UPDATE:&lt;/p&gt; &lt;p&gt;Here's a screenshot of the email &lt;a href="https://imgur.com/a/Qh1nYb1"&gt;https://imgur.com/a/Qh1nYb1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I also talked on the phone with a rep and talked him down to $3,800 for 4 units. 5+ units down to $3,000. Still not worth it if the $500 price point for the 24GB cards are to be believed.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Airwalker19"&gt; /u/Airwalker19 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lokp88/intel_arc_pro_b60_dual_48g_turbo_maxsun_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lokp88/intel_arc_pro_b60_dual_48g_turbo_maxsun_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lokp88/intel_arc_pro_b60_dual_48g_turbo_maxsun_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T22:04:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1lom2r9</id>
    <title>With the OpenAI employees that Meta hired, do you think this will be positive for local models?</title>
    <updated>2025-06-30T23:02:37+00:00</updated>
    <author>
      <name>/u/LarDark</name>
      <uri>https://old.reddit.com/user/LarDark</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lom2r9/with_the_openai_employees_that_meta_hired_do_you/"&gt; &lt;img alt="With the OpenAI employees that Meta hired, do you think this will be positive for local models?" src="https://preview.redd.it/ymsyhfb2b5af1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6adc725dda988a88523c2dd76383f72148e4d67a" title="With the OpenAI employees that Meta hired, do you think this will be positive for local models?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I mean, if these people hired were so important to developing powerful and important OpenAI models. Hopefully the next Llama models will be much better than Llama 4... and raise the bar like Llama did before.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LarDark"&gt; /u/LarDark &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ymsyhfb2b5af1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lom2r9/with_the_openai_employees_that_meta_hired_do_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lom2r9/with_the_openai_employees_that_meta_hired_do_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T23:02:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1lp2ji0</id>
    <title>Reasoning models are risky. Anyone else experiencing this?</title>
    <updated>2025-07-01T14:04:52+00:00</updated>
    <author>
      <name>/u/interviuu</name>
      <uri>https://old.reddit.com/user/interviuu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm building a job application tool and have been testing pretty much every LLM model out there for different parts of the product. One thing that's been driving me crazy: reasoning models seem particularly dangerous for business applications that need to go from A to B in a somewhat rigid way.&lt;/p&gt; &lt;p&gt;I wouldn't call it &amp;quot;deterministic output&amp;quot; because that's not really what LLMs do, but there are definitely use cases where you need a certain level of consistency and predictability, you know?&lt;/p&gt; &lt;p&gt;Here's what I keep running into with reasoning models:&lt;/p&gt; &lt;p&gt;During the reasoning process (and I know Anthropic has shown that what we read isn't the &amp;quot;real&amp;quot; reasoning happening), the LLM tends to ignore guardrails and specific instructions I've put in the prompt. The output becomes way more unpredictable than I need it to be.&lt;/p&gt; &lt;p&gt;Sure, I can define the format with JSON schemas (or objects) and that works fine. But the actual content? It's all over the place. Sometimes it follows my business rules perfectly, other times it just doesn't. And there's no clear pattern I can identify.&lt;/p&gt; &lt;p&gt;For example, I need the model to extract specific information from resumes and job posts, then match them according to pretty clear criteria. With regular models, I get consistent behavior most of the time. With reasoning models, it's like they get &amp;quot;creative&amp;quot; during their internal reasoning and decide my rules are more like suggestions.&lt;/p&gt; &lt;p&gt;I've tested almost all of them (from Gemini to DeepSeek) and honestly, none have convinced me for this type of structured business logic. They're incredible for complex problem-solving, but for &amp;quot;follow these specific steps and don't deviate&amp;quot; tasks? Not so much.&lt;/p&gt; &lt;p&gt;Anyone else dealing with this? Am I missing something in my prompting approach, or is this just the trade-off we make with reasoning models? I'm curious if others have found ways to make them more reliable for business applications.&lt;/p&gt; &lt;p&gt;What's been your experience with reasoning models in production?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/interviuu"&gt; /u/interviuu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp2ji0/reasoning_models_are_risky_anyone_else/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp2ji0/reasoning_models_are_risky_anyone_else/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lp2ji0/reasoning_models_are_risky_anyone_else/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T14:04:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1lovuxp</id>
    <title>Current state of Intel A770 16GB GPU for Inference?</title>
    <updated>2025-07-01T07:55:04+00:00</updated>
    <author>
      <name>/u/Karim_acing_it</name>
      <uri>https://old.reddit.com/user/Karim_acing_it</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;I could only find old posts regarding how the Intel A770 fares with LLMs, specifically people notice the high idle power consumption and difficult setup depending on what framework you use. At least a year ago, it was supposed to be a pain to use with Ollama.&lt;/p&gt; &lt;p&gt;Here in Germany, it is by far the cheapest 16GB card, in summary:&lt;br /&gt; - Intel A770, prices starting at 280-300‚Ç¨&lt;br /&gt; - AMD 9060 XT starting at 370‚Ç¨ (+32%)&lt;br /&gt; - Nvidia RTX 5060 Ti starting at 440‚Ç¨ (+57%)&lt;/p&gt; &lt;p&gt;Price-wise the A770 is a no-brainer, but what is your current experience? Currently using an RTX 4060 8GB and LMStudio on Windows 11 (+32GB DDR5).&lt;/p&gt; &lt;p&gt;Thanks for any insights&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Karim_acing_it"&gt; /u/Karim_acing_it &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lovuxp/current_state_of_intel_a770_16gb_gpu_for_inference/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lovuxp/current_state_of_intel_a770_16gb_gpu_for_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lovuxp/current_state_of_intel_a770_16gb_gpu_for_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T07:55:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1lojlrw</id>
    <title>[WIRED] Here Is Everyone Mark Zuckerberg Has Hired So Far for Meta‚Äôs ‚ÄòSuperintelligence‚Äô Team</title>
    <updated>2025-06-30T21:19:51+00:00</updated>
    <author>
      <name>/u/bllshrfv</name>
      <uri>https://old.reddit.com/user/bllshrfv</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lojlrw/wired_here_is_everyone_mark_zuckerberg_has_hired/"&gt; &lt;img alt="[WIRED] Here Is Everyone Mark Zuckerberg Has Hired So Far for Meta‚Äôs ‚ÄòSuperintelligence‚Äô Team" src="https://external-preview.redd.it/hHtdtWWuX05qlxJIIZFgrRzaMxdrmlIQ8OiqTPog1_w.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e97f33d6160ce6f067a79278cab0942d295e3325" title="[WIRED] Here Is Everyone Mark Zuckerberg Has Hired So Far for Meta‚Äôs ‚ÄòSuperintelligence‚Äô Team" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bllshrfv"&gt; /u/bllshrfv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.wired.com/story/mark-zuckerberg-welcomes-superintelligence-team/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lojlrw/wired_here_is_everyone_mark_zuckerberg_has_hired/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lojlrw/wired_here_is_everyone_mark_zuckerberg_has_hired/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T21:19:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1lotza5</id>
    <title>KrunchWrapper - a LLM compression proxy (beta)</title>
    <updated>2025-07-01T05:51:25+00:00</updated>
    <author>
      <name>/u/LA_rent_Aficionado</name>
      <uri>https://old.reddit.com/user/LA_rent_Aficionado</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lotza5/krunchwrapper_a_llm_compression_proxy_beta/"&gt; &lt;img alt="KrunchWrapper - a LLM compression proxy (beta)" src="https://preview.redd.it/c4bjroisb7af1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a7b39a5201c024ced3ca9aba3ebe3b3090ade2d9" title="KrunchWrapper - a LLM compression proxy (beta)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With context limits being the way there are I wanted to experiment with creating a standalone middleman API server that &amp;quot;compresses&amp;quot; requests sent to models as a proof of concept. I've seen other methods employed that use a seperate model for compression but, Krunchwrapper completely avoids the need for running a model as an intermediary - which I find particularly in VRAM constrained environments. With KrunchWrapper I wanted to avoid this dependency and instead rely on local processing to identify areas for compression and pass a &amp;quot;decoder&amp;quot; to the LLM via a system prompt.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Github Link&lt;/strong&gt;: &lt;a href="https://github.com/thad0ctor/KrunchWrapper"&gt;https://github.com/thad0ctor/KrunchWrapper&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The server runs on Python 3.12 from its own venv and curently works on both Linux and Windows (mostly tested on linux but I did a few runs on windows). Currently, I have tested it to work on its own embedded WebUI (thank you llama.cpp), SillyTavern and with Cline interfacing with a locally hosted OpenAI compatible server. I also have support for using Cline with the Anthropic API.&lt;/p&gt; &lt;p&gt;Between compression and (optional) comment stripping, &lt;strong&gt;I have been able to acheive &amp;gt;40% compression when passing code files to the LLM that contain lots of repetition.&lt;/strong&gt; So far I haven't had any issues with fairly smart models like Qwen3 (14B, 32B, 235B) and Gemma3 understanding and adhering to the compression instructions.&lt;/p&gt; &lt;p&gt;At its core, what KrunchWrapper essentially does is:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Receive:&lt;/strong&gt; Establishes a proxy server that &amp;quot;intercepts&amp;quot; prompts going to a LLM server&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Analyze:&lt;/strong&gt; Analyzes those prompts for common patterns of text&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Assign:&lt;/strong&gt; Maps a unicode symbol (known to use fewer tokens) to that pattern of text &lt;ol&gt; &lt;li&gt;Analyzes whether savings &amp;gt; system prompt overhead&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Compress:&lt;/strong&gt; Replaces all identified patterns of text with the selected symbol(s) &lt;ol&gt; &lt;li&gt; Preserves JSON, markdown, tool calls&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Intercept:&lt;/strong&gt; Passes a system prompt with the compression decoder to the LLM along with the compressed message&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Instruct:&lt;/strong&gt; Instucts the LLM to use the compressed symbols in any response&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Decompress:&lt;/strong&gt; Decodes any responses received from the LLM that contain the compressed symbols&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Repeat:&lt;/strong&gt; Intilligently adds to and re-uses any compression dictionaries in follow-on messages&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Beyond the basic functionality there is a wide range of customization and documentation to explain the settings to fine tune compression to your individual needs. For example: users can defer compression to subsequent messages if they intended to provide other files and not &amp;quot;waste&amp;quot; compression tokens on minimal impact compression opportunities.&lt;/p&gt; &lt;p&gt;Looking ahead, I would like to expand this for other popular tools like Roo, Aider, etc. and other APIs. I beleive this could really help save on API costs once expanded.I also did some initial testing with Cursor but given it is proprietary nature and that its requests are encrypted with SSL a lot more work needs to be done to properly intercept its traffic to apply compression for non-local API requests.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Disclaimers:&lt;/strong&gt; I am not a programmer by trade. I refuse to use the v-word I so often see on here but let's just say I could have never even attempted this without agentic coding and API invoice payments flying out the door. This is reflected in the code. I have done my best to employ best practices and not have this be some spaghetti code quagmire but to say this tool is production ready would be an insult to every living software engineer - I would like to stress how Beta this is - like Tarkov 2016, not Tarkov 2025.&lt;/p&gt; &lt;p&gt;This type of compression does not come without latency. Be sure to change the thread settings in the configs to maximize throughput. That said, there is a cost to using less context by means of an added processing delay. Lastly, I highly recommend not turning on DEBUG and verbose logging in your terminal output... seriously.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LA_rent_Aficionado"&gt; /u/LA_rent_Aficionado &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/c4bjroisb7af1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lotza5/krunchwrapper_a_llm_compression_proxy_beta/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lotza5/krunchwrapper_a_llm_compression_proxy_beta/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T05:51:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1lp37v0</id>
    <title>LoRA training on NVIDIA Jetson AGX Orin 64GB</title>
    <updated>2025-07-01T14:32:48+00:00</updated>
    <author>
      <name>/u/ahstanin</name>
      <uri>https://old.reddit.com/user/ahstanin</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp37v0/lora_training_on_nvidia_jetson_agx_orin_64gb/"&gt; &lt;img alt="LoRA training on NVIDIA Jetson AGX Orin 64GB" src="https://b.thumbs.redditmedia.com/oh8UHfFWv9AwkzOmulxotDC0dlTYauybGEDMEiEkogE.jpg" title="LoRA training on NVIDIA Jetson AGX Orin 64GB" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/sye5ssnxv9af1.png?width=3456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5d312de9207cf7cd5edd21029849b10a3b23bbb5"&gt;https://preview.redd.it/sye5ssnxv9af1.png?width=3456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5d312de9207cf7cd5edd21029849b10a3b23bbb5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/dy46sdb5x9af1.png?width=3456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=afe062e1e653c8467fb97741a2b2591a467e2c3d"&gt;https://preview.redd.it/dy46sdb5x9af1.png?width=3456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=afe062e1e653c8467fb97741a2b2591a467e2c3d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I successfully ran LoRA training on an NVIDIA Jetson AGX Orin 64GB. Both 8-bit and FP16 modes are working. I'm currently training the Qwen 2.5 7B model. Although the process is slow, it's sufficient for my needs since there's no urgency.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ahstanin"&gt; /u/ahstanin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp37v0/lora_training_on_nvidia_jetson_agx_orin_64gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp37v0/lora_training_on_nvidia_jetson_agx_orin_64gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lp37v0/lora_training_on_nvidia_jetson_agx_orin_64gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T14:32:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1lp2h0e</id>
    <title>Training and Finetuning Sparse Embedding Models with Sentence Transformers v5</title>
    <updated>2025-07-01T14:02:02+00:00</updated>
    <author>
      <name>/u/-Cubie-</name>
      <uri>https://old.reddit.com/user/-Cubie-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp2h0e/training_and_finetuning_sparse_embedding_models/"&gt; &lt;img alt="Training and Finetuning Sparse Embedding Models with Sentence Transformers v5" src="https://external-preview.redd.it/Q7oEnpq4LYUPvgpkKMeoddSo-4Wn8UDKMbqnVIBZL8s.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2eca6467642c913566d063063339907e970775c0" title="Training and Finetuning Sparse Embedding Models with Sentence Transformers v5" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sentence Transformers v5.0 was just released, and it introduced sparse embedding models. These are the kind of search models that are often combined with the &amp;quot;standard&amp;quot; dense embedding models for &amp;quot;hybrid search&amp;quot;. On paper, this can help performance a lot. From the release notes:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;A big question is: How do sparse embedding models stack up against the ‚Äústandard‚Äù dense embedding models, and what kind of performance can you expect when combining various?&lt;/p&gt; &lt;p&gt;For this, I ran a variation of our &lt;a href="https://github.com/UKPLab/sentence-transformers/blob/master/examples/sparse_encoder/applications/retrieve_rerank/hybrid_search.py"&gt;hybrid_search.py&lt;/a&gt; evaluation script, with:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The &lt;a href="https://huggingface.co/datasets/zeta-alpha-ai/NanoMSMARCO"&gt;NanoMSMARCO&lt;/a&gt; dataset (a subset of the MS MARCO eval split)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-Embedding-0.6B"&gt;Qwen/Qwen3-Embedding-0.6B&lt;/a&gt; dense embedding model&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/naver/splade-v3-doc"&gt;naver/splade-v3-doc&lt;/a&gt; sparse embedding model, inference free for queries&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Alibaba-NLP/gte-reranker-modernbert-base"&gt;Alibaba-NLP/gte-reranker-modernbert-base&lt;/a&gt; reranker&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Which resulted in this evaluation:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Dense&lt;/th&gt; &lt;th&gt;Sparse&lt;/th&gt; &lt;th&gt;Reranker&lt;/th&gt; &lt;th&gt;NDCG@10&lt;/th&gt; &lt;th&gt;MRR@10&lt;/th&gt; &lt;th&gt;MAP&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;x&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;65.33&lt;/td&gt; &lt;td&gt;57.56&lt;/td&gt; &lt;td&gt;57.97&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;x&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;67.34&lt;/td&gt; &lt;td&gt;59.59&lt;/td&gt; &lt;td&gt;59.98&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;x&lt;/td&gt; &lt;td&gt;x&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;72.39&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;66.99&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;67.59&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;x&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;x&lt;/td&gt; &lt;td&gt;68.37&lt;/td&gt; &lt;td&gt;62.76&lt;/td&gt; &lt;td&gt;63.56&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;x&lt;/td&gt; &lt;td&gt;x&lt;/td&gt; &lt;td&gt;69.02&lt;/td&gt; &lt;td&gt;63.66&lt;/td&gt; &lt;td&gt;64.44&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;x&lt;/td&gt; &lt;td&gt;x&lt;/td&gt; &lt;td&gt;x&lt;/td&gt; &lt;td&gt;68.28&lt;/td&gt; &lt;td&gt;62.66&lt;/td&gt; &lt;td&gt;63.44&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Here, the sparse embedding model actually already outperforms the dense one, but the real magic happens when combining the two: hybrid search. In our case, we used Reciprocal Rank Fusion to merge the two rankings. &lt;/p&gt; &lt;p&gt;Rerankers also help improve the performance of the dense or sparse model here, but hurt the performance of the hybrid search, as its performance is already beyond what the reranker can achieve.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;So, on paper you can now get more freedom over the &amp;quot;lexical&amp;quot; part of your hybrid search pipelines. I'm very excited about it personally.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-Cubie-"&gt; /u/-Cubie- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/blog/train-sparse-encoder"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp2h0e/training_and_finetuning_sparse_embedding_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lp2h0e/training_and_finetuning_sparse_embedding_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T14:02:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1lorbc5</id>
    <title>Is the rumours true about Apple abandoning MLX?</title>
    <updated>2025-07-01T03:17:23+00:00</updated>
    <author>
      <name>/u/No_Conversation9561</name>
      <uri>https://old.reddit.com/user/No_Conversation9561</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Some folks on X are saying&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Conversation9561"&gt; /u/No_Conversation9561 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lorbc5/is_the_rumours_true_about_apple_abandoning_mlx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lorbc5/is_the_rumours_true_about_apple_abandoning_mlx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lorbc5/is_the_rumours_true_about_apple_abandoning_mlx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T03:17:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1lp653l</id>
    <title>Reuse non-prefix KV Cache and speed up RAG by 3X with LMCache.</title>
    <updated>2025-07-01T16:26:03+00:00</updated>
    <author>
      <name>/u/Nice-Comfortable-650</name>
      <uri>https://old.reddit.com/user/Nice-Comfortable-650</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp653l/reuse_nonprefix_kv_cache_and_speed_up_rag_by_3x/"&gt; &lt;img alt="Reuse non-prefix KV Cache and speed up RAG by 3X with LMCache." src="https://preview.redd.it/9eq6ted4haaf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=576ff3381e11049410b474e07e3ae6108a604a27" title="Reuse non-prefix KV Cache and speed up RAG by 3X with LMCache." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey&lt;a href="https://www.reddit.com/r/MachineLearning/"&gt; &lt;/a&gt;&lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; !&lt;/p&gt; &lt;p&gt;A while back, we shared our open-source project LMCache here and were blown away by the incredible support and feedback. Today, our team is thrilled to share more about one of our core components: &lt;strong&gt;CacheBlend&lt;/strong&gt;. Recognized with a &lt;strong&gt;Best Paper Award at ACM EuroSys 2025,&lt;/strong&gt; this technique is a pain killer for efficient RAG applications &lt;/p&gt; &lt;h1&gt;The Problem: Your KV Cache is Wasting Potential&lt;/h1&gt; &lt;p&gt;In modern LLM applications like RAG and Agents, we constantly feed the model new context. For example, in RAG, we retrieve relevant documents and stuff them into the prompt.&lt;/p&gt; &lt;p&gt;The issue is that this dynamically retrieved context doesn't always appear at the beginning of the input sequence. Traditional KV caching only reuses a &amp;quot;common prefix,&amp;quot; so if the new information isn't at the very start, the cache hit rate plummets, and your GPU ends up recomputing the same things over and over.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Solution: CacheBlend - 100% Hit Rate, No Compromises&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;CacheBlend changes the game by allowing for the reuse of pre-computed KV caches &lt;strong&gt;regardless of their position in the input sequence&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;This means we can finally achieve a &lt;strong&gt;100% KV Cache hit rate&lt;/strong&gt; in applications like RAG. The performance gains are significant:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Faster Time-To-First-Token (TTFT):&lt;/strong&gt; Get your initial response much quicker.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;More Throughput:&lt;/strong&gt; Serve significantly more users with the same hardware.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Almost lossless Output Quality:&lt;/strong&gt; All of this is achieved with little degradation in the model's generation quality.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;How does it work?&lt;/h1&gt; &lt;p&gt;CacheBlend intelligently handles the two main challenges of reusing non-prefix caches:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Positional Encoding Update:&lt;/strong&gt; It efficiently updates positional encodings to ensure the model always knows the correct position of each token, even when we're stitching together cached and new data.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Selective Attention Recalculation:&lt;/strong&gt; Instead of recomputing everything, it strategically recalculates only the minimal cross-attention needed between the new and cached chunks to maintain perfect generation quality.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;For detailed analysis, please refer to the official paper: &lt;a href="https://dl.acm.org/doi/10.1145/3689031.3696098"&gt;https://dl.acm.org/doi/10.1145/3689031.3696098&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Where can I try it?&lt;/h1&gt; &lt;p&gt;Try the newest interactive CacheBlend demo at: &lt;a href="https://github.com/LMCache/LMCache-Examples/tree/main/demo-rag-blending"&gt;https://github.com/LMCache/LMCache-Examples/tree/main/demo-rag-blending&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Ask us anything!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nice-Comfortable-650"&gt; /u/Nice-Comfortable-650 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9eq6ted4haaf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp653l/reuse_nonprefix_kv_cache_and_speed_up_rag_by_3x/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lp653l/reuse_nonprefix_kv_cache_and_speed_up_rag_by_3x/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T16:26:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1lp5nhy</id>
    <title>Gemma 3n Fine-tuning now in Unsloth - 1.5x faster with 50% less VRAM + Fixes</title>
    <updated>2025-07-01T16:07:29+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp5nhy/gemma_3n_finetuning_now_in_unsloth_15x_faster/"&gt; &lt;img alt="Gemma 3n Fine-tuning now in Unsloth - 1.5x faster with 50% less VRAM + Fixes" src="https://external-preview.redd.it/Z9xq13sybpzZE8ZKJtOY4SuppgTg5x6rIeOPF_Fl1qk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7593f97dd0c1af68e044aad5a89b7cf7f0e2b642" title="Gemma 3n Fine-tuning now in Unsloth - 1.5x faster with 50% less VRAM + Fixes" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey LocalLlama! We made finetuning Gemma 3N 1.5x faster in a free Colab with &lt;a href="http://github.com/unslothai/unsloth"&gt;Unsloth&lt;/a&gt; in under 16GB of VRAM! We also managed to find and fix issues for Gemma 3N:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Ollama &amp;amp; GGUF fixes&lt;/strong&gt; - All Gemma 3N GGUFs could not load in Ollama properly since &lt;code&gt;per_layer_token_embd&lt;/code&gt; had loading issues. Use our quants in Ollama for our fixes. All dynamic quants in our &lt;a href="https://huggingface.co/collections/unsloth/gemma-3n-685d3874830e49e1c93f9339"&gt;Gemma 3N collection&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;NaN and infinities in float16 GPUs&lt;/strong&gt; - we found Conv2D weights (the vision part) have very large magnitudes - we upcast them to float32 to remove infinities.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/v2w9vippbaaf1.jpg?width=1888&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=9c617026ca9deecc699787547badded628f081bc"&gt;Green crosses are large Conv2D weights&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Free Colab to fine-tune Gemma 3N 4B&lt;/strong&gt; in a free Colab + audio + text + vision inference: &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3N_(4B"&gt;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3N_(4B)-Conversational.ipynb&lt;/a&gt;-Conversational.ipynb)&lt;/p&gt; &lt;p&gt;Update Unsloth via &lt;code&gt;pip install --upgrade unsloth unsloth_zoo&lt;/code&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from unsloth import FastModel import torch model, tokenizer = FastModel.from_pretrained( model_name = &amp;quot;unsloth/gemma-3n-E4B-it&amp;quot;, max_seq_length = 1024, load_in_4bit = True, full_finetuning = False, ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Detailed technical analysis&lt;/strong&gt; and guide on how to use Gemma 3N effectively: &lt;a href="https://docs.unsloth.ai/basics/gemma-3n"&gt;https://docs.unsloth.ai/basics/gemma-3n&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We also uploaded GGUFs for the new FLUX model: &lt;a href="https://huggingface.co/unsloth/FLUX.1-Kontext-dev-GGUF"&gt;https://huggingface.co/unsloth/FLUX.1-Kontext-dev-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp5nhy/gemma_3n_finetuning_now_in_unsloth_15x_faster/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp5nhy/gemma_3n_finetuning_now_in_unsloth_15x_faster/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lp5nhy/gemma_3n_finetuning_now_in_unsloth_15x_faster/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T16:07:29+00:00</published>
  </entry>
</feed>
