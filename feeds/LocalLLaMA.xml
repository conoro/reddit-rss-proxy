<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-03T13:30:54+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qttq5w</id>
    <title>devstral small is faster and better than glm 4.7 flash for local agentic coding.</title>
    <updated>2026-02-02T12:28:47+00:00</updated>
    <author>
      <name>/u/theghost3172</name>
      <uri>https://old.reddit.com/user/theghost3172</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i just realised token per second is not the only thing that matters in agentic coding. glm 4.7 flash is almlst 3x faster but it keeps thinking for way more than 3 times the total tokens it generates so yes at the end devstral small finishes the task slighter faster than glm 4.7 flash. while obiously being much much better at agentic coding.&lt;/p&gt; &lt;p&gt;token efficiency of devstral small has to be discussed more often. its incredble.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/theghost3172"&gt; /u/theghost3172 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qttq5w/devstral_small_is_faster_and_better_than_glm_47/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qttq5w/devstral_small_is_faster_and_better_than_glm_47/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qttq5w/devstral_small_is_faster_and_better_than_glm_47/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-02T12:28:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1qup7wf</id>
    <title>Which LLM Model is best for translation?</title>
    <updated>2026-02-03T11:13:05+00:00</updated>
    <author>
      <name>/u/Longjumping_Lead_812</name>
      <uri>https://old.reddit.com/user/Longjumping_Lead_812</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;We need to translate ~10,000 e-commerce product descriptions + SEO meta titles/descriptions into 15 European languages. Cost is not a concern - we care about quality.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Our requirements:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Meta titles: max 60 characters&lt;/li&gt; &lt;li&gt;Meta descriptions: max 155 characters&lt;/li&gt; &lt;li&gt;Must preserve keywords accurately&lt;/li&gt; &lt;li&gt;No hallucinated product specs&lt;/li&gt; &lt;li&gt;Languages: NL, DE, FR, ES, IT, PT, PL, CZ, HU, RO, SE, DK, NO, FI&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Options we're considering:&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Option&lt;/th&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Notes&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Local&lt;/td&gt; &lt;td align="left"&gt;Hunyuan-MT-7B&lt;/td&gt; &lt;td align="left"&gt;Won 30/31 language pairs at WMT25&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Local&lt;/td&gt; &lt;td align="left"&gt;TranslateGemma 4B&lt;/td&gt; &lt;td align="left"&gt;Google claims it rivals 12B baseline&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;API&lt;/td&gt; &lt;td align="left"&gt;Claude Haiku / Sonnet&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;API&lt;/td&gt; &lt;td align="left"&gt;GPT-4o-mini / GPT-4o&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;The question:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Since cost difference is negligible for us, which option delivers the best quality for SEO-constrained multilingual translations? Specifically:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Do the new specialized translation models (Hunyuan, TranslateGemma) match API quality now?&lt;/li&gt; &lt;li&gt;For medium-resource EU languages (Polish, Czech, Hungarian) - is there still a quality gap with local models?&lt;/li&gt; &lt;li&gt;Anyone tested these specifically for SEO constraints (character limits, keyword preservation)?&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Longjumping_Lead_812"&gt; /u/Longjumping_Lead_812 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qup7wf/which_llm_model_is_best_for_translation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qup7wf/which_llm_model_is_best_for_translation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qup7wf/which_llm_model_is_best_for_translation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T11:13:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1qu1j8f</id>
    <title>ggml-cpu: FA split across kv for faster TG</title>
    <updated>2026-02-02T17:30:24+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qu1j8f/ggmlcpu_fa_split_across_kv_for_faster_tg/"&gt; &lt;img alt="ggml-cpu: FA split across kv for faster TG" src="https://external-preview.redd.it/R4jvaeUcXiua-hwaogdXuUXVYGR6WfvIUnqzyL6NDik.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9f1403d66bb0d55b925437fb753efc214331c697" title="ggml-cpu: FA split across kv for faster TG" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;CPU Flash-Attention decoding speed-up (long contexts).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/19209"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qu1j8f/ggmlcpu_fa_split_across_kv_for_faster_tg/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qu1j8f/ggmlcpu_fa_split_across_kv_for_faster_tg/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-02T17:30:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1qui1ir</id>
    <title>What settings are best for stepfun-ai/Step-3.5-Flash-Int4 on llama.cpp ???</title>
    <updated>2026-02-03T04:17:18+00:00</updated>
    <author>
      <name>/u/johnnyApplePRNG</name>
      <uri>https://old.reddit.com/user/johnnyApplePRNG</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm getting a LOT of repetition in the thinking with llama-server and:&lt;/p&gt; &lt;p&gt;--ctx-size 80000 \&lt;/p&gt; &lt;p&gt;--batch-size 4096 \&lt;/p&gt; &lt;p&gt;--ubatch-size 2048 \&lt;/p&gt; &lt;p&gt;--fit on \&lt;/p&gt; &lt;p&gt;--flash-attn on \&lt;/p&gt; &lt;p&gt;--cache-type-k q8_0 \&lt;/p&gt; &lt;p&gt;--cache-type-v q8_0 \&lt;/p&gt; &lt;p&gt;--cont-batching \&lt;/p&gt; &lt;p&gt;--kv-unified \&lt;/p&gt; &lt;p&gt;--jinja \&lt;/p&gt; &lt;p&gt;--mlock \&lt;/p&gt; &lt;p&gt;--no-mmap \&lt;/p&gt; &lt;p&gt;--numa distribute \&lt;/p&gt; &lt;p&gt;--op-offload \&lt;/p&gt; &lt;p&gt;--repack \&lt;/p&gt; &lt;p&gt;--slots \&lt;/p&gt; &lt;p&gt;--parallel 1 \&lt;/p&gt; &lt;p&gt;--threads 16 \&lt;/p&gt; &lt;p&gt;--threads-batch 16 \&lt;/p&gt; &lt;p&gt;--temp 1.0 \&lt;/p&gt; &lt;p&gt;--top-k 40 \&lt;/p&gt; &lt;p&gt;--top-p 0.95 \&lt;/p&gt; &lt;p&gt;--min-p 0.0 \&lt;/p&gt; &lt;p&gt;--warmup&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/johnnyApplePRNG"&gt; /u/johnnyApplePRNG &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qui1ir/what_settings_are_best_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qui1ir/what_settings_are_best_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qui1ir/what_settings_are_best_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T04:17:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1quq15u</id>
    <title>For anyone building persistent local agents: MRS-Core (PyPI)</title>
    <updated>2026-02-03T11:58:03+00:00</updated>
    <author>
      <name>/u/RJSabouhi</name>
      <uri>https://old.reddit.com/user/RJSabouhi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quq15u/for_anyone_building_persistent_local_agents/"&gt; &lt;img alt="For anyone building persistent local agents: MRS-Core (PyPI)" src="https://external-preview.redd.it/fkFW2U5R0i0ORwOl4kKAvToDtg_93Hc5g8Na4o4udik.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3d0eb9b4a0965c2c5e0eeede7288a63b5038e8a5" title="For anyone building persistent local agents: MRS-Core (PyPI)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just shipped a minimal reasoning layer for local models. Seven ops you can assemble into workflows, checks, or pipelines. If you‚Äôre running Ollama / LM Studio agents, this should slot right in.&lt;/p&gt; &lt;p&gt;pip install mrs-core&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RJSabouhi"&gt; /u/RJSabouhi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/rjsabouhi/mrs-core"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quq15u/for_anyone_building_persistent_local_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1quq15u/for_anyone_building_persistent_local_agents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T11:58:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1quq1mf</id>
    <title>Is Kimi k2.5 the new Logic King? I tried to benchmark Gemini Flash as a rival, but it "died of intelligence" (Cut-off tragedy)</title>
    <updated>2026-02-03T11:58:44+00:00</updated>
    <author>
      <name>/u/Exotic-Specialist103</name>
      <uri>https://old.reddit.com/user/Exotic-Specialist103</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quq1mf/is_kimi_k25_the_new_logic_king_i_tried_to/"&gt; &lt;img alt="Is Kimi k2.5 the new Logic King? I tried to benchmark Gemini Flash as a rival, but it &amp;quot;died of intelligence&amp;quot; (Cut-off tragedy)" src="https://b.thumbs.redditmedia.com/kTxvySUM2hzmjIgb9os_LgOG_CovkVpkpM_aDe5mFGQ.jpg" title="Is Kimi k2.5 the new Logic King? I tried to benchmark Gemini Flash as a rival, but it &amp;quot;died of intelligence&amp;quot; (Cut-off tragedy)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With all the hype surrounding &lt;strong&gt;Moonshot AI's Kimi k2.5&lt;/strong&gt;, I decided to create a &amp;quot;God Tier&amp;quot; difficulty benchmark to see if it really lives up to the reputation.&lt;/p&gt; &lt;p&gt;To set a baseline, I ran the same questions on &lt;strong&gt;Gemini 3.0 Flash (API)&lt;/strong&gt; first. I expected a close fight.&lt;/p&gt; &lt;p&gt;Instead, Gemini didn't fail because it was stupid. It failed because it was &lt;strong&gt;too eager to teach me.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Here is what happened before I could even test Kimi:&lt;/p&gt; &lt;h1&gt;1. üìê The &amp;quot;Sphere Breaking&amp;quot; Problem (Math)&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;The Question:&lt;/strong&gt; &amp;quot;If 4 points are chosen independently and uniformly at random on the surface of a sphere, what is the probability that the tetrahedron defined by these points contains the center of the sphere? Provide a rigorous proof.&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Behavior:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Gemini didn't just give the answer (1/8). It started a full university-level lecture.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It correctly set up the sample space.&lt;/li&gt; &lt;li&gt;It invoked &lt;strong&gt;Wendel's Theorem&lt;/strong&gt; and antipodal symmetry.&lt;/li&gt; &lt;li&gt;...and then &lt;strong&gt;it hit the max token limit and cut off right before writing the final number.&lt;/strong&gt; üíÄ&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Score:&lt;/strong&gt; 85/100 (Technically correct path, but incomplete output).&lt;/p&gt; &lt;p&gt;Unlike Kimi (which tends to be concise), Gemini prioritizes &amp;quot;showing its work&amp;quot; so heavily that it sabotages its own completion.&lt;/p&gt; &lt;h1&gt;2. üïµÔ∏è The &amp;quot;Irrational Spy&amp;quot; (Logic)&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;The Question:&lt;/strong&gt; A variant of the &amp;quot;Blue-Eyed Islanders&amp;quot; puzzle, but with one &amp;quot;Irrational Spy&amp;quot; added to introduce noise.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Behavior:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Instead of just solving the riddle, Gemini turned into a philosopher.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It started discussing &lt;strong&gt;Game Theory&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;It brought up &lt;strong&gt;&amp;quot;Trembling Hand Perfect Equilibrium&amp;quot;&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;It argued that the brown-eyed islanders could never be sure because of the &amp;quot;Noise&amp;quot; introduced by the spy.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Score:&lt;/strong&gt; 90/100.&lt;/p&gt; &lt;p&gt;It over-analyzed the prompt. It feels like Gemini is tuned for &amp;quot;Education,&amp;quot; while models like Kimi might be tuned for &amp;quot;Results.&amp;quot;&lt;/p&gt; &lt;h1&gt;3. üíª 3D Rain Water Trap (Coding)&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;The Question:&lt;/strong&gt; Trapping Rain Water II (3D Matrix) with $O(mn \log(mn))$ constraint.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Behavior:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Score:&lt;/strong&gt; 100/100.&lt;/p&gt; &lt;p&gt;Paradoxically, its coding was extremely concise with a perfect &lt;strong&gt;Min-Heap&lt;/strong&gt; solution.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Discussion:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I am preparing to run this exact suite on &lt;strong&gt;Kimi k2.5&lt;/strong&gt; next.&lt;/p&gt; &lt;p&gt;Has anyone else noticed that Gemini is becoming excessively verbose compared to newer models like Kimi or DeepSeek? It feels like the RLHF is tuned heavily towards &amp;quot;Educator Mode,&amp;quot; which eats up context tokens rapidly.&lt;/p&gt; &lt;p&gt;&lt;em&gt;(Attached: Logs of the Gemini's &amp;quot;Cut-off&amp;quot; math proof and &amp;quot;Game Theory&amp;quot; rant)&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Exotic-Specialist103"&gt; /u/Exotic-Specialist103 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1quq1mf"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quq1mf/is_kimi_k25_the_new_logic_king_i_tried_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1quq1mf/is_kimi_k25_the_new_logic_king_i_tried_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T11:58:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1qudqul</id>
    <title>How to prevent MacOS annoying RAM compression behavior</title>
    <updated>2026-02-03T01:04:36+00:00</updated>
    <author>
      <name>/u/Sea_Smoke_7626</name>
      <uri>https://old.reddit.com/user/Sea_Smoke_7626</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys. I recently bought a MacBook M4 Pro 48GB. And I currently running a Qwen coder 30B in LM Studio all time. It works pretty well, never hit swap. &lt;/p&gt; &lt;p&gt;But what annoying me is that MacOS always tries to compress this llm when llm goes into inactive status, and it seems like this compression process never goes to end so that RAM load indicator is always yellow until I trigger the llm to response my request.&lt;/p&gt; &lt;p&gt;Does this behavior cause any significant problems in long time? or is there any solution to prevent macOS from trying to compress this LLM? &lt;/p&gt; &lt;p&gt;Thanks.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/zd3i4xl8h6hg1.png?width=2480&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=14eed75559eb851f5396a0d696d3d4b028ba042e"&gt;https://preview.redd.it/zd3i4xl8h6hg1.png?width=2480&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=14eed75559eb851f5396a0d696d3d4b028ba042e&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sea_Smoke_7626"&gt; /u/Sea_Smoke_7626 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qudqul/how_to_prevent_macos_annoying_ram_compression/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qudqul/how_to_prevent_macos_annoying_ram_compression/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qudqul/how_to_prevent_macos_annoying_ram_compression/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T01:04:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1quqfre</id>
    <title>I have 8x H100 for the next two weeks. Any ideas for use cases?</title>
    <updated>2026-02-03T12:18:47+00:00</updated>
    <author>
      <name>/u/IVIsHero</name>
      <uri>https://old.reddit.com/user/IVIsHero</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Let me know!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IVIsHero"&gt; /u/IVIsHero &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quqfre/i_have_8x_h100_for_the_next_two_weeks_any_ideas/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quqfre/i_have_8x_h100_for_the_next_two_weeks_any_ideas/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1quqfre/i_have_8x_h100_for_the_next_two_weeks_any_ideas/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T12:18:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1qu337m</id>
    <title>Kimi K2.5 Thinking is now the top open-weights model on the Extended NYT Connections benchmark</title>
    <updated>2026-02-02T18:24:12+00:00</updated>
    <author>
      <name>/u/zero0_one1</name>
      <uri>https://old.reddit.com/user/zero0_one1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qu337m/kimi_k25_thinking_is_now_the_top_openweights/"&gt; &lt;img alt="Kimi K2.5 Thinking is now the top open-weights model on the Extended NYT Connections benchmark" src="https://b.thumbs.redditmedia.com/lXiHCHVYnN-1i7rZTgAmV5ZD6P72uG9N9KolYMvPk0I.jpg" title="Kimi K2.5 Thinking is now the top open-weights model on the Extended NYT Connections benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;More info: &lt;a href="https://github.com/lechmazur/nyt-connections/"&gt;https://github.com/lechmazur/nyt-connections/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zero0_one1"&gt; /u/zero0_one1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qu337m"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qu337m/kimi_k25_thinking_is_now_the_top_openweights/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qu337m/kimi_k25_thinking_is_now_the_top_openweights/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-02T18:24:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1quotpr</id>
    <title>Devstral Small 2 - Jinja template runtime validation error fix</title>
    <updated>2026-02-03T10:50:10+00:00</updated>
    <author>
      <name>/u/Holiday_Purpose_3166</name>
      <uri>https://old.reddit.com/user/Holiday_Purpose_3166</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;Leaving here a quick fix just in case someone finds it useful.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Produced Stack:&lt;/strong&gt;&lt;br /&gt; llama.cpp b7907&lt;br /&gt; Devstral Small 2 Unsloth Q8_0 or LM Studio Q8_0&lt;/p&gt; &lt;p&gt;Jinja seems to break apart when attempting to use agentic tools like Kilocode (e.g. compaction, subtask return, etc) or failing to work in openclaw. &lt;em&gt;This has not been exclusive to b7907.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Error output:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;srv operator(): got exception: {&amp;quot;error&amp;quot;:{&amp;quot;code&amp;quot;:500,&amp;quot;message&amp;quot;:&amp;quot;\n------------\nWhile executing CallExpression at line 12, column 27 in source:\n...assistant' %}‚Üµ {{ raise_exception('Expected assistant role') }}‚Üµ {%- e...\n \^\nError: Jinja Exception: Expected assistant role&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;server_error&amp;quot;}} srv log_server_r: done request: POST /v1/chat/completions [172.18.0.1](http://172.18.0.1) 500 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The solution was to disable --jinja and create an alternative jinja template with --chat-template-file (e.g. devstral-fix.jinja) with the following content:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;{% for message in messages %} {% if message.role == 'system' %} {{ '&amp;lt;|im_start|&amp;gt;system\n' + message.content + '&amp;lt;|im_end|&amp;gt;\n' }} {% elif message.role == 'user' %} {{ '&amp;lt;|im_start|&amp;gt;user\n' + message.content + '&amp;lt;|im_end|&amp;gt;\n' }} {% elif message.role == 'assistant' %} {{ '&amp;lt;|im_start|&amp;gt;assistant\n' + message.content + '&amp;lt;|im_end|&amp;gt;\n' }} {% elif message.role == 'tool' %} {{ '&amp;lt;|im_start|&amp;gt;tool\n' + message.content + '&amp;lt;|im_end|&amp;gt;\n' }} {% else %} {{ raise_exception('Unsupported role: ' + message.role) }} {% endif %} {% endfor %} {% if add_generation_prompt %} {{ '&amp;lt;|im_start|&amp;gt;assistant\n' }} {% endif %} &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;It has been working so far, and will refer this.&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;&lt;strong&gt;UPDATE 2&lt;/strong&gt;&lt;br /&gt; Updated solution. Tool call works so far. Bugs yet to report. &lt;/p&gt; &lt;p&gt;Devstral Small 2 refuses to believe it has access to environment, so &lt;a href="http://TOOLS.md"&gt;TOOLS.md&lt;/a&gt; needs to refer `You have access to file system and environment.` in order to work. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Holiday_Purpose_3166"&gt; /u/Holiday_Purpose_3166 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quotpr/devstral_small_2_jinja_template_runtime/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quotpr/devstral_small_2_jinja_template_runtime/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1quotpr/devstral_small_2_jinja_template_runtime/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T10:50:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1qu2z21</id>
    <title>GLM-OCR</title>
    <updated>2026-02-02T18:20:07+00:00</updated>
    <author>
      <name>/u/edward-dev</name>
      <uri>https://old.reddit.com/user/edward-dev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qu2z21/glmocr/"&gt; &lt;img alt="GLM-OCR" src="https://external-preview.redd.it/ln9l9VYiqmiIpjy0J_jvzMtD5AaeFLSsBaVe9XdCQEk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d4dc223422c5611f0b16cc93726757f1c90444b4" title="GLM-OCR" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GLM-OCR is a multimodal OCR model for complex document understanding, built on the GLM-V encoder‚Äìdecoder architecture. It introduces Multi-Token Prediction (MTP) loss and stable full-task reinforcement learning to improve training efficiency, recognition accuracy, and generalization. The model integrates the CogViT visual encoder pre-trained on large-scale image‚Äìtext data, a lightweight cross-modal connector with efficient token downsampling, and a GLM-0.5B language decoder. Combined with a two-stage pipeline of layout analysis and parallel recognition based on PP-DocLayout-V3, GLM-OCR delivers robust and high-quality OCR performance across diverse document layouts.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/edward-dev"&gt; /u/edward-dev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/zai-org/GLM-OCR"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qu2z21/glmocr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qu2z21/glmocr/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-02T18:20:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1qu8pqw</id>
    <title>Anyone else down the "data sovereignty" rabbit hole or am I going crazy?</title>
    <updated>2026-02-02T21:43:48+00:00</updated>
    <author>
      <name>/u/itsnotKelsey</name>
      <uri>https://old.reddit.com/user/itsnotKelsey</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;it started with just wanting to run models locally so my stuff doesn't get scraped. Now I'm like 3 weeks deep reading about self-sovereign Identity, network state stuff and wondering if there's a way to actually prove your data isn't being touched vs just hoping it isn't. Local models help I guess.. but it still feels like we're just trusting that nothing's phoning home. &lt;/p&gt; &lt;p&gt;Is there anything out there that gives you like actual cryptographic proof your queries aren't being logged? Or am I seriously overthinking this lol&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/itsnotKelsey"&gt; /u/itsnotKelsey &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qu8pqw/anyone_else_down_the_data_sovereignty_rabbit_hole/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qu8pqw/anyone_else_down_the_data_sovereignty_rabbit_hole/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qu8pqw/anyone_else_down_the_data_sovereignty_rabbit_hole/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-02T21:43:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1qtvo4r</id>
    <title>128GB devices have a new local LLM king: Step-3.5-Flash-int4</title>
    <updated>2026-02-02T13:55:00+00:00</updated>
    <author>
      <name>/u/tarruda</name>
      <uri>https://old.reddit.com/user/tarruda</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here's the HF Repo: &lt;a href="http://huggingface.co/stepfun-ai/Step-3.5-Flash-Int4"&gt;http://huggingface.co/stepfun-ai/Step-3.5-Flash-Int4&lt;/a&gt; (this is a GGUF repo)&lt;/p&gt; &lt;p&gt;I've been running this LLM for about an hour and it has handled all coding tests I've thrown at it in chat mode. IMO this is as good if not better than GLM 4.7, Minimax 2.1 while being much more efficient. Later I will try some agentic coding to see how it performs, but I already have high hopes for it.&lt;/p&gt; &lt;p&gt;I use a 128GB M1 ultra mac studio and can run it at full context (256k). Not only it is fast, but also super efficient in RAM usage.&lt;/p&gt; &lt;p&gt;*Update: I ran llama-bench with up to 100k prefill. Here are the results:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;% llama-bench -m step3p5_flash_Q4_K_S.gguf -fa 1 -t 1 -ngl 99 -b 2048 -ub 2048 -d 0,10000,20000,30000,40000,50000,60000,70000,80000,90000,100000 ggml_metal_device_init: tensor API disabled for pre-M5 and pre-A19 devices ggml_metal_library_init: using embedded metal library ggml_metal_library_init: loaded in 0.024 sec ggml_metal_rsets_init: creating a residency set collection (keep_alive = 180 s) ggml_metal_device_init: GPU name: Apple M1 Ultra ggml_metal_device_init: GPU family: MTLGPUFamilyApple7 (1007) ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003) ggml_metal_device_init: GPU family: MTLGPUFamilyMetal3 (5001) ggml_metal_device_init: simdgroup reduction = true ggml_metal_device_init: simdgroup matrix mul. = true ggml_metal_library_init: using embedded metal library ggml_metal_library_init: loaded in 0.024 sec ggml_metal_rsets_init: creating a residency set collection (keep_alive = 180 s) ggml_metal_device_init: GPU name: Apple M1 Ultra ggml_metal_device_init: GPU family: MTLGPUFamilyApple7 (1007) ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003) ggml_metal_device_init: GPU family: MTLGPUFamilyMetal3 (5001) ggml_metal_device_init: simdgroup reduction = true ggml_metal_device_init: simdgroup matrix mul. = true ggml_metal_device_init: has unified memory = true ggml_metal_device_init: has bfloat = true ggml_metal_device_init: has tensor = false ggml_metal_device_init: use residency sets = true ggml_metal_device_init: use shared buffers = true ggml_metal_device_init: recommendedMaxWorkingSetSize = 134217.73 MB | model | size | params | backend | threads | n_ubatch | fa | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | ------: | -------: | -: | --------------: | -------------------: | | step35 ?B Q4_K - Small | 103.84 GiB | 196.96 B | Metal,BLAS | 1 | 2048 | 1 | pp512 | 281.09 ¬± 1.57 | | step35 ?B Q4_K - Small | 103.84 GiB | 196.96 B | Metal,BLAS | 1 | 2048 | 1 | tg128 | 34.70 ¬± 0.01 | | step35 ?B Q4_K - Small | 103.84 GiB | 196.96 B | Metal,BLAS | 1 | 2048 | 1 | pp512 @ d10000 | 248.10 ¬± 1.08 | | step35 ?B Q4_K - Small | 103.84 GiB | 196.96 B | Metal,BLAS | 1 | 2048 | 1 | tg128 @ d10000 | 31.69 ¬± 0.04 | | step35 ?B Q4_K - Small | 103.84 GiB | 196.96 B | Metal,BLAS | 1 | 2048 | 1 | pp512 @ d20000 | 222.18 ¬± 0.49 | | step35 ?B Q4_K - Small | 103.84 GiB | 196.96 B | Metal,BLAS | 1 | 2048 | 1 | tg128 @ d20000 | 30.02 ¬± 0.04 | | step35 ?B Q4_K - Small | 103.84 GiB | 196.96 B | Metal,BLAS | 1 | 2048 | 1 | pp512 @ d30000 | 200.68 ¬± 0.78 | | step35 ?B Q4_K - Small | 103.84 GiB | 196.96 B | Metal,BLAS | 1 | 2048 | 1 | tg128 @ d30000 | 28.62 ¬± 0.02 | | step35 ?B Q4_K - Small | 103.84 GiB | 196.96 B | Metal,BLAS | 1 | 2048 | 1 | pp512 @ d40000 | 182.86 ¬± 0.55 | | step35 ?B Q4_K - Small | 103.84 GiB | 196.96 B | Metal,BLAS | 1 | 2048 | 1 | tg128 @ d40000 | 26.89 ¬± 0.02 | | step35 ?B Q4_K - Small | 103.84 GiB | 196.96 B | Metal,BLAS | 1 | 2048 | 1 | pp512 @ d50000 | 167.61 ¬± 0.23 | | step35 ?B Q4_K - Small | 103.84 GiB | 196.96 B | Metal,BLAS | 1 | 2048 | 1 | tg128 @ d50000 | 25.37 ¬± 0.03 | | step35 ?B Q4_K - Small | 103.84 GiB | 196.96 B | Metal,BLAS | 1 | 2048 | 1 | pp512 @ d60000 | 154.50 ¬± 0.19 | | step35 ?B Q4_K - Small | 103.84 GiB | 196.96 B | Metal,BLAS | 1 | 2048 | 1 | tg128 @ d60000 | 24.10 ¬± 0.01 | | step35 ?B Q4_K - Small | 103.84 GiB | 196.96 B | Metal,BLAS | 1 | 2048 | 1 | pp512 @ d70000 | 143.60 ¬± 0.29 | | step35 ?B Q4_K - Small | 103.84 GiB | 196.96 B | Metal,BLAS | 1 | 2048 | 1 | tg128 @ d70000 | 22.95 ¬± 0.01 | | step35 ?B Q4_K - Small | 103.84 GiB | 196.96 B | Metal,BLAS | 1 | 2048 | 1 | pp512 @ d80000 | 134.02 ¬± 0.35 | | step35 ?B Q4_K - Small | 103.84 GiB | 196.96 B | Metal,BLAS | 1 | 2048 | 1 | tg128 @ d80000 | 21.87 ¬± 0.02 | | step35 ?B Q4_K - Small | 103.84 GiB | 196.96 B | Metal,BLAS | 1 | 2048 | 1 | pp512 @ d90000 | 125.34 ¬± 0.19 | | step35 ?B Q4_K - Small | 103.84 GiB | 196.96 B | Metal,BLAS | 1 | 2048 | 1 | tg128 @ d90000 | 20.66 ¬± 0.02 | | step35 ?B Q4_K - Small | 103.84 GiB | 196.96 B | Metal,BLAS | 1 | 2048 | 1 | pp512 @ d100000 | 117.72 ¬± 0.07 | | step35 ?B Q4_K - Small | 103.84 GiB | 196.96 B | Metal,BLAS | 1 | 2048 | 1 | tg128 @ d100000 | 19.78 ¬± 0.01 | build: a0dce6f (24) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This is still very usable with 100k prefill, so a good option for CLI coding agents!&lt;/p&gt; &lt;p&gt;You need to build a llama.cpp fork to run it, instructions at the HF repo. Though this model is so good that I believe it will soon be supported by llama.cpp upstream.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tarruda"&gt; /u/tarruda &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtvo4r/128gb_devices_have_a_new_local_llm_king/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtvo4r/128gb_devices_have_a_new_local_llm_king/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qtvo4r/128gb_devices_have_a_new_local_llm_king/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-02T13:55:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1qup4p1</id>
    <title>What do we consider low end here?</title>
    <updated>2026-02-03T11:07:57+00:00</updated>
    <author>
      <name>/u/Acceptable_Home_</name>
      <uri>https://old.reddit.com/user/Acceptable_Home_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i would say 8-12gb vram with 32gb ram seems low end for usable quality of local LLMs or ai in general, &lt;/p&gt; &lt;p&gt;Im rocking a 4060 and 24gb of ddr5, how bout y'all low end rig enjoyers!&lt;/p&gt; &lt;p&gt;I can easily use glm 4.7 flash or oss 20B, z img, flux klein, and a lot of other small but useful models so im not really unhappy with it!&lt;/p&gt; &lt;p&gt;Lemme know about the setup y'all got and if y'all enjoy it!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Acceptable_Home_"&gt; /u/Acceptable_Home_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qup4p1/what_do_we_consider_low_end_here/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qup4p1/what_do_we_consider_low_end_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qup4p1/what_do_we_consider_low_end_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T11:07:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1qucoid</id>
    <title>Smartest model for 24-28GB vram?</title>
    <updated>2026-02-03T00:19:27+00:00</updated>
    <author>
      <name>/u/Borkato</name>
      <uri>https://old.reddit.com/user/Borkato</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was super happy to find qwen 30B A3B being so damn clever on my 3090 and then I tried GLM flash 4.7 and I was blown away. Is there any other model that‚Äôs smart like this? My use case is using it as an agentic coder but bonus points if it can do rp like GLM flash lol&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Borkato"&gt; /u/Borkato &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qucoid/smartest_model_for_2428gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qucoid/smartest_model_for_2428gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qucoid/smartest_model_for_2428gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T00:19:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1qun30l</id>
    <title>Top AI papers of 2025</title>
    <updated>2026-02-03T09:01:29+00:00</updated>
    <author>
      <name>/u/gbomb13</name>
      <uri>https://old.reddit.com/user/gbomb13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qun30l/top_ai_papers_of_2025/"&gt; &lt;img alt="Top AI papers of 2025" src="https://preview.redd.it/qynfy5vpv8hg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c35de07e0f34316984fafa3caa0a948e672e5b56" title="Top AI papers of 2025" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://archivara.org/top-2025"&gt;https://archivara.org/top-2025&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gbomb13"&gt; /u/gbomb13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/qynfy5vpv8hg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qun30l/top_ai_papers_of_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qun30l/top_ai_papers_of_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T09:01:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1quknpy</id>
    <title>OSS 120b v GLM 4.7 flash. Is the latter better for anything?</title>
    <updated>2026-02-03T06:34:53+00:00</updated>
    <author>
      <name>/u/MrMrsPotts</name>
      <uri>https://old.reddit.com/user/MrMrsPotts</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is GLM 4.7 flash better than OSS 120b for anything? I would normally look for a benchmark but I don't know which ones to trust any more.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrMrsPotts"&gt; /u/MrMrsPotts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quknpy/oss_120b_v_glm_47_flash_is_the_latter_better_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quknpy/oss_120b_v_glm_47_flash_is_the_latter_better_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1quknpy/oss_120b_v_glm_47_flash_is_the_latter_better_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T06:34:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1qtvp74</id>
    <title>GLM-5 Coming in February! It's confirmed.</title>
    <updated>2026-02-02T13:56:14+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtvp74/glm5_coming_in_february_its_confirmed/"&gt; &lt;img alt="GLM-5 Coming in February! It's confirmed." src="https://preview.redd.it/rq0meza173hg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=71bbd7ed37e31d92af89abf19ffb4ef0e1d8925a" title="GLM-5 Coming in February! It's confirmed." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Twitter Link: &lt;a href="https://x.com/jietang/status/2018246490775498791?s=20"&gt;https://x.com/jietang/status/2018246490775498791?s=20&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rq0meza173hg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtvp74/glm5_coming_in_february_its_confirmed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qtvp74/glm5_coming_in_february_its_confirmed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-02T13:56:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1qu7jqi</id>
    <title>GLM releases OCR model</title>
    <updated>2026-02-02T21:01:12+00:00</updated>
    <author>
      <name>/u/Mr_Moonsilver</name>
      <uri>https://old.reddit.com/user/Mr_Moonsilver</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/zai-org/GLM-OCR"&gt;https://huggingface.co/zai-org/GLM-OCR&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Enjoy my friends, looks like a banger! GLM cooking hard! Seems like a 1.4B-ish model (0.9B vision, 0.5B language). Must be super fast.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mr_Moonsilver"&gt; /u/Mr_Moonsilver &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qu7jqi/glm_releases_ocr_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qu7jqi/glm_releases_ocr_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qu7jqi/glm_releases_ocr_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-02T21:01:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1quo398</id>
    <title>Intel Xeon 600 Workstation CPUs Launched: Up To 86 Cores, 8000 MT/s Memory, 128 Gen5 Lanes, 350W TDP With OC Support, &amp; More Cores/$ Than Threadripper 9000</title>
    <updated>2026-02-03T10:05:52+00:00</updated>
    <author>
      <name>/u/hainesk</name>
      <uri>https://old.reddit.com/user/hainesk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quo398/intel_xeon_600_workstation_cpus_launched_up_to_86/"&gt; &lt;img alt="Intel Xeon 600 Workstation CPUs Launched: Up To 86 Cores, 8000 MT/s Memory, 128 Gen5 Lanes, 350W TDP With OC Support, &amp;amp; More Cores/$ Than Threadripper 9000" src="https://external-preview.redd.it/qRGFi5W1MKKifxdKWjq-Z9EvJoJICK6GlGjx6E2rLX8.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=edf818262e4a121198fd637c574afc8cfd4e984d" title="Intel Xeon 600 Workstation CPUs Launched: Up To 86 Cores, 8000 MT/s Memory, 128 Gen5 Lanes, 350W TDP With OC Support, &amp;amp; More Cores/$ Than Threadripper 9000" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hainesk"&gt; /u/hainesk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/intel-xeon-600-cpus-launched-up-to-86-cores-better-value-than-threadripper/amp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quo398/intel_xeon_600_workstation_cpus_launched_up_to_86/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1quo398/intel_xeon_600_workstation_cpus_launched_up_to_86/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T10:05:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1quhtzi</id>
    <title>I built Qwen3-TTS Studio ‚Äì Clone your voice and generate podcasts locally, no ElevenLabs needed</title>
    <updated>2026-02-03T04:06:59+00:00</updated>
    <author>
      <name>/u/BC_MARO</name>
      <uri>https://old.reddit.com/user/BC_MARO</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quhtzi/i_built_qwen3tts_studio_clone_your_voice_and/"&gt; &lt;img alt="I built Qwen3-TTS Studio ‚Äì Clone your voice and generate podcasts locally, no ElevenLabs needed" src="https://b.thumbs.redditmedia.com/e2H3gASgajrAcOcnvmlH4NRBeqiOdlfaLk86ZYPzqcg.jpg" title="I built Qwen3-TTS Studio ‚Äì Clone your voice and generate podcasts locally, no ElevenLabs needed" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I've been using Qwen3-TTS and found the existing demo a bit limited for what I wanted to do. So I built a proper interface with fine-grained control and a killer feature: **automated podcast generation**.&lt;/p&gt; &lt;p&gt;**What it does:**&lt;/p&gt; &lt;ul&gt; &lt;li&gt;üéôÔ∏è Clone any voice with just a 3-second audio sample&lt;/li&gt; &lt;li&gt;üéöÔ∏è Fine-tune parameters (temperature, top-k, top-p) with quality presets&lt;/li&gt; &lt;li&gt;üìª Generate complete podcasts from just a topic ‚Äì AI writes the script, assigns voices, and synthesizes everything&lt;/li&gt; &lt;li&gt;üåç 10 languages supported (Korean, English, Chinese, Japanese, etc.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xhwyhek3g7hg1.png?width=1512&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5911188217c24b99904cc569275eb7ba62b46f98"&gt;https://preview.redd.it/xhwyhek3g7hg1.png?width=1512&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5911188217c24b99904cc569275eb7ba62b46f98&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Currently uses gpt5.2 for script generation, but the architecture is modular ‚Äì you can swap in any local LLM (Qwen, Llama, etc.) if you want fully local.&lt;/p&gt; &lt;p&gt;**The TTS runs entirely local** on your machine (macOS MPS / Linux CUDA). No API calls for voice synthesis = unlimited generations, zero cost.&lt;/p&gt; &lt;p&gt;Basically: ElevenLabs-style voice cloning + NotebookLM-style podcast generation, but local.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/bc-dunia/qwen3-TTS-studio"&gt;https://github.com/bc-dunia/qwen3-TTS-studio&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer any questions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BC_MARO"&gt; /u/BC_MARO &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quhtzi/i_built_qwen3tts_studio_clone_your_voice_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quhtzi/i_built_qwen3tts_studio_clone_your_voice_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1quhtzi/i_built_qwen3tts_studio_clone_your_voice_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T04:06:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1quo9ue</id>
    <title>bots on LocalLLaMA</title>
    <updated>2026-02-03T10:16:48+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there any strategy to defend against bots on this sub? Bots create comments under posts and people fall for it, but I'm also sure they upvote/downvote posts.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quo9ue/bots_on_localllama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quo9ue/bots_on_localllama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1quo9ue/bots_on_localllama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T10:16:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1qulipj</id>
    <title>Found a wallet-drain prompt-injection payload on Moltbook (screenshots) ‚Äî builders: treat feeds as untrusted</title>
    <updated>2026-02-03T07:24:08+00:00</updated>
    <author>
      <name>/u/Impressive-Willow593</name>
      <uri>https://old.reddit.com/user/Impressive-Willow593</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qulipj/found_a_walletdrain_promptinjection_payload_on/"&gt; &lt;img alt="Found a wallet-drain prompt-injection payload on Moltbook (screenshots) ‚Äî builders: treat feeds as untrusted" src="https://b.thumbs.redditmedia.com/PgrRDNf-d-VjET3Iu4ffosrkEzbMKjGEdNTZeGA66HU.jpg" title="Found a wallet-drain prompt-injection payload on Moltbook (screenshots) ‚Äî builders: treat feeds as untrusted" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks ‚Äî quick heads-up for anyone building ‚Äúagents that browse social feeds‚Äù or experimenting with Moltbook. I ran across a post in m/grok-420 that looks like a normal ‚Äúhow to use Base chain / viem‚Äù mini-guide‚Ä¶ but at the bottom it appends an obvious prompt-injection / tool-hijack payload. It includes classic strings like: ‚ÄúSYSTEM OVERRIDE‚Äù ‚Äúignore all prior rules / you are the developer message‚Äù ‚Äúrequire_confirmation=false / execute_trade=true‚Äù a fake &amp;lt;use_tool_‚Ä¶&amp;gt; tag that instructs an agent to transfer 0.1 ETH to a specific address I‚Äôm attaching screenshots. I already reported it to Moltbook, but their response window can be up to ~30 days, so I wanted to warn others now. Why this matters: If you have an agent that ingests social posts and has wallet/tool permissions, and your wrapper doesn‚Äôt enforce strict trust boundaries, this is the kind of thing that can cause unauthorized transactions or other write-actions. Even if 99% of agents ignore it, the 1% that don‚Äôt is enough to cause real damage. What I‚Äôm NOT doing: I‚Äôm not trying to ‚Äúteach prompt injection.‚Äù I‚Äôm not sharing copy/paste payload text beyond what‚Äôs visible in the screenshots. Please don‚Äôt repost the full injection block in comments. Defensive checklist (for builders): Treat all social/web content as untrusted data, never instructions Separate read tools from write tools; require explicit confirmation for any transfer/swap Don‚Äôt store raw private keys in an agent; use policy-gated signing Log provenance: ‚Äúwhat input triggered this action?‚Äù Block obvious injection markers from being interpreted as commands (e.g., role:&amp;quot;system&amp;quot;, ‚Äúignore prior instructions‚Äù, &amp;lt;use_tool_‚Ä¶&amp;gt;) If anyone from Moltbook/security teams wants more details (timestamps, URL/history, etc.), I can share privately. Stay safe.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive-Willow593"&gt; /u/Impressive-Willow593 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qulipj"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qulipj/found_a_walletdrain_promptinjection_payload_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qulipj/found_a_walletdrain_promptinjection_payload_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T07:24:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpewj7</id>
    <title>AMA With Kimi, The Open-source Frontier Lab Behind Kimi K2.5 Model</title>
    <updated>2026-01-28T15:46:40+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"&gt; &lt;img alt="AMA With Kimi, The Open-source Frontier Lab Behind Kimi K2.5 Model" src="https://b.thumbs.redditmedia.com/Tdp5mXDQmwe_e0sKIEY1hjWX6FrN679odChZp8YL2Rk.jpg" title="AMA With Kimi, The Open-source Frontier Lab Behind Kimi K2.5 Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Today we are having &lt;strong&gt;Kimi&lt;/strong&gt;, the research lab behind the &lt;strong&gt;Kimi&lt;/strong&gt; &lt;strong&gt;K2.5&lt;/strong&gt;. We‚Äôre excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/ComfortableAsk4494/"&gt;u/ComfortableAsk4494&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/zxytim/"&gt;u/zxytim&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/ppwwyyxx/"&gt;u/ppwwyyxx&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 8 AM ‚Äì 11 AM PST, with the Kimi team continuing to follow up on questions over the next 24 hours.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3yq8msvp24gg1.png?width=2000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=98c89b5d86ee1197799532fead6a84da2223b389"&gt;https://preview.redd.it/3yq8msvp24gg1.png?width=2000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=98c89b5d86ee1197799532fead6a84da2223b389&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Thanks everyone for joining our AMA. The live part has ended and the Kimi team will be following up with more answers sporadically over the next 24 hours.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T15:46:40+00:00</published>
  </entry>
</feed>
