<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-24T21:32:18+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1rdef9x</id>
    <title>Qwen3.5-397B-A17B-UD-TQ1 bench results FW Desktop Strix Halo 128GB</title>
    <updated>2026-02-24T12:02:39+00:00</updated>
    <author>
      <name>/u/dabiggmoe2</name>
      <uri>https://old.reddit.com/user/dabiggmoe2</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdef9x/qwen35397ba17budtq1_bench_results_fw_desktop/"&gt; &lt;img alt="Qwen3.5-397B-A17B-UD-TQ1 bench results FW Desktop Strix Halo 128GB" src="https://preview.redd.it/o0xbpnavmflg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=da67bbf4e8279cb0b472d61b07eaa8886c76693e" title="Qwen3.5-397B-A17B-UD-TQ1 bench results FW Desktop Strix Halo 128GB" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just sharing the bench results for unsloth Qwen3.5-397B-A17B-UD-TQ1 on my FW desktop with 128GB VRAM&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dabiggmoe2"&gt; /u/dabiggmoe2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/o0xbpnavmflg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdef9x/qwen35397ba17budtq1_bench_results_fw_desktop/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rdef9x/qwen35397ba17budtq1_bench_results_fw_desktop/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T12:02:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdkze3</id>
    <title>M3 Ultra 512GB - real-world performance of MiniMax-M2.5, GLM-5, and Qwen3-Coder-Next</title>
    <updated>2026-02-24T16:31:29+00:00</updated>
    <author>
      <name>/u/cryingneko</name>
      <uri>https://old.reddit.com/user/cryingneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdkze3/m3_ultra_512gb_realworld_performance_of/"&gt; &lt;img alt="M3 Ultra 512GB - real-world performance of MiniMax-M2.5, GLM-5, and Qwen3-Coder-Next" src="https://preview.redd.it/id901lm6vglg1.png?width=140&amp;amp;height=140&amp;amp;auto=webp&amp;amp;s=35ba17c83733f5851af19860e422989e7fea5da0" title="M3 Ultra 512GB - real-world performance of MiniMax-M2.5, GLM-5, and Qwen3-Coder-Next" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A lot of people have been asking about real-world performance of recent models on apple silicon, especially on the ultra chips. I've been running MiniMax-M2.5, GLM-5, and Qwen3-Coder-80B on my M3 Ultra 512GB and wanted to share the results.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Quick summary&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Qwen3-Coder-Next-80B&lt;/strong&gt; - the standout for local coding. i've been using it as a backend for Claude Code, and it honestly performs at a level comparable to commercial coding services. if you have an M-series Pro/Max with 64GB+ RAM, this model alone could make a solid local coding machine.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;MiniMax-M2.5&lt;/strong&gt; - the initial prefill takes a moment, but once prefix caching kicks in, TTFT drops a lot on follow-up requests. with continuous batching on top of that, it's surprisingly usable as a local coding assistant.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GLM-5&lt;/strong&gt; - raw speed isn't great for interactive coding where you need fast back-and-forth. but with continuous batching and persistent KV cache, it's way more manageable than you'd expect. for example, translation tasks with big glossaries in the system message work really well since the system prompt gets cached once and batch requests just fly through after that.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Benchmark results&lt;/strong&gt;&lt;br /&gt; &lt;strong&gt;oMLX&lt;/strong&gt; &lt;a href="https://github.com/jundot/omlx"&gt;&lt;strong&gt;https://github.com/jundot/omlx&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Benchmark Model: MiniMax-M2.5-8bit&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;oMLX - LLM inference, optimized for your Mac https://github.com/jundot/omlx Benchmark Model: MiniMax-M2.5-8bit ================================================================================ Single Request Results -------------------------------------------------------------------------------- Test TTFT(ms) TPOT(ms) pp TPS tg TPS E2E(s) Throughput Peak Mem pp1024/tg128 1741.4 29.64 588.0 tok/s 34.0 tok/s 5.506 209.2 tok/s 227.17 GB pp4096/tg128 5822.0 33.29 703.5 tok/s 30.3 tok/s 10.049 420.3 tok/s 228.20 GB pp8192/tg128 12363.9 38.36 662.6 tok/s 26.3 tok/s 17.235 482.7 tok/s 229.10 GB pp16384/tg128 29176.8 47.09 561.5 tok/s 21.4 tok/s 35.157 469.7 tok/s 231.09 GB pp32768/tg128 76902.8 67.54 426.1 tok/s 14.9 tok/s 85.480 384.8 tok/s 234.96 GB Continuous Batching — Same Prompt pp1024 / tg128 · partial prefix cache hit -------------------------------------------------------------------------------- Batch tg TPS Speedup pp TPS pp TPS/req TTFT(ms) E2E(s) 1x 34.0 tok/s 1.00x 588.0 tok/s 588.0 tok/s 1741.4 5.506 2x 49.1 tok/s 1.44x 688.6 tok/s 344.3 tok/s 2972.0 8.190 4x 70.7 tok/s 2.08x 1761.3 tok/s 440.3 tok/s 2317.3 9.568 8x 89.3 tok/s 2.63x 1906.7 tok/s 238.3 tok/s 4283.7 15.759 Continuous Batching — Different Prompts pp1024 / tg128 · no cache reuse -------------------------------------------------------------------------------- Batch tg TPS Speedup pp TPS pp TPS/req TTFT(ms) E2E(s) 1x 34.0 tok/s 1.00x 588.0 tok/s 588.0 tok/s 1741.4 5.506 2x 49.7 tok/s 1.46x 686.2 tok/s 343.1 tok/s 2978.6 8.139 4x 109.8 tok/s 3.23x 479.4 tok/s 119.8 tok/s 4526.7 13.207 8x 126.3 tok/s 3.71x 590.3 tok/s 73.8 tok/s 7421.6 21.987 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Benchmark Model: GLM-5-4bit&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;oMLX - LLM inference, optimized for your Mac https://github.com/jundot/omlx Benchmark Model: GLM-5-4bit ================================================================================ Single Request Results -------------------------------------------------------------------------------- Test TTFT(ms) TPOT(ms) pp TPS tg TPS E2E(s) Throughput Peak Mem pp1024/tg128 5477.3 60.46 187.0 tok/s 16.7 tok/s 13.156 87.6 tok/s 391.82 GB pp4096/tg128 22745.2 73.39 180.1 tok/s 13.7 tok/s 32.066 131.7 tok/s 394.07 GB pp8192/tg128 53168.8 76.07 154.1 tok/s 13.2 tok/s 62.829 132.4 tok/s 396.69 GB pp16384/tg128 139545.0 83.67 117.4 tok/s 12.0 tok/s 150.171 110.0 tok/s 402.72 GB pp32768/tg128 421954.5 94.47 77.7 tok/s 10.7 tok/s 433.952 75.8 tok/s 415.41 GB Continuous Batching — Same Prompt pp1024 / tg128 · partial prefix cache hit -------------------------------------------------------------------------------- Batch tg TPS Speedup pp TPS pp TPS/req TTFT(ms) E2E(s) 1x 16.7 tok/s 1.00x 187.0 tok/s 187.0 tok/s 5477.3 13.156 2x 24.7 tok/s 1.48x 209.3 tok/s 104.7 tok/s 9782.5 20.144 4x 30.4 tok/s 1.82x 619.7 tok/s 154.9 tok/s 6595.2 23.431 8x 40.2 tok/s 2.41x 684.5 tok/s 85.6 tok/s 11943.7 37.447 Continuous Batching — Different Prompts pp1024 / tg128 · no cache reuse -------------------------------------------------------------------------------- Batch tg TPS Speedup pp TPS pp TPS/req TTFT(ms) E2E(s) 1x 16.7 tok/s 1.00x 187.0 tok/s 187.0 tok/s 5477.3 13.156 2x 23.7 tok/s 1.42x 206.9 tok/s 103.5 tok/s 9895.4 20.696 4x 47.0 tok/s 2.81x 192.6 tok/s 48.1 tok/s 10901.6 32.156 8x 60.3 tok/s 3.61x 224.1 tok/s 28.0 tok/s 18752.5 53.537 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Benchmark Model: Qwen3-Coder-Next-8bit&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;oMLX - LLM inference, optimized for your Mac https://github.com/jundot/omlx Benchmark Model: Qwen3-Coder-Next-8bit ================================================================================ Single Request Results -------------------------------------------------------------------------------- Test TTFT(ms) TPOT(ms) pp TPS tg TPS E2E(s) Throughput Peak Mem pp1024/tg128 700.6 17.18 1461.7 tok/s 58.7 tok/s 2.882 399.7 tok/s 80.09 GB pp4096/tg128 2083.1 17.65 1966.3 tok/s 57.1 tok/s 4.324 976.8 tok/s 82.20 GB pp8192/tg128 4077.6 18.38 2009.0 tok/s 54.9 tok/s 6.411 1297.7 tok/s 82.63 GB pp16384/tg128 8640.3 19.25 1896.2 tok/s 52.3 tok/s 11.085 1489.5 tok/s 83.48 GB pp32768/tg128 20176.3 22.33 1624.1 tok/s 45.1 tok/s 23.013 1429.5 tok/s 85.20 GB Continuous Batching — Same Prompt pp1024 / tg128 · partial prefix cache hit -------------------------------------------------------------------------------- Batch tg TPS Speedup pp TPS pp TPS/req TTFT(ms) E2E(s) 1x 58.7 tok/s 1.00x 1461.7 tok/s 1461.7 tok/s 700.6 2.882 2x 101.1 tok/s 1.72x 1708.7 tok/s 854.4 tok/s 1196.1 3.731 4x 194.2 tok/s 3.31x 891.1 tok/s 222.8 tok/s 3614.7 7.233 8x 243.0 tok/s 4.14x 1903.5 tok/s 237.9 tok/s 4291.5 8.518 Continuous Batching — Different Prompts pp1024 / tg128 · no cache reuse -------------------------------------------------------------------------------- Batch tg TPS Speedup pp TPS pp TPS/req TTFT(ms) E2E(s) 1x 58.7 tok/s 1.00x 1461.7 tok/s 1461.7 tok/s 700.6 2.882 2x 100.5 tok/s 1.71x 1654.5 tok/s 827.3 tok/s 1232.8 3.784 4x 164.0 tok/s 2.79x 1798.2 tok/s 449.6 tok/s 2271.3 5.401 8x 243.3 tok/s 4.14x 1906.9 tok/s 238.4 tok/s 4281.4 8.504 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Takeaways&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- If you're on apple silicon with 64GB+ memory, Qwen3-Coder-80B is genuinely viable for daily coding work with Claude Code or similar agents&lt;/p&gt; &lt;p&gt;- Prefix caching and continuous batching make a huge difference for models that are borderline too slow for interactive use. turns &amp;quot;unusable&amp;quot; into &amp;quot;totally fine with a small wait&amp;quot;&lt;/p&gt; &lt;p&gt;- M3 Ultra 512GB is obviously overkill for a single model, but loading multiple models at once (LLM + embedding + reranker) without swapping is where the extra memory pays off&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Happy to test other models if you're curious. just drop a comment and i'll run it!&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cryingneko"&gt; /u/cryingneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1rdkze3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdkze3/m3_ultra_512gb_realworld_performance_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rdkze3/m3_ultra_512gb_realworld_performance_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T16:31:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdoldt</id>
    <title>Steerling-8B - Inherently Interpretable Foundation Model</title>
    <updated>2026-02-24T18:38:58+00:00</updated>
    <author>
      <name>/u/ScatteringSepoy</name>
      <uri>https://old.reddit.com/user/ScatteringSepoy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdoldt/steerling8b_inherently_interpretable_foundation/"&gt; &lt;img alt="Steerling-8B - Inherently Interpretable Foundation Model" src="https://external-preview.redd.it/W36wlT2FZ1hudJiJnCzPO3HZkbJh13qfUnXtx9cKhB4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9813c08be5262702b5b744b0cd48a4f3ffd847cd" title="Steerling-8B - Inherently Interpretable Foundation Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ScatteringSepoy"&gt; /u/ScatteringSepoy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.guidelabs.ai/post/steerling-8b-base-model-release/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdoldt/steerling8b_inherently_interpretable_foundation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rdoldt/steerling8b_inherently_interpretable_foundation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T18:38:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdq1zl</id>
    <title>No Gemma 4 until Google IO?</title>
    <updated>2026-02-24T19:30:17+00:00</updated>
    <author>
      <name>/u/Ok-Recognition-3177</name>
      <uri>https://old.reddit.com/user/Ok-Recognition-3177</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdq1zl/no_gemma_4_until_google_io/"&gt; &lt;img alt="No Gemma 4 until Google IO?" src="https://preview.redd.it/6whnc24zuhlg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ec01e33ed48289b330b08ffaa91195e5fa89087f" title="No Gemma 4 until Google IO?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With Google I/O running from May 19th - 20th we're not likely to see any Gemma updates until then, right?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Recognition-3177"&gt; /u/Ok-Recognition-3177 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6whnc24zuhlg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdq1zl/no_gemma_4_until_google_io/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rdq1zl/no_gemma_4_until_google_io/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T19:30:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdpfy6</id>
    <title>Open vs Closed Source SOTA - Benchmark overview</title>
    <updated>2026-02-24T19:08:38+00:00</updated>
    <author>
      <name>/u/Pristine-Woodpecker</name>
      <uri>https://old.reddit.com/user/Pristine-Woodpecker</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdpfy6/open_vs_closed_source_sota_benchmark_overview/"&gt; &lt;img alt="Open vs Closed Source SOTA - Benchmark overview" src="https://preview.redd.it/5bgiva65rhlg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=065f62ffff5572f425f0451422266a099fa8b195" title="Open vs Closed Source SOTA - Benchmark overview" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sonnet 4.5 was released about 6 months ago. What's the advantage of the closed source labs? About that amount of time? Even less?&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Benchmark&lt;/th&gt; &lt;th align="left"&gt;GPT-5.2&lt;/th&gt; &lt;th align="left"&gt;Opus 4.6&lt;/th&gt; &lt;th align="left"&gt;Opus 4.5&lt;/th&gt; &lt;th align="left"&gt;Sonnet 4.6&lt;/th&gt; &lt;th align="left"&gt;Sonnet 4.5&lt;/th&gt; &lt;th align="left"&gt;Q3.5 397B-A17B&lt;/th&gt; &lt;th align="left"&gt;Q3.5 122B-A10B&lt;/th&gt; &lt;th align="left"&gt;Q3.5 35B-A3B&lt;/th&gt; &lt;th align="left"&gt;Q3.5 27B&lt;/th&gt; &lt;th align="left"&gt;GLM-5&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Release date&lt;/td&gt; &lt;td align="left"&gt;Dec 2025&lt;/td&gt; &lt;td align="left"&gt;Feb 2026&lt;/td&gt; &lt;td align="left"&gt;Nov 2025&lt;/td&gt; &lt;td align="left"&gt;Feb 2026&lt;/td&gt; &lt;td align="left"&gt;Nov 2025&lt;/td&gt; &lt;td align="left"&gt;Feb 2026&lt;/td&gt; &lt;td align="left"&gt;Feb 2026&lt;/td&gt; &lt;td align="left"&gt;Feb 2026&lt;/td&gt; &lt;td align="left"&gt;Feb 2026&lt;/td&gt; &lt;td align="left"&gt;Feb 2026&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Reasoning &amp;amp; STEM&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPQA Diamond&lt;/td&gt; &lt;td align="left"&gt;93.2&lt;/td&gt; &lt;td align="left"&gt;91.3&lt;/td&gt; &lt;td align="left"&gt;87.0&lt;/td&gt; &lt;td align="left"&gt;89.9&lt;/td&gt; &lt;td align="left"&gt;83.4&lt;/td&gt; &lt;td align="left"&gt;88.4&lt;/td&gt; &lt;td align="left"&gt;86.6&lt;/td&gt; &lt;td align="left"&gt;84.2&lt;/td&gt; &lt;td align="left"&gt;85.5&lt;/td&gt; &lt;td align="left"&gt;86.0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;HLE — no tools&lt;/td&gt; &lt;td align="left"&gt;36.6&lt;/td&gt; &lt;td align="left"&gt;40.0&lt;/td&gt; &lt;td align="left"&gt;30.8&lt;/td&gt; &lt;td align="left"&gt;33.2&lt;/td&gt; &lt;td align="left"&gt;17.7&lt;/td&gt; &lt;td align="left"&gt;28.7&lt;/td&gt; &lt;td align="left"&gt;25.3&lt;/td&gt; &lt;td align="left"&gt;22.4&lt;/td&gt; &lt;td align="left"&gt;24.3&lt;/td&gt; &lt;td align="left"&gt;30.5&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;HLE — with tools&lt;/td&gt; &lt;td align="left"&gt;50.0&lt;/td&gt; &lt;td align="left"&gt;53.0&lt;/td&gt; &lt;td align="left"&gt;43.4&lt;/td&gt; &lt;td align="left"&gt;49.0&lt;/td&gt; &lt;td align="left"&gt;33.6&lt;/td&gt; &lt;td align="left"&gt;48.3&lt;/td&gt; &lt;td align="left"&gt;47.5&lt;/td&gt; &lt;td align="left"&gt;47.4&lt;/td&gt; &lt;td align="left"&gt;48.5&lt;/td&gt; &lt;td align="left"&gt;50.4&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;HMMT Feb 2025&lt;/td&gt; &lt;td align="left"&gt;99.4&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;92.9&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;94.8&lt;/td&gt; &lt;td align="left"&gt;91.4&lt;/td&gt; &lt;td align="left"&gt;89.0&lt;/td&gt; &lt;td align="left"&gt;92.0&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;HMMT Nov 2025&lt;/td&gt; &lt;td align="left"&gt;100&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;93.3&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;92.7&lt;/td&gt; &lt;td align="left"&gt;90.3&lt;/td&gt; &lt;td align="left"&gt;89.2&lt;/td&gt; &lt;td align="left"&gt;89.8&lt;/td&gt; &lt;td align="left"&gt;96.9&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Coding &amp;amp; Agentic&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;SWE-bench Verified&lt;/td&gt; &lt;td align="left"&gt;80.0&lt;/td&gt; &lt;td align="left"&gt;80.8&lt;/td&gt; &lt;td align="left"&gt;80.9&lt;/td&gt; &lt;td align="left"&gt;79.6&lt;/td&gt; &lt;td align="left"&gt;77.2&lt;/td&gt; &lt;td align="left"&gt;76.4&lt;/td&gt; &lt;td align="left"&gt;72.0&lt;/td&gt; &lt;td align="left"&gt;69.2&lt;/td&gt; &lt;td align="left"&gt;72.4&lt;/td&gt; &lt;td align="left"&gt;77.8&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Terminal-Bench 2.0&lt;/td&gt; &lt;td align="left"&gt;64.7&lt;/td&gt; &lt;td align="left"&gt;65.4&lt;/td&gt; &lt;td align="left"&gt;59.8&lt;/td&gt; &lt;td align="left"&gt;59.1&lt;/td&gt; &lt;td align="left"&gt;51.0&lt;/td&gt; &lt;td align="left"&gt;52.5&lt;/td&gt; &lt;td align="left"&gt;49.4&lt;/td&gt; &lt;td align="left"&gt;40.5&lt;/td&gt; &lt;td align="left"&gt;41.6&lt;/td&gt; &lt;td align="left"&gt;56.2&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;OSWorld-Verified&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;72.7&lt;/td&gt; &lt;td align="left"&gt;66.3&lt;/td&gt; &lt;td align="left"&gt;72.5&lt;/td&gt; &lt;td align="left"&gt;61.4&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;58.0&lt;/td&gt; &lt;td align="left"&gt;54.5&lt;/td&gt; &lt;td align="left"&gt;56.2&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;τ²-bench Retail&lt;/td&gt; &lt;td align="left"&gt;82.0&lt;/td&gt; &lt;td align="left"&gt;91.9&lt;/td&gt; &lt;td align="left"&gt;88.9&lt;/td&gt; &lt;td align="left"&gt;91.7&lt;/td&gt; &lt;td align="left"&gt;86.2&lt;/td&gt; &lt;td align="left"&gt;86.7&lt;/td&gt; &lt;td align="left"&gt;79.5&lt;/td&gt; &lt;td align="left"&gt;81.2&lt;/td&gt; &lt;td align="left"&gt;79.0&lt;/td&gt; &lt;td align="left"&gt;89.7&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MCP-Atlas&lt;/td&gt; &lt;td align="left"&gt;60.6&lt;/td&gt; &lt;td align="left"&gt;59.5&lt;/td&gt; &lt;td align="left"&gt;62.3&lt;/td&gt; &lt;td align="left"&gt;61.3&lt;/td&gt; &lt;td align="left"&gt;43.8&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;67.8&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;BrowseComp&lt;/td&gt; &lt;td align="left"&gt;65.8&lt;/td&gt; &lt;td align="left"&gt;84.0&lt;/td&gt; &lt;td align="left"&gt;67.8&lt;/td&gt; &lt;td align="left"&gt;74.7&lt;/td&gt; &lt;td align="left"&gt;43.9&lt;/td&gt; &lt;td align="left"&gt;69.0&lt;/td&gt; &lt;td align="left"&gt;63.8&lt;/td&gt; &lt;td align="left"&gt;61.0&lt;/td&gt; &lt;td align="left"&gt;61.0&lt;/td&gt; &lt;td align="left"&gt;75.9&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;LiveCodeBench v6&lt;/td&gt; &lt;td align="left"&gt;87.7&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;84.8&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;83.6&lt;/td&gt; &lt;td align="left"&gt;78.9&lt;/td&gt; &lt;td align="left"&gt;74.6&lt;/td&gt; &lt;td align="left"&gt;80.7&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;BFCL-V4&lt;/td&gt; &lt;td align="left"&gt;63.1&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;77.5&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;72.9&lt;/td&gt; &lt;td align="left"&gt;72.2&lt;/td&gt; &lt;td align="left"&gt;67.3&lt;/td&gt; &lt;td align="left"&gt;68.5&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Knowledge&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MMLU-Pro&lt;/td&gt; &lt;td align="left"&gt;87.4&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;89.5&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;87.8&lt;/td&gt; &lt;td align="left"&gt;86.7&lt;/td&gt; &lt;td align="left"&gt;85.3&lt;/td&gt; &lt;td align="left"&gt;86.1&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MMLU-Redux&lt;/td&gt; &lt;td align="left"&gt;95.0&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;95.6&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;94.9&lt;/td&gt; &lt;td align="left"&gt;94.0&lt;/td&gt; &lt;td align="left"&gt;93.3&lt;/td&gt; &lt;td align="left"&gt;93.2&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;SuperGPQA&lt;/td&gt; &lt;td align="left"&gt;67.9&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;70.6&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;70.4&lt;/td&gt; &lt;td align="left"&gt;67.1&lt;/td&gt; &lt;td align="left"&gt;63.4&lt;/td&gt; &lt;td align="left"&gt;65.6&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Instruction Following&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IFEval&lt;/td&gt; &lt;td align="left"&gt;94.8&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;90.9&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;92.6&lt;/td&gt; &lt;td align="left"&gt;93.4&lt;/td&gt; &lt;td align="left"&gt;91.9&lt;/td&gt; &lt;td align="left"&gt;95.0&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IFBench&lt;/td&gt; &lt;td align="left"&gt;75.4&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;58.0&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;76.5&lt;/td&gt; &lt;td align="left"&gt;76.1&lt;/td&gt; &lt;td align="left"&gt;70.2&lt;/td&gt; &lt;td align="left"&gt;76.5&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MultiChallenge&lt;/td&gt; &lt;td align="left"&gt;57.9&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;54.2&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;67.6&lt;/td&gt; &lt;td align="left"&gt;61.5&lt;/td&gt; &lt;td align="left"&gt;60.0&lt;/td&gt; &lt;td align="left"&gt;60.8&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Long Context&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;LongBench v2&lt;/td&gt; &lt;td align="left"&gt;54.5&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;64.4&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;63.2&lt;/td&gt; &lt;td align="left"&gt;60.2&lt;/td&gt; &lt;td align="left"&gt;59.0&lt;/td&gt; &lt;td align="left"&gt;60.6&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;AA-LCR&lt;/td&gt; &lt;td align="left"&gt;72.7&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;74.0&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;68.7&lt;/td&gt; &lt;td align="left"&gt;66.9&lt;/td&gt; &lt;td align="left"&gt;58.5&lt;/td&gt; &lt;td align="left"&gt;66.1&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Multilingual&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MMMLU&lt;/td&gt; &lt;td align="left"&gt;89.6&lt;/td&gt; &lt;td align="left"&gt;91.1&lt;/td&gt; &lt;td align="left"&gt;90.8&lt;/td&gt; &lt;td align="left"&gt;89.3&lt;/td&gt; &lt;td align="left"&gt;89.5&lt;/td&gt; &lt;td align="left"&gt;88.5&lt;/td&gt; &lt;td align="left"&gt;86.7&lt;/td&gt; &lt;td align="left"&gt;85.2&lt;/td&gt; &lt;td align="left"&gt;85.9&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MMLU-ProX&lt;/td&gt; &lt;td align="left"&gt;83.7&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;85.7&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;84.7&lt;/td&gt; &lt;td align="left"&gt;82.2&lt;/td&gt; &lt;td align="left"&gt;81.0&lt;/td&gt; &lt;td align="left"&gt;82.2&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;PolyMATH&lt;/td&gt; &lt;td align="left"&gt;62.5&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;79.0&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;td align="left"&gt;73.3&lt;/td&gt; &lt;td align="left"&gt;68.9&lt;/td&gt; &lt;td align="left"&gt;64.4&lt;/td&gt; &lt;td align="left"&gt;71.2&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pristine-Woodpecker"&gt; /u/Pristine-Woodpecker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5bgiva65rhlg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdpfy6/open_vs_closed_source_sota_benchmark_overview/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rdpfy6/open_vs_closed_source_sota_benchmark_overview/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T19:08:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1rd80gx</id>
    <title>I just saw something amazing</title>
    <updated>2026-02-24T05:49:17+00:00</updated>
    <author>
      <name>/u/ayanami0011</name>
      <uri>https://old.reddit.com/user/ayanami0011</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rd80gx/i_just_saw_something_amazing/"&gt; &lt;img alt="I just saw something amazing" src="https://preview.redd.it/rr17jgdksdlg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8c7fff37ff972da0293a348d64378188d1acef13" title="I just saw something amazing" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.asus.com/displays-desktops/workstations/performance/expertcenter-pro-et900n-g3/"&gt;https://www.asus.com/displays-desktops/workstations/performance/expertcenter-pro-et900n-g3/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.azken.com/Workstations/nvidia-series/Asus-ExpertCenter-Pro-ET900N-G3?utm%5C_source=chatgpt.com"&gt;https://www.azken.com/Workstations/nvidia-series/Asus-ExpertCenter-Pro-ET900N-G3?utm\_source=chatgpt.com&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ayanami0011"&gt; /u/ayanami0011 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rr17jgdksdlg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rd80gx/i_just_saw_something_amazing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rd80gx/i_just_saw_something_amazing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T05:49:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdjqeh</id>
    <title>This is the OPEN AI and sharing of Knowledge we were promised, keep accelerating or pop the bubble. Stop complaining. All gas no brakes!</title>
    <updated>2026-02-24T15:46:14+00:00</updated>
    <author>
      <name>/u/TroyDoesAI</name>
      <uri>https://old.reddit.com/user/TroyDoesAI</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdjqeh/this_is_the_open_ai_and_sharing_of_knowledge_we/"&gt; &lt;img alt="This is the OPEN AI and sharing of Knowledge we were promised, keep accelerating or pop the bubble. Stop complaining. All gas no brakes!" src="https://preview.redd.it/e3lkcom2rglg1.jpg?width=140&amp;amp;height=89&amp;amp;auto=webp&amp;amp;s=5b8b397c306b1521a19548c11d6f28b05027e9c9" title="This is the OPEN AI and sharing of Knowledge we were promised, keep accelerating or pop the bubble. Stop complaining. All gas no brakes!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Do you agree?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TroyDoesAI"&gt; /u/TroyDoesAI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1rdjqeh"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdjqeh/this_is_the_open_ai_and_sharing_of_knowledge_we/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rdjqeh/this_is_the_open_ai_and_sharing_of_knowledge_we/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T15:46:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdsnk3</id>
    <title>Qwen3.5 27B solves Car wash test!</title>
    <updated>2026-02-24T21:03:37+00:00</updated>
    <author>
      <name>/u/Ok-Scarcity-7875</name>
      <uri>https://old.reddit.com/user/Ok-Scarcity-7875</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdsnk3/qwen35_27b_solves_car_wash_test/"&gt; &lt;img alt="Qwen3.5 27B solves Car wash test!" src="https://preview.redd.it/9l26xxambilg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=275e71ef3332482e7619efffee825531a8926877" title="Qwen3.5 27B solves Car wash test!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's better than GPT-5.2 (in this regard)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Scarcity-7875"&gt; /u/Ok-Scarcity-7875 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9l26xxambilg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdsnk3/qwen35_27b_solves_car_wash_test/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rdsnk3/qwen35_27b_solves_car_wash_test/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T21:03:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdpapc</id>
    <title>Chinese AI Models Capture Majority of OpenRouter Token Volume as MiniMax M2.5 Surges to the Top</title>
    <updated>2026-02-24T19:03:31+00:00</updated>
    <author>
      <name>/u/Koyaanisquatsi_</name>
      <uri>https://old.reddit.com/user/Koyaanisquatsi_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdpapc/chinese_ai_models_capture_majority_of_openrouter/"&gt; &lt;img alt="Chinese AI Models Capture Majority of OpenRouter Token Volume as MiniMax M2.5 Surges to the Top" src="https://external-preview.redd.it/UdA_L_LSkBxAXLcEK0SYU0vLwVAamHz6zalROM7oXL4.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6daf7b75d52a71971a90702c20da023dbf05a439" title="Chinese AI Models Capture Majority of OpenRouter Token Volume as MiniMax M2.5 Surges to the Top" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Koyaanisquatsi_"&gt; /u/Koyaanisquatsi_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wealthari.com/chinese-ai-models-capture-majority-of-openrouter-token-volume-as-minimax-m2-5-surges-to-the-top/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdpapc/chinese_ai_models_capture_majority_of_openrouter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rdpapc/chinese_ai_models_capture_majority_of_openrouter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T19:03:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdnxe6</id>
    <title>Qwen3-Coder-Next vs Qwen3.5-35B-A3B vs Qwen3.5-27B - A quick coding test</title>
    <updated>2026-02-24T18:15:17+00:00</updated>
    <author>
      <name>/u/bobaburger</name>
      <uri>https://old.reddit.com/user/bobaburger</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdnxe6/qwen3codernext_vs_qwen3535ba3b_vs_qwen3527b_a/"&gt; &lt;img alt="Qwen3-Coder-Next vs Qwen3.5-35B-A3B vs Qwen3.5-27B - A quick coding test" src="https://preview.redd.it/hu6rne78hhlg1.png?width=140&amp;amp;height=63&amp;amp;auto=webp&amp;amp;s=aec2c2eeecc0656b1903e464de78880329384341" title="Qwen3-Coder-Next vs Qwen3.5-35B-A3B vs Qwen3.5-27B - A quick coding test" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/hu6rne78hhlg1.png?width=2546&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f5ba5093633344e41f2c35671835f75e738f08d9"&gt;https://preview.redd.it/hu6rne78hhlg1.png?width=2546&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f5ba5093633344e41f2c35671835f75e738f08d9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;While we're waiting for the GGUF, I ran a quick test to compare the one shot ability between the 3 models on Qwen Chat.&lt;/p&gt; &lt;p&gt;Building two examples: a jumping knight game and a sand game. You can see the live version here &lt;a href="https://qwen-bench.vercel.app/"&gt;https://qwen-bench.vercel.app/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Knight game&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The three models completed the knight game with good results, the game is working, knight placing and jumping animation works, with Qwen3.5 models has better styling, but Qwen3 is more functional, since it can place multiple knights on the board. In my experience, smaller quants of Qwen3-Coder-Next like Q3, IQ3, IQ2, TQ1,... all struggling to make the working board, not even having animation.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Score&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-Coder-Next&lt;/td&gt; &lt;td align="left"&gt;2.5&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3.5-35B-A3B&lt;/td&gt; &lt;td align="left"&gt;2.5&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3.5-27B&lt;/td&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Sand game&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Qwen3.5 27B was a disappointment here, the game was broken. 35B created the most beautiful version in term of colors. Functionality, both 35B and Qwen3 Coder Next done well, but Qwen3 Coder Next has a better fire animation and burning effect. In fact, 35B's fire was like a stage firework. It only damage the part of the wood it touched. Qwen3 Coder Next was able to make the spreading fire to burn the wood better, so the clear winner for this test is Qwen3 Coder Next.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Score&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-Coder-Next&lt;/td&gt; &lt;td align="left"&gt;3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3.5-35B-A3B&lt;/td&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3.5-27B&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Final score&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Qwen3 Coder Next still a clear winner, but I'm moving to Qwen3.5 35B for local coding now, since it's definitely smaller and faster, fit better for my PC. You served me well, rest in peace Qwen3 Coder Next!&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Score&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-Coder-Next&lt;/td&gt; &lt;td align="left"&gt;5.5&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3.5-35B-A3B&lt;/td&gt; &lt;td align="left"&gt;4.5&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3.5-27B&lt;/td&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;**Update:** managed to get sometime running this using Claude Code + llama.cpp, so far, it can run fast, using tools, thinking, loading custom skills, doing code edit well. You can see the example session log and llama log here &lt;a href="https://gist.github.com/huytd/43c9826d269b59887eab3e05a7bcb99c"&gt;https://gist.github.com/huytd/43c9826d269b59887eab3e05a7bcb99c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;On average, here's the speed for MXFP4 on 64 GB M2 Max MBP:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;PP Speed: 398.06 tokens/sec&lt;/li&gt; &lt;li&gt;TG Speed: 27.91 tokens/sec&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bobaburger"&gt; /u/bobaburger &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdnxe6/qwen3codernext_vs_qwen3535ba3b_vs_qwen3527b_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdnxe6/qwen3codernext_vs_qwen3535ba3b_vs_qwen3527b_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rdnxe6/qwen3codernext_vs_qwen3535ba3b_vs_qwen3527b_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T18:15:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdpz30</id>
    <title>Connected LFM2.5-VL-1.6B to my Blink security camera — 51 tokens/sec with APPLE GPU</title>
    <updated>2026-02-24T19:27:23+00:00</updated>
    <author>
      <name>/u/solderzzc</name>
      <uri>https://old.reddit.com/user/solderzzc</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdpz30/connected_lfm25vl16b_to_my_blink_security_camera/"&gt; &lt;img alt="Connected LFM2.5-VL-1.6B to my Blink security camera — 51 tokens/sec with APPLE GPU" src="https://preview.redd.it/z7feshqbuhlg1.png?width=140&amp;amp;height=82&amp;amp;auto=webp&amp;amp;s=447f0c1be012010975ae4d3ed043c15cfb828988" title="Connected LFM2.5-VL-1.6B to my Blink security camera — 51 tokens/sec with APPLE GPU" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've tested a lot of local VLMs for security camera analysis — SmolVLM2, Qwen3-VL, MiniCPM-V, LLaVA.&lt;/p&gt; &lt;p&gt;LFM2.5-VL-1.6B from LiquidAI is the one I keep coming back to. Here's why.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;One example output:&lt;/strong&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&amp;quot;A mailman is delivering mail to a suburban house. The mailman is wearing a blue uniform and carrying a white mail bag. The house is white with a brown roof, and there's a driveway with a black car parked in front. The mailman is walking on a brick path surrounded by green bushes and trees.&amp;quot;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;For a 1.6B parameter model, that's remarkable scene comprehension — roles, clothing, objects, spatial layout, all correctly identified. Not &amp;quot;person detected.&amp;quot; A full narrative.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What makes LFM2.5 special for this use case:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Speed:&lt;/strong&gt; ~51.8 tokens/sec on Apple Silicon — fast enough for continuous monitoring without bottlenecking&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Efficiency:&lt;/strong&gt; Fully utilizes Apple GPU via Metal during inference (~99% GPU, ~2.3 GB GPU memory), then drops back to idle immediately — inference is so fast it's hard to even screenshot at peak&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Size:&lt;/strong&gt; Q8_0 quantization is 1.2 GB model + 556 MB projector = 1.7 GB total. Fits comfortably on 8GB machines&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Consistency:&lt;/strong&gt; After months of daily use, it reliably produces useful scene descriptions across day/night, indoor/outdoor, and IR cameras&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Setup:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;MacBook M3 Air 24GB&lt;/li&gt; &lt;li&gt;SharpAI Aegis (free): &lt;a href="https://www.sharpai.org"&gt;https://www.sharpai.org&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Model: LiquidAI/LFM2.5-VL-1.6B-GGUF (Q8_0)&lt;/li&gt; &lt;li&gt;Total model size: ~1.7 GB (model + vision projector)&lt;/li&gt; &lt;li&gt;Camera: Blink Battery 4th Gen&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/solderzzc"&gt; /u/solderzzc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1rdpz30"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdpz30/connected_lfm25vl16b_to_my_blink_security_camera/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rdpz30/connected_lfm25vl16b_to_my_blink_security_camera/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T19:27:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1rd2x61</id>
    <title>People are getting it wrong; Anthropic doesn't care about the distillation, they just want to counter the narrative about Chinese open-source models catching up with closed-source frontier models</title>
    <updated>2026-02-24T02:54:22+00:00</updated>
    <author>
      <name>/u/obvithrowaway34434</name>
      <uri>https://old.reddit.com/user/obvithrowaway34434</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rd2x61/people_are_getting_it_wrong_anthropic_doesnt_care/"&gt; &lt;img alt="People are getting it wrong; Anthropic doesn't care about the distillation, they just want to counter the narrative about Chinese open-source models catching up with closed-source frontier models" src="https://preview.redd.it/1ulaheylwclg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7333a0173119c9f64b93f296b5b27a05c6260830" title="People are getting it wrong; Anthropic doesn't care about the distillation, they just want to counter the narrative about Chinese open-source models catching up with closed-source frontier models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Why would they care about distillation when they probably have done the same with OpenAI models and the Chinese labs are paying for the tokens? This is just their attempt to explain to investors and the US government that cheap Chinese models will never be as good as their models without distillation or stealing model weights from them. And they need to put more restrictions on China to prevent the technology transfer.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/obvithrowaway34434"&gt; /u/obvithrowaway34434 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1ulaheylwclg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rd2x61/people_are_getting_it_wrong_anthropic_doesnt_care/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rd2x61/people_are_getting_it_wrong_anthropic_doesnt_care/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T02:54:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdpuwy</id>
    <title>Qwen 3.5 family benchmarks</title>
    <updated>2026-02-24T19:23:19+00:00</updated>
    <author>
      <name>/u/tarruda</name>
      <uri>https://old.reddit.com/user/tarruda</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdpuwy/qwen_35_family_benchmarks/"&gt; &lt;img alt="Qwen 3.5 family benchmarks" src="https://external-preview.redd.it/uvtYuVLX1W6lNW0vkuVFAlyQWzygqnzGzHojqz3TXJY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f267cd2548c9b615466c10a981b0c958821223d3" title="Qwen 3.5 family benchmarks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tarruda"&gt; /u/tarruda &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://beige-babbette-30.tiiny.site/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdpuwy/qwen_35_family_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rdpuwy/qwen_35_family_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T19:23:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcpmwn</id>
    <title>Anthropic: "We’ve identified industrial-scale distillation attacks on our models by DeepSeek, Moonshot AI, and MiniMax." 🚨</title>
    <updated>2026-02-23T18:32:45+00:00</updated>
    <author>
      <name>/u/KvAk_AKPlaysYT</name>
      <uri>https://old.reddit.com/user/KvAk_AKPlaysYT</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcpmwn/anthropic_weve_identified_industrialscale/"&gt; &lt;img alt="Anthropic: &amp;quot;We’ve identified industrial-scale distillation attacks on our models by DeepSeek, Moonshot AI, and MiniMax.&amp;quot; 🚨" src="https://preview.redd.it/94fbimavfalg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c2ad159232448ffd7033d6be4fa96582b674e461" title="Anthropic: &amp;quot;We’ve identified industrial-scale distillation attacks on our models by DeepSeek, Moonshot AI, and MiniMax.&amp;quot; 🚨" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KvAk_AKPlaysYT"&gt; /u/KvAk_AKPlaysYT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/94fbimavfalg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcpmwn/anthropic_weve_identified_industrialscale/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rcpmwn/anthropic_weve_identified_industrialscale/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T18:32:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdmbhv</id>
    <title>Qwen3.5 - The middle child's 122B-A10B benchmarks looking seriously impressive - on par or edges out gpt-5-mini consistently</title>
    <updated>2026-02-24T17:18:38+00:00</updated>
    <author>
      <name>/u/carteakey</name>
      <uri>https://old.reddit.com/user/carteakey</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdmbhv/qwen35_the_middle_childs_122ba10b_benchmarks/"&gt; &lt;img alt="Qwen3.5 - The middle child's 122B-A10B benchmarks looking seriously impressive - on par or edges out gpt-5-mini consistently" src="https://preview.redd.it/zb1gzzm9ahlg1.png?width=140&amp;amp;height=92&amp;amp;auto=webp&amp;amp;s=dd9a6363f0aaf6174a644600da4f8c0b32b87331" title="Qwen3.5 - The middle child's 122B-A10B benchmarks looking seriously impressive - on par or edges out gpt-5-mini consistently" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/zb1gzzm9ahlg1.png?width=3000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2fe11dfb13a252dacd0ae8c250f4ec17d1a51d93"&gt;https://preview.redd.it/zb1gzzm9ahlg1.png?width=3000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2fe11dfb13a252dacd0ae8c250f4ec17d1a51d93&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Qwen3.5-122B-A10B generally comes out ahead of gpt-5-mini and gpt-oss-120b across most benchmarks.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;vs GPT-5-mini:&lt;/strong&gt; Qwen3.5 wins on knowledge (MMLU-Pro 86.7 vs 83.7), STEM reasoning (GPQA Diamond 86.6 vs 82.8), agentic tasks (BFCL-V4 72.2 vs 55.5), and vision tasks (MathVision 86.2 vs 71.9). GPT-5-mini is only competitive in a few coding benchmarks and translation.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;vs GPT-OSS-120B:&lt;/strong&gt; Qwen3.5 wins more decisively. GPT-OSS-120B holds its own in competitive coding (LiveCodeBench 82.7 vs 78.9) but falls behind significantly on knowledge, agents, vision, and multilingual tasks.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; Qwen3.5-122B-A10B is the strongest of the three overall. GPT-5-mini is its closest rival in coding/translation. GPT-OSS-120B trails outside of coding.&lt;/p&gt; &lt;p&gt;Lets see if the quants hold up to the benchmarks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/carteakey"&gt; /u/carteakey &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdmbhv/qwen35_the_middle_childs_122ba10b_benchmarks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdmbhv/qwen35_the_middle_childs_122ba10b_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rdmbhv/qwen35_the_middle_childs_122ba10b_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T17:18:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdo1z5</id>
    <title>Qwen 3.5 122b/35b is fire 🔥 Score comparision between Qwen 3 35B-A3B, GPT-5 High, Qwen 3 122B-A10B, and GPT-OSS 120B.</title>
    <updated>2026-02-24T18:19:40+00:00</updated>
    <author>
      <name>/u/9r4n4y</name>
      <uri>https://old.reddit.com/user/9r4n4y</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdo1z5/qwen_35_122b35b_is_fire_score_comparision_between/"&gt; &lt;img alt="Qwen 3.5 122b/35b is fire 🔥 Score comparision between Qwen 3 35B-A3B, GPT-5 High, Qwen 3 122B-A10B, and GPT-OSS 120B." src="https://preview.redd.it/01tsyrq8ihlg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=60c3fdc89c1d046a8cdd786122d9e9d61d8c1f82" title="Qwen 3.5 122b/35b is fire 🔥 Score comparision between Qwen 3 35B-A3B, GPT-5 High, Qwen 3 122B-A10B, and GPT-OSS 120B." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;EDIT: ⚠️⚠️⚠️ SORRY 🥲 --&amp;gt; in graph its should be qwen 3.5 not qwen 3 ⚠️⚠️&lt;/p&gt; &lt;p&gt;Benchmark Comparison&lt;/p&gt; &lt;p&gt;👉🔴GPT-OSS 120B [defeated by qwen 3.5 35b 🥳]&lt;/p&gt; &lt;p&gt;MMLU-Pro: 80.8&lt;/p&gt; &lt;p&gt;HLE (Humanity’s Last Exam): 14.9&lt;/p&gt; &lt;p&gt;GPQA Diamond: 80.1&lt;/p&gt; &lt;p&gt;IFBench: 69.0&lt;/p&gt; &lt;p&gt;👉🔴Qwen 3.5 122B-A10B&lt;/p&gt; &lt;p&gt;MMLU-Pro: 86.7&lt;/p&gt; &lt;p&gt;HLE (Humanity’s Last Exam): 25.3 (47.5 with tools — 🏆 Winner)&lt;/p&gt; &lt;p&gt;GPQA Diamond: 86.6 (🏆 Winner)&lt;/p&gt; &lt;p&gt;IFBench: 76.1 (🏆 Winner)&lt;/p&gt; &lt;p&gt;👉🔴Qwen 3.5 35B-A3B&lt;/p&gt; &lt;p&gt;MMLU-Pro: 85.3&lt;/p&gt; &lt;p&gt;HLE (Humanity’s Last Exam): 22.4 (47.4 with tools)&lt;/p&gt; &lt;p&gt;GPQA Diamond: 84.2&lt;/p&gt; &lt;p&gt;IFBench: 70.2&lt;/p&gt; &lt;p&gt;👉🔴GPT-5 High&lt;/p&gt; &lt;p&gt;MMLU-Pro: 87.1 (🏆 Winner)&lt;/p&gt; &lt;p&gt;HLE (Humanity’s Last Exam): 26.5 (🏆 Winner, no tools)&lt;/p&gt; &lt;p&gt;GPQA Diamond: 85.4&lt;/p&gt; &lt;p&gt;IFBench: 73.1&lt;/p&gt; &lt;p&gt;Summary: GPT 5 [HIGH] ≈ Qwen 3.5 122b &amp;gt; qwen 35b &amp;gt; gpt oss 120 [high]&lt;/p&gt; &lt;p&gt;👉Sources: OPENROUTER, ARTIFICIAL ANALYSIS, HUGGING FACE&lt;/p&gt; &lt;p&gt;GGUF Download 💚 link 🔗 : &lt;a href="https://huggingface.co/collections/unsloth/qwen35"&gt;https://huggingface.co/collections/unsloth/qwen35&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/9r4n4y"&gt; /u/9r4n4y &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/01tsyrq8ihlg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdo1z5/qwen_35_122b35b_is_fire_score_comparision_between/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rdo1z5/qwen_35_122b35b_is_fire_score_comparision_between/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T18:19:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1rd8cfw</id>
    <title>Anthropic's recent distillation blog should make anyone only ever want to use local open-weight models; it's scary and dystopian</title>
    <updated>2026-02-24T06:07:02+00:00</updated>
    <author>
      <name>/u/obvithrowaway34434</name>
      <uri>https://old.reddit.com/user/obvithrowaway34434</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rd8cfw/anthropics_recent_distillation_blog_should_make/"&gt; &lt;img alt="Anthropic's recent distillation blog should make anyone only ever want to use local open-weight models; it's scary and dystopian" src="https://preview.redd.it/086f3wnavdlg1.png?width=140&amp;amp;height=66&amp;amp;auto=webp&amp;amp;s=3125bb81f69aa57e4305e4471c6284c4a9a52a12" title="Anthropic's recent distillation blog should make anyone only ever want to use local open-weight models; it's scary and dystopian" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's quite ironic that they went for the censorship and authoritarian angles here.&lt;/p&gt; &lt;p&gt;Full blog: &lt;a href="https://www.anthropic.com/news/detecting-and-preventing-distillation-attacks"&gt;https://www.anthropic.com/news/detecting-and-preventing-distillation-attacks&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/obvithrowaway34434"&gt; /u/obvithrowaway34434 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1rd8cfw"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rd8cfw/anthropics_recent_distillation_blog_should_make/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rd8cfw/anthropics_recent_distillation_blog_should_make/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T06:07:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcvimv</id>
    <title>Distillation when you do it. Training when we do it.</title>
    <updated>2026-02-23T22:04:41+00:00</updated>
    <author>
      <name>/u/Xhehab_</name>
      <uri>https://old.reddit.com/user/Xhehab_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcvimv/distillation_when_you_do_it_training_when_we_do_it/"&gt; &lt;img alt="Distillation when you do it. Training when we do it." src="https://preview.redd.it/9rc0jqbohblg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=05481c4cef786a02ca1e5d0b968e61114727348f" title="Distillation when you do it. Training when we do it." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xhehab_"&gt; /u/Xhehab_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9rc0jqbohblg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcvimv/distillation_when_you_do_it_training_when_we_do_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rcvimv/distillation_when_you_do_it_training_when_we_do_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T22:04:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdi26s</id>
    <title>Liquid AI releases LFM2-24B-A2B</title>
    <updated>2026-02-24T14:43:33+00:00</updated>
    <author>
      <name>/u/PauLabartaBajo</name>
      <uri>https://old.reddit.com/user/PauLabartaBajo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdi26s/liquid_ai_releases_lfm224ba2b/"&gt; &lt;img alt="Liquid AI releases LFM2-24B-A2B" src="https://preview.redd.it/28drgi3ufglg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=211c668e7fadb69afdf5c7c5c74fa9ee4e0e85d1" title="Liquid AI releases LFM2-24B-A2B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Today, Liquid AI releases LFM2-24B-A2B, their largest LFM2 model to date&lt;/p&gt; &lt;p&gt;LFM2-24B-A2B is a sparse Mixture-of-Experts (MoE) model with 24 billion total parameters with 2 billion active per token, showing that the LFM2 hybrid architecture scales effectively to larger sizes maintaining quality without inflating per-token compute.&lt;/p&gt; &lt;p&gt;This release expands the LFM2 family from 350M to 24B parameters, demonstrating predictable scaling across nearly two orders of magnitude.&lt;/p&gt; &lt;p&gt;Key highlights:&lt;/p&gt; &lt;p&gt;-&amp;gt; MoE architecture: 40 layers, 64 experts per MoE block with top-4 routing, maintaining the hybrid conv + GQA design -&amp;gt; 2.3B active parameters per forward pass -&amp;gt; Designed to run within 32GB RAM, enabling deployment on high-end consumer laptops and desktops -&amp;gt; Day-zero support for inference through llama.cpp, vLLM, and SGLang -&amp;gt; Multiple GGUF quantizations available&lt;/p&gt; &lt;p&gt;Across benchmarks including GPQA Diamond, MMLU-Pro, IFEval, IFBench, GSM8K, and MATH-500, quality improves log-linearly as we scale from 350M to 24B, confirming that the LFM2 architecture does not plateau at small sizes.&lt;/p&gt; &lt;p&gt;LFM2-24B-A2B is released as an instruct model and is available open-weight on Hugging Face. We designed this model to concentrate capacity in total parameters, not active compute, keeping inference latency and energy consumption aligned with edge and local deployment constraints.&lt;/p&gt; &lt;p&gt;This is the next step in making fast, scalable, efficient AI accessible in the cloud and on-device. &lt;/p&gt; &lt;p&gt;-&amp;gt; Read the blog: &lt;a href="https://www.liquid.ai/blog/lfm2-24b-a2b"&gt;https://www.liquid.ai/blog/lfm2-24b-a2b&lt;/a&gt; -&amp;gt; Download weights: &lt;a href="https://huggingface.co/LiquidAI/LFM2-24B-A2B"&gt;https://huggingface.co/LiquidAI/LFM2-24B-A2B&lt;/a&gt; -&amp;gt; Check out our docs on how to run or fine-tune it locally: docs.liquid.ai -&amp;gt; Try it now: playground.liquid.ai&lt;/p&gt; &lt;p&gt;Run it locally or in the cloud and tell us what you build!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PauLabartaBajo"&gt; /u/PauLabartaBajo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/28drgi3ufglg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdi26s/liquid_ai_releases_lfm224ba2b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rdi26s/liquid_ai_releases_lfm224ba2b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T14:43:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdptw8</id>
    <title>more qwens will appear</title>
    <updated>2026-02-24T19:22:21+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdptw8/more_qwens_will_appear/"&gt; &lt;img alt="more qwens will appear" src="https://preview.redd.it/vxo4n3uhthlg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=75897cb1fe4342d9e8d35b46d9d2a84a28f17dc6" title="more qwens will appear" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;(remember that 9B was promised before)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vxo4n3uhthlg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdptw8/more_qwens_will_appear/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rdptw8/more_qwens_will_appear/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T19:22:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdlbvc</id>
    <title>Qwen/Qwen3.5-35B-A3B · Hugging Face</title>
    <updated>2026-02-24T16:44:05+00:00</updated>
    <author>
      <name>/u/ekojsalim</name>
      <uri>https://old.reddit.com/user/ekojsalim</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdlbvc/qwenqwen3535ba3b_hugging_face/"&gt; &lt;img alt="Qwen/Qwen3.5-35B-A3B · Hugging Face" src="https://external-preview.redd.it/9t9hISbgGxfk479gTZKF1XJ1oO6QhRPmNzUYpMNUbjs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f4846b1e5fd750530b9aa43eb95e74460e90d4ec" title="Qwen/Qwen3.5-35B-A3B · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ekojsalim"&gt; /u/ekojsalim &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3.5-35B-A3B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdlbvc/qwenqwen3535ba3b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rdlbvc/qwenqwen3535ba3b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T16:44:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdfhfx</id>
    <title>New Qwen3.5 models spotted on qwen chat</title>
    <updated>2026-02-24T12:55:10+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdfhfx/new_qwen35_models_spotted_on_qwen_chat/"&gt; &lt;img alt="New Qwen3.5 models spotted on qwen chat" src="https://preview.redd.it/h1c3uk0iwflg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b026f6069f044a6b506e0aae9a0c418d76865997" title="New Qwen3.5 models spotted on qwen chat" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/h1c3uk0iwflg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdfhfx/new_qwen35_models_spotted_on_qwen_chat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rdfhfx/new_qwen35_models_spotted_on_qwen_chat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T12:55:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdlc02</id>
    <title>Qwen/Qwen3.5-122B-A10B · Hugging Face</title>
    <updated>2026-02-24T16:44:13+00:00</updated>
    <author>
      <name>/u/coder543</name>
      <uri>https://old.reddit.com/user/coder543</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdlc02/qwenqwen35122ba10b_hugging_face/"&gt; &lt;img alt="Qwen/Qwen3.5-122B-A10B · Hugging Face" src="https://external-preview.redd.it/jXshLXVh7iCkI_DkUnvVFkKtp2L9P6wekJnwAzaRzjM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=298bac8d8df642a16a7b098a721723a8766a21d8" title="Qwen/Qwen3.5-122B-A10B · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/coder543"&gt; /u/coder543 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3.5-122B-A10B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdlc02/qwenqwen35122ba10b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rdlc02/qwenqwen35122ba10b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T16:44:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7bsfd</id>
    <title>Best Audio Models - Feb 2026</title>
    <updated>2026-02-17T17:01:37+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;They've been a ton of audio models released of late, the most notable perhaps being Qwen3 TTS. So its time for another &lt;strong&gt;Best Audio Models&lt;/strong&gt; megathread&lt;/p&gt; &lt;p&gt;Share what your favorite ASR, TTS, STT, Text to Music models are right now &lt;strong&gt;and why.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Given the the amount of ambiguity and subjectivity in rating/testing these models, please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks etc. Closed models like Elevenlabs v3 seem to continue to be a few levels above open models especially for production use cases with long lengths/stability requirements, so comparisons, especially empirical ones are welcome.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please use the top level comments to thread your responses.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T17:01:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8snay</id>
    <title>AMA with StepFun AI - Ask Us Anything</title>
    <updated>2026-02-19T07:15:35+00:00</updated>
    <author>
      <name>/u/StepFun_ai</name>
      <uri>https://old.reddit.com/user/StepFun_ai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt; &lt;img alt="AMA with StepFun AI - Ask Us Anything" src="https://external-preview.redd.it/bqLigXK_A-XO-amDfdyeQenKCxR18nhChWpDnQXYmr4.png?width=140&amp;amp;height=75&amp;amp;auto=webp&amp;amp;s=72c79bc533c167d8f77ecf629e3b7af53ee1e6b6" title="AMA with StepFun AI - Ask Us Anything" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/w8274fg1jekg1.png?width=1785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fadbd0ec26a56e60900f9ed667ae808217d70cf2"&gt;https://preview.redd.it/w8274fg1jekg1.png?width=1785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fadbd0ec26a56e60900f9ed667ae808217d70cf2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; !&lt;/p&gt; &lt;p&gt;We are &lt;strong&gt;StepFun&lt;/strong&gt;, the team behind the &lt;strong&gt;Step&lt;/strong&gt; family models, including &lt;a href="https://huggingface.co/collections/stepfun-ai/step-35-flash"&gt;&lt;strong&gt;Step 3.5 Flash&lt;/strong&gt;&lt;/a&gt; and &lt;a href="https://huggingface.co/collections/stepfun-ai/step3-vl-10b"&gt;&lt;strong&gt;Step-3-VL-10B&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;We are super excited to host our first AMA tomorrow in this community. Our participants include CEO, CTO, Chief Scientist, LLM Researchers.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Participants&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Ok_Reach_5122"&gt;u/Ok_Reach_5122&lt;/a&gt; (Co-founder &amp;amp; CEO of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/bobzhuyb"&gt;u/bobzhuyb&lt;/a&gt; (Co-founder &amp;amp; CTO of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/user/Lost-Nectarine1016"&gt;u/Lost-Nectarine1016&lt;/a&gt; (Co-founder &amp;amp; Chief Scientist of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Elegant-Sale-1328"&gt;u/Elegant-Sale-1328&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/SavingsConclusion298"&gt;u/SavingsConclusion298&lt;/a&gt; (Post-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Spirited_Spirit3387"&gt;u/Spirited_Spirit3387&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/These-Nothing-8564/"&gt;u/These-Nothing-8564&lt;/a&gt; (Technical Project Manager)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Either-Beyond-7395"&gt;u/Either-Beyond-7395&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Human_Ad_162"&gt;u/Human_Ad_162&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Icy_Dare_3866"&gt;u/Icy_Dare_3866&lt;/a&gt; (Post-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Big-Employee5595"&gt;u/Big-Employee5595&lt;/a&gt; (Agent Algorithms Lead&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run 8 - 11 AM PST, Feburary 19th. The StepFun team will monitor and answer questions over the 24 hours after the live session.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StepFun_ai"&gt; /u/StepFun_ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T07:15:35+00:00</published>
  </entry>
</feed>
