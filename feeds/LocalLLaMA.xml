<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-11T21:24:07+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ouknj3</id>
    <title>Compared 5 LLM observability platforms after production issues kept hitting us - here's what works</title>
    <updated>2025-11-11T20:33:40+00:00</updated>
    <author>
      <name>/u/Otherwise_Flan7339</name>
      <uri>https://old.reddit.com/user/Otherwise_Flan7339</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Our LLM app kept having silent failures in production. Responses would drift, costs would spike randomly, and we'd only find out when users complained. Realized we had zero visibility into what was actually happening.&lt;/p&gt; &lt;p&gt;Tested LangSmith, Arize, Langfuse, Braintrust, and Maxim over the last few months. Here's what I found:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;LangSmith&lt;/strong&gt; - Best if you're already deep in LangChain ecosystem. Full-stack tracing, prompt management, evaluation workflows. Python and TypeScript SDKs. OpenTelemetry integration is solid.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Arize&lt;/strong&gt; - Strong real-time monitoring and cost analytics. Good guardrail metrics for bias and toxicity detection. Focuses heavily on debugging model outputs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Langfuse&lt;/strong&gt; - Open-source option with self-hosting. Session tracking, batch exports, SOC2 compliant. Good if you want control over your deployment.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Braintrust&lt;/strong&gt; - Simulation and evaluation focused. External annotator integration for quality checks. Lighter on production observability compared to others.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Maxim&lt;/strong&gt; - Covers simulation, evaluation, and observability together. Granular agent-level tracing, automated eval workflows, enterprise compliance (SOC2). They also have their open source &lt;a href="https://getmax.im/bifr0st"&gt;Bifrost&lt;/a&gt; LLM Gateway with ultra low overhead at high RPS (~5k) which is wild for high-throughput deployments.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Biggest learning: you need observability before things break, not after. Tracing at the agent-level matters more than just logging inputs/outputs. Cost and quality drift silently without proper monitoring.&lt;/p&gt; &lt;p&gt;What are you guys using for production monitoring? Anyone dealing with non-deterministic output issues?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Otherwise_Flan7339"&gt; /u/Otherwise_Flan7339 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ouknj3/compared_5_llm_observability_platforms_after/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ouknj3/compared_5_llm_observability_platforms_after/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ouknj3/compared_5_llm_observability_platforms_after/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-11T20:33:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1otscki</id>
    <title>Reflection AI reached human-level performance (85%) on ARC-AGI v1 for under $10k and within 12 hours. You can run this code yourself, it‚Äôs open source.</title>
    <updated>2025-11-10T22:37:40+00:00</updated>
    <author>
      <name>/u/balianone</name>
      <uri>https://old.reddit.com/user/balianone</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otscki/reflection_ai_reached_humanlevel_performance_85/"&gt; &lt;img alt="Reflection AI reached human-level performance (85%) on ARC-AGI v1 for under $10k and within 12 hours. You can run this code yourself, it‚Äôs open source." src="https://external-preview.redd.it/ARR7y9mlLeCC9oWmE5UREkOw8RADA8XOccGD021Q5lw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e4be25bea8245c672919ac843febfe66e1da8de0" title="Reflection AI reached human-level performance (85%) on ARC-AGI v1 for under $10k and within 12 hours. You can run this code yourself, it‚Äôs open source." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/balianone"&gt; /u/balianone &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/jerber/arc-lang-public"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otscki/reflection_ai_reached_humanlevel_performance_85/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1otscki/reflection_ai_reached_humanlevel_performance_85/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T22:37:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1otwek3</id>
    <title>Full Replication of Google's Nested Learning Paper in PyTorch ‚Äì code now live</title>
    <updated>2025-11-11T01:29:37+00:00</updated>
    <author>
      <name>/u/complains_constantly</name>
      <uri>https://old.reddit.com/user/complains_constantly</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Some of you may have seen Google Research‚Äôs &lt;a href="https://research.google/blog/introducing-nested-learning-a-new-ml-paradigm-for-continual-learning/"&gt;&lt;strong&gt;Nested Learning paper&lt;/strong&gt;&lt;/a&gt;. They introduced HOPE, a self-modifying TITAN variant with a Continuum Memory System (multi-frequency FFN chain) + deep optimizer stack. They published the research but no code (like always), so I rebuilt the architecture and infra in PyTorch over the weekend.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/kmccleary3301/nested_learning"&gt;https://github.com/kmccleary3301/nested_learning&lt;/a&gt;&lt;/p&gt; &lt;h2&gt;Highlights&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Level clock + CMS implementation (update-period gating, associative-memory optimizers).&lt;/li&gt; &lt;li&gt;HOPE block w/ attention, TITAN memory, self-modifier pathway.&lt;/li&gt; &lt;li&gt;Hydra configs for pilot/mid/target scales, uv-managed env, Deepspeed/FSDP launchers.&lt;/li&gt; &lt;li&gt;Data pipeline: filtered RefinedWeb + supplements (C4, RedPajama, code) with tokenizer/sharding scripts.&lt;/li&gt; &lt;li&gt;Evaluation: zero-shot harness covering PIQA, HellaSwag, WinoGrande, ARC-E/C, BoolQ, SIQA, CommonsenseQA, OpenBookQA + NIAH long-context script.&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;What I need help with:&lt;/h2&gt; &lt;ol&gt; &lt;li&gt;Running larger training configs (760M+, 4‚Äì8k context) and reporting W&amp;amp;B benchmarks.&lt;/li&gt; &lt;li&gt;Stress-testing CMS/self-modifier stability + alternative attention backbones.&lt;/li&gt; &lt;li&gt;Continual-learning evaluation (streaming domains) &amp;amp; regression tests.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;If you try it, please file issues/PRs‚Äîespecially around stability tricks, data pipelines, or eval scripts. Would love to see how it stacks up against these Qwen, DeepSeek, Minimax, and Kimi architectures.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/complains_constantly"&gt; /u/complains_constantly &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otwek3/full_replication_of_googles_nested_learning_paper/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otwek3/full_replication_of_googles_nested_learning_paper/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1otwek3/full_replication_of_googles_nested_learning_paper/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-11T01:29:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1ou4qvh</id>
    <title>RAG Paper 25.11.11</title>
    <updated>2025-11-11T09:06:14+00:00</updated>
    <author>
      <name>/u/Cheryl_Apple</name>
      <uri>https://old.reddit.com/user/Cheryl_Apple</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ol&gt; &lt;li&gt;&lt;p&gt;&lt;a href="http://arxiv.org/abs/2511.07328v1"&gt;Q-RAG: Long Context Multi-step Retrieval via Value-based Embedder Training&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;a href="http://arxiv.org/abs/2511.07262v1"&gt;AgenticSciML: Collaborative Multi-Agent Systems for Emergent Discovery in Scientific Machine Learning&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;a href="http://arxiv.org/abs/2511.06973v1"&gt;Oh That Looks Familiar: A Novel Similarity Measure for Spreadsheet Template Discovery&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;a href="http://arxiv.org/abs/2511.06738v1"&gt;Rethinking Retrieval-Augmented Generation for Medicine: A Large-Scale, Systematic Expert Evaluation and Practical Insights&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;a href="http://arxiv.org/abs/2511.06668v1"&gt;When Evidence Contradicts: Toward Safer Retrieval-Augmented Generation in Healthcare&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;a href="http://arxiv.org/abs/2511.06582v1"&gt;TabRAG: Tabular Document Retrieval via Structured Language Representations&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Collected by OpenBMB, transferred by&lt;/strong&gt; &lt;a href="https://www.ragview.ai/"&gt;&lt;strong&gt;RagView&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cheryl_Apple"&gt; /u/Cheryl_Apple &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ou4qvh/rag_paper_251111/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ou4qvh/rag_paper_251111/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ou4qvh/rag_paper_251111/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-11T09:06:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1oudxai</id>
    <title>Would Kimi K2 Thinking be decent at 2.5-3.5bpw quant range, given it is native 4 bits? Like ~3bpw and above for DeepSeek models that are native 8 bit.</title>
    <updated>2025-11-11T16:26:58+00:00</updated>
    <author>
      <name>/u/panchovix</name>
      <uri>https://old.reddit.com/user/panchovix</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello guys, hoping you're fine.&lt;/p&gt; &lt;p&gt;I was wondering, given that Kimi K2 thinking is a native 4bit model, would a quantization not lobotomize that much in the 2.5-3.5 bpw range (like Q2_M to Q3_M size on lcpp terms)?&lt;/p&gt; &lt;p&gt;It was discussed that on the case of DeepSeek models, 3bpw and a bit higher (like IQ3_XXS and such) are pretty good despite being a quite substantial quantization.&lt;/p&gt; &lt;p&gt;What do you guys think? Have you tried a Kimi K2 Thinking quant? I'm trying Q2_K_XL (which is 3bpw) locally and it seems to be pretty good, but I can't run native 4bpw/4bit to compare.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/panchovix"&gt; /u/panchovix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oudxai/would_kimi_k2_thinking_be_decent_at_2535bpw_quant/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oudxai/would_kimi_k2_thinking_be_decent_at_2535bpw_quant/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oudxai/would_kimi_k2_thinking_be_decent_at_2535bpw_quant/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-11T16:26:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1ouea52</id>
    <title>MoE expert distributions for Kimi K2 thinking?</title>
    <updated>2025-11-11T16:40:15+00:00</updated>
    <author>
      <name>/u/Stunning-Document-53</name>
      <uri>https://old.reddit.com/user/Stunning-Document-53</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Does anyone have any idea what the expert distribution is for kimi k2 thinking? Would be good to know to estimate memory usage + performance. Ie, is the model using the same 8 experts across many tokens in a single task or does it regularly touch all ~300 experts&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Stunning-Document-53"&gt; /u/Stunning-Document-53 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ouea52/moe_expert_distributions_for_kimi_k2_thinking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ouea52/moe_expert_distributions_for_kimi_k2_thinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ouea52/moe_expert_distributions_for_kimi_k2_thinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-11T16:40:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1oub8dt</id>
    <title>Anyone been using local LLMs with Claude Code?</title>
    <updated>2025-11-11T14:44:46+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking for feedback/experience in using Qwen3-Coder:a3b, gpt-oss-120b or GLM 4.5 air with Claude Code locally.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oub8dt/anyone_been_using_local_llms_with_claude_code/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oub8dt/anyone_been_using_local_llms_with_claude_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oub8dt/anyone_been_using_local_llms_with_claude_code/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-11T14:44:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1ou4ubn</id>
    <title>Building LLM inference from scratch - clean, minimal and (sort of) fast</title>
    <updated>2025-11-11T09:12:29+00:00</updated>
    <author>
      <name>/u/garg-aayush</name>
      <uri>https://old.reddit.com/user/garg-aayush</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ou4ubn/building_llm_inference_from_scratch_clean_minimal/"&gt; &lt;img alt="Building LLM inference from scratch - clean, minimal and (sort of) fast" src="https://preview.redd.it/sozysc8wgl0g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5fd1ce0ff5b4fb76f2aa3daa6eb7e75a6ad13138" title="Building LLM inference from scratch - clean, minimal and (sort of) fast" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wrote my own LLM inference script for gpt-2 models from scratch following first principles with the motto of &lt;strong&gt;learning by building&lt;/strong&gt;. I built it incrementally starting from a very naive greedy decoding-based inference all the way to latency optimized (kv-cache/speculative decoding) inference using pytorch.&lt;/p&gt; &lt;p&gt;My implementation includes:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Inference &amp;amp; Sampling:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;greedy decoding, EOS handling, context window management using sliding window&lt;/li&gt; &lt;li&gt;temperature scaling, multinomial sampling&lt;/li&gt; &lt;li&gt;top-k and top-p (nucleus) sampling&lt;/li&gt; &lt;li&gt;presence, frequency, and repetition penalties controls&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Latency Optimizations:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;fp16/bf16 optimized inference&lt;/li&gt; &lt;li&gt;kv-cache (dynamic -&amp;gt; static + overflow fix) integration&lt;/li&gt; &lt;li&gt;variable-length batching with right-padding (allows for samples with different lengths)&lt;/li&gt; &lt;li&gt;draft-verify speculative decoding based on the &lt;a href="https://arxiv.org/abs/2302.01318"&gt;DeepMind paper&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I also benchmarked my kv-cache and speculative decoding implementations on GPT-2 models to see what kind of speedups are achievable using my implementations.&lt;/p&gt; &lt;p&gt;Here are the best speedups I was able to get:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;config:&lt;/strong&gt; RTX 4090, cuda 12.8, torch 2.9.0&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Optimization&lt;/th&gt; &lt;th align="left"&gt;Best Speedup (float32)&lt;/th&gt; &lt;th align="left"&gt;Best Speedup (float16)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;kv-cache&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;2.76√ó&lt;/strong&gt; (gpt2-large, 800 tokens)&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;1.48√ó&lt;/strong&gt; (gpt2-xl, 800 tokens)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;speculative decoding&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;1.63√ó&lt;/strong&gt; (draft: gpt2 -&amp;gt; target: gpt2-xl, gamma=5)&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;1.31√ó&lt;/strong&gt; (draft: gpt2 -&amp;gt; target: gpt2-xl, gamma=3)&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;The speedups are quite encouraging given the relatively small model sizes and my basic implementations without fancy tricks. :)&lt;/p&gt; &lt;p&gt;Like always, I've documented everything from the code, implementations and notes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Repo:&lt;/strong&gt; &lt;a href="https://github.com/garg-aayush/building-from-scratch/tree/main/llm-inference"&gt;https://github.com/garg-aayush/building-from-scratch/tree/main/llm-inference&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Detailed Readme and benchmarks:&lt;/strong&gt; &lt;a href="https://github.com/garg-aayush/building-from-scratch/blob/main/llm-inference/Readme.md"&gt;https://github.com/garg-aayush/building-from-scratch/blob/main/llm-inference/Readme.md&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Commit-by-commit development&lt;/strong&gt;: Each implementation and optimization is a separate commit for easy understanding&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/garg-aayush"&gt; /u/garg-aayush &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sozysc8wgl0g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ou4ubn/building_llm_inference_from_scratch_clean_minimal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ou4ubn/building_llm_inference_from_scratch_clean_minimal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-11T09:12:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1oufp8s</id>
    <title>Unlimited Cloud this week on Observer as a Thank You to r/LocalLLaMA! Free and local, now and forever after.</title>
    <updated>2025-11-11T17:31:48+00:00</updated>
    <author>
      <name>/u/Roy3838</name>
      <uri>https://old.reddit.com/user/Roy3838</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TLDR: Saved up some money to give you guys unlimited cloud access as a &lt;strong&gt;Thank You&lt;/strong&gt; and to stress test it. &lt;strong&gt;Comment an agent idea or feedback,&lt;/strong&gt; i'll DM you the unlimited access link, and &lt;strong&gt;build stuff&lt;/strong&gt;! It's Free for Local Inference now and always &amp;lt;3&lt;/p&gt; &lt;p&gt;Observer lets you build micro-agents that &lt;strong&gt;watch your screen, camera and microphone and trigger actions&lt;/strong&gt; - all running locally with your own models.&lt;/p&gt; &lt;p&gt;Hey &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;Okay so... I posted two days ago and it got downvoted because I sounded like a SaaS trying to trap people. That's completely on me! I've been talking to investors lately and had my &amp;quot;business brain&amp;quot; on (not very developed hahaha), but I shouldn't talk to you guys like that. I'm sorry!&lt;/p&gt; &lt;p&gt;So let me be super clear: &lt;strong&gt;Observer is free and open-source. Forever.&lt;/strong&gt; If you compile it yourself, point it at your local llama.cpp server, and use Discord notifications (which go straight from your computer to Discord), I literally have no way of knowing you exist. &lt;strong&gt;That's by design.&lt;/strong&gt; Privacy-first means privacy-first.&lt;/p&gt; &lt;p&gt;But here's the thing: I built an optional cloud backend so people who &lt;strong&gt;don't run LLMs&lt;/strong&gt; on their machines have a convenient option. And this week I need to stress test it. I saved up for API costs specifically so &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt; could use it for free this week - because if I'm giving anyone free unlimited access, it's &lt;strong&gt;you guys who supported this thing from the beginning.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;What I'm asking:&lt;/p&gt; &lt;p&gt;- &lt;strong&gt;Comment a cool agent idea&lt;/strong&gt; (seeing them is honestly my favorite part) and i'll &lt;strong&gt;DM you the link&lt;/strong&gt; that gives you unlimited access.&lt;/p&gt; &lt;p&gt;- Try building some agents (local or cloud, whatever you want!)&lt;/p&gt; &lt;p&gt;- Please don't abuse it - I saved up for this but I'm not Bezos üòÖ&lt;/p&gt; &lt;p&gt;Some agent ideas from the last post to get you started:&lt;/p&gt; &lt;p&gt;- &amp;quot;While a tuner connected to my microphone is listening to my practicing session on my violin I would like to get a ping by the AI everytime I'm out of tune by a particular cent parameter!&amp;quot; - &lt;a href="https://www.reddit.com/user/philosophissima/"&gt;philosophissima&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- &amp;quot;I'd like to use it to monitor email for certain keywords and notify different contacts based on the content&amp;quot; - &lt;a href="https://www.reddit.com/user/IbetitsBen/"&gt;IbetitsBen&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- &amp;quot;Ping my phone when the UPS van stops outside, but not the USPS one. I need to sign for a package.&amp;quot; &lt;a href="https://www.reddit.com/user/__JockY__/"&gt;__JockY__&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- Track long-running processes and notify when complete - i use this almost every day&lt;/p&gt; &lt;p&gt;- Literally anything that involves &lt;strong&gt;&amp;quot;watch this thing and tell me when X happens&amp;quot;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Just drop a comment with what you want to build&lt;/strong&gt; and I'll DM you unlimited cloud access. Or if you want to go full local, the GitHub has all the instructions.&lt;/p&gt; &lt;p&gt;Thanks for everything, I genuinely just want to see what this community builds and make sure the infrastructure can handle it.&lt;/p&gt; &lt;p&gt;Thanks for being patient with me, i'm just a guy learning and building cool stuff for you guys! :)&lt;/p&gt; &lt;p&gt;Roy&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/Roy3838/Observer"&gt;https://github.com/Roy3838/Observer&lt;/a&gt;&lt;/p&gt; &lt;p&gt;WebApp: &lt;a href="https://app.observer-ai.com/"&gt;https://app.observer-ai.com/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Roy3838"&gt; /u/Roy3838 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oufp8s/unlimited_cloud_this_week_on_observer_as_a_thank/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oufp8s/unlimited_cloud_this_week_on_observer_as_a_thank/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oufp8s/unlimited_cloud_this_week_on_observer_as_a_thank/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-11T17:31:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1ou8b89</id>
    <title>Why is MiniMax M2 a Full Attention model?</title>
    <updated>2025-11-11T12:36:28+00:00</updated>
    <author>
      <name>/u/InternationalAsk1490</name>
      <uri>https://old.reddit.com/user/InternationalAsk1490</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The CEO of MiniMax addresses frequent community questions about why MiniMax M2 sticks with Full Attention instead of adopting more efficient alternatives like Linear or Sparse Attention. After many repeated private explanations, they decided to publicly share the reasoning and lessons behind this decision.&lt;/p&gt; &lt;h1&gt;Theory vs. Reality: The Efficient Attention Dilemma&lt;/h1&gt; &lt;p&gt;While the benefits of Linear/Sparse Attention are widely discussed, real-world implementation in large-scale, industrial LLM systems is much more complex. Full Attention still holds practical advantages across various scenarios (code/math, agents, multimodal tasks, long chain-of-thought, RL, low-precision compute, speculative decoding, etc.). To justify switching to efficient attention, many technical and evaluation challenges need to be overcome.&lt;/p&gt; &lt;h1&gt;Motivation: Why Even Try Efficient Attention?&lt;/h1&gt; &lt;p&gt;If compute were unlimited, most wouldn‚Äôt bother with Linear/Sparse Attention. Today, all efforts to develop efficient attention are fundamentally about saving compute, not necessarily about reducing token counts or hitting scaling limits. The goal is to build a model structure that delivers the best performance under fixed compute budgets for both training and inference.&lt;/p&gt; &lt;h1&gt;Core Problems: Effectiveness, Speed, and Price&lt;/h1&gt; &lt;p&gt;To make efficient attention viable in production, three key factors must be balanced: effectiveness (the model‚Äôs floor), speed (throughput), and cost. The biggest hurdle is not the structure itself, but the limitations of current evaluation methodologies. Comprehensive benchmarks and real-world metrics are both necessary and difficult to build.&lt;/p&gt; &lt;h1&gt;1. Limitations of Evaluation&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Observability&lt;/strong&gt;: Benchmarks rapidly improve as models are optimized for them, but creating a truly comprehensive evaluation pipeline to expose real capability gaps remains unsolved‚Äîespecially for new attention mechanisms.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;No Free Lunch&lt;/strong&gt;: Reducing attention complexity isn‚Äôt without trade-offs. Earlier, hybrid models combining Lightning Attention and Full Attention seemed to perform well on standard benchmarks, but larger models exposed clear weaknesses in complex, multi-step reasoning tasks.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Proxy Metrics and Scaling&lt;/strong&gt;: Proxy metrics can match or beat MHA on benchmarks after several iterations, but may not generalize as models scale up. Many issues only emerge at scale.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;High Observation Cost&lt;/strong&gt;: Early proxy indicators for complex tasks are hard to measure during pretraining, and as task complexity grows, so does the compute needed to reach statistical confidence, slowing iteration.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Other Variables&lt;/strong&gt;: There are many confounding factors‚Äîmodel structure, data distribution, optimizer choice‚Äîall can sway outcomes, and conclusions may flip as the data pipeline evolves.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;2. Infrastructure Gaps for Efficient Attention&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Training&lt;/strong&gt;: Linear/Sparse Attention often becomes memory-bound rather than compute-bound. Without deep IO optimization, GPU utilization suffers.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Inference&lt;/strong&gt;: Delivering truly faster, cheaper inference is difficult. Theoretical memory/computation savings only kick in for long enough sequences (several thousand tokens), which is still short for modern LLMs. &lt;ul&gt; &lt;li&gt;Challenges include: &lt;ul&gt; &lt;li&gt;Low-precision state storage (more sensitive for linear attention)&lt;/li&gt; &lt;li&gt;Efficient prefix caching (critical for practical workloads)&lt;/li&gt; &lt;li&gt;Speculative decoding optimizations&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Fortunately, these are solvable, but require engineering effort.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Next Steps: What Needs to Happen&lt;/h1&gt; &lt;p&gt;Scaling remains a central theme. As context lengths increase faster than GPU compute, the payoff from efficient attention will become more pronounced. To prepare, the team needs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;More diverse and information-rich long-form data&lt;/li&gt; &lt;li&gt;Better evaluation systems and experimental paradigms for rapid iteration&lt;/li&gt; &lt;li&gt;Improved training/inference infrastructure to fully exploit available hardware&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Appendix: Lessons from Open-Source and Failed Experiments&lt;/h1&gt; &lt;p&gt;They briefly discusses the (now-removed) SWA inference code and why it didn‚Äôt make the cut‚Äîit simply didn‚Äôt work well enough. Hybrid approaches (mixing CPT and SWA, inter/intra-layer hybridization) were explored, but all exhibited significant performance drops with longer contexts, especially in agent scenarios. Analysis revealed entrenched attention patterns (like retrieval and induction heads) are established early and hard to adapt via hybridization, and probing to selectively retain full attention wasn‚Äôt practically successful. This issue isn‚Äôt related to ‚Äúattention sink.‚Äù Readers interested in this line of thinking are encouraged to analyze performance in models like GPT-OSS, CWM, and Gemma, especially for long-context tasks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InternationalAsk1490"&gt; /u/InternationalAsk1490 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ou8b89/why_is_minimax_m2_a_full_attention_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ou8b89/why_is_minimax_m2_a_full_attention_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ou8b89/why_is_minimax_m2_a_full_attention_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-11T12:36:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1oul7rv</id>
    <title>How to create local AI assistant/companion/whatever it is called with long term memory? Do you just ask for summarize previous talks or what?</title>
    <updated>2025-11-11T20:54:31+00:00</updated>
    <author>
      <name>/u/film_man_84</name>
      <uri>https://old.reddit.com/user/film_man_84</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, I am curious to know that if anybody here have crated LLM to work as a personal assistant/chatbot/companion or whatever the term is, and how you have done it.&lt;/p&gt; &lt;p&gt;Since the term I mean might be wrong I want to explain first what I mean. I mean simply the local LLM chat where I can talk all the things with the AI bot like &amp;quot;What's up, how's your day&amp;quot; so it would work as a friend or assistant or whatever. Then I can also ask &amp;quot;How could I write these lines better for my email&amp;quot; and so on and it would work for that.&lt;/p&gt; &lt;p&gt;Basically a chat LLM. That is not the issue for me, I can easily do this with LM Studio, KoboldCpp and whatever using just whatever model I want to.&lt;/p&gt; &lt;p&gt;The question what I am trying to get answer is, have you ever done this kind of companion what will stay there with days, weeks, months or longer with you and it have at least some kind of memory of previous chats?&lt;/p&gt; &lt;p&gt;If so - how? Context lenghts are limited, normal average user GPU have memory limits and so on and chats easily might get long and context will end. &lt;/p&gt; &lt;p&gt;One thing what came to my mind is that do people just start new chat every day/week or whatever and ask summary for that previous chat, then use that summary on the new chat and use it as a backstory/lore/whatever it is called, or how?&lt;/p&gt; &lt;p&gt;Or is this totally not realistic to make it work currently on consumer grade GPU's? I have 16 GB of VRAM (RTX 4060 Ti).&lt;/p&gt; &lt;p&gt;Have any of you made this and how? And yes, I have social life in case before somebody is wondering and giving tips to go out and meet people instead or whatever :D &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/film_man_84"&gt; /u/film_man_84 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oul7rv/how_to_create_local_ai_assistantcompanionwhatever/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oul7rv/how_to_create_local_ai_assistantcompanionwhatever/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oul7rv/how_to_create_local_ai_assistantcompanionwhatever/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-11T20:54:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1ouh46d</id>
    <title>What happened with Kimi Linear?</title>
    <updated>2025-11-11T18:23:30+00:00</updated>
    <author>
      <name>/u/TokenRingAI</name>
      <uri>https://old.reddit.com/user/TokenRingAI</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's been out for a bit, is it any good? It looks like Llama.cpp support is currently lacking&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TokenRingAI"&gt; /u/TokenRingAI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ouh46d/what_happened_with_kimi_linear/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ouh46d/what_happened_with_kimi_linear/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ouh46d/what_happened_with_kimi_linear/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-11T18:23:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1ouh5c1</id>
    <title>I built a tool that maps and visualizes backend codebases</title>
    <updated>2025-11-11T18:24:37+00:00</updated>
    <author>
      <name>/u/Weary-Commercial-922</name>
      <uri>https://old.reddit.com/user/Weary-Commercial-922</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ouh5c1/i_built_a_tool_that_maps_and_visualizes_backend/"&gt; &lt;img alt="I built a tool that maps and visualizes backend codebases" src="https://b.thumbs.redditmedia.com/C7Mq5RMriMimKlbkiuPoIWaRGKqTCSrZx9l_q4mDzbs.jpg" title="I built a tool that maps and visualizes backend codebases" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For some weeks, I‚Äôve been trying to solve the problem of how to make LLMs actually understand a codebase architecture. Most coding tools can generate good code, but they don‚Äôt usually get how systems fit together.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6n870x947o0g1.png?width=2556&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9a4625070306da3d852acaa8a48a6e4e428299a1"&gt;https://preview.redd.it/6n870x947o0g1.png?width=2556&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9a4625070306da3d852acaa8a48a6e4e428299a1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So I started working on a solution, a tool that parses backend codebases (FastAPI, Django, Node, etc.) into a semantic graph. It maps every endpoint, service, and method as nodes, and connects them through their relationships, requests, dependencies, or data flows. From there, it can visualize backend like a living system. Then I found out this might be useful for engineers instead of LLMs, as a way to rapidly understand a codebase.&lt;/p&gt; &lt;p&gt;The architecture side looks a bit like an interactive diagramming tool, but everything is generated automatically from real code. You can ask it things like &lt;em&gt;‚ÄúShow me everything that depends on the auth router‚Äù&lt;/em&gt; or &lt;em&gt;‚ÄúExplain how does the parsing works?‚Äù&lt;/em&gt; and it will generate a node map of the focalized query.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/pff5x7uc7o0g1.png?width=2512&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a689dd64f90616daa6ba138c80c920b70ca3f589"&gt;https://preview.redd.it/pff5x7uc7o0g1.png?width=2512&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a689dd64f90616daa6ba138c80c920b70ca3f589&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/mwk5rzce7o0g1.png?width=682&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0091b437372ef4e02e2b1772ef801ce21cddf7d7"&gt;https://preview.redd.it/mwk5rzce7o0g1.png?width=682&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0091b437372ef4e02e2b1772ef801ce21cddf7d7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I‚Äôm also working in a PR review engine that uses the graph to detect when a change might affect another service (e.g., modifying a shared database method). And because it understands system context, it can connect through MCP to AI tools like Claude or Cursor, in an effort to make them ‚Äúarchitecture-aware.‚Äù&lt;/p&gt; &lt;p&gt;I‚Äôm mostly curious to hear if others have tried solving similar problems, or if you believe this is a problem at all, especially around codebase understanding, feature planning, or context-aware AI tooling.&lt;/p&gt; &lt;p&gt;Built with FastAPI, Tree Sitter, Supabase, Pinecone, and a React/Next.js frontend.&lt;/p&gt; &lt;p&gt;Would love to get feedback or ideas on what you‚Äôd want a system like this to do.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Weary-Commercial-922"&gt; /u/Weary-Commercial-922 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ouh5c1/i_built_a_tool_that_maps_and_visualizes_backend/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ouh5c1/i_built_a_tool_that_maps_and_visualizes_backend/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ouh5c1/i_built_a_tool_that_maps_and_visualizes_backend/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-11T18:24:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1otxs37</id>
    <title>Our sub got a shout-out from the Corridor Crew</title>
    <updated>2025-11-11T02:33:11+00:00</updated>
    <author>
      <name>/u/onil_gova</name>
      <uri>https://old.reddit.com/user/onil_gova</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otxs37/our_sub_got_a_shoutout_from_the_corridor_crew/"&gt; &lt;img alt="Our sub got a shout-out from the Corridor Crew" src="https://external-preview.redd.it/MG5qbWE3OXZoajBnMfJFc8SM8imSZJpbD6BkmsMZ2u1jbLaP-XMJEPc_yiXX.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=269b48a90a4a2de8bb79dd262a86c54e29a97ed9" title="Our sub got a shout-out from the Corridor Crew" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From their recent video &lt;a href="https://youtu.be/6hI9T4jnrSI?si=h7An0736C93hs7YO"&gt;AI Experts Debunk The Latest SLOP&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/onil_gova"&gt; /u/onil_gova &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/10yfbe8vhj0g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otxs37/our_sub_got_a_shoutout_from_the_corridor_crew/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1otxs37/our_sub_got_a_shoutout_from_the_corridor_crew/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-11T02:33:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1ouk53u</id>
    <title>Local, multi-model AI that runs on a toaster. One-click setup, 2GB GPU enough</title>
    <updated>2025-11-11T20:13:43+00:00</updated>
    <author>
      <name>/u/VivianIto</name>
      <uri>https://old.reddit.com/user/VivianIto</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is a desktop program that runs multiple AI models in parallel on hardware most people would consider e-waste. Built from the ground up to be lightweight.&lt;/p&gt; &lt;p&gt;The device only uses a 2GB GPU. If there's a gaming laptop or a mid-tier PC from the last 5-7 years lying around, this will probably run on it.&lt;/p&gt; &lt;p&gt;What it does:&lt;/p&gt; &lt;p&gt;&amp;gt; Runs 100% offline. No internet needed after the first model download.&lt;/p&gt; &lt;p&gt;&amp;gt; One-click installer for Windows/Mac/Linux auto-detects the OS and handles setup. (The release is a pre-compiled binary. You only need Rust installed if you're building from source.)&lt;/p&gt; &lt;p&gt;&amp;gt; Three small, fast models (Gemma2:2b, TinyLlama, DistilBERT) collaborate on each response. They make up for their small size with teamwork.&lt;/p&gt; &lt;p&gt;&amp;gt; Includes a smart, persistent memory system. Remembers past chats without ballooning in size.&lt;/p&gt; &lt;p&gt;Real-time metrics show the models working together live.&lt;/p&gt; &lt;p&gt;No cloud, no API keys, no subscriptions. The installers are on the releases page. Lets you run three models at once locally.&lt;/p&gt; &lt;p&gt;Check it out here: &lt;a href="https://github.com/ryanj97g/Project_VI"&gt;https://github.com/ryanj97g/Project_VI&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VivianIto"&gt; /u/VivianIto &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ouk53u/local_multimodel_ai_that_runs_on_a_toaster/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ouk53u/local_multimodel_ai_that_runs_on_a_toaster/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ouk53u/local_multimodel_ai_that_runs_on_a_toaster/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-11T20:13:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1ou14ry</id>
    <title>baidu/ERNIE-4.5-VL-28B-A3B-Thinking released. Curious case..</title>
    <updated>2025-11-11T05:21:46+00:00</updated>
    <author>
      <name>/u/PaceZealousideal6091</name>
      <uri>https://old.reddit.com/user/PaceZealousideal6091</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ou14ry/baiduernie45vl28ba3bthinking_released_curious_case/"&gt; &lt;img alt="baidu/ERNIE-4.5-VL-28B-A3B-Thinking released. Curious case.." src="https://external-preview.redd.it/81GI5f2SH41ji6Aiuro1sKkxz-x19lfHg7ZgCRL6MOI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4217ff485db0b42df0be643ea3fe3e8636aa4480" title="baidu/ERNIE-4.5-VL-28B-A3B-Thinking released. Curious case.." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It seems Baidu has released the &amp;quot;thinking&amp;quot; variant if their vl model silently. The earlier model was supposedly hybrid, supporting both &amp;quot;thinking&amp;quot; and &amp;quot;non-thinking&amp;quot;. The model card says that they have introduced something called &amp;quot;thinking with images&amp;quot; without explaining what it is. They have one put a small hardly visible graph comparing it with gemini 2.5 pro and gpt-5 high in various benchmarks . If you squint your eye enough, then you'll see they claim using the graph that this model keeps up or beat them good in many of the benchmarks. Surely benchmaxxed. Its too good to believe. Has anyone tried it? The previous ernie versions have been decent. It might be worth testing it. Does anyone have any idea how is this &amp;quot;thinking&amp;quot; variant different?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PaceZealousideal6091"&gt; /u/PaceZealousideal6091 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/baidu/ERNIE-4.5-VL-28B-A3B-Thinking"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ou14ry/baiduernie45vl28ba3bthinking_released_curious_case/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ou14ry/baiduernie45vl28ba3bthinking_released_curious_case/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-11T05:21:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1otveug</id>
    <title>A startup Olares is attempting to launch a small 3.5L MiniPC dedicated to local AI, with RTX 5090 Mobile (24GB VRAM) and 96GB of DDR5 RAM for $3K</title>
    <updated>2025-11-11T00:44:49+00:00</updated>
    <author>
      <name>/u/FullOf_Bad_Ideas</name>
      <uri>https://old.reddit.com/user/FullOf_Bad_Ideas</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otveug/a_startup_olares_is_attempting_to_launch_a_small/"&gt; &lt;img alt="A startup Olares is attempting to launch a small 3.5L MiniPC dedicated to local AI, with RTX 5090 Mobile (24GB VRAM) and 96GB of DDR5 RAM for $3K" src="https://external-preview.redd.it/j6x6Pm9GXcBDejuI8fZ_JaGjEF5FKmyowYdHbKM_k34.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2f2207a39f85b0be48a03566c4c904bcc528405b" title="A startup Olares is attempting to launch a small 3.5L MiniPC dedicated to local AI, with RTX 5090 Mobile (24GB VRAM) and 96GB of DDR5 RAM for $3K" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullOf_Bad_Ideas"&gt; /u/FullOf_Bad_Ideas &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.techpowerup.com/342779/olares-to-launch-a-personal-ai-device-bringing-cloud-level-performance-home"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otveug/a_startup_olares_is_attempting_to_launch_a_small/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1otveug/a_startup_olares_is_attempting_to_launch_a_small/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-11T00:44:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ou8t7z</id>
    <title>Kimi K2 Thinking is a Better Agentic AI than I thought</title>
    <updated>2025-11-11T13:00:09+00:00</updated>
    <author>
      <name>/u/InternationalAsk1490</name>
      <uri>https://old.reddit.com/user/InternationalAsk1490</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1ou8t7z/video/9dtnlbhhlm0g1/player"&gt;https://reddit.com/link/1ou8t7z/video/9dtnlbhhlm0g1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;just ran a quick eval on a deep agent built for customer support. It‚Äòs on par with GPT-5 in agentic capabilities.&lt;br /&gt; It's a bigger deal than I thought!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InternationalAsk1490"&gt; /u/InternationalAsk1490 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ou8t7z/kimi_k2_thinking_is_a_better_agentic_ai_than_i/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ou8t7z/kimi_k2_thinking_is_a_better_agentic_ai_than_i/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ou8t7z/kimi_k2_thinking_is_a_better_agentic_ai_than_i/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-11T13:00:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1oueq51</id>
    <title>Agentic RAG: from Zero to Hero</title>
    <updated>2025-11-11T16:56:38+00:00</updated>
    <author>
      <name>/u/CapitalShake3085</name>
      <uri>https://old.reddit.com/user/CapitalShake3085</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;After spending several months building agents and experimenting with RAG systems, I decided to publish a GitHub repository to help those who are approaching agents and RAG for the first time.&lt;/p&gt; &lt;p&gt;I created an &lt;strong&gt;agentic RAG&lt;/strong&gt; with an educational purpose, aiming to provide a clear and practical reference. When I started, I struggled to find a single, structured place where all the key concepts were explained. I had to gather information from many different sources‚Äîand that‚Äôs exactly why I wanted to build something more accessible and beginner-friendly.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;üìö What you‚Äôll learn in this repository&lt;/h2&gt; &lt;p&gt;An end-to-end walkthrough of the essential building blocks:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;PDF ‚Üí Markdown conversion&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hierarchical chunking&lt;/strong&gt; (parent/child structure)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hybrid embeddings&lt;/strong&gt; (dense + sparse)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Vector storage&lt;/strong&gt; of chunks using &lt;em&gt;Qdrant&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Parallel multi-query handling&lt;/strong&gt; ‚Äî ability to generate and evaluate multiple queries simultaneously&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Query rewriting&lt;/strong&gt; ‚Äî automatically rephrases unclear or incomplete queries before retrieval&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Human-in-the-loop&lt;/strong&gt; to clarify ambiguous user queries&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Context management&lt;/strong&gt; across multiple messages using summarization&lt;/li&gt; &lt;li&gt;A &lt;strong&gt;fully working agentic RAG&lt;/strong&gt; using LangGraph that retrieves, evaluates, corrects, and generates answers&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Simple chatbot&lt;/strong&gt; using Gradio library &lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;p&gt;I hope this repository can be helpful to anyone starting their journey. &lt;/p&gt; &lt;p&gt;Thanks to everyone who takes a look and finds it useful! GitHub: &lt;a href="https://github.com/GiovanniPasq/agentic-rag-for-dummies"&gt;https://github.com/GiovanniPasq/agentic-rag-for-dummies&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CapitalShake3085"&gt; /u/CapitalShake3085 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oueq51/agentic_rag_from_zero_to_hero/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oueq51/agentic_rag_from_zero_to_hero/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oueq51/agentic_rag_from_zero_to_hero/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-11T16:56:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1oueiuj</id>
    <title>Half-trillion parameter model on a machine with 128 GB RAM + 24 GB VRAM</title>
    <updated>2025-11-11T16:49:09+00:00</updated>
    <author>
      <name>/u/pulse77</name>
      <uri>https://old.reddit.com/user/pulse77</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;just wanted to share that I‚Äôve successfully run &lt;strong&gt;Qwen3-Coder-480B&lt;/strong&gt; on &lt;strong&gt;llama.cpp&lt;/strong&gt; using the following setup:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;CPU:&lt;/strong&gt; Intel i9-13900KS&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RAM:&lt;/strong&gt; 128 GB (DDR5 4800 MT/s)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPU:&lt;/strong&gt; RTX 4090 (24 GB VRAM)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I‚Äôm using the &lt;strong&gt;4-bit and 3-bit Unsloth quantizations&lt;/strong&gt; from Hugging Face: &lt;a href="https://huggingface.co/unsloth/Qwen3-Coder-480B-A35B-Instruct-GGUF"&gt;https://huggingface.co/unsloth/Qwen3-Coder-480B-A35B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Performance results:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;UD-Q3_K_XL:&lt;/strong&gt; ~2.0 tokens/sec (generation)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;UD-Q4_K_XL:&lt;/strong&gt; ~1.0 token/sec (generation)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Command lines used (llama.cpp):&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;llama-server \&lt;/code&gt;&lt;br /&gt; &lt;code&gt;--threads 32 --jinja --flash-attn on \&lt;/code&gt;&lt;br /&gt; &lt;code&gt;--cache-type-k q8_0 --cache-type-v q8_0 \&lt;/code&gt;&lt;br /&gt; &lt;code&gt;--model &amp;lt;YOUR-MODEL-DIR&amp;gt;/Qwen3-Coder-480B-A35B-Instruct-UD-Q3_K_XL-00001-of-00005.gguf \&lt;/code&gt;&lt;br /&gt; &lt;code&gt;--ctx-size 131072 --n-cpu-moe 9999 --no-warmup&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;llama-server \&lt;/code&gt;&lt;br /&gt; &lt;code&gt;--threads 32 --jinja --flash-attn on \&lt;/code&gt;&lt;br /&gt; &lt;code&gt;--cache-type-k q8_0 --cache-type-v q8_0 \&lt;/code&gt;&lt;br /&gt; &lt;code&gt;--model &amp;lt;YOUR-MODEL-DIR&amp;gt;/Qwen3-Coder-480B-A35B-Instruct-UD-Q4_K_XL-00001-of-00006.gguf \&lt;/code&gt;&lt;br /&gt; &lt;code&gt;--ctx-size 131072 --n-cpu-moe 9999 --no-warmup&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Important:&lt;/strong&gt; The &lt;em&gt;--no-warmup&lt;/em&gt; flag is &lt;strong&gt;required&lt;/strong&gt; - without it, the process will terminate before you can start chatting.&lt;/p&gt; &lt;p&gt;In short: yes, it‚Äôs possible to run a &lt;strong&gt;half-trillion parameter model&lt;/strong&gt; on a machine with &lt;strong&gt;128 GB RAM + 24 GB VRAM&lt;/strong&gt;!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pulse77"&gt; /u/pulse77 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oueiuj/halftrillion_parameter_model_on_a_machine_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oueiuj/halftrillion_parameter_model_on_a_machine_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oueiuj/halftrillion_parameter_model_on_a_machine_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-11T16:49:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1ou1j3e</id>
    <title>Seems like the new K2 benchmarks are not too representative of real-world performance</title>
    <updated>2025-11-11T05:45:03+00:00</updated>
    <author>
      <name>/u/cobalt1137</name>
      <uri>https://old.reddit.com/user/cobalt1137</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ou1j3e/seems_like_the_new_k2_benchmarks_are_not_too/"&gt; &lt;img alt="Seems like the new K2 benchmarks are not too representative of real-world performance" src="https://preview.redd.it/awzjyvo3gk0g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c7f2dd0a4c362653c960b794e0943c0b3784b17b" title="Seems like the new K2 benchmarks are not too representative of real-world performance" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cobalt1137"&gt; /u/cobalt1137 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/awzjyvo3gk0g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ou1j3e/seems_like_the_new_k2_benchmarks_are_not_too/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ou1j3e/seems_like_the_new_k2_benchmarks_are_not_too/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-11T05:45:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1ou1emx</id>
    <title>We put a lot of work into a 1.5B reasoning model ‚Äî now it beats bigger ones on math &amp; coding benchmarks</title>
    <updated>2025-11-11T05:37:41+00:00</updated>
    <author>
      <name>/u/innocent2powerful</name>
      <uri>https://old.reddit.com/user/innocent2powerful</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ou1emx/we_put_a_lot_of_work_into_a_15b_reasoning_model/"&gt; &lt;img alt="We put a lot of work into a 1.5B reasoning model ‚Äî now it beats bigger ones on math &amp;amp; coding benchmarks" src="https://preview.redd.it/fnpk5t7kbk0g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3c57d729e6fb3ff57f9b7d7ba1d9d6be31f27588" title="We put a lot of work into a 1.5B reasoning model ‚Äî now it beats bigger ones on math &amp;amp; coding benchmarks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ol&gt; &lt;li&gt;We put a lot of care into making sure the &lt;strong&gt;training data is fully decontaminated&lt;/strong&gt; ‚Äî every stage (SFT and RL) went through strict filtering to avoid any overlap with evaluation benchmarks.&lt;/li&gt; &lt;li&gt;It achieves state-of-the-art performance among small (&amp;lt;4B) models, both in competitive math and competitive coding tasks. Even &lt;strong&gt;surpass the DeepSeek R1 0120 in competitive math benchmarks&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;It‚Äôs not designed as a general chatbot&lt;/strong&gt; (though it can handle basic conversation and factual QA). Our main goal was to &lt;strong&gt;prove that small models can achieve strong reasoning&lt;/strong&gt; ability, and we‚Äôve put a lot of work and iteration into achieving that, starting from a base like Qwen2.5-Math-1.5B (which originally had weak math and almost no coding ability) to reach this point.&lt;/li&gt; &lt;li&gt;We‚Äôd love for the community to &lt;strong&gt;test it on your own competitive math/coding benchmarks&lt;/strong&gt; and share results or feedback here. Any insights will help us keep improving.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;HuggingFace Paper: &lt;a href="https://huggingface.co/papers/2511.06221"&gt;paper&lt;/a&gt;&lt;br /&gt; X Post: &lt;a href="https://x.com/WeiboLLM/status/1988109435902832896?s=20"&gt;X&lt;/a&gt;&lt;br /&gt; Model: &lt;a href="https://huggingface.co/WeiboAI/VibeThinker-1.5B"&gt;Download Model&lt;/a&gt; Ôºàset resp_len=40k, temp=0.6 / 1.0, top_p=0.95, top_k=-1 for better performance.Ôºâ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/innocent2powerful"&gt; /u/innocent2powerful &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fnpk5t7kbk0g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ou1emx/we_put_a_lot_of_work_into_a_15b_reasoning_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ou1emx/we_put_a_lot_of_work_into_a_15b_reasoning_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-11T05:37:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1oubrbc</id>
    <title>Meta chief AI scientist Yann LeCun plans to exit to launch startup, FT reports</title>
    <updated>2025-11-11T15:05:23+00:00</updated>
    <author>
      <name>/u/brown2green</name>
      <uri>https://old.reddit.com/user/brown2green</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/brown2green"&gt; /u/brown2green &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reuters.com/technology/meta-chief-ai-scientist-yann-lecun-plans-exit-launch-startup-ft-reports-2025-11-11/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oubrbc/meta_chief_ai_scientist_yann_lecun_plans_to_exit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oubrbc/meta_chief_ai_scientist_yann_lecun_plans_to_exit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-11T15:05:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ougamx</id>
    <title>gpt-oss-120b on Cerebras</title>
    <updated>2025-11-11T17:53:30+00:00</updated>
    <author>
      <name>/u/Corporate_Drone31</name>
      <uri>https://old.reddit.com/user/Corporate_Drone31</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ougamx/gptoss120b_on_cerebras/"&gt; &lt;img alt="gpt-oss-120b on Cerebras" src="https://preview.redd.it/qkygjyoz1o0g1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d0028679c0cac64d9ce3f55e2a3aad86019108bc" title="gpt-oss-120b on Cerebras" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;gpt-oss-120b reasoning CoT on Cerebras be like &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Corporate_Drone31"&gt; /u/Corporate_Drone31 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/qkygjyoz1o0g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ougamx/gptoss120b_on_cerebras/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ougamx/gptoss120b_on_cerebras/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-11T17:53:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1ouazho</id>
    <title>Egocentric-10K is the largest egocentric dataset. It is the first dataset collected exclusively in real factories (Build AI - 10,000 hours - 2,153 factory workers - 1,080,000,000 frame)</title>
    <updated>2025-11-11T14:34:43+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ouazho/egocentric10k_is_the_largest_egocentric_dataset/"&gt; &lt;img alt="Egocentric-10K is the largest egocentric dataset. It is the first dataset collected exclusively in real factories (Build AI - 10,000 hours - 2,153 factory workers - 1,080,000,000 frame)" src="https://external-preview.redd.it/Z2ZjbDkzdmowbjBnMUB18FDMNIKrOWZMaI6GCxWf_t_2BvSabc90NvjIF-MD.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=57cf924fd9176e70ff73462191aaf19ef1b60b55" title="Egocentric-10K is the largest egocentric dataset. It is the first dataset collected exclusively in real factories (Build AI - 10,000 hours - 2,153 factory workers - 1,080,000,000 frame)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hugging Face, (apache 2.0): &lt;a href="https://huggingface.co/datasets/builddotai/Egocentric-10K"&gt;https://huggingface.co/datasets/builddotai/Egocentric-10K&lt;/a&gt;&lt;br /&gt; Eddy Xu on ùïè: &lt;a href="https://x.com/eddybuild/status/1987951619804414416"&gt;https://x.com/eddybuild/status/1987951619804414416&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/nlsslzuj0n0g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ouazho/egocentric10k_is_the_largest_egocentric_dataset/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ouazho/egocentric10k_is_the_largest_egocentric_dataset/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-11T14:34:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1oth5pw</id>
    <title>AMA With Moonshot AI, The Open-source Frontier Lab Behind Kimi K2 Thinking Model</title>
    <updated>2025-11-10T15:44:10+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/"&gt; &lt;img alt="AMA With Moonshot AI, The Open-source Frontier Lab Behind Kimi K2 Thinking Model" src="https://b.thumbs.redditmedia.com/yz9_FpdLcHNiCkaH5fLEIoXS2f5u5twNBr7SQ9Go3AI.jpg" title="AMA With Moonshot AI, The Open-source Frontier Lab Behind Kimi K2 Thinking Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Today we are having &lt;strong&gt;Moonshot AI&lt;/strong&gt;, the research lab behind the &lt;strong&gt;Kimi&lt;/strong&gt; &lt;strong&gt;models&lt;/strong&gt;. We‚Äôre excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="/u/ComfortableAsk4494"&gt;u/ComfortableAsk4494&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="/u/zxytim"&gt;u/zxytim&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="/u/ppwwyyxx"&gt;u/ppwwyyxx&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 8 AM ‚Äì 11 AM PST, with the Kimi team continuing to follow up on questions over the next 24 hours.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5yg0ncsn7g0g1.png?width=3525&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5318680204ef7502ad349aec148147d9e3398f87"&gt;https://preview.redd.it/5yg0ncsn7g0g1.png?width=3525&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5318680204ef7502ad349aec148147d9e3398f87&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Thanks everyone for joining our AMA. The live part has ended and the Kimi team will be following up with more answers sporadically over the next 24 hours.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T15:44:10+00:00</published>
  </entry>
</feed>
