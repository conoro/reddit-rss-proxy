<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-04T16:57:31+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1pe4vpc</id>
    <title>Fine tune for rp world?</title>
    <updated>2025-12-04T16:45:16+00:00</updated>
    <author>
      <name>/u/JaxxonAI</name>
      <uri>https://old.reddit.com/user/JaxxonAI</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, I have sillytavern setup locally and run a local llm for rp. I have been toying with the idea of making a specific finetune for an existing world. As an example, would it make sense to take all the text from the Wheel of Time novels and use that to create a finetune? &lt;/p&gt; &lt;p&gt;I've create Loras off of image models but never thought to try an language model until recently. Any thoughts on this? Pros and cons would we great!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JaxxonAI"&gt; /u/JaxxonAI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pe4vpc/fine_tune_for_rp_world/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pe4vpc/fine_tune_for_rp_world/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pe4vpc/fine_tune_for_rp_world/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T16:45:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdswk2</id>
    <title>Spec-Kit with Ministral 3 14b</title>
    <updated>2025-12-04T06:30:23+00:00</updated>
    <author>
      <name>/u/International_Quail8</name>
      <uri>https://old.reddit.com/user/International_Quail8</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Had been fighting with a few models to get spec-kit working locally. gpt-oss-20b, qwen3-coder-30b and qwen3-next all failed me! Used lmstudio for local inference and qwen code as the codegen cli. &lt;/p&gt; &lt;p&gt;I gave the new ministral 3 14b reasoning model a shot and was very impressed that it was able to follow the spec-kit process, work with the templates and generate my feature as spec’d! It also did it with reasonably good speed. Not perfect, but got through an entire complex feature from start to finish where the other models failed. Mistral did it again! Was a huge fan of Mixtral 8x7B from “back in the day”&lt;/p&gt; &lt;p&gt;Nice one Mistral AI!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/International_Quail8"&gt; /u/International_Quail8 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdswk2/speckit_with_ministral_3_14b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdswk2/speckit_with_ministral_3_14b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdswk2/speckit_with_ministral_3_14b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T06:30:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdin3b</id>
    <title>The Best Open Weights Coding Models of 2025</title>
    <updated>2025-12-03T22:29:44+00:00</updated>
    <author>
      <name>/u/mr_riptano</name>
      <uri>https://old.reddit.com/user/mr_riptano</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdin3b/the_best_open_weights_coding_models_of_2025/"&gt; &lt;img alt="The Best Open Weights Coding Models of 2025" src="https://external-preview.redd.it/wxWktbYwfvy3j0Y1BhifZGPwnHUnY7JGZEyykx4N_HM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=48a8af816e33acba1ab83ce8086f21346740c37d" title="The Best Open Weights Coding Models of 2025" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, I'm back with uncontaminated evals for DeepSeek-V3.2, Kimi K2 Thinking, and MiniMax M2. (We caught GLM 4.6 last time around.) &lt;/p&gt; &lt;p&gt;If you just want the numbers, you can find them for the finalists &lt;a href="https://brokk.ai/power-ranking?models=dsv3.2%2Cglm4.6-fp8%2Ck2-thinking%2Cm2"&gt;here&lt;/a&gt; and for everyone else &lt;a href="https://brokk.ai/power-ranking?dataset=openround"&gt;here&lt;/a&gt;. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mr_riptano"&gt; /u/mr_riptano &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://blog.brokk.ai/the-best-open-weights-coding-models-of-2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdin3b/the_best_open_weights_coding_models_of_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdin3b/the_best_open_weights_coding_models_of_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T22:29:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdtj6c</id>
    <title>entered a memory competition with my local llama setup, results were weird</title>
    <updated>2025-12-04T07:07:11+00:00</updated>
    <author>
      <name>/u/FeelingWatercress871</name>
      <uri>https://old.reddit.com/user/FeelingWatercress871</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Saw this long term memory competition thing on twitter a few weeks back and decided to enter with my local setup. Llama 3.1 8B Instruct + some memory hacks i've been working on.&lt;/p&gt; &lt;p&gt;Competition had 3 main tasks:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Long-term dialogue (50+ turns, reference stuff from turn 5 at turn 45)&lt;/li&gt; &lt;li&gt;Multi-person conversation tracking (track who said what when) &lt;/li&gt; &lt;li&gt;Causal reasoning (if X happened because of Y, remember the connection)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;My approach was pretty basic. Used transformers library, monkey patched the generate() function to not reset past_key_values between conversation turns. Added some janky importance scoring - basically tracked which tokens got high attention scores and tried to keep those when hitting memory limits. Nothing fancy, just hacked together over a weekend.&lt;/p&gt; &lt;p&gt;Results were all over the place:&lt;/p&gt; &lt;p&gt;Task 1 (long conversations): 72.3% - not bad Task 2 (multi person): 43.8% - terrible Task 3 (causal reasoning): 81.7% - surprisingly good&lt;/p&gt; &lt;p&gt;The weird part is task 3. My system somehow got causal connections way better than conversation tracking. No clue why that worked.&lt;/p&gt; &lt;p&gt;Looking at other entries, most people did RAG stuff. Vector DBs, embeddings, retrieval, you know. Standard approach. My KV cache thing was kinda different.&lt;/p&gt; &lt;p&gt;Top scorer got 92.3% overall using some open source memory system. Way better than my 65.9% average but their approach was completely different from mine. From the leaderboard description, they used hybrid retrieval with multiple databases instead of just KV cache hacks. Found the repo later: github.com/EverMind-AI/EverMemOS. Seemed like a proper memory framework with MongoDB, Elasticsearch, and vector databases vs my simple KV cache approach.&lt;/p&gt; &lt;p&gt;Couple things i figured out:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;KV cache stuff works but eats memory like crazy (hit 22.8GB on my 3090 for the 50+ turn conversations, had to restart multiple times)&lt;/li&gt; &lt;li&gt;importance scoring is key, otherwise you run out of space fast&lt;/li&gt; &lt;li&gt;multi person chats are a nightmare, way harder than i expected. spent most time debugging this&lt;/li&gt; &lt;li&gt;causal reasoning was surprisingly ok, not sure why. maybe got lucky?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Might look into other approaches. My hack was fun but obviously not great lol. The winning approach looked more serious but setup seemed complicated from what i could see. Maybe worth checking out if i have time.&lt;/p&gt; &lt;p&gt;Competition was actually useful tho. Made me test things properly instead of just &amp;quot;eh seems to work&amp;quot;. Realized my approach had way more issues than i thought.&lt;/p&gt; &lt;p&gt;Anyone else tried these memory challenge things? Curious what approaches worked for you. Mine was obviously not great but learned a lot about the limitations of simple KV cache approaches.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FeelingWatercress871"&gt; /u/FeelingWatercress871 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdtj6c/entered_a_memory_competition_with_my_local_llama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdtj6c/entered_a_memory_competition_with_my_local_llama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdtj6c/entered_a_memory_competition_with_my_local_llama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T07:07:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdk4zx</id>
    <title>[Resource] 20,000+ Pages of U.S. House Oversight Epstein Estate Docs (OCR'd &amp; Cleaned for RAG/Analysis)</title>
    <updated>2025-12-03T23:30:44+00:00</updated>
    <author>
      <name>/u/Ok-District-1330</name>
      <uri>https://old.reddit.com/user/Ok-District-1330</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve reuploaded the recent release of 20,000+ pages of documents regarding the Epstein Estate from the U.S. House Oversight Committee. The goal is to make these scattered government files accessible for journalists and researchers using open-source tools.&lt;/p&gt; &lt;p&gt;Note, this was originally shared here, by another user who's account has now been deleted, then uploaded to huggingface. The original huggingface repo has since been removed, as well as the original uploaders account. Credit for the original dataset goes to him/her. This is simply a clone, hosted on Github and my huggingface account, with a gradio app I built for interacting/searching it.&lt;/p&gt; &lt;p&gt;The original release contained mixed file formats and nested folders. This dataset converts images/PDFs to text (via Tesseract OCR) and standardizes them into a single CSV format.&lt;/p&gt; &lt;p&gt;Searchable App: A Gradio browser to search the corpus without downloading the full set.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/spaces/theelderemo/epstein-files"&gt;Hugging Face Gradio App and Repo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/theelderemo/Epstein-files"&gt;Github Mirror&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-District-1330"&gt; /u/Ok-District-1330 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdk4zx/resource_20000_pages_of_us_house_oversight/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdk4zx/resource_20000_pages_of_us_house_oversight/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdk4zx/resource_20000_pages_of_us_house_oversight/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T23:30:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdxk97</id>
    <title>I'm hex editing an old videogame, how do I feed a (locally run) AI the game's code?</title>
    <updated>2025-12-04T11:23:36+00:00</updated>
    <author>
      <name>/u/Xaxaxa-9</name>
      <uri>https://old.reddit.com/user/Xaxaxa-9</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to mod an old videogame and I want an AI to read through a 2.7 MB .dpl file and explain it to me so I know what to change.&lt;/p&gt; &lt;p&gt;I had some success feeding snippets to Grok, which were copy-pasted from a disassembler (a lot of old Borland Delphi labels are still present which helps), and I've just now copy-pasted the entire thing into a text file.... which is 1.35 million lines long. 62 million characters, 6 million words. Apparently that's too big for RAG usage?&lt;/p&gt; &lt;p&gt;So, do I need to manually copy-paste relevant chunks into multiple text files in a folder - what I'm curious about is, how long can those files be? Gemini told me about 500-800 lines of assembly code each, but is that true?&lt;/p&gt; &lt;p&gt;These are my hardware specs.&lt;/p&gt; &lt;p&gt;Installed Physical Memory (RAM) 64.0 GB&lt;/p&gt; &lt;p&gt;Total Physical Memory 61.6 GB&lt;/p&gt; &lt;p&gt;Available Physical Memory 31.1 GB&lt;/p&gt; &lt;p&gt;Total Virtual Memory 85.6 GB&lt;/p&gt; &lt;p&gt;Available Virtual Memory 44.3 GB&lt;/p&gt; &lt;p&gt;Page File Space 24.0 GB&lt;/p&gt; &lt;p&gt;Edit: also GPU is a 48 GB VRAM 4090 Dual&lt;/p&gt; &lt;p&gt;I'm really new to running local AI, but I'm downloading some 20 GB - 40 GB Qwen models right now via LM Studio.&lt;/p&gt; &lt;p&gt;Grok just told me to automatically chop the text file from the .dpl using Ghidra or IDA, how many files can I put in the RAG folder?&lt;/p&gt; &lt;p&gt;P.S. I'm also new to programming, my only programming knowledge is literally just some ASM picked up for videogame modding.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xaxaxa-9"&gt; /u/Xaxaxa-9 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdxk97/im_hex_editing_an_old_videogame_how_do_i_feed_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdxk97/im_hex_editing_an_old_videogame_how_do_i_feed_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdxk97/im_hex_editing_an_old_videogame_how_do_i_feed_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T11:23:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1pe4xm4</id>
    <title>VLLM v0.12.0 supports NVFP4 for SM120 (RTX 50xx and RTX PRO 6000 Blackwell)</title>
    <updated>2025-12-04T16:47:16+00:00</updated>
    <author>
      <name>/u/Rascazzione</name>
      <uri>https://old.reddit.com/user/Rascazzione</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My kudos for the VLLM team that has release the v0.12.0 with support for NVFP4 for the SM120 family!&lt;/p&gt; &lt;h1&gt;Quantization&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;W4A8&lt;/strong&gt;: Marlin kernel support (&lt;a href="https://github.com/vllm-project/vllm/pull/24722"&gt;#24722&lt;/a&gt;).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;NVFP4&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;MoE CUTLASS support for SM120 (&lt;a href="https://github.com/vllm-project/vllm/pull/29242"&gt;#29242&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;TRTLLM MoE NVFP4 kernel (&lt;a href="https://github.com/vllm-project/vllm/pull/28892"&gt;#28892&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;CuteDSL MoE with NVFP4 DeepEP dispatch (&lt;a href="https://github.com/vllm-project/vllm/pull/27141"&gt;#27141&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;Non-gated activations support in modelopt path (&lt;a href="https://github.com/vllm-project/vllm/pull/29004"&gt;#29004&lt;/a&gt;)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;AWQ&lt;/strong&gt;: Compressed-tensors AWQ support for Turing GPUs (&lt;a href="https://github.com/vllm-project/vllm/pull/29732"&gt;#29732&lt;/a&gt;).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LoRA&lt;/strong&gt;: FusedMoE LoRA Triton kernel for MXFP4 (&lt;a href="https://github.com/vllm-project/vllm/pull/29708"&gt;#29708&lt;/a&gt;).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Online quantization&lt;/strong&gt;: Moved to &lt;code&gt;model.load_weights&lt;/code&gt; (&lt;a href="https://github.com/vllm-project/vllm/pull/26327"&gt;#26327&lt;/a&gt;).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://github.com/vllm-project/vllm/releases"&gt;https://github.com/vllm-project/vllm/releases&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Finally, we'll be able to make some models fly!&lt;/p&gt; &lt;p&gt;In an ubuntu 24.04 with driver 580 and nvidia rtx 6000 pro, just &lt;code&gt;uv pip install vllm --upgrade&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Then &lt;code&gt;vllm bench serve --model &amp;quot;openai/gpt-oss-120b&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;============ Serving Benchmark Result ============ Successful requests: 1000 Failed requests: 0 Benchmark duration (s): 90.34 Total input tokens: 1024000 Total generated tokens: 128000 Request throughput (req/s): 11.07 Output token throughput (tok/s): 1416.83 Peak output token throughput (tok/s): 4598.00 Peak concurrent requests: 1000.00 Total Token throughput (tok/s): 12751.46 ---------------Time to First Token---------------- Mean TTFT (ms): 40130.28 Median TTFT (ms): 38045.08 P99 TTFT (ms): 83577.52 -----Time per Output Token (excl. 1st token)------ Mean TPOT (ms): 167.93 Median TPOT (ms): 182.06 P99 TPOT (ms): 222.48 ---------------Inter-token Latency---------------- Mean ITL (ms): 167.95 Median ITL (ms): 67.44 P99 ITL (ms): 498.29 ================================================== &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rascazzione"&gt; /u/Rascazzione &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pe4xm4/vllm_v0120_supports_nvfp4_for_sm120_rtx_50xx_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pe4xm4/vllm_v0120_supports_nvfp4_for_sm120_rtx_50xx_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pe4xm4/vllm_v0120_supports_nvfp4_for_sm120_rtx_50xx_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T16:47:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdlfu3</id>
    <title>Frozen networks show usable early-layer intent: 1370× fewer FLOPs and 10× faster inference (code + weights)9</title>
    <updated>2025-12-04T00:26:15+00:00</updated>
    <author>
      <name>/u/anima-core</name>
      <uri>https://old.reddit.com/user/anima-core</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been experimenting with whether a frozen network’s early activations contain enough “semantic intent” to skip most of the compute.&lt;/p&gt; &lt;p&gt;I used a standard ResNet-18 trained on CIFAR-10 (87.89 percent accuracy), pulled a single 64-dimensional vector from an early layer, and trained a tiny decoder on top of it.&lt;/p&gt; &lt;p&gt;Results on the same hardware: • 72.57 percent accuracy from that early-layer vector • ~10× faster real latency • 1370× fewer FLOPs • No pruning, distillation, quantization, early exit tricks, or sparsity • The full model stayed completely frozen&lt;/p&gt; &lt;p&gt;This means 99.93 percent of the original network’s compute was not required to recover 82.6 percent of its performance.&lt;/p&gt; &lt;p&gt;Code + one-click run script: &lt;a href="https://github.com/Anima-Core/an1-meaning-engine"&gt;https://github.com/Anima-Core/an1-meaning-engine&lt;/a&gt;&lt;/p&gt; &lt;p&gt;HF demo + pretrained weights: &lt;a href="https://huggingface.co/Anima-Core/an1-meaning-engine"&gt;https://huggingface.co/Anima-Core/an1-meaning-engine&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Runs end to end on almost any GPU or CPU in a few minutes.&lt;/p&gt; &lt;p&gt;Dedicated to my late father, Asad Shamim, whose loss opened the path that led me here.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/anima-core"&gt; /u/anima-core &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdlfu3/frozen_networks_show_usable_earlylayer_intent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdlfu3/frozen_networks_show_usable_earlylayer_intent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdlfu3/frozen_networks_show_usable_earlylayer_intent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T00:26:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1pe3sme</id>
    <title>Had anyone tried to make llama.cpp vulkan work on mali gpus?</title>
    <updated>2025-12-04T16:04:06+00:00</updated>
    <author>
      <name>/u/bulieme0</name>
      <uri>https://old.reddit.com/user/bulieme0</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ive tried installing &lt;code&gt;llama-cpp-backend-vulkan&lt;/code&gt; on termux, and tried installing other prequisites (i.e. the vulkan header)&lt;/p&gt; &lt;p&gt;but it give me errors or dont detect the gpu entirely.&lt;/p&gt; &lt;p&gt;here are my terminal logs (yes i restarted termux for this, apologize for lacking of details here)&lt;/p&gt; &lt;p&gt;&lt;code&gt; ~ $ llama-cli --version ggml_vulkan: No devices found. load_backend: loaded Vulkan backend from /data/data/com.termux/files/usr/bin/../lib/libggml-vulkan.so load_backend: loaded CPU backend from /data/data/com.termux/files/usr/bin/../lib/libggml-cpu.so version: 0 (unknown) built with Android (13989888, +pgo, +bolt, +lto, +mlgo, based on r563880c) clang version 21.0.0 (https://android.googlesource.com/toolchain/llvm-project 5e96669f06077099aa41290cdb4c5e6fa0f59349) for x86_64-unknown-linux-gnu ~ $ cp /system/lib64/libvulkan.so $PREFIX/lib/libvulkan.so ~ $ cat &amp;gt; $HOME/mali.json &amp;lt;&amp;lt; 'EOF' { &amp;quot;file_format_version&amp;quot;: &amp;quot;1.0.0&amp;quot;, &amp;quot;ICD&amp;quot;: { &amp;quot;library_path&amp;quot;: &amp;quot;/vendor/lib64/hw/vulkan.mali.so&amp;quot;, &amp;quot;api_version&amp;quot;: &amp;quot;1.1.177&amp;quot; } } EOF ~ $ llama-cli --version ggml_vulkan: WARNING: Instance extension VK_EXT_debug_utils not found. ggml_vulkan: Found 1 Vulkan devices: ggml_vulkan: 0 = Mali-G57 MC2 (Mali-G57 MC2) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 0 | matrix cores: none load_backend: loaded Vulkan backend from /data/data/com.termux/files/usr/bin/../lib/libggml-vulkan.so load_backend: loaded CPU backend from /data/data/com.termux/files/usr/bin/../lib/libggml-cpu.so version: 0 (unknown) built with Android (13989888, +pgo, +bolt, +lto, +mlgo, based on r563880c) clang version 21.0.0 (https://android.googlesource.com/toolchain/llvm-project 5e96669f06077099aa41290cdb4c5e6fa0f59349) for x86_64-unknown-linux-gnu ~ $ export VK_ICD_FILENAMES=$HOME/mali.json &amp;amp;&amp;amp; export LD_LIBRARY_PATH=/vendor/lib64/hw:$PREFIX/lib:$LD_LIBRARY_PATH ~ $ llama-cli --version load_backend: loaded CPU backend from /data/data/com.termux/files/usr/bin/../lib/libggml-cpu.so version: 0 (unknown) built with Android (13989888, +pgo, +bolt, +lto, +mlgo, based on r563880c) clang version 21.0.0 (https://android.googlesource.com/toolchain/llvm-project 5e96669f06077099aa41290cdb4c5e6fa0f59349) for x86_64-unknown-linux-gnu ~ $ unset LD_LIBRARY_PATH ~ $ unset LD_LIBRARY_PATH ~ $ llama-cli --version ggml_vulkan: WARNING: Instance extension VK_EXT_debug_utils not found. ggml_vulkan: Found 1 Vulkan devices: ggml_vulkan: 0 = Mali-G57 MC2 (Mali-G57 MC2) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 0 | matrix cores: none load_backend: loaded Vulkan backend from /data/data/com.termux/files/usr/bin/../lib/libggml-vulkan.so load_backend: loaded CPU backend from /data/data/com.termux/files/usr/bin/../lib/libggml-cpu.so version: 0 (unknown) built with Android (13989888, +pgo, +bolt, +lto, +mlgo, based on r563880c) clang version 21.0.0 (https://android.googlesource.com/toolchain/llvm-project 5e96669f06077099aa41290cdb4c5e6fa0f59349) for x86_64-unknown-linux-gnu ~ $ ls mali.json ~ $ llama-cli -m /sdcard/Huihui-Qwen3-0.6B-abliterated-v2.i1-Q4_K_M.gguf -ngl 99 ggml_vulkan: WARNING: Instance extension VK_EXT_debug_utils not found. ggml_vulkan: Found 1 Vulkan devices: ggml_vulkan: 0 = Mali-G57 MC2 (Mali-G57 MC2) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 0 | matrix cores: none load_backend: loaded Vulkan backend from /data/data/com.termux/files/usr/bin/../lib/libggml-vulkan.so load_backend: loaded CPU backend from /data/data/com.termux/files/usr/bin/../lib/libggml-cpu.so build: 0 (unknown) with Android (13989888, +pgo, +bolt, +lto, +mlgo, based on r563880c) clang version 21.0.0 (https://android.googlesource.com/toolchain/llvm-project 5e96669f06077099aa41290cdb4c5e6fa0f59349) for x86_64-unknown-linux-gnu main: llama backend init main: load the model and apply lora adapter, if any llama_model_load_from_file_impl: using device Vulkan0 (Mali-G57 MC2) (unknown id) - 7627 MiB free llama_model_loader: loaded meta data with 46 key-value pairs and 310 tensors from /sdcard/Huihui-Qwen3-0.6B-abliterated-v2.i1-Q4_K_M.gguf (version GGUF V3 (latest)) llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output. llama_model_loader: - kv 0: general.architecture str = qwen3 llama_model_loader: - kv 1: general.type str = model llama_model_loader: - kv 2: general.name str = Huihui Qwen3 0.6B Abliterated v2 llama_model_loader: - kv 3: general.version str = v2 llama_model_loader: - kv 4: general.finetune str = abliterated llama_model_loader: - kv 5: general.basename str = Huihui-Qwen3 llama_model_loader: - kv 6: general.size_label str = 0.6B llama_model_loader: - kv 7: general.license str = apache-2.0 llama_model_loader: - kv 8: general.license.link str = https://huggingface.co/Qwen/Qwen3-0.6... llama_model_loader: - kv 9: general.base_model.count u32 = 1 llama_model_loader: - kv 10: general.base_model.0.name str = Qwen3 0.6B llama_model_loader: - kv 11: general.base_model.0.organization str = Qwen llama_model_loader: - kv 12: general.base_model.0.repo_url str = https://huggingface.co/Qwen/Qwen3-0.6B llama_model_loader: - kv 13: general.tags arr[str,4] = [&amp;quot;chat&amp;quot;, &amp;quot;abliterated&amp;quot;, &amp;quot;uncensored&amp;quot;,... llama_model_loader: - kv 14: qwen3.block_count u32 = 28 llama_model_loader: - kv 15: qwen3.context_length u32 = 40960 llama_model_loader: - kv 16: qwen3.embedding_length u32 = 1024 llama_model_loader: - kv 17: qwen3.feed_forward_length u32 = 3072 llama_model_loader: - kv 18: qwen3.attention.head_count u32 = 16 llama_model_loader: - kv 19: qwen3.attention.head_count_kv u32 = 8 llama_model_loader: - kv 20: qwen3.rope.freq_base f32 = 1000000.000000 llama_model_loader: - kv 21: qwen3.attention.layer_norm_rms_epsilon f32 = 0.000001 llama_model_loader: - kv 22: qwen3.attention.key_length u32 = 128 llama_model_loader: - kv 23: qwen3.attention.value_length u32 = 128 llama_model_loader: - kv 24: tokenizer.ggml.model str = gpt2 llama_model_loader: - kv 25: tokenizer.ggml.pre str = qwen2 llama_model_loader: - kv 26: tokenizer.ggml.tokens arr[str,151936] = [&amp;quot;!&amp;quot;, &amp;quot;\&amp;quot;&amp;quot;, &amp;quot;#&amp;quot;, &amp;quot;$&amp;quot;, &amp;quot;%&amp;quot;, &amp;quot;&amp;amp;&amp;quot;, &amp;quot;'&amp;quot;, ... llama_model_loader: - kv 27: tokenizer.ggml.token_type arr[i32,151936] = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ... llama_model_loader: - kv 28: tokenizer.ggml.merges arr[str,151387] = [&amp;quot;Ġ Ġ&amp;quot;, &amp;quot;ĠĠ ĠĠ&amp;quot;, &amp;quot;i n&amp;quot;, &amp;quot;Ġ t&amp;quot;,... llama_model_loader: - kv 29: tokenizer.ggml.eos_token_id u32 = 151645 llama_model_loader: - kv 30: tokenizer.ggml.padding_token_id u32 = 151643 llama_model_loader: - kv 31: tokenizer.ggml.bos_token_id u32 = 151643 llama_model_loader: - kv 32: tokenizer.ggml.add_bos_token bool = false llama_model_loader: - kv 33: general.quantization_version u32 = 2 llama_model_loader: - kv 34: general.file_type u32 = 15 llama_model_loader: - kv 35: general.url str = https://huggingface.co/mradermacher/H... llama_model_loader: - kv 36: mradermacher.quantize_version str = 2 llama_model_loader: - kv 37: mradermacher.quantized_by str = mradermacher llama_model_loader: - kv 38: mradermacher.quantized_at str = 2025-06-19T15:14:20+02:00 llama_model_loader: - kv 39: mradermacher.quantized_on str = nico1 llama_model_loader: - kv 40: general.source.url str = https://huggingface.co/huihui-ai/Huih... llama_model_loader: - kv 41: mradermacher.convert_type str = hf llama_model_loader: - kv 42: quantize.imatrix.file str = Huihui-Qwen3-0.6B-abliterated-v2-i1-G... llama_model_loader: - kv 43: quantize.imatrix.dataset str = imatrix-training-full-3 llama_model_loader: - kv 44: quantize.imatrix.entries_count i32 = 196 llama_model_loader: - kv 45: quantize.imatrix.chunks_count i32 = 318 llama_model_loader: - type f32: 113 tensors llama_model_loader: - type q4_K: 168 tensors llama_model_loader: - type q6_K: 29 tensors print_info: file format = GGUF V3 (latest) print_info: file type = Q4_K - Medium print_info: file size = 372.65 MiB (5.24 BPW) load: printing all EOG tokens: load: - 151643 ('&amp;lt;|endoftext|&amp;gt;') load: - 151645 ('&amp;lt;|im_end|&amp;gt;') load: - 151662 ('&amp;lt;|fim_pad|&amp;gt;') load: - 151663 ('&amp;lt;|repo_name|&amp;gt;') load: - 151664 ('&amp;lt;|file_sep|&amp;gt;') load: special tokens cache size = 26 load: token to piece cache size = 0.9311 MB print_info: arch = qwen3 print_info: vocab_only = 0 print_info: n_ctx_train = 40960 print_info: n_embd = 1024 print_info: n_embd_inp = 1024 print_info: n_layer = 28 print_info: n_head = 16 print_info: n_head_kv = 8 print_info: n_rot = 128 print_info: n_swa = 0 print_info: is_swa_any = 0 print_info: n_embd_head_k = 128 print_info: n_embd_head_v = 128 print_info: n_gqa = 2 print_info: n_embd_k_gqa = 1024 print_info: n_embd_v_gqa = 1024 print_info: f_norm_eps = 0.0e+00 print_info: f_norm_rms_eps = 1.0e-06 print_info: f_clamp_kqv = 0.0e+00 print_info: f_max_alibi_bias = 0.0e+00 print_info: f_logit_scale = 0.0e+00 print_info: f_attn_scale = 0.0e+00 print_info: n_ff = 3072 print_info: n_expert = 0 print_info: n_expert_used = 0 print_info: n_expert_groups = 0 print_info: n_group_used = 0 print_info: causal attn = 1 print_info: pooling type = -1 print_info: rope type = 2 print_info: rope scaling = linear print_info: freq_base_train = 1000000.0 print_info: freq_scale_train = 1 print_info: n_ctx_orig_yarn = 40960 print_info: rope_finetuned = unknown print_info: model type = 0.6B print_info: model params = 596.05 M print_info: general.name = Huihui Qwen3 0.6B Abliterated v2 print_info: vocab type = BPE print_info: n_vocab = 151936 print_info: n_merges = 151387 print_info: BOS token = 151643 '&amp;lt;|endoftext|&amp;gt;' print_info: EOS token = 151645 '&amp;lt;|im_end|&amp;gt;' print_info: EOT token = 151645 '&amp;lt;|im_end|&amp;gt;' print_info: PAD token = 151643 '&amp;lt;|endoftext|&amp;gt;' print_info: LF token = 198 'Ċ' print_info: FIM PRE token = 151659 '&amp;lt;|fim_prefix|&amp;gt;' print_info: FIM SUF token = 151661 '&amp;lt;|fim_suffix|&amp;gt;' print_info: FIM MID token = 151660 '&amp;lt;|fim_middle|&amp;gt;' print_info: FIM PAD token = 151662 '&amp;lt;|fim_pad|&amp;gt;' print_info: FIM REP token = 151663 '&amp;lt;|repo_name|&amp;gt;' print_info: FIM SEP token = 151664 '&amp;lt;|file_sep|&amp;gt;' print_info: EOG token = 151643 '&amp;lt;|endoftext|&amp;gt;' print_info: EOG token = 151645 '&amp;lt;|im_end|&amp;gt;' print_info: EOG token = 151662 '&amp;lt;|fim_pad|&amp;gt;' print_info: EOG token = 151663 '&amp;lt;|repo_name|&amp;gt;' print_info: EOG token = 151664 '&amp;lt;|file_sep|&amp;gt;' print_info: max token length = 256 load_tensors: loading model tensors, this can take a while... (mmap = true) Segmentation fault llama-cli -m /sdcard/Huihui-Qwen3-0.6B-abliterated-v2.i1-Q4_K_M.gguf -ngl 99 ~ $ &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Device: Samsung A15 GPU: Mali G57 MC2 Chipset: Mediatek Helio G99&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bulieme0"&gt; /u/bulieme0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pe3sme/had_anyone_tried_to_make_llamacpp_vulkan_work_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pe3sme/had_anyone_tried_to_make_llamacpp_vulkan_work_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pe3sme/had_anyone_tried_to_make_llamacpp_vulkan_work_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T16:04:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1pe4iev</id>
    <title>We Got Claude to Fine-Tune an Open Source LLM</title>
    <updated>2025-12-04T16:31:07+00:00</updated>
    <author>
      <name>/u/PotentialFunny7143</name>
      <uri>https://old.reddit.com/user/PotentialFunny7143</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/blog/hf-skills-training"&gt;https://huggingface.co/blog/hf-skills-training&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PotentialFunny7143"&gt; /u/PotentialFunny7143 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pe4iev/we_got_claude_to_finetune_an_open_source_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pe4iev/we_got_claude_to_finetune_an_open_source_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pe4iev/we_got_claude_to_finetune_an_open_source_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T16:31:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdtht0</id>
    <title>Do you think we’ll get GLM 4.6 Air one day?</title>
    <updated>2025-12-04T07:04:52+00:00</updated>
    <author>
      <name>/u/Leflakk</name>
      <uri>https://old.reddit.com/user/Leflakk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Was about to forget, it’s probably not the priority of zai but hope remain!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Leflakk"&gt; /u/Leflakk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdtht0/do_you_think_well_get_glm_46_air_one_day/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdtht0/do_you_think_well_get_glm_46_air_one_day/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdtht0/do_you_think_well_get_glm_46_air_one_day/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T07:04:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdm268</id>
    <title>How Attention Got So Efficient [GQA/MLA/DSA]</title>
    <updated>2025-12-04T00:53:59+00:00</updated>
    <author>
      <name>/u/onil_gova</name>
      <uri>https://old.reddit.com/user/onil_gova</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdm268/how_attention_got_so_efficient_gqamladsa/"&gt; &lt;img alt="How Attention Got So Efficient [GQA/MLA/DSA]" src="https://external-preview.redd.it/4QixmEzxJtTr5ZgAjR4FoJjK4qVPLU4zAuNo-fsPzgM.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5f8badcadc6197f760be212560f8188dc2793fa6" title="How Attention Got So Efficient [GQA/MLA/DSA]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For anyone trying to understand why Deepseek 3.2 DSA is a milestone in terms of solving long context, I really recommend this video.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/onil_gova"&gt; /u/onil_gova &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/Y-o545eYjXM?si=pt-SxR5anfLNSN8j"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdm268/how_attention_got_so_efficient_gqamladsa/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdm268/how_attention_got_so_efficient_gqamladsa/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T00:53:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdfk0o</id>
    <title>Hermes 4.3 - 36B Model released</title>
    <updated>2025-12-03T20:30:22+00:00</updated>
    <author>
      <name>/u/crazeum</name>
      <uri>https://old.reddit.com/user/crazeum</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdfk0o/hermes_43_36b_model_released/"&gt; &lt;img alt="Hermes 4.3 - 36B Model released" src="https://external-preview.redd.it/thAQxjbw3fpc9fgR1nrJDb-3cDeZ9f7TtJWveW5lCQ4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=49b96ff1b32dfa841362b8c2a0d4449fdd83b1f0" title="Hermes 4.3 - 36B Model released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hermes uncensored line models with apache 2 license. Post trained from Seed-OSS-36B-Base on their psyche network. The cool bit is they also trained it centralized and the distributed psyche trained version outperformed the centrally trained one.&lt;/p&gt; &lt;p&gt;GGUF links: &lt;a href="https://huggingface.co/NousResearch/Hermes-4.3-36B-GGUF"&gt;https://huggingface.co/NousResearch/Hermes-4.3-36B-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/crazeum"&gt; /u/crazeum &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://nousresearch.com/introducing-hermes-4-3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdfk0o/hermes_43_36b_model_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdfk0o/hermes_43_36b_model_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T20:30:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdwzgb</id>
    <title>Deepseek 3.2 just does not seem to perform (for me)</title>
    <updated>2025-12-04T10:49:38+00:00</updated>
    <author>
      <name>/u/klippers</name>
      <uri>https://old.reddit.com/user/klippers</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been using RooCode and just not feeling DeepSeek 3.2 at all. &lt;/p&gt; &lt;p&gt;Is it just me? Any tips? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/klippers"&gt; /u/klippers &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdwzgb/deepseek_32_just_does_not_seem_to_perform_for_me/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdwzgb/deepseek_32_just_does_not_seem_to_perform_for_me/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdwzgb/deepseek_32_just_does_not_seem_to_perform_for_me/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T10:49:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdzmxh</id>
    <title>BrowseSafe, An Open-Source Model for AI Agents Browser Security</title>
    <updated>2025-12-04T13:11:36+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdzmxh/browsesafe_an_opensource_model_for_ai_agents/"&gt; &lt;img alt="BrowseSafe, An Open-Source Model for AI Agents Browser Security" src="https://preview.redd.it/x84cad88q65g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0239420f122e41f3ba7514e3d0d9eaaa88de923f" title="BrowseSafe, An Open-Source Model for AI Agents Browser Security" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;BrowseSafe is an open-source security model trained to protect AI browser agents from prompt injection attacks embedded in real-world web content. BrowseSafe model is based on the &lt;strong&gt;Qwen3-30B-A3B.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Here is a brief overview of key features of BrowseSafe model:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. State-of-the-Art Detection&lt;/strong&gt;: Achieves a 90.4% F1 score on the BrowseSafe-Bench test set outperforming models like GPT-5 and Sonnet 4.5.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Robustness to Distractors&lt;/strong&gt;: Specifically trained to distinguish between malicious instructions and benign, structure-rich HTML &amp;quot;noise&amp;quot;.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Real-Time Latency&lt;/strong&gt;: Optimized for agent loops, enabling async security checks without degrading user experience.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Comprehensive Coverage&lt;/strong&gt;: Validated against 11 attack types with different security criticality levels.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;BrowseSafe model overview&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Type&lt;/strong&gt;: Fine-tuned Causal Language Model (MoE) for SFT Classification&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Training Stage&lt;/strong&gt;: Post-training (Fine-tuning on BrowseSafe-Bench)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Dataset&lt;/strong&gt;: BrowseSafe-Bench&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Base Model&lt;/strong&gt;: Qwen/Qwen3-30B-A3B-Instruct-2507&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Context Length&lt;/strong&gt;: Up to 16,384 tokens&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Input&lt;/strong&gt;: Raw HTML content&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Output&lt;/strong&gt;: Single token, &amp;quot;yes&amp;quot; or &amp;quot;no&amp;quot; classification&lt;/li&gt; &lt;li&gt;&lt;strong&gt;License&lt;/strong&gt;: MIT&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://huggingface.co/perplexity-ai/browsesafe"&gt;BrowseSafe model&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/x84cad88q65g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdzmxh/browsesafe_an_opensource_model_for_ai_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdzmxh/browsesafe_an_opensource_model_for_ai_agents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T13:11:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdcytv</id>
    <title>Micron Announces Exit from Crucial Consumer Business</title>
    <updated>2025-12-03T18:54:47+00:00</updated>
    <author>
      <name>/u/FullstackSensei</name>
      <uri>https://old.reddit.com/user/FullstackSensei</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Technically speaking, we're screwed.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullstackSensei"&gt; /u/FullstackSensei &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://investors.micron.com/news-releases/news-release-details/micron-announces-exit-crucial-consumer-business"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdcytv/micron_announces_exit_from_crucial_consumer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdcytv/micron_announces_exit_from_crucial_consumer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T18:54:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1pe1bd4</id>
    <title>The "Confident Idiot" Problem: Why LLM-as-a-Judge fails in production.</title>
    <updated>2025-12-04T14:25:15+00:00</updated>
    <author>
      <name>/u/Proud-Employ5627</name>
      <uri>https://old.reddit.com/user/Proud-Employ5627</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been struggling with agent reliability lately. I noticed that the industry standard for fixing hallucinations is &amp;quot;LLM-as-a-Judge&amp;quot; (asking a larger model to grade the output).&lt;/p&gt; &lt;p&gt;But I'm finding this creates a circular dependency. If the underlying models suffer from sycophancy or hallucination, the Judge often hallucinates a passing grade. We are trying to fix probability with more probability.&lt;/p&gt; &lt;p&gt;I wrote up a deep dive on why I think we need to re-introduce &lt;strong&gt;Deterministic Assertions&lt;/strong&gt; (running actual code/regex/SQL parsing) into the agent loop instead of just relying on &amp;quot;Vibe Checks.&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Core Argument:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Don't ask an LLM if a URL is valid. Run &lt;code&gt;requests.get()&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Don't ask an LLM if a SQL query is safe. Parse the AST.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;If the code says &amp;quot;No&amp;quot;, the agent stops. No matter how confident the LLM is.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Full analysis here: &lt;a href="https://steerlabs.substack.com/p/confident-idiot-problem"&gt;https://steerlabs.substack.com/p/confident-idiot-problem&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Curious how others are handling this? Are you using LLM-as-a-Judge successfully, or do you rely on hard constraints?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Proud-Employ5627"&gt; /u/Proud-Employ5627 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pe1bd4/the_confident_idiot_problem_why_llmasajudge_fails/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pe1bd4/the_confident_idiot_problem_why_llmasajudge_fails/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pe1bd4/the_confident_idiot_problem_why_llmasajudge_fails/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T14:25:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdqxbw</id>
    <title>Chinese CXMT unveils DDR5-8000 RAM</title>
    <updated>2025-12-04T04:43:35+00:00</updated>
    <author>
      <name>/u/Ok_Warning2146</name>
      <uri>https://old.reddit.com/user/Ok_Warning2146</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.techpowerup.com/343185/chinese-cxmt-shows-homegrown-ddr5-8000-and-lpddr5x-10667-memory"&gt;https://www.techpowerup.com/343185/chinese-cxmt-shows-homegrown-ddr5-8000-and-lpddr5x-10667-memory&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Chinese RAM might be the way to buck the trend of rising prices.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Warning2146"&gt; /u/Ok_Warning2146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdqxbw/chinese_cxmt_unveils_ddr58000_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdqxbw/chinese_cxmt_unveils_ddr58000_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdqxbw/chinese_cxmt_unveils_ddr58000_ram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T04:43:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdh0sm</id>
    <title>8 local LLMs on a single Strix Halo debating whether a hot dog is a sandwich</title>
    <updated>2025-12-03T21:26:43+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdh0sm/8_local_llms_on_a_single_strix_halo_debating/"&gt; &lt;img alt="8 local LLMs on a single Strix Halo debating whether a hot dog is a sandwich" src="https://external-preview.redd.it/ZzlmajZ6b3gzMjVnMYyMXOA9G9iEfbHd4uR1YsqLbApEsnv66h0V49mXIA5l.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=001f3aaa8ebe40ec05d81117c0df8ce6a792a1bd" title="8 local LLMs on a single Strix Halo debating whether a hot dog is a sandwich" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/o8n25oox325g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdh0sm/8_local_llms_on_a_single_strix_halo_debating/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdh0sm/8_local_llms_on_a_single_strix_halo_debating/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T21:26:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdupdg</id>
    <title>Deepseek's progress</title>
    <updated>2025-12-04T08:21:16+00:00</updated>
    <author>
      <name>/u/onil_gova</name>
      <uri>https://old.reddit.com/user/onil_gova</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdupdg/deepseeks_progress/"&gt; &lt;img alt="Deepseek's progress" src="https://preview.redd.it/zpkzyrrxc55g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=88e4dbc74aac37f16270f4775ec470f375eab2f5" title="Deepseek's progress" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's fascinating that DeepSeek has been able to make all this progress with the same pre-trained model since the start of the year, and has just improved post-training and attention mechanisms. It makes you wonder if other labs are misusing their resources by training new base models so often.&lt;/p&gt; &lt;p&gt;Also, what is going on with the Mistral Large 3 benchmarks?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/onil_gova"&gt; /u/onil_gova &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zpkzyrrxc55g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdupdg/deepseeks_progress/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdupdg/deepseeks_progress/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T08:21:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdu5pe</id>
    <title>WTF are these AI companies doing where they supposedly are the cause of the ram price spike?</title>
    <updated>2025-12-04T07:46:40+00:00</updated>
    <author>
      <name>/u/Red_Redditor_Reddit</name>
      <uri>https://old.reddit.com/user/Red_Redditor_Reddit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I don't understand what could justify that much investment. Maybe I'm way out of the loop, but what huge application are they expecting that would have this kind of payout? Why is there all of the sudden this spike instead of a slower increase in demand? Like I kinda get the overall GPU demand, but this sudden dramatic change in RAM demand doesn't make sense to me. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Red_Redditor_Reddit"&gt; /u/Red_Redditor_Reddit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdu5pe/wtf_are_these_ai_companies_doing_where_they/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdu5pe/wtf_are_these_ai_companies_doing_where_they/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdu5pe/wtf_are_these_ai_companies_doing_where_they/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T07:46:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdvupp</id>
    <title>Cruxy: Train 1.5B models on 4GB VRAM - new optimiser just released</title>
    <updated>2025-12-04T09:37:58+00:00</updated>
    <author>
      <name>/u/National_Control4101</name>
      <uri>https://old.reddit.com/user/National_Control4101</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all, I've just released Cruxy - an adaptive optimiser that lets you fine-tune billion-parameter models on consumer GPUs.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it does:&lt;/strong&gt; - Drop-in replacement for AdamW - Meta-Lion mode uses 1/3 the memory of AdamW - Automatic stability control - no scheduler tuning needed - Verified on TinyLlama 1.1B and Qwen 2.5 1.5B on a GTX 1650 (4GB)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Benchmarks (Shakespeare GPT):&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Optimiser&lt;/th&gt; &lt;th&gt;Final Loss&lt;/th&gt; &lt;th&gt;Memory&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;AdamW&lt;/td&gt; &lt;td&gt;1.6843&lt;/td&gt; &lt;td&gt;100%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Cruxy Meta3&lt;/td&gt; &lt;td&gt;1.6413&lt;/td&gt; &lt;td&gt;100%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Cruxy Meta-Lion&lt;/td&gt; &lt;td&gt;1.6633&lt;/td&gt; &lt;td&gt;33%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Install:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Pip install Cruxy&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/christophergardner-star/Crux1"&gt;https://github.com/christophergardner-star/Crux1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer questions. Built this on evenings and weekends because cloud GPUs are expensive.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/National_Control4101"&gt; /u/National_Control4101 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdvupp/cruxy_train_15b_models_on_4gb_vram_new_optimiser/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdvupp/cruxy_train_15b_models_on_4gb_vram_new_optimiser/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdvupp/cruxy_train_15b_models_on_4gb_vram_new_optimiser/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T09:37:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdu46s</id>
    <title>New model, microsoft/VibeVoice-Realtime-0.5B</title>
    <updated>2025-12-04T07:43:58+00:00</updated>
    <author>
      <name>/u/edward-dev</name>
      <uri>https://old.reddit.com/user/edward-dev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdu46s/new_model_microsoftvibevoicerealtime05b/"&gt; &lt;img alt="New model, microsoft/VibeVoice-Realtime-0.5B" src="https://external-preview.redd.it/yC3RHTaiptQZaDONKxzLP6lQoJh8pT8uDk6mruPADNY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2b568bf1e3f993edb57eab9f43241d593fd7c1c2" title="New model, microsoft/VibeVoice-Realtime-0.5B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;VibeVoice: A Frontier Open-Source Text-to-Speech Model&lt;/p&gt; &lt;p&gt;VibeVoice-Realtime is a lightweight real‑time text-to-speech model supporting streaming text input. It can be used to build realtime TTS services, narrate live data streams, and let different LLMs start speaking from their very first tokens (plug in your preferred model) long before a full answer is generated. It produces initial audible speech in ~300 ms (hardware dependent).&lt;/p&gt; &lt;p&gt;Key features:&lt;/p&gt; &lt;p&gt;Parameter size: 0.5B (deployment-friendly) Realtime TTS (~300 ms first audible latency) Streaming text input Robust long-form speech generation&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/edward-dev"&gt; /u/edward-dev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/microsoft/VibeVoice-Realtime-0.5B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdu46s/new_model_microsoftvibevoicerealtime05b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdu46s/new_model_microsoftvibevoicerealtime05b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T07:43:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdzn2n</id>
    <title>legends</title>
    <updated>2025-12-04T13:11:47+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdzn2n/legends/"&gt; &lt;img alt="legends" src="https://preview.redd.it/vu26lxrns65g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8a61d2260347cccaa67517ffc3812c121edcd5d0" title="legends" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vu26lxrns65g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdzn2n/legends/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdzn2n/legends/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T13:11:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
