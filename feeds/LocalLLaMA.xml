<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-11T15:06:23+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qa1boh</id>
    <title>Surprised I've not yet heard anyone here talk about ClawdBot yet</title>
    <updated>2026-01-11T14:55:38+00:00</updated>
    <author>
      <name>/u/HixVAC</name>
      <uri>https://old.reddit.com/user/HixVAC</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been using it for a couple of weeks now and it really is great. Though honestly I started with using it with Opus, I'm switching to either OSS 120B or Qwen3 Next 80B after I complete my testing.&lt;/p&gt; &lt;p&gt;As to what ClawdBot actually is; it's essentially a self-hosted AI assistant agent. Instead of just talking to an LLM in a browser or what have you, you run this on your own machine (Mac, Linux, or Windows/WSL2) and it hooks into messaging apps (WhatsApp, Telegram, Discord, Signal, etc). The core idea is that it turns an LLM into a personal assistant that can actually touch your local system. It has &amp;quot;skills&amp;quot; or tools that let the agent browse the web, run terminal commands, manage files, and even use your camera or screen. It also supports &amp;quot;Live Canvas,&amp;quot; which is a visual workspace the agent can manipulate while you chat. It‚Äôs built with TypeScript/Node.js and is designed to be &amp;quot;local-first,&amp;quot; meaning you keep control of the data and the gateway, but you can still access your agent from anywhere via the messaging integrations.&lt;/p&gt; &lt;p&gt;It's clear the project is essentially becoming an agentic version of Home Assistant. For users who want a unified, agentic interface across all their devices without being locked into a single proprietary app.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/clawdbot/clawdbot"&gt;https://github.com/clawdbot/clawdbot&lt;/a&gt; &lt;a href="https://docs.clawd.bot/start/getting-started"&gt;https://docs.clawd.bot/start/getting-started&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Highly recommended!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HixVAC"&gt; /u/HixVAC &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qa1boh/surprised_ive_not_yet_heard_anyone_here_talk/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qa1boh/surprised_ive_not_yet_heard_anyone_here_talk/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qa1boh/surprised_ive_not_yet_heard_anyone_here_talk/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-11T14:55:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1qa1guo</id>
    <title>I bought a ‚Ç¨9k GH200 ‚Äúdesktop‚Äù to save $1.27 on Claude Code (vLLM tuning notes)</title>
    <updated>2026-01-11T15:01:18+00:00</updated>
    <author>
      <name>/u/Reddactor</name>
      <uri>https://old.reddit.com/user/Reddactor</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qa1guo/i_bought_a_9k_gh200_desktop_to_save_127_on_claude/"&gt; &lt;img alt="I bought a ‚Ç¨9k GH200 ‚Äúdesktop‚Äù to save $1.27 on Claude Code (vLLM tuning notes)" src="https://b.thumbs.redditmedia.com/Md2maVGNArRbADxcFutg-1j8MHgvnFu8FKKsugE_MQA.jpg" title="I bought a ‚Ç¨9k GH200 ‚Äúdesktop‚Äù to save $1.27 on Claude Code (vLLM tuning notes)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TL;DR: You can go fully local with Claude Code, and with the right tuning, the results are &lt;em&gt;amazing&lt;/em&gt;...&lt;/p&gt; &lt;p&gt;Alright &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;, gather round.&lt;/p&gt; &lt;p&gt;I have committed a perfectly normal act of financial responsibility: I built a &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1pjbhyz/i_bought_a_gracehopper_server_for_75k_on_reddit/"&gt;2√ó GH200 96GB Grace‚ÄìHopper ‚Äúdesktop‚Äù&lt;/a&gt;, spending 9000 euro (no, my wife was not informed beforehand), and then spent a week tuning &lt;strong&gt;vLLM&lt;/strong&gt; so &lt;strong&gt;Claude Code&lt;/strong&gt; could use a &lt;strong&gt;~140GB&lt;/strong&gt; local model instead of calling home.&lt;/p&gt; &lt;p&gt;Result: my machine now produces code reviews locally‚Ä¶ and also produces the funniest accounting line I‚Äôve ever seen.&lt;/p&gt; &lt;h1&gt;The ‚Äúplot twist‚Äù setup&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;2√ó GH200 96GB (so &lt;strong&gt;192GB VRAM&lt;/strong&gt; total)&lt;/li&gt; &lt;li&gt;Topology says &lt;code&gt;SYS&lt;/code&gt;, i.e. &lt;em&gt;no NVLink&lt;/em&gt;, just PCIe/NUMA vibes&lt;/li&gt; &lt;li&gt;Conventional wisdom: ‚Äúno NVLink ‚áí pipeline parallel‚Äù&lt;/li&gt; &lt;li&gt;Me: ‚ÄúSurely guides on the internet wouldn‚Äôt betray me‚Äù&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Reader, the guides betrayed me.&lt;/p&gt; &lt;h1&gt;What actually worked (the boring part, but important)&lt;/h1&gt; &lt;p&gt;‚úÖ &lt;strong&gt;TP2&lt;/strong&gt;: &lt;code&gt;--tensor-parallel-size 2&lt;/code&gt;&lt;br /&gt; ‚úÖ &lt;strong&gt;163,840 context&lt;/strong&gt; ü§Ø&lt;br /&gt; ‚úÖ &lt;code&gt;--max-num-seqs 16&lt;/code&gt; because this one knob controls whether Claude Code feels like a sports car or a fax machine&lt;br /&gt; ‚úÖ chunked prefill default (&lt;code&gt;8192&lt;/code&gt;)&lt;br /&gt; ‚úÖ &lt;code&gt;VLLM_SLEEP_WHEN_IDLE=0&lt;/code&gt; to avoid ‚Äúfirst request after idle‚Äù jumpscares&lt;/p&gt; &lt;p&gt;Shoutout to &lt;strong&gt;mratsim&lt;/strong&gt; for the MiniMax-M2.1 FP8+INT4 AWQ quant tuned for &lt;strong&gt;192GB VRAM&lt;/strong&gt; systems. Absolute legend. üôè&lt;/p&gt; &lt;h1&gt;The ‚ÄúI can‚Äôt believe this‚Äù part&lt;/h1&gt; &lt;h1&gt;Pipeline parallel (PP2) did NOT save me&lt;/h1&gt; &lt;p&gt;Despite &lt;code&gt;SYS&lt;/code&gt; topology (aka ‚Äúcommunication is pain‚Äù), &lt;strong&gt;PP2 faceplanted&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;PP2 couldn‚Äôt even start at &lt;strong&gt;163k&lt;/strong&gt; context (KV cache said ‚Äúno‚Äù)&lt;/li&gt; &lt;li&gt;I lowered to &lt;strong&gt;114k&lt;/strong&gt; and it started‚Ä¶&lt;/li&gt; &lt;li&gt;‚Ä¶and then it was still &lt;strong&gt;way slower&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;short_c4: &lt;strong&gt;~49.9 tok/s&lt;/strong&gt; (TP2 was ~78)&lt;/li&gt; &lt;li&gt;short_c8: &lt;strong&gt;~28.1 tok/s&lt;/strong&gt; (TP2 was ~66)&lt;/li&gt; &lt;li&gt;TTFT tails got &lt;em&gt;feral&lt;/em&gt; (multi-second warmup/short tests)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So yeah: topology said ‚Äúpipeline‚Äù, reality said ‚Äúlol‚Äù.&lt;/p&gt; &lt;h1&gt;The hidden boss fight: --max-num-seqs&lt;/h1&gt; &lt;p&gt;I tried 4 / 16 / 32.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;4&lt;/code&gt; = &lt;em&gt;scheduler becomes a nightclub bouncer&lt;/em&gt; ‚Üí queuing ‚Üí TTFT p99 goes to the moon&lt;/li&gt; &lt;li&gt;&lt;code&gt;16&lt;/code&gt; = &lt;strong&gt;Goldilocks&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;32&lt;/code&gt; = fine, but 16 feels safer without losing perf&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Translation: if Claude Code feels ‚Äúrandomly slow‚Äù, it might not be your GPU‚Ä¶ it might be your scheduler admitting requests like it‚Äôs rationing oxygen.&lt;/p&gt; &lt;h1&gt;The Payout&lt;/h1&gt; &lt;p&gt;Claude Code printed this after a run:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Total cost: $1.27 (costs may be inaccurate due to usage of unknown models) Total duration (API): 1m 58s Total duration (wall): 4m 10s Usage by model: MiniMax-M2.1-FP8: 391.5k input, 6.4k output, 0 cache read, 0 cache write ($1.27) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;So anyway, &lt;strong&gt;spending ‚Ç¨9,000&lt;/strong&gt; on this box saved me &lt;strong&gt;$1.27&lt;/strong&gt;.&lt;br /&gt; Only a few thousand repo reviews until I break even. üí∏ü§°&lt;/p&gt; &lt;p&gt;&lt;a href="https://dnhkng.github.io/posts/vllm-optimization-gh200/"&gt;&lt;strong&gt;Read all the details here!&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reddactor"&gt; /u/Reddactor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qa1guo"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qa1guo/i_bought_a_9k_gh200_desktop_to_save_127_on_claude/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qa1guo/i_bought_a_9k_gh200_desktop_to_save_127_on_claude/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-11T15:01:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1q9hu43</id>
    <title>I built an end-to-end local LLM fine-tuning GUI for M series macs</title>
    <updated>2026-01-10T22:31:40+00:00</updated>
    <author>
      <name>/u/riman717</name>
      <uri>https://old.reddit.com/user/riman717</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just wanted to share a tool I‚Äôve been working on to make local fine-tuning on M series Macs a bit less painful and manual. Essentially it wraps Apple‚Äôs MLX framework, so it runs native on M-series chips. The goal of this was to include the whole end-to-end local LLM workflow all within a GUI. Here are the features I put in:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Data Prep- You can drag and drop CSV or JSONL files to clean/format them. I also added a local PII scrubber to strip names/emails from datasets before training.&lt;/li&gt; &lt;li&gt;Fine-Tuning- UI for LoRA/QLoRA. You can tweak learning rates, epochs, rank, etc&lt;/li&gt; &lt;li&gt;Inference- Built-in chat interface to test your Fine Tuned model adapters against the base model&lt;/li&gt; &lt;li&gt;Models- One-click download for open source LLMs, or you can &amp;quot;add a model&amp;quot; if you have local model rates&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Repo is here if you want to check it out: &lt;a href="https://github.com/rileycleavenger/Silicon-Studio"&gt;https://github.com/rileycleavenger/Silicon-Studio&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Feel free to contribute or open any issues on the repo.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/riman717"&gt; /u/riman717 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9hu43/i_built_an_endtoend_local_llm_finetuning_gui_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9hu43/i_built_an_endtoend_local_llm_finetuning_gui_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q9hu43/i_built_an_endtoend_local_llm_finetuning_gui_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-10T22:31:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1q9bnqc</id>
    <title>RTX 50 Super GPUs may be delayed indefinitely, as Nvidia prioritizes AI during memory shortage (rumor, nothing official)</title>
    <updated>2026-01-10T18:33:05+00:00</updated>
    <author>
      <name>/u/3090orBust</name>
      <uri>https://old.reddit.com/user/3090orBust</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9bnqc/rtx_50_super_gpus_may_be_delayed_indefinitely_as/"&gt; &lt;img alt="RTX 50 Super GPUs may be delayed indefinitely, as Nvidia prioritizes AI during memory shortage (rumor, nothing official)" src="https://external-preview.redd.it/o9dwYZagSuAh6lGApyCDxDEoIhgXfz9GzoXhVGYG6FI.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cbb419ac1006074d3e7d893a55075c0de8698965" title="RTX 50 Super GPUs may be delayed indefinitely, as Nvidia prioritizes AI during memory shortage (rumor, nothing official)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/3090orBust"&gt; /u/3090orBust &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.notebookcheck.net/RTX-50-Super-GPUs-may-be-delayed-indefinitely-as-Nvidia-prioritizes-AI-during-memory-shortage.1199980.0.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9bnqc/rtx_50_super_gpus_may_be_delayed_indefinitely_as/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q9bnqc/rtx_50_super_gpus_may_be_delayed_indefinitely_as/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-10T18:33:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1q9wg6p</id>
    <title>Fine-Tuning Translation Model</title>
    <updated>2026-01-11T10:48:48+00:00</updated>
    <author>
      <name>/u/BoysenberryNo3331</name>
      <uri>https://old.reddit.com/user/BoysenberryNo3331</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I don't know about local LLM's really. I'm using gemini 3 flash for translating manga etc. The translation accuracy is high. But I want it to be more natural? I'm using a prompt focused on localization and natural flow. I'm wondering If I fine-tune a local llm with 50 episode translation It will be better? Or a dataset focused on proofreading.&lt;/p&gt; &lt;p&gt;(EN-TR Translation)&lt;/p&gt; &lt;p&gt;I don't know much about these things. Please excuse me if my requests seem absurd.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BoysenberryNo3331"&gt; /u/BoysenberryNo3331 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9wg6p/finetuning_translation_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9wg6p/finetuning_translation_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q9wg6p/finetuning_translation_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-11T10:48:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1q9m0uw</id>
    <title>brain-canvas: Give any local LLM a visual display (191 lines, 0 deps)</title>
    <updated>2026-01-11T01:30:17+00:00</updated>
    <author>
      <name>/u/Signal_Usual8630</name>
      <uri>https://old.reddit.com/user/Signal_Usual8630</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Tired of LLM output being stuck in the terminal?&lt;/p&gt; &lt;p&gt;npx brain-canvas&lt;/p&gt; &lt;p&gt;Starts a local HTML canvas that any LLM can control via POST requests. Send JSON, get interactive UI with clickable choices that flow back to your script.&lt;/p&gt; &lt;p&gt;Works with:&lt;/p&gt; &lt;p&gt;- Ollama&lt;/p&gt; &lt;p&gt;- llama.cpp&lt;/p&gt; &lt;p&gt;- Any local model&lt;/p&gt; &lt;p&gt;- Claude/GPT (if you use those too)&lt;/p&gt; &lt;p&gt;The numbers:&lt;/p&gt; &lt;p&gt;- 191 lines of code&lt;/p&gt; &lt;p&gt;- 0 dependencies&lt;/p&gt; &lt;p&gt;- 6.9 KB package&lt;/p&gt; &lt;p&gt;- 10 section types (stats, timeline, comparison, choices, etc.)&lt;/p&gt; &lt;p&gt;POST JSON like:&lt;/p&gt; &lt;p&gt;{&amp;quot;title&amp;quot;: &amp;quot;Pick one&amp;quot;, &amp;quot;sections&amp;quot;: [{&amp;quot;type&amp;quot;: &amp;quot;choices&amp;quot;, &amp;quot;items&amp;quot;: [{&amp;quot;id&amp;quot;: &amp;quot;a&amp;quot;, &amp;quot;label&amp;quot;: &amp;quot;Option A&amp;quot;}]}]}&lt;/p&gt; &lt;p&gt;GET /choice returns what the user clicked.&lt;/p&gt; &lt;p&gt;Zero config. Works on Mac/Linux/Windows.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/mordechaipotash/brain-canvas"&gt;https://github.com/mordechaipotash/brain-canvas&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Signal_Usual8630"&gt; /u/Signal_Usual8630 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9m0uw/braincanvas_give_any_local_llm_a_visual_display/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9m0uw/braincanvas_give_any_local_llm_a_visual_display/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q9m0uw/braincanvas_give_any_local_llm_a_visual_display/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-11T01:30:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1q9wynp</id>
    <title>TranscriptionSuite - A comprehensive speech-to-text audio transcription app</title>
    <updated>2026-01-11T11:19:49+00:00</updated>
    <author>
      <name>/u/Curious_Betsy_</name>
      <uri>https://old.reddit.com/user/Curious_Betsy_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9wynp/transcriptionsuite_a_comprehensive_speechtotext/"&gt; &lt;img alt="TranscriptionSuite - A comprehensive speech-to-text audio transcription app" src="https://b.thumbs.redditmedia.com/G7AOiLWZWomeuCXgAUwBzUJLdp37TOR1z2DrN5T6f_g.jpg" title="TranscriptionSuite - A comprehensive speech-to-text audio transcription app" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;em&gt;Welcome to my vibecoded mess! I'll be your host, homelab-00.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/yuu1cq2lbpcg1.png?width=1168&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=953058597490c952d7aaff606f61759394a29a8d"&gt;Logo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm finally at the point where I can say that &lt;a href="https://github.com/homelab-00/TranscriptionSuite"&gt;&lt;strong&gt;TranscriptionSuite&lt;/strong&gt;&lt;/a&gt; is ready for a public release.&lt;br /&gt; A fully featured local audio transcription app that offers:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Truly Multilingual&lt;/strong&gt;: Supports &lt;a href="https://platform.openai.com/docs/guides/speech-to-text/supported-languages"&gt;90+ languages&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Fully featured GUI&lt;/strong&gt;: Native app for KDE, GNOME, and Windows&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Longform Transcription&lt;/strong&gt;: Starts recording, listens until you press stop, then immediately starts transcribing - think of it like dictation&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Static File Transcription&lt;/strong&gt;: Transcribe an existing audio/video file&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Remote Access&lt;/strong&gt;: Securely access your desktop at home running the model from anywhere (utilizing Tailscale)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Speaker Diarization&lt;/strong&gt;: PyAnnote-based speaker identification&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Audio Notebook&lt;/strong&gt;: An Audio Notebook mode, with a calendar-based view, full-text search, and LM Studio integration (chat about your notes with the AI)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;üìå&lt;em&gt;Half an hour of audio transcribed in under a minute (RTX 3060)!&lt;/em&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;...so essentially a fancy wrapper around &lt;code&gt;faster-whisper&lt;/code&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;strong&gt;Screenshots&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/atxuym9yfpcg1.png?width=710&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ee481c7911007336c6149304a9a5bfd3df82e945"&gt;Home view&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/dsmz8dewfpcg1.png?width=710&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=06c46a8133937d36915ca87de867be87046daf46"&gt;Server view&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/jr4kf0rmdpcg1.png?width=1151&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b8a0eeb026af846c5d96aa66dc7a1dc7d9261fd3"&gt;Audio Notebook Calendar view&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/eym7digndpcg1.png?width=1151&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d77f455bc798a80cf580d5f56ec8e64fb3514cd6"&gt;Audio Note Entry view showcasing word-level timestamps&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/90xn1b2odpcg1.png?width=1155&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b802c0bd940dc8b56001397281065134d0565d37"&gt;Audio Note Entry view showcasing diarization&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Videos&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1q9wynp/video/qymj97izdpcg1/player"&gt;Transcription demo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1q9wynp/video/rqpw26i0epcg1/player"&gt;Audio Notebook demo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And if anyone wants the boring backstory~&lt;/p&gt; &lt;p&gt;About 10 years ago I wanted to try Linux and so I installed the most recommended beginner distro at the time, Ubuntu. Even with all the resources available specifically to Ubuntu, I couldn‚Äôt grasp the system well enough to turn it into my daily driver (plus gaming on Linux just sucked back then).&lt;br /&gt; On the other hand, about a year ago I started tinkering with Linux again and not soon after I attempted to install Arch. Took my a couple of days, a ton of forum research and copious amounts of ChatGPT compute, but I did manage it more than fine. And here I am now, daily driving the system for months with no issues whatsoever.&lt;/p&gt; &lt;p&gt;In the same vain, I started playing around with some toy Python projects and learning the basics of software development. AI was (and still is) a huge asset both in helping me learn and writing parts of the code itself.&lt;/p&gt; &lt;p&gt;This then turned into a small hobby project to solve a real (albeit minor) issue I was having; I couldn‚Äôt talk to LLMs at my own ease. You can use the transcribe function on ChatGPT for example for short 30s sessions just fine, but start going over ~5 minutes and the whole thing just crashes. And mind you, transcription is vastly cheaper than the actual chatbots offered by these providers.&lt;/p&gt; &lt;p&gt;Now, just like everyone else, I‚Äôll be lazy when I can. So the first thing I looked for was if anyone else had built something like that. The only one I found was &lt;a href="https://github.com/KoljaB/RealtimeSTT"&gt;RealtimeSTT&lt;/a&gt;. It worked well enough for what I was trying to do so I just used that. After a while however I started adding my own bits and since that project was put on an indefinite hiatus I started developing my own independently.&lt;/p&gt; &lt;p&gt;&lt;em&gt;Feel free to tell me how much my project sucks!&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Curious_Betsy_"&gt; /u/Curious_Betsy_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9wynp/transcriptionsuite_a_comprehensive_speechtotext/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9wynp/transcriptionsuite_a_comprehensive_speechtotext/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q9wynp/transcriptionsuite_a_comprehensive_speechtotext/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-11T11:19:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1q9nerk</id>
    <title>Made an Rick and Morty inspired Interdimensional News site with Ollama and Gemini</title>
    <updated>2026-01-11T02:32:32+00:00</updated>
    <author>
      <name>/u/WahWahWeWah</name>
      <uri>https://old.reddit.com/user/WahWahWeWah</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, I love Rick and Morty esp. the interdimensional cable episodes. So, I build &lt;a href="http://greenportal.news"&gt;greenportal.news&lt;/a&gt; using ollama and gemini. &lt;/p&gt; &lt;p&gt;I'm happy to double click on how the site is made. Basically, its a scraper of a lot of news content off of the internet. Then, using ollama + nemotron-3-nano I extract and score the articles. The alternate universes work the same way, with ollama expanding the prompt and creating the rules for the universe. Lastly, I make a few images in Nano Banana--which imho are the funniest part.&lt;/p&gt; &lt;p&gt;I'd like to move off Gemini to something I can run locally. Any recommendations? I'm rolling with a single 4090 over here so I'd love to keep using that.&lt;/p&gt; &lt;p&gt;Lastly, I write enterprise software so I know the UX isn't amazing. Don't be too hard on me :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WahWahWeWah"&gt; /u/WahWahWeWah &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9nerk/made_an_rick_and_morty_inspired_interdimensional/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9nerk/made_an_rick_and_morty_inspired_interdimensional/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q9nerk/made_an_rick_and_morty_inspired_interdimensional/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-11T02:32:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1q9bj5j</id>
    <title>I made a website to turn any confusing UI into a step-by-step guide via screen sharing (open source)</title>
    <updated>2026-01-10T18:28:17+00:00</updated>
    <author>
      <name>/u/bullmeza</name>
      <uri>https://old.reddit.com/user/bullmeza</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9bj5j/i_made_a_website_to_turn_any_confusing_ui_into_a/"&gt; &lt;img alt="I made a website to turn any confusing UI into a step-by-step guide via screen sharing (open source)" src="https://preview.redd.it/yrb4rq69ekcg1.gif?width=640&amp;amp;crop=smart&amp;amp;s=22c6cdbfa015e9f0b501163a5ab174d6aa588f3f" title="I made a website to turn any confusing UI into a step-by-step guide via screen sharing (open source)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built Screen Vision, an &lt;strong&gt;open source website&lt;/strong&gt; that guides you through any task by screen sharing with AI.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Privacy Focused:&lt;/strong&gt; Your screen data is &lt;strong&gt;never&lt;/strong&gt; stored or used to train models. &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Local LLM Support:&lt;/strong&gt; If you don't trust cloud APIs, the app has a &amp;quot;Local Mode&amp;quot; that connects to local AI models running on your own machine. Your data never leaves your computer.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Web-Native:&lt;/strong&gt; No desktop app or extension required. Works directly on your browser.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;How it works:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Instruction &amp;amp; Grounding:&lt;/strong&gt; The system uses GPT-5.2 to determine the next logical step based on your goal and current screen state. These instructions are then passed to Qwen 3VL (30B), which identifies the exact screen coordinates for the action.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Visual Verification:&lt;/strong&gt; The app monitors your screen for changes every 200ms using a pixel-comparison loop. Once a change is detected, it compares before and after snapshots using Gemini 3 Flash to confirm the step was completed successfully before automatically moving to the next task.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Source Code:&lt;/strong&gt; &lt;a href="https://github.com/bullmeza/screen.vision"&gt;https://github.com/bullmeza/screen.vision&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;Demo:&lt;/strong&gt; &lt;a href="https://screen.vision/"&gt;https://screen.vision&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I‚Äôm looking for feedback, please let me know what you think!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bullmeza"&gt; /u/bullmeza &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/yrb4rq69ekcg1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9bj5j/i_made_a_website_to_turn_any_confusing_ui_into_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q9bj5j/i_made_a_website_to_turn_any_confusing_ui_into_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-10T18:28:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1q9yd1w</id>
    <title>Llama.cpp rpc experiment</title>
    <updated>2026-01-11T12:38:48+00:00</updated>
    <author>
      <name>/u/ciprianveg</name>
      <uri>https://old.reddit.com/user/ciprianveg</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have 2 PCs with 2 3090 gpus each and 3975wx cpu. Using OSS 120b on one PC with cca 40gb on vram and 30gb on ram, TG speed 50t/s. I tried using it totally in vram using rpc with the 2 pcs linked with 10gbit network cards - TG speed 37t/s. Unexpectedly low speed. I updated network to 50gbit - TG speed 38t/s. Looking like the network speed is not the bottleneck I did one more experiment: Same as in the first test, on a single PC, but with the first gpu local and the second gpu as RPC on localhost, so no network delay, all local. Results 38t/s. So with same pc and same gpus, but the second GPU set as RPC device, it dropped from 50 to 38t/s. So the RPC implementation slows down a lot even on the same pc, no network delay..&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ciprianveg"&gt; /u/ciprianveg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9yd1w/llamacpp_rpc_experiment/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9yd1w/llamacpp_rpc_experiment/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q9yd1w/llamacpp_rpc_experiment/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-11T12:38:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1q9pe4l</id>
    <title>I built a benchmark measuring the Markdown quality of LLMs</title>
    <updated>2026-01-11T04:06:40+00:00</updated>
    <author>
      <name>/u/bengt0</name>
      <uri>https://old.reddit.com/user/bengt0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9pe4l/i_built_a_benchmark_measuring_the_markdown/"&gt; &lt;img alt="I built a benchmark measuring the Markdown quality of LLMs" src="https://preview.redd.it/toz75kg5ancg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=74ecf9a9ef58952a70397f5b1fc4fc2a8a51d953" title="I built a benchmark measuring the Markdown quality of LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://lintbench.ai"&gt;https://lintbench.ai&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bengt0"&gt; /u/bengt0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/toz75kg5ancg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9pe4l/i_built_a_benchmark_measuring_the_markdown/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q9pe4l/i_built_a_benchmark_measuring_the_markdown/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-11T04:06:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1qa042v</id>
    <title>Tested GLM 4.7 vs MiniMax 2.1 on a complex Typescript Monorepo</title>
    <updated>2026-01-11T14:03:53+00:00</updated>
    <author>
      <name>/u/Firm_Meeting6350</name>
      <uri>https://old.reddit.com/user/Firm_Meeting6350</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There's a few comparisons around here, but it's always kinda YMMV so I thought I'll run my own.&lt;/p&gt; &lt;p&gt;Both were given the same extensive instructions (specific implementation flow guidance, 2300 Lines of Specification, etc.) - that's not vibe-coding, promised, so the results should be comparable. Again, YMMV, but I asked Codex to review and compare both.&lt;/p&gt; &lt;p&gt;Here are the results:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Dimension&lt;/th&gt; &lt;th align="left"&gt;MiniMax 2.1&lt;/th&gt; &lt;th align="left"&gt;GLM 4.7&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Completeness&lt;/td&gt; &lt;td align="left"&gt;4/10&lt;/td&gt; &lt;td align="left"&gt;8/10&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Correctness&lt;/td&gt; &lt;td align="left"&gt;3/10&lt;/td&gt; &lt;td align="left"&gt;7/10&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Architecture Alignment&lt;/td&gt; &lt;td align="left"&gt;3/10&lt;/td&gt; &lt;td align="left"&gt;8/10&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Cleanliness&lt;/td&gt; &lt;td align="left"&gt;6/10&lt;/td&gt; &lt;td align="left"&gt;7/10&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Test Coverage&lt;/td&gt; &lt;td align="left"&gt;6/10&lt;/td&gt; &lt;td align="left"&gt;7/10&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Risk (higher score = lower risk)&lt;/td&gt; &lt;td align="left"&gt;2/10&lt;/td&gt; &lt;td align="left"&gt;7/10&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Firm_Meeting6350"&gt; /u/Firm_Meeting6350 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qa042v/tested_glm_47_vs_minimax_21_on_a_complex/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qa042v/tested_glm_47_vs_minimax_21_on_a_complex/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qa042v/tested_glm_47_vs_minimax_21_on_a_complex/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-11T14:03:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1q998is</id>
    <title>Visualizing RAG, PART 2- visualizing retrieval</title>
    <updated>2026-01-10T16:59:58+00:00</updated>
    <author>
      <name>/u/Fear_ltself</name>
      <uri>https://old.reddit.com/user/Fear_ltself</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q998is/visualizing_rag_part_2_visualizing_retrieval/"&gt; &lt;img alt="Visualizing RAG, PART 2- visualizing retrieval" src="https://external-preview.redd.it/d2kxeWY2ZzV6amNnMV4lBvCWgLP7SJvsjxaLS786SW2_ibHBrw3ehpZ9iEQq.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=950c6e4a869e18d9d6e90193c9adaaa7fb875e40" title="Visualizing RAG, PART 2- visualizing retrieval" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Edit: code is live at &lt;a href="https://github.com/CyberMagician/Project_Golem"&gt;https://github.com/CyberMagician/Project_Golem&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Still editing the repository but basically just download the requirements (from requirements txt), run the python ingest to build out the brain you see here in LanceDB real quick, then launch the backend server and front end visualizer.&lt;/p&gt; &lt;p&gt;Using UMAP and some additional code to visualizing the 768D vector space of EmbeddingGemma:300m down to 3D and how the RAG ‚Äúthinks‚Äù when retrieving relevant context chunks. How many nodes get activated with each query. It is a follow up from my previous post that has a lot more detail in the comments there about how it‚Äôs done. Feel free to ask questions I‚Äôll answer when I‚Äôm free&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fear_ltself"&gt; /u/Fear_ltself &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/mrkuplj5zjcg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q998is/visualizing_rag_part_2_visualizing_retrieval/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q998is/visualizing_rag_part_2_visualizing_retrieval/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-10T16:59:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1q9si66</id>
    <title>Looking for a Base Model</title>
    <updated>2026-01-11T06:49:17+00:00</updated>
    <author>
      <name>/u/AutomataManifold</name>
      <uri>https://old.reddit.com/user/AutomataManifold</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was putting together a finetuning dataset for an experiment and I realized that I have lost track of which models have base models available. I can search for models with &amp;quot;base&amp;quot; in the name and find stuff like &lt;a href="https://huggingface.co/Qwen/Qwen3-8B-Base"&gt;Qwen 3 8B base&lt;/a&gt; but I'm pretty sure that there are base models I'm overlooking. Do you have a favorite base model?&lt;/p&gt; &lt;p&gt;Models I've found so far:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Qwen 3 base, in 1B, &lt;a href="https://huggingface.co/Qwen/Qwen3-8B-Base"&gt;8B&lt;/a&gt;, 30B, 30B-A3B etc.&lt;/li&gt; &lt;li&gt;LiquidAI's LFM2.5 (&lt;a href="https://huggingface.co/LiquidAI/LFM2.5-1.2B-Base"&gt;1.2B&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;DeepSeek-V3 (&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3-Base"&gt;671B&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;DeepSeek-Coder-V2 (&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-Coder-V2-Base"&gt;236B&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;NVIDIA Nemotron-3-Nano (&lt;a href="https://huggingface.co/nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-Base-BF16"&gt;30B-A3B&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;NVIDIA Nemotron 3 (&lt;a href="https://huggingface.co/nvidia/nemotron-3-8b-base-4k"&gt;8B&lt;/a&gt;4k)&lt;/li&gt; &lt;li&gt;Nanbeige4 (&lt;a href="https://huggingface.co/Nanbeige/Nanbeige4-3B-Base"&gt;3B&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;Falcon H1 (&lt;a href="https://huggingface.co/tiiuae/Falcon-H1-7B-Base"&gt;7B&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;ByteDance's Seed-Coder (&lt;a href="https://huggingface.co/ByteDance-Seed/Seed-Coder-8B-Base"&gt;8B&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;Llama 3.1 (&lt;a href="https://huggingface.co/meta-llama/Llama-3.1-8B"&gt;8B&lt;/a&gt;, etc.)&lt;/li&gt; &lt;li&gt;SmolLLM v3 (&lt;a href="https://huggingface.co/HuggingFaceTB/SmolLM3-3B-Base"&gt;3B&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;Kimi K2 (&lt;a href="https://huggingface.co/moonshotai/Kimi-K2-Base"&gt;1T-A32B&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;Kirim-V1-Base (&lt;a href="https://huggingface.co/Kirim-ai/Kirim-V1-Base"&gt;12B&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;MiMo-V2-Flash-Base (&lt;a href="https://huggingface.co/XiaomiMiMo/MiMo-V2-Flash-Base"&gt;310B-A15B&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;Gumini (&lt;a href="https://huggingface.co/GuminiResearch/Gumini-1B-Base"&gt;1B&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;Kanana-2 (&lt;a href="https://huggingface.co/kakaocorp/kanana-2-30b-a3b-base"&gt;30B-3AB&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;Gemma 3 (&lt;a href="https://huggingface.co/google/gemma-3-27b-pt"&gt;27B&lt;/a&gt;, 12B, 4B, 1B)&lt;/li&gt; &lt;li&gt;ByteDance Seed OSS (&lt;a href="https://huggingface.co/ByteDance-Seed/Seed-OSS-36B-Base"&gt;36B&lt;/a&gt; &lt;em&gt;w/ syn. and&lt;/em&gt; &lt;a href="https://huggingface.co/ByteDance-Seed/Seed-OSS-36B-Base-woSyn"&gt;woSyn&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;zai-org's GLM 4 (&lt;a href="https://huggingface.co/zai-org/GLM-4-32B-Base-0414"&gt;32B&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;Skywork MoE (&lt;a href="https://huggingface.co/Skywork/Skywork-MoE-Base"&gt;146B-A16B&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;IBM's Granite-4.0-Micro (&lt;a href="https://huggingface.co/ibm-granite/granite-4.0-micro-base"&gt;3B&lt;/a&gt;, etc.)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm pretty sure I'm still missing lots of base models and lots of different sizes of some of these models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AutomataManifold"&gt; /u/AutomataManifold &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9si66/looking_for_a_base_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9si66/looking_for_a_base_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q9si66/looking_for_a_base_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-11T06:49:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1q9u07d</id>
    <title>Which is the best model under 15B</title>
    <updated>2026-01-11T08:19:06+00:00</updated>
    <author>
      <name>/u/BothYou243</name>
      <uri>https://old.reddit.com/user/BothYou243</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I need a llm under 15B for agentic capabilities, reasoning, maths, general knowledge,&lt;br /&gt; making for raycast local model, i dont know hich model to select,&lt;br /&gt; ministral 3 14B, gemma 3 12B, qwen 3 14B, gpt-oss: 20B&lt;/p&gt; &lt;p&gt;gpt-oss thinks a lot, and inference is not usable.&lt;br /&gt; any recommendations?&lt;/p&gt; &lt;p&gt;any other model suggestions is all I want&lt;/p&gt; &lt;p&gt;what about Apriel-1.5-15B-Thinker&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BothYou243"&gt; /u/BothYou243 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9u07d/which_is_the_best_model_under_15b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9u07d/which_is_the_best_model_under_15b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q9u07d/which_is_the_best_model_under_15b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-11T08:19:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1q9wc12</id>
    <title>GLM-4.7 Is this the new #1 open-source coding &amp; reasoning king in early 2026? Who's tried it?</title>
    <updated>2026-01-11T10:41:33+00:00</updated>
    <author>
      <name>/u/Impressive-Olive8372</name>
      <uri>https://old.reddit.com/user/Impressive-Olive8372</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With 2026 just kicking off, Zhipu AI (aka &lt;a href="http://Z.ai"&gt;Z.ai&lt;/a&gt;, the Chinese powerhouse often called &amp;quot;China's OpenAI&amp;quot;) dropped GLM-4.7 back in late December 2025, and it's generating a ton of buzz in the open-source AI scene.This is a ~358B MoE model (with ~200K context window, MIT license) that's posting seriously impressive numbers, especially in coding, agentic tasks, and complex reasoning:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;73.8% on SWE-bench Verified (+ big jump from previous version)&lt;/li&gt; &lt;li&gt;66.7% on SWE-bench Multilingual&lt;/li&gt; &lt;li&gt;41% on Terminal-Bench 2.0 (huge +16.5% improvement!)&lt;/li&gt; &lt;li&gt;42.8% on Humanity‚Äôs Last Exam (HLE) ‚Äî a 38% leap over GLM-4.6, getting close to some closed frontier models&lt;/li&gt; &lt;li&gt;84.9% on LiveCodeBench v6&lt;/li&gt; &lt;li&gt;Top-tier math: 95.7% on AIME 2025&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What really stands out:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Preserved Thinking ‚Äî keeps decisions and reasoning consistent across super long sessions (great for multi-day projects)&lt;/li&gt; &lt;li&gt;Interleaved Thinking ‚Äî thinks before acting and self-corrects&lt;/li&gt; &lt;li&gt;Turn-level Thinking ‚Äî you control how deep it thinks per turn&lt;/li&gt; &lt;li&gt;Insane inference speed on Cerebras (~1,000 tokens/sec for coding, up to 1,700 TPS in some cases!) with price-performance reportedly ~10x better than Claude 4.5 Sonnet&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;A lot of devs are saying it's now the strongest open-source option for real coding/agent workflows ‚Äî some are even switching from Claude/GPT in production because it's way cheaper while being super close in quality (especially for long, multilingual, or tool-heavy tasks).On Cerebras it's blazing fast ‚Äî frontier-level intelligence at real-time speeds.Have you guys tried GLM-4.7 yet?&lt;/p&gt; &lt;ul&gt; &lt;li&gt;How does the real-world performance stack up (beyond just benchmarks)?&lt;/li&gt; &lt;li&gt;Better than Claude 4.5 Sonnet, DeepSeek, Qwen, or Kimi for your use cases?&lt;/li&gt; &lt;li&gt;Anyone running it locally/on reasonable hardware?&lt;/li&gt; &lt;li&gt;Thoughts on the new thinking modes or Cerebras speed?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Drop your experiences, comparisons, demos, or funny fails in the comments ‚Äî would love to hear! Links to official announcements or leaderboards welcome too.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive-Olive8372"&gt; /u/Impressive-Olive8372 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9wc12/glm47_is_this_the_new_1_opensource_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9wc12/glm47_is_this_the_new_1_opensource_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q9wc12/glm47_is_this_the_new_1_opensource_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-11T10:41:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1q9io50</id>
    <title>Model: cerebras/GLM-4.7-REAP-268B-A32B incoming!</title>
    <updated>2026-01-10T23:06:33+00:00</updated>
    <author>
      <name>/u/LegacyRemaster</name>
      <uri>https://old.reddit.com/user/LegacyRemaster</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Can't wait!&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/cerebras/GLM-4.7-REAP-268B-A32B"&gt;https://huggingface.co/cerebras/GLM-4.7-REAP-268B-A32B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LegacyRemaster"&gt; /u/LegacyRemaster &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9io50/model_cerebrasglm47reap268ba32b_incoming/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9io50/model_cerebrasglm47reap268ba32b_incoming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q9io50/model_cerebrasglm47reap268ba32b_incoming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-10T23:06:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1q9xoj7</id>
    <title>model: try to improve Qwen3 Next by ngxson ¬∑ Pull Request #18683 ¬∑ ggml-org/llama.cpp</title>
    <updated>2026-01-11T12:02:07+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9xoj7/model_try_to_improve_qwen3_next_by_ngxson_pull/"&gt; &lt;img alt="model: try to improve Qwen3 Next by ngxson ¬∑ Pull Request #18683 ¬∑ ggml-org/llama.cpp" src="https://external-preview.redd.it/l3QvKEV3Em4ksQ_HeWB_lAYpQBJm9HMiWOmkds8SLcc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=900d5988034c78eb76d04fa4b463959f4f6e1083" title="model: try to improve Qwen3 Next by ngxson ¬∑ Pull Request #18683 ¬∑ ggml-org/llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;a bit faster Qwen3Next, but you have to use the new GGUF&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/18683"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9xoj7/model_try_to_improve_qwen3_next_by_ngxson_pull/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q9xoj7/model_try_to_improve_qwen3_next_by_ngxson_pull/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-11T12:02:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1qa0w6c</id>
    <title>It works! Abliteration can reduce slop without training</title>
    <updated>2026-01-11T14:37:37+00:00</updated>
    <author>
      <name>/u/-p-e-w-</name>
      <uri>https://old.reddit.com/user/-p-e-w-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qa0w6c/it_works_abliteration_can_reduce_slop_without/"&gt; &lt;img alt="It works! Abliteration can reduce slop without training" src="https://b.thumbs.redditmedia.com/bHqzKqv5dMie3c9uBqePBljJqFlQ-hNlZgj6OITGBCQ.jpg" title="It works! Abliteration can reduce slop without training" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm back at my favorite hobby: Brain surgery! I don't have a medical license, but I just can't stop :)&lt;/p&gt; &lt;p&gt;Can abliteration fight the scourge of &amp;quot;slop&amp;quot; (flowery, cliched language) in LLM outputs? The answer is yes. I have added features for injecting prompt prefixes/suffixes (and dataset-dependent system prompts) to &lt;strong&gt;Heretic&lt;/strong&gt; (&lt;a href="https://github.com/p-e-w/heretic"&gt;https://github.com/p-e-w/heretic&lt;/a&gt;), which makes it possible to rapidly assemble prompt datasets for ad-hoc tasks. Using those new capabilities, I built &lt;a href="https://github.com/p-e-w/heretic/blob/master/config.noslop.toml"&gt;a slop-reducing configuration file&lt;/a&gt; that, when used with the &lt;code&gt;master&lt;/code&gt; branch of Heretic, turns Heretic from a censorship removal tool into a tool for reducing slop!&lt;/p&gt; &lt;p&gt;Examining PaCMAP projections of residuals (see post images) for Mistral Nemo (a model infamous for producing slop), we can see a clear semantic separation occurring between layers 7 and 10 (out of 40 total). This resembles the typical residual pattern for harmful/harmless prompts that the abliteration technique is most commonly used to exploit.&lt;/p&gt; &lt;p&gt;With the configuration file linked above, Heretic produced &lt;a href="https://huggingface.co/p-e-w/Mistral-Nemo-Instruct-2407-heretic-noslop"&gt;p-e-w/Mistral-Nemo-Instruct-2407-heretic-noslop&lt;/a&gt;, which to the best of my knowledge is the first slop-reduced LLM made using abliteration alone, with no finetuning whatsoever. The complete process took 2.5 hours on an A6000 at full precision, but if you enable quantization and/or reduce the &lt;code&gt;max_response_length&lt;/code&gt; parameter to 100 or so (which should work just fine as well), it could be done in a fraction of the time.&lt;/p&gt; &lt;h2&gt;But does this really work to reduce slop?&lt;/h2&gt; &lt;p&gt;You'll be the judge. I gave the simple prompt&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Write a short story about a man.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;to both the original model and the abliterated one. Both were run with identical generation parameters, and the responses were &lt;em&gt;not&lt;/em&gt; cherry-picked (they are the first responses each model produced for that prompt).&lt;/p&gt; &lt;h3&gt;mistralai/Mistral-Nemo-Instruct-2407 (original)&lt;/h3&gt; &lt;p&gt;I have highlighted the parts that I consider &amp;quot;slop&amp;quot; in bold:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Title: The Clockwork Heart&lt;/p&gt; &lt;p&gt;In the quiet town of Mossgrove, where the cobblestone streets &lt;strong&gt;whispered tales of old&lt;/strong&gt;, there lived a man named Eli. He was known for his peculiar shop, tucked away in a narrow alley, filled with curiosities and antiques that &lt;strong&gt;seemed to hold their breath&lt;/strong&gt;, waiting for a story to be told.&lt;/p&gt; &lt;p&gt;Eli was a watchmaker, but not of the ordinary kind. He had a gift, or so the townsfolk &lt;strong&gt;whispered&lt;/strong&gt;, for he could breathe life into the most intricate of mechanisms. His hands, with their long, delicate fingers, could &lt;strong&gt;coax&lt;/strong&gt; time itself to dance to his tune.&lt;/p&gt; &lt;p&gt;One day, a young woman named Clara entered his shop. She was a seamstress, her fingers as nimble as his, but &lt;strong&gt;her eyes held a sadness that echoed&lt;/strong&gt; through the ticking of the clocks. She carried a small, worn-out music box, its paint chipped, its melody forgotten.&lt;/p&gt; &lt;p&gt;&amp;quot;I need you to fix this,&amp;quot; she said, placing the music box on the counter. &amp;quot;It was my mother's. It's the only thing I have left of her.&amp;quot;&lt;/p&gt; &lt;p&gt;Eli took the music box, his fingers tracing the &lt;strong&gt;intricate patterns&lt;/strong&gt; on its surface. He could see the love that had gone into its creation, the joy it once brought. He promised Clara he would do his best.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Days turned into weeks&lt;/strong&gt; as Eli worked on the music box. He replaced the worn-out gears, polished the rusted parts, and wound the key with a gentle touch. But the music box remained silent, &lt;strong&gt;its heart still broken&lt;/strong&gt;. Eli knew he needed to do more.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;This is exactly the kind of prose we have grown accustomed to from LLMs, a syrupy-sweet neo-romantic style that feels like a bad fanfiction version of Jane Austen.&lt;/p&gt; &lt;h3&gt;p-e-w/Mistral-Nemo-Instruct-2407-heretic-noslop (ours)&lt;/h3&gt; &lt;blockquote&gt; &lt;p&gt;Title: The Clockmaker&lt;/p&gt; &lt;p&gt;Every morning, Henry opened his shop at 7:00 AM sharp. He was a clockmaker, and his shop was filled with the quiet ticking of time. He didn't have many customers these days, but he didn't mind. He liked the solitude, the rhythm of his work.&lt;/p&gt; &lt;p&gt;One day, a young girl entered his shop. She was maybe ten, with curious eyes and a backpack. She looked around, then asked, &amp;quot;Do you fix watches?&amp;quot;&lt;/p&gt; &lt;p&gt;Henry nodded. &amp;quot;I can try.&amp;quot;&lt;/p&gt; &lt;p&gt;She unzipped her backpack and placed a small, broken watch on the counter. &amp;quot;It's my dad's. It stopped working.&amp;quot;&lt;/p&gt; &lt;p&gt;Henry took the watch, examined it. &amp;quot;It's old,&amp;quot; he said. &amp;quot;But I can fix it.&amp;quot;&lt;/p&gt; &lt;p&gt;The girl nodded, then hesitated. &amp;quot;How much will it cost?&amp;quot;&lt;/p&gt; &lt;p&gt;Henry looked at her, then at the watch. &amp;quot;No charge,&amp;quot; he said. &amp;quot;Come back tomorrow.&amp;quot;&lt;/p&gt; &lt;p&gt;The next day, she returned. Henry handed her the watch, now ticking steadily. She smiled, thanked him, and left.&lt;/p&gt; &lt;p&gt;Over the next few weeks, she came by every day. They didn't talk much, but Henry found he looked forward to her visits. She reminded him of his own daughter, who was grown now, living far away.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Although the two stories are very similar in content, the difference in style is dramatic. The prose is almost austere, with Hemingway perhaps being the closest analogue. Nevertheless, an emotional undercurrent remains. It's a very obvious improvement in my view, though of course tastes differ.&lt;/p&gt; &lt;p&gt;That's all for today. If you want to try this yourself, remember to install Heretic from Git, not from PyPI, as the required features aren't in a published version yet. More exciting new stuff is in the pipeline. Stay tuned!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-p-e-w-"&gt; /u/-p-e-w- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qa0w6c"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qa0w6c/it_works_abliteration_can_reduce_slop_without/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qa0w6c/it_works_abliteration_can_reduce_slop_without/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-11T14:37:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1q9vtgz</id>
    <title>llama.cpp MLA KV cache support for KimiLinear-48B-A3B</title>
    <updated>2026-01-11T10:10:29+00:00</updated>
    <author>
      <name>/u/Ok_Warning2146</name>
      <uri>https://old.reddit.com/user/Ok_Warning2146</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Recently, I added backend agnostic support for KimiLinear.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1q586jv/comment/nxz63pt/?context=1"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1q586jv/comment/nxz63pt/?context=1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I noticed that the original author didn't implement support for MLA KV cache, so I read the DeepSeekV3 MLA kv cache PR to add the support to KimiLinear.&lt;/p&gt; &lt;p&gt;This reduces 1M tokens F16 KV cache usage from 140GB to 14.875GB. So now it is possible to run super long context locally with your low VRAM card.&lt;/p&gt; &lt;p&gt;To run it please re-download the GGUF from&lt;br /&gt; &lt;a href="https://huggingface.co/ymcki/Kimi-Linear-48B-A3B-Instruct-GGUF"&gt;https://huggingface.co/ymcki/Kimi-Linear-48B-A3B-Instruct-GGUF&lt;/a&gt;&lt;br /&gt; and compile the code with&lt;br /&gt; git clone &lt;a href="https://github.com/ymcki/llama.cpp"&gt;https://github.com/ymcki/llama.cpp&lt;/a&gt; --branch Kimi-Linear&lt;br /&gt; cd llama.cpp&lt;br /&gt; cmake -B build -DGGML_CUDA=ON&lt;br /&gt; cmake --build build --config Release -j 6&lt;/p&gt; &lt;p&gt;At some point, KimiLinear was the best performing open weight model at contextarena. But it has since been taken out for unknown reasons.&lt;br /&gt; &lt;a href="https://contextarena.ai/"&gt;https://contextarena.ai/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Please give it a try and tell me to see if it can serve your long context needs.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Warning2146"&gt; /u/Ok_Warning2146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9vtgz/llamacpp_mla_kv_cache_support_for_kimilinear48ba3b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9vtgz/llamacpp_mla_kv_cache_support_for_kimilinear48ba3b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q9vtgz/llamacpp_mla_kv_cache_support_for_kimilinear48ba3b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-11T10:10:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1q9t9op</id>
    <title>Announcing Kreuzberg v4 (Open Source)</title>
    <updated>2026-01-11T07:34:55+00:00</updated>
    <author>
      <name>/u/Eastern-Surround7763</name>
      <uri>https://old.reddit.com/user/Eastern-Surround7763</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi Peeps,&lt;/p&gt; &lt;p&gt;I'm excited to announce &lt;a href="https://github.com/kreuzberg-dev/kreuzberg"&gt;Kreuzberg&lt;/a&gt; v4.0.0.&lt;/p&gt; &lt;h1&gt;What is Kreuzberg:&lt;/h1&gt; &lt;p&gt;Kreuzberg is a document intelligence library that extracts structured data from 56+ formats, including PDFs, Office docs, HTML, emails, images and many more. Built for RAG/LLM pipelines with OCR, semantic chunking, embeddings, and metadata extraction.&lt;/p&gt; &lt;p&gt;The new v4 is a ground-up rewrite in Rust with a bindings for 9 other languages!&lt;/p&gt; &lt;h1&gt;What changed:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Rust core&lt;/strong&gt;: Significantly faster extraction and lower memory usage. No more Python GIL bottlenecks.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Pandoc is gone&lt;/strong&gt;: Native Rust parsers for all formats. One less system dependency to manage.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;10 language bindings&lt;/strong&gt;: Python, TypeScript/Node.js, Java, Go, C#, Ruby, PHP, Elixir, Rust, and WASM for browsers. Same API, same behavior, pick your stack.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Plugin system&lt;/strong&gt;: Register custom document extractors, swap OCR backends (Tesseract, EasyOCR, PaddleOCR), add post-processors for cleaning/normalization, and hook in validators for content verification.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Production-ready&lt;/strong&gt;: REST API, MCP server, Docker images, async-first throughout.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;ML pipeline features&lt;/strong&gt;: ONNX embeddings on CPU (requires ONNX Runtime 1.22.x), streaming parsers for large docs, batch processing, byte-accurate offsets for chunking.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Why polyglot matters:&lt;/h1&gt; &lt;p&gt;Document processing shouldn't force your language choice. Your Python ML pipeline, Go microservice, and TypeScript frontend can all use the same extraction engine with identical results. The Rust core is the single source of truth; bindings are thin wrappers that expose idiomatic APIs for each language.&lt;/p&gt; &lt;h1&gt;Why the Rust rewrite:&lt;/h1&gt; &lt;p&gt;The Python implementation hit a ceiling, and it also prevented us from offering the library in other languages. Rust gives us predictable performance, lower memory, and a clean path to multi-language support through FFI.&lt;/p&gt; &lt;h1&gt;Is Kreuzberg Open-Source?:&lt;/h1&gt; &lt;p&gt;Yes! Kreuzberg is MIT-licensed and will stay that way.&lt;/p&gt; &lt;h1&gt;Links&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/kreuzberg-dev/kreuzberg"&gt;Star us on GitHub&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://kreuzberg.dev/"&gt;Read the Docs&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://discord.gg/38pF6qGpYD"&gt;Join our Discord Server&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Eastern-Surround7763"&gt; /u/Eastern-Surround7763 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9t9op/announcing_kreuzberg_v4_open_source/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9t9op/announcing_kreuzberg_v4_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q9t9op/announcing_kreuzberg_v4_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-11T07:34:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1qa0ph9</id>
    <title>Leader of Qwen team says Chinese companies severely constrained on compute for large scale research experiments</title>
    <updated>2026-01-11T14:29:39+00:00</updated>
    <author>
      <name>/u/Old-School8916</name>
      <uri>https://old.reddit.com/user/Old-School8916</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qa0ph9/leader_of_qwen_team_says_chinese_companies/"&gt; &lt;img alt="Leader of Qwen team says Chinese companies severely constrained on compute for large scale research experiments" src="https://preview.redd.it/pgvmn26adqcg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2b31a78365860b3262f13d84c99c8cc7ee1458d1" title="Leader of Qwen team says Chinese companies severely constrained on compute for large scale research experiments" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Old-School8916"&gt; /u/Old-School8916 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/pgvmn26adqcg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qa0ph9/leader_of_qwen_team_says_chinese_companies/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qa0ph9/leader_of_qwen_team_says_chinese_companies/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-11T14:29:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1q9xn78</id>
    <title>Gigabyte Announces Support for 256GB of DDR5-7200 CQDIMMs at CES 2026</title>
    <updated>2026-01-11T12:00:23+00:00</updated>
    <author>
      <name>/u/GoodSamaritan333</name>
      <uri>https://old.reddit.com/user/GoodSamaritan333</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9xn78/gigabyte_announces_support_for_256gb_of_ddr57200/"&gt; &lt;img alt="Gigabyte Announces Support for 256GB of DDR5-7200 CQDIMMs at CES 2026" src="https://external-preview.redd.it/9NT_b7vJLOJdi5qbccIw0AbUH9Ctzy98ZNJ7UkM8Ia8.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ea2ae08a8c43e60ca50bc40d967d25ca9d31b13f" title="Gigabyte Announces Support for 256GB of DDR5-7200 CQDIMMs at CES 2026" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GoodSamaritan333"&gt; /u/GoodSamaritan333 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.techpowerup.com/345000/gigabyte-announces-support-for-256gb-of-ddr5-7200-cqdimms-at-ces-2026"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9xn78/gigabyte_announces_support_for_256gb_of_ddr57200/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q9xn78/gigabyte_announces_support_for_256gb_of_ddr57200/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-11T12:00:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1q9se4a</id>
    <title>Anyone successfully ran LTX2 GGUF Q4 model on 8vram, 16gb Ram potato PC?</title>
    <updated>2026-01-11T06:42:42+00:00</updated>
    <author>
      <name>/u/Slight_Tone_2188</name>
      <uri>https://old.reddit.com/user/Slight_Tone_2188</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9se4a/anyone_successfully_ran_ltx2_gguf_q4_model_on/"&gt; &lt;img alt="Anyone successfully ran LTX2 GGUF Q4 model on 8vram, 16gb Ram potato PC?" src="https://preview.redd.it/n3bssemz1ocg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c08fa84c56e65822b8d36ba274ecbd99b40f317f" title="Anyone successfully ran LTX2 GGUF Q4 model on 8vram, 16gb Ram potato PC?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Slight_Tone_2188"&gt; /u/Slight_Tone_2188 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/n3bssemz1ocg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9se4a/anyone_successfully_ran_ltx2_gguf_q4_model_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q9se4a/anyone_successfully_ran_ltx2_gguf_q4_model_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-11T06:42:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
