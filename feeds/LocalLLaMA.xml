<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-07T18:52:22+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI. Hey, Tony</subtitle>
  <entry>
    <id>t3_1o0hpfw</id>
    <title>3090 + 128GB DDR4 worth it?</title>
    <updated>2025-10-07T15:19:17+00:00</updated>
    <author>
      <name>/u/randomsolutions1</name>
      <uri>https://old.reddit.com/user/randomsolutions1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have an RTX 3090 with 16GB of DDR4. I was wondering if I should upgrade to 128GB of DDR4? Or is it not worthwhile and I need to get a DDR5 motherboard + RAM? Will I see a massive difference between them?&lt;/p&gt; &lt;p&gt;What models will 128GB RAM open up for me if I do the upgrade?&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/randomsolutions1"&gt; /u/randomsolutions1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0hpfw/3090_128gb_ddr4_worth_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0hpfw/3090_128gb_ddr4_worth_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o0hpfw/3090_128gb_ddr4_worth_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T15:19:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1o065ig</id>
    <title>AudioBook Maker with Ebook Editor Using Chatterbox TTS</title>
    <updated>2025-10-07T05:23:16+00:00</updated>
    <author>
      <name>/u/Devajyoti1231</name>
      <uri>https://old.reddit.com/user/Devajyoti1231</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Desktop application to create Full Audiobooks from ebook(epub/text) , chapterwise audio for the ebook etc using chatterbox tts and Easy Ebook Editor to Edit ebooks, export chapters from it, import chapters, create new ebook, edit metadata etc&lt;/p&gt; &lt;p&gt;Other options are-&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Direct Local TTS&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Remote API Support with tts-webui (&lt;/strong&gt;&lt;a href="https://github.com/rsxdalv/TTS-WebUI"&gt;&lt;strong&gt;https://github.com/rsxdalv/TTS-WebUI&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Multiple Input Formats&lt;/strong&gt; - TXT, PDF, EPUB support&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Voice Management&lt;/strong&gt; - Easy voice reference handling&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Advanced Settings&lt;/strong&gt; - Full control over TTS parameters&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Preset System&lt;/strong&gt; - Save and load your favorite settings&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Audio Player&lt;/strong&gt; - Preview generated audio instantly&lt;/p&gt; &lt;p&gt;Github link - &lt;a href="https://github.com/D3voz/audiobook-maker-pro"&gt;https://github.com/D3voz/audiobook-maker-pro&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Full 33 min long one chapter sample from final empire - &lt;a href="https://screenapp.io/app/#/shared/JQh3r66YZw"&gt;https://screenapp.io/app/#/shared/JQh3r66YZw&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Performance Comparison (NVIDIA 4060 Ti):&lt;/h1&gt; &lt;p&gt;-Local Mode Speed: ~37 iterations/sec&lt;/p&gt; &lt;p&gt;-API Mode Speed(using tts-webui) : ~80+ iterations/sec (over 2x faster)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Devajyoti1231"&gt; /u/Devajyoti1231 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o065ig/audiobook_maker_with_ebook_editor_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o065ig/audiobook_maker_with_ebook_editor_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o065ig/audiobook_maker_with_ebook_editor_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T05:23:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1nzl8y5</id>
    <title>How Transformers avoids becoming a black box, even at 1M+ LOC</title>
    <updated>2025-10-06T14:53:04+00:00</updated>
    <author>
      <name>/u/El_Olbap</name>
      <uri>https://old.reddit.com/user/El_Olbap</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I'm Pablo from Hugging Face Open-Source team. We just wrote a software-engineering focused deep dive on how we keep the `transformers` library hackable/maintainable while it keeps growing and growing. If you're running models locally, fine-tuning on your own hardware, or just want to understand the code you're using, I recommend the read!&lt;/p&gt; &lt;p&gt;Light spoilers about what's in it:&lt;/p&gt; &lt;p&gt;- ****One Model, One File:**** You can still read a `modeling_*.py` top-to-bottom and see exactly what's happening.&lt;/p&gt; &lt;p&gt;- ****Modular Transformers:**** This is our trick to fight code bloat. Contributors can reuse code via a small `modular_*.py` file, but we auto-generate the full, readable modeling file so you never lose the &amp;quot;one file&amp;quot; experience. It cut our maintenance work by ~15x.&lt;/p&gt; &lt;p&gt;- ****Config-Driven Performance:**** Features like FlashAttention(and ofc 2,3..), tensor parallelism (`tp_plan`), and per-layer attention schedules are enabled in the config, not by changing the model code. A `Linear` layer is always just a `Linear` layer, you don't have to change it depending on how it's sliced.&lt;/p&gt; &lt;p&gt;- ****Tools for Local Use:**** This philosophy lets us build helpful tools. The post covers an attention visualizer, a model tracer for debugging ports, and faster CUDA warmups, and we also go over `transformers serve` usage. &lt;/p&gt; &lt;p&gt;Hope you enjoy the read!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/El_Olbap"&gt; /u/El_Olbap &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/spaces/transformers-community/Transformers-tenets"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzl8y5/how_transformers_avoids_becoming_a_black_box_even/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nzl8y5/how_transformers_avoids_becoming_a_black_box_even/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T14:53:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0lygb</id>
    <title>Is it possible to add new characters in Kokoro TTS?</title>
    <updated>2025-10-07T17:52:49+00:00</updated>
    <author>
      <name>/u/Muzamilkhan7</name>
      <uri>https://old.reddit.com/user/Muzamilkhan7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, I wanna know if there is way to add new characters in Kokoro Or there will be any future updates expected in this model? I have been using Kokoro for quite a while now. Although its voice are Good but not suitable for all type of narration. I have tried searching different tts models that are resource demanding, which I don't have.I am running kokoro on cpu only at the moment. If you know something very similar in the same range. Please share I would appreciate that. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Muzamilkhan7"&gt; /u/Muzamilkhan7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0lygb/is_it_possible_to_add_new_characters_in_kokoro_tts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0lygb/is_it_possible_to_add_new_characters_in_kokoro_tts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o0lygb/is_it_possible_to_add_new_characters_in_kokoro_tts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T17:52:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0m4vh</id>
    <title>Minimum specs to fine-tune 27b parameter model</title>
    <updated>2025-10-07T17:59:16+00:00</updated>
    <author>
      <name>/u/kalyankd03</name>
      <uri>https://old.reddit.com/user/kalyankd03</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi.. in new to running local LLMs . I have 5070ti and I have successfully finetuned 3b parameter model. I want to know minimum gpu specs required to perform some fine-tuning 27b parameter model on gpu to see if I can afford it (with and without quantization)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kalyankd03"&gt; /u/kalyankd03 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0m4vh/minimum_specs_to_finetune_27b_parameter_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0m4vh/minimum_specs_to_finetune_27b_parameter_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o0m4vh/minimum_specs_to_finetune_27b_parameter_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T17:59:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0me3x</id>
    <title>What needs to change to make LLMs more efficient?</title>
    <updated>2025-10-07T18:08:18+00:00</updated>
    <author>
      <name>/u/swagonflyyyy</name>
      <uri>https://old.reddit.com/user/swagonflyyyy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;LLMs are great in a lot of ways, and they are showing signs of improvement. &lt;/p&gt; &lt;p&gt;I also think they're incredibly inefficient when it comes to resource consumption because they use up &lt;em&gt;far&lt;/em&gt; too much of everything:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Too much heat generated.&lt;/li&gt; &lt;li&gt;Too much power consumed.&lt;/li&gt; &lt;li&gt;Too much storage space used up.&lt;/li&gt; &lt;li&gt;Too much RAM to fall back on.&lt;/li&gt; &lt;li&gt;Too much VRAM to load and run them.&lt;/li&gt; &lt;li&gt;Too many calculations when processing input.&lt;/li&gt; &lt;li&gt;Too much money to train them (mostly).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Most of these problems require solutions in the form of expensive hardware upgrades. Its a miracle we can even run them at all locally, and my hats off to those who can run decent-quality models on mobile. It almost feels like those room-sized computers many decades ago that used up that much space to run simple commands at a painstakingly slow pace.&lt;/p&gt; &lt;p&gt;There's just something about frontier models that, although they are a huge leap from what we had a few years ago, still feel like they use up a lot more resources than they should.&lt;/p&gt; &lt;p&gt;Do you think we might reach a watershed moment, like computers did with transistors, integrated circuits and microprocessors back then, that would make it exponentially cheaper to run the models locally? &lt;/p&gt; &lt;p&gt;Or are we reaching a wall with modern LLMs/LMMs that require a fundamentally different solution?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/swagonflyyyy"&gt; /u/swagonflyyyy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0me3x/what_needs_to_change_to_make_llms_more_efficient/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0me3x/what_needs_to_change_to_make_llms_more_efficient/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o0me3x/what_needs_to_change_to_make_llms_more_efficient/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T18:08:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0g9i1</id>
    <title>What are some good frontends to use on an android phone? (native app only and preferably FOSS)</title>
    <updated>2025-10-07T14:25:53+00:00</updated>
    <author>
      <name>/u/Striking_Wedding_461</name>
      <uri>https://old.reddit.com/user/Striking_Wedding_461</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm tired of PWA's they're buggy and you can just feel when something was designed to be used with a mouse and keyboard.&lt;br /&gt; Something you can use with both Local and OpenRoute/r API.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Striking_Wedding_461"&gt; /u/Striking_Wedding_461 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0g9i1/what_are_some_good_frontends_to_use_on_an_android/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0g9i1/what_are_some_good_frontends_to_use_on_an_android/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o0g9i1/what_are_some_good_frontends_to_use_on_an_android/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T14:25:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0mph2</id>
    <title>For MAC LLM Prompt processing speeds Gemma 3 seems like an ideal LLM</title>
    <updated>2025-10-07T18:19:14+00:00</updated>
    <author>
      <name>/u/supermazdoor</name>
      <uri>https://old.reddit.com/user/supermazdoor</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been looking for solutions on this issue for a while now with MAC, MLX and unified memory. The prompt processing speed. It is like everyone one else says; simply put, not practical for turn based conversations. &lt;/p&gt; &lt;p&gt;What you see instantly with checkpoints like QWEN3 30B INS in 8bit or 4bit MLX quants is instant speed token generation, but as the conversation grows the prompt processing times are significant. For example on a 100K context window the Qwen 3 MOE A3B 30B takes about 3-5 minutes of processing time depending on your context type. And that is a LOT and not practical.&lt;/p&gt; &lt;p&gt;So enter GEMMA 3 12B GGUF (llama.cpp) Q8. I've tested this model (Not MLX) and noticed that although its tokens per second might not be a match with the MLX variant, it makes up a whole lot more with prompt processing times. &lt;/p&gt; &lt;p&gt;My test using this model with &amp;quot;flash attention (experimental)&amp;quot; on on LM studio on a 100K context window has been stellar. Initial prompt processing 1-3 minutes and subsequent prompts take about 15-30 seconds roughly the same amount of time the GEMINI 2.5 flash takes to process. &lt;/p&gt; &lt;p&gt;This tells me that enterprise grade prompt processing times on MAC is not just possible, but its already here and proven in a model as dense as 12B which is vision capable and surprisingly the solution seems to be the llama.cpp framework and not MLX.&lt;/p&gt; &lt;p&gt;I've tried other gguf quants with other models with flash attention, none gave me the same results as this one. If someone with actual technical understanding can understand what makes this particular 12B architecture almost instant, then I truly see MACs competing with Nvidia in daily use cases. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/supermazdoor"&gt; /u/supermazdoor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0mph2/for_mac_llm_prompt_processing_speeds_gemma_3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0mph2/for_mac_llm_prompt_processing_speeds_gemma_3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o0mph2/for_mac_llm_prompt_processing_speeds_gemma_3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T18:19:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1o065kb</id>
    <title>Can you recommend a course for my youngster?</title>
    <updated>2025-10-07T05:23:21+00:00</updated>
    <author>
      <name>/u/pleok</name>
      <uri>https://old.reddit.com/user/pleok</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a 13-year-old whose school rules do not allow kids to pass off AI work as their own, which I generally support. Whether my kids starts using AI now or later, I know it's going to be ubiquitous tech throughout my kid's formative years, so I am thinking of a positive way my family can dispell some of the mystique, learn about it, and take advantage of the tech while keeping our eyes out for potential dangers. I feel my kid should know a little about what an LLm is comprised of and how it works. To that end, I am looking for an online course on how to build and train your own LLM from scratch, would be appropriate for tech savvy kids, requires little to no programming skills (or just basic programming skills that can be learned along the way), and whose goals would be to teach the &amp;quot;basics&amp;quot; of how an LLM works by having the student follow along and build/train their own with ollama or whatever. While I am not a complete novice when it comes to LLMs, I have never built/trained my own models. For my kid's setup, we could use a Lenovo gaming laptop i9, 32 gb ram, Nvidia geforce rtx4070, 8 gb vram. Not good for big models but maybe enough for the basics (?). I suppose we could just buy the compute power, but I think having a local model residing on our own machine would be cooler and provide some good learning opportunities. Heck, I might even join my kid in the course. Any suggestions for an online course (free or paid)? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pleok"&gt; /u/pleok &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o065kb/can_you_recommend_a_course_for_my_youngster/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o065kb/can_you_recommend_a_course_for_my_youngster/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o065kb/can_you_recommend_a_course_for_my_youngster/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T05:23:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1nzn1mk</id>
    <title>Running GPT-OSS (OpenAI) Exclusively on AMD Ryzen™ AI NPU</title>
    <updated>2025-10-06T15:58:14+00:00</updated>
    <author>
      <name>/u/BandEnvironmental834</name>
      <uri>https://old.reddit.com/user/BandEnvironmental834</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzn1mk/running_gptoss_openai_exclusively_on_amd_ryzen_ai/"&gt; &lt;img alt="Running GPT-OSS (OpenAI) Exclusively on AMD Ryzen™ AI NPU" src="https://external-preview.redd.it/_S9eclPc4WRscWHOsVO80UpXnpu4dfbG_wCYpnVLuPA.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d53f3fcd941a63a8ecb5a50a6c26e1cf55db3e1a" title="Running GPT-OSS (OpenAI) Exclusively on AMD Ryzen™ AI NPU" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We’re a small team building &lt;strong&gt;FastFlowLM (FLM)&lt;/strong&gt; — a fast runtime for running &lt;strong&gt;GPT-OSS (first MoE on NPUs), Gemma3 (vision), Medgemma,&lt;/strong&gt; &lt;strong&gt;Qwen3,&lt;/strong&gt; &lt;strong&gt;DeepSeek-R1&lt;/strong&gt;, &lt;strong&gt;LLaMA3.x,&lt;/strong&gt; and others &lt;strong&gt;entirely on the AMD Ryzen AI NPU&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Think &lt;strong&gt;Ollama&lt;/strong&gt;, but deeply optimized for AMD NPUs — with both &lt;strong&gt;CLI&lt;/strong&gt; and &lt;strong&gt;Server Mode (OpenAI-compatible)&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;✨ &lt;strong&gt;From Idle Silicon to Instant Power — FastFlowLM (FLM) Makes Ryzen™ AI Shine.&lt;/strong&gt;&lt;/p&gt; &lt;h1&gt;Key Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;No GPU fallback&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Faster and over 10× more power efficient.&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Supports context lengths up to 256k tokens (qwen3:4b-2507).&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ultra-Lightweight (14 MB). Installs within 20 seconds.&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Try It Out&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/FastFlowLM/FastFlowLM"&gt;github.com/FastFlowLM/FastFlowLM&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Live Demo → Remote machine access on the repo page&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;YouTube Demos:&lt;/strong&gt; &lt;a href="https://www.youtube.com/@FastFlowLM-YT/playlists"&gt;FastFlowLM - YouTube&lt;/a&gt; &lt;em&gt;→ Quick start guide, NPU vs CPU vs GPU, etc.&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We’re iterating fast and would &lt;strong&gt;love your feedback, critiques, and ideas&lt;/strong&gt;🙏&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BandEnvironmental834"&gt; /u/BandEnvironmental834 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/ksYyiUQvYfo?si=zfBjb7U86P947OYW"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzn1mk/running_gptoss_openai_exclusively_on_amd_ryzen_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nzn1mk/running_gptoss_openai_exclusively_on_amd_ryzen_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T15:58:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0a437</id>
    <title>Human or LLM? - Guess the human-written sentence</title>
    <updated>2025-10-07T09:34:53+00:00</updated>
    <author>
      <name>/u/n00bi3s</name>
      <uri>https://old.reddit.com/user/n00bi3s</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How many times can you find the human written texts?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/n00bi3s"&gt; /u/n00bi3s &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ai-or-human.com/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0a437/human_or_llm_guess_the_humanwritten_sentence/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o0a437/human_or_llm_guess_the_humanwritten_sentence/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T09:34:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0fjpa</id>
    <title>Top performing models across 4 professions covered by APEX</title>
    <updated>2025-10-07T13:59:21+00:00</updated>
    <author>
      <name>/u/RaselMahadi</name>
      <uri>https://old.reddit.com/user/RaselMahadi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0fjpa/top_performing_models_across_4_professions/"&gt; &lt;img alt="Top performing models across 4 professions covered by APEX" src="https://preview.redd.it/tt20ohtd4ptf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=44c9eea4438984a3bfb8f2c4c0087eeb1920ef40" title="Top performing models across 4 professions covered by APEX" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RaselMahadi"&gt; /u/RaselMahadi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/tt20ohtd4ptf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0fjpa/top_performing_models_across_4_professions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o0fjpa/top_performing_models_across_4_professions/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T13:59:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1o04s7y</id>
    <title>2 things we never forget, our first GPU and when your first GPU dies</title>
    <updated>2025-10-07T04:07:24+00:00</updated>
    <author>
      <name>/u/segmond</name>
      <uri>https://old.reddit.com/user/segmond</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just had a 3090 die, maybe I will resurrect it, maybe not. It comes with the territory of buying used GPUs from miners.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/segmond"&gt; /u/segmond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o04s7y/2_things_we_never_forget_our_first_gpu_and_when/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o04s7y/2_things_we_never_forget_our_first_gpu_and_when/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o04s7y/2_things_we_never_forget_our_first_gpu_and_when/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T04:07:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1o00ban</id>
    <title>Open Source Alternative to Perplexity</title>
    <updated>2025-10-07T00:31:00+00:00</updated>
    <author>
      <name>/u/Uiqueblhats</name>
      <uri>https://old.reddit.com/user/Uiqueblhats</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those of you who aren't familiar with SurfSense, it aims to be the &lt;strong&gt;open-source alternative to NotebookLM, Perplexity, or Glean.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;In short, it's a Highly Customizable AI Research Agent that connects to your personal external sources and Search Engines (Tavily, LinkUp), Slack, Linear, Jira, ClickUp, Confluence, Gmail, Notion, YouTube, GitHub, Discord, Airtable, Google Calendar and more to come.&lt;/p&gt; &lt;p&gt;I'm looking for contributors to help shape the future of SurfSense! If you're interested in AI agents, RAG, browser extensions, or building open-source research tools, this is a great place to jump in.&lt;/p&gt; &lt;p&gt;Here’s a quick look at what SurfSense offers right now:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Supports 100+ LLMs&lt;/li&gt; &lt;li&gt;Supports local Ollama or vLLM setups&lt;/li&gt; &lt;li&gt;6000+ Embedding Models&lt;/li&gt; &lt;li&gt;50+ File extensions supported (Added Docling recently)&lt;/li&gt; &lt;li&gt;Podcasts support with local TTS providers (Kokoro TTS)&lt;/li&gt; &lt;li&gt;Connects with 15+ external sources such as Search Engines, Slack, Notion, Gmail, Notion, Confluence etc&lt;/li&gt; &lt;li&gt;Cross-Browser Extension to let you save any dynamic webpage you want, including authenticated content.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Upcoming Planned Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Mergeable MindMaps.&lt;/li&gt; &lt;li&gt;Note Management&lt;/li&gt; &lt;li&gt;Multi Collaborative Notebooks.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Interested in contributing?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;SurfSense is completely open source, with an active roadmap. Whether you want to pick up an existing feature, suggest something new, fix bugs, or help improve docs, you're welcome to join in.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/MODSetter/SurfSense"&gt;https://github.com/MODSetter/SurfSense&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Uiqueblhats"&gt; /u/Uiqueblhats &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o00ban/open_source_alternative_to_perplexity/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o00ban/open_source_alternative_to_perplexity/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o00ban/open_source_alternative_to_perplexity/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T00:31:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0kf1o</id>
    <title>Best ways to run Qwen3 on CPU with 16 GB RAM</title>
    <updated>2025-10-07T16:58:23+00:00</updated>
    <author>
      <name>/u/Remarkable_Story_310</name>
      <uri>https://old.reddit.com/user/Remarkable_Story_310</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Any further technique than Quantization?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Remarkable_Story_310"&gt; /u/Remarkable_Story_310 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0kf1o/best_ways_to_run_qwen3_on_cpu_with_16_gb_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0kf1o/best_ways_to_run_qwen3_on_cpu_with_16_gb_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o0kf1o/best_ways_to_run_qwen3_on_cpu_with_16_gb_ram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T16:58:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1o08igx</id>
    <title>Improved "time to first token" in LM Studio</title>
    <updated>2025-10-07T07:51:05+00:00</updated>
    <author>
      <name>/u/waescher</name>
      <uri>https://old.reddit.com/user/waescher</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o08igx/improved_time_to_first_token_in_lm_studio/"&gt; &lt;img alt="Improved &amp;quot;time to first token&amp;quot; in LM Studio" src="https://preview.redd.it/m2ttxrud9ntf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ca5e7ac12bb3b1413f0d820c13cc0f0b9bd9d1b5" title="Improved &amp;quot;time to first token&amp;quot; in LM Studio" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was benching some of my models on my M4 Max 128GB a few days ago, see the attached image.&lt;/p&gt; &lt;p&gt;Today I noticed an update of the MLX runtime in LM Studio:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;MLX version info: - mlx-engine==6a8485b - mlx==0.29.1 - mlx-lm==0.28.1 - mlx-vlm==0.3.3 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;With this, &amp;quot;time to first token&amp;quot; has been improved dramatically. As an example:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Qwen3-Next:80b&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;4 bit MLX&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;// 80k context window + 36k token prompt length Time to first token: 47 ➔ 46 seconds :| // 120k context window + 97k token prompt length Time to first token: 406 ➔ 178 seconds &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Qwen3-Next:80b&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;6 bit MLX&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;// 80k context window + 36k token prompt length Time to first token: 140 ➔ 48 seconds // 120k context window + 97k token prompt length Time to first token: 436 ➔ 190 seconds &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Can anyone confirm?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/waescher"&gt; /u/waescher &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/m2ttxrud9ntf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o08igx/improved_time_to_first_token_in_lm_studio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o08igx/improved_time_to_first_token_in_lm_studio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T07:51:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0n17o</id>
    <title>2 month MiniPC mini-review: Minisforum AI X1 Pro (AMD HX 370)</title>
    <updated>2025-10-07T18:30:41+00:00</updated>
    <author>
      <name>/u/ivoras</name>
      <uri>https://old.reddit.com/user/ivoras</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0n17o/2_month_minipc_minireview_minisforum_ai_x1_pro/"&gt; &lt;img alt="2 month MiniPC mini-review: Minisforum AI X1 Pro (AMD HX 370)" src="https://external-preview.redd.it/9caqHyt_1L7Xsp4TpYFjM4oPMVmVIW9sJF7IMxoOn8I.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a78dfd017145cb54d5d1bf3ff16da49935fcf92a" title="2 month MiniPC mini-review: Minisforum AI X1 Pro (AMD HX 370)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;tl;dr: it's the AI Max 395+'s little brother. Half the price, but not a serious AI workstation.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ivoras"&gt; /u/ivoras &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ivoras.substack.com/p/2-month-minipc-mini-review-minisforum"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0n17o/2_month_minipc_minireview_minisforum_ai_x1_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o0n17o/2_month_minipc_minireview_minisforum_ai_x1_pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T18:30:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0kwx3</id>
    <title>Granite 4.0 on iGPU AMD Ryzen 6800H llama.cpp benchmark</title>
    <updated>2025-10-07T17:15:53+00:00</updated>
    <author>
      <name>/u/tabletuser_blogspot</name>
      <uri>https://old.reddit.com/user/tabletuser_blogspot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;New MoE model for testing:&lt;/p&gt; &lt;p&gt;Granite-4.0-H-Small is a 32B parameter, 9B active and long-context instruct model &lt;a href="https://huggingface.co/unsloth/granite-4.0-h-small-GGUF"&gt;unsloth&lt;/a&gt;&lt;/p&gt; &lt;p&gt;System: Kubuntu 25.10 OS, Kernel 6.17.0-5-generic with 64GB DDR5 ram. AMD Radeon Graphics (RADV REMBRANDT) Ryzen 6800H and 680M iGPU&lt;br /&gt; Llama.cpp Vulkan build: ca71fb9b (&lt;a href="https://github.com/ggml-org/llama.cpp/releases/tag/b6692"&gt;6692&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;granite-4.0-h-small-UD-Q8_K_XL.gguf&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;granitehybrid ?B Q8_0&lt;/td&gt; &lt;td align="left"&gt;35.47 GiB&lt;/td&gt; &lt;td align="left"&gt;32.21 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;72.56 ± 0.79&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;granitehybrid ?B Q8_0&lt;/td&gt; &lt;td align="left"&gt;35.47 GiB&lt;/td&gt; &lt;td align="left"&gt;32.21 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;4.26 ± 0.49&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;granite-4.0-h-small-UD-Q6_K_XL.gguf&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;granitehybrid ?B Q6_K&lt;/td&gt; &lt;td align="left"&gt;25.95 GiB&lt;/td&gt; &lt;td align="left"&gt;32.21 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;54.77 ± 1.87&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;granitehybrid ?B Q6_K&lt;/td&gt; &lt;td align="left"&gt;25.95 GiB&lt;/td&gt; &lt;td align="left"&gt;32.21 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;5.51 ± 0.49&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;granite-4.0-h-small-UD-Q5_K_XL.gguf&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;granitehybrid ?B Q5_K - Medium&lt;/td&gt; &lt;td align="left"&gt;21.53 GiB&lt;/td&gt; &lt;td align="left"&gt;32.21 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;57.90 ± 4.46&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;granitehybrid ?B Q5_K - Medium&lt;/td&gt; &lt;td align="left"&gt;21.53 GiB&lt;/td&gt; &lt;td align="left"&gt;32.21 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;6.36 ± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;granite-4.0-h-small-UD-Q4_K_XL.gguf&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;granitehybrid ?B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;17.49 GiB&lt;/td&gt; &lt;td align="left"&gt;32.21 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;57.26 ± 2.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;granitehybrid ?B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;17.49 GiB&lt;/td&gt; &lt;td align="left"&gt;32.21 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;7.21 ± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;granite-4.0-h-small-IQ4_XS.gguf&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;granitehybrid ?B IQ4_XS - 4.25 bpw&lt;/td&gt; &lt;td align="left"&gt;16.23 GiB&lt;/td&gt; &lt;td align="left"&gt;32.21 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;57.31 ± 2.65&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;granitehybrid ?B IQ4_XS - 4.25 bpw&lt;/td&gt; &lt;td align="left"&gt;16.23 GiB&lt;/td&gt; &lt;td align="left"&gt;32.21 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;7.17 ± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Add this for comparison:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;t/s (pp512)&lt;/th&gt; &lt;th align="left"&gt;t/s (tg128)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q4_K&lt;/td&gt; &lt;td align="left"&gt;17.28&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;134.46 ± 0.45&lt;/td&gt; &lt;td align="left"&gt;28.26 ± 0.46&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Simplified view:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;t/s (pp512)&lt;/th&gt; &lt;th align="left"&gt;t/s (tg128)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;granitehybrid_Q8_0&lt;/td&gt; &lt;td align="left"&gt;35.47 GiB&lt;/td&gt; &lt;td align="left"&gt;32.21 B&lt;/td&gt; &lt;td align="left"&gt;72.56 ± 0.79&lt;/td&gt; &lt;td align="left"&gt;4.26 ± 0.49&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;granitehybrid_Q6_K&lt;/td&gt; &lt;td align="left"&gt;25.95 GiB&lt;/td&gt; &lt;td align="left"&gt;32.21 B&lt;/td&gt; &lt;td align="left"&gt;54.77 ± 1.87&lt;/td&gt; &lt;td align="left"&gt;5.51 ± 0.49&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;granitehybrid_Q5_K - Medium&lt;/td&gt; &lt;td align="left"&gt;21.53 GiB&lt;/td&gt; &lt;td align="left"&gt;32.21 B&lt;/td&gt; &lt;td align="left"&gt;57.90 ± 4.46&lt;/td&gt; &lt;td align="left"&gt;6.36 ± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;granitehybrid_Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;17.49 GiB&lt;/td&gt; &lt;td align="left"&gt;32.21 B&lt;/td&gt; &lt;td align="left"&gt;57.26 ± 2.02&lt;/td&gt; &lt;td align="left"&gt;7.21 ± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;iGPU has flexibility of using system RAM as VRAM and can load larger models 32B and take advantage of using active parameters 9B to get decent speed from bigger parameter models. Looks like using Q8_K_XL has prompt processing benefit and Q5_K_XL for balance of speed on both sides of inference. Post here if you have an iGPU results to compare. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tabletuser_blogspot"&gt; /u/tabletuser_blogspot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0kwx3/granite_40_on_igpu_amd_ryzen_6800h_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0kwx3/granite_40_on_igpu_amd_ryzen_6800h_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o0kwx3/granite_40_on_igpu_amd_ryzen_6800h_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T17:15:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1nzwnbj</id>
    <title>The qwen3-next pr in llamacpp has been validated with a small test model</title>
    <updated>2025-10-06T21:52:11+00:00</updated>
    <author>
      <name>/u/Betadoggo_</name>
      <uri>https://old.reddit.com/user/Betadoggo_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzwnbj/the_qwen3next_pr_in_llamacpp_has_been_validated/"&gt; &lt;img alt="The qwen3-next pr in llamacpp has been validated with a small test model" src="https://b.thumbs.redditmedia.com/WI7I1cXVA_soqNsjPi7jb3WgjXU9vLhmlQDKP6zYAUY.jpg" title="The qwen3-next pr in llamacpp has been validated with a small test model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Link to comment: &lt;a href="https://github.com/ggml-org/llama.cpp/pull/16095#issuecomment-3373977382"&gt;https://github.com/ggml-org/llama.cpp/pull/16095#issuecomment-3373977382&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I've been stalking this pr since it was opened and figured I'd share this update since I know a lot of others were interested in this model. Pwilkin has done some crazy work getting this together so quickly.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Betadoggo_"&gt; /u/Betadoggo_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nzwnbj"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzwnbj/the_qwen3next_pr_in_llamacpp_has_been_validated/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nzwnbj/the_qwen3next_pr_in_llamacpp_has_been_validated/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T21:52:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0k8vf</id>
    <title>ryzen 395+ with 96gb on sale sale for $1728</title>
    <updated>2025-10-07T16:52:21+00:00</updated>
    <author>
      <name>/u/thebadslime</name>
      <uri>https://old.reddit.com/user/thebadslime</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0k8vf/ryzen_395_with_96gb_on_sale_sale_for_1728/"&gt; &lt;img alt="ryzen 395+ with 96gb on sale sale for $1728" src="https://external-preview.redd.it/wyRlnnC4nIHWRfWMUIBnHvHMsP98N9mROJtKXbnwKWI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=290ace7209dd3df0a237ec970a6a8b1662d523e1" title="ryzen 395+ with 96gb on sale sale for $1728" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been watching mini PCs and this is $600 off&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thebadslime"&gt; /u/thebadslime &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.amazon.com/GMKtec_ryzen_ai_max_395_mini_pc/dp/B0FLDJBS79?crid=1Q219TSIMC6E5&amp;amp;dib=eyJ2IjoiMSJ9.I2mCpbFC-I1kX_zpKzZjAVsC3UFEmWuAGsNmMRg4JjW-m65FEqL2voOm1dEASZH9A7BoEcVRQFBh4B8XK42Pd3cmsD6a0J3Puup9S6jg7SKf9mcVXlN4AxOZU88HfLTVjyD2uDnYWQg1dXLvo8EC33ImbdTdRO6_DV1m7kDC5Xo.DOidGG2jBnTmZefIX55ouM3iX6383KiTOjHDcWIesxo&amp;amp;dib_tag=se&amp;amp;keywords=395%2B%2Bmax&amp;amp;qid=1759855684&amp;amp;sprefix=395%2B%2Caps%2C130&amp;amp;sr=8-5&amp;amp;th=1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0k8vf/ryzen_395_with_96gb_on_sale_sale_for_1728/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o0k8vf/ryzen_395_with_96gb_on_sale_sale_for_1728/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T16:52:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0ly7m</id>
    <title>Fan shroud for AMD MI50</title>
    <updated>2025-10-07T17:52:35+00:00</updated>
    <author>
      <name>/u/Bit_Matter</name>
      <uri>https://old.reddit.com/user/Bit_Matter</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, since the AMD MI50 is the cheapest graphic card with 32GB VRAM you can get at the moment, I bought 3 of them. In order to make them fit better in my case, I designed a new shroud for the card which integrates a blower fan. You can find it here: &lt;a href="https://www.printables.com/model/1421067-amd-instinct-mi50-shroud"&gt;https://www.printables.com/model/1421067-amd-instinct-mi50-shroud&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bit_Matter"&gt; /u/Bit_Matter &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0ly7m/fan_shroud_for_amd_mi50/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0ly7m/fan_shroud_for_amd_mi50/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o0ly7m/fan_shroud_for_amd_mi50/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T17:52:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0dy0y</id>
    <title>More love for GLM4.6 (evaluation vs. Claude 4.5 for NLP tasks)</title>
    <updated>2025-10-07T12:53:15+00:00</updated>
    <author>
      <name>/u/LoveMind_AI</name>
      <uri>https://old.reddit.com/user/LoveMind_AI</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been putting GLM4.6 and Claude 4.5 head to head relentlessly since both were released, and really can't overstate how impressive GLM4.6 is. I'm using both over OpenRouter. &lt;/p&gt; &lt;p&gt;My use case: critically evaluating published AI literature, working on my own architecture ideas, summarizing large articles, picking through sprawling conversations for the salient ideas.&lt;/p&gt; &lt;p&gt;What's really impressive to me is how good GLM4.6 is at following my instructions to the letter, understanding nuanced ways that I want it to analyze data, and avoiding putting its own spin on things. It's also absolutely fantastic at &amp;quot;thinking in character&amp;quot; (I use persona prompts to process information in parallel from different perspectives - ie. one run to critique literature and probe quality of experimental set-ups, another run to evaluate whether are creative implications that I'm missing, etc.) - this is a model that loves a great system prompt. The ability to shape the way GLM4.6 reasons is really impressive. The draw back in terms of persona prompting is that while GLM4.6 is great at functionally behaving according to the prompt, its tonal style usually drifts. I think this is more a factor of how MoE models process RP-adjacent prompting (I find that dense models are massively better at this) than it is a GLM4.6 problem specifically. GLM4.6 holds on to technical details of what I'm either reading or writing *spectacularly* well. It seems even more clear-headed than Claude when it comes to working on implementation ideas, or paying attention to implementation that I'm reading about. &lt;/p&gt; &lt;p&gt;Claude Sonnet 4.5 is impressive in terms of its ability to follow a huge list of complicated topics across many turns. Of every LLM I have tried, this truly keeps its head together longer than any I've tried. I have pushed the context window ridiculously far and have only seen one or two minor factual errors. Exact instruction following (ie. system instructions about cognitive processing requirements) gets dulled over time, for sure. And while 4.5 seems far better at persona prompting than 4 did, there's an underlying Claude-ness that just can't be denied. Even without the obnoxious LCR stuff going on in the Anthropic UI (not to mention their shady data mining reversal), Claude can't help but lapse into Professor Dad mode. (Just like Gemini can't really avoid being a former high school valedictorian who got into an Ivy on a lacrosse scholarship while still suffering from imposter syndrome) &lt;/p&gt; &lt;p&gt;GLM4.6 doesn't stay coherent quite as long - and there are some weird glitches: lapses into Chinese, confusing its reasoning layer for its response layer, and becoming repetitive in long responses (ie. saying the same thing twice). Still, it remains coherent FAR longer than Gemini 2.5 Pro. &lt;/p&gt; &lt;p&gt;What I find really interesting about GLM4.6 is that it seems to have no overtly detectable ideological bias - it's really open, and depending on how you prompt it, can truly look at things from multiple perspectives. DeepSeek and Kimi K2 both have slants (which I happen to dig!) - this might be the most flexible model I have tried, period. &lt;/p&gt; &lt;p&gt;If the lapse-into-chinese and repetitive loops could be stamped out a bit, this would be the no-brainer LLM to build with for what I do. (As always, with the caveat that I'm praying daily for a dense Gemma 3 or Gemma 4 model in the 50B+ range)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LoveMind_AI"&gt; /u/LoveMind_AI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0dy0y/more_love_for_glm46_evaluation_vs_claude_45_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0dy0y/more_love_for_glm46_evaluation_vs_claude_45_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o0dy0y/more_love_for_glm46_evaluation_vs_claude_45_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T12:53:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0i4fz</id>
    <title>Will DDR6 be the answer to LLM?</title>
    <updated>2025-10-07T15:34:17+00:00</updated>
    <author>
      <name>/u/fungnoth</name>
      <uri>https://old.reddit.com/user/fungnoth</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Bandwidth doubles every generation of system memory. And we need that for LLMs. &lt;/p&gt; &lt;p&gt;If DDR6 is going to be 10000+ MT/s easily, and then dual channel and quad channel would boast that even more. Maybe we casual AI users would be able to run large models around 2028. Like deepseek sized full models in a chat-able speed. And the workstation GPUs will only be worth buying for commercial use because they serve more than one user at a time. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fungnoth"&gt; /u/fungnoth &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0i4fz/will_ddr6_be_the_answer_to_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0i4fz/will_ddr6_be_the_answer_to_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o0i4fz/will_ddr6_be_the_answer_to_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T15:34:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0f2uf</id>
    <title>Hi folks, sorry for the self‑promo. I’ve built an open‑source project that could be useful to some of you</title>
    <updated>2025-10-07T13:40:33+00:00</updated>
    <author>
      <name>/u/panos_s_</name>
      <uri>https://old.reddit.com/user/panos_s_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0f2uf/hi_folks_sorry_for_the_selfpromo_ive_built_an/"&gt; &lt;img alt="Hi folks, sorry for the self‑promo. I’ve built an open‑source project that could be useful to some of you" src="https://preview.redd.it/1tzatvfz0ptf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=149ffc98b84835693e3aa54c4c554277120de6ea" title="Hi folks, sorry for the self‑promo. I’ve built an open‑source project that could be useful to some of you" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;: Web dashboard for NVIDIA GPUs with 30+ real-time metrics (utilisation, memory, temps, clocks, power, processes). Live charts over WebSockets, multi‑GPU support, and one‑command Docker deployment. No agents, minimal setup.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/psalias2006/gpu-hot"&gt;https://github.com/psalias2006/gpu-hot&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why I built it&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Wanted simple, real‑time visibility without standing up a full metrics stack.&lt;/li&gt; &lt;li&gt;Needed clear insight into temps, throttling, clocks, and active processes during GPU work.&lt;/li&gt; &lt;li&gt;A lightweight dashboard that’s easy to run at home or on a workstation.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What it does&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Polls nvidia-smi and streams 30+ metrics every ~2s via WebSockets.&lt;/li&gt; &lt;li&gt;Tracks per‑GPU utilization, memory (used/free/total), temps, power draw/limits, fan, clocks, PCIe, P‑State, encoder/decoder stats, driver/VBIOS, throttle status.&lt;/li&gt; &lt;li&gt;Shows active GPU processes with PIDs and memory usage.&lt;/li&gt; &lt;li&gt;Clean, responsive UI with live historical charts and basic stats (min/max/avg).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Setup (Docker)&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;git clone https://github.com/psalias2006/gpu-hot cd gpu-hot docker-compose up --build # open http://localhost:1312 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Looking for feedback&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/panos_s_"&gt; /u/panos_s_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1tzatvfz0ptf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0f2uf/hi_folks_sorry_for_the_selfpromo_ive_built_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o0f2uf/hi_folks_sorry_for_the_selfpromo_ive_built_an/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T13:40:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0ifyr</id>
    <title>Glm 4.6 air is coming</title>
    <updated>2025-10-07T15:46:04+00:00</updated>
    <author>
      <name>/u/Namra_7</name>
      <uri>https://old.reddit.com/user/Namra_7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0ifyr/glm_46_air_is_coming/"&gt; &lt;img alt="Glm 4.6 air is coming" src="https://preview.redd.it/nmwtp72fnptf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=78e29ea88c42c50216e45dc228bec7e885394f0c" title="Glm 4.6 air is coming" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Namra_7"&gt; /u/Namra_7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nmwtp72fnptf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0ifyr/glm_46_air_is_coming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o0ifyr/glm_46_air_is_coming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T15:46:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvryo4</id>
    <title>AMA Announcement: Prime Intellect — The Open‑Source Distributed Training Lab (Thu, Oct 2 • 10 AM – 1 PM PDT)</title>
    <updated>2025-10-02T02:31:01+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"&gt; &lt;img alt="AMA Announcement: Prime Intellect — The Open‑Source Distributed Training Lab (Thu, Oct 2 • 10 AM – 1 PM PDT)" src="https://preview.redd.it/222kj50x0msf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c1c747c42690f74b8d08690dcdbb1ef23aece13b" title="AMA Announcement: Prime Intellect — The Open‑Source Distributed Training Lab (Thu, Oct 2 • 10 AM – 1 PM PDT)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/222kj50x0msf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T02:31:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwaoyd</id>
    <title>AMA with Prime Intellect — Ask Us Anything!</title>
    <updated>2025-10-02T17:47:44+00:00</updated>
    <author>
      <name>/u/kindacognizant</name>
      <uri>https://old.reddit.com/user/kindacognizant</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;AMA with Prime Intellect — Ask Us Anything!&lt;/h1&gt; &lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;! We’re excited for this AMA, thank you for having us.&lt;/p&gt; &lt;p&gt;I’m Kalomaze (&lt;a href="/u/kindacognizant"&gt;u/kindacognizant&lt;/a&gt;), a researcher at Prime Intellect, the lab behind:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Distributed training &lt;a href="https://www.primeintellect.ai/#research"&gt;efforts&lt;/a&gt; including INTELLECT-1 + INTELLECT-2&lt;/li&gt; &lt;li&gt;Open-source RL efforts including &lt;a href="https://github.com/PrimeIntellect-ai/verifiers"&gt;verifiers&lt;/a&gt;, &lt;a href="https://github.com/PrimeIntellect-ai/prime-rl"&gt;prime-rl&lt;/a&gt;, and the &lt;a href="https://app.primeintellect.ai/dashboard/environments"&gt;Environments Hub&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Our other participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Sami Jaghouar, &lt;a href="/u/samsja19"&gt;u/samsja19&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Will Brown, &lt;a href="/u/willccbb"&gt;u/willccbb&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Jack Min Ong, &lt;a href="/u/Cinamic"&gt;u/Cinamic&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Mika Senghaas, &lt;a href="/u/mikasenghaas"&gt;u/mikasenghaas&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 11:00 AM – 2:00 PM PST, with the Prime Intellect team continuing to follow up on questions over the next 48 hours.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kindacognizant"&gt; /u/kindacognizant &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwaoyd/ama_with_prime_intellect_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwaoyd/ama_with_prime_intellect_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwaoyd/ama_with_prime_intellect_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T17:47:44+00:00</published>
  </entry>
</feed>
