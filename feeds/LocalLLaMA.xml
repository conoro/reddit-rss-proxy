<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-05T11:07:01+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1mi5s6w</id>
    <title>Mi50 32gb (Working config, weirdness and performance)</title>
    <updated>2025-08-05T10:43:56+00:00</updated>
    <author>
      <name>/u/Danternas</name>
      <uri>https://old.reddit.com/user/Danternas</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Thought I'd share some knowledge after a week with an Mi50 32gb bought from Ebay. Was originally supposed to be a response but hyper-focus took over and this is more suited as a post.&lt;/p&gt; &lt;p&gt;It arrived new-looking. Anti-static bag, not a spec of dust and plastic peel still on the AMD Instinct branded shroud. Mine came with an extra radial fan which can be mounted on the back and connected to a 12v header. Some tape was necessary to direct the air into the heat-sink. I was sceptical about the capability of this small radial fan but it seem to keep the GPU edge under 80C under heavy use, though I have not stress tested it.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Weirdness&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;One weird thing is how it is listed in lspci:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;0a:00.0 VGA compatible controller [0300]: Advanced Micro Devices, Inc. [AMD/ATI] Vega 20 [Radeon Pro Vega II/Radeon Pro Vega II Duo] [1002:66a3]&lt;/p&gt; &lt;p&gt;Subsystem: Apple Inc. Vega 20 [Radeon Pro Vega II/Radeon Pro Vega II Duo] [106b:0201]&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Which suggests it is not an Mi50 at all? Or some weird Chinese shifting of components. Note the Apple subsystem. In rocm-smi it does boost over 1700mhz and pull near 300w, which is consistent with Mi50 specs. However, Mi50 seem to be a cut down Radeon Pro Vega II. So maybe it is a Radeon Pro Vega II put on a Mi50 board and flashed with Mi50 BIOS? Could it be flashed back to a Radeon Pro Vega II. I have no idea, even less why that would make any sense. Maybe I'm just overthinking it.&lt;/p&gt; &lt;p&gt;Another curious thing is that the card lacks a fan or even fan header but reports fan speed in rocm-smi.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Working configuration&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I got it to work on the following configuration&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;GPU: AMD Instinct MI50 (32 GB, gfx906)&lt;/p&gt; &lt;p&gt;Proxmox: 8.4.6&lt;/p&gt; &lt;p&gt;Kernel: 6.8.12-4-pve (downgraded from 6.8.12-13-pve, though I am unsure if this mattered)&lt;/p&gt; &lt;p&gt;OS in the Proxmox host: Debian 12 (Bookworm) + Ubuntu 24.04 (&amp;quot;Noble&amp;quot;) repositories for ROCm&lt;/p&gt; &lt;p&gt;ROCm-version: 6.4.2&lt;/p&gt; &lt;p&gt;Driver: amdgpu-dkms installed after headers&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;My method was as stupid as it sounds. But it worked after hours if trial and error. Right now I am just happy it works.&lt;/p&gt; &lt;p&gt;&lt;a href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/quick-start.html"&gt;https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/quick-start.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Run the commands for ROCm Ubuntu 24.04, then AMDGPU driver commands for Ubuntu 24.04, and then the commands for ROCm Ubuntu 24.04 again. There's probably some way simpler way and maybe something else I did contributed. But right now I am happy it works without installing a 5.15 Ubuntu kernel and I can still use Proxmox.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Pass-through not working, LXC working fine&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Once it register in rocm-smi it was easy to use the OpenWebUI LXC community script to make an LXC container. Then I manually installed Ollama inside of it. I did not get it to work pass-through and I have not seen any example where this works. AMD also lists it as not compatible with pass-through. Use it bare metal. Make sure to give the LXC the resources /dev/kfd, /dev/dri/card0, and /dev/dri/renderD128 with the right GID.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Power draw&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Idle power draw is 25w according to rocm-smi, which seems accurate compared to measure usage from the wall and UPS. During benchmarking it reached 220-260w and 68c.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Performance&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The card is in a server with a Ryzen 5 3600 and 64gb of ram, where the LXC container is limited to 8 cores and 8gb of ram. This seem to be overkill as basically all computation is done in the GPU and usage is under 20% of the 8 logical cores/4gb. The Mi50 boosts all the way to 1730mhz/&amp;gt;95% usage and remains there.&lt;/p&gt; &lt;p&gt;llm_benchmark:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;mistral:7b Median run average of eval rate: 63.754 tokens/s&lt;/p&gt; &lt;p&gt;llama3.1:8b Median run average of eval rate: 56.772 tokens/s&lt;/p&gt; &lt;p&gt;gemma2:9b Median run average of eval rate: 43.736 tokens/s&lt;/p&gt; &lt;p&gt;llava:7b Median run average of eval rate: 74.874 tokens/s&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;It had a dip in performance on the 2nd run of 5 prompts and for some reason couldn't finish deepseek-r1:8b. Not sure why as I have been able to do deepseek-r1:32b just fine in OpenWebUI.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;VRAM&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;VRAM is absolutely fantastic of course and the main reason to consider the Mi50 in my opinion. If not for the VRAM you may as well get an RTX 3060 12gb or similar from Nvidia to save you from some AMD driver headaches. 30b models doesn't seem to be any issues at all with vram to spare.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The Mi50 right now gives you big GPU capability for a cheap price. In my opinion it is mainly for you who want the 32gb. I see less point in the 16gb, but it is even cheaper I suppose. Be aware though that AMD considers the Mi50 unsupported and depending on your use-case you may encounter a poor experience getting the drivers to work properly. Not to mention I don't think it works at all in Windows. It is not a card for someone who just want things to work, but it is cheap 32gb of HBM.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Danternas"&gt; /u/Danternas &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mi5s6w/mi50_32gb_working_config_weirdness_and_performance/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mi5s6w/mi50_32gb_working_config_weirdness_and_performance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mi5s6w/mi50_32gb_working_config_weirdness_and_performance/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T10:43:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1mhpm02</id>
    <title>Quick Qwen Image Gen with 4090+3060</title>
    <updated>2025-08-04T20:59:58+00:00</updated>
    <author>
      <name>/u/fp4guru</name>
      <uri>https://old.reddit.com/user/fp4guru</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhpm02/quick_qwen_image_gen_with_40903060/"&gt; &lt;img alt="Quick Qwen Image Gen with 4090+3060" src="https://b.thumbs.redditmedia.com/rc6cs4fuadas1yBd9N2cMHV6iAM0qvhoekwcLUHsAfU.jpg" title="Quick Qwen Image Gen with 4090+3060" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just tested the new &lt;strong&gt;Qwen-Image&lt;/strong&gt; model from Alibaba using ü§ó Diffusers with bfloat16 + dual-GPU memory config (4090 + 3060). Prompted it to generate a &lt;strong&gt;cyberpunk night market scene&lt;/strong&gt;‚Äîcomplete with neon signs, rainy pavement, futuristic street food vendors, and a monorail in the background.&lt;/p&gt; &lt;p&gt;Ran at &lt;code&gt;1472x832&lt;/code&gt;, 32 steps, &lt;code&gt;true_cfg_scale=3.0&lt;/code&gt;. No LoRA, no refiner‚Äîjust straight from the base checkpoint.&lt;/p&gt; &lt;p&gt;Full prompt and code below. Let me know what you think of the result or if you‚Äôve got prompt ideas to push it further.&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;from diffusers import DiffusionPipeline&lt;/p&gt; &lt;p&gt;import torch, gc&lt;/p&gt; &lt;p&gt;pipe = DiffusionPipeline.from_pretrained(&lt;/p&gt; &lt;p&gt;&amp;quot;Qwen/Qwen-Image&amp;quot;,&lt;/p&gt; &lt;p&gt;torch_dtype=torch.bfloat16,&lt;/p&gt; &lt;p&gt;device_map=&amp;quot;balanced&amp;quot;,&lt;/p&gt; &lt;p&gt;max_memory={0: &amp;quot;23GiB&amp;quot;, 1: &amp;quot;11GiB&amp;quot;},&lt;/p&gt; &lt;p&gt;)&lt;/p&gt; &lt;p&gt;pipe.enable_attention_slicing()&lt;/p&gt; &lt;p&gt;pipe.enable_vae_tiling()&lt;/p&gt; &lt;p&gt;prompt = (&lt;/p&gt; &lt;p&gt;&amp;quot;A bustling cyberpunk night market street scene. Neon signs in Chinese hang above steaming food stalls. &amp;quot;&lt;/p&gt; &lt;p&gt;&amp;quot;A robotic vendor is grilling skewers while a crowd of futuristic characters‚Äîsome wearing glowing visors, &amp;quot;&lt;/p&gt; &lt;p&gt;&amp;quot;some holding umbrellas under a light drizzle‚Äîgathers around. Bright reflections on the wet pavement. &amp;quot;&lt;/p&gt; &lt;p&gt;&amp;quot;In the distance, a monorail passes by above the alley. Ultra HD, 4K, cinematic composition.&amp;quot;&lt;/p&gt; &lt;p&gt;)&lt;/p&gt; &lt;p&gt;negative_prompt = (&lt;/p&gt; &lt;p&gt;&amp;quot;low quality, blurry, distorted, bad anatomy, text artifacts, poor lighting&amp;quot;&lt;/p&gt; &lt;p&gt;)&lt;/p&gt; &lt;p&gt;img = pipe(&lt;/p&gt; &lt;p&gt;prompt=prompt,&lt;/p&gt; &lt;p&gt;negative_prompt=negative_prompt,&lt;/p&gt; &lt;p&gt;width=1472, height=832,&lt;/p&gt; &lt;p&gt;num_inference_steps=32,&lt;/p&gt; &lt;p&gt;true_cfg_scale=3.0,&lt;/p&gt; &lt;p&gt;generator=torch.Generator(&amp;quot;cuda&amp;quot;).manual_seed(8899)&lt;/p&gt; &lt;p&gt;).images[0]&lt;/p&gt; &lt;p&gt;img.save(&amp;quot;qwen_cyberpunk_market.png&amp;quot;)&lt;/p&gt; &lt;p&gt;del pipe; gc.collect(); torch.cuda.empty_cache()&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/0djfy60ms3hf1.png?width=1472&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=77be5a785c6eb51c997d0faa3226371547b7fcdc"&gt;https://preview.redd.it/0djfy60ms3hf1.png?width=1472&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=77be5a785c6eb51c997d0faa3226371547b7fcdc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;thanks to &lt;a href="https://www.reddit.com/user/motorcycle_frenzy889/"&gt;motorcycle_frenzy889&lt;/a&gt; , 60 steps can craft correct text.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fp4guru"&gt; /u/fp4guru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhpm02/quick_qwen_image_gen_with_40903060/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhpm02/quick_qwen_image_gen_with_40903060/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mhpm02/quick_qwen_image_gen_with_40903060/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-04T20:59:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1mhg8rt</id>
    <title>Get ready for GLM-4-5 local gguf woot woot</title>
    <updated>2025-08-04T15:16:51+00:00</updated>
    <author>
      <name>/u/segmond</name>
      <uri>https://old.reddit.com/user/segmond</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This model is insane! I have been testing the ongoing llama.cpp PR and this morning has been amazing! GLM can spit out LOOOOOOOOOOOOOOOOOONG tokens! The original was a beast, and the new one is even better. I gave it 2500 lines of python code, told it to refactor it, it do so without dropping anything! Then I told it to translate it to ruby and it did so completely. The model is very coherent across long contexts, the quality so far is great. The model is fast! Full loaded on 3090's, It starts out at 45tk/sec and this is with llama.cpp.&lt;/p&gt; &lt;p&gt;I have only driven it for about an hour and this is the smaller model air, not the big one! I'm very convinced that this will replace deepseek-r1/chimera/v3/ernie-300b/kimi-k2 for me.&lt;/p&gt; &lt;p&gt;Is this better than sonnet/opus/gemini/openai? For me yup! I don't use closed models, so I really can't tell, but this so far is looking like the best damn model locally. I have only thrown code generation at it, so I can't tell how it would perform in creative writing, role play, other sorts of generation etc. I haven't played at all with tool calling, instruction following, etc, but based on how well it's responding, I think it's going to be great. The only short coming I see is the 128k context window.&lt;/p&gt; &lt;p&gt;It's fast too, 50k+ token, 16.44 tk/sec&lt;/p&gt; &lt;p&gt;slot release: id 0 | task 42155 | stop processing: n_past = 51785, truncated = 0&lt;/p&gt; &lt;p&gt;slot print_timing: id 0 | task 42155 |&lt;/p&gt; &lt;p&gt;prompt eval time = 421.72 ms / 35 tokens ( 12.05 ms per token, 82.99 tokens per second)&lt;/p&gt; &lt;p&gt;eval time = 983525.01 ms / 16169 tokens ( 60.83 ms per token, 16.44 tokens per second)&lt;/p&gt; &lt;p&gt;Edit:&lt;br /&gt; q4 quants down to 67.85gb&lt;br /&gt; I decide to run q4, offload only shared experts to 1 3090 GPU and the rest to system ram (ddr4 2400mhz quad channel on dual x99 platform). The entire shared experts for 47 layers takes about 4gb of vram, that means you can put all of the shared expert on your 8gb GPU. I decide to not load any other tensor but just these and see how it performs. It start out at 10tk/sec. I'm going to run q3_k_l on a 3060 and P40 and put up the results later.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/segmond"&gt; /u/segmond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhg8rt/get_ready_for_glm45_local_gguf_woot_woot/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhg8rt/get_ready_for_glm45_local_gguf_woot_woot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mhg8rt/get_ready_for_glm45_local_gguf_woot_woot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-04T15:16:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1mhhhpi</id>
    <title>Qwen-Image ‚Äî a 20B MMDiT model</title>
    <updated>2025-08-04T16:02:49+00:00</updated>
    <author>
      <name>/u/Xhehab_</name>
      <uri>https://old.reddit.com/user/Xhehab_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;üöÄ Meet Qwen-Image ‚Äî a 20B MMDiT model for next-gen text-to-image generation. Especially strong at creating stunning graphic posters with native text. Now open-source.&lt;/p&gt; &lt;p&gt;üîç Key Highlights:&lt;/p&gt; &lt;p&gt;üîπ SOTA text rendering ‚Äî rivals GPT-4o in English, best-in-class for Chinese&lt;/p&gt; &lt;p&gt;üîπ In-pixel text generation ‚Äî no overlays, fully integrated&lt;/p&gt; &lt;p&gt;üîπ Bilingual support, diverse fonts, complex layouts&lt;/p&gt; &lt;p&gt;üé® Also excels at general image generation ‚Äî from photorealistic to anime, impressionist to minimalist. A true creative powerhouse.&lt;/p&gt; &lt;p&gt;Blog: &lt;a href="https://qwenlm.github.io/blog/qwen-image/%5BBlog%5D(https://qwenlm.github.io/blog/qwen-image/)"&gt;https://qwenlm.github.io/blog/qwen-image/[Blog](https://qwenlm.github.io/blog/qwen-image/)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hugging Face: &lt;a href="http://huggingface.co/Qwen/Qwen-Image"&gt;huggingface.co/Qwen/Qwen-Image&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xhehab_"&gt; /u/Xhehab_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhhhpi/qwenimage_a_20b_mmdit_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhhhpi/qwenimage_a_20b_mmdit_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mhhhpi/qwenimage_a_20b_mmdit_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-04T16:02:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1mhlo6g</id>
    <title>Google introduces a new Benchmark: Game Arena and they're streaming your favorite open weight models playing chess against close source models.</title>
    <updated>2025-08-04T18:33:46+00:00</updated>
    <author>
      <name>/u/mtmttuan</name>
      <uri>https://old.reddit.com/user/mtmttuan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhlo6g/google_introduces_a_new_benchmark_game_arena_and/"&gt; &lt;img alt="Google introduces a new Benchmark: Game Arena and they're streaming your favorite open weight models playing chess against close source models." src="https://external-preview.redd.it/UFZqcZq9i0MOfYzexLrIUEZbhTVVNRWezkExEe_y0E4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cc604dc195cda31f472811e38b2354a3cb7b4e27" title="Google introduces a new Benchmark: Game Arena and they're streaming your favorite open weight models playing chess against close source models." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here is the original blog post: &lt;a href="https://blog.google/technology/ai/kaggle-game-arena/"&gt;https://blog.google/technology/ai/kaggle-game-arena/&lt;/a&gt; &lt;/p&gt; &lt;p&gt;About the benchmark, I personally prefer game as a head-to-head benchmark to LMArena. At least if they do benchmaxxing, we might have models that's more intelligent comparing to the more glazing effect of LMArena. &lt;/p&gt; &lt;p&gt;About the exhibition stream, it's funny to see they let Deepseek R1 play against o4-mini and Grok 4 play against gemini flash. Kimi-K2 vs O3 would be fun though. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/83xmndz6q1hf1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=376f9bc5e41eb6f87a3f4c9cfd3e4be8aadf8814"&gt;https://preview.redd.it/83xmndz6q1hf1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=376f9bc5e41eb6f87a3f4c9cfd3e4be8aadf8814&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mtmttuan"&gt; /u/mtmttuan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhlo6g/google_introduces_a_new_benchmark_game_arena_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhlo6g/google_introduces_a_new_benchmark_game_arena_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mhlo6g/google_introduces_a_new_benchmark_game_arena_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-04T18:33:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1mhctvk</id>
    <title>Huawei released weights of Pangu Ultra,a 718B model.</title>
    <updated>2025-08-04T13:02:04+00:00</updated>
    <author>
      <name>/u/Overflow_al</name>
      <uri>https://old.reddit.com/user/Overflow_al</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhctvk/huawei_released_weights_of_pangu_ultraa_718b_model/"&gt; &lt;img alt="Huawei released weights of Pangu Ultra,a 718B model." src="https://external-preview.redd.it/b4vAhRXu0QISFGzKNI7MMEeFrdQG1UWQqC8GhQPUCNU.png?width=70&amp;amp;height=70&amp;amp;crop=70:70,smart&amp;amp;auto=webp&amp;amp;s=42d2a16045706201098a626b24ce7402818223f6" title="Huawei released weights of Pangu Ultra,a 718B model." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Overflow_al"&gt; /u/Overflow_al &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ai.gitcode.com/ascend-tribe/openpangu-ultra-moe-718b-model/blob/main/README_EN.md"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhctvk/huawei_released_weights_of_pangu_ultraa_718b_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mhctvk/huawei_released_weights_of_pangu_ultraa_718b_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-04T13:02:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1mhx1kc</id>
    <title>I see people rushing to GLM Air GGUF's on this repo - what does this warning usually mean? I haven't seen a model flagged since we passed around pickled weights</title>
    <updated>2025-08-05T02:17:40+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhx1kc/i_see_people_rushing_to_glm_air_ggufs_on_this/"&gt; &lt;img alt="I see people rushing to GLM Air GGUF's on this repo - what does this warning usually mean? I haven't seen a model flagged since we passed around pickled weights" src="https://preview.redd.it/8xcfsxcl14hf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=50f98e0a3e5a4233ccef0bc4ee09bdf72af5c26e" title="I see people rushing to GLM Air GGUF's on this repo - what does this warning usually mean? I haven't seen a model flagged since we passed around pickled weights" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8xcfsxcl14hf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhx1kc/i_see_people_rushing_to_glm_air_ggufs_on_this/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mhx1kc/i_see_people_rushing_to_glm_air_ggufs_on_this/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T02:17:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1mi3igq</id>
    <title>The translation capability of GLM4.5 for Chinese slang.</title>
    <updated>2025-08-05T08:22:17+00:00</updated>
    <author>
      <name>/u/OddUnderstanding1633</name>
      <uri>https://old.reddit.com/user/OddUnderstanding1633</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I find that GLM4.5 can successfully understand and translate the slang in Chinese. Take an example in Seed-X-Challenge benchmark: the source text is &amp;quot;Á¶ªË∞±Â•πÂ¶àÁªôÁ¶ªË∞±ÂºÄÈó® ‚Äã Á¶ªË∞±Âà∞ÂÆ∂‰∫Ü&amp;quot;, and this sentence needs to be translated in a way that captures its extremely absurd, rather than being translated literally.&lt;/p&gt; &lt;p&gt;The translation result of GPT-4o is &amp;quot;Absurdity's mom opens the door for absurdity‚Äîit's utterly absurd.&amp;quot;&lt;/p&gt; &lt;p&gt;While the translation result of GLM4.5 is &amp;quot;Ridiculous to the extreme - it's reached peak ridiculousness.&amp;quot;&lt;/p&gt; &lt;p&gt;It seems that GLM4.5 has a better understanding of Chinese slang and produces better translations. Has anyone tried GLM4.5‚Äôs translation capabilities?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OddUnderstanding1633"&gt; /u/OddUnderstanding1633 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mi3igq/the_translation_capability_of_glm45_for_chinese/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mi3igq/the_translation_capability_of_glm45_for_chinese/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mi3igq/the_translation_capability_of_glm45_for_chinese/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T08:22:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1mhf0kl</id>
    <title>Qwen image 20B is coming!</title>
    <updated>2025-08-04T14:30:09+00:00</updated>
    <author>
      <name>/u/sunshinecheung</name>
      <uri>https://old.reddit.com/user/sunshinecheung</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhf0kl/qwen_image_20b_is_coming/"&gt; &lt;img alt="Qwen image 20B is coming!" src="https://external-preview.redd.it/qRUOYZYoHmNR9iE-xb-2D9P2t108utUKO0BsEEFsXs0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9b2d120bc77d25aceff54c01b358c0fa229b74ef" title="Qwen image 20B is coming!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/3n3tfhqbj0hf1.png?width=529&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5234584d7973049a12fc3c428b50a1d35e48858f"&gt;https://preview.redd.it/3n3tfhqbj0hf1.png?width=529&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5234584d7973049a12fc3c428b50a1d35e48858f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/uxg8kr5ej0hf1.png?width=1664&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f06cc61180cfced07aa66367d23e552e605c0f75"&gt;https://preview.redd.it/uxg8kr5ej0hf1.png?width=1664&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f06cc61180cfced07aa66367d23e552e605c0f75&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Qwen image is ready to drop:&lt;a href="https://github.com/huggingface/diffusers/pull/12055"&gt;https://github.com/huggingface/diffusers/pull/12055&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sunshinecheung"&gt; /u/sunshinecheung &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhf0kl/qwen_image_20b_is_coming/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhf0kl/qwen_image_20b_is_coming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mhf0kl/qwen_image_20b_is_coming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-04T14:30:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1mhrx3m</id>
    <title>How to use your Local Models to watch your screen. Open Source and Completely Free!!</title>
    <updated>2025-08-04T22:30:05+00:00</updated>
    <author>
      <name>/u/Roy3838</name>
      <uri>https://old.reddit.com/user/Roy3838</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhrx3m/how_to_use_your_local_models_to_watch_your_screen/"&gt; &lt;img alt="How to use your Local Models to watch your screen. Open Source and Completely Free!!" src="https://external-preview.redd.it/czVxbDlzeWx3MmhmMTeXrb7fw6xx0BNL_5u8ms92EIrAoiEjzO7YOwhlTBj3.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ba0ed7244b693bf15ef5f04c6f2d7cddc4f49c67" title="How to use your Local Models to watch your screen. Open Source and Completely Free!!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TLDR:&lt;/strong&gt; I built this &lt;strong&gt;open source&lt;/strong&gt; and &lt;strong&gt;local&lt;/strong&gt; app that lets your local models &lt;strong&gt;watch your screen&lt;/strong&gt; and do stuff! It is now &lt;strong&gt;suuuper easy to install&lt;/strong&gt; and use, to make local AI &lt;strong&gt;accessible to&lt;/strong&gt; &lt;strong&gt;everybody&lt;/strong&gt;!&lt;/p&gt; &lt;p&gt;Hey r/LocalLLaMA! I'm back with some Observer updates c: first of all &lt;strong&gt;Thank You&lt;/strong&gt; so much for all of your support and feedback, i've been working hard to take this project to this current state. I added the app installation which is a significant QOL improvement for ease of use for first time users!! The docker-compose option is still supported and viable for people wanting a more specific and custom install.&lt;/p&gt; &lt;p&gt;The new app tools are a &lt;strong&gt;game-changer&lt;/strong&gt;!! You can now have direct system-level pop ups or notifications that come up right &lt;strong&gt;up to your face&lt;/strong&gt; hahaha. And sorry to everyone who tried out SMS and WhatsApp and were frustrated because you weren't getting notifications, Meta started blocking my account thinking i was just spamming messages to you guys.&lt;/p&gt; &lt;p&gt;But the pushover and discord notifications work perfectly well!&lt;/p&gt; &lt;p&gt;If you have any feedback please reach out through the discord, i'm really open to suggestions.&lt;/p&gt; &lt;p&gt;This is the projects &lt;a href="https://github.com/Roy3838/Observer"&gt;Github&lt;/a&gt; (completely open source)&lt;br /&gt; And the discord: &lt;a href="https://discord.gg/wnBb7ZQDUC"&gt;https://discord.gg/wnBb7ZQDUC&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you have any questions i'll be hanging out here for a while!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Roy3838"&gt; /u/Roy3838 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/g3pod2zlw2hf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhrx3m/how_to_use_your_local_models_to_watch_your_screen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mhrx3m/how_to_use_your_local_models_to_watch_your_screen/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-04T22:30:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1mhl5yo</id>
    <title>Gemini 3 is coming?..</title>
    <updated>2025-08-04T18:15:40+00:00</updated>
    <author>
      <name>/u/SlerpE</name>
      <uri>https://old.reddit.com/user/SlerpE</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhl5yo/gemini_3_is_coming/"&gt; &lt;img alt="Gemini 3 is coming?.." src="https://preview.redd.it/59joqndkn1hf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1e89c768daaac9653e4f2ad00c6a0ee5f6412107" title="Gemini 3 is coming?.." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/OfficialLoganK/status/1952430214375493808"&gt;https://x.com/OfficialLoganK/status/1952430214375493808&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SlerpE"&gt; /u/SlerpE &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/59joqndkn1hf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhl5yo/gemini_3_is_coming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mhl5yo/gemini_3_is_coming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-04T18:15:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1mhbpmo</id>
    <title>New Qwen Models Today!!!</title>
    <updated>2025-08-04T12:12:00+00:00</updated>
    <author>
      <name>/u/R46H4V</name>
      <uri>https://old.reddit.com/user/R46H4V</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhbpmo/new_qwen_models_today/"&gt; &lt;img alt="New Qwen Models Today!!!" src="https://preview.redd.it/qemmgysvuzgf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9e45a424e82bdde4384e3d7ba1be6631b2a25639" title="New Qwen Models Today!!!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/R46H4V"&gt; /u/R46H4V &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/qemmgysvuzgf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhbpmo/new_qwen_models_today/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mhbpmo/new_qwen_models_today/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-04T12:12:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1mhlkyx</id>
    <title>support for GLM 4.5 family of models has been merged into llama.cpp</title>
    <updated>2025-08-04T18:30:31+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhlkyx/support_for_glm_45_family_of_models_has_been/"&gt; &lt;img alt="support for GLM 4.5 family of models has been merged into llama.cpp" src="https://external-preview.redd.it/c5JLWNvDayy9hBNWlkTcKlG0BX-MgLkUBV-jJh9mTeo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e04ffa9cbd0a435f87d74eaf876a5853c1e06023" title="support for GLM 4.5 family of models has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/14939"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhlkyx/support_for_glm_45_family_of_models_has_been/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mhlkyx/support_for_glm_45_family_of_models_has_been/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-04T18:30:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1mhe1rl</id>
    <title>r/LocalLLaMA right now</title>
    <updated>2025-08-04T13:52:26+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhe1rl/rlocalllama_right_now/"&gt; &lt;img alt="r/LocalLLaMA right now" src="https://preview.redd.it/f0xr7mshc0hf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a0d701cbdf33b1ec6ea47dd4b4874202dbea647e" title="r/LocalLLaMA right now" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/f0xr7mshc0hf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhe1rl/rlocalllama_right_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mhe1rl/rlocalllama_right_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-04T13:52:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1mi1vov</id>
    <title>Qwen-image now supported in ComfyUI</title>
    <updated>2025-08-05T06:37:16+00:00</updated>
    <author>
      <name>/u/Lopsided_Dot_4557</name>
      <uri>https://old.reddit.com/user/Lopsided_Dot_4557</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;At last after wait of few hours, ComfyUI now has support for Qwen-Image. Its from their &lt;a href="https://github.com/comfyanonymous/ComfyUI/pull/9179"&gt;git repo&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lopsided_Dot_4557"&gt; /u/Lopsided_Dot_4557 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mi1vov/qwenimage_now_supported_in_comfyui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mi1vov/qwenimage_now_supported_in_comfyui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mi1vov/qwenimage_now_supported_in_comfyui/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T06:37:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1mht910</id>
    <title>GLM 4.5 GGUFs are coming</title>
    <updated>2025-08-04T23:26:08+00:00</updated>
    <author>
      <name>/u/Pro-editor-1105</name>
      <uri>https://old.reddit.com/user/Pro-editor-1105</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mht910/glm_45_ggufs_are_coming/"&gt; &lt;img alt="GLM 4.5 GGUFs are coming" src="https://external-preview.redd.it/mPuzW0dQMIeYzrva9cFzmx9vYSbfW4-X3nbfzwnmTUI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cbaa78bb76999536a7337e9b0c9e2f578691b200" title="GLM 4.5 GGUFs are coming" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;FINALLY&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pro-editor-1105"&gt; /u/Pro-editor-1105 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/mradermacher/GLM-4.5-Air-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mht910/glm_45_ggufs_are_coming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mht910/glm_45_ggufs_are_coming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-04T23:26:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1mi45h1</id>
    <title>Kitten TTS Web Demo</title>
    <updated>2025-08-05T09:04:22+00:00</updated>
    <author>
      <name>/u/CommunityTough1</name>
      <uri>https://old.reddit.com/user/CommunityTough1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I made a quick web demo of the new &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1mhyzp7/kitten_tts_sota_supertiny_tts_model_less_than_25/"&gt;Kitten TTS&lt;/a&gt;. Loads the model up using transformers.js in the browser, running fully locally client-side: &lt;a href="https://clowerweb.github.io/kitten-tts-web-demo/"&gt;https://clowerweb.github.io/kitten-tts-web-demo/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/clowerweb/kitten-tts-web-demo"&gt;https://github.com/clowerweb/kitten-tts-web-demo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Only uses CPU for now, but I'm going to add WebGPU support for it later today, plus maybe a Whisper implementation also in transformers.js for a nice little local STS pipeline, if anyone is interested in something like that.&lt;/p&gt; &lt;p&gt;I also have a little open-source chat interface in progress that I might plop the STS pipeline into here: &lt;a href="https://github.com/clowerweb/Simple-AI"&gt;https://github.com/clowerweb/Simple-AI&lt;/a&gt; (built with Nuxt 3 &amp;amp; Tailwind 4) -- supports chat tabs &amp;amp; history, markdown, code highlighting, and LaTeX, and also lets you run Qwen3 4B via transformers.js or add your own custom API endpoints, with settings for temperature, top_p, top_k, etc. Only supports OpenAI-compatible endpoints currently. You can add custom API providers (including your own llama.cpp servers and whatnot), custom models with their own settings, custom system prompts, etc. If you're interested in seeing an STS pipeline added to that though with Kitten &amp;amp; Whisper, lemme know what the interest levels are for something like that. I'll probably toss this project into Electron when it's ready and make it into a desktop app for Mac, Windows, and Linux as well.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CommunityTough1"&gt; /u/CommunityTough1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mi45h1/kitten_tts_web_demo/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mi45h1/kitten_tts_web_demo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mi45h1/kitten_tts_web_demo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T09:04:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1mhhctd</id>
    <title>üöÄ Meet Qwen-Image</title>
    <updated>2025-08-04T15:58:11+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhhctd/meet_qwenimage/"&gt; &lt;img alt="üöÄ Meet Qwen-Image" src="https://preview.redd.it/7a463it8z0hf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2a9bc46837ad4e1dac08bb6879d131c650ac6476" title="üöÄ Meet Qwen-Image" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;üöÄ Meet Qwen-Image ‚Äî a 20B MMDiT model for next-gen text-to-image generation. Especially strong at creating stunning graphic posters with native text. Now open-source.&lt;/p&gt; &lt;p&gt;üîç Key Highlights:&lt;/p&gt; &lt;p&gt;üîπ SOTA text rendering ‚Äî rivals GPT-4o in English, best-in-class for Chinese&lt;/p&gt; &lt;p&gt;üîπ In-pixel text generation ‚Äî no overlays, fully integrated&lt;/p&gt; &lt;p&gt;üîπ Bilingual support, diverse fonts, complex layouts&lt;/p&gt; &lt;p&gt;üé® Also excels at general image generation ‚Äî from photorealistic to anime, impressionist to minimalist. A true creative powerhouse.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7a463it8z0hf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhhctd/meet_qwenimage/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mhhctd/meet_qwenimage/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-04T15:58:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1mhgu6t</id>
    <title>Sam Altman watching Qwen drop model after model</title>
    <updated>2025-08-04T15:38:39+00:00</updated>
    <author>
      <name>/u/TheRealSerdra</name>
      <uri>https://old.reddit.com/user/TheRealSerdra</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhgu6t/sam_altman_watching_qwen_drop_model_after_model/"&gt; &lt;img alt="Sam Altman watching Qwen drop model after model" src="https://preview.redd.it/g7t8cmgrv0hf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=460e01fe2091b6bc2347e41a918d258022a40353" title="Sam Altman watching Qwen drop model after model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheRealSerdra"&gt; /u/TheRealSerdra &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/g7t8cmgrv0hf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhgu6t/sam_altman_watching_qwen_drop_model_after_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mhgu6t/sam_altman_watching_qwen_drop_model_after_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-04T15:38:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1mhiqqn</id>
    <title>Qwen-Image is out</title>
    <updated>2025-08-04T16:49:14+00:00</updated>
    <author>
      <name>/u/BoJackHorseMan53</name>
      <uri>https://old.reddit.com/user/BoJackHorseMan53</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhiqqn/qwenimage_is_out/"&gt; &lt;img alt="Qwen-Image is out" src="https://external-preview.redd.it/ZXc5MXNwZzA4MWhmMcYwrNxnpQZAm2APaO7BYeGkDJuDLh9yjPxalcFVQ96q.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2392cbc2ee2f68a83914b65971bd3a688fc24739" title="Qwen-Image is out" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/Alibaba_Qwen/status/1952398250121756992"&gt;https://x.com/Alibaba_Qwen/status/1952398250121756992&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It's better than Flux Kontext, gpt-image level&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BoJackHorseMan53"&gt; /u/BoJackHorseMan53 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/4077mfg081hf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhiqqn/qwenimage_is_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mhiqqn/qwenimage_is_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-04T16:49:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1mhhdig</id>
    <title>QWEN-IMAGE is released!</title>
    <updated>2025-08-04T15:58:55+00:00</updated>
    <author>
      <name>/u/TheIncredibleHem</name>
      <uri>https://old.reddit.com/user/TheIncredibleHem</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhhdig/qwenimage_is_released/"&gt; &lt;img alt="QWEN-IMAGE is released!" src="https://external-preview.redd.it/qvxd1_x1PaBd3IEw-2xS9ifjngcFBwLHvsX1ihQDi64.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a65aaca919e956009709dd069f70c0c907403912" title="QWEN-IMAGE is released!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;and it's better than Flux Kontext Pro (according to their benchmarks). That's insane. Really looking forward to it. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheIncredibleHem"&gt; /u/TheIncredibleHem &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Qwen/Qwen-Image"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhhdig/qwenimage_is_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mhhdig/qwenimage_is_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-04T15:58:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1mi1fdc</id>
    <title>DFLoat11 Quantization for Qwen-Image Drops ‚Äì Run It on 17GB VRAM with CPU Offloading!</title>
    <updated>2025-08-05T06:09:21+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mi1fdc/dfloat11_quantization_for_qwenimage_drops_run_it/"&gt; &lt;img alt="DFLoat11 Quantization for Qwen-Image Drops ‚Äì Run It on 17GB VRAM with CPU Offloading!" src="https://preview.redd.it/sv779zmy65hf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=935b56fc5932d42d92b2f9647262c1937a3e7446" title="DFLoat11 Quantization for Qwen-Image Drops ‚Äì Run It on 17GB VRAM with CPU Offloading!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sv779zmy65hf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mi1fdc/dfloat11_quantization_for_qwenimage_drops_run_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mi1fdc/dfloat11_quantization_for_qwenimage_drops_run_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T06:09:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1mi0luy</id>
    <title>generated using Qwen</title>
    <updated>2025-08-05T05:20:23+00:00</updated>
    <author>
      <name>/u/Vision--SuperAI</name>
      <uri>https://old.reddit.com/user/Vision--SuperAI</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mi0luy/generated_using_qwen/"&gt; &lt;img alt="generated using Qwen" src="https://b.thumbs.redditmedia.com/xCXpPKDOF28RU5xW4gyR6Ubwr1ZXuMCJDS0Yp62MVtI.jpg" title="generated using Qwen" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vision--SuperAI"&gt; /u/Vision--SuperAI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mi0luy"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mi0luy/generated_using_qwen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mi0luy/generated_using_qwen/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T05:20:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1mi0co2</id>
    <title>Anthropic's CEO dismisses open source as 'red herring' - but his reasoning seems to miss the point entirely!</title>
    <updated>2025-08-05T05:06:08+00:00</updated>
    <author>
      <name>/u/MrJiks</name>
      <uri>https://old.reddit.com/user/MrJiks</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mi0co2/anthropics_ceo_dismisses_open_source_as_red/"&gt; &lt;img alt="Anthropic's CEO dismisses open source as 'red herring' - but his reasoning seems to miss the point entirely!" src="https://preview.redd.it/9z1vbpnsu4hf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bafb282a6194808b25822f60262b2b9d1dd1570e" title="Anthropic's CEO dismisses open source as 'red herring' - but his reasoning seems to miss the point entirely!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From Dario Amodei's recent interview on Big Technology Podcast discussing open source AI models. Thoughts on this reasoning?&lt;/p&gt; &lt;p&gt;Source: &lt;a href="https://x.com/jikkujose/status/1952588432280051930"&gt;https://x.com/jikkujose/status/1952588432280051930&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrJiks"&gt; /u/MrJiks &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9z1vbpnsu4hf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mi0co2/anthropics_ceo_dismisses_open_source_as_red/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mi0co2/anthropics_ceo_dismisses_open_source_as_red/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T05:06:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1mhyzp7</id>
    <title>Kitten TTS : SOTA Super-tiny TTS Model (Less than 25 MB)</title>
    <updated>2025-08-05T03:52:26+00:00</updated>
    <author>
      <name>/u/ElectricalBar7464</name>
      <uri>https://old.reddit.com/user/ElectricalBar7464</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhyzp7/kitten_tts_sota_supertiny_tts_model_less_than_25/"&gt; &lt;img alt="Kitten TTS : SOTA Super-tiny TTS Model (Less than 25 MB)" src="https://external-preview.redd.it/ajlvMGd2aWhpNGhmMUpar5lWZvhVHx9_BWGYhGbOyuld4cLO275_Q90LHrwX.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fc51726f166f8072e9a0767410a3b105af874a6d" title="Kitten TTS : SOTA Super-tiny TTS Model (Less than 25 MB)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Model introduction:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Kitten ML has released open source code and weights of their new TTS model's preview.&lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/KittenML/KittenTTS"&gt;https://github.com/KittenML/KittenTTS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Huggingface: &lt;a href="https://huggingface.co/KittenML/kitten-tts-nano-0.1"&gt;https://huggingface.co/KittenML/kitten-tts-nano-0.1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The model is less than 25 MB, around 15M parameters. The full release next week will include another open source ~80M parameter model with these same 8 voices, that can also run on CPU.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key features and Advantages&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Eight Different Expressive voices - 4 female and 4 male voices. For a tiny model, the expressivity sounds pretty impressive. This release will support TTS in English and multilingual support expected in future releases.&lt;/li&gt; &lt;li&gt;Super-small in size: The two text to speech models will be ~15M and ~80M parameters .&lt;/li&gt; &lt;li&gt;Can literally run anywhere lol : Forget ‚ÄúNo gpu required.‚Äù - this thing can even run on raspberry pi‚Äôs and phones. Great news for gpu-poor folks like me.&lt;/li&gt; &lt;li&gt;Open source (hell yeah!): the model can used for free.&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ElectricalBar7464"&gt; /u/ElectricalBar7464 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/vdfv5uihi4hf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhyzp7/kitten_tts_sota_supertiny_tts_model_less_than_25/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mhyzp7/kitten_tts_sota_supertiny_tts_model_less_than_25/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T03:52:26+00:00</published>
  </entry>
</feed>
