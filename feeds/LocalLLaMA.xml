<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-19T12:32:41+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qgv2ey</id>
    <title>BFL FLUX.2 Klein tutorial and some optimizations - under 1s latency on an A100</title>
    <updated>2026-01-19T05:04:18+00:00</updated>
    <author>
      <name>/u/LayerHot</name>
      <uri>https://old.reddit.com/user/LayerHot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A quick tutorial on running FLUX.2 Klein (the new BFL model from last week). Here's what we're seeing on A100:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;4B distilled&lt;/strong&gt;: ~0.9s per image (1024x1024, 4 steps) with torch.compile + fused QKV&lt;/li&gt; &lt;li&gt;&lt;strong&gt;9B distilled&lt;/strong&gt;: ~1.8s per image with same optimizations&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;These models are pretty good and fast for basic image generation (the 4B model sometimes messes up the image structure, but works quite well for it's size)&lt;/p&gt; &lt;p&gt;We put together Gradio and FastAPI scripts with the optimizations: &lt;a href="https://docs.jarvislabs.ai/tutorials/running-flux2-klein"&gt;https://docs.jarvislabs.ai/tutorials/running-flux2-klein&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LayerHot"&gt; /u/LayerHot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgv2ey/bfl_flux2_klein_tutorial_and_some_optimizations/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgv2ey/bfl_flux2_klein_tutorial_and_some_optimizations/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qgv2ey/bfl_flux2_klein_tutorial_and_some_optimizations/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T05:04:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1qh1a0c</id>
    <title>Ollama nooby in need of help</title>
    <updated>2026-01-19T11:02:50+00:00</updated>
    <author>
      <name>/u/Schaksie</name>
      <uri>https://old.reddit.com/user/Schaksie</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried to pull a model from Hugging Face with Ollama on a Raspberry Pi 5, just playing around, please don't judge, and I am constantly getting the Error 429 rate limit from your address. I already tried adding a Hugging Face token, which apparently didn't change anything; still getting the error. On the Pi, I was testing Raspberry Pi OS and Ubuntu 25.10.&lt;/p&gt; &lt;p&gt;I also tested the py over a hotspot to see if the IP was the issue, but as with the token, nothing changed. Also waiting a day or two to see if there was a used-up rate limit didn’t change the result, still 429.&lt;/p&gt; &lt;p&gt;Interestingly enough, I am running Ollama on my PC, running on Ubuntu, and pulling the same model does not result in the same Error 429.&lt;/p&gt; &lt;p&gt;Model I tried to pull: &lt;a href="https://huggingface.co/byteshape/Qwen3-30B-A3B-Instruct-2507-GGUF/blob/main/Qwen3-30B-A3B-Instruct-2507-Q3_K_S-2.70bpw.gguf"&gt;https://huggingface.co/byteshape/Qwen3-30B-A3B-Instruct-2507-GGUF/blob/main/Qwen3-30B-A3B-Instruct-2507-Q3_K_S-2.70bpw.gguf&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Schaksie"&gt; /u/Schaksie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh1a0c/ollama_nooby_in_need_of_help/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh1a0c/ollama_nooby_in_need_of_help/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qh1a0c/ollama_nooby_in_need_of_help/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T11:02:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1qh1p49</id>
    <title>Something Intresting</title>
    <updated>2026-01-19T11:26:30+00:00</updated>
    <author>
      <name>/u/USERNAMETAKEN11238</name>
      <uri>https://old.reddit.com/user/USERNAMETAKEN11238</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried to automate my job 10 years ago and came across a way to write responses. (I have an administrative job (AML)). I coded some really impressive things and eventually got to where I can code functions of my job completely. I have since worked in several departments and have brought my code to write most aspects of what I do professionally. Which is interesting as I am not a coder, nor when I started was there a concept of an LLM. &lt;/p&gt; &lt;p&gt;Anyway, I have been trying to understand code and LLMs, and I believe I have a process that is better at some aspects of LLMs. some of the things my code does is:&lt;/p&gt; &lt;p&gt;correct itself (recersive) &lt;/p&gt; &lt;p&gt;understand context&lt;/p&gt; &lt;p&gt;it can use symbols, text, or numbers as data. &lt;/p&gt; &lt;p&gt;it can be used in several applications&lt;/p&gt; &lt;p&gt;leverages negitive inferences and dynamic states.&lt;/p&gt; &lt;p&gt;dynamic compilation &lt;/p&gt; &lt;p&gt;can do it locally as the logic is formulaic&lt;/p&gt; &lt;p&gt;I call the mechine a quantom madlib choose your own adventure story. &lt;/p&gt; &lt;p&gt;Anyway, I was looking to release it into the public domain and just create a github as I don't have a CS background, and I think the application is larger than what I would use it for. &lt;/p&gt; &lt;p&gt;I finished aspects of the mechine many years ago and have been testing on automating aspects of the law (as i am educted as an attorney).. however, i made a customer service bot also. The logic is that it is able to navigate a conversation really well. it's also self correcting... &lt;/p&gt; &lt;p&gt;I had a lot of trouble with scale and compute, and I am not looking to get profits off of the idea or patent it (as I believe there is a large social value). therefore, I will be working on writing my own github and white paper discussing my findings. &lt;/p&gt; &lt;p&gt;I dont have a CS background and am more interested in how languages work. but because I don't know a lot about that I was wondering if someone can speak with me about it. in hopes of understanding what this is? &lt;/p&gt; &lt;p&gt;I have only built aspects of the mechine and have not created a working model (IP reasons), but I hope that maybe people can learn my system and replicate it. &lt;/p&gt; &lt;p&gt;I am working and am not well, so I will try to respond as I can. please be patient with me. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/USERNAMETAKEN11238"&gt; /u/USERNAMETAKEN11238 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh1p49/something_intresting/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh1p49/something_intresting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qh1p49/something_intresting/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T11:26:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1qh254m</id>
    <title>Is Token-based CoT going to Die? My 2026 Prediction for the next generation of LLMs &amp; VLMs - A Deep-Dive into the rise of Latent Reasoning.</title>
    <updated>2026-01-19T11:51:26+00:00</updated>
    <author>
      <name>/u/madSaiyanUltra_9789</name>
      <uri>https://old.reddit.com/user/madSaiyanUltra_9789</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh254m/is_tokenbased_cot_going_to_die_my_2026_prediction/"&gt; &lt;img alt="Is Token-based CoT going to Die? My 2026 Prediction for the next generation of LLMs &amp;amp; VLMs - A Deep-Dive into the rise of Latent Reasoning." src="https://b.thumbs.redditmedia.com/eHD5Q8sZzMc8BvlNifGDZHN9NqVtbd4A_3fFspIH6NU.jpg" title="Is Token-based CoT going to Die? My 2026 Prediction for the next generation of LLMs &amp;amp; VLMs - A Deep-Dive into the rise of Latent Reasoning." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;For the past few years, we’ve lived in the era of Chain-of-Thought (CoT) which forced models to &amp;quot;show their work&amp;quot; token-by-token to solve complex problems. It &amp;quot;works&amp;quot; but it’s slow, expensive, inefficient and limited by the &amp;quot;one-to-one&amp;quot; law of autoregression.&lt;/p&gt; &lt;p&gt;Based on three papers released this week (January 2026), I'm agreeing with the prediction that a massive architectural shift is coming for LLMs/VLMs in 2026 (made on &lt;a href="https://www.youtube.com/watch?v=O9HxArmWChs"&gt;Discover-AI&lt;/a&gt; video), in 2026 we will likely move from Simulating Reasoning (imitating human speech) to Optimizing Reasoning (pure vector operations).&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Reasoning is a &amp;quot;State of Mind,&amp;quot; Not a Word Cloud (&lt;a href="https://arxiv.org/abs/2601.08058"&gt;Source_1&lt;/a&gt;)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Recent research from the University of Virginia proves that reasoning is actually a specific latent configuration within the model. By using Sparse Autoencoders (SAEs), researchers identified &amp;quot;Feature #8629&amp;quot; in LLaMA-3 - a literal &amp;quot;Reasoning Mode&amp;quot; switch.&lt;/p&gt; &lt;p&gt;When they &amp;quot;hot-wired&amp;quot; this switch using latent steering, the model solved complex math without needing the &amp;quot;Let's think step by step&amp;quot; prompt and, more importantly, without generating the verbose text. Reasoning happened &lt;em&gt;silently&lt;/em&gt; in the residual stream.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Enter &amp;quot;Schrödinger’s Token&amp;quot; (Multiplex Thinking) (&lt;a href="https://arxiv.org/abs/2601.08808"&gt;Source_2&lt;/a&gt;)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;UPenn and Microsoft just introduced Multiplex Thinking. In standard CoT, the model acts as a sequential processor; if it picks the wrong path at a fork, the chain breaks.&lt;/p&gt; &lt;p&gt;In 2026, we’re moving to superposition vectors. The model can now carry multiple reasoning timelines (e.g., &amp;quot;Multiply&amp;quot; AND &amp;quot;Add&amp;quot;) simultaneously within the same high-dimensional wire. This &amp;quot;Breadth-First Search&amp;quot; approach makes reasoning far more robust by exploring every option at once in a single flow of vectors.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;The 30x Compression: NVIDIA’s Latent Planning (&lt;a href="https://arxiv.org/abs/2601.09708v1"&gt;Source_3&lt;/a&gt;)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;NVIDIA’s new &amp;quot;Fast-ThinkAct&amp;quot; architecture is perhaps the most practical leap. They’ve found a way to compress a 200-token reasoning plan (about 800,000 numbers) into just 6 Latent Tokens (roughly 25,000 numbers).&lt;/p&gt; &lt;p&gt;This is a 30x compression that allows robots to &amp;quot;think&amp;quot; at 10Hz without the latency of generating English sentences. To keep it safe, they use a &amp;quot;Verbalizer Lock&amp;quot;, ensuring this internal &amp;quot;alien logic&amp;quot; remains homeomorphic to human language even if we don't read the raw vectors.&lt;/p&gt; &lt;p&gt;My Prediction for the Next-gen Architecture of LLMs/VLMs&lt;/p&gt; &lt;p&gt;I believe the next generation of LLMs will treat text merely as an I/O layer, not the processing layer. The &amp;quot;GPT-6&amp;quot; pipeline will likely look like this:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Input: User Query.&lt;/li&gt; &lt;li&gt;Trigger: An SAE classifier steers the initial state to &amp;quot;Reasoning Mode&amp;quot;.&lt;/li&gt; &lt;li&gt;Latent Processing: The model generates a short sequence of Multiplexed Latent Tokens - compressed and containing multiple hypotheses in superposition.&lt;/li&gt; &lt;li&gt;Output: The final dense vectors project directly to an action head (robotics) or a language head (final answer).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;TLDR:&lt;/strong&gt; We are moving from Symbolic Processing to Signal Processing. The &amp;quot;Silent Thought&amp;quot; AI era will arrive soon.&lt;/p&gt; &lt;p&gt;This is merely conjecture but what do you think?&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9yd5czupnaeg1.png?width=2400&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=55804df5fa765e4506f725a6892c023c16a490ff"&gt;https://preview.redd.it/9yd5czupnaeg1.png?width=2400&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=55804df5fa765e4506f725a6892c023c16a490ff&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/madSaiyanUltra_9789"&gt; /u/madSaiyanUltra_9789 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh254m/is_tokenbased_cot_going_to_die_my_2026_prediction/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh254m/is_tokenbased_cot_going_to_die_my_2026_prediction/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qh254m/is_tokenbased_cot_going_to_die_my_2026_prediction/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T11:51:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1qgv0vk</id>
    <title>Has anyone quantized VibeVoice-Realtime-0.5B (Stream) for edge devices yet?</title>
    <updated>2026-01-19T05:02:09+00:00</updated>
    <author>
      <name>/u/New_Source_6765</name>
      <uri>https://old.reddit.com/user/New_Source_6765</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking for a quantized version (GGUF or ONNX) of the &lt;strong&gt;Microsoft VibeVoice-Realtime-0.5B&lt;/strong&gt; model to run on an SBC (Orange Pi).&lt;/p&gt; &lt;p&gt;I've seen some repos for the 7B version, but I specifically need the lightweight 0.5B stream version for an edge project. Has anyone successfully converted this, or can point me to a guide on how to quantize this specific architecture?&lt;/p&gt; &lt;p&gt;thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/New_Source_6765"&gt; /u/New_Source_6765 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgv0vk/has_anyone_quantized_vibevoicerealtime05b_stream/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgv0vk/has_anyone_quantized_vibevoicerealtime05b_stream/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qgv0vk/has_anyone_quantized_vibevoicerealtime05b_stream/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T05:02:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1qg8yoh</id>
    <title>The sad state of the GPU market in Germany and EU, some of them are not even available</title>
    <updated>2026-01-18T13:45:52+00:00</updated>
    <author>
      <name>/u/HumanDrone8721</name>
      <uri>https://old.reddit.com/user/HumanDrone8721</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qg8yoh/the_sad_state_of_the_gpu_market_in_germany_and_eu/"&gt; &lt;img alt="The sad state of the GPU market in Germany and EU, some of them are not even available" src="https://preview.redd.it/9mmc603p34eg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7fe5b9d5a452e5ddb136ff1be2c60fdc5b0ed2c5" title="The sad state of the GPU market in Germany and EU, some of them are not even available" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HumanDrone8721"&gt; /u/HumanDrone8721 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9mmc603p34eg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qg8yoh/the_sad_state_of_the_gpu_market_in_germany_and_eu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qg8yoh/the_sad_state_of_the_gpu_market_in_germany_and_eu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-18T13:45:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1qgzjpf</id>
    <title>Good NSFW LLM for writing story on a strix halo?</title>
    <updated>2026-01-19T09:19:43+00:00</updated>
    <author>
      <name>/u/GiumboJet</name>
      <uri>https://old.reddit.com/user/GiumboJet</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello. I just myself one of those strix halo apu's with shared memory (mine is a 64gb model which I split in 32gb as vram and 32 as system ram). I want to test out some local llm with it to see what it's capable off. I am new to this and heard that some models are heavy, other more lightweight... Basically looking for something that can write mature/explicit content (like a fantasy anime hentai, or explicit s** scenes). What options are out there? There are literally millions of models&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GiumboJet"&gt; /u/GiumboJet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgzjpf/good_nsfw_llm_for_writing_story_on_a_strix_halo/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgzjpf/good_nsfw_llm_for_writing_story_on_a_strix_halo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qgzjpf/good_nsfw_llm_for_writing_story_on_a_strix_halo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T09:19:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1qgnbx8</id>
    <title>Textual game world generation Instructor pipeline</title>
    <updated>2026-01-18T23:09:08+00:00</updated>
    <author>
      <name>/u/JEs4</name>
      <uri>https://old.reddit.com/user/JEs4</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgnbx8/textual_game_world_generation_instructor_pipeline/"&gt; &lt;img alt="Textual game world generation Instructor pipeline" src="https://b.thumbs.redditmedia.com/USiUjlxVILJBRKiTUNitW_RRdgq53FnRT-0iGK8J3CE.jpg" title="Textual game world generation Instructor pipeline" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I threw together an instructor/pydantic pipeline for generating interconnected RPG world content using a local LM.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/jwest33/lm_world_gen"&gt;https://github.com/jwest33/lm_world_gen&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It starts from a high concept you define in a yaml file, and it iteratively generates regions, factions, characters, and branching dialog trees that all reference each other consistently using an in-memory (sqlite) fact registry.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Generates structured JSON content using Pydantic schemas + Instructor&lt;/li&gt; &lt;li&gt;Two-phase generation (skeletons first, then expansion) to ensure variety &lt;ul&gt; &lt;li&gt;This was pretty key as trying to generate complete branches resulted in far too little variety despite efforts to alter context dynamically (seeds, temp walking, context filling etc)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;SQLite (in-memory) fact registry prevents contradictions across generations&lt;/li&gt; &lt;li&gt;Saves progress incrementally so you can resume interrupted runs&lt;/li&gt; &lt;li&gt;Web-based viewer/editor for browsing and regenerating content&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It should work with any OpenAI-compatible API but I only used llama.cpp.&lt;/p&gt; &lt;p&gt;The example below (full json is in the repo with the config file too) was generated using off-the-shelf gemma-27b-it in a single pass. It is has 5 regions, 8 factions, 50 characters, 50 dialogs, and 1395 canonical facts.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/i8hs04swv6eg1.jpg?width=1248&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=186f9f17ff1a81e4ad8ca02b4bfcf8bbbc01bac6"&gt;https://preview.redd.it/i8hs04swv6eg1.jpg?width=1248&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=186f9f17ff1a81e4ad8ca02b4bfcf8bbbc01bac6&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/r0wktvjyv6eg1.jpg?width=2079&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=121a2a29605c726ab518e2af2d066e9291241d26"&gt;https://preview.redd.it/r0wktvjyv6eg1.jpg?width=2079&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=121a2a29605c726ab518e2af2d066e9291241d26&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/sal25j9zv6eg1.jpg?width=2067&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=ca980f560e16b86ed13691b6338f6e02bacc2cd4"&gt;https://preview.redd.it/sal25j9zv6eg1.jpg?width=2067&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=ca980f560e16b86ed13691b6338f6e02bacc2cd4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/w7kjv4uzv6eg1.jpg?width=2104&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=516f7ae120f463a9b98527fdd6d1938bb8e7afc8"&gt;https://preview.redd.it/w7kjv4uzv6eg1.jpg?width=2104&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=516f7ae120f463a9b98527fdd6d1938bb8e7afc8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ci700n60w6eg1.jpg?width=2104&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=fb6b7537ac9c6681744638a365d716fac64a4ac2"&gt;https://preview.redd.it/ci700n60w6eg1.jpg?width=2104&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=fb6b7537ac9c6681744638a365d716fac64a4ac2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Anyway, I didn’t spend any time optimizing since I’m just using it for a game I’m building so it’s a bit slow, but while it’s not perfect, I found it to be much more useful then I expected so I figured I’d share.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JEs4"&gt; /u/JEs4 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgnbx8/textual_game_world_generation_instructor_pipeline/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgnbx8/textual_game_world_generation_instructor_pipeline/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qgnbx8/textual_game_world_generation_instructor_pipeline/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-18T23:09:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1qgyrt2</id>
    <title>Agent Zero Discussion</title>
    <updated>2026-01-19T08:32:29+00:00</updated>
    <author>
      <name>/u/Noobysz</name>
      <uri>https://old.reddit.com/user/Noobysz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;so i discovered Agent Zero 2 days ago, got it up and running saw what it can do with models like Claude opus *top high end tier*.&lt;/p&gt; &lt;p&gt;but id like to run it locally i have 84 gb of VRAM (3x 3090, 1 4070ti) 96 gb of RAM ,CPU i7 13k , i tried gptoss 120b as chat model, it was ok but very restrictive, and i used llama 3.1 8b for utility ( i think this was my biggest problem need to try today a bit stronger model than that) and nomic embeder which is also i think very buggy with it (but i wanted GPU embeding ) because i noticed the longest step was for me the memmory processing step, yet the most capable almost as calude opus trial youtube video i saw was GLM4.7 of course Q2KL quantize and that is almost all my VRAM (81 gb), its really capable as i saw but halucinate i think because of the low quant and is running at 7-11 Tokens /sec on lammacpp. &lt;/p&gt; &lt;p&gt;i also try to utilize the bigger model on lamacpp since its faster and the others on ollama so i dont have to keep loading and unloading models in the GPU for speed.&lt;/p&gt; &lt;p&gt;im thinking of trying GLM Air since its a bit smaller so i can run a better Quant on my hardware.&lt;/p&gt; &lt;p&gt;but my frustration untill now that it starts good (with GLM 4.7) and get some really good planning and start working but at somepoint it starts halucinations and stops and i dont get any result and tbh also untill now i didnt really get any materialistic result i even tried to ask it to make a notepad txt file and write a word in it even that didnt get to work somehow xD, keeps halucinating and repeating its thoughts with gpt oss 120b i didnt try this simple task with GLM yet.&lt;/p&gt; &lt;p&gt;but i just wanted to open a discussion and see what people use if there is better opensource apps like Agentzero, whats their model combination for Agent zero, Context and parameters so it works reliably localy, recommendations for my written hardware.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Noobysz"&gt; /u/Noobysz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgyrt2/agent_zero_discussion/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgyrt2/agent_zero_discussion/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qgyrt2/agent_zero_discussion/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T08:32:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1qh1b8e</id>
    <title>Best open-source voice cloning model with emotional control? (Worked with VibeVoice 7B &amp; 1.5B)</title>
    <updated>2026-01-19T11:04:43+00:00</updated>
    <author>
      <name>/u/Junior-Media-8668</name>
      <uri>https://old.reddit.com/user/Junior-Media-8668</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I’ve been working with open-source voice cloning models and have some experience&lt;/p&gt; &lt;p&gt;with **VibeVoice 7B and 1.5B**, but I’m still looking for something that delivers&lt;/p&gt; &lt;p&gt;**better emotional expression and natural prosody**.&lt;/p&gt; &lt;p&gt;My main goals:&lt;/p&gt; &lt;p&gt;- High-quality voice cloning (few-shot or zero-shot)&lt;/p&gt; &lt;p&gt;- Strong emotional control (e.g., happy, sad, calm, expressive storytelling)&lt;/p&gt; &lt;p&gt;- Natural pacing and intonation (not flat or robotic)&lt;/p&gt; &lt;p&gt;- Good for long-form narration / audiobooks&lt;/p&gt; &lt;p&gt;- Open-source models preferred&lt;/p&gt; &lt;p&gt;I’ve seen mentions of models like XTTS v2, StyleTTS 2, OpenVoice, Bark, etc.,&lt;/p&gt; &lt;p&gt;but I’d love to hear from people who’ve used them in practice.&lt;/p&gt; &lt;p&gt;**What open-source model would you recommend now (2025) for my use case**, and&lt;/p&gt; &lt;p&gt;why? Any comparisons, demos, or benchmarks would be awesome too.&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Junior-Media-8668"&gt; /u/Junior-Media-8668 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh1b8e/best_opensource_voice_cloning_model_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh1b8e/best_opensource_voice_cloning_model_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qh1b8e/best_opensource_voice_cloning_model_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T11:04:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1qgbkcd</id>
    <title>Running language models where they don't belong</title>
    <updated>2026-01-18T15:33:03+00:00</updated>
    <author>
      <name>/u/Brief_Argument8155</name>
      <uri>https://old.reddit.com/user/Brief_Argument8155</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We have seen a cool counter-trend recently to the typical scaleup narrative (see Smol/Phi and ZIT most notably). I've been on a mission to push this to the limit (mainly for fun), moving LMs into environments where they have no business existing.&lt;/p&gt; &lt;p&gt;My thesis is that even the most primitive environments can host generative capabilities if you bake them in correctly.&lt;/p&gt; &lt;p&gt;So here goes:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. The NES LM (inference on 1983 hardware)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I started by writing a char-level bigram model in straight 6502 asm for the original Nintendo Entertainment System.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;2KB of RAM and a CPU with no multiplication opcode, let alone float math.&lt;/li&gt; &lt;li&gt;The model compresses a name space of 18 million possibilities into a footprint smaller than a Final Fantasy black mage sprite (729 bytes of weights).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For extra fun I packaged it into a romhack for Final Fantasy I and Dragon Warrior to generate fantasy names at game time, on original hardware.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Code:&lt;/strong&gt; &lt;a href="https://github.com/erodola/bigram-nes"&gt;https://github.com/erodola/bigram-nes&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. The Compile-Time LM (inference while compiling, duh)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Then I realized that even the NES was too much runtime. Why even wait for the code to run at all? I built a model that does inference entirely at compile-time using C++ template metaprogramming. &lt;/p&gt; &lt;p&gt;Because the compiler itself is Turing complete you know. You could run Doom in it.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The C++ compiler acts as the inference engine. It performs the multinomial sampling and Markov chain transitions &lt;em&gt;while&lt;/em&gt; you are building the project.&lt;/li&gt; &lt;li&gt;Since compilers are deterministic, I hashed &lt;strong&gt;TIME&lt;/strong&gt; into an FNV-1a seed to power a constexpr Xorshift32 RNG.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;When the binary finally runs, the CPU does zero math. The generated text is already there, baked into the data segment as a constant string.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Code:&lt;/strong&gt; &lt;a href="https://github.com/erodola/bigram-metacpp"&gt;https://github.com/erodola/bigram-metacpp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Next up is ofc attempting to scale this toward TinyStories-style models. Or speech synthesis, or OCR. I wont stop until my build logs are more sentient than the code they're actually producing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Brief_Argument8155"&gt; /u/Brief_Argument8155 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgbkcd/running_language_models_where_they_dont_belong/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgbkcd/running_language_models_where_they_dont_belong/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qgbkcd/running_language_models_where_they_dont_belong/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-18T15:33:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1qh213r</id>
    <title>Model Meshing</title>
    <updated>2026-01-19T11:45:18+00:00</updated>
    <author>
      <name>/u/Spare_Grape_962</name>
      <uri>https://old.reddit.com/user/Spare_Grape_962</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all. I am currently using various models to build my small projects. I have taken an interest in white hat hacking and bug bounties. Knowing nothing about coding. &lt;/p&gt; &lt;p&gt;I ask four smaller models to write small snippets of code, and then they each share their output with each other via agents, and then they can vote on the best one and my program chooses that one. Now I want to be able to feed the final output of the full file from the four agents code added together, with two smarter cloud agents where they will edit the overall file in case putting small snippets together has left errors.&lt;/p&gt; &lt;p&gt;My asktion is, are coding outputs more accurate from smarter larger models, or using smaller smartish models to form a itty bitty coding committe? Or do you want me to tell you in a few days. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Spare_Grape_962"&gt; /u/Spare_Grape_962 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh213r/model_meshing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh213r/model_meshing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qh213r/model_meshing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T11:45:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1qgxgns</id>
    <title>built a (free) photo based nutrition tracker for iOS, with local LLM support</title>
    <updated>2026-01-19T07:13:26+00:00</updated>
    <author>
      <name>/u/Agusx1211</name>
      <uri>https://old.reddit.com/user/Agusx1211</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgxgns/built_a_free_photo_based_nutrition_tracker_for/"&gt; &lt;img alt="built a (free) photo based nutrition tracker for iOS, with local LLM support" src="https://a.thumbs.redditmedia.com/XMo038VJinSiM5XwZcMNBgiITaVwM1pBgX9oQVUFR58.jpg" title="built a (free) photo based nutrition tracker for iOS, with local LLM support" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/u7heybr4a9eg1.png?width=3342&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a64a6e55169e346bae43751c34393d89afb90c39"&gt;https://preview.redd.it/u7heybr4a9eg1.png?width=3342&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a64a6e55169e346bae43751c34393d89afb90c39&lt;/a&gt;&lt;/p&gt; &lt;p&gt;During my Christmas break (and weekends), I started working on this project. It is mostly vibe coded, but it works quite well. I built it for myself really, but since you have to pay Apple 100 bucks even if you want an app only for yourself, I decided to go through publishing it.&lt;/p&gt; &lt;p&gt;It is a nutrition tracker; it integrates with Apple Health and uses LLMs to estimate portions, nutrients, etc. The flow is quite simple: take a picture and click send. In my (biased) opinion, it is a lot simpler than Lose It and similar apps imho.&lt;/p&gt; &lt;p&gt;It supports local LLMs, even running them on device. The on-device models are not great, but you can connect the app to LM Studio and run more powerful models there. The app works best with Gemini 3.0 Flash, but for that you need to add an OpenRouter key. I am also running a free &amp;quot;cloud&amp;quot; service for quick testing, but please don't hammer it to death.&lt;/p&gt; &lt;p&gt;The app is free, and I have no plans on monetizing it. The data never leaves your device if you use a local LLM (or LM Studio). It doesn't even have analytics, so if you try it, give me a ping, because I won't be able to tell!&lt;/p&gt; &lt;p&gt;Link: &lt;a href="https://apps.apple.com/us/app/flog-ai-food-tracker/id6756525727"&gt;https://apps.apple.com/us/app/flog-ai-food-tracker/id6756525727&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Agusx1211"&gt; /u/Agusx1211 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgxgns/built_a_free_photo_based_nutrition_tracker_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgxgns/built_a_free_photo_based_nutrition_tracker_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qgxgns/built_a_free_photo_based_nutrition_tracker_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T07:13:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1qh0abp</id>
    <title>We built a small GPU platform and are looking for early users’ feedback</title>
    <updated>2026-01-19T10:05:21+00:00</updated>
    <author>
      <name>/u/Nora_ww</name>
      <uri>https://old.reddit.com/user/Nora_ww</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh0abp/we_built_a_small_gpu_platform_and_are_looking_for/"&gt; &lt;img alt="We built a small GPU platform and are looking for early users’ feedback" src="https://b.thumbs.redditmedia.com/GXYYQ8UdUlANcA95D8KEstCVG5YizI9lO6Hs9Ti45xo.jpg" title="We built a small GPU platform and are looking for early users’ feedback" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;We’re a small team building a GPU platform mainly for our own model training and inference experiments. While testing it internally, we realized we have spare GPU capacity sitting idle.&lt;/p&gt; &lt;p&gt;Instead of letting it go unused, we’d love to open it up to the community and get some real-world feedback. We’re offering &lt;strong&gt;free compute credits&lt;/strong&gt; in exchange for honest usage feedback (what works, what breaks, what’s annoying).&lt;/p&gt; &lt;p&gt;Currently available GPUs include &lt;strong&gt;RTX 5090 and Pro 6000&lt;/strong&gt;, suitable for LLM inference, fine-tuning, or other ML workloads.&lt;/p&gt; &lt;p&gt;If you’re interested in trying it or have specific workloads in mind, feel free to comment or DM me. I’m happy to answer technical questions as well.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/m9j4c7ud5aeg1.png?width=1020&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6caed0d0b7af9cf0edea8b9471afe3e01d94d625"&gt;https://preview.redd.it/m9j4c7ud5aeg1.png?width=1020&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6caed0d0b7af9cf0edea8b9471afe3e01d94d625&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nora_ww"&gt; /u/Nora_ww &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh0abp/we_built_a_small_gpu_platform_and_are_looking_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh0abp/we_built_a_small_gpu_platform_and_are_looking_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qh0abp/we_built_a_small_gpu_platform_and_are_looking_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T10:05:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1qh2qv9</id>
    <title>Self‑taught NLP/Deep Learning theory for over a year — seeking advice for first hands‑on project</title>
    <updated>2026-01-19T12:22:35+00:00</updated>
    <author>
      <name>/u/Heavy-Vegetable4808</name>
      <uri>https://old.reddit.com/user/Heavy-Vegetable4808</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am from Ethiopia and have been self‑studying Deep Learning and NLP for more than a year using only my phone. I have read books like:&lt;/p&gt; &lt;p&gt;· Deep Learning (Goodfellow et al.)&lt;/p&gt; &lt;p&gt;· Mathematics for Machine Learning&lt;/p&gt; &lt;p&gt;· Speech and Language Processing (Jurafsky &amp;amp; Martin, 3rd ed. draft)&lt;/p&gt; &lt;p&gt;…and others, along with many papers and lectures.&lt;/p&gt; &lt;p&gt;So far this has been entirely theory—I have not written any code or built a project yet, because I do not own a laptop (hope to get one soon).&lt;/p&gt; &lt;p&gt;I now want to start my first practical NLP project, likely focusing on Amharic or other Ethiopian languages.&lt;/p&gt; &lt;p&gt;Questions:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;What is a good first project that balances feasibility and learning value?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;How can I prepare on paper/mobile before I can code?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Are there lightweight models or tools that work well for low‑resource languages?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Any advice on structuring a self‑taught portfolio to move toward freelance/remote work?&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Thank you for any guidance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Heavy-Vegetable4808"&gt; /u/Heavy-Vegetable4808 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh2qv9/selftaught_nlpdeep_learning_theory_for_over_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh2qv9/selftaught_nlpdeep_learning_theory_for_over_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qh2qv9/selftaught_nlpdeep_learning_theory_for_over_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T12:22:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1qgksrm</id>
    <title>Roast my build</title>
    <updated>2026-01-18T21:28:13+00:00</updated>
    <author>
      <name>/u/RoboDogRush</name>
      <uri>https://old.reddit.com/user/RoboDogRush</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgksrm/roast_my_build/"&gt; &lt;img alt="Roast my build" src="https://b.thumbs.redditmedia.com/b9pW-_QRs8a6Yq5-9Nf1d_lQSgXCDYpLuPpj2b0jIXM.jpg" title="Roast my build" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This started as an Optiplex 990 with a 2nd gen i5 as a home server. Someone gave me a 3060, I started running Ollama with Gemma 7B to help manage my Home Assistant, and it became addicting.&lt;/p&gt; &lt;p&gt;The upgrades outgrew the SFF case, PSU and GPU spilling out the side, and it slowly grew into this beast. Around the time I bought the open frame, my wife said it's gotta move out of sight, so I got banished to the unfinished basement, next to the sewage pump. Honestly, better for me, got to plug directly into the network and get off wifi.&lt;/p&gt; &lt;p&gt;6 months of bargain hunting, eBay alerts at 2am, Facebook Marketplace meetups in parking lots, explaining what VRAM is for the 47th time. The result:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;6x RTX 3090 (24GB each)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;1x RTX 5090 (32GB), $1,700 open box Microcenter&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;ROMED8-2T + EPYC 7282&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;2x ASRock 1600W PSUs (both open box)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;32GB A-Tech DDR4 ECC RDIMM&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;$10 Phanteks 300mm PCIe 4.0 riser cables (too long for the lower rack, but costs more to replace with shorter ones)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;176GB total VRAM, ~$6,500 all-in&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;First motherboard crapped out, but got a warranty replacement right before they went out of stock.&lt;/p&gt; &lt;p&gt;Currently running Unsloth's GPT-OSS 120B MXFP4 GGUF. Also been doing Ralph Wiggum loops with Devstral-2 Q8_0 via Mistral Vibe, which yes, I know is unlimited free and full precision in the cloud. But the cloud can't hear my sewage pump.&lt;/p&gt; &lt;p&gt;I think I'm finally done adding on. I desperately needed this. Now I'm not sure what to do with it.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Edit: Fixed the GPT-OSS precision claim. It's natively MXFP4, not F16. The model was trained that way. Thanks to the commenters who caught it.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RoboDogRush"&gt; /u/RoboDogRush &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qgksrm"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgksrm/roast_my_build/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qgksrm/roast_my_build/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-18T21:28:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1qh10q9</id>
    <title>Demo: On-device browser agent (Qwen) running locally in Chrome</title>
    <updated>2026-01-19T10:48:29+00:00</updated>
    <author>
      <name>/u/thecoder12322</name>
      <uri>https://old.reddit.com/user/thecoder12322</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh10q9/demo_ondevice_browser_agent_qwen_running_locally/"&gt; &lt;img alt="Demo: On-device browser agent (Qwen) running locally in Chrome" src="https://external-preview.redd.it/MzNzcDNuMGdjYWVnMcLtcBYDX5SdJ9uQfQaEUwyxr5ovu1B5qUxuDFDhwgNH.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=67f9a368e6dac7735d2b2e3f72bc407aa88b54c0" title="Demo: On-device browser agent (Qwen) running locally in Chrome" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys! wanted to share a cool demo of LOCAL Browser agent (powered by Web GPU Liquid LFM &amp;amp; Alibaba Qwen models) opening the All in Podcast on Youtube running as a chrome extension. &lt;/p&gt; &lt;p&gt;Source: &lt;a href="https://github.com/RunanywhereAI/on-device-browser-agent"&gt;https://github.com/RunanywhereAI/on-device-browser-agent&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thecoder12322"&gt; /u/thecoder12322 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ljp6zwzfcaeg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh10q9/demo_ondevice_browser_agent_qwen_running_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qh10q9/demo_ondevice_browser_agent_qwen_running_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T10:48:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1qgj2n9</id>
    <title>Are most major agents really just markdown todo list processors?</title>
    <updated>2026-01-18T20:15:25+00:00</updated>
    <author>
      <name>/u/TheDigitalRhino</name>
      <uri>https://old.reddit.com/user/TheDigitalRhino</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been poking around different code bases and scrutixzing logs from the majors LLM providers, and it seems like every agent just decomposes task to a todo list and process them one by one.&lt;/p&gt; &lt;p&gt;Has anyone found a different approach?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheDigitalRhino"&gt; /u/TheDigitalRhino &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgj2n9/are_most_major_agents_really_just_markdown_todo/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgj2n9/are_most_major_agents_really_just_markdown_todo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qgj2n9/are_most_major_agents_really_just_markdown_todo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-18T20:15:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1qglyqz</id>
    <title>how do you pronounce “gguf”?</title>
    <updated>2026-01-18T22:14:35+00:00</updated>
    <author>
      <name>/u/Hamfistbumhole</name>
      <uri>https://old.reddit.com/user/Hamfistbumhole</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;is it “jee - guff”? “giguff”? or the full “jee jee you eff”? others???&lt;/p&gt; &lt;p&gt;discuss.&lt;/p&gt; &lt;p&gt;and sorry for not using proper international phonetic alphabet symbol things&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hamfistbumhole"&gt; /u/Hamfistbumhole &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qglyqz/how_do_you_pronounce_gguf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qglyqz/how_do_you_pronounce_gguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qglyqz/how_do_you_pronounce_gguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-18T22:14:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1qgrw3d</id>
    <title>Just put together my new setup(3x v620 for 96gb vram)</title>
    <updated>2026-01-19T02:31:42+00:00</updated>
    <author>
      <name>/u/PraxisOG</name>
      <uri>https://old.reddit.com/user/PraxisOG</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgrw3d/just_put_together_my_new_setup3x_v620_for_96gb/"&gt; &lt;img alt="Just put together my new setup(3x v620 for 96gb vram)" src="https://b.thumbs.redditmedia.com/mJVsjBo7IHRjbuffjsuNrdAMt7krK8sWOZUchd5L7tE.jpg" title="Just put together my new setup(3x v620 for 96gb vram)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PraxisOG"&gt; /u/PraxisOG &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qgrw3d"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgrw3d/just_put_together_my_new_setup3x_v620_for_96gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qgrw3d/just_put_together_my_new_setup3x_v620_for_96gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T02:31:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1qgdb7f</id>
    <title>4x AMD R9700 (128GB VRAM) + Threadripper 9955WX Build</title>
    <updated>2026-01-18T16:39:42+00:00</updated>
    <author>
      <name>/u/NunzeCs</name>
      <uri>https://old.reddit.com/user/NunzeCs</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgdb7f/4x_amd_r9700_128gb_vram_threadripper_9955wx_build/"&gt; &lt;img alt="4x AMD R9700 (128GB VRAM) + Threadripper 9955WX Build" src="https://b.thumbs.redditmedia.com/bQ4SRK8dHDz2IGShLwX64vLIVj0fWUigDqG_dO43P-U.jpg" title="4x AMD R9700 (128GB VRAM) + Threadripper 9955WX Build" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Disclaimer: I am from Germany and my English is not perfect, so I used an LLM to help me structure and write this post.&lt;/p&gt; &lt;p&gt;Context &amp;amp; Motivation: I built this system for my small company. The main reason for all new hardware is that I received a 50% subsidy/refund from my local municipality for digitalization investments. To qualify for this funding, I had to buy new hardware and build a proper &amp;quot;server-grade&amp;quot; system.&lt;/p&gt; &lt;p&gt;My goal was to run large models (120B+) locally for data privacy. With the subsidy in mind, I had a budget of around 10,000€ (pre-refund). I initially considered NVIDIA, but I wanted to maximize VRAM. I decided to go with 4x AMD RDNA4 cards (ASRock R9700) to get 128GB VRAM total and used the rest of the budget for a solid Threadripper platform.&lt;/p&gt; &lt;p&gt;Hardware Specs:&lt;/p&gt; &lt;p&gt;Total Cost: ~9,800€ (I get ~50% back, so effectively ~4,900€ for me).&lt;/p&gt; &lt;ul&gt; &lt;li&gt;CPU: AMD Ryzen Threadripper PRO 9955WX (16 Cores) &lt;/li&gt; &lt;li&gt;Mainboard: ASRock WRX90 WS EVO &lt;/li&gt; &lt;li&gt;RAM: 128GB DDR5 5600MHz &lt;/li&gt; &lt;li&gt;GPU: 4x ASRock Radeon AI PRO R9700 32GB (Total 128GB VRAM) &lt;ul&gt; &lt;li&gt;Configuration: All cards running at full PCIe 5.0 x16 bandwidth. &lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Storage: 2x 2TB PCIe 4.0 SSD &lt;/li&gt; &lt;li&gt;PSU: Seasonic 2200W &lt;/li&gt; &lt;li&gt;Cooling: Alphacool Eisbaer Pro Aurora 360 CPU AIO&lt;/li&gt; &lt;li&gt;Case: PHANTEKS Enthoo Pro 2 Server&lt;/li&gt; &lt;li&gt;Fans: 11x Arctic P12 Pro&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Benchmark Results&lt;/p&gt; &lt;p&gt;I tested various models ranging from 8B to 230B parameters.&lt;/p&gt; &lt;p&gt;Llama.cpp (Focus: Single User Latency) Settings: Flash Attention ON, Batch 2048&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Modell&lt;/th&gt; &lt;th align="left"&gt;NGL&lt;/th&gt; &lt;th align="left"&gt;Prompt t/s&lt;/th&gt; &lt;th align="left"&gt;Gen t/s&lt;/th&gt; &lt;th align="left"&gt;Größe&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;GLM-4.7-REAP-218B-A32B-Q3_K_M&lt;/td&gt; &lt;td align="left"&gt;999&lt;/td&gt; &lt;td align="left"&gt;504.15&lt;/td&gt; &lt;td align="left"&gt;17.48&lt;/td&gt; &lt;td align="left"&gt;97.6GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GLM-4.7-REAP-218B-A32B-Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;65&lt;/td&gt; &lt;td align="left"&gt;428.80&lt;/td&gt; &lt;td align="left"&gt;9.48&lt;/td&gt; &lt;td align="left"&gt;123.0GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss-120b-GGUF&lt;/td&gt; &lt;td align="left"&gt;999&lt;/td&gt; &lt;td align="left"&gt;2977.83&lt;/td&gt; &lt;td align="left"&gt;97.47&lt;/td&gt; &lt;td align="left"&gt;58.4GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Meta-Llama-3.1-70B-Instruct-Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;999&lt;/td&gt; &lt;td align="left"&gt;399.03&lt;/td&gt; &lt;td align="left"&gt;12.66&lt;/td&gt; &lt;td align="left"&gt;39.6GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Meta-Llama-3.1-8B-Instruct-Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;999&lt;/td&gt; &lt;td align="left"&gt;3169.16&lt;/td&gt; &lt;td align="left"&gt;81.01&lt;/td&gt; &lt;td align="left"&gt;4.6GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MiniMax-M2.1-Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;55&lt;/td&gt; &lt;td align="left"&gt;668.99&lt;/td&gt; &lt;td align="left"&gt;34.85&lt;/td&gt; &lt;td align="left"&gt;128.83 GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen2.5-32B-Instruct-Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;999&lt;/td&gt; &lt;td align="left"&gt;848.68&lt;/td&gt; &lt;td align="left"&gt;25.14&lt;/td&gt; &lt;td align="left"&gt;18.5GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-235B-A22B-Instruct-2507-Q3_K_M&lt;/td&gt; &lt;td align="left"&gt;999&lt;/td&gt; &lt;td align="left"&gt;686.45&lt;/td&gt; &lt;td align="left"&gt;24.45&lt;/td&gt; &lt;td align="left"&gt;104.7GB&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Side note: I found that with PCIe 5.0, standard Pipeline Parallelism (Layer Split) is significantly faster (~97 t/s) than Tensor Parallelism/Row Split (~67 t/s) for a single user on this setup.&lt;/p&gt; &lt;p&gt;vLLM (Focus: Throughput) Model: GPT-OSS-120B (bfloat16), TP=4, test for 20 requests&lt;/p&gt; &lt;p&gt;Total Throughput: ~314 tokens/s (Generation) Prompt Processing: ~5339 tokens/s Single user throughput 50 tokens/s&lt;/p&gt; &lt;p&gt;I used rocm 7.1.1 for llama.cpp also testet Vulkan but it was worse&lt;/p&gt; &lt;p&gt;If I could do it again, I would have used the budget to buy a single NVIDIA RTX Pro 6000 Blackwell (96GB). Maybe I will, if local AI is going well for my use case, I swap the R9700 with Pro 6000 in the future.&lt;/p&gt; &lt;p&gt;**Edit nicer view for the results&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NunzeCs"&gt; /u/NunzeCs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qgdb7f"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgdb7f/4x_amd_r9700_128gb_vram_threadripper_9955wx_build/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qgdb7f/4x_amd_r9700_128gb_vram_threadripper_9955wx_build/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-18T16:39:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1qgwup8</id>
    <title>Is Local Coding even worth setting up</title>
    <updated>2026-01-19T06:38:37+00:00</updated>
    <author>
      <name>/u/Interesting-Fish6494</name>
      <uri>https://old.reddit.com/user/Interesting-Fish6494</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi I am new to Local LLM but have been having a lot of issues setting up a local LLM coding environment so wanted some suggestions from people.I have a 5070 ti (16gb vram).&lt;/p&gt; &lt;p&gt;I have tried to use Kilo code with qwen 2.5 coder 7B running through ollama but the context size feels so low that it finishes the context within a single file of my project.&lt;/p&gt; &lt;p&gt;How are other people with a 16gb GPU dealing with local llm?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Interesting-Fish6494"&gt; /u/Interesting-Fish6494 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgwup8/is_local_coding_even_worth_setting_up/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgwup8/is_local_coding_even_worth_setting_up/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qgwup8/is_local_coding_even_worth_setting_up/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T06:38:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1qgx83t</id>
    <title>3x3090 + 3060 in a mid tower case</title>
    <updated>2026-01-19T06:59:39+00:00</updated>
    <author>
      <name>/u/liviuberechet</name>
      <uri>https://old.reddit.com/user/liviuberechet</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgx83t/3x3090_3060_in_a_mid_tower_case/"&gt; &lt;img alt="3x3090 + 3060 in a mid tower case" src="https://b.thumbs.redditmedia.com/isvewN3PNijf6_OZxoF82ROhZEAD0nfG7ddsr3ghkYU.jpg" title="3x3090 + 3060 in a mid tower case" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Decided to go all out and max out this desktop. I was lucky to find 3090 cards for around 600 usd, over a period of 3 months and decided to go for it. &lt;/p&gt; &lt;p&gt;The RAM was a bit more expensive, but I had 64 bought before the price spiked.&lt;/p&gt; &lt;p&gt;I didn’t want to change the case, because I through it’s a high quality case and it would be a shame to toss it. So made the most out of it!&lt;/p&gt; &lt;p&gt;Specs: * Fractal Define 7 Mid Tower * 3x3090 + 1x3060 (86gb total, but 72gb VRAM main) * 128GB DDR4 (Corsair 4x32) * Corsair HX1500i 1500w (has 7 PCIe power cables) * Vertical mounts are all cheap from AliExpress * ASUS Maximus XII Hero — has only 3x PCIe16x, had to deactivate the 2nd NVMe to use the 3rd PCIe16x in 4x, the 4th GPU (the 3060) is on a riser from a PCIe1x. * For drives, only one NVMe of 1TB works, I also bought 2x2TB SSDs that I tried in RAID but the performance was terrible (and they are limited to 500mb from the SATA interface, which I didn’t know…) so I keep them as 2 drives.&lt;/p&gt; &lt;p&gt;Temperatures are holding surprisingly well. The gap between the cards is about the size of an empty PCIe slot, maybe a bit more. &lt;/p&gt; &lt;p&gt;Temperature was a big improvement compared to having just 2x3090 stacked without any space between them — the way the motherboard is designed to use them.&lt;/p&gt; &lt;p&gt;In terms of performance 3x3090 is great! There are great options in the 60-65gb range with the extra space to 72gb VRAM used for context.&lt;/p&gt; &lt;p&gt;I am not using the RAM for anything other than to load models, and the speed is amazing when everything is loaded in VRAM! &lt;/p&gt; &lt;p&gt;Models I started using a lot: * gpt-oss-120b in MXFP4 with 60k context * glm-4.5-air in IQ4_NL with 46k context * qwen3-vl-235b in TQ1_0 (surprisingly good!) * minimax-M2-REAP-139B in Q3_K_S with 40k context&lt;/p&gt; &lt;p&gt;But still return a lot to old models for context and speed: * devstral-small-2-24 in Q8_0 with 200k context * qwen3-coder in Q8 with 1M (!!) context (using RAM) * qwen3-next-80b in Q6_K with 60k context — still my favourite for general chat, and the Q6 makes me trust it more than Q3-Q4 models&lt;/p&gt; &lt;p&gt;The 3060 on the riser from PCIe1x is very slow at loading the models, however, once it’s loaded it works great! I am using it for image generation and TTS audio generation mostly (for Open WebUI).&lt;/p&gt; &lt;p&gt;Also did a lot of testing on using 2x3090 via normal PCIe, with a 3rd card via riser — it works same as normal PCIe! But the loading takes forever (sometimes over 2-3 minutes) and you simply can’t use the RAM for context because of how slow it is — so I am considering the current setup to be “maxed out” because I don’t think adding a 4th 3090 will be useful. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/liviuberechet"&gt; /u/liviuberechet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qgx83t"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgx83t/3x3090_3060_in_a_mid_tower_case/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qgx83t/3x3090_3060_in_a_mid_tower_case/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T06:59:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1qh0yq8</id>
    <title>I made a Top-K implementation that's up to 20x faster than PyTorch CPU (open source)</title>
    <updated>2026-01-19T10:45:28+00:00</updated>
    <author>
      <name>/u/andreabarbato</name>
      <uri>https://old.reddit.com/user/andreabarbato</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Spent way too long optimizing Top-K selection for LLM sampling and finally hit some stupid numbers.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; AVX2-optimized batched Top-K that beats PyTorch CPU by 4-20x depending on vocab size. Sometimes competitive with CUDA for small batches.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Benchmarks (K=50):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Vocab=32K: 0.043ms vs PyTorch's 0.173ms (4x faster)&lt;/li&gt; &lt;li&gt;Vocab=128K: 0.057ms vs PyTorch's 0.777ms (13x faster)&lt;/li&gt; &lt;li&gt;Vocab=256K: 0.079ms vs PyTorch's 1.56ms (20x faster)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Integrated it into llama.cpp and got 63% faster prompt processing on a 120B MoE model (81→142 tokens/sec).&lt;/p&gt; &lt;p&gt;Uses adaptive sampling + AVX2 SIMD + cache-optimized scanning. Has fast paths for sorted/constant inputs. Single-pass algorithm, no GPU needed.&lt;/p&gt; &lt;p&gt;Includes pre-built DLLs and llama.cpp implementation (for windows).&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/RAZZULLIX/fast_topk_batched"&gt;https://github.com/RAZZULLIX/fast_topk_batched&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love feedback or roasting, whichever you prefer.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/andreabarbato"&gt; /u/andreabarbato &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh0yq8/i_made_a_topk_implementation_thats_up_to_20x/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh0yq8/i_made_a_topk_implementation_thats_up_to_20x/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qh0yq8/i_made_a_topk_implementation_thats_up_to_20x/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T10:45:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
