<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-16T06:27:46+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1pnn7rr</id>
    <title>GLM4.5-air VS GLM4.6V (TEXT GENERATION)</title>
    <updated>2025-12-16T00:00:22+00:00</updated>
    <author>
      <name>/u/LetterheadNeat8035</name>
      <uri>https://old.reddit.com/user/LetterheadNeat8035</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone done a comparison between GLM4.5-air and GLM4.6V specifically for text generation and agentic performance?&lt;/p&gt; &lt;p&gt;I know GLM4.6V is marketed as a vision model, but I'm curious about how it performs in pure text generation and agentic tasks compared to GLM4.5-air.&lt;/p&gt; &lt;p&gt;Has anyone tested both models side by side for things like:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Reasoning and logic&lt;/li&gt; &lt;li&gt;Code generation&lt;/li&gt; &lt;li&gt;Instruction following&lt;/li&gt; &lt;li&gt;Function calling/tool use&lt;/li&gt; &lt;li&gt;Multi-turn conversations&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm trying to decide which one to use for a text-heavy project and wondering if the newer V model has improvements beyond just vision capabilities, or if 4.5-air is still the better choice for text-only tasks.&lt;/p&gt; &lt;p&gt;Any benchmarks or real-world experience would be appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LetterheadNeat8035"&gt; /u/LetterheadNeat8035 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnn7rr/glm45air_vs_glm46v_text_generation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnn7rr/glm45air_vs_glm46v_text_generation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pnn7rr/glm45air_vs_glm46v_text_generation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T00:00:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmungj</id>
    <title>Aaaand... is gone...</title>
    <updated>2025-12-15T01:18:27+00:00</updated>
    <author>
      <name>/u/HumanDrone8721</name>
      <uri>https://old.reddit.com/user/HumanDrone8721</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmungj/aaaand_is_gone/"&gt; &lt;img alt="Aaaand... is gone..." src="https://preview.redd.it/g7ahg4per97g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3cb28b1a23cbf35375171b5f0b3fcb2d63310818" title="Aaaand... is gone..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HumanDrone8721"&gt; /u/HumanDrone8721 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/g7ahg4per97g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmungj/aaaand_is_gone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pmungj/aaaand_is_gone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T01:18:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1pn6ijr</id>
    <title>How to do a RTX Pro 6000 build right</title>
    <updated>2025-12-15T12:48:06+00:00</updated>
    <author>
      <name>/u/GPTrack_dot_ai</name>
      <uri>https://old.reddit.com/user/GPTrack_dot_ai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn6ijr/how_to_do_a_rtx_pro_6000_build_right/"&gt; &lt;img alt="How to do a RTX Pro 6000 build right" src="https://a.thumbs.redditmedia.com/In8IH1Q_hHUabRuaqY3yOiZcHAQpBhmIHFzJ_jWQyx4.jpg" title="How to do a RTX Pro 6000 build right" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The RTX PRO 6000 is missing NVlink, that is why Nvidia came up with idea to integrate high-speed networking directly at each GPU. This is called the RTX PRO server. There are 8 PCIe slots for 8 RTX Pro 6000 server version cards and each one has a 400G networking connection. The good thing is that it is basically ready to use. The only thing you need to decide on is Switch, CPU, RAM and storage. Not much can go wrong there. If you want multiple RTX PRO 6000 this the way to go.&lt;/p&gt; &lt;p&gt;Exemplary Specs:&lt;br /&gt; 8x Nvidia RTX PRO 6000 Blackwell Server Edition GPU&lt;br /&gt; 8x Nvidia ConnectX-8 1-port 400G QSFP112&lt;br /&gt; 1x Nvidia Bluefield-3 2-port 200G total 400G QSFP112 (optional)&lt;br /&gt; 2x Intel Xeon 6500/6700&lt;br /&gt; 32x 6400 RDIMM or 8000 MRDIMM&lt;br /&gt; 6000W TDP&lt;br /&gt; 4x High-efficiency 3200W PSU&lt;br /&gt; 2x PCIe gen4 M.2 slots on board&lt;br /&gt; 8x PCIe gen5 U.2&lt;br /&gt; 2x USB 3.2 port&lt;br /&gt; 2x RJ45 10GbE ports&lt;br /&gt; RJ45 IPMI port&lt;br /&gt; Mini display port&lt;br /&gt; 10x 80x80x80mm fans&lt;br /&gt; 4U 438 x 176 x 803 mm (17.2 x 7 x 31.6&amp;quot;)&lt;br /&gt; 70 kg (150 lbs)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GPTrack_dot_ai"&gt; /u/GPTrack_dot_ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pn6ijr"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn6ijr/how_to_do_a_rtx_pro_6000_build_right/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pn6ijr/how_to_do_a_rtx_pro_6000_build_right/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T12:48:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1pn7c3f</id>
    <title>Alibaba Tongyi Open Sources Two Audio Models: Fun-CosyVoice 3.0 (TTS) and Fun-ASR-Nano-2512 (ASR)</title>
    <updated>2025-12-15T13:28:12+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn7c3f/alibaba_tongyi_open_sources_two_audio_models/"&gt; &lt;img alt="Alibaba Tongyi Open Sources Two Audio Models: Fun-CosyVoice 3.0 (TTS) and Fun-ASR-Nano-2512 (ASR)" src="https://preview.redd.it/1v2kztejdd7g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=756f684594b46bdb6022b8853734dc4f1ad2ec1c" title="Alibaba Tongyi Open Sources Two Audio Models: Fun-CosyVoice 3.0 (TTS) and Fun-ASR-Nano-2512 (ASR)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Fun-ASR-Nano (0.8B) — Open-sourced - Lightweight Fun-ASR variant - Lower inference cost - Local deployment &amp;amp; custom fine-tuning supported&lt;/p&gt; &lt;p&gt;Fun-CosyVoice3 (0.5B) — Open-sourced - Zero-shot voice cloning - Local deployment &amp;amp; secondary development ready&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1v2kztejdd7g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn7c3f/alibaba_tongyi_open_sources_two_audio_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pn7c3f/alibaba_tongyi_open_sources_two_audio_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T13:28:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1pn8ww0</id>
    <title>GLM 4.6V support coming to llama.cpp</title>
    <updated>2025-12-15T14:36:56+00:00</updated>
    <author>
      <name>/u/tarruda</name>
      <uri>https://old.reddit.com/user/tarruda</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tarruda"&gt; /u/tarruda &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/18042"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn8ww0/glm_46v_support_coming_to_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pn8ww0/glm_46v_support_coming_to_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T14:36:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnnigk</id>
    <title>[Research] I added a "System 2" Planning Head to Mistral-7B. It fixes associative drift with ZERO inference latency (beat baseline PPL).</title>
    <updated>2025-12-16T00:13:45+00:00</updated>
    <author>
      <name>/u/Leading_Wrangler_708</name>
      <uri>https://old.reddit.com/user/Leading_Wrangler_708</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnnigk/research_i_added_a_system_2_planning_head_to/"&gt; &lt;img alt="[Research] I added a &amp;quot;System 2&amp;quot; Planning Head to Mistral-7B. It fixes associative drift with ZERO inference latency (beat baseline PPL)." src="https://preview.redd.it/0j0kcpyvkg7g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c3c235b0053bb4b767c0d6deea0ff6d67c8c5ff4" title="[Research] I added a &amp;quot;System 2&amp;quot; Planning Head to Mistral-7B. It fixes associative drift with ZERO inference latency (beat baseline PPL)." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;​Hey everyone, ​I’ve been working on a new architecture called Idea-Gated Transformers, and I just finished scaling it up to a Mistral-7B backbone using QLoRA. ​I wanted to share the results here because I think it solves a specific annoyance we all face with local models: Associative Drift (where the model gets distracted by a high-probability word and derails the whole generation).&lt;/p&gt; &lt;p&gt;​The Problem: &amp;quot;The Batman Effect&amp;quot; Standard LLMs are &amp;quot;System 1&amp;quot; thinkers—they just surf statistical correlations. If you prompt a base model with: &amp;quot;The bat flew out of the cave...&amp;quot; It often drifts into: &amp;quot;...and into Gotham City. Batman is a fictional superhero...&amp;quot; ​The model ignores the biological context because the token &amp;quot;Batman&amp;quot; has such a high probability weight in the training data (Web text).&lt;/p&gt; &lt;p&gt;​The Architecture: Differentiable Vocabulary Pruning Instead of using Chain-of-Thought (which is slow and eats up context), I trained a lightweight auxiliary Idea Head (2-layer MLP) that runs in parallel with the main model. ​Lookahead: Before generating a token, the Idea Head predicts a &amp;quot;Bag of Words&amp;quot; for the next 20 tokens (the future concept).&lt;/p&gt; &lt;p&gt;​Gating: This prediction generates a gate vector that suppresses irrelevant tokens in the vocabulary. ​Generation: The standard frozen Mistral head picks the next token from this pruned list.&lt;/p&gt; &lt;p&gt;​The Results (Mistral-7B-v0.1 + FineWeb-Edu): ​Drift: In adversarial stress tests, the standard LoRA baseline drifted to &amp;quot;Pop Culture&amp;quot; 100% of the time. The Idea-Gated model stayed locked on &amp;quot;Biology&amp;quot; (0% drift). ​Perplexity: This isn't just a safety filter. The gated model actually achieved better validation perplexity (7.78) than the standard QLoRA baseline (8.08). It turns out, forcing the model to &amp;quot;plan&amp;quot; helps it predict better. ​Latency: Because the Idea Head is a tiny MLP and runs in parallel, there is effectively zero inference latency penalty. You get &amp;quot;reasoning-like&amp;quot; stability at full generation speed.&lt;/p&gt; &lt;p&gt;This is a parameter-efficient way (QLoRA) to make 7B models behave like much larger models in terms of coherence and topic adherence, without the massive slowdown of Contrastive Decoding or CoT. ​I’ve open-sourced the code and the paper. Would love to hear what you guys think about this approach to &amp;quot;System 2&amp;quot; logic.&lt;/p&gt; &lt;p&gt;​Paper:&lt;a href="https://arxiv.org/html/2512.03343v2"&gt;https://arxiv.org/html/2512.03343v2&lt;/a&gt; Code: &lt;a href="https://github.com/DarshanFofadiya/idea-gated-transformers"&gt;https://github.com/DarshanFofadiya/idea-gated-transformers&lt;/a&gt;&lt;/p&gt; &lt;p&gt;​(I included an &amp;quot;X-Ray&amp;quot; analysis in the paper showing exactly how the model suppresses the token &amp;quot;Batman&amp;quot; by -90% while boosting &amp;quot;Mammal&amp;quot; by +60%. It’s pretty cool to see the mechanism working visually).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Leading_Wrangler_708"&gt; /u/Leading_Wrangler_708 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0j0kcpyvkg7g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnnigk/research_i_added_a_system_2_planning_head_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pnnigk/research_i_added_a_system_2_planning_head_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T00:13:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnusq8</id>
    <title>Open Source Alternative to Perplexity</title>
    <updated>2025-12-16T06:16:11+00:00</updated>
    <author>
      <name>/u/Uiqueblhats</name>
      <uri>https://old.reddit.com/user/Uiqueblhats</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those of you who aren't familiar with SurfSense, it aims to be the &lt;strong&gt;open-source alternative to NotebookLM, Perplexity, or Glean.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;In short, it's a Highly Customizable AI Research Agent that connects to your personal external sources and Search Engines (SearxNG, Tavily, LinkUp), Slack, Linear, Jira, ClickUp, Confluence, Gmail, Notion, YouTube, GitHub, Discord, Airtable, Google Calendar and more to come.&lt;/p&gt; &lt;p&gt;I'm looking for contributors. If you're interested in AI agents, RAG, browser extensions, or building open-source research tools, this is a great place to jump in.&lt;/p&gt; &lt;p&gt;Here’s a quick look at what SurfSense offers right now:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;RBAC (Role Based Access for Teams)&lt;/li&gt; &lt;li&gt;Supports 100+ LLMs&lt;/li&gt; &lt;li&gt;Supports local Ollama or vLLM setups&lt;/li&gt; &lt;li&gt;6000+ Embedding Models&lt;/li&gt; &lt;li&gt;50+ File extensions supported (Added Docling recently)&lt;/li&gt; &lt;li&gt;Podcasts support with local TTS providers (Kokoro TTS)&lt;/li&gt; &lt;li&gt;Connects with 15+ external sources such as Search Engines, Slack, Notion, Gmail, Notion, Confluence etc&lt;/li&gt; &lt;li&gt;Cross-Browser Extension to let you save any dynamic webpage you want, including authenticated content.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Upcoming Planned Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Agentic chat&lt;/li&gt; &lt;li&gt;Note Management (Like Notion)&lt;/li&gt; &lt;li&gt;Multi Collaborative Chats.&lt;/li&gt; &lt;li&gt;Multi Collaborative Documents.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Installation (Self-Host)&lt;/strong&gt;&lt;/p&gt; &lt;h1&gt;Linux/macOS:&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;docker run -d -p 3000:3000 -p 8000:8000 \ -v surfsense-data:/data \ --name surfsense \ --restart unless-stopped \ ghcr.io/modsetter/surfsense:latest &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Windows (PowerShell):&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;docker run -d -p 3000:3000 -p 8000:8000 ` -v surfsense-data:/data ` --name surfsense ` --restart unless-stopped ` ghcr.io/modsetter/surfsense:latest &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/MODSetter/SurfSense"&gt;https://github.com/MODSetter/SurfSense&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Uiqueblhats"&gt; /u/Uiqueblhats &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnusq8/open_source_alternative_to_perplexity/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnusq8/open_source_alternative_to_perplexity/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pnusq8/open_source_alternative_to_perplexity/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T06:16:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnep8j</id>
    <title>Key Highlights of AI2's New Byte Level LLM: Bolmo</title>
    <updated>2025-12-15T18:18:55+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;[1] Bolmo: First Fully Open Byte-Level Language Models&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Processes raw UTF-8 bytes instead of subword tokens, improving handling of spelling, whitespace, rare words, and multilingual text without a fixed vocabulary.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;[2] Built on Olmo 3 Transformer Backbone&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Rather than training from scratch, Bolmo reuses a strong subword Olmo 3 model and retrofits it into a byte-level model, enabling competitive performance with lower training cost.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;[3] Two-Stage Training for Efficiency&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Stage 1:&lt;/strong&gt; Train local encoder, decoder, and boundary predictor while freezing the transformer — fast learning with fewer tokens.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Stage 2:&lt;/strong&gt; Unfreeze and train globally for deeper byte-level understanding while keeping efficiency.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;[4] Strong Task Performance&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Competitive on Core LLM Benchmarks:&lt;/strong&gt; Bolmo 7B rivals its subword Olmo 3 counterpart across math, reasoning, QA, code, and general knowledge tasks.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Excels in Character-Focused Benchmarks:&lt;/strong&gt; Substantially better accuracy on character-centered tests like CUTE and EXECUTE compared to the base Olmo models.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;[5] Fully Open Ecosystem&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Open Weights, Code, Data &amp;amp; Reports:&lt;/strong&gt; Bolmo 1B and 7B checkpoints, training code, tech reports, and datasets are publicly available.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Source: &lt;a href="https://allenai.org/blog/bolmo"&gt;https://allenai.org/blog/bolmo&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnep8j/key_highlights_of_ai2s_new_byte_level_llm_bolmo/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnep8j/key_highlights_of_ai2s_new_byte_level_llm_bolmo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pnep8j/key_highlights_of_ai2s_new_byte_level_llm_bolmo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T18:18:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnusp9</id>
    <title>Alibaba Open-Sources CosyVoice 3, a New TTS Model</title>
    <updated>2025-12-16T06:16:09+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Key Features&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Language Coverage&lt;/strong&gt;: Covers 9 common languages (Chinese, English, Japanese, Korean, German, Spanish, French, Italian, Russian), 18+ Chinese dialects/accents and meanwhile supports both multi-lingual/cross-lingual zero-shot voice cloning.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Content Consistency &amp;amp; Naturalness&lt;/strong&gt;: Achieves state-of-the-art performance in content consistency, speaker similarity, and prosody naturalness.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Pronunciation Inpainting&lt;/strong&gt;: Supports pronunciation inpainting of Chinese Pinyin and English CMU phonemes, providing more controllability and thus suitable for production use.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Text Normalization&lt;/strong&gt;: Supports reading of numbers, special symbols and various text formats without a traditional frontend module.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Bi-Streaming&lt;/strong&gt;: Support both text-in streaming and audio-out streaming, and achieves latency as low as 150ms while maintaining high-quality audio output.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Instruct Support&lt;/strong&gt;: Supports various instructions such as languages, dialects, emotions, speed, volume, etc.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Weight: &lt;a href="https://huggingface.co/FunAudioLLM/Fun-CosyVoice3-0.5B-2512"&gt;https://huggingface.co/FunAudioLLM/Fun-CosyVoice3-0.5B-2512&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Paper: &lt;a href="https://arxiv.org/abs/2505.17589"&gt;https://arxiv.org/abs/2505.17589&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnusp9/alibaba_opensources_cosyvoice_3_a_new_tts_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnusp9/alibaba_opensources_cosyvoice_3_a_new_tts_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pnusp9/alibaba_opensources_cosyvoice_3_a_new_tts_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T06:16:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnmaya</id>
    <title>My llama.cpp fork: GLM-4V vision, Qwen3-Next Delta-Net kernels, Devstral YaRN fix</title>
    <updated>2025-12-15T23:20:41+00:00</updated>
    <author>
      <name>/u/hauhau901</name>
      <uri>https://old.reddit.com/user/hauhau901</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnmaya/my_llamacpp_fork_glm4v_vision_qwen3next_deltanet/"&gt; &lt;img alt="My llama.cpp fork: GLM-4V vision, Qwen3-Next Delta-Net kernels, Devstral YaRN fix" src="https://b.thumbs.redditmedia.com/14523J2vjsPH5Z5O1mgmcTfh5YqTtPLAg9bqWJHQZrE.jpg" title="My llama.cpp fork: GLM-4V vision, Qwen3-Next Delta-Net kernels, Devstral YaRN fix" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I’ve been hacking on a few llama.cpp things that aren’t upstream yet and figured I’d share in case they help someone.&lt;/p&gt; &lt;p&gt;I’ve got GLM-4V (Tested on 4.6V Flash, full 4.6V momentarily) running with full multimodal vision support now. Vision uses proper 2D RoPE for spatial positions while text stays sequential, image resolution is handled dynamically with aspect ratio preserved, and patch embedding follows the EVA-style Conv3D setup (basically dual Conv2D). Works fine with the usual &lt;code&gt;llama-server -m GLM-4.6V-Flash.gguf --mmproj GLM-4.6V-Flash-mmproj.gguf -ngl 99&lt;/code&gt; flow.&lt;/p&gt; &lt;p&gt;On the Qwen3-Next side, I added custom CUDA kernels for the Delta-Net linear attention layers. There’s a Blackwell-optimized path that keeps the full 128×128 state in shared memory, plus an FP16 kernel using &lt;code&gt;hfma2&lt;/code&gt; for roughly 2× throughput. On an RTX 6000 Pro I’m seeing ~45–55 tok/s with Q4/MXFP4 and around ~40 tok/s with BF16.&lt;/p&gt; &lt;p&gt;I also fixed an attention scaling issue with YaRN on Devstral / Mistral-3 that shows up when you extend context — looks related to upstream issue #17980.&lt;/p&gt; &lt;p&gt;Fork’s here if you want to poke around: &lt;a href="https://github.com/hauhaut/llama.cpp"&gt;https://github.com/hauhaut/llama.cpp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you’re a contributor and want to use or merge any of this, feel free. A small acknowledgment would be appreciated. Happy to answer questions.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/y69d6or9bg7g1.png?width=799&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8f920876ce585ddf6b0e922850b3d433cb779c10"&gt;https://preview.redd.it/y69d6or9bg7g1.png?width=799&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8f920876ce585ddf6b0e922850b3d433cb779c10&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hauhau901"&gt; /u/hauhau901 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnmaya/my_llamacpp_fork_glm4v_vision_qwen3next_deltanet/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnmaya/my_llamacpp_fork_glm4v_vision_qwen3next_deltanet/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pnmaya/my_llamacpp_fork_glm4v_vision_qwen3next_deltanet/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T23:20:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnslcb</id>
    <title>My Local coding agent worked 2 hours unsupervised and here is my setup</title>
    <updated>2025-12-16T04:14:38+00:00</updated>
    <author>
      <name>/u/Express_Quail_1493</name>
      <uri>https://old.reddit.com/user/Express_Quail_1493</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Setup&lt;/p&gt; &lt;p&gt;--- Model&lt;br /&gt; devstral-small-2 from bartowski IQ3_xxs version.&lt;br /&gt; Run with lm studio &amp;amp; intentionally limit the context at 40960 which should't take more than (14gb ram even when context is full)&lt;/p&gt; &lt;p&gt;---Tool&lt;br /&gt; kilo code (set file limit to 500 lines) it will read in chunks&lt;br /&gt; 40960 ctx limit is actually a strength not weakness (more ctx = easier confusion)&lt;br /&gt; Paired with qdrant in the kilo code UI.&lt;br /&gt; Setup the indexing with qdrant (the little database icon) use model &lt;a href="https://ollama.com/toshk0/nomic-embed-text-v2-moe"&gt;https://ollama.com/toshk0/nomic-embed-text-v2-moe&lt;/a&gt; in ollama (i choose ollama to keep indexing and seperate from Lm studio to allow lm studio to focus on the heavy lifting)&lt;/p&gt; &lt;p&gt;--Result&lt;br /&gt; minimal drift on tasks&lt;br /&gt; slight errors on tool call but the model quickly realign itself. A oneshot prompt implimentation of a new feature in my codebase in architect mode resulted in 2 hours of coding unsupervised kilo code auto switches to code mode to impliment after planning in architect mode which is amazing. Thats been my lived experience&lt;/p&gt; &lt;p&gt;Feel free to also share your fully localhost setup that also solved long running tasks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Express_Quail_1493"&gt; /u/Express_Quail_1493 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnslcb/my_local_coding_agent_worked_2_hours_unsupervised/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnslcb/my_local_coding_agent_worked_2_hours_unsupervised/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pnslcb/my_local_coding_agent_worked_2_hours_unsupervised/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T04:14:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnnkxc</id>
    <title>Qwen3 next 80B w/ 250k tok context fits fully on one 7900 XTX (24 GB) and runs at 41 tok/s</title>
    <updated>2025-12-16T00:16:44+00:00</updated>
    <author>
      <name>/u/1ncehost</name>
      <uri>https://old.reddit.com/user/1ncehost</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Late to the party, but better late than never. Using IQ2_XSS quant, Q4_0 KV quants, &amp;amp; FA enabled.&lt;/p&gt; &lt;p&gt;I feel like this is a major milestone in general for single card LLM usage. It seems very usable for programming at this quant level.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1ncehost"&gt; /u/1ncehost &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnnkxc/qwen3_next_80b_w_250k_tok_context_fits_fully_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnnkxc/qwen3_next_80b_w_250k_tok_context_fits_fully_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pnnkxc/qwen3_next_80b_w_250k_tok_context_fits_fully_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T00:16:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnib03</id>
    <title>This price jumping for older hardware is insane</title>
    <updated>2025-12-15T20:36:24+00:00</updated>
    <author>
      <name>/u/Savantskie1</name>
      <uri>https://old.reddit.com/user/Savantskie1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;About two weeks ago maybe a tad longer but not much, i was looking at MI50 32GB's to upgrade my rig. They were around $160-$200. Now looking on Ebay, they're nearly $300 to $500! That jump in just two weeks is insane. Same as DDR4 ram. That nearly doubled overnight. I was looking at a 64GB kit to upgrade my current 32GB kit. And it nearly trippled in price. This is fucking ridiculous! And now with Micron killing Crucial for consumers? This is damn near the Crypto Currency boom all over again. And it's looking to last a lot longer.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Savantskie1"&gt; /u/Savantskie1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnib03/this_price_jumping_for_older_hardware_is_insane/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnib03/this_price_jumping_for_older_hardware_is_insane/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pnib03/this_price_jumping_for_older_hardware_is_insane/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T20:36:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnjdx9</id>
    <title>Ryzen 395 (Strix Halo) massive performance degradation at high context with ROCm bug I found, may explain speed differences between ROCm and Vulkan with llama-cpp</title>
    <updated>2025-12-15T21:21:57+00:00</updated>
    <author>
      <name>/u/Goldkoron</name>
      <uri>https://old.reddit.com/user/Goldkoron</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;To preface this, I can only confirm this happens on Windows, but if it happens on Linux too it might explain why in some benchmarks Vulkan appeared to have faster token generation yet slower prompt processing speeds.&lt;/p&gt; &lt;p&gt;ROCm has up to 3x the prompt processing speed than Vulkan, but I had noticed for some reason it massively falls behind on token generation at high context.&lt;/p&gt; &lt;p&gt;It turns out that as long as you have 96GB set in UMA in BIOS for the igpu, llama-cpp dumps all the KV cache into shared memory instead of igpu memory, and it seems shared memory is the culprit for the massive slowdown in speed. I tried comparing a 40GB size quant of Qwen3 Next at 64k context with ROCm, and when 96gb was set in UMA, it dumped KV cache into shared memory and token generation speed was 9t/s. When I set UMA to 64GB, token generation speed at same prompt was 23t/s.&lt;/p&gt; &lt;p&gt;In comparison, Vulkan got around 21t/s but was literally more than 3x the prompt processing time. (640s vs 157s).&lt;/p&gt; &lt;p&gt;If anyone has a Linux setup and can confirm or deny whether this happens there it would help. I also have a bug report on github.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/issues/18011"&gt;https://github.com/ggml-org/llama.cpp/issues/18011&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This does also happen for Lemonade llama-cpp builds which typically use latest builds of ROCm.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Goldkoron"&gt; /u/Goldkoron &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnjdx9/ryzen_395_strix_halo_massive_performance/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnjdx9/ryzen_395_strix_halo_massive_performance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pnjdx9/ryzen_395_strix_halo_massive_performance/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T21:21:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1pndzy7</id>
    <title>Bolmo-the first family of competitive fully open byte-level language models (LMs) at the 1B and 7B parameter scales.</title>
    <updated>2025-12-15T17:52:18+00:00</updated>
    <author>
      <name>/u/BreakfastFriendly728</name>
      <uri>https://old.reddit.com/user/BreakfastFriendly728</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pndzy7/bolmothe_first_family_of_competitive_fully_open/"&gt; &lt;img alt="Bolmo-the first family of competitive fully open byte-level language models (LMs) at the 1B and 7B parameter scales." src="https://b.thumbs.redditmedia.com/vi4EeoMmrDA7LW8-ozOS-EvPxXycfomN1kwGNq7doIA.jpg" title="Bolmo-the first family of competitive fully open byte-level language models (LMs) at the 1B and 7B parameter scales." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/collections/allenai/bolmo"&gt;https://huggingface.co/collections/allenai/bolmo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/allenai/bolmo-core"&gt;https://github.com/allenai/bolmo-core&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.datocms-assets.com/64837/1765814974-bolmo.pdf"&gt;https://www.datocms-assets.com/64837/1765814974-bolmo.pdf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/h6jffcdune7g1.png?width=2616&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f15bc148dc0d4cffc997ccb8356f7c5244f80cb4"&gt;https://preview.redd.it/h6jffcdune7g1.png?width=2616&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f15bc148dc0d4cffc997ccb8356f7c5244f80cb4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What are byte-level language models?&lt;/p&gt; &lt;p&gt;Byte-level language models (LMs) are a class of models that process text by tokenizing the input into &lt;strong&gt;UTF-8 bytes&lt;/strong&gt; (a smaller set of finer-grained atomic units) instead of relying on the traditional subword tokenization approach. In this context, UTF-8 is considered the tokenizer, and the vocabulary consists of the 256 distinct bytes.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BreakfastFriendly728"&gt; /u/BreakfastFriendly728 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pndzy7/bolmothe_first_family_of_competitive_fully_open/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pndzy7/bolmothe_first_family_of_competitive_fully_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pndzy7/bolmothe_first_family_of_competitive_fully_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T17:52:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnc045</id>
    <title>status of Nemotron 3 Nano support in llama.cpp</title>
    <updated>2025-12-15T16:38:13+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnc045/status_of_nemotron_3_nano_support_in_llamacpp/"&gt; &lt;img alt="status of Nemotron 3 Nano support in llama.cpp" src="https://preview.redd.it/glwccqikbe7g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=71433581f884eabbfd427838b07ab54bbdbbd438" title="status of Nemotron 3 Nano support in llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/18058"&gt;https://github.com/ggml-org/llama.cpp/pull/18058&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/glwccqikbe7g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnc045/status_of_nemotron_3_nano_support_in_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pnc045/status_of_nemotron_3_nano_support_in_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T16:38:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1pn8h5h</id>
    <title>NVIDIA Nemotron 3 Nano 30B A3B released</title>
    <updated>2025-12-15T14:18:28+00:00</updated>
    <author>
      <name>/u/rerri</name>
      <uri>https://old.reddit.com/user/rerri</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16"&gt;https://huggingface.co/nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-Base-BF16"&gt;https://huggingface.co/nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-Base-BF16&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Unsloth GGUF quants: &lt;a href="https://huggingface.co/unsloth/Nemotron-3-Nano-30B-A3B-GGUF/tree/main"&gt;https://huggingface.co/unsloth/Nemotron-3-Nano-30B-A3B-GGUF/tree/main&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Nvidia blog post: &lt;a href="https://developer.nvidia.com/blog/inside-nvidia-nemotron-3-techniques-tools-and-data-that-make-it-efficient-and-accurate/"&gt;https://developer.nvidia.com/blog/inside-nvidia-nemotron-3-techniques-tools-and-data-that-make-it-efficient-and-accurate/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;HF blog post: &lt;a href="https://huggingface.co/blog/nvidia/nemotron-3-nano-efficient-open-intelligent-models"&gt;https://huggingface.co/blog/nvidia/nemotron-3-nano-efficient-open-intelligent-models&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Highlights (copy-pasta from HF blog):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Hybrid Mamba-Transformer MoE architecture:&lt;/strong&gt; Mamba‑2 for long-context, low-latency inference combined with transformer attention for high-accuracy, fine-grained reasoning&lt;/li&gt; &lt;li&gt;&lt;strong&gt;31.6B total parameters, ~3.6B active per token:&lt;/strong&gt; Designed for high throughput and low latency&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Exceptional inference efficiency:&lt;/strong&gt; Up to 4x faster than Nemotron Nano 2 and up to 3.3x faster than leading models in its size category&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Best-in-class reasoning accuracy:&lt;/strong&gt; Across reasoning, coding, tools, and multi-step agentic tasks&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Reasoning controls:&lt;/strong&gt; Reasoning ON/OFF modes plus a configurable thinking budget to cap “thinking” tokens and keep inference cost predictable&lt;/li&gt; &lt;li&gt;&lt;strong&gt;1M-token context window:&lt;/strong&gt; Ideal for long-horizon workflows, retrieval-augmented tasks, and persistent memory&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Fully open:&lt;/strong&gt; Open Weights, datasets, training recipes, and framework&lt;/li&gt; &lt;li&gt;&lt;strong&gt;A full open data stack&lt;/strong&gt;: 3T new high-quality pre-training tokens, 13M cross-disciplinary post-training samples, 10+ RL environments with datasets covering more than 900k tasks in math, coding, reasoning, and tool-use, and ~11k agent-safety traces&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Easy deployment:&lt;/strong&gt; Seamless serving with vLLM and SGLang, and integration via OpenRouter, popular inference service providers, and &lt;a href="http://build.nvidia.com"&gt;build.nvidia.com&lt;/a&gt; endpoints&lt;/li&gt; &lt;li&gt;&lt;strong&gt;License:&lt;/strong&gt; Released under the &lt;a href="https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-open-model-license/"&gt;nvidia-open-model-license&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;PS. Nemotron 3 Super (~4x bigger than Nano) and Ultra (~16x bigger than Nano) to follow.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rerri"&gt; /u/rerri &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn8h5h/nvidia_nemotron_3_nano_30b_a3b_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn8h5h/nvidia_nemotron_3_nano_30b_a3b_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pn8h5h/nvidia_nemotron_3_nano_30b_a3b_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T14:18:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnb824</id>
    <title>Chatterbox Turbo, new open-source voice AI model, just released on Hugging Face</title>
    <updated>2025-12-15T16:08:45+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnb824/chatterbox_turbo_new_opensource_voice_ai_model/"&gt; &lt;img alt="Chatterbox Turbo, new open-source voice AI model, just released on Hugging Face" src="https://external-preview.redd.it/Mno1ZHBiNTg0ZTdnMetROQBwb-dMzbNK88p-4KlSnzkAfcO7Jy5xOmtEL7Fy.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ea250dde0c1f1556ba5b404754e09373d2c88623" title="Chatterbox Turbo, new open-source voice AI model, just released on Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Links:&lt;br /&gt; - Model (PyTorch): &lt;a href="https://huggingface.co/ResembleAI/chatterbox-turbo"&gt;https://huggingface.co/ResembleAI/chatterbox-turbo&lt;/a&gt;&lt;br /&gt; - Model (ONNX): &lt;a href="https://huggingface.co/ResembleAI/chatterbox-turbo-ONNX"&gt;https://huggingface.co/ResembleAI/chatterbox-turbo-ONNX&lt;/a&gt;&lt;br /&gt; - GitHub: &lt;a href="https://github.com/resemble-ai/chatterbox"&gt;https://github.com/resemble-ai/chatterbox&lt;/a&gt;&lt;br /&gt; - Demo: &lt;a href="https://huggingface.co/spaces/ResembleAI/chatterbox-turbo-demo"&gt;https://huggingface.co/spaces/ResembleAI/chatterbox-turbo-demo&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/6v5yql484e7g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnb824/chatterbox_turbo_new_opensource_voice_ai_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pnb824/chatterbox_turbo_new_opensource_voice_ai_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T16:08:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnllux</id>
    <title>New budget local AI rig</title>
    <updated>2025-12-15T22:51:32+00:00</updated>
    <author>
      <name>/u/vucamille</name>
      <uri>https://old.reddit.com/user/vucamille</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnllux/new_budget_local_ai_rig/"&gt; &lt;img alt="New budget local AI rig" src="https://preview.redd.it/6aavy1486g7g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=505d5c7891c288215bdfa28b4ea82e8ed8df45bc" title="New budget local AI rig" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to buy 32GB Mi50s but decided against it because of their recent inflated prices. However, the 16GB versions are still affordable! I might buy another one in the future, or wait until the 32GB gets cheaper again.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Qiyida X99 mobo with 32GB RAM and Xeon E5 2680 V4: 90 USD (AliExpress)&lt;/li&gt; &lt;li&gt;2x MI50 16GB with dual fan mod: 108 USD each plus 32 USD shipping (Alibaba)&lt;/li&gt; &lt;li&gt;1200W PSU bought in my country: 160 USD - lol the most expensive component in the PC&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In total, I spent about 650 USD. ROCm 7.0.2 works, and I have done some basic inference tests with llama.cpp and the two MI50, everything works well. Initially I tried with the latest ROCm release but multi GPU was not working for me.&lt;/p&gt; &lt;p&gt;I still need to buy brackets to prevent the bottom MI50 from sagging and maybe some decorations and LEDs, but so far super happy! And as a bonus, this thing can game!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vucamille"&gt; /u/vucamille &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6aavy1486g7g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnllux/new_budget_local_ai_rig/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pnllux/new_budget_local_ai_rig/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T22:51:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnd5uf</id>
    <title>They're finally here (Radeon 9700)</title>
    <updated>2025-12-15T17:20:23+00:00</updated>
    <author>
      <name>/u/Zeikos</name>
      <uri>https://old.reddit.com/user/Zeikos</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnd5uf/theyre_finally_here_radeon_9700/"&gt; &lt;img alt="They're finally here (Radeon 9700)" src="https://b.thumbs.redditmedia.com/LlhzLUprDuJWJk6b4cZsmRPc06FSQCX3yS5XKj_YEOk.jpg" title="They're finally here (Radeon 9700)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zeikos"&gt; /u/Zeikos &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pnd5uf"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnd5uf/theyre_finally_here_radeon_9700/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pnd5uf/theyre_finally_here_radeon_9700/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T17:20:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1pn37mw</id>
    <title>New Google model incoming!!!</title>
    <updated>2025-12-15T09:26:05+00:00</updated>
    <author>
      <name>/u/R46H4V</name>
      <uri>https://old.reddit.com/user/R46H4V</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn37mw/new_google_model_incoming/"&gt; &lt;img alt="New Google model incoming!!!" src="https://preview.redd.it/ho8nhiae6c7g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=53bdb437b0e3d6b162a1f97be9b2f4ae540eda69" title="New Google model incoming!!!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/osanseviero/status/2000493503860892049?s=20"&gt;https://x.com/osanseviero/status/2000493503860892049?s=20&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/google"&gt;https://huggingface.co/google&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/R46H4V"&gt; /u/R46H4V &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ho8nhiae6c7g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn37mw/new_google_model_incoming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pn37mw/new_google_model_incoming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T09:26:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1pn8upp</id>
    <title>NVIDIA releases Nemotron 3 Nano, a new 30B hybrid reasoning model!</title>
    <updated>2025-12-15T14:34:28+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn8upp/nvidia_releases_nemotron_3_nano_a_new_30b_hybrid/"&gt; &lt;img alt="NVIDIA releases Nemotron 3 Nano, a new 30B hybrid reasoning model!" src="https://preview.redd.it/sic85bvhpd7g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d01067e3a3899e680b913799c37c8ef9b609ff4c" title="NVIDIA releases Nemotron 3 Nano, a new 30B hybrid reasoning model!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Unsloth GGUF: &lt;a href="https://huggingface.co/unsloth/Nemotron-3-Nano-30B-A3B-GGUF"&gt;https://huggingface.co/unsloth/Nemotron-3-Nano-30B-A3B-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Nemotron 3 has a 1M context window and the best in class performance for SWE-Bench, reasoning and chat.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sic85bvhpd7g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn8upp/nvidia_releases_nemotron_3_nano_a_new_30b_hybrid/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pn8upp/nvidia_releases_nemotron_3_nano_a_new_30b_hybrid/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T14:34:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnfaqo</id>
    <title>I'm strong enough to admit that this bugs the hell out of me</title>
    <updated>2025-12-15T18:40:57+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnfaqo/im_strong_enough_to_admit_that_this_bugs_the_hell/"&gt; &lt;img alt="I'm strong enough to admit that this bugs the hell out of me" src="https://preview.redd.it/9xkz6sfcxe7g1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=55149229a153c6c87d61ae1aa53e61a1b3a65df8" title="I'm strong enough to admit that this bugs the hell out of me" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9xkz6sfcxe7g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnfaqo/im_strong_enough_to_admit_that_this_bugs_the_hell/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pnfaqo/im_strong_enough_to_admit_that_this_bugs_the_hell/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T18:40:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1pniwfj</id>
    <title>Ai2 Open Modeling AMA ft researchers from the Molmo and Olmo teams.</title>
    <updated>2025-12-15T21:02:55+00:00</updated>
    <author>
      <name>/u/ai2_official</name>
      <uri>https://old.reddit.com/user/ai2_official</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pniwfj/ai2_open_modeling_ama_ft_researchers_from_the/"&gt; &lt;img alt="Ai2 Open Modeling AMA ft researchers from the Molmo and Olmo teams." src="https://b.thumbs.redditmedia.com/bsv34WIHZXC49Az9mFES5lSIAtaQ2CuLZJ4dCaLxsEY.jpg" title="Ai2 Open Modeling AMA ft researchers from the Molmo and Olmo teams." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Tuesday, Dec 16 from 1-2pm PST, join us for an AMA with researchers and engineers from Ai2, the nonprofit AI lab behind the fully open Olmo &amp;amp; Molmo models. &lt;/p&gt; &lt;p&gt;Please feel free to ask your questions now! Our team will begin answering them as soon as the AMA begins. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fxw1g2fcmf7g1.jpg?width=1080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=009a9377edfefefc5efd52db0af81b807b9971b8"&gt;https://preview.redd.it/fxw1g2fcmf7g1.jpg?width=1080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=009a9377edfefefc5efd52db0af81b807b9971b8&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ai2_official"&gt; /u/ai2_official &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pniwfj/ai2_open_modeling_ama_ft_researchers_from_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pniwfj/ai2_open_modeling_ama_ft_researchers_from_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pniwfj/ai2_open_modeling_ama_ft_researchers_from_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T21:02:55+00:00</published>
  </entry>
</feed>
