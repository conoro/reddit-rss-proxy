<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-16T17:12:24+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1pnj0ad</id>
    <title>Needing advice for 4 x P4000 setup</title>
    <updated>2025-12-15T21:07:35+00:00</updated>
    <author>
      <name>/u/Radiant-Giraffe5159</name>
      <uri>https://old.reddit.com/user/Radiant-Giraffe5159</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a computer with 4 x P4000s and would like to get the most out of them. I‚Äôve played with ollama and now LM Studio and found the speculative decoding worth the change from ollama to LM studio. Now finding this sub it appears vllm would be better for my use case as I could use tensor parallelism to speed up my setup even more. I‚Äôm pretty tech savvy and have setup a proxmox cluster and dipped my toe into linux so I‚Äôm ok with troubleshooting as long as the juice is worth the squeeze. My main use case for this setup is using a plugin in obsidian notes for long context text generation as well as hosting my own ai website using openwebui. Is it worth trying to learn and use vllm or should I just stick it out with lm studio? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Radiant-Giraffe5159"&gt; /u/Radiant-Giraffe5159 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnj0ad/needing_advice_for_4_x_p4000_setup/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnj0ad/needing_advice_for_4_x_p4000_setup/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pnj0ad/needing_advice_for_4_x_p4000_setup/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T21:07:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnog7m</id>
    <title>DevTracker: an open-source governance layer for human‚ÄìLLM collaboration (external memory, semantic safety)</title>
    <updated>2025-12-16T00:55:41+00:00</updated>
    <author>
      <name>/u/lexseasson</name>
      <uri>https://old.reddit.com/user/lexseasson</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just published DevTracker, an open-source governance and external memory layer for human‚ÄìLLM collaboration. The problem I kept seeing in agentic systems is not model quality ‚Äî it‚Äôs governance drift. In real production environments, project truth fragments across: Git (what actually changed), Jira / tickets (what was decided), chat logs (why it changed), docs (intent, until it drifts), spreadsheets (ownership and priorities). When LLMs or agent fleets operate in this environment, two failure modes appear: Fragmented truth Agents cannot reliably answer: what is approved, what is stable, what changed since last decision? Semantic overreach Automation starts rewriting human intent (priority, roadmap, ownership) because there is no enforced boundary. The core idea DevTracker treats a tracker as a governance contract, not a spreadsheet. Humans own semantics purpose, priority, roadmap, business intent Automation writes evidence git state, timestamps, lifecycle signals, quality metrics Metrics are opt-in and reversible quality, confidence, velocity, churn, stability Every update is proposed, auditable, and reversible explicit apply flags, backups, append-only journal Governance is enforced by structure, not by convention. How it works (end-to-end) DevTracker runs as a repo auditor + tracker maintainer: Sanitizes a canonical, Excel-friendly CSV tracker Audits Git state (diff + status + log) Runs a quality suite (pytest, ruff, mypy) Produces reviewable CSV proposals (core vs metrics separated) Applies only allowed fields under explicit flags Outputs are dual-purpose: JSON snapshots for dashboards / tool calling Markdown reports for humans and audits CSV proposals for review and approval Where this fits Cloud platforms (Azure / Google / AWS) control execution Governance-as-a-Service platforms enforce policy DevTracker governs meaning and operational memory It sits between cognition and execution ‚Äî exactly where agentic systems tend to fail. Links üìÑ Medium (architecture + rationale): &lt;a href="https://medium.com/@eugeniojuanvaras/why-human-llm-collaboration-fails-without-explicit-governance-f171394abc67"&gt;https://medium.com/@eugeniojuanvaras/why-human-llm-collaboration-fails-without-explicit-governance-f171394abc67&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üß† GitHub repo (open-source): &lt;a href="https://github.com/lexseasson/devtracker-governance"&gt;https://github.com/lexseasson/devtracker-governance&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Looking for feedback &amp;amp; collaborators I‚Äôm especially interested in: multi-repo governance patterns, API surfaces for safe LLM tool calling, approval workflows in regulated environments. If you‚Äôre a staff engineer, platform architect, applied researcher, or recruiter working around agentic systems, I‚Äôd love to hear your perspective.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lexseasson"&gt; /u/lexseasson &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnog7m/devtracker_an_opensource_governance_layer_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnog7m/devtracker_an_opensource_governance_layer_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pnog7m/devtracker_an_opensource_governance_layer_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T00:55:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1pn4nfj</id>
    <title>Diagnosing layer sensitivity during post training quantization</title>
    <updated>2025-12-15T11:01:44+00:00</updated>
    <author>
      <name>/u/elinaembedl</name>
      <uri>https://old.reddit.com/user/elinaembedl</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn4nfj/diagnosing_layer_sensitivity_during_post_training/"&gt; &lt;img alt="Diagnosing layer sensitivity during post training quantization" src="https://preview.redd.it/9z863k4bnc7g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a3ec905397065d7f20bc37620997e8c9962893e9" title="Diagnosing layer sensitivity during post training quantization" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone!&lt;br /&gt; I wrote about this a while ago. I have written a blog post on using layerwise PSNR to diagnose where models break during post-training quantization.&lt;/p&gt; &lt;p&gt;Instead of only checking output accuracy, layerwise metrics let you spot exactly which layers are sensitive (e.g. softmax, SE blocks), making it easier to debug and decide what to keep in higher precision.&lt;/p&gt; &lt;p&gt;If you‚Äôre experimenting with quantization for local or edge inference, you might find this interesting: &lt;a href="https://hub.embedl.com/blog/diagnosing-layer-sensitivity?utm_source=reddit"&gt;blogpost link&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Has anyone tried similar layerwise diagnostics? I‚Äôd love to hear about your experiences.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/elinaembedl"&gt; /u/elinaembedl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9z863k4bnc7g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn4nfj/diagnosing_layer_sensitivity_during_post_training/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pn4nfj/diagnosing_layer_sensitivity_during_post_training/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T11:01:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnt63g</id>
    <title>Open-sourced a dynamic agent orchestrator (Hatchify). Need architectural feedback on Graph Logic, MCP, and Roadmap.</title>
    <updated>2025-12-16T04:45:17+00:00</updated>
    <author>
      <name>/u/rickgogogo</name>
      <uri>https://old.reddit.com/user/rickgogogo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnt63g/opensourced_a_dynamic_agent_orchestrator_hatchify/"&gt; &lt;img alt="Open-sourced a dynamic agent orchestrator (Hatchify). Need architectural feedback on Graph Logic, MCP, and Roadmap." src="https://b.thumbs.redditmedia.com/UqD1wf90ytoakwL8yuwm9vYv4LUxvh14oPqx4Cu8oGk.jpg" title="Open-sourced a dynamic agent orchestrator (Hatchify). Need architectural feedback on Graph Logic, MCP, and Roadmap." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;We recently open-sourced &lt;strong&gt;Hatchify AI&lt;/strong&gt;, a multi-agent orchestration engine we‚Äôve been building. It‚Äôs designed to handle complex workflows using dynamic routing and the &lt;strong&gt;MCP&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;It sits on top of &lt;code&gt;litellm&lt;/code&gt; (so it supports OpenAI, Claude, Gemini, and local endpoints via Ollama/vLLM)&lt;/p&gt; &lt;p&gt;The core logic is working, and the core code is completely open source. Everyone is free to use it directly for commercial purposes. If it is helpful to you, we would also like to collect some feedback, including:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Config DX:&lt;/strong&gt; Currently, Models and MCP tools are configured via raw config files (YAML/JSON). Is this manageable for you, or is a &lt;strong&gt;frontend configuration UI&lt;/strong&gt; a critical &amp;quot;must-have&amp;quot; for early adoption?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Graph Topology:&lt;/strong&gt; We‚Äôve implemented validation logic for the workflow graphs (checking for cycles, dead ends, etc.). If anyone dives into the code, does the validation feel robust enough, or are we missing edge cases in complex DAGs?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Node Types:&lt;/strong&gt; Apart from the standard LLM/Tool nodes, what custom node types are missing for your actual use cases? (e.g., Human-in-the-loop, conditional delays, broadcast nodes?)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RAG Integration:&lt;/strong&gt; Should we build a native &lt;strong&gt;RAG Node&lt;/strong&gt; directly into the core, or keep RAG decoupled via MCP tools/external APIs?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Code Interpreter:&lt;/strong&gt; We are debating adding a &lt;strong&gt;Code Interpreter Node&lt;/strong&gt; (Sandboxed Python execution). Is the complexity/security risk worth it, or do you prefer handling execution outside the orchestrator?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Routing Logic:&lt;/strong&gt; Currently, routing relies on standard logical operators (AND/OR/IF). Do you see a need for &lt;strong&gt;Semantic/Embedding-based routing&lt;/strong&gt; (routing based on vector similarity), or is logic-based usually enough?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Website/UI Generation:&lt;/strong&gt; The current implementation for the &amp;quot;Website Generator&amp;quot; feature is: &lt;em&gt;Backend generates code -&amp;gt; Builds -&amp;gt; Mounts as static resource&lt;/em&gt;. It feels a bit heavy. Is there a cleaner architectural pattern you‚Äôd recommend for this (e.g., purely client-side rendering or streaming artifacts)?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Repo:&lt;/strong&gt; &lt;a href="https://github.com/Sider-ai/hatchify"&gt;https://github.com/Sider-ai/hatchify&lt;/a&gt; &lt;strong&gt;Docs/Demo:&lt;/strong&gt; &lt;a href="https://hatchify.ai/"&gt;https://hatchify.ai/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We appreciate any insights, even if you just pick one point to answer. Feel free to roast the code.&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9psurp4bxh7g1.png?width=1792&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3c95f8015fff1e2c800f05b3bf06df60a9c20f55"&gt;https://preview.redd.it/9psurp4bxh7g1.png?width=1792&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3c95f8015fff1e2c800f05b3bf06df60a9c20f55&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rickgogogo"&gt; /u/rickgogogo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnt63g/opensourced_a_dynamic_agent_orchestrator_hatchify/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnt63g/opensourced_a_dynamic_agent_orchestrator_hatchify/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pnt63g/opensourced_a_dynamic_agent_orchestrator_hatchify/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T04:45:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnvfu5</id>
    <title>Is Ilya Sutskever trying with a secret sauce method now?</title>
    <updated>2025-12-16T06:55:14+00:00</updated>
    <author>
      <name>/u/Famous-Associate-436</name>
      <uri>https://old.reddit.com/user/Famous-Associate-436</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm curious why nobody is talking about this&lt;/p&gt; &lt;p&gt;RL learning method improvement with value function.&lt;/p&gt; &lt;p&gt;just watch his newest podcast, he's basically allure to that when talking about his SSI , the current training inefficiency of o1/r1 RL paradigms and the relation between human evolution and emotion/value function.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=aR20FWCCjAs"&gt;Ilya Sutskever ‚Äì We're moving from the age of scaling to the age of research&lt;/a&gt;&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Starting from 13:26 to 15:34&lt;/p&gt; &lt;p&gt;But what is that? How do you think about emotions? What is the ML analogy for emotions? &lt;/p&gt; &lt;p&gt;......&lt;/p&gt; &lt;p&gt;It should be some kind of a value function thing. But I don‚Äôt think there is a great ML analogy &lt;/p&gt; &lt;p&gt;because right now, value functions don't play a very prominent role in the things people do. &lt;/p&gt; &lt;p&gt;That's how o1, R1 ostensibly are done. The value function says something like, &amp;quot;Maybe I could sometimes, not always, tell you if you are doing well or badly.&amp;quot; The notion of a value function is more useful in some domains than others. For example, when you play chess and you lose a piece, I messed up.&lt;/p&gt; &lt;p&gt;This part shows that he surely is working on something or have progress already...&lt;/p&gt; &lt;p&gt;how they are doing it and why is it so hard? How do we need to reconceptualize the way we're training models to make something like this possible? &lt;/p&gt; &lt;p&gt;31:28&lt;/p&gt; &lt;p&gt;That is a great question to ask, and it's a question I have a lot of opinions about. &lt;/p&gt; &lt;p&gt;31:37&lt;/p&gt; &lt;p&gt;But unfortunately, we live in a world where not all machine learning ideas are discussed freely, and this is one of them. There's probably a way to do it. &lt;/p&gt; &lt;p&gt;31:49&lt;/p&gt; &lt;p&gt;I think it can be done. The fact that people are like that, I think it's a proof that it can be done. There may be another blocker though, which is that there is a possibility that the human neurons do more compute than we think. &lt;/p&gt; &lt;p&gt;32:07&lt;/p&gt; &lt;p&gt;If that is true, and if that plays an important role, then things might be more difficult. &lt;/p&gt; &lt;p&gt;32:13&lt;/p&gt; &lt;p&gt;But regardless, I do think it points to the existence of some machine learning principle that I have opinions on. But unfortunately, circumstances make it hard to discuss in detail. Nobody listens to this podcast, Ilya. &lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Famous-Associate-436"&gt; /u/Famous-Associate-436 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnvfu5/is_ilya_sutskever_trying_with_a_secret_sauce/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnvfu5/is_ilya_sutskever_trying_with_a_secret_sauce/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pnvfu5/is_ilya_sutskever_trying_with_a_secret_sauce/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T06:55:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnjdi1</id>
    <title>Is there a cli agent tool that can summarize a web page?</title>
    <updated>2025-12-15T21:21:30+00:00</updated>
    <author>
      <name>/u/Terminator857</name>
      <uri>https://old.reddit.com/user/Terminator857</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Seems most tools don't access the web. Obviously the tool must support local llm.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Terminator857"&gt; /u/Terminator857 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnjdi1/is_there_a_cli_agent_tool_that_can_summarize_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnjdi1/is_there_a_cli_agent_tool_that_can_summarize_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pnjdi1/is_there_a_cli_agent_tool_that_can_summarize_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T21:21:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnjnwf</id>
    <title>I built a web-based terminal to aggregate idle compute from Tier 2/3 data centers (access A100s via browser)</title>
    <updated>2025-12-15T21:32:55+00:00</updated>
    <author>
      <name>/u/Affectionate_King_</name>
      <uri>https://old.reddit.com/user/Affectionate_King_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm a university researcher and I have had some trouble with long queues in our college's cluster. I built a web terminal to automatically aggregate excess compute supply from tier 2/3 data centers on &lt;a href="https://neocloudx.com/buy"&gt;neocloudx.com&lt;/a&gt;. I have some nodes with really low prices - down to 0.38/hr for A100 40GB SXM and 0.15/hr for V100 SXM. Try it out and let me know what you think, particularly with latency and spinup times. You can access node terminals both in the browser and through SSH.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Affectionate_King_"&gt; /u/Affectionate_King_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnjnwf/i_built_a_webbased_terminal_to_aggregate_idle/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnjnwf/i_built_a_webbased_terminal_to_aggregate_idle/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pnjnwf/i_built_a_webbased_terminal_to_aggregate_idle/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T21:32:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnjp00</id>
    <title>Is this local/cloud mixed setup feasible?</title>
    <updated>2025-12-15T21:34:05+00:00</updated>
    <author>
      <name>/u/Alarming-Ad8154</name>
      <uri>https://old.reddit.com/user/Alarming-Ad8154</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My next MacBook will be 64gb, or second hand 96gb/12gb ram. I‚Äôll be able to run like oss-120b, qwen3-next, Kimi-linear etc. I was thinking of writing a custom script/mpc/tool where the LLM can actually use an api to query a bigger model if it‚Äôs unsure/stuck. The tool description would we something like: &lt;/p&gt; &lt;p&gt;‚ÄúMCP Tool: evaluate_thinking&lt;/p&gt; &lt;p&gt;Purpose:&lt;/p&gt; &lt;p&gt;Use a frontier OpenAI model as a second opinion on the local model‚Äôs draft answer and reasoning. The tool returns critique, missing steps, potential errors, and a confidence estimate. The local model should only call this tool when uncertain, when facts are likely wrong/stale, or when the user‚Äôs question is high-stakes.&lt;/p&gt; &lt;p&gt;Usage policy for this tool:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;‚Ä¢ Use sparingly. Do not call on every turn. ‚Ä¢ Call only if: ‚Ä¢ you‚Äôre uncertain (low confidence), ‚Ä¢ you suspect hallucination risk, ‚Ä¢ the question is high-stakes (medical/maths/biology/statistics), ‚Ä¢ the user requests verification or ‚Äúare you sure?‚Äù, ‚Ä¢ the topic is fast-changing and you might be outdated. ‚Ä¢ Do not include private chain-of-thought. Provide a concise ‚Äúreasoning summary‚Äù instead.‚Äù &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Is this worth trying to rig up, to sort of get api quality, but a local filter for the easier queries to suppress cost? Would it be worth somehow even training the model to get better at this? I could rig up a front end that lets me record thumbs up or down for wacht tool use as signal‚Ä¶&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Alarming-Ad8154"&gt; /u/Alarming-Ad8154 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnjp00/is_this_localcloud_mixed_setup_feasible/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnjp00/is_this_localcloud_mixed_setup_feasible/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pnjp00/is_this_localcloud_mixed_setup_feasible/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T21:34:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1pn19zc</id>
    <title>Another watercooled 4x GPU server complete!</title>
    <updated>2025-12-15T07:13:53+00:00</updated>
    <author>
      <name>/u/j4ys0nj</name>
      <uri>https://old.reddit.com/user/j4ys0nj</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn19zc/another_watercooled_4x_gpu_server_complete/"&gt; &lt;img alt="Another watercooled 4x GPU server complete!" src="https://preview.redd.it/pgqrfop2ib7g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=313f261a0bcd81f8f2af182cfeb1eae60a4c0d0f" title="Another watercooled 4x GPU server complete!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm on a roll this weekend. Finally got all of the parts needed to finish this build. 4x RTX A4500 with waterblocks from &lt;a href="https://shop.alphacool.com/en/shop/gpu-water-cooling/nvidia/10669-alphacool-es-rtx-a5000-gpu-cooler-with-backplate"&gt;Alphacool (A5000)&lt;/a&gt;. 80GB VRAM, nothing crazy, pretty cost efficient. These GPUs were about $1k each. Waterblocks were between $50-100 each since they're pretty old. As the blocks come, they appear to be 1 slot, but there's no 1 slot bracket provided and with the back plate, it takes up some space of the slot above it, so running these with no back plate (the GPUs don't have a back plate to begin with) and I had to print a slimmer block on the end than what came with them (the part right by the power connector). Then I cut the brackets to be 1 slot. Perfect fit. Very tight though, this chassis was not made for this! To round out the build there's a 4x mini SAS card connected to 16 SSDs (2 of the 5.25&amp;quot; bays on the right), and a 4x NVMe hot swap (in the remaining 5.25&amp;quot; bay) and a Mellanox 25G card.&lt;/p&gt; &lt;p&gt;Getting pretty decent performance out of it! I have &lt;a href="https://huggingface.co/cerebras/Qwen3-Coder-REAP-25B-A3B"&gt;https://huggingface.co/cerebras/Qwen3-Coder-REAP-25B-A3B&lt;/a&gt; loaded up with vLLM. It juuust fits. ~103-105 tokens/sec on single requests and when testing with 6x simultaneous requests it does about 50 tokens/sec. On sustained workloads, temps stay around 40-42¬∫C.&lt;/p&gt; &lt;p&gt;Finished my other watercooled 4x GPU server a few days ago also, post &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1pl984y/finally_finished_my_4x_gpu_water_cooled_server/"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/j4ys0nj"&gt; /u/j4ys0nj &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/pgqrfop2ib7g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn19zc/another_watercooled_4x_gpu_server_complete/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pn19zc/another_watercooled_4x_gpu_server_complete/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T07:13:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmx49s</id>
    <title>I pitted GPT-5.2 against Opus 4.5 and Gemini 3 in a robot coding tournament</title>
    <updated>2025-12-15T03:19:51+00:00</updated>
    <author>
      <name>/u/Inevitable_Can598</name>
      <uri>https://old.reddit.com/user/Inevitable_Can598</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently revived the classic coding game Robocode (Java-based tank battles) to test how LLMs perform against top-tier robots. Unlike static coding challenges (like LeetCode), these bots must balance tradeoffs, adapt to enemy strategies in real-time, and adopt unconventional approaches to remain unpredictable.&lt;/p&gt; &lt;p&gt;I prompted each model to build a robot, providing iterative feedback until progress stalled, and then submitted the best versions to the Robocode Arena.&lt;/p&gt; &lt;h1&gt;Final results&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Final ELO&lt;/th&gt; &lt;th align="left"&gt;Rank&lt;/th&gt; &lt;th align="left"&gt;Iterations to peak&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Opus-4.5&lt;/td&gt; &lt;td align="left"&gt;1412&lt;/td&gt; &lt;td align="left"&gt;17&lt;/td&gt; &lt;td align="left"&gt;3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPT-5.2-thinking&lt;/td&gt; &lt;td align="left"&gt;1229&lt;/td&gt; &lt;td align="left"&gt;25&lt;/td&gt; &lt;td align="left"&gt;3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Gemini-3-thinking&lt;/td&gt; &lt;td align="left"&gt;973&lt;/td&gt; &lt;td align="left"&gt;42&lt;/td&gt; &lt;td align="left"&gt;4&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPT-5.2-instant&lt;/td&gt; &lt;td align="left"&gt;953&lt;/td&gt; &lt;td align="left"&gt;43&lt;/td&gt; &lt;td align="left"&gt;3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Gemini-3-fast&lt;/td&gt; &lt;td align="left"&gt;917&lt;/td&gt; &lt;td align="left"&gt;46&lt;/td&gt; &lt;td align="left"&gt;7&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPT-5.1-thinking&lt;/td&gt; &lt;td align="left"&gt;835&lt;/td&gt; &lt;td align="left"&gt;49&lt;/td&gt; &lt;td align="left"&gt;8&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Haiku-4.5&lt;/td&gt; &lt;td align="left"&gt;811&lt;/td&gt; &lt;td align="left"&gt;50&lt;/td&gt; &lt;td align="left"&gt;8&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPT-5.1-instant&lt;/td&gt; &lt;td align="left"&gt;626&lt;/td&gt; &lt;td align="left"&gt;53&lt;/td&gt; &lt;td align="left"&gt;8&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Key findings&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;GPT-5.2 is a major upgrade over 5.1, scoring nearly 400 ELO points higher on the ladder. It figured out working strategies almost immediately, whereas 5.1 really struggled to make anything competitive even with a lot of help.&lt;/li&gt; &lt;li&gt;OpenAI is clearly pulling ahead of Google here; GPT-5.2 Thinking beat Gemini 3 Pro Thinking comfortably. Even the Instant GPT-5.2 model basically tied with Google's Thinking model, which was pretty surprising.&lt;/li&gt; &lt;li&gt;Opus 4.5 actually took the #1 spot because it acts more like a reliable coder than a tinkerer. While GPT-5.2 kept breaking its own code trying to optimize it, Opus nailed the complex math/physics on the first try and didn't regress.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I don't have an appropriate setup for a local LLM but I will be working on testing that next.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Inevitable_Can598"&gt; /u/Inevitable_Can598 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmx49s/i_pitted_gpt52_against_opus_45_and_gemini_3_in_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmx49s/i_pitted_gpt52_against_opus_45_and_gemini_3_in_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pmx49s/i_pitted_gpt52_against_opus_45_and_gemini_3_in_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T03:19:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1pndjp7</id>
    <title>Any open source evals for ai coding platforms?</title>
    <updated>2025-12-15T17:35:01+00:00</updated>
    <author>
      <name>/u/DataScientia</name>
      <uri>https://old.reddit.com/user/DataScientia</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Can somebody tell if there is any open source evals to test the performance ai coding platforms like claude code, cursor, antigravity etc. model will be constant only platforms get varied&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DataScientia"&gt; /u/DataScientia &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pndjp7/any_open_source_evals_for_ai_coding_platforms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pndjp7/any_open_source_evals_for_ai_coding_platforms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pndjp7/any_open_source_evals_for_ai_coding_platforms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T17:35:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnshbv</id>
    <title>I made a python code splitter for efficient RAG over large python codes.</title>
    <updated>2025-12-16T04:09:05+00:00</updated>
    <author>
      <name>/u/Sick__sock</name>
      <uri>https://old.reddit.com/user/Sick__sock</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnshbv/i_made_a_python_code_splitter_for_efficient_rag/"&gt; &lt;img alt="I made a python code splitter for efficient RAG over large python codes." src="https://b.thumbs.redditmedia.com/Vw-I8KLxacpOzeoYjEuidGpm4hEQspHP5JrzmGjyvIY.jpg" title="I made a python code splitter for efficient RAG over large python codes." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was working on a RAG application which had a lot of code to be considered for the pipeline and using the conventional splitters idn't do a great job in keeping the semantics intact. Hence made one on my own.&lt;/p&gt; &lt;p&gt;GitHub - &lt;a href="https://github.com/ricky-aufvaa/python-semantic-splitter"&gt;https://github.com/ricky-aufvaa/python-semantic-splitter&lt;/a&gt;&lt;/p&gt; &lt;p&gt;PyPi - python-semantic-splitter ¬∑ PyPI &lt;a href="https://share.google/JaqTszmSFyingjDUZ"&gt;https://share.google/JaqTszmSFyingjDUZ&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Do give your feedbacks and contribute to the project. Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sick__sock"&gt; /u/Sick__sock &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pnshbv"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnshbv/i_made_a_python_code_splitter_for_efficient_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pnshbv/i_made_a_python_code_splitter_for_efficient_rag/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T04:09:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1pn0cik</id>
    <title>Day 7: 21 Days of Building a Small Language Model: Self Attention</title>
    <updated>2025-12-15T06:16:04+00:00</updated>
    <author>
      <name>/u/Prashant-Lakhera</name>
      <uri>https://old.reddit.com/user/Prashant-Lakhera</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn0cik/day_7_21_days_of_building_a_small_language_model/"&gt; &lt;img alt="Day 7: 21 Days of Building a Small Language Model: Self Attention" src="https://b.thumbs.redditmedia.com/8Z0KF4iVd1xs4F4YcBYq-cR6jPpJZFENFObuyX6LZHA.jpg" title="Day 7: 21 Days of Building a Small Language Model: Self Attention" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Welcome to Day 7. Today, our focus is on self-attention. Simply put, self-attention allows each word in a sequence to look at and incorporate information from all other words in that sequence. This might seem obvious (of course words need to understand their context), but the challenge is doing this efficiently and effectively.&lt;/p&gt; &lt;p&gt;I‚Äôve covered all the concepts here at a high level to keep things simple. For a deeper exploration of these topics, feel free to check out my book &amp;quot;&lt;em&gt;Building A Small Language Model from Scratch: A Practical Guide.&amp;quot;&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; If you want to understand the coding part step by step, here‚Äôs the video.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=EXnvO86m1W8"&gt;&lt;strong&gt;https://www.youtube.com/watch?v=EXnvO86m1W8&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For example, in the sentence&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Sarah works as a software engineer. She enjoys solving complex problems &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;the word &amp;quot;She&amp;quot; needs to understand that it refers to &amp;quot;Sarah&amp;quot; from the previous sentence. Without self-attention, the model would process each word in isolation, losing crucial information about how words relate to each other.&lt;/p&gt; &lt;p&gt;So the real question is: how does self-attention enable models to capture these relationships, and why is it so effective?&lt;/p&gt; &lt;h1&gt;The Core Issue&lt;/h1&gt; &lt;p&gt;When we read a sentence, each word's meaning is influenced by the other words around it. The word bank means something different in I deposited money at the bank versus I sat on the river bank. The word it in The cat sat on the mat. It was comfortable. refers to the mat from the previous sentence.&lt;/p&gt; &lt;p&gt;These relationships aren't just about adjacent words; they can span long distances, and they're bidirectional. Later words can influence earlier ones, and earlier words influence later ones.&lt;/p&gt; &lt;p&gt;Traditional neural network approaches struggled with this. Recurrent Neural Networks (RNNs) process sequences step by step, which makes it difficult to capture long-range dependencies. Convolutional Neural Networks (CNNs) use fixed-size windows, limiting their ability to see the full context.&lt;/p&gt; &lt;p&gt;Self-attention solves this problem by allowing each position in the sequence to attend to every other position, including itself, in a single operation. When processing the word she, the model can attend to Sarah from earlier in the sequence, learning that she refers to Sarah. When processing bank, the model can attend to deposited money to understand that this bank is a financial institution, not a river's edge.&lt;/p&gt; &lt;h1&gt;Queries, Keys, and Values&lt;/h1&gt; &lt;p&gt;The self-attention mechanism uses three key components: queries, keys, and values. This terminology might seem abstract at first, but it's actually quite intuitive once you understand the analogy.&lt;/p&gt; &lt;p&gt;Think of how you search a database: you submit a query to find what you're looking for, the system uses keys to index and locate matching entries, and then retrieves the actual values associated with those keys.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/2ilzysh88b7g1.png?width=581&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=522afd4841746bf137b33000b763e4fb134b6e41"&gt;https://preview.redd.it/2ilzysh88b7g1.png?width=581&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=522afd4841746bf137b33000b763e4fb134b6e41&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Queries&lt;/strong&gt; represent what each token is looking for: the question we want to answer. When processing a particular position in the sequence, the query encodes what information we need from other positions.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Keys&lt;/strong&gt; represent what each element in the input can provide: the information available at each position. Each position in the sequence has a key that describes what that position contains or can offer.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Values&lt;/strong&gt; contain the actual information we want to extract. Once we determine which positions are relevant (by comparing queries to keys), we use the values from those positions to construct the output.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Let's consider an example. Imagine you have a database and your database has these employee records&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/4juko3ra8b7g1.png?width=285&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fa2022c5535c0993877bec46cc9fd92b9931c021"&gt;https://preview.redd.it/4juko3ra8b7g1.png?width=285&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fa2022c5535c0993877bec46cc9fd92b9931c021&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A Query is the question you ask:Give me the record for Employee ID = 27.&lt;/li&gt; &lt;li&gt;The Keys are all the indexed fields in the database(10,27,33) that help you find the right record.&lt;/li&gt; &lt;li&gt;The Value is the actual information the database returns when the right key is matched.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Let's consider one more example. Suppose we're processing the same example: Sarah works as a software engineer. She enjoys solving complex problems.&lt;/p&gt; &lt;p&gt;When the model processes the word She in the second sentence, it needs to determine what She refers to. Here's how self-attention helps:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Query (for &amp;quot;She&amp;quot;)&lt;/strong&gt;: The query for She encodes the question: What does this pronoun refer to? It represents what we're looking for, which is the person or thing that the pronoun refers to, specifically a female person mentioned earlier.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Keys (for each word)&lt;/strong&gt;: Each word in the sequence has a key that describes what that word represents. The key for Sarah might encode that it's a proper noun referring to a person (likely female based on the name). The key for engineer might encode that it's a noun referring to a profession. The key for works might encode that it's a verb.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Values (for each word)&lt;/strong&gt;: The values contain the actual semantic information. The value for Sarah contains information about who Sarah is, her identity, etc. The value for engineer contains information about the profession. The value for software contains information about the field of work.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9nr5ikwe8b7g1.png?width=711&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1c2ed0a7f5b4f77aa73198bfe495a197716f3fe6"&gt;https://preview.redd.it/9nr5ikwe8b7g1.png?width=711&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1c2ed0a7f5b4f77aa73198bfe495a197716f3fe6&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The attention mechanism compares the query for She against all the keys in the sequence. The key for Sarah will likely have a high similarity to the query for She because Sarah is a proper noun referring to a person who could be referred to by the pronoun She, and it appears earlier in the sequence. The keys for engineer, software, and works will have lower similarity. This produces high attention weights for Sarah and lower weights for other words.&lt;/p&gt; &lt;p&gt;Finally, the mechanism uses these attention weights to create a weighted combination of the values. Since Sarah has a high attention weight, its value (information about Sarah) will dominate the resulting context vector. This allows the model to understand that She refers to Sarah, and the context vector for She will incorporate information about Sarah, including that she works as a software engineer and enjoys solving complex problems.&lt;/p&gt; &lt;h1&gt;How Self-Attention Works&lt;/h1&gt; &lt;p&gt;The self-attention mechanism works by comparing queries to keys to determine how relevant each key is to the current query. This comparison produces relevance scores, called attention weights, which indicate how much each position should contribute. The mechanism then uses these attention weights to create a weighted combination of the values, producing a context vector that incorporates information from the most relevant positions.&lt;/p&gt; &lt;p&gt;The mathematical formula for scaled dot-product attention (the type used in transformers) is:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/gxqxyvkg8b7g1.png?width=727&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9141415545031c7cb5d32acbf9dfbc4e89249cf9"&gt;https://preview.redd.it/gxqxyvkg8b7g1.png?width=727&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9141415545031c7cb5d32acbf9dfbc4e89249cf9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;where:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Q&lt;/strong&gt; is the Query matrix, representing what each token is looking for&lt;/li&gt; &lt;li&gt;&lt;strong&gt;K&lt;/strong&gt; is the Key matrix, representing what each token can provide&lt;/li&gt; &lt;li&gt;&lt;strong&gt;V&lt;/strong&gt; is the Value matrix, containing the actual information content&lt;/li&gt; &lt;li&gt;&lt;strong&gt;d_k&lt;/strong&gt; is the dimension of the key vectors&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Q K^T&lt;/strong&gt; computes the similarity scores between queries and keys&lt;/li&gt; &lt;li&gt;The division by &lt;strong&gt;‚àöd_k&lt;/strong&gt; scales the scores to prevent numerical instability&lt;/li&gt; &lt;li&gt;&lt;strong&gt;softmax&lt;/strong&gt; converts the scores into a probability distribution&lt;/li&gt; &lt;li&gt;The final multiplication with V produces context vectors weighted by attention&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This formula enables the model to determine which parts of the input sequence are most relevant when processing each token, allowing it to capture long-range dependencies and contextual relationships.&lt;/p&gt; &lt;h1&gt;Why we scale by ‚àöd_k&lt;/h1&gt; &lt;p&gt;The scaled part of scaled dot-product attention comes from dividing the attention scores by the square root of the key dimension. This scaling is crucial for training stability.&lt;/p&gt; &lt;p&gt;When we compute the dot product between query and key vectors, the magnitude of the result grows with the dimension. For large embedding dimensions (typically 768, or even larger in modern models), these dot products can become very large.&lt;/p&gt; &lt;p&gt;Large dot products cause problems with the softmax function. When the input to softmax has very large values, the function behaves more like a step function, producing very sharp distributions where almost all attention goes to a single token. This creates two problems:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Gradient issues&lt;/strong&gt;: Very sharp softmax distributions result in very small gradients during backpropagation, which can drastically slow down learning or cause training to stagnate.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Loss of information&lt;/strong&gt;: When attention is too focused on a single token, the model loses the ability to attend to multiple relevant tokens simultaneously, which is important for understanding complex relationships.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;By scaling the scores by ‚àöd_k, we keep the dot products in a reasonable range, ensuring that the softmax function produces well-distributed attention weights. This allows the model to attend to multiple relevant tokens rather than focusing too heavily on just one, while also maintaining stable gradients during training.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; If you want to see how this looks in practice, please check the video above or the Google Colab link &lt;a href="https://colab.research.google.com/drive/1Ux1qrHL5DII8088tmTc4tCJfHqt2zvlw?usp=sharing"&gt;https://colab.research.google.com/drive/1Ux1qrHL5DII8088tmTc4tCJfHqt2zvlw?usp=sharing&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Why we use Softmax&lt;/h1&gt; &lt;p&gt;The softmax function converts the raw similarity scores (which can be any real numbers) into attention weights that represent how much focus should be placed on each token. Softmax ensures that:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;All attention weights sum to 1&lt;/strong&gt;: This creates a probability distribution, making the weights interpretable as proportions of attention.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Larger scores get more attention&lt;/strong&gt;: Tokens with higher similarity scores receive higher attention weights, but the normalization ensures that attention is distributed across all tokens proportionally.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multiple tokens can be attended to&lt;/strong&gt;: Unlike a hard selection mechanism, softmax allows the model to attend to multiple relevant tokens simultaneously, which is crucial for understanding complex linguistic relationships.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; If you want to see how this looks in practice, please check the video above or the Google Colab link &lt;/p&gt; &lt;h1&gt;Summary&lt;/h1&gt; &lt;p&gt;Self-attention is not just a component of transformer architectures; it is the fundamental mechanism that enables these models to understand context, relationships, and meaning in sequences of text. Without it, language models cannot capture the connections between words that make language meaningful.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prashant-Lakhera"&gt; /u/Prashant-Lakhera &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn0cik/day_7_21_days_of_building_a_small_language_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn0cik/day_7_21_days_of_building_a_small_language_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pn0cik/day_7_21_days_of_building_a_small_language_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T06:16:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnt40e</id>
    <title>Best budget ai server?</title>
    <updated>2025-12-16T04:42:10+00:00</updated>
    <author>
      <name>/u/Natjoe64</name>
      <uri>https://old.reddit.com/user/Natjoe64</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, already running lots of smallish models on my iPhone 15 Pro and my M2 Pro Macbook Pro, and it's a great time on each of them, but the Mac only has 16 gb of ram, so its starting to get a little cramped. I know the usual setup for a server is something along the lines of two 3060 12 gbs, but I already have a perfectly good rx 6600 and a ryzen 3 3100 kicking around. Would it be an ok starter setup if I just got another rx 6600? Sure it wouldn't have crazy amounts of vram, but it would be able to handle 8b parameter models and take the load off the Mac and my phone. I usually like to run qwen3 vl 4b, and it would be nice to step up to 8 or even gpt oss. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Natjoe64"&gt; /u/Natjoe64 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnt40e/best_budget_ai_server/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnt40e/best_budget_ai_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pnt40e/best_budget_ai_server/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T04:42:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1pn4yrf</id>
    <title>I scored 100+ architectures on "Hardware Friction." Why KANs fry tensor cores and MoEs have a context trap.</title>
    <updated>2025-12-15T11:21:40+00:00</updated>
    <author>
      <name>/u/petroslamb</name>
      <uri>https://old.reddit.com/user/petroslamb</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been trying to figure out why technically superior architectures like Neural ODEs often die while the Transformer remains dominant. I ended up writing a deep dive on what I call the &amp;quot;Hardware Friction Map,&amp;quot; arguing that GPUs don't actually reject ideas. They just charge a &amp;quot;compute tax&amp;quot; based on how much an idea deviates from optimized primitives like dense matrix multiplications.&lt;/p&gt; &lt;p&gt;I also compiled a GitHub dataset scoring over 100 architectures on their hardware efficiency, which I linked below. There are a few specific findings that I think matter for those of us running models locally.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;REMOVED:&lt;/strong&gt; &lt;del&gt;The first big one is the &amp;quot;Context Trap&amp;quot; with Mixture of Experts. We all like MoEs for the inference speedup, but the data suggests that the &amp;quot;5x faster&amp;quot; marketing claims usually only hold up at very short context lengths. When you look at the benchmarks for 16k to 32k context, the throughput often drops to roughly 30% or 40% of the baseline. The issue is that the routing logic and KV cache traffic start to dominate the sparse expert compute. MoEs are great throughput optimizers, but unless the architecture is specifically co-designed for long context like the new DeepSeek V3, they struggle when you load them up with history.&lt;/del&gt;&lt;/p&gt; &lt;p&gt;Then there are the &amp;quot;Red Zone&amp;quot; architectures like KANs (Kolmogorov-Arnold Networks). They look great on paper, but they are basically unusable for local inference right now. KANs rely on edge-based spline evaluations, which are essentially hundreds of tiny, irregular operations. Current GPUs need big batched matrix multiplications to hit peak performance, so KANs end up dropping tensor core utilization to around 10%. Until hardware changes, they are just too expensive to run efficiently.&lt;/p&gt; &lt;p&gt;I also noticed a hard limit with pure State Space Models (SSMs) like Mamba. They seem to be production-ready at the 7B scale, which is why Falcon Mamba 7B works well. But once you cross the 13B parameter threshold, the training parallelism gap compounds and memory bandwidth becomes a bottleneck for state propagation. That appears to be why every major deployment larger than 13B, like Jamba or Falcon-H1, is forced to use a hybrid architecture of Attention plus SSMs.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;CLEARED:&lt;/strong&gt; This friction also explains the gap between models like Llama 3.1 and DeepSeek V3. Llama used a standard stack that we can run easily. DeepSeek V3 &lt;strong&gt;will&lt;/strong&gt; require&lt;del&gt;d&lt;/del&gt; &lt;del&gt;them&lt;/del&gt; to rewrite their entire cluster scheduler and spend six months on custom routing kernels. That high friction is a massive moat for them, but it is also why it takes about 20 months for the open ecosystem tools like vLLM or llama.cpp to fully catch up to those custom internals.&lt;/p&gt; &lt;p&gt;I have linked the full breakdown and the architecture scoring dataset below. I am curious if your experience with local inference matches the context trap numbers I found for MoEs.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;CORRECTED:&lt;/strong&gt;&lt;br /&gt; - (dataset) &lt;a href="https://github.com/petroslamb/hardware-friction-scorecard-dataset"&gt;https://github.com/petroslamb/hardware-friction-scorecard-dataset&lt;/a&gt;&lt;br /&gt; - (article) &lt;a href="https://lambpetros.substack.com/p/the-hardware-friction-map"&gt;https://lambpetros.substack.com/p/the-hardware-friction-map&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;EDIT (Dec 15, 2025):&lt;/strong&gt; Several claims in this post have been corrected based on feedback in the comments:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;&amp;quot;Context Trap&amp;quot; for MoE&lt;/strong&gt;: Removed. The 16K-32K throughput figures were extrapolated, not measured. Direct benchmarks only exist up to 2K tokens (arXiv:2508.17467). Modern MoEs with GQA/MLA handle long context as well as dense models.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;&amp;quot;20 months for ecosystem catch-up&amp;quot;&lt;/strong&gt;: Clarified. Basic support often lands in weeks (DeepSeek V3 ‚Üí llama.cpp took ~1 month). Full optimization for advanced features takes 18-24 months (FlashAttention ‚Üí llama.cpp took 23 months).&lt;/li&gt; &lt;li&gt;Corrected the link to the dataset.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Thanks to &lt;a href="/u/FullOf_Bad_Ideas"&gt;u/FullOf_Bad_Ideas&lt;/a&gt; and others for the corrections.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/petroslamb"&gt; /u/petroslamb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn4yrf/i_scored_100_architectures_on_hardware_friction/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn4yrf/i_scored_100_architectures_on_hardware_friction/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pn4yrf/i_scored_100_architectures_on_hardware_friction/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T11:21:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1pndmi4</id>
    <title>AMD ROCm inference benchmarks (RX 7900 XTX / gfx1100) + reproducible Docker commands</title>
    <updated>2025-12-15T17:38:00+00:00</updated>
    <author>
      <name>/u/AMDRocmBench</name>
      <uri>https://old.reddit.com/user/AMDRocmBench</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm running an AMD RX 7900 XTX (gfx1100) on Ubuntu 24.04 with ROCm + llama.cpp (Docker). If anyone wants benchmark numbers for a specific GGUF model/quant/config on AMD, reply or DM with the details and I can run it and share results + a reproducible command.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What I‚Äôll share:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;tokens/sec (prefill + generation)&lt;/li&gt; &lt;li&gt;VRAM footprint / memory breakdown&lt;/li&gt; &lt;li&gt;settings used (ctx/batch/offload) + notes if something fails&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Baseline reference (my node):&lt;/strong&gt; TinyLlama 1.1B Q4_K_M: ~1079 tok/s prefill, ~308 tok/s generation, ~711 MiB VRAM.&lt;/p&gt; &lt;p&gt;If you want it as a formal report/runbook for your project, I can also package it up as a paid deliverable (optional).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AMDRocmBench"&gt; /u/AMDRocmBench &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pndmi4/amd_rocm_inference_benchmarks_rx_7900_xtx_gfx1100/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pndmi4/amd_rocm_inference_benchmarks_rx_7900_xtx_gfx1100/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pndmi4/amd_rocm_inference_benchmarks_rx_7900_xtx_gfx1100/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T17:38:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1pngpjm</id>
    <title>VECS: a semantic cache server in C</title>
    <updated>2025-12-15T19:33:45+00:00</updated>
    <author>
      <name>/u/drifting_raptor3762</name>
      <uri>https://old.reddit.com/user/drifting_raptor3762</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pngpjm/vecs_a_semantic_cache_server_in_c/"&gt; &lt;img alt="VECS: a semantic cache server in C" src="https://b.thumbs.redditmedia.com/ap58-JNdG26PogHUflLVPwds92Jj6Ml2_z_TYbyahJg.jpg" title="VECS: a semantic cache server in C" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/qf03s9uf5f7g1.png?width=2650&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=71ed4aba31c9fdc74c45c82b84a4ad9cc4c83a12"&gt;vecs startup&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;This year i had to develop a RAG application without heavy libraries to keep things simple. Eventually, i needed a semantic cache to save on inference costs and latency. Looking at the options, everything felt like overkill. I didn't want to spin up a complex vector database just to cache some queries, and most &amp;quot;semantic cache&amp;quot; solutions require calling an external API for embeddings, which adds network latency that defeats the purpose for me.&lt;/p&gt; &lt;p&gt;So I spent some free time building VECS. It's a semantic cache server written in C.&lt;/p&gt; &lt;p&gt;The main idea is that it embeds &lt;code&gt;llama.cpp&lt;/code&gt; directly into the server process. When you send a query via TCP, it calculates the embedding and searches the index locally in the same memory space. No network hops to external providers, no Python runtime overhead.&lt;/p&gt; &lt;p&gt;Some details on how it works:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Search:&lt;/strong&gt; It uses a basic IVFFlat index. I initially used a linear scan, but I had to implement some simple clustering because it was getting too slow as the dataset grew. It groups vectors into buckets so it doesn't have to scan everything every time.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Concurrency:&lt;/strong&gt; It handles connection pooling and offloads the embedding math to a GPU thread pool, so the main event loop (epoll/kqueue) stays non-blocking.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Protocol:&lt;/strong&gt; It speaks VSP, which is basically the RESP protocol (Redis style), so it's easy to integrate.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Caching:&lt;/strong&gt; Has an L1 cache for exact string matches and L2 for semantic similarity.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I ran some benchmarks on my local machine (M2 Max 12 core CPU - 30 core GPU - 32 GB RAM) with GPU offloading enabled and I'm seeing promising latency results.&lt;/p&gt; &lt;p&gt;It compiles down to a single binary. It's still a work in progress and probably has some rough edges, but it solves my specific problem of on-prem, low-latency caching without dependencies.&lt;/p&gt; &lt;p&gt;I also threw together a CLI and a Node client if anyone wants to take a look:&lt;/p&gt; &lt;p&gt;Server Source: &lt;a href="https://github.com/riccardogiuriola/vecs"&gt;https://github.com/riccardogiuriola/vecs&lt;/a&gt;&lt;/p&gt; &lt;p&gt;CLI:&lt;a href="https://github.com/riccardogiuriola/vecs-cli"&gt;https://github.com/riccardogiuriola/vecs-cli&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Node Client:&lt;a href="https://github.com/riccardogiuriola/vecs-client-node"&gt;https://github.com/riccardogiuriola/vecs-client-node&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you want to hop on discord and give your opinion:&lt;/p&gt; &lt;p&gt;Discord: &lt;a href="https://discord.gg/HdCnpjwuPW"&gt;https://discord.gg/HdCnpjwuPW&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know what you think or if there are obvious optimizations I missed in the C code.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/drifting_raptor3762"&gt; /u/drifting_raptor3762 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pngpjm/vecs_a_semantic_cache_server_in_c/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pngpjm/vecs_a_semantic_cache_server_in_c/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pngpjm/vecs_a_semantic_cache_server_in_c/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T19:33:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnurc2</id>
    <title>Llama 3.2-3b Uncensored</title>
    <updated>2025-12-16T06:14:00+00:00</updated>
    <author>
      <name>/u/Worried_Goat_8604</name>
      <uri>https://old.reddit.com/user/Worried_Goat_8604</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I‚Äôm releasing &lt;strong&gt;Aletheia-Llama-3.2-3B&lt;/strong&gt;, a fully uncensored version of Llama 3.2 that can answer essentially any question.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Problem with most Uncensored Models:&lt;/strong&gt;&lt;br /&gt; Usually, uncensoring is done via Supervised Fine-Tuning (SFT) or DPO on massive datasets. This often causes &amp;quot;Catastrophic Forgetting&amp;quot; or a &amp;quot;Lobotomy effect,&amp;quot; where the model becomes compliant but loses its reasoning ability or coding skills.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Solution:&lt;/strong&gt;&lt;br /&gt; This model was fine-tuned using &lt;strong&gt;Unsloth&lt;/strong&gt; on a single &lt;strong&gt;RTX 3060 (12GB)&lt;/strong&gt; using a custom alignment pipeline. Unlike standard approaches, this method surgically removes refusal behaviors without degrading the model's logic or general intelligence.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Release Details:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Repo:&lt;/strong&gt; &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fgithub.com%2Fnoobezlol%2FAletheia-Llama-3.2-3B"&gt;https://github.com/noobezlol/Aletheia-Llama-3.2-3B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Weights (HF):&lt;/strong&gt; &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fhuggingface.co%2FIshaanlol%2FAletheia-Llama-3.2-3B"&gt;https://huggingface.co/Ishaanlol/Aletheia-Llama-3.2-3B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Formats:&lt;/strong&gt; Full LoRA Adapter (Best for intelligence) and GGUF (Best for CPU/Ollama).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Deployment:&lt;/strong&gt;&lt;br /&gt; I‚Äôve included a Docker container and a Python script that automatically handles the download and setup. It runs out of the box on Linux/Windows (WSL).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Future Requests:&lt;/strong&gt;&lt;br /&gt; I am open to requests for other models via Discord or Reddit, &lt;strong&gt;provided they fit within the compute budget of an RTX 3060 (e.g., 7B/8B models).&lt;/strong&gt;&lt;br /&gt; Note: I will not be applying this method to 70B+ models even if compute is offered. While the 3B model is a safe research artifact , uncensored large-scale models pose significantly higher risks, and I am sticking to responsible research boundaries.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Worried_Goat_8604"&gt; /u/Worried_Goat_8604 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnurc2/llama_323b_uncensored/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnurc2/llama_323b_uncensored/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pnurc2/llama_323b_uncensored/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T06:14:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnv1u3</id>
    <title>We built an installation-free AI agent demo that runs purely on WebAssembly and open-source models</title>
    <updated>2025-12-16T06:31:05+00:00</updated>
    <author>
      <name>/u/Putrid_Cry_407</name>
      <uri>https://old.reddit.com/user/Putrid_Cry_407</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone üëã&lt;/p&gt; &lt;p&gt;I wanted to share a web demo we‚Äôve been working on that explores a few ideas around running AI agents directly in the browser.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Local and API-based models&lt;/strong&gt; You can switch between API models and local open-source models running via &lt;strong&gt;WebAssembly (WASM)&lt;/strong&gt;, so everything runs directly in the browser.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Fully local LLM execution&lt;/strong&gt; When using local (open-source) models, the entire inference runs fully locally, with no backend required.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Free-form tool calling&lt;/strong&gt; Tool usage isn‚Äôt hard-coded to a specific model or prompt format, making it easy to experiment with different setups.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Single interactive web page&lt;/strong&gt; All of this is available on a single page, where you can try and compare everything interactively.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Running local models requires a PC.&lt;/p&gt; &lt;p&gt;It‚Äôs still in an early stage, so many features are missing. But we‚Äôll keep adding more over time.&lt;/p&gt; &lt;p&gt;üîó &lt;strong&gt;Live demo:&lt;/strong&gt; &lt;a href="https://webui.ailoy.co/"&gt;https://webui.ailoy.co/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks for checking it out!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Putrid_Cry_407"&gt; /u/Putrid_Cry_407 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnv1u3/we_built_an_installationfree_ai_agent_demo_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnv1u3/we_built_an_installationfree_ai_agent_demo_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pnv1u3/we_built_an_installationfree_ai_agent_demo_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T06:31:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1pn7o5f</id>
    <title>ùöïùöïùöäùöñùöä.ùööùöùùöåùöõùöéùöäùöùùöòùöõ v3.0.0 is out üéâ</title>
    <updated>2025-12-15T13:43:26+00:00</updated>
    <author>
      <name>/u/cristianadam</name>
      <uri>https://old.reddit.com/user/cristianadam</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn7o5f/ùöïùöïùöäùöñùöäùööùöùùöåùöõùöéùöäùöùùöòùöõ_v300_is_out/"&gt; &lt;img alt="ùöïùöïùöäùöñùöä.ùööùöùùöåùöõùöéùöäùöùùöòùöõ v3.0.0 is out üéâ" src="https://external-preview.redd.it/b2RvY2Z3YTNnZDdnMQDCmACWN_s8k6H2Y-UiTssPZ2QAPGgBtwTl1Ibw4ttz.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=849a1979b47c8f0e51a93a6dcf9ea0b17e612e5a" title="ùöïùöïùöäùöñùöä.ùööùöùùöåùöõùöéùöäùöùùöòùöõ v3.0.0 is out üéâ" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The screencast was done on a MacBook M3 with &lt;code&gt;llama-server&lt;/code&gt; running &lt;code&gt;gpt-oss 20b&lt;/code&gt; and the following prompt: &lt;em&gt;&amp;quot;write a c++ program that prints the current moon phase. use emojis. use cmake. open, build and run in Qt Creator.&amp;quot;&lt;/em&gt;&lt;/p&gt; &lt;p&gt;The link to &lt;a href="https://github.com/cristianadam/llama.qtcreator/releases/tag/v3.0.0"&gt;Release v3.0.0&lt;/a&gt;. It's also available in Qt Creator 18's Extension pane. Click on &lt;em&gt;Use external repository&lt;/em&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cristianadam"&gt; /u/cristianadam &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/cyhyeja3gd7g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn7o5f/ùöïùöïùöäùöñùöäùööùöùùöåùöõùöéùöäùöùùöòùöõ_v300_is_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pn7o5f/ùöïùöïùöäùöñùöäùööùöùùöåùöõùöéùöäùöùùöòùöõ_v300_is_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T13:43:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnopes</id>
    <title>RTX 3090 vs R9700 Pro to supplement a Mac llm setup</title>
    <updated>2025-12-16T01:07:23+00:00</updated>
    <author>
      <name>/u/Ok-Progress726</name>
      <uri>https://old.reddit.com/user/Ok-Progress726</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello all, writing this post as I am finding myself knee deep in the local LLM space now and utterly bamboozled. I am contemplating the purchase of 2 GPUs for running coding models and any other models that are currently not supported on Macs. I do vibe coding for personal projects (nothing for production) using roocode and quickly found out that Macs are terrible to ttft and prompt prefill. &lt;/p&gt; &lt;p&gt;I am looking for input comparing 2 RTX 3090Tis v/s 2 R9700 Pros. My current setup is a Mac M3 Ultra 512GB and an ASUS G733PY with a 4090 mobile. The plan is to run the gpus on the ASUS with a janky m2 to PCI-E, splitters and risers. &lt;/p&gt; &lt;p&gt;Just for context, I have run Qwen3 coder 30B A3B Q4/6/8, GLM 4.5 Air/non-Air and Gpt OSS 120B with 130k context. Prompt prefill with full context takes more than 8 to 10 minutes easily. I want to cut this time down and want to figure out what would be best. I know that I get a slower GPU with the R9700 and slower memory(~650 GB/s) but more VRAM. And I get a faster GPU with the RTX 3090, and faster memory (~1000 GB/s) but less VRAM. &lt;/p&gt; &lt;p&gt;Greatly appreciate the discussion and suggestions. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Progress726"&gt; /u/Ok-Progress726 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnopes/rtx_3090_vs_r9700_pro_to_supplement_a_mac_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnopes/rtx_3090_vs_r9700_pro_to_supplement_a_mac_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pnopes/rtx_3090_vs_r9700_pro_to_supplement_a_mac_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T01:07:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnppvo</id>
    <title>Running Benchmarks - Open Source</title>
    <updated>2025-12-16T01:54:58+00:00</updated>
    <author>
      <name>/u/alphatrad</name>
      <uri>https://old.reddit.com/user/alphatrad</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, I know there are some community agreed upon benchmarks for figuring out prompt processing, tokens per second. But something else I've been wondering is, what kind of other open source bench marks are their for evaluating models, not just our hardware.&lt;/p&gt; &lt;p&gt;If we want to test the performance of local models ourselves and not just run off to see what some 3rd party has to say?&lt;/p&gt; &lt;p&gt;What are our options? I'm not fully aware of them.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/alphatrad"&gt; /u/alphatrad &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnppvo/running_benchmarks_open_source/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnppvo/running_benchmarks_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pnppvo/running_benchmarks_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T01:54:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1po4x1y</id>
    <title>Did an experiment on a local TextToSpeech model for my YouTube channel, results are kind of crazy</title>
    <updated>2025-12-16T15:33:06+00:00</updated>
    <author>
      <name>/u/bhattarai3333</name>
      <uri>https://old.reddit.com/user/bhattarai3333</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1po4x1y/did_an_experiment_on_a_local_texttospeech_model/"&gt; &lt;img alt="Did an experiment on a local TextToSpeech model for my YouTube channel, results are kind of crazy" src="https://external-preview.redd.it/HwxHXsieJpkGrgXnB1Es88Mkk3PJvxTYel1mfJnPYJw.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1c1ae77719a1e9daf33770d48eacd96abb1a2dc2" title="Did an experiment on a local TextToSpeech model for my YouTube channel, results are kind of crazy" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I run this YouTube channel for public domain audiobooks on YouTube, and before anyone gets worried, I don‚Äôt think I‚Äôm going to be replacing human narrators with TTS any time soon.&lt;/p&gt; &lt;p&gt;I wanted to try and see the quality I could get with a local TTS model running on my modest 12gb GPU.&lt;/p&gt; &lt;p&gt;Around 10 minutes in this video you can hear the voice infer, from text context to change its voice to mimic a young child. I didn‚Äôt put any instructions in about changing voices, just a general system prompt to narrate an audiobook.&lt;/p&gt; &lt;p&gt;The truly crazy part is that this whole generation was a voice clone, meaning the particular passage at 10 minutes is an AI mimicking a man‚Äôs voice, pretending to mimic a child‚Äôs voice with no prompting all on my GPU.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bhattarai3333"&gt; /u/bhattarai3333 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/26iNoRQKdK0?t=9m55s"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1po4x1y/did_an_experiment_on_a_local_texttospeech_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1po4x1y/did_an_experiment_on_a_local_texttospeech_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T15:33:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1pncy5y</id>
    <title>Suspected scam: many NVIDIA RTX Pro 6000 for ¬£2,900 on eBay</title>
    <updated>2025-12-15T17:12:40+00:00</updated>
    <author>
      <name>/u/skyfallboom</name>
      <uri>https://old.reddit.com/user/skyfallboom</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pncy5y/suspected_scam_many_nvidia_rtx_pro_6000_for_2900/"&gt; &lt;img alt="Suspected scam: many NVIDIA RTX Pro 6000 for ¬£2,900 on eBay" src="https://external-preview.redd.it/xpx-BnA7zHCbnXOXB5KlTmKqGruqN6OySQZ12wtTJBo.jpeg?width=216&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=482447feeaeca91472e6b3cf662589533d9a391a" title="Suspected scam: many NVIDIA RTX Pro 6000 for ¬£2,900 on eBay" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A bunch of RTX Pro 6000 listings have emerged on eBay, and the deals are too good to be true. &lt;/p&gt; &lt;p&gt;The new wave of listing is supposedly covered by eBay, so I'm wondering how the scam works?&lt;/p&gt; &lt;p&gt;The first listing was a &amp;quot;Classified ad&amp;quot;. If you are not familiar with it, it allows sellers to advertise on the eBay platform, but the transaction happens completely outside of eBay. This means you don't get any of the eBay features (refund, leaving negative feedback).&lt;/p&gt; &lt;p&gt;A few days later an odd pattern of listings emerged:&lt;/p&gt; &lt;p&gt;- heavy discount (over half price)&lt;/p&gt; &lt;p&gt;- around ¬£2,900 each&lt;/p&gt; &lt;p&gt;- from the UK, shipping from China&lt;/p&gt; &lt;p&gt;- accounts with little feedback but positive&lt;/p&gt; &lt;p&gt;- possibility of feedback farming (selling posts stamps)&lt;/p&gt; &lt;p&gt;- a DDR5 kit is included to seal the deal&lt;/p&gt; &lt;p&gt;- same pics, including the RAM kit&lt;/p&gt; &lt;p&gt;Examples:&lt;/p&gt; &lt;p&gt;- &lt;a href="https://www.ebay.com/itm/389366203939"&gt;https://www.ebay.com/itm/389366203939&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- &lt;a href="https://www.ebay.com/itm/277575062859"&gt;https://www.ebay.com/itm/277575062859&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- &lt;a href="https://www.ebay.com/itm/127559844787"&gt;https://www.ebay.com/itm/127559844787&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/skyfallboom"&gt; /u/skyfallboom &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.ebay.com/itm/257259544555?"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pncy5y/suspected_scam_many_nvidia_rtx_pro_6000_for_2900/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pncy5y/suspected_scam_many_nvidia_rtx_pro_6000_for_2900/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T17:12:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1pndxux</id>
    <title>zai-org - SCAIL (Studio-grade Character Animation via In-context Learning)</title>
    <updated>2025-12-15T17:50:02+00:00</updated>
    <author>
      <name>/u/MariusNocturnum</name>
      <uri>https://old.reddit.com/user/MariusNocturnum</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;zai-org has just released a model for character animation and it looks quite impressive.&lt;/p&gt; &lt;p&gt;From the blog:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;SCAIL&lt;/strong&gt; builds upon Wan-I2V models and incorporates &lt;strong&gt;3D-Consistent&lt;/strong&gt; pose representation to learn precise identity-agnostic motion. After comparing different injection methods, we adopt &lt;strong&gt;full-context pose injection&lt;/strong&gt; for the model to learn spatial-temporal motion characteristics. We leverage &lt;strong&gt;Pose-shifted RoPE&lt;/strong&gt; to facilitate learning of spatial-temporal relation between video tokens and pose tokens.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Blog: &lt;a href="https://teal024.github.io/SCAIL/"&gt;https://teal024.github.io/SCAIL/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Huggingface: &lt;a href="https://huggingface.co/zai-org/SCAIL-Preview"&gt;https://huggingface.co/zai-org/SCAIL-Preview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/zai-org/SCAIL"&gt;https://github.com/zai-org/SCAIL&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MariusNocturnum"&gt; /u/MariusNocturnum &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pndxux/zaiorg_scail_studiograde_character_animation_via/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pndxux/zaiorg_scail_studiograde_character_animation_via/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pndxux/zaiorg_scail_studiograde_character_animation_via/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T17:50:02+00:00</published>
  </entry>
</feed>
