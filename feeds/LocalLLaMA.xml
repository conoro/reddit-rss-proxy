<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-30T10:24:52+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1nu8d48</id>
    <title>LLM DevRel Lead needed in US</title>
    <updated>2025-09-30T09:12:19+00:00</updated>
    <author>
      <name>/u/Ginger_finger_</name>
      <uri>https://old.reddit.com/user/Ginger_finger_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;First time I’m trying Reddit for hiring… &lt;/p&gt; &lt;p&gt;I’m sourcing for a DevRel Lead who has experience and knowledge of LLMs. &lt;/p&gt; &lt;p&gt;My client are a Series B Open Source LLMOps business. Product is doing very well! &lt;/p&gt; &lt;p&gt;US Remote, paying up to $280k base + benefits &lt;/p&gt; &lt;p&gt;Please drop me a DM if you’re interested!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ginger_finger_"&gt; /u/Ginger_finger_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nu8d48/llm_devrel_lead_needed_in_us/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nu8d48/llm_devrel_lead_needed_in_us/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nu8d48/llm_devrel_lead_needed_in_us/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-30T09:12:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ntv3g0</id>
    <title>Sonnet 4.5 reaches top of SWE-bench leaderboard for minimal agent. Detailed cost analysis + all the logs with minimal agent</title>
    <updated>2025-09-29T21:46:23+00:00</updated>
    <author>
      <name>/u/klieret</name>
      <uri>https://old.reddit.com/user/klieret</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntv3g0/sonnet_45_reaches_top_of_swebench_leaderboard_for/"&gt; &lt;img alt="Sonnet 4.5 reaches top of SWE-bench leaderboard for minimal agent. Detailed cost analysis + all the logs with minimal agent" src="https://b.thumbs.redditmedia.com/pbpZUohySfxup74HXQGV3PiRsWMXPo2bAG4a77jTApM.jpg" title="Sonnet 4.5 reaches top of SWE-bench leaderboard for minimal agent. Detailed cost analysis + all the logs with minimal agent" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We just finished evaluating Sonnet 4.5 on SWE-bench verified with our minimal agent and it's quite a big leap, reaching 70.6% making it the solid #1 of all the models we have evaluated.&lt;/p&gt; &lt;p&gt;This is all independently run with a minimal agent with a very common sense prompt that is the same for all language models. You can see them in our trajectories here: &lt;a href="https://docent.transluce.org/dashboard/a4844da1-fbb9-4d61-b82c-f46e471f748a"&gt;https://docent.transluce.org/dashboard/a4844da1-fbb9-4d61-b82c-f46e471f748a&lt;/a&gt; (if you wanna check out specific tasks, you can filter by &lt;code&gt;instance_id&lt;/code&gt;). You can also compare it with Sonnet 4 here: &lt;a href="https://docent.transluce.org/dashboard/0cb59666-bca8-476b-bf8e-3b924fafcae7"&gt;https://docent.transluce.org/dashboard/0cb59666-bca8-476b-bf8e-3b924fafcae7&lt;/a&gt; ).&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/yautm1ivb6sf1.png?width=2534&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5a75926b9465df90c1043e3e33ba5f8a2efda359"&gt;https://preview.redd.it/yautm1ivb6sf1.png?width=2534&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5a75926b9465df90c1043e3e33ba5f8a2efda359&lt;/a&gt;&lt;/p&gt; &lt;p&gt;One interest thing is that Sonnet 4.5 takes a lot more steps than Sonnet 4, so even though it's the same pricing per token, the final run is more expensive ($279 vs $186). You can see that in this cumulative histogram: Half of the trajectories take more than 50 steps.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6f45czlwb6sf1.png?width=780&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e8dae8887799b93e1ec387bb60716c03d68f934e"&gt;https://preview.redd.it/6f45czlwb6sf1.png?width=780&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e8dae8887799b93e1ec387bb60716c03d68f934e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you wanna have a bit more control over the cost per instance, you can vary the step limit and you get a curve like this, balancing average cost per task vs the score.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ojldgx4yb6sf1.png?width=695&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=76c389b9be39da322a25ff23d43b725e36f3c50c"&gt;https://preview.redd.it/ojldgx4yb6sf1.png?width=695&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=76c389b9be39da322a25ff23d43b725e36f3c50c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You can also reproduce all these yourself with our minimal agent: &lt;a href="https://github.com/SWE-agent/mini-swe-agent/"&gt;https://github.com/SWE-agent/mini-swe-agent/&lt;/a&gt;, it's described here &lt;a href="https://mini-swe-agent.com/latest/usage/swebench/"&gt;https://mini-swe-agent.com/latest/usage/swebench/&lt;/a&gt; (it's just one command + one command with our swebench cloud evaluation).&lt;/p&gt; &lt;p&gt;We also added more support for local models in mini recently and added openrouter and portkey support on top of litellm that we use as default to support as many models possible. Would be super interested if there's a more elegant way to support models. Any feedback on how we can support local models better is much appreciated.&lt;/p&gt; &lt;p&gt;Currently, our best open model is Qwen3 coder with 55% (&lt;a href="https://www.swebench.com/"&gt;https://www.swebench.com/&lt;/a&gt;), but there's also a few more models we're missing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/klieret"&gt; /u/klieret &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntv3g0/sonnet_45_reaches_top_of_swebench_leaderboard_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntv3g0/sonnet_45_reaches_top_of_swebench_leaderboard_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ntv3g0/sonnet_45_reaches_top_of_swebench_leaderboard_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-29T21:46:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ntiua9</id>
    <title>We just open-sourced Kroko ASR: a fast, streaming alternative to Whisper. It’s early days, we’d love testers, feedback, and contributors.</title>
    <updated>2025-09-29T14:01:58+00:00</updated>
    <author>
      <name>/u/banafo</name>
      <uri>https://old.reddit.com/user/banafo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Edit:&lt;/strong&gt; I forgot to add that the pro models are free for non-commercial use, you can get your key on our website &lt;a href="https://kroko.ai"&gt;kroko.ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;First batch&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Streaming models (CC-BY-SA), ready for CPU, mobile, or browser&lt;/li&gt; &lt;li&gt;More extreme but affordable commercial models (with Apache inference code)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Languages&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A dozen to start, more on the way (Polish and Japanese coming next.)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Why it’s different&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Much smaller download than Whisper&lt;/li&gt; &lt;li&gt;Much faster on CPU (runs on mobile or even in the browser, try the the demo on android)&lt;/li&gt; &lt;li&gt;(Almost) hallucination-free&lt;/li&gt; &lt;li&gt;Streaming support: great for voice assistants, live agent assist, note taking, or just yelling at your computer&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Quality&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Offline models beat Whisper v3-large while being about 10× smaller&lt;/li&gt; &lt;li&gt;Streaming models are comparable (or better) at 1s chunk size&lt;/li&gt; &lt;li&gt;There’s a trade-off in quality at ultra-low latency&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Project goals&lt;/strong&gt;&lt;br /&gt; Build a community and democratize speech-to-text, making it easier to train models and run them at the edge (without needing a PhD in speech AI).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;website &amp;amp; cloud demo: &lt;a href="https://kroko.ai"&gt;kroko.ai&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Android model explorer: &lt;a href="https://play.google.com/store/apps/details?id=com.krokoasr.demo&amp;amp;hl=en"&gt;Google Play&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Discord: &lt;a href="https://discord.gg/nnY9nQac"&gt;discord.gg/nnY9nQac&lt;/a&gt;&lt;/li&gt; &lt;li&gt;GitHub: &lt;a href="https://github.com/kroko-ai/kroko-onnx"&gt;https://github.com/kroko-ai/kroko-onnx&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Hugging Face Demo: &lt;a href="https://huggingface.co/spaces/Banafo/Kroko-Streaming-ASR-Wasm"&gt;Kroko Streaming ASR Wasm&lt;/a&gt; (older models, updates coming soon)&lt;/li&gt; &lt;li&gt;community models page: &lt;a href="https://huggingface.co/Banafo/Kroko-ASR"&gt;https://huggingface.co/Banafo/Kroko-ASR&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Thoughts / caveats&lt;/strong&gt;&lt;br /&gt; We’re still ironing out some things, especially around licensing limits and how to release models in the fairest way. Our philosophy is: easier to give more than to give less later. Some details may change as we learn from the community.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Future&lt;/strong&gt;&lt;br /&gt; There is plenty of room to improve the models, as most are still trained on our older pipeline.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;&lt;br /&gt; Smaller, faster, (almost) hallucination-free Whisper replacement that streams on CPU/mobile. Looking for testers!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/banafo"&gt; /u/banafo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntiua9/we_just_opensourced_kroko_asr_a_fast_streaming/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntiua9/we_just_opensourced_kroko_asr_a_fast_streaming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ntiua9/we_just_opensourced_kroko_asr_a_fast_streaming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-29T14:01:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1nu88eq</id>
    <title>Local LLM Stack Documentation</title>
    <updated>2025-09-30T09:03:37+00:00</updated>
    <author>
      <name>/u/gulensah</name>
      <uri>https://old.reddit.com/user/gulensah</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Especially for enterprise companies, the use of internet-based LLMs raises serious &lt;strong&gt;information security concerns&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;As a result, &lt;strong&gt;local LLM stacks&lt;/strong&gt; are becoming increasingly popular as a safer alternative. &lt;/p&gt; &lt;p&gt;However, many of us — myself included — are not experts in AI or LLMs. During my research, I found that most of the available documentation is either too technical or too high-level, making it difficult to implement a local LLM stack effectively. Also, finding a complete and well-integrated solution can be challenging. &lt;/p&gt; &lt;p&gt;To make this more accessible, I’ve built a local LLM stack with open-source components and documented the installation and configuration steps. &lt;strong&gt;I learnt alot from this community&lt;/strong&gt; so, I want to share my own stack publicly incase it can help anyone out there. Please feel free to give feedbacks and ask questions.&lt;/p&gt; &lt;p&gt;Linkedin post if you want to read from there: &lt;a href="https://www.linkedin.com/posts/muratbuker_localllm-enterpriseai-aiinfrastructure-activity-7378697265768595456-XaLE/?utm_source=share&amp;amp;utm_medium=member_desktop&amp;amp;rcm=ACoAAAavCJ4Bw7TLktSQb4MsPCfEEvYTVHvM5Dg"&gt;link&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GitHub Repo with several config files: &lt;a href="https://github.com/MuratBuker/Local-LLM-Stack-Documentation"&gt;link&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What does this stack provide&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A web-based chat interface to interact with various LLMs.&lt;/li&gt; &lt;li&gt;Document processing and embedding capabilities.&lt;/li&gt; &lt;li&gt;Integration with multiple LLM servers for flexibility and performance.&lt;/li&gt; &lt;li&gt;A vector database for efficient storage and retrieval of embeddings.&lt;/li&gt; &lt;li&gt;A relational database for storing configurations and chat history.&lt;/li&gt; &lt;li&gt;MCP servers for enhanced functionalities.&lt;/li&gt; &lt;li&gt;User authentication and management.&lt;/li&gt; &lt;li&gt;Web search capabilities for your LLMs.&lt;/li&gt; &lt;li&gt;Easy management of Docker containers via Portainer.&lt;/li&gt; &lt;li&gt;GPU support for high-performance computing.&lt;/li&gt; &lt;li&gt;And more...&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;blockquote&gt; &lt;p&gt;⚠️ &lt;strong&gt;Disclaimer&lt;/strong&gt;&lt;br /&gt; I am not an expert in this field. The information I share is based solely on my personal experience and research.&lt;br /&gt; Please make sure to conduct your own research and thorough testing before applying any of these solutions in a production environment.&lt;/p&gt; &lt;/blockquote&gt; &lt;hr /&gt; &lt;p&gt;The stack is composed of the following components:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Portainer&lt;/strong&gt;: A web-based management interface for Docker environments. We will use lots containers in this stack, so Portainer will help us manage them easily.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ollama&lt;/strong&gt;: A local LLM server that hosts various language models. Not the best performance-wise, but easy to set up and use.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;vLLM&lt;/strong&gt;: A high-performance language model server. It supports a wide range of models and is optimized for speed and efficiency.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Open-WebUI&lt;/strong&gt;: A web-based user interface for interacting with language models. It supports multiple backends, including Ollama and vLLM.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Docling&lt;/strong&gt;: A document processing and embedding service. It extracts text from various document formats and generates embeddings for use in LLMs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;MCPO&lt;/strong&gt;: A multi-cloud proxy orchestrator that integrates with various MCP servers.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Netbox MCP&lt;/strong&gt;: A server for managing network devices and configurations.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Time MCP&lt;/strong&gt;: A server for providing time-related functionalities.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Qdrant&lt;/strong&gt;: A vector database for storing and querying embeddings.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;PostgreSQL&lt;/strong&gt;: A relational database for storing configuration and chat history.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gulensah"&gt; /u/gulensah &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nu88eq/local_llm_stack_documentation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nu88eq/local_llm_stack_documentation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nu88eq/local_llm_stack_documentation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-30T09:03:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1ntkhy4</id>
    <title>3 Tesla GPUs in a Desktop Case</title>
    <updated>2025-09-29T15:06:42+00:00</updated>
    <author>
      <name>/u/eso_logic</name>
      <uri>https://old.reddit.com/user/eso_logic</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntkhy4/3_tesla_gpus_in_a_desktop_case/"&gt; &lt;img alt="3 Tesla GPUs in a Desktop Case" src="https://b.thumbs.redditmedia.com/SCJrZr3BN3gZMwMfhfc1yRYq4ZCMxF344eoPEV5hcjQ.jpg" title="3 Tesla GPUs in a Desktop Case" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Plus a slot leftover for a dual 10G ethernet adapter. Originally, a goal of the &lt;a href="https://esologic.com/new-cooler-first-look/"&gt;cooler project&lt;/a&gt; was to be able to do 4 cards in a desktop case but after a lot of experimentation, I don't think it's realistic to be able to dissapate 1000W+ with only your standard case fans.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eso_logic"&gt; /u/eso_logic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ntkhy4"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntkhy4/3_tesla_gpus_in_a_desktop_case/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ntkhy4/3_tesla_gpus_in_a_desktop_case/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-29T15:06:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ntmj9c</id>
    <title>Fiction.liveBench tested DeepSeek 3.2, Qwen-max, grok-4-fast, Nemotron-nano-9b</title>
    <updated>2025-09-29T16:23:13+00:00</updated>
    <author>
      <name>/u/fictionlive</name>
      <uri>https://old.reddit.com/user/fictionlive</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntmj9c/fictionlivebench_tested_deepseek_32_qwenmax/"&gt; &lt;img alt="Fiction.liveBench tested DeepSeek 3.2, Qwen-max, grok-4-fast, Nemotron-nano-9b" src="https://preview.redd.it/2krrie9kq4sf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b4324894b3e610dcdabb98a60c481e0333d12c3a" title="Fiction.liveBench tested DeepSeek 3.2, Qwen-max, grok-4-fast, Nemotron-nano-9b" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fictionlive"&gt; /u/fictionlive &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2krrie9kq4sf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntmj9c/fictionlivebench_tested_deepseek_32_qwenmax/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ntmj9c/fictionlivebench_tested_deepseek_32_qwenmax/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-29T16:23:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1nu1k9h</id>
    <title>Update on dual b580 llm setup</title>
    <updated>2025-09-30T02:34:40+00:00</updated>
    <author>
      <name>/u/hasanismail_</name>
      <uri>https://old.reddit.com/user/hasanismail_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nu1k9h/update_on_dual_b580_llm_setup/"&gt; &lt;img alt="Update on dual b580 llm setup" src="https://b.thumbs.redditmedia.com/qSywsjDXRQpInV4W0as8QaOMqeYNvPBUOkYPnZFYUqY.jpg" title="Update on dual b580 llm setup" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Finally, after so much work, I got dual Intel ARK B580 GPUs working in LM Studio on an X99 system that has 80 PCIe lanes. Now I'm gonna install two more GPUs to get a total of 48 gigs of VRAM, and test it out. Right now, with both GPUs, I can run a 20 gig model at 60 tokens per second.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hasanismail_"&gt; /u/hasanismail_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nu1k9h"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nu1k9h/update_on_dual_b580_llm_setup/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nu1k9h/update_on_dual_b580_llm_setup/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-30T02:34:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1nu06l4</id>
    <title>Ring 1T Preview out??</title>
    <updated>2025-09-30T01:30:11+00:00</updated>
    <author>
      <name>/u/ComplexType568</name>
      <uri>https://old.reddit.com/user/ComplexType568</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nu06l4/ring_1t_preview_out/"&gt; &lt;img alt="Ring 1T Preview out??" src="https://external-preview.redd.it/4BcKbo7xe3TnsDWWKr8OW54tcjXZDo7muOhK2VwktmU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2da594e626c0f695f51deb12b2499b0ca73799e6" title="Ring 1T Preview out??" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i heard a national holiday is coming soon for China, i guess EVERYONE is pumping out some wild stuff... Qwen VL, Omni, Guard, DeepSeek 3.2-Exp and now inclusionAI somehow. hopefully the model isnt benchmaxxed as its already so massive (ive tested Ling 1.5 and its... interesting)... and i guess it wont matter cuz this is already on the cusp of requiring you to have at least 20K worth of equipment to run (at least we have their smaller counterparts) hopefully the BailingMoE arch gets implemented into llamacpp cuz I have been quite interested to see how Ling &amp;amp; Ring Flash compare to Qwen3 Next &amp;amp; gpt-oss-120b&lt;/p&gt; &lt;p&gt;(p.s. this is my first post, no clue how the &amp;quot;etiquette&amp;quot; works around here, sorry if i messed something up)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ComplexType568"&gt; /u/ComplexType568 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/inclusionAI/Ring-1T-preview"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nu06l4/ring_1t_preview_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nu06l4/ring_1t_preview_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-30T01:30:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1nu961v</id>
    <title>Best real-time speech-to-speech model?</title>
    <updated>2025-09-30T10:04:09+00:00</updated>
    <author>
      <name>/u/ffinzy</name>
      <uri>https://old.reddit.com/user/ffinzy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We've been using &lt;a href="https://github.com/kyutai-labs/unmute"&gt;unmute&lt;/a&gt;, and it's the best open source real-time STT -&amp;gt; LLM -&amp;gt; TTS model/system that I know so far.&lt;/p&gt; &lt;p&gt;Now we're looking for a more accurate STT while maintaining real-time speed and high throughput. Ideally the model is speech-to-speech directly so the AI can provide feedback on the input voice itself and not just the transcription.&lt;/p&gt; &lt;p&gt;We want to try the Qwen3-Omni but AFAIK there's no speech-to-speech support in vLLM yet. There's a hosted model but we want to use the open source if possible.&lt;/p&gt; &lt;p&gt;We're building a free real-time AI app for people to practice their English speaking skills.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ffinzy"&gt; /u/ffinzy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nu961v/best_realtime_speechtospeech_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nu961v/best_realtime_speechtospeech_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nu961v/best_realtime_speechtospeech_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-30T10:04:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1nu1rul</id>
    <title>Ling-mini-2.0 finally almost here. Lets push context size</title>
    <updated>2025-09-30T02:45:00+00:00</updated>
    <author>
      <name>/u/tabletuser_blogspot</name>
      <uri>https://old.reddit.com/user/tabletuser_blogspot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been keeping an eye on &lt;a href="https://huggingface.co/inclusionAI/Ling-mini-2.0-GGUF"&gt;Ling 2.0&lt;/a&gt; and today I finally got to benchmark it. I does require a special &lt;a href="https://github.com/im0qianqian/llama.cpp/releases"&gt;build&lt;/a&gt; b6570 to get some models to work. I'm using the Vulkan build.&lt;/p&gt; &lt;p&gt;System: AMD Radeon RX 7900 GRE 16GB Vram GPU. Kubuntu 24.04 OS with 64GB DDR4 system RAM.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/inclusionAI/Ling-mini-2.0-GGUF"&gt;Ling-mini-2.0-Q6_K.gguf&lt;/a&gt; - Works&lt;/p&gt; &lt;p&gt;Ling-mini-2.0-IQ3_XXS.gguf - Failed to load&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;bailingmoe2 16B.A1B Q6_K&lt;/td&gt; &lt;td align="left"&gt;12.45 GiB&lt;/td&gt; &lt;td align="left"&gt;16.26 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;3225.27 ± 25.23&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;bailingmoe2 16B.A1B Q6_K&lt;/td&gt; &lt;td align="left"&gt;12.45 GiB&lt;/td&gt; &lt;td align="left"&gt;16.26 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;246.42 ± 2.02&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;So Ling 2.0 model runs fast on my Radeon GPU so that gave me the chance to see how much prompt processing via context size (&lt;code&gt;--n-prompt&lt;/code&gt; or &lt;code&gt;-p&lt;/code&gt; ) effects overall token per second speed.&lt;/p&gt; &lt;p&gt;&lt;code&gt;/build-b6570-Ling/bin/llama-bench -m /Ling-mini-2.0-Q6_K.gguf -p 1024,2048,4096,8192,16384,32768&lt;/code&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;bailingmoe2 16B.A1B Q6_K&lt;/td&gt; &lt;td align="left"&gt;12.45 GiB&lt;/td&gt; &lt;td align="left"&gt;16.26 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp1024&lt;/td&gt; &lt;td align="left"&gt;3227.30 ± 27.81&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;bailingmoe2 16B.A1B Q6_K&lt;/td&gt; &lt;td align="left"&gt;12.45 GiB&lt;/td&gt; &lt;td align="left"&gt;16.26 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp2048&lt;/td&gt; &lt;td align="left"&gt;3140.33 ± 5.50&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;bailingmoe2 16B.A1B Q6_K&lt;/td&gt; &lt;td align="left"&gt;12.45 GiB&lt;/td&gt; &lt;td align="left"&gt;16.26 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp4096&lt;/td&gt; &lt;td align="left"&gt;2706.48 ± 11.89&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;bailingmoe2 16B.A1B Q6_K&lt;/td&gt; &lt;td align="left"&gt;12.45 GiB&lt;/td&gt; &lt;td align="left"&gt;16.26 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp8192&lt;/td&gt; &lt;td align="left"&gt;2327.70 ± 13.88&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;bailingmoe2 16B.A1B Q6_K&lt;/td&gt; &lt;td align="left"&gt;12.45 GiB&lt;/td&gt; &lt;td align="left"&gt;16.26 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp16384&lt;/td&gt; &lt;td align="left"&gt;1899.15 ± 9.70&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;bailingmoe2 16B.A1B Q6_K&lt;/td&gt; &lt;td align="left"&gt;12.45 GiB&lt;/td&gt; &lt;td align="left"&gt;16.26 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp32768&lt;/td&gt; &lt;td align="left"&gt;1327.07 ± 3.94&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;bailingmoe2 16B.A1B Q6_K&lt;/td&gt; &lt;td align="left"&gt;12.45 GiB&lt;/td&gt; &lt;td align="left"&gt;16.26 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;247.00 ± 0.51&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Well doesn't that take a hit. Went from pp512 of 3225 t/s to pp32768 getting 1327 t/s. Losing almost 2/3 process speed, but gaining lots of run for input more data. This is still very impressive. We have a 16B parameter model posting some faster numbers. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tabletuser_blogspot"&gt; /u/tabletuser_blogspot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nu1rul/lingmini20_finally_almost_here_lets_push_context/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nu1rul/lingmini20_finally_almost_here_lets_push_context/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nu1rul/lingmini20_finally_almost_here_lets_push_context/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-30T02:45:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1nte1kr</id>
    <title>DeepSeek-V3.2 released</title>
    <updated>2025-09-29T10:04:40+00:00</updated>
    <author>
      <name>/u/Leather-Term-30</name>
      <uri>https://old.reddit.com/user/Leather-Term-30</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/collections/deepseek-ai/deepseek-v32-68da2f317324c70047c28f66"&gt;https://huggingface.co/collections/deepseek-ai/deepseek-v32-68da2f317324c70047c28f66&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Leather-Term-30"&gt; /u/Leather-Term-30 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nte1kr/deepseekv32_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nte1kr/deepseekv32_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nte1kr/deepseekv32_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-29T10:04:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1ntmiz1</id>
    <title>Sammyuri built a redstone system to run a small language model (~5M params) in Minecraft!</title>
    <updated>2025-09-29T16:22:54+00:00</updated>
    <author>
      <name>/u/Daniel_H212</name>
      <uri>https://old.reddit.com/user/Daniel_H212</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntmiz1/sammyuri_built_a_redstone_system_to_run_a_small/"&gt; &lt;img alt="Sammyuri built a redstone system to run a small language model (~5M params) in Minecraft!" src="https://external-preview.redd.it/9KGtXL31_ILzBLGvG1YO0OZiv4dTVrqsaYqfruNLKD8.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c7208d66dc5511d3ad04d8f44f11e2cfe87d5239" title="Sammyuri built a redstone system to run a small language model (~5M params) in Minecraft!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;May not be interesting to most people, but as a Minecraft player, this is insane and I think deserves recognition. This is running a local language model after all, so I think it fits here.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Daniel_H212"&gt; /u/Daniel_H212 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=VaeI9YgE1o8"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntmiz1/sammyuri_built_a_redstone_system_to_run_a_small/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ntmiz1/sammyuri_built_a_redstone_system_to_run_a_small/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-29T16:22:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1nu71rx</id>
    <title>More detail about GLM4.6</title>
    <updated>2025-09-30T07:45:17+00:00</updated>
    <author>
      <name>/u/Angel-Karlsson</name>
      <uri>https://old.reddit.com/user/Angel-Karlsson</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It seems glm4.6 is finally out! &lt;/p&gt; &lt;p&gt;Blog post: &lt;a href="https://z.ai/blog/glm-4.6"&gt;https://z.ai/blog/glm-4.6&lt;/a&gt; Hugging face (not working now but later): &lt;a href="https://huggingface.co/zai-org/GLM-4.6"&gt;https://huggingface.co/zai-org/GLM-4.6&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Context window from 128k to 200k, better coding, reasoning and agentic performance... &lt;/p&gt; &lt;p&gt;That's quite a nice upgrade!&lt;/p&gt; &lt;p&gt;&amp;quot;The Z.ai API platform offers both GLM-4.6 and GLM-4.6-Air models&amp;quot;&lt;/p&gt; &lt;p&gt;There is an air version but not that's much information...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Angel-Karlsson"&gt; /u/Angel-Karlsson &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nu71rx/more_detail_about_glm46/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nu71rx/more_detail_about_glm46/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nu71rx/more_detail_about_glm46/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-30T07:45:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1ntrau8</id>
    <title>inclusionAI/Ring-1T-preview</title>
    <updated>2025-09-29T19:21:03+00:00</updated>
    <author>
      <name>/u/TKGaming_11</name>
      <uri>https://old.reddit.com/user/TKGaming_11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntrau8/inclusionairing1tpreview/"&gt; &lt;img alt="inclusionAI/Ring-1T-preview" src="https://preview.redd.it/7vb7yumam5sf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f95b7ca10a14cb69158cd8b654359ce4e30e802f" title="inclusionAI/Ring-1T-preview" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Weights: &lt;a href="https://huggingface.co/inclusionAI/Ring-1T-preview"&gt;https://huggingface.co/inclusionAI/Ring-1T-preview&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TKGaming_11"&gt; /u/TKGaming_11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7vb7yumam5sf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntrau8/inclusionairing1tpreview/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ntrau8/inclusionairing1tpreview/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-29T19:21:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1ntw4vz</id>
    <title>The Most Esoteric eGPU: Dual NVIDIA Tesla V100 (64G) for AI &amp; LLM</title>
    <updated>2025-09-29T22:28:44+00:00</updated>
    <author>
      <name>/u/rexyuan</name>
      <uri>https://old.reddit.com/user/rexyuan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntw4vz/the_most_esoteric_egpu_dual_nvidia_tesla_v100_64g/"&gt; &lt;img alt="The Most Esoteric eGPU: Dual NVIDIA Tesla V100 (64G) for AI &amp;amp; LLM" src="https://a.thumbs.redditmedia.com/SdG7NEM3u3jA0Yf8ao8JR5xQUo8y2pfnRQweIt5W_b4.jpg" title="The Most Esoteric eGPU: Dual NVIDIA Tesla V100 (64G) for AI &amp;amp; LLM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Read this with images on my blog:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://blog.rexyuan.com/the-most-esoteric-egpu-dual-nvidia-tesla-v100-64g-for-ai-llm-41a3166dc2ac"&gt;medium&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://jekyll.rexyuan.com/2025/09/30/v100/"&gt;static site&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;(I was going to buy one of these and make a whole YouTube video about it, but I am a bit tight on money rn, so I decided just to share my research as a blog post.)&lt;/p&gt; &lt;h2&gt;Preface&lt;/h2&gt; &lt;p&gt;The &lt;a href="https://www.techpowerup.com/gpu-specs/tesla-v100-pcie-16-gb.c2957"&gt;Nvidia Tesla V100&lt;/a&gt; was released in mid-2017. It was a PCIe Gen 3.0 GPU, primarily designed for machine learning tasks. These Tesla GPUs, although almost a decade old now, remain moderately popular among AI enthusiasts due to their low market price and large VRAM.&lt;/p&gt; &lt;p&gt;In addition to the regular PCIe version, there is also the &lt;a href="https://www.techpowerup.com/gpu-specs/tesla-v100-sxm2-16-gb.c3018"&gt;Nvidia Tesla V100 SXM2&lt;/a&gt; module version. These are modular GPUs that you plug into dedicated slots on an Nvidia server motherboard.&lt;/p&gt; &lt;p&gt;One thing to note is that these GPUs do not use GDDR for VRAM. They use another memory called HBM, which has a much higher bandwidth than GDDR of the same generation. For comparison, the GTX 1080 Ti, the best consumer GPU released in the same year as V100, uses GDDR5X with 484.4 GB/s bandwidth, while V100 uses HBM2 with a whopping 897.0 GB/s bandwidth.&lt;/p&gt; &lt;h2&gt;The Summit Supercomputer&lt;/h2&gt; &lt;p&gt;The &lt;a href="https://en.wikipedia.org/wiki/Summit_(supercomputer"&gt;Summit supercomputer&lt;/a&gt;) in the US was decommissioned last November. In it were almost 30000 pieces of V100 in the SXM2 form factor. These V100s were then disposed of. But much like most enterprise hardware, there’s a whole supply chain of companies that specialize in turning a man’s garbage into another man’s treasure in the used enterprise gear market.&lt;/p&gt; &lt;p&gt;Earlier this year, as the Chinese hardware enthusiasts would call it, the “big boat” arrived, meaning there was now a sizable supply of these V100 SXM2 GPUs on the Chinese domestic market. And most importantly, they’re cheap. These can be &lt;a href="https://e.tb.cn/h.SfQlb1RyJW3P9m5?tk=uBXu4CAPRtW"&gt;purchased&lt;/a&gt; for as low as around 400 RMB(~56 USD).&lt;/p&gt; &lt;h2&gt;SXM2?&lt;/h2&gt; &lt;p&gt;Now they have the cheap hardware, but these can’t just be plugged into your PCIe slot like a regular consumer GPU. Normally, these SXM form factor GPUs are designed to be plugged directly into dedicated slots in a pre-built dedicated Nvidia-based server, which poses the question of how on earth are they gonna use them?&lt;/p&gt; &lt;p&gt;So people got to work. Some people reverse-engineered the pinouts of those server slots and then created &lt;a href="https://e.tb.cn/h.SUe1QaFSxJP4Ccu?tk=vwVA4CzrcKe"&gt;PCIe adapter boards&lt;/a&gt;(286 RMB(~40 USD)) for these SXM2 GPUs. Currently, there are already finished &lt;a href="https://e.tb.cn/h.SUV7a7SkKGvYRiN?tk=l3OU4ya00z7"&gt;V100 SXM2-adapted-to-PCIe GPUs&lt;/a&gt; at 1459 RMB(~205 USD) from NEOPC, complete with cooling and casing.&lt;/p&gt; &lt;p&gt;But this isn’t all that interesting, is it? This is just turning a V100 SXM2 version into a V100 PCIe version. But here comes the kicker: one particular company, 39com, decided to go further. They’re going to make NVLink work with these adapters.&lt;/p&gt; &lt;h2&gt;NVLink&lt;/h2&gt; &lt;p&gt;One of the unique features of Nvidia-based servers is the &lt;a href="https://en.wikichip.org/wiki/nvidia/nvlink"&gt;NVLink&lt;/a&gt; feature, which provides unparalleled bandwidth between GPUs, so much so that most people would consider them essentially sharing the VRAM. In particular, the V100 is a Tesla Volta generation model, which utilizes NVLink 2.0, supporting a bandwidth of up to 300 GB/s.&lt;/p&gt; &lt;p&gt;39com reverse-engineered NVLink and got it working on their &lt;a href="https://e.tb.cn/h.SfQlu1DHRVlqLkV?tk=yDif4CAPUu6"&gt;adapter boards&lt;/a&gt;. Currently, you can put two V100 SXM2 on their board and have them connected with full NVLink 2.0 at 300 GB/s. This is currently priced at 911 RMB(~128 USD).&lt;/p&gt; &lt;p&gt;However, at this point, the adapter boards have become so big that it no longer makes sense to plug them directly into your motherboard's PCIe slot anymore. So their board’s I/O uses 4 SlimSAS(SFF-8654 8i) ports, two ports for each V100.&lt;/p&gt; &lt;p&gt;Additionally, to connect these multiple GPUs to your motherboard with a single PCIe x 16 slot, you need to either have a motherboard that supports bifurcation and get a PCIe 3.0 to SlimSAS adapter card with two 8654 8i ports, or get a PLX8749(PCIe Gen 3.0 Switch) PCIe card that has 4 8654 8i ports.&lt;/p&gt; &lt;p&gt;Together with the dual SXM2 slot adapter board, a PLX8749 SlimSAS PCIe card, and cables, it is priced at 1565 RMB (~220 USD)&lt;/p&gt; &lt;h2&gt;Cooler&lt;/h2&gt; &lt;p&gt;Since these V100 SXM2 GPUs come as modules without coolers. They need to find another way to cool these things. The prime candidate is the stock cooler for the A100 SXM4. It has amazing cooling capacity and can fit the V100 SXM2 with minimal modification.&lt;/p&gt; &lt;h2&gt;“eGPU”&lt;/h2&gt; &lt;p&gt;There are now some pre-built systems readily available on Taobao(Chinese Amazon). One seller particularly stands out, 1CATai TECH, who seems to provide the most comprehensive solution.&lt;/p&gt; &lt;p&gt;They also directly work with 39com on the adapter boards design, so I was going to buy one of their systems, but due to my current financial situation, I just couldn’t justify the purchase.&lt;/p&gt; &lt;p&gt;Their &lt;a href="https://e.tb.cn/h.SfWy6cClZZELARJ?tk=u3sb4CAmAKJ"&gt;main product&lt;/a&gt; is a one-package system that includes the case, 39com adapter board, two V100 SXM2 GPUs with A100 coolers, an 850W PSU, SlimSAS cables, and a PCIe adapter card. It is priced from 3699 RMB(~520 USD) with two V100 16G to 12999 RMB(1264 USD) with two V100 32G.&lt;/p&gt; &lt;p&gt;I know I’m stretching the definition of eGPU, but technically, since this “thing” contains GPUs and sits outside of your main PC and you connect to it via some cables, I’d say it still is an eGPU, albeit the most esoteric one. Besides, even for a full-size desktop PC, this setup actually necessitates the use of an external placement because of the sheer size of the coolers. Additionally, there are already &lt;a href="https://www.bilibili.com/video/BV16AWGzGEuQ"&gt;major Chinese content creators&lt;/a&gt; testing this kind of “eGPU” setup out on Bilibili, hence the title of this post.&lt;/p&gt; &lt;h2&gt;Performance&lt;/h2&gt; &lt;p&gt;Since I don’t have the machine in my hand, I will quote the performance reports from their &lt;a href="https://www.bilibili.com/video/BV1nbLXzME81"&gt;official Bilibili video&lt;/a&gt;. Running &lt;a href="https://huggingface.co/Qwen/QwQ-32B"&gt;Qwen/QwQ-32B&lt;/a&gt;, the speed is 29.9 token/s on a single stream and 50.9 token/s on four concurrent streams. Running &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B"&gt;deepseek-ai/DeepSeek-R1-Distill-Llama-70B&lt;/a&gt;, the speed is 12.7 token/s on a single stream and 36 token/s on four concurrent streams.&lt;/p&gt; &lt;h2&gt;More GPUs?&lt;/h2&gt; &lt;p&gt;In theory, NVLink 2.0 supports connecting 4 GPUs together at once. But 1CATai TECH told me that they’ve been working with 39com on building an adapter that reliably works with 4 GPUs for months to no avail. Still, they said it’s definitely not impossible. They’re even planning to make an 8-GPU eGPU. They have previously successfully gotten a monstrous setup with 16 V100 SXM2 GPUs to work with multiple PLX switches for a university.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rexyuan"&gt; /u/rexyuan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ntw4vz"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntw4vz/the_most_esoteric_egpu_dual_nvidia_tesla_v100_64g/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ntw4vz/the_most_esoteric_egpu_dual_nvidia_tesla_v100_64g/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-29T22:28:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1nu3slg</id>
    <title>An Open-source Omni Chatbot for Long Speech and Voice Clone</title>
    <updated>2025-09-30T04:27:06+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nu3slg/an_opensource_omni_chatbot_for_long_speech_and/"&gt; &lt;img alt="An Open-source Omni Chatbot for Long Speech and Voice Clone" src="https://preview.redd.it/q9gqi8orb8sf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6567da929a7f403cb01556f4edb0436352b3fb36" title="An Open-source Omni Chatbot for Long Speech and Voice Clone" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Paper: &lt;a href="https://arxiv.org/abs/2509.25131"&gt;https://arxiv.org/abs/2509.25131&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Code: &lt;a href="https://github.com/dvlab-research/MGM-Omni"&gt;https://github.com/dvlab-research/MGM-Omni&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/q9gqi8orb8sf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nu3slg/an_opensource_omni_chatbot_for_long_speech_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nu3slg/an_opensource_omni_chatbot_for_long_speech_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-30T04:27:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1nth7cb</id>
    <title>The reason why Deepseek V3.2 is so cheap</title>
    <updated>2025-09-29T12:52:22+00:00</updated>
    <author>
      <name>/u/Js8544</name>
      <uri>https://old.reddit.com/user/Js8544</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nth7cb/the_reason_why_deepseek_v32_is_so_cheap/"&gt; &lt;img alt="The reason why Deepseek V3.2 is so cheap" src="https://external-preview.redd.it/FcVX6tzRZ8tGOgZD8bxf6fQ4_S6KT4J6UgLFbHWcrYo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2dddb9f56e7fc9cdc0774a534e8c31cbb7079188" title="The reason why Deepseek V3.2 is so cheap" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TLDR: It's a near linear model with almost O(kL) attention complexity.&lt;/p&gt; &lt;p&gt;Paper link: &lt;a href="https://github.com/deepseek-ai/DeepSeek-V3.2-Exp/blob/main/DeepSeek_V3_2.pdf"&gt;https://github.com/deepseek-ai/DeepSeek-V3.2-Exp/blob/main/DeepSeek_V3_2.pdf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;According to their paper, the Deepseek Sparse Attention computes attention for only k selected previous tokens, meaning it's a linear attention model with decoding complexity O(kL). What's different from previous linear models is it has a O(L^2) index selector to select the tokens to compute attention for. Even though the index selector has square complexity but it's fast enough to be neglected. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/h0zys7b4o3sf1.png?width=1390&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=00a7ea8ada91109d417b8d6e3f490ae9743c18b2"&gt;https://preview.redd.it/h0zys7b4o3sf1.png?width=1390&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=00a7ea8ada91109d417b8d6e3f490ae9743c18b2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/has2qyz7o3sf1.png?width=1300&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0742135b2cb1be9bd853b614097597d521a4ef54"&gt;https://preview.redd.it/has2qyz7o3sf1.png?width=1300&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0742135b2cb1be9bd853b614097597d521a4ef54&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/053i7pdro3sf1.png?width=1356&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=52adfb1bf9d0ee03f0a7d8e7b31340ab63b2f4b4"&gt;Cost for V3.2 only increase very little thanks to linear attention&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Previous linear model attempts for linear models from other teams like Google and Minimax have not been successful. Let's see if DS can make the breakthrough this time.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Js8544"&gt; /u/Js8544 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nth7cb/the_reason_why_deepseek_v32_is_so_cheap/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nth7cb/the_reason_why_deepseek_v32_is_so_cheap/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nth7cb/the_reason_why_deepseek_v32_is_so_cheap/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-29T12:52:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ntg6sp</id>
    <title>Chinese AI Labs Tier List</title>
    <updated>2025-09-29T12:05:39+00:00</updated>
    <author>
      <name>/u/sahilypatel</name>
      <uri>https://old.reddit.com/user/sahilypatel</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntg6sp/chinese_ai_labs_tier_list/"&gt; &lt;img alt="Chinese AI Labs Tier List" src="https://preview.redd.it/ur65noupg3sf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=19a3d6f6cec05bb06281985755fbed368e5c9ecf" title="Chinese AI Labs Tier List" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sahilypatel"&gt; /u/sahilypatel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ur65noupg3sf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntg6sp/chinese_ai_labs_tier_list/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ntg6sp/chinese_ai_labs_tier_list/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-29T12:05:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1nu4llz</id>
    <title>qwen3-from-scratch — readable PyTorch impl of Qwen3 (0.6B) for learning &amp; research</title>
    <updated>2025-09-30T05:12:55+00:00</updated>
    <author>
      <name>/u/freesysck</name>
      <uri>https://old.reddit.com/user/freesysck</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;An educational, from-scratch &lt;strong&gt;Qwen3&lt;/strong&gt; implementation with minimal deps, plus converted &lt;strong&gt;0.6B (base &amp;amp; reasoning) weights&lt;/strong&gt;. Easy to try via the &lt;code&gt;llms-from-scratch&lt;/code&gt; PyPI package. &lt;/p&gt; &lt;ul&gt; &lt;li&gt;What it is: clean PyTorch Qwen3 aimed at teaching/experimentation. &lt;/li&gt; &lt;li&gt;Weights: PyTorch state dicts converted from the official &lt;strong&gt;Qwen3-0.6B / 0.6B-Base&lt;/strong&gt; releases. &lt;/li&gt; &lt;li&gt;Try it: &lt;code&gt;pip install llms_from_scratch&lt;/code&gt;; choose base vs reasoning; ~&lt;strong&gt;1.5 GB&lt;/strong&gt; for ~150 tokens; &lt;code&gt;torch.compile&lt;/code&gt; showed ~&lt;strong&gt;4×&lt;/strong&gt; speedup (&lt;strong&gt;25→101 tok/s&lt;/strong&gt; on A100).&lt;/li&gt; &lt;li&gt;Extras: standalone notebooks (dense, +KV cache, &lt;strong&gt;MoE&lt;/strong&gt;, MoE+KV)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://huggingface.co/rasbt/qwen3-from-scratch"&gt;https://huggingface.co/rasbt/qwen3-from-scratch&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Looking for feedback from folks teaching or tinkering with small LLMs!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/freesysck"&gt; /u/freesysck &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nu4llz/qwen3fromscratch_readable_pytorch_impl_of_qwen3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nu4llz/qwen3fromscratch_readable_pytorch_impl_of_qwen3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nu4llz/qwen3fromscratch_readable_pytorch_impl_of_qwen3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-30T05:12:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1nu6i00</id>
    <title>z.ai glm-4.6 is alive now</title>
    <updated>2025-09-30T07:09:51+00:00</updated>
    <author>
      <name>/u/cobra91310</name>
      <uri>https://old.reddit.com/user/cobra91310</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nu6i00/zai_glm46_is_alive_now/"&gt; &lt;img alt="z.ai glm-4.6 is alive now" src="https://a.thumbs.redditmedia.com/Z_UukH0kiXMLnIsw7KdZt46P6CBNTiF2Rn7DRNBC7E8.jpg" title="z.ai glm-4.6 is alive now" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;incredible perforamnce for this outsider !&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/x1503sc159sf1.png?width=3390&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b175e8d38bd2593f0c52a54ed645a9d89b240f19"&gt;https://preview.redd.it/x1503sc159sf1.png?width=3390&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b175e8d38bd2593f0c52a54ed645a9d89b240f19&lt;/a&gt;&lt;/p&gt; &lt;p&gt;full detail on &lt;a href="https://z.ai/blog/glm-4.6"&gt;https://z.ai/blog/glm-4.6&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You can use it on claude code with&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&amp;quot;env&amp;quot;: {&lt;/p&gt; &lt;p&gt;&amp;quot;ANTHROPIC_AUTH_TOKEN&amp;quot;: &amp;quot;APIKEY&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;ANTHROPIC_BASE_URL&amp;quot;: &amp;quot;&lt;a href="https://api.z.ai/api/anthropic"&gt;https://api.z.ai/api/anthropic&lt;/a&gt;&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;API_TIMEOUT_MS&amp;quot;: &amp;quot;3000000&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;ANTHROPIC_MODEL&amp;quot;: &amp;quot;glm-4.6&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;ANTHROPIC_SMALL_FAST_MODEL&amp;quot;: &amp;quot;glm-4.6-air&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;ENABLE_THINKING&amp;quot;: &amp;quot;true&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;REASONING_EFFORT&amp;quot;: &amp;quot;ultrathink&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;MAX_THINKING_TOKENS&amp;quot;: &amp;quot;32000&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;ENABLE_STREAMING&amp;quot;: &amp;quot;true&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;MAX_OUTPUT_TOKENS&amp;quot;: &amp;quot;96000&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;MAX_MCP_OUTPUT_TOKENS&amp;quot;: &amp;quot;64000&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;AUTH_HEADER_MODE&amp;quot;: &amp;quot;x-api-key&amp;quot;&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;promotional code &lt;a href="https://z.ai/subscribe?ic=DJA7GX6IUW"&gt;https://z.ai/subscribe?ic=DJA7GX6IUW&lt;/a&gt; for a discount !&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cobra91310"&gt; /u/cobra91310 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nu6i00/zai_glm46_is_alive_now/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nu6i00/zai_glm46_is_alive_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nu6i00/zai_glm46_is_alive_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-30T07:09:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1nu44n4</id>
    <title>1T open source reasoning model with 50B activation</title>
    <updated>2025-09-30T04:46:03+00:00</updated>
    <author>
      <name>/u/Full_Piano_3448</name>
      <uri>https://old.reddit.com/user/Full_Piano_3448</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nu44n4/1t_open_source_reasoning_model_with_50b_activation/"&gt; &lt;img alt="1T open source reasoning model with 50B activation" src="https://preview.redd.it/evmdgk53f8sf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e3816f8287d904b019c157e3029ce60ec1892bb6" title="1T open source reasoning model with 50B activation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ring-1T-preview: &lt;a href="https://huggingface.co/inclusionAI/Ring-1T-preview"&gt;https://huggingface.co/inclusionAI/Ring-1T-preview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The first 1 trillion open-source thinking model&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Full_Piano_3448"&gt; /u/Full_Piano_3448 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/evmdgk53f8sf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nu44n4/1t_open_source_reasoning_model_with_50b_activation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nu44n4/1t_open_source_reasoning_model_with_50b_activation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-30T04:46:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1nu6dmo</id>
    <title>GLM-4.6 beats Claude Sonnet 4.5???</title>
    <updated>2025-09-30T07:02:12+00:00</updated>
    <author>
      <name>/u/ramphyx</name>
      <uri>https://old.reddit.com/user/ramphyx</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nu6dmo/glm46_beats_claude_sonnet_45/"&gt; &lt;img alt="GLM-4.6 beats Claude Sonnet 4.5???" src="https://preview.redd.it/qm4pw6oh39sf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a7504dacba279de1b6f2a5e8909c2c5ba1a28ecd" title="GLM-4.6 beats Claude Sonnet 4.5???" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://docs.z.ai/guides/llm/glm-4.6"&gt;https://docs.z.ai/guides/llm/glm-4.6&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ramphyx"&gt; /u/ramphyx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/qm4pw6oh39sf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nu6dmo/glm46_beats_claude_sonnet_45/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nu6dmo/glm46_beats_claude_sonnet_45/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-30T07:02:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1nu75l3</id>
    <title>Glm 4.6 is out and it's going against claude 4.5</title>
    <updated>2025-09-30T07:52:13+00:00</updated>
    <author>
      <name>/u/Independent-Wind4462</name>
      <uri>https://old.reddit.com/user/Independent-Wind4462</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nu75l3/glm_46_is_out_and_its_going_against_claude_45/"&gt; &lt;img alt="Glm 4.6 is out and it's going against claude 4.5" src="https://preview.redd.it/xdaov3whc9sf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=51b49323776db858f9523557bb7f0b2fbdcab9f2" title="Glm 4.6 is out and it's going against claude 4.5" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Wind4462"&gt; /u/Independent-Wind4462 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xdaov3whc9sf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nu75l3/glm_46_is_out_and_its_going_against_claude_45/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nu75l3/glm_46_is_out_and_its_going_against_claude_45/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-30T07:52:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1nu6kjc</id>
    <title>Hot take: ALL Coding tools are bullsh*t</title>
    <updated>2025-09-30T07:14:23+00:00</updated>
    <author>
      <name>/u/Adventurous-Slide776</name>
      <uri>https://old.reddit.com/user/Adventurous-Slide776</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Let me tell you about the dumbest fucking trend in software development: taking the most powerful reasoning engines humanity has ever created and lobotomizing them with middleware.&lt;/p&gt; &lt;p&gt;We have these incredible language models—DeepSeek 3.2, GLM-4.5, Qwen 3 Coder—that can understand complex problems, reason through edge cases, and generate genuinely good code. And what did we do? We wrapped them in so many layers of bullshit that they can barely function.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Scam:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Every coding tool follows the same playbook:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Inject a 20,000 token system prompt explaining how to use tools&lt;/li&gt; &lt;li&gt;Add tool-calling ceremonies for every filesystem operation&lt;/li&gt; &lt;li&gt;Send timezone, task lists, environment info with EVERY request&lt;/li&gt; &lt;li&gt;Read the same files over and over and over&lt;/li&gt; &lt;li&gt;Make tiny edits one at a time&lt;/li&gt; &lt;li&gt;Re-read everything to &amp;quot;verify&amp;quot;&lt;/li&gt; &lt;li&gt;Repeat until you've burned 50,000 tokens&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;And then they market this as &amp;quot;agentic&amp;quot; and &amp;quot;autonomous&amp;quot; and charge you $20/month.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Reality:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The model spends 70% of its context window reading procedural garbage it's already seen five times. It's not thinking about your problem—it's playing filesystem navigator. It's not reasoning deeply—it's pattern matching through the noise because it's cognitively exhausted.&lt;/p&gt; &lt;p&gt;You ask it to fix a bug. It reads the file (3k tokens). Checks the timezone (why?). Reviews the task list (who asked?). Makes a one-line change. Reads the file AGAIN to verify. Runs a command. Reads the output. And somehow the bug still isn't fixed because the model never had enough clean context to actually understand the problem.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Insanity:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;What you can accomplish in 15,000 tokens with a direct conversation—problem explained, context provided, complete solution generated—these tools spread across 50,000 tokens of redundant slop.&lt;/p&gt; &lt;p&gt;The model generates the same code snippets again and again. It sees the same file contents five times in one conversation. It's drowning in its own output, suffocating under layers of middleware-generated vomit.&lt;/p&gt; &lt;p&gt;And the worst part? &lt;strong&gt;It gives worse results.&lt;/strong&gt; The solutions are half-assed because the model is working with a fraction of its actual reasoning capacity. Everything else is burned on ceremonial bullshit.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Market Dynamics:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;VCs threw millions at &amp;quot;AI coding agents.&amp;quot; Companies rushed to ship agentic frameworks. Everyone wanted to be the &amp;quot;autonomous&amp;quot; solution. So they added more tools, more features, more automation.&lt;/p&gt; &lt;p&gt;More context r*pe.&lt;/p&gt; &lt;p&gt;They optimized for demos, not for actual utility. Because in a demo, watching the tool &amp;quot;autonomously&amp;quot; read files and run commands looks impressive. In reality, you're paying 3x the API costs for 0.5x the quality.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Simple Truth:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Just upload your fucking files to a local chat interface like LobeHub (Open Source). Explain the problem. Let the model think. Get your code in one artifact. Copy it. Done.&lt;/p&gt; &lt;p&gt;No tool ceremonies. No context pollution. No reading the same file seven times. No timezone updates nobody asked for.&lt;/p&gt; &lt;p&gt;The model's full intelligence goes toward your problem, not toward navigating a filesystem through an API. You get better code, faster, for less money.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Irony:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;We spent decades making programming languages more expressive so humans could think at a higher level. Then we built AI that can understand natural language and reason about complex systems.&lt;/p&gt; &lt;p&gt;And then we forced it back down into the machine-level bullsh*t of &amp;quot;read file, edit line 47, write file, run command, read output.&amp;quot;&lt;/p&gt; &lt;p&gt;We took reasoning engines and turned them into glorified bash scripts.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Future:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I hope we look back at this era and laugh. The &amp;quot;agentic coding tool&amp;quot; phase where everyone was convinced that more automation meant better results. Where we drowned AI in context pollution and called it progress.&lt;/p&gt; &lt;p&gt;The tools that will win aren't the ones with the most features or the most autonomy. They're the ones that get out of the model's way and let it do what it's actually good at: thinking.&lt;/p&gt; &lt;p&gt;Until then, I'll be over here using the chat interface like a sane person, getting better results for less money, while the rest of you pay for the privilege of context r*pe.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Adventurous-Slide776"&gt; /u/Adventurous-Slide776 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nu6kjc/hot_take_all_coding_tools_are_bullsht/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nu6kjc/hot_take_all_coding_tools_are_bullsht/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nu6kjc/hot_take_all_coding_tools_are_bullsht/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-30T07:14:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1nturn1</id>
    <title>Full fine-tuning is not needed anymore.</title>
    <updated>2025-09-29T21:33:22+00:00</updated>
    <author>
      <name>/u/yoracale</name>
      <uri>https://old.reddit.com/user/yoracale</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nturn1/full_finetuning_is_not_needed_anymore/"&gt; &lt;img alt="Full fine-tuning is not needed anymore." src="https://preview.redd.it/69mpyf7476sf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dc01e45e5ea6eb71334a7054098e9326040ccd84" title="Full fine-tuning is not needed anymore." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A new Thinking Machines blog led by John Schulman (OpenAI co-founder) shows how LoRA in reinforcement learning (RL) can match full-finetuning performance when done right! And all while using 2/3 of the resources of FFT. Blog: &lt;a href="https://thinkingmachines.ai/blog/lora/"&gt;https://thinkingmachines.ai/blog/lora/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is super important as previously, there was a misconception that you must have tonnes (8+) of GPUs to achieve a great thinking model with FFT, but now, with just LoRA, you can achieve the same results on just a single GPU!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/dnj5h595d6sf1.png?width=1718&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5d529f363c48f1fd837a40277375fcf67f041d03"&gt;https://preview.redd.it/dnj5h595d6sf1.png?width=1718&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5d529f363c48f1fd837a40277375fcf67f041d03&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The belief that “LoRA is worse” was a misconception, it simply hadn’t been applied properly. This result reinforces that parameter-efficient fine-tuning is highly effective for most post-training use cases.&lt;/li&gt; &lt;li&gt;Apply LoRA across &lt;strong&gt;every layer&lt;/strong&gt;, not only attention - this includes MLP/MoE blocks.&lt;/li&gt; &lt;li&gt;Train with a learning rate about &lt;strong&gt;10× higher&lt;/strong&gt; than what’s used for full fine-tuning.&lt;/li&gt; &lt;li&gt;LoRA requires only about &lt;strong&gt;two-thirds of the compute&lt;/strong&gt; compared to full fine-tuning.&lt;/li&gt; &lt;li&gt;Even at &lt;strong&gt;rank = 1&lt;/strong&gt;, it performs very well for RL.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This goes to show that you that anyone can train a fantastic RL model with algorithms like GRPO, GSPO etc. for free, even on - all you need to do is have the right &lt;a href="https://docs.unsloth.ai/get-started/fine-tuning-llms-guide/lora-hyperparameters-guide"&gt;hyper-parameters&lt;/a&gt; and strategy!&lt;/p&gt; &lt;p&gt;Ofc FFT still has many use-cases however, but this goes to show that it doesn't need to be forced literally everywhere and in every training run. P.S. some people might've been misinterpreting my title, I'm not saying FFT is dead or useless now, 'not needed anymore' means it's not a 'must' or a 'requirement' anymore!&lt;/p&gt; &lt;p&gt;So hopefully this will make RL so much more accessible to everyone, especially in the long run!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yoracale"&gt; /u/yoracale &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/69mpyf7476sf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nturn1/full_finetuning_is_not_needed_anymore/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nturn1/full_finetuning_is_not_needed_anymore/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-29T21:33:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
