<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-10T11:34:27+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qzz0vr</id>
    <title>GLM 5 is coming! spotted on vllm PR</title>
    <updated>2026-02-09T08:39:31+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzz0vr/glm_5_is_coming_spotted_on_vllm_pr/"&gt; &lt;img alt="GLM 5 is coming! spotted on vllm PR" src="https://preview.redd.it/285aias7lfig1.jpg?width=140&amp;amp;height=78&amp;amp;auto=webp&amp;amp;s=5a644c4fce313f2c4b8643b1d8a7931145a54db1" title="GLM 5 is coming! spotted on vllm PR" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/285aias7lfig1.jpg?width=680&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=5287959d193fad4f96c5c80ec8b7546a7dcbe023"&gt;https://preview.redd.it/285aias7lfig1.jpg?width=680&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=5287959d193fad4f96c5c80ec8b7546a7dcbe023&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/vllm-project/vllm/pull/34124"&gt;https://github.com/vllm-project/vllm/pull/34124&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzz0vr/glm_5_is_coming_spotted_on_vllm_pr/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzz0vr/glm_5_is_coming_spotted_on_vllm_pr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qzz0vr/glm_5_is_coming_spotted_on_vllm_pr/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-09T08:39:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1r0akbh</id>
    <title>LLaDA2.1-flash (103B) and LLaDA2.1-mini (16B)</title>
    <updated>2026-02-09T17:31:19+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0akbh/llada21flash_103b_and_llada21mini_16b/"&gt; &lt;img alt="LLaDA2.1-flash (103B) and LLaDA2.1-mini (16B)" src="https://external-preview.redd.it/5n3mc4pDM1OpAZhRMkcJ8p4UVJvhRxbwGRsFzFjYGnk.png?width=140&amp;amp;height=75&amp;amp;auto=webp&amp;amp;s=6d973feb83cc911296f0f4f8c40875351ac703a7" title="LLaDA2.1-flash (103B) and LLaDA2.1-mini (16B)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;note: this is a diffusion model&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;LLaDA2.1-flash&lt;/strong&gt; is a diffusion language model of the LLaDA series featuring the editing enhancement. It significantly improves inference speed while delivering strong task performance.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/0zc0kqvw7iig1.png?width=1391&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c9c347ed3fe4b69f50acf4af01e3d6f96ad616f8"&gt;https://preview.redd.it/0zc0kqvw7iig1.png?width=1391&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c9c347ed3fe4b69f50acf4af01e3d6f96ad616f8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/biz1dmry7iig1.png?width=1372&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0f9e9af10dae02d44553059f9654c8bc0683cf39"&gt;https://preview.redd.it/biz1dmry7iig1.png?width=1372&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0f9e9af10dae02d44553059f9654c8bc0683cf39&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/inclusionAI/LLaDA2.1-flash"&gt;https://huggingface.co/inclusionAI/LLaDA2.1-flash&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/inclusionAI/LLaDA2.1-mini"&gt;https://huggingface.co/inclusionAI/LLaDA2.1-mini&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0akbh/llada21flash_103b_and_llada21mini_16b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0akbh/llada21flash_103b_and_llada21mini_16b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r0akbh/llada21flash_103b_and_llada21mini_16b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-09T17:31:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1r02o7o</id>
    <title>GLM 5 Support Is On It's Way For Transformers</title>
    <updated>2026-02-09T12:16:36+00:00</updated>
    <author>
      <name>/u/Few_Painter_5588</name>
      <uri>https://old.reddit.com/user/Few_Painter_5588</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r02o7o/glm_5_support_is_on_its_way_for_transformers/"&gt; &lt;img alt="GLM 5 Support Is On It's Way For Transformers" src="https://external-preview.redd.it/_RA8pRu79eov51fP28AH3ibXc2RY_CG7SQQVryJy9WU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=810b321415879975e3408c463a34398fefd38bf5" title="GLM 5 Support Is On It's Way For Transformers" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This probably means the model launch is imminent, and all evidence points to Pony Alpha on OpenRouter being a stealth deployment of GLM 5&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Few_Painter_5588"&gt; /u/Few_Painter_5588 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/huggingface/transformers/pull/43858"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r02o7o/glm_5_support_is_on_its_way_for_transformers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r02o7o/glm_5_support_is_on_its_way_for_transformers/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-09T12:16:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1r0wavy</id>
    <title>What'd be the best 30B model for programming?</title>
    <updated>2026-02-10T09:30:28+00:00</updated>
    <author>
      <name>/u/Hikolakita</name>
      <uri>https://old.reddit.com/user/Hikolakita</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know my question is pretty vague but everytime I do researches I find different advices. Sometimes it's qwen3, sometimes GLM, sometimes deepseek, etc&lt;/p&gt; &lt;p&gt;Honestly I'd do any kind of code with it except small, easy repetitive tasks which I already have codium for. And I'm also not a vibecoder, I need an AI that can do &lt;strong&gt;deep reasoning&lt;/strong&gt; and do good at software organization, app developement, code review, bug fixes, etc... (basically any moderately complex task)&lt;br /&gt; But it doesn't need to write big and long pieces of code. It just should assist me as much as possible cause of course AI assisted coding is the future.&lt;/p&gt; &lt;p&gt;Thanks in advance for your help!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hikolakita"&gt; /u/Hikolakita &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0wavy/whatd_be_the_best_30b_model_for_programming/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0wavy/whatd_be_the_best_30b_model_for_programming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r0wavy/whatd_be_the_best_30b_model_for_programming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-10T09:30:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1r0q02v</id>
    <title>Last Week in Multimodal AI - Local Edition</title>
    <updated>2026-02-10T03:36:19+00:00</updated>
    <author>
      <name>/u/Vast_Yak_4147</name>
      <uri>https://old.reddit.com/user/Vast_Yak_4147</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0q02v/last_week_in_multimodal_ai_local_edition/"&gt; &lt;img alt="Last Week in Multimodal AI - Local Edition" src="https://external-preview.redd.it/TcZl6oKplNwXQ9Bc2m9EOXWKDxbucouTXSa7fjmn-fA.png?width=140&amp;amp;height=75&amp;amp;auto=webp&amp;amp;s=37a8a1126001d49366fbecd978f54552cd091f4f" title="Last Week in Multimodal AI - Local Edition" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I curate a weekly multimodal AI roundup, here are the local/open-source highlights from last week:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;MiniCPM-o 4.5 - 9B Multimodal Model for Phones&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Beats GPT-4o on vision benchmarks at 9B parameters with real-time bilingual voice conversations.&lt;/li&gt; &lt;li&gt;Runs entirely on-device with no cloud dependency. Privacy by default.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/openbmb/MiniCPM-o-4_5"&gt;Hugging Face&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1r0q02v/video/1zof97mq7lig1/player"&gt;https://reddit.com/link/1r0q02v/video/1zof97mq7lig1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Nemotron ColEmbed V2 - Visual Document Retrieval&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;NVIDIA's family of visual document retrieval models (3B, 4B, 8B) with the 8B topping ViDoRe V3 benchmark by 3%.&lt;/li&gt; &lt;li&gt;Purpose-built for finding information inside scanned documents and PDFs. Weights on Hugging Face.&lt;/li&gt; &lt;li&gt;&lt;a href="https://arxiv.org/abs/2602.03992"&gt;Paper&lt;/a&gt; | &lt;a href="https://huggingface.co/nvidia/nemotron-colembed-vl-8b-v2"&gt;Hugging Face&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Cropper - Local Private Media Cropper&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A local, private media cropper built entirely by GPT-5.3-Codex. Runs locally with no cloud calls.&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/cocktailpeanut/status/2019834796026081667?s=20"&gt;Post&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1r0q02v/video/hvkykb8p7lig1/player"&gt;https://reddit.com/link/1r0q02v/video/hvkykb8p7lig1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Lingbot World Launcher - 1-Click Gradio Launcher&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="/u/zast57"&gt;u/zast57&lt;/a&gt; built a 1-click Gradio launcher for the Lingbot World Model. Anyone with a GPU can test it.&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/zast57/status/2020522559222026478?s=20"&gt;Post&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1r0q02v/video/lkoxzwqk7lig1/player"&gt;https://reddit.com/link/1r0q02v/video/lkoxzwqk7lig1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;VK-LSVD - 40B Interaction Short-Video Dataset&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Massive dataset of 40 billion user interactions for short-video recommendation research.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/datasets/deepvk/VK-LSVD"&gt;Hugging Face&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;LTX-2 Pet Video Fun&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Community members have been animating pet photos with LTX-2 v2v and getting great results.&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/r/StableDiffusion/comments/1qxs6uz/prompting_your_pets_is_easy_with_ltx2_v2v/"&gt;Reddit Thread&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1r0q02v/video/wr4llm4y7lig1/player"&gt;https://reddit.com/link/1r0q02v/video/wr4llm4y7lig1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Honorable Mention:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TinyLoRA - Single-Parameter Fine-Tuning&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Meta FAIR method that fine-tunes models with as few as one trainable parameter.&lt;/li&gt; &lt;li&gt;Drops the compute requirement for model customization to near zero. No GPU cluster needed.&lt;/li&gt; &lt;li&gt;&lt;a href="https://arxiv.org/abs/2602.04118"&gt;Paper&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Checkout the &lt;a href="https://open.substack.com/pub/thelivingedge/p/last-week-in-multimodal-ai-44-small?utm_campaign=post-expanded-share&amp;amp;utm_medium=web"&gt;full roundup&lt;/a&gt; for more demos, papers, and resources.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vast_Yak_4147"&gt; /u/Vast_Yak_4147 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0q02v/last_week_in_multimodal_ai_local_edition/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0q02v/last_week_in_multimodal_ai_local_edition/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r0q02v/last_week_in_multimodal_ai_local_edition/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-10T03:36:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1r0la37</id>
    <title>Qwen3-v1-8b is Capable of Solving Captchas</title>
    <updated>2026-02-10T00:08:16+00:00</updated>
    <author>
      <name>/u/TheyCallMeDozer</name>
      <uri>https://old.reddit.com/user/TheyCallMeDozer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0la37/qwen3v18b_is_capable_of_solving_captchas/"&gt; &lt;img alt="Qwen3-v1-8b is Capable of Solving Captchas" src="https://preview.redd.it/prijluyk6kig1.png?width=140&amp;amp;height=91&amp;amp;auto=webp&amp;amp;s=fd129265161fe3820389b74a0d3773a7fe0b9d58" title="Qwen3-v1-8b is Capable of Solving Captchas" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen3-v1-8b is capable of solving captchas with semi-solid accracy... might need to write a simple python script that finds them on the page and uses the LLM to try to solve them and input the output.&lt;/p&gt; &lt;p&gt;Not sure if anyone else tried this before, just thought could be a handy thing for people to know, accidentally found it when passing it a screenshot&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/prijluyk6kig1.png?width=1038&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=29f55976839c594bd72eae9c2d0e6e2b9ce9a0d5"&gt;https://preview.redd.it/prijluyk6kig1.png?width=1038&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=29f55976839c594bd72eae9c2d0e6e2b9ce9a0d5&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheyCallMeDozer"&gt; /u/TheyCallMeDozer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0la37/qwen3v18b_is_capable_of_solving_captchas/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0la37/qwen3v18b_is_capable_of_solving_captchas/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r0la37/qwen3v18b_is_capable_of_solving_captchas/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-10T00:08:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1r0py0k</id>
    <title>IRIS 18B</title>
    <updated>2026-02-10T03:33:38+00:00</updated>
    <author>
      <name>/u/thebadslime</name>
      <uri>https://old.reddit.com/user/thebadslime</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;IRIS 18B started off as ERNIE 21BA3B, first I reap pruned ERNIE by 20%, then trained on 3B tokens of thinking traces. This improved benchmarks and led to a more usable model. It takes a prompt very well, has no repetition or hallucinated user speaking bugs.&lt;/p&gt; &lt;p&gt;I attempted SFT, but it did not go super well and introduced a number of bugs, as well as locking in rigid tool calls that didn't always match the actual tools. &lt;/p&gt; &lt;p&gt;So I made the decision to release the CPT checkpoint.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/jerrimu/IRIS-18B-CPT"&gt;https://huggingface.co/jerrimu/IRIS-18B-CPT&lt;/a&gt; HF version.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/jerrimu/IRIS-18B-GGUFS"&gt;https://huggingface.co/jerrimu/IRIS-18B-GGUFS&lt;/a&gt; GGUFS ( 16, 8, 4, 2 bit)&lt;/p&gt; &lt;p&gt;I have been daily driving the model for days and find it great, it works well with the two tools built into my inference app ( web search and file access)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thebadslime"&gt; /u/thebadslime &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0py0k/iris_18b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0py0k/iris_18b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r0py0k/iris_18b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-10T03:33:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1r0wgaf</id>
    <title>I built an autonomous research agent in C# that runs entirely on local LLMs (Ollama + llama3.1:8b)</title>
    <updated>2026-02-10T09:39:54+00:00</updated>
    <author>
      <name>/u/Dynamic-Styles</name>
      <uri>https://old.reddit.com/user/Dynamic-Styles</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I got tired of manually copy-pasting URLs into ChatGPT for research, so I built an agent that does it autonomously. Figured I'd share since this sub loves practical local LLM projects.&lt;/p&gt; &lt;p&gt;What it does: - You give it a topic (&amp;quot;persistent memory for AI agents&amp;quot;) - It generates 5-8 search queries - Searches the web via Brave Search API - Fetches and reads the top sources - Analyzes each page for relevant findings - Synthesizes everything into a structured markdown report&lt;/p&gt; &lt;p&gt;All inference runs locally via Ollama (llama3.1:8b). No OpenAI/Anthropic API needed.&lt;/p&gt; &lt;p&gt;Performance on my setup (Ryzen 5 5500, CPU-only, 16GB RAM): - ~15 minutes per research run - 8-12 sources analyzed - 5-8 key findings extracted - Structured report with citations&lt;/p&gt; &lt;p&gt;What I learned: - 3B models (llama3.2) are unreliable for tool calling. 8B minimum. - You MUST truncate findings before synthesis or the model chokes on long context - SQLite + embeddings works great for memory at personal scale — no vector DB needed - C# is actually a great language for AI agents (fast, typed, good tooling)&lt;/p&gt; &lt;p&gt;Tech stack: C# / .NET 8, Ollama, SQLite, Brave Search API (free tier)&lt;/p&gt; &lt;p&gt;Source: &lt;a href="https://github.com/DynamicCSharp/hex-dynamics"&gt;https://github.com/DynamicCSharp/hex-dynamics&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you want to build your own agent from scratch, I also made a starter kit with an 8-chapter guide: &lt;a href="https://github.com/DynamicCSharp/agentkit"&gt;https://github.com/DynamicCSharp/agentkit&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer questions about the architecture or share specific code. The whole thing is MIT licensed.&lt;/p&gt; &lt;p&gt;Known limitations: - CPU inference is slow (~15min). With a GPU it'd be much faster. - 8B models still occasionally produce malformed tool calls — I retry with fallback prompts - Research quality depends heavily on what Brave Search returns for your topic&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dynamic-Styles"&gt; /u/Dynamic-Styles &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0wgaf/i_built_an_autonomous_research_agent_in_c_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0wgaf/i_built_an_autonomous_research_agent_in_c_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r0wgaf/i_built_an_autonomous_research_agent_in_c_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-10T09:39:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1r0bd4i</id>
    <title>New "Stealth" Model - Aurora Alpha - (Free on OpenRouter)</title>
    <updated>2026-02-09T18:00:10+00:00</updated>
    <author>
      <name>/u/-pawix</name>
      <uri>https://old.reddit.com/user/-pawix</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0bd4i/new_stealth_model_aurora_alpha_free_on_openrouter/"&gt; &lt;img alt="New &amp;quot;Stealth&amp;quot; Model - Aurora Alpha - (Free on OpenRouter)" src="https://preview.redd.it/9t7ajm04diig1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=28bf73099a957820854270db4b7e2e87db1b2055" title="New &amp;quot;Stealth&amp;quot; Model - Aurora Alpha - (Free on OpenRouter)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;New cloaked reasoning model dropped on OpenRouter for $0/M tokens&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-pawix"&gt; /u/-pawix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9t7ajm04diig1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0bd4i/new_stealth_model_aurora_alpha_free_on_openrouter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r0bd4i/new_stealth_model_aurora_alpha_free_on_openrouter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-09T18:00:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1r0x4zk</id>
    <title>Real world usage, feedback and suggestions for best LLM for C#</title>
    <updated>2026-02-10T10:22:40+00:00</updated>
    <author>
      <name>/u/bloodbath_mcgrath666</name>
      <uri>https://old.reddit.com/user/bloodbath_mcgrath666</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Over the last several months I have started exploring LLM's and AI as it doesnt look like its going away anytime soon now. (A1111 / comfyUI / Ollama / ChatGPT / claude / gemini)&lt;/p&gt; &lt;p&gt;I dabble in a bit of programming too (unity game engine), I want to run local models and have been learning how to use them, testing a few different models here and there, general chat ones through to coding, nothing serious yet, really basic stuff just to see how they respond, figure out some promp engineering etc.&lt;/p&gt; &lt;p&gt;However I have started to expand my knowledge, tokens, weights etc.&lt;/p&gt; &lt;p&gt;But this brings me to the subjective question of &amp;quot;best LLM for xxxx&amp;quot;&lt;br /&gt; this will also be hardware dependent I know, but this brings me to an interesing question itself, whats best for different hardware setups.&lt;/p&gt; &lt;p&gt;Can people add their thoughts on their best LLM for coding, any experience with C# + specified LLM, and what hardware they are running including if possible what speeds/context limits they are getting/running&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bloodbath_mcgrath666"&gt; /u/bloodbath_mcgrath666 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0x4zk/real_world_usage_feedback_and_suggestions_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0x4zk/real_world_usage_feedback_and_suggestions_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r0x4zk/real_world_usage_feedback_and_suggestions_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-10T10:22:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1r0ser2</id>
    <title>Any latest OCR model I can run locally in 18GB RAM?</title>
    <updated>2026-02-10T05:35:19+00:00</updated>
    <author>
      <name>/u/A-n-d-y-R-e-d</name>
      <uri>https://old.reddit.com/user/A-n-d-y-R-e-d</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Do you know any OCR model I can run on an 18GB MarkBook Pro to convert PDF to markdown accurately and quickly? &lt;/p&gt; &lt;p&gt;I tested the glmocr, which took exactly 45 minutes &amp;amp; 10 seconds to process a 200-page PDF document. &lt;/p&gt; &lt;p&gt;Please share the steps to set it up as well!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/A-n-d-y-R-e-d"&gt; /u/A-n-d-y-R-e-d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0ser2/any_latest_ocr_model_i_can_run_locally_in_18gb_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0ser2/any_latest_ocr_model_i_can_run_locally_in_18gb_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r0ser2/any_latest_ocr_model_i_can_run_locally_in_18gb_ram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-10T05:35:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1r0ekq2</id>
    <title>Who is waiting for deepseek v4 ,GLM 5 and Qwen 3.5 and MiniMax 2.2?</title>
    <updated>2026-02-09T19:54:09+00:00</updated>
    <author>
      <name>/u/power97992</name>
      <uri>https://old.reddit.com/user/power97992</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The title? I hope they come out soon... I'm especially waiting for DS V4, it should be pretty good, hopefully it will be reasonably fast(probably slow though since it is gonna be bigger than v3.2) via OpenRouter. Well, glm 5 is out already technically on Open Router. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/power97992"&gt; /u/power97992 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0ekq2/who_is_waiting_for_deepseek_v4_glm_5_and_qwen_35/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0ekq2/who_is_waiting_for_deepseek_v4_glm_5_and_qwen_35/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r0ekq2/who_is_waiting_for_deepseek_v4_glm_5_and_qwen_35/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-09T19:54:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1r0domc</id>
    <title>Qwen to the rescue</title>
    <updated>2026-02-09T19:22:00+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0domc/qwen_to_the_rescue/"&gt; &lt;img alt="Qwen to the rescue" src="https://external-preview.redd.it/WEJxFtDPKCN6TKUmgiGRQqR9H_BOQlE9OiaOmXHqz_8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1b3946db287286eed978b63c0503ea93c3e10526" title="Qwen to the rescue" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;...does this mean that we are close?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/19468"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0domc/qwen_to_the_rescue/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r0domc/qwen_to_the_rescue/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-09T19:22:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1r0vbe6</id>
    <title>Built a real-time agent execution visualizer for OpenCode — watching agents think is addicting</title>
    <updated>2026-02-10T08:27:06+00:00</updated>
    <author>
      <name>/u/jiwonme</name>
      <uri>https://old.reddit.com/user/jiwonme</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0vbe6/built_a_realtime_agent_execution_visualizer_for/"&gt; &lt;img alt="Built a real-time agent execution visualizer for OpenCode — watching agents think is addicting" src="https://external-preview.redd.it/aWkwNjhncHRubWlnMSRPpC6DAaBm6WYT_LarrMwD93Xxp2yjAWpr41ra18A4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d2e478f5ecb343014c58f3bb3549fa3f3c955f53" title="Built a real-time agent execution visualizer for OpenCode — watching agents think is addicting" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I've been hacking on a real-time visualization tool that hooks into OpenCode and renders the agent's execution graph as it runs.&lt;/p&gt; &lt;p&gt;You can see:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Tasks getting dispatched in parallel (delegate_task spawning subtasks)&lt;/li&gt; &lt;li&gt;Each tool call with latency (bash 29ms, delegate_task 59ms etc.)&lt;/li&gt; &lt;li&gt;Token usage and cost per node&lt;/li&gt; &lt;li&gt;The agent catching errors and self-correcting in real time&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In the screenshot, the orchestrator fires off two parallel tasks (&amp;quot;Height measurement state model&amp;quot; &amp;amp; &amp;quot;Question answer API contract&amp;quot;), both subagents come back with &amp;quot;Unauthorized&amp;quot; errors, and the agent goes &amp;quot;this is suspicious&amp;quot; and starts verifying — all visualized live as a flowing graph.&lt;/p&gt; &lt;p&gt;Honestly the biggest thing is it just makes the whole experience way more dynamic. Instead of watching terminal text scroll by, you actually &lt;em&gt;see&lt;/em&gt; the agent's decision tree branching and converging. Makes debugging so much easier too — you can immediately spot where things went sideways.&lt;/p&gt; &lt;p&gt;Still early days but pretty hooked on this. Anyone else building agent observability stuff?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jiwonme"&gt; /u/jiwonme &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ssgn36ptnmig1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0vbe6/built_a_realtime_agent_execution_visualizer_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r0vbe6/built_a_realtime_agent_execution_visualizer_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-10T08:27:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1r0qur4</id>
    <title>Deepseek architecture, but without all the parameters</title>
    <updated>2026-02-10T04:16:13+00:00</updated>
    <author>
      <name>/u/silenceimpaired</name>
      <uri>https://old.reddit.com/user/silenceimpaired</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m seeing a pattern that perhaps is not legitimate, but it seems everyone is copying the latest Deepseek architecture on their latest releases. In the process though they are also copying the parameter count (roughly), which makes the models inaccessible to most (unless you use their API or spent as much as you would to buy a used car).&lt;/p&gt; &lt;p&gt;So my question is, are there smaller models using the same tech but with less parameters?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/silenceimpaired"&gt; /u/silenceimpaired &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0qur4/deepseek_architecture_but_without_all_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0qur4/deepseek_architecture_but_without_all_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r0qur4/deepseek_architecture_but_without_all_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-10T04:16:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1r03wfq</id>
    <title>Bad news for local bros</title>
    <updated>2026-02-09T13:14:31+00:00</updated>
    <author>
      <name>/u/FireGuy324</name>
      <uri>https://old.reddit.com/user/FireGuy324</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r03wfq/bad_news_for_local_bros/"&gt; &lt;img alt="Bad news for local bros" src="https://preview.redd.it/ui5ovstbygig1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1eaeb40f2ac5a09ac1ba2fe03e433877561acb20" title="Bad news for local bros" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FireGuy324"&gt; /u/FireGuy324 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ui5ovstbygig1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r03wfq/bad_news_for_local_bros/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r03wfq/bad_news_for_local_bros/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-09T13:14:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1r0gju0</id>
    <title>Kimi-Linear-48B-A3B-Instruct</title>
    <updated>2026-02-09T21:05:29+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0gju0/kimilinear48ba3binstruct/"&gt; &lt;img alt="Kimi-Linear-48B-A3B-Instruct" src="https://a.thumbs.redditmedia.com/Bu8mu8gAAQcXdPhDs_xaj8m-19PQF2_a4_iwxZQsj70.jpg" title="Kimi-Linear-48B-A3B-Instruct" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;three days after the release we finally have a GGUF: &lt;a href="https://huggingface.co/bartowski/moonshotai_Kimi-Linear-48B-A3B-Instruct-GGUF"&gt;https://huggingface.co/bartowski/moonshotai_Kimi-Linear-48B-A3B-Instruct-GGUF&lt;/a&gt; - big thanks to Bartowski!&lt;/p&gt; &lt;p&gt;long context looks more promising than GLM 4.7 Flash&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1r0gju0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0gju0/kimilinear48ba3binstruct/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r0gju0/kimilinear48ba3binstruct/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-09T21:05:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1r0khh8</id>
    <title>Step-3.5-Flash IS A BEAST</title>
    <updated>2026-02-09T23:35:09+00:00</updated>
    <author>
      <name>/u/SennVacan</name>
      <uri>https://old.reddit.com/user/SennVacan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i was browsing around for models to run for my openclaw instant and this thing is such a good model for it's size, on the other hand the gpt oss 120b hung at each every step, this model does everything without me telling it technical stuff yk. Its also free on openrouter for now so i have been using it from there, i ligit rivels Deepseek V3.2 at 1/3rd of the size. I hope its api is cheap upon release &lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/stepfun-ai/Step-3.5-Flash"&gt;https://huggingface.co/stepfun-ai/Step-3.5-Flash&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SennVacan"&gt; /u/SennVacan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0khh8/step35flash_is_a_beast/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0khh8/step35flash_is_a_beast/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r0khh8/step35flash_is_a_beast/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-09T23:35:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1r0v0y1</id>
    <title>Opus 4.6 Reasoning Distill 3k prompts</title>
    <updated>2026-02-10T08:08:23+00:00</updated>
    <author>
      <name>/u/volious-ka</name>
      <uri>https://old.reddit.com/user/volious-ka</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just finished a 3k distill of Opus 4.6. Let me know what you think and how it affects your model! I've used it on DASD-4B-Thinking and the difference is insane. &lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/datasets/crownelius/Opus-4.6-CoT-3000x"&gt;https://huggingface.co/datasets/crownelius/Opus-4.6-CoT-3000x&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/volious-ka"&gt; /u/volious-ka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0v0y1/opus_46_reasoning_distill_3k_prompts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0v0y1/opus_46_reasoning_distill_3k_prompts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r0v0y1/opus_46_reasoning_distill_3k_prompts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-10T08:08:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1r0or7s</id>
    <title>Femtobot: A 10MB Rust Agent for Low-Resource Machines</title>
    <updated>2026-02-10T02:40:21+00:00</updated>
    <author>
      <name>/u/yunfoe</name>
      <uri>https://old.reddit.com/user/yunfoe</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0or7s/femtobot_a_10mb_rust_agent_for_lowresource/"&gt; &lt;img alt="Femtobot: A 10MB Rust Agent for Low-Resource Machines" src="https://external-preview.redd.it/cmw5ZTJ5bnd3a2lnMa2OwS6wmI-E0GDGdMuj7R4EL-J7nO8YwfKZKjv0DlnG.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d58c63cf47b8d8004de9dcc138a3388beabe0a83" title="Femtobot: A 10MB Rust Agent for Low-Resource Machines" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to run &lt;a href="https://github.com/openclaw/openclaw"&gt;OpenClaw&lt;/a&gt;-style workflows on very low-resource machines (older Raspberry Pis, cheap VPS instances), but most “lightweight” stacks still end up dragging in large runtimes and slow startup costs.&lt;/p&gt; &lt;p&gt;After trying &lt;a href="https://github.com/HKUDS/nanobot"&gt;nanobot&lt;/a&gt; and seeing disk usage climb past ~350MB once Python, virtualenvs, and dependencies were installed, I rewrote the core ideas in Rust to see how small and fast it could be.&lt;/p&gt; &lt;p&gt;The result is &lt;a href="https://github.com/enzofrasca/femtobot"&gt;femtobot&lt;/a&gt;: a single ~10MB binary that currently supports:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Telegram polling&lt;/li&gt; &lt;li&gt;Local memory (SQLite + vector storage)&lt;/li&gt; &lt;li&gt;Tool execution (shell, filesystem, web) via &lt;a href="https://github.com/0xPlaygrounds/rig"&gt;rig-core&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The implementation was done quickly with heavy AI assistance, so the code prioritizes simplicity and size over perfect Rust idioms. It works well on constrained hardware, but there are definitely rough edges.&lt;/p&gt; &lt;p&gt;Sharing in case it’s useful or interesting to others experimenting with small, local, or low-power agent setups. You are also welcome to contribute.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/enzofrasca/femtobot"&gt;https://github.com/enzofrasca/femtobot&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yunfoe"&gt; /u/yunfoe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/nbv8vsnwwkig1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0or7s/femtobot_a_10mb_rust_agent_for_lowresource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r0or7s/femtobot_a_10mb_rust_agent_for_lowresource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-10T02:40:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1r0abpl</id>
    <title>Do not Let the "Coder" in Qwen3-Coder-Next Fool You! It's the Smartest, General Purpose Model of its Size</title>
    <updated>2026-02-09T17:23:31+00:00</updated>
    <author>
      <name>/u/Iory1998</name>
      <uri>https://old.reddit.com/user/Iory1998</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Like many of you, I like to use LLM as tools to help improve my daily life, from editing my emails, to online search.&lt;/p&gt; &lt;p&gt;However, I like to use them as an &amp;quot;inner voice&amp;quot; to discuss general thoughts and get constructive critic. For instance, when I face life-related problems take might take me hours or days to figure out, a short session with an LLM can significantly quicken that process.&lt;/p&gt; &lt;p&gt;Since the original Llama was leaked, I've been using LLMs locally, but they I always felt they were lacking behind OpenAI or Google models. Thus, I would always go back to using ChatGPT or Gemini when I need serious output. If I needed a long chatting session or help with long documents, I didn't have choice to use the SOTA models, and that means willingly leaking personal or work-related data.&lt;/p&gt; &lt;p&gt;For me, Gemini-3 is the best model I've ever tried. I don't know about you, but I struggle sometimes to follow chatGPT's logic, but I find it easy to follow Gemini's. It's like that best friend who just gets you and speaks in your language.&lt;/p&gt; &lt;p&gt;Well, that was the case until I tried Qwen3-Coder-Next. For the first time, I could have stimulating and enlightening conversations with a local model. Previously, I used not-so-seriously Qwen3-Next-80B-A3B-Thinking as local daily driver, but that model always felt a bit inconsistent; sometimes, I get good output, and sometimes I get dumb one.&lt;/p&gt; &lt;p&gt;However, Qwen3-Coder-Next is more consistent, and you can feel that it's a pragmatic model trained to be a problem-solver rather than being a sycophant. Unprompted, it will suggest an author, a book, or a theory that already exists that might help. I genuinely feel I am conversing with a fellow thinker rather than a echo chamber constantly paraphrasing my prompts in a more polish way. It's the closest model to Gemini-2.5/3 that I can run locally in terms of quality of experience.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;For non-coders, my point is do not sleep on Qwen3-Coder-Next simply because it's has the &amp;quot;coder&amp;quot; tag attached.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I can't wait for for Qwen-3.5 models. If Qwen3-Coder-Next is an early preview, we are in a real treat.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Iory1998"&gt; /u/Iory1998 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0abpl/do_not_let_the_coder_in_qwen3codernext_fool_you/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0abpl/do_not_let_the_coder_in_qwen3codernext_fool_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r0abpl/do_not_let_the_coder_in_qwen3codernext_fool_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-09T17:23:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1r0nd6m</id>
    <title>A fully local home automation voice assistant using Qwen3 ASR, LLM and TTS on an RTX 5060 Ti with 16GB VRAM</title>
    <updated>2026-02-10T01:39:23+00:00</updated>
    <author>
      <name>/u/liampetti</name>
      <uri>https://old.reddit.com/user/liampetti</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0nd6m/a_fully_local_home_automation_voice_assistant/"&gt; &lt;img alt="A fully local home automation voice assistant using Qwen3 ASR, LLM and TTS on an RTX 5060 Ti with 16GB VRAM" src="https://external-preview.redd.it/MGRhbXB0cmhta2lnMey19SmkPge57MTwSl95CCxzGWVZmEEqcz1nfiupw6bq.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ce3c06c8aea8e1cd702c51582c7c4e11ddf54870" title="A fully local home automation voice assistant using Qwen3 ASR, LLM and TTS on an RTX 5060 Ti with 16GB VRAM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Video shows the latency and response times running everything Qwen3 (ASR&amp;amp;TTS 1.7B, Qwen3 4B Instruct 2507) with a Morgan Freeman voice clone on an RTX 5060 Ti with 16GB VRAM. In this example the SearXNG server is not running so it shows the model reverting to its own knowledge when unable to obtain web search information.&lt;/p&gt; &lt;p&gt;I tested other smaller models for intent generation but response quality dropped dramatically on the LLM models under 4B. Kokoro (TTS) and Moonshine (ASR) are also included as options for smaller systems.&lt;/p&gt; &lt;p&gt;The project comes with a bunch of tools it can use, such as Spotify, Philips Hue light control, AirTouch climate control and online weather retrieval (Australian project so uses the BOM). &lt;/p&gt; &lt;p&gt;I have called the project &amp;quot;Fulloch&amp;quot;. Try it out or build your own project out of it from here: &lt;a href="https://github.com/liampetti/fulloch"&gt;https://github.com/liampetti/fulloch&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/liampetti"&gt; /u/liampetti &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/feropirhmkig1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0nd6m/a_fully_local_home_automation_voice_assistant/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r0nd6m/a_fully_local_home_automation_voice_assistant/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-10T01:39:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1r0eo44</id>
    <title>MechaEpstein-8000</title>
    <updated>2026-02-09T19:57:33+00:00</updated>
    <author>
      <name>/u/ortegaalfredo</name>
      <uri>https://old.reddit.com/user/ortegaalfredo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0eo44/mechaepstein8000/"&gt; &lt;img alt="MechaEpstein-8000" src="https://external-preview.redd.it/xypXKrxWxdZlS8MfiDHiCuqwqkIzWDQHn3pcj2ChEio.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f3c824638b39c14125f9a5dcd28ddf84eb8a3622" title="MechaEpstein-8000" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know it has already been done but this is my AI trained on Epstein Emails. Surprisingly hard to do, as most LLMs will refuse to generate the dataset for Epstein, lol. Everything about this is local, the dataset generation, training, etc. Done in a 16GB RTX-5000 ADA. &lt;/p&gt; &lt;p&gt;Anyway, it's based on Qwen3-8B and its quite funny. GGUF available at link.&lt;br /&gt; Also I have it online here if you dare: &lt;a href="https://www.neuroengine.ai/Neuroengine-MechaEpstein"&gt;https://www.neuroengine.ai/Neuroengine-MechaEpstein&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ortegaalfredo"&gt; /u/ortegaalfredo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/ortegaalfredo/MechaEpstein-8000-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0eo44/mechaepstein8000/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r0eo44/mechaepstein8000/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-09T19:57:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1r0w7st</id>
    <title>Qwen-Image-2.0 is out - 7B unified gen+edit model with native 2K and actual text rendering</title>
    <updated>2026-02-10T09:25:15+00:00</updated>
    <author>
      <name>/u/RIPT1D3_Z</name>
      <uri>https://old.reddit.com/user/RIPT1D3_Z</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen team just released Qwen-Image-2.0. Before anyone asks - no open weights yet, it's API-only on Alibaba Cloud (invite beta) and free demo on Qwen Chat. But given their track record with Qwen-Image v1 (weights dropped like a month after launch, Apache 2.0), I'd be surprised if this stays closed for long.&lt;/p&gt; &lt;p&gt;So what's the deal:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;7B model, down from 20B in v1, which is great news for local runners&lt;/li&gt; &lt;li&gt;Unified generation + editing in one pipeline, no need for separate models&lt;/li&gt; &lt;li&gt;Native 2K (2048×2048), realistic textures that actually look good&lt;/li&gt; &lt;li&gt;Text rendering from prompts up to 1K tokens. Infographics, posters, slides, even Chinese calligraphy. Probably the best text-in-image I've seen from an open lab&lt;/li&gt; &lt;li&gt;Multi-panel comic generation (4×6) with consistent characters&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The 7B size is the exciting part here. If/when weights drop, this should be very runnable on consumer hardware. V1 at 20B was already popular in ComfyUI, a 7B version doing more with less is exactly what local community needs.&lt;/p&gt; &lt;p&gt;Demo is up on Qwen Chat if you want to test before committing any hopium to weights release.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RIPT1D3_Z"&gt; /u/RIPT1D3_Z &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://qwen.ai/blog?id=qwen-image-2.0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0w7st/qwenimage20_is_out_7b_unified_genedit_model_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r0w7st/qwenimage20_is_out_7b_unified_genedit_model_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-10T09:25:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
