<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-25T12:09:41+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ofnx9i</id>
    <title>Performance difference while using Ollama Model vs HF Model</title>
    <updated>2025-10-25T10:10:09+00:00</updated>
    <author>
      <name>/u/Warriorsito</name>
      <uri>https://old.reddit.com/user/Warriorsito</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofnx9i/performance_difference_while_using_ollama_model/"&gt; &lt;img alt="Performance difference while using Ollama Model vs HF Model" src="https://b.thumbs.redditmedia.com/nb_0VQ-8yyT4WuDSZ2r03ssWj2mS9LZ9paeQMPj5ivg.jpg" title="Performance difference while using Ollama Model vs HF Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Downloaded the exact same model (gpt-oss 20b) from &lt;strong&gt;Ollama Hub&lt;/strong&gt; and &lt;strong&gt;Hugging Face&lt;/strong&gt;. Both run using &lt;strong&gt;Ollama to do inference&lt;/strong&gt;, but the Ollama-Hub copy drives my GPU Power and Usage to ~&lt;strong&gt;100%&lt;/strong&gt; and ~&lt;strong&gt;150 t/s&lt;/strong&gt;, while the HF copy only uses ~&lt;strong&gt;50%&lt;/strong&gt; GPU and ~&lt;strong&gt;80 t/s&lt;/strong&gt;. Both are the same quant (I assumed by model size), so I’m trying to understand what can still cause this perf difference and what to check next.&lt;/p&gt; &lt;p&gt;-------------------------------------------------------&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Models:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Ollama (14Gb): &lt;code&gt;ollama pull gpt-oss:20b&lt;/code&gt;&lt;/li&gt; &lt;li&gt;HF (14Gb, unsloth GGUF at F16): &lt;code&gt;ollama pull&lt;/code&gt; &lt;a href="http://hf.co/unsloth/gpt-oss-20b-GGUF:F16"&gt;&lt;code&gt;hf.co/unsloth/gpt-oss-20b-GGUF:F16&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For testing I prompted the exact same message multiple times and in all the cases I made sure to offload the model and create a new chat to reset the context.&lt;/p&gt; &lt;p&gt;It is clearly seen in afterburner that while inference using the Ollama model the GPU power and usage goes and stays at 100% whereas while doing the same with the HF GGUF the GPU power doesn't go past 50% and takes quite longer to finish.&lt;/p&gt; &lt;p&gt;For both cases the model is being fully loaded into the GPU VRAM (24Gb available) and the CPU usage is more or less the same.&lt;/p&gt; &lt;p&gt;Finally, checked and compared both modelfiles using the &lt;code&gt;show&lt;/code&gt; command from Ollama and the only differences I found where at the end of the files:&lt;/p&gt; &lt;p&gt;Ollama:&lt;/p&gt; &lt;p&gt;&lt;code&gt;PARAMETER temperature 1&lt;/code&gt;&lt;/p&gt; &lt;p&gt;HF GGUF:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;PARAMETER top_p 1 PARAMETER stop &amp;lt;|endoftext|&amp;gt; PARAMETER stop &amp;lt;|return|&amp;gt; PARAMETER temperature 1 PARAMETER min_p 0 PARAMETER top_k 0 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;What can be the cause for this performance difference?&lt;/strong&gt;&lt;br /&gt; &lt;strong&gt;Is this caused by any of the PARAMETER present in the HF Model?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Thanks and sorry if this is a noob question or obvious for some people, I'm just trying to learn!&lt;/p&gt; &lt;p&gt;-------------------------------------------------------&lt;/p&gt; &lt;p&gt;&lt;strong&gt;EDIT:&lt;/strong&gt; ollama ps and afterburner image.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;NAME SIZE PROCESSOR CONTEXT UNTIL gpt-oss:20b 14 GB 100% GPU 8192 Forever NAME SIZE PROCESSOR CONTEXT UNTIL hf.co/unsloth/gpt-oss-20b-GGUF:F16 14 GB 100% GPU 8192 Forever &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/hxe244f2j8xf1.png?width=1572&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5ba232beb9e5570d2f2d35f28a959fe3b02dba70"&gt;First peak is Ollama Model, second one is HF Model.&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Warriorsito"&gt; /u/Warriorsito &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofnx9i/performance_difference_while_using_ollama_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofnx9i/performance_difference_while_using_ollama_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ofnx9i/performance_difference_while_using_ollama_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-25T10:10:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1ofo8zs</id>
    <title>Anyone used Reducto for parsing? How good is their embedding-aware chunking?</title>
    <updated>2025-10-25T10:30:52+00:00</updated>
    <author>
      <name>/u/BriefCardiologist656</name>
      <uri>https://old.reddit.com/user/BriefCardiologist656</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Curious if anyone here has used Reducto for document parsing or retrieval pipelines.&lt;/p&gt; &lt;p&gt;They seem to focus on generating LLM-ready chunks using a mix of vision-language models and something they call “embedding-optimized” or intelligent chunking. The idea is that it preserves document layout and meaning (tables, figures, etc.) before generating embeddings for RAG or vector search systems.&lt;/p&gt; &lt;p&gt;I’m mostly wondering how this works in practice&lt;/p&gt; &lt;p&gt;- Does their “embedding-aware” chunking noticeably improve retrieval or reduce hallucinations?&lt;/p&gt; &lt;p&gt;- Did you still need to run additional preprocessing or custom chunking on top of it?&lt;/p&gt; &lt;p&gt;Would appreciate hearing from anyone who’s tried it in production or at scale.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BriefCardiologist656"&gt; /u/BriefCardiologist656 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofo8zs/anyone_used_reducto_for_parsing_how_good_is_their/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofo8zs/anyone_used_reducto_for_parsing_how_good_is_their/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ofo8zs/anyone_used_reducto_for_parsing_how_good_is_their/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-25T10:30:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1ofom66</id>
    <title>Enable Gemma 2 2b thinking in LM studio</title>
    <updated>2025-10-25T10:54:01+00:00</updated>
    <author>
      <name>/u/ywis797</name>
      <uri>https://old.reddit.com/user/ywis797</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofom66/enable_gemma_2_2b_thinking_in_lm_studio/"&gt; &lt;img alt="Enable Gemma 2 2b thinking in LM studio" src="https://b.thumbs.redditmedia.com/GBkySLTnFhFeNkWs6BV_lAq6UcS37pvw4uvtyP_d6Tc.jpg" title="Enable Gemma 2 2b thinking in LM studio" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi All 28cm and E cups, &lt;/p&gt; &lt;p&gt;I was trying to break Gemma 2. I happened to enable Gemma 2 think, the response was blank. I am not sure if it's because I use Qwen3-4b to think first then switch to Gemmma. I think the system prompts play little part. &lt;/p&gt; &lt;p&gt;Any one knows how to recreate such without fail?&lt;/p&gt; &lt;p&gt;I use LM studio 0.3.31. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ywis797"&gt; /u/ywis797 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ofom66"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofom66/enable_gemma_2_2b_thinking_in_lm_studio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ofom66/enable_gemma_2_2b_thinking_in_lm_studio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-25T10:54:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1of4ypq</id>
    <title>Benchmarking the DGX Spark against the RTX 3090</title>
    <updated>2025-10-24T18:07:49+00:00</updated>
    <author>
      <name>/u/florinandrei</name>
      <uri>https://old.reddit.com/user/florinandrei</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ollama has benchmarked the DGX Spark for inference using some of the models in their own collection. They have also released the benchmark script for the test. They used Spark firmware 580.95.05 and Ollama v0.12.6.&lt;/p&gt; &lt;p&gt;&lt;a href="https://ollama.com/blog/nvidia-spark-performance"&gt;https://ollama.com/blog/nvidia-spark-performance&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I did a comparison of their numbers on the DGX Spark vs my own RTX 3090. This is how much faster the RTX 3090 is, compared to the DGX Spark, looking only at decode speed (tokens / sec), when using models that fit in a single 3090:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;gemma3 27B q4_K_M: 3.71x gpt-oss 20B MXFP4: 2.52x qwen3 32B q4_K_M: 3.78x &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;EDIT: Bigger models, that don't fit in the VRAM of a single RTX 3090, running straight out of the benchmark script with no changes whatsoever:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;gpt-oss 120B MXFP4: 0.235x llama3.1 70B q4_K_M: 0.428x &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;My system: Ubuntu 24.04, kernel 6.14.0-33-generic, NVIDIA driver 580.95.05, Ollama v0.12.6, 64 GB system RAM.&lt;/p&gt; &lt;p&gt;So the Spark is quite clearly a CUDA development machine. If you do inference and only inference with relatively small models, it's not the best bang for the buck - use something else instead.&lt;/p&gt; &lt;p&gt;Might still be worth it for pure inference with bigger models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/florinandrei"&gt; /u/florinandrei &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1of4ypq/benchmarking_the_dgx_spark_against_the_rtx_3090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1of4ypq/benchmarking_the_dgx_spark_against_the_rtx_3090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1of4ypq/benchmarking_the_dgx_spark_against_the_rtx_3090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-24T18:07:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ofp0ck</id>
    <title>Tired of tweaking your resume for every job description? I made a project that will do that and much more</title>
    <updated>2025-10-25T11:17:35+00:00</updated>
    <author>
      <name>/u/Sick__sock</name>
      <uri>https://old.reddit.com/user/Sick__sock</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofp0ck/tired_of_tweaking_your_resume_for_every_job/"&gt; &lt;img alt="Tired of tweaking your resume for every job description? I made a project that will do that and much more" src="https://external-preview.redd.it/bTZ4MnZlbHZyOHhmMYjCPdWOhWOC0mMCMfhtU8UInvZvl5_PVa6FYnX-eAIM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bb7fcdaf4a52fc9de33b8ee47704a6b9e64fae7c" title="Tired of tweaking your resume for every job description? I made a project that will do that and much more" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sick__sock"&gt; /u/Sick__sock &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/5eamalkvr8xf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofp0ck/tired_of_tweaking_your_resume_for_every_job/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ofp0ck/tired_of_tweaking_your_resume_for_every_job/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-25T11:17:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1ofpcje</id>
    <title>Unable to setup Cline in VScode with LM studio. Cant set context window.</title>
    <updated>2025-10-25T11:37:37+00:00</updated>
    <author>
      <name>/u/Pack_Commercial</name>
      <uri>https://old.reddit.com/user/Pack_Commercial</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofpcje/unable_to_setup_cline_in_vscode_with_lm_studio/"&gt; &lt;img alt="Unable to setup Cline in VScode with LM studio. Cant set context window." src="https://preview.redd.it/sgtruy7iv8xf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8e8e670e8943850816cfa1042f926ce0fea083ac" title="Unable to setup Cline in VScode with LM studio. Cant set context window." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Would anyone with some Cline setup experience help me 🙂&lt;/p&gt; &lt;p&gt;I just installed and setting up cline extension in VScode with my local llm on LM studio. But after installing I started the below steps.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;When I clicked LM studio provider it did not show list of models, So I did manually typed the model ID (seen on left from LM studio)&lt;/li&gt; &lt;li&gt;Next I was unable to set Context window length. It has a hard value 0, I can't modify.&lt;/li&gt; &lt;li&gt;Then I proceeded in chat asking simple question, and checking bg status on LM studio, Nothing happened even there.. &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Did I miss anything ? PS: I skipped signin process, everything is on my Win11 machine. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pack_Commercial"&gt; /u/Pack_Commercial &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sgtruy7iv8xf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofpcje/unable_to_setup_cline_in_vscode_with_lm_studio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ofpcje/unable_to_setup_cline_in_vscode_with_lm_studio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-25T11:37:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1ofprkv</id>
    <title>Any local model that can rival gemini 2.5 flash?</title>
    <updated>2025-10-25T12:00:58+00:00</updated>
    <author>
      <name>/u/AldebaranReborn</name>
      <uri>https://old.reddit.com/user/AldebaranReborn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been using gemini-cli a lot these days. I'm no programmer nor do i like to program. I only do it because i want to save time by automating some things with scripts. And using gemini-cli with the flash model has been enough for my meager needs. &lt;/p&gt; &lt;p&gt;But i wonder if there's any local models that can compete with it? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AldebaranReborn"&gt; /u/AldebaranReborn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofprkv/any_local_model_that_can_rival_gemini_25_flash/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofprkv/any_local_model_that_can_rival_gemini_25_flash/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ofprkv/any_local_model_that_can_rival_gemini_25_flash/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-25T12:00:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1of0tnr</id>
    <title>Built a fully local, on-device AI Scribe for clinicians — finally real, finally private</title>
    <updated>2025-10-24T15:29:12+00:00</updated>
    <author>
      <name>/u/MajesticAd2862</name>
      <uri>https://old.reddit.com/user/MajesticAd2862</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1of0tnr/built_a_fully_local_ondevice_ai_scribe_for/"&gt; &lt;img alt="Built a fully local, on-device AI Scribe for clinicians — finally real, finally private" src="https://external-preview.redd.it/dTBucnhtbHB1MnhmMbBjCm2a85KdqkMjM1vyg4FaNP4KyPH0k1X5BnGsr-w4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=89a1120225a393e05b05bcfcec0fb4230cbed364" title="Built a fully local, on-device AI Scribe for clinicians — finally real, finally private" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;After two years of tinkering nights and weekends, I finally built what I had in mind: a &lt;strong&gt;fully local, on-device AI scribe&lt;/strong&gt; for clinicians.&lt;/p&gt; &lt;p&gt;👉 Records, transcribes, and generates structured notes — &lt;strong&gt;all running locally on your Mac&lt;/strong&gt;, no cloud, no API calls, no data leaving your device.&lt;/p&gt; &lt;p&gt;The system uses a small foundation model + LoRA adapter that we’ve optimized for clinical language. And the best part: it &lt;strong&gt;anchors every sentence of the note to the original transcript&lt;/strong&gt; — so you can hover over any finding and see exactly &lt;em&gt;where&lt;/em&gt; in the conversation it came from. We call this &lt;strong&gt;Evidence Anchoring&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;It’s been wild seeing it outperform GPT-5 on hallucination tests — about 3× fewer unsupported claims — simply because everything it writes must tie back to actual evidence in the transcript.&lt;/p&gt; &lt;p&gt;If you’re on macOS (M1/M2/M3) and want to try it, we’ve opened a &lt;strong&gt;beta&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;You can sign up at &lt;a href="https://omiscribe.com/"&gt;omiscribe.com&lt;/a&gt; or DM me for a TestFlight invite.&lt;/p&gt; &lt;p&gt;LocalLLama and the local-AI community honestly kept me believing this was possible. 🙏 Would love to hear what you think — especially from anyone doing clinical documentation, med-AI, or just interested in local inference on Apple hardware.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MajesticAd2862"&gt; /u/MajesticAd2862 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/fhq9jnlpu2xf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1of0tnr/built_a_fully_local_ondevice_ai_scribe_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1of0tnr/built_a_fully_local_ondevice_ai_scribe_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-24T15:29:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1of0xc1</id>
    <title>GLM 4.6 coding Benchmarks</title>
    <updated>2025-10-24T15:33:10+00:00</updated>
    <author>
      <name>/u/IndependentFresh628</name>
      <uri>https://old.reddit.com/user/IndependentFresh628</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Did they fake Coding benchmarks where it is visible GLM 4.6 is neck to neck with Claude Sonnet 4.5 however, in real world Use it is not even close to Sonnet when it comes Debug or Efficient problem solving.&lt;/p&gt; &lt;p&gt;But yeah, GLM can generate massive amount of Coding tokens in one prompt.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IndependentFresh628"&gt; /u/IndependentFresh628 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1of0xc1/glm_46_coding_benchmarks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1of0xc1/glm_46_coding_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1of0xc1/glm_46_coding_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-24T15:33:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1oeyzxq</id>
    <title>[🪨 Onyx v2.0.0] Self-hosted chat and RAG - now with FOSS repo, SSO, new design/colors, and projects!</title>
    <updated>2025-10-24T14:19:13+00:00</updated>
    <author>
      <name>/u/Weves11</name>
      <uri>https://old.reddit.com/user/Weves11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oeyzxq/onyx_v200_selfhosted_chat_and_rag_now_with_foss/"&gt; &lt;img alt="[🪨 Onyx v2.0.0] Self-hosted chat and RAG - now with FOSS repo, SSO, new design/colors, and projects!" src="https://b.thumbs.redditmedia.com/1g7bw_MAWvmmqQ_XM_zZ3Opapd4T9MrJQmyySlO22qg.jpg" title="[🪨 Onyx v2.0.0] Self-hosted chat and RAG - now with FOSS repo, SSO, new design/colors, and projects!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey friends, I’ve got a big Onyx update for you guys! &lt;/p&gt; &lt;p&gt;I heard your feedback loud and clear last time - and thanks to the great suggestions I’ve 1/ released a fully FOSS, MIT-licensed version of Onyx, 2/ open-sourced OIDC/SAML, and 3/ did a complete makeover of the design and colors. &lt;/p&gt; &lt;p&gt;If you don’t know - Onyx is an open-source, self-hostable chat UI that has support for every LLM plus built in RAG + connectors + MCP + web search + deep research.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Everything that’s new:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Open-sourced SSO (OIDC + SAML) &lt;/li&gt; &lt;li&gt;onyx-foss (&lt;a href="https://github.com/onyx-dot-app/onyx-foss"&gt;https://github.com/onyx-dot-app/onyx-foss&lt;/a&gt;), a completely MIT licensed version of Onyx&lt;/li&gt; &lt;li&gt;Brand new design / colors&lt;/li&gt; &lt;li&gt;Projects (think Claude projects, but with any model + self-hosted)&lt;/li&gt; &lt;li&gt;Organization info and personalization&lt;/li&gt; &lt;li&gt;Reworked core tool-calling loop. Uses native tool calling for better adherence, fewer history rewrites for better prompt caching, and less hand-crafted prompts for fewer artifacts in longer runs&lt;/li&gt; &lt;li&gt;OAuth support for OpenAPI-based tools&lt;/li&gt; &lt;li&gt;A bunch of bug fixes&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Really appreciate all the feedback from last time, and looking forward to more of it here. Onyx was briefly #1 python and #2 github trending repo of the day, which is so crazy to me.&lt;/p&gt; &lt;p&gt;If there’s anything else that you would find useful that’s NOT part of the MIT license please let me know and I’ll do my best to move it over. All of the core functionality mentioned above is 100% FOSS. I want everything needed for the best open-source chat UI to be completely free and usable by all!&lt;/p&gt; &lt;p&gt;Repo:&lt;a href="https://github.com/onyx-dot-app/onyx"&gt; https://github.com/onyx-dot-app/onyx&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Full release notes:&lt;a href="https://docs.onyx.app/changelog#v2-0-0"&gt; https://docs.onyx.app/changelog#v2-0-0&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Weves11"&gt; /u/Weves11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1oeyzxq"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oeyzxq/onyx_v200_selfhosted_chat_and_rag_now_with_foss/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oeyzxq/onyx_v200_selfhosted_chat_and_rag_now_with_foss/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-24T14:19:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1of8f1i</id>
    <title>First attempt at building a local LLM setup in my mini rack</title>
    <updated>2025-10-24T20:23:54+00:00</updated>
    <author>
      <name>/u/Von_plaf</name>
      <uri>https://old.reddit.com/user/Von_plaf</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1of8f1i/first_attempt_at_building_a_local_llm_setup_in_my/"&gt; &lt;img alt="First attempt at building a local LLM setup in my mini rack" src="https://preview.redd.it/aseoosei84xf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=da5c51332fc5fb2400148e14f00664703a266818" title="First attempt at building a local LLM setup in my mini rack" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I finally got around to attempting to build a local LLM setup.&lt;br /&gt; Got my hands on 3 x Nvidia Jetson Orin nano's and put them into my mini rack and started to see if I could make them into a cluster.&lt;br /&gt; Long story short ... &lt;strong&gt;YES and NOOooo..&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I got all 3 Jetsons running llama.cpp and got them working in a cluster using llama-server on the first Jetson and rpc-server on the two other.&lt;br /&gt; But using llama-bench they produced only about 7 tokens/sec. when working together, but just one Jetson working alone i got about 22 tokens/sec. &lt;/p&gt; &lt;p&gt;Model I was using was Llama-3.2-3B-Instruct-Q4_K_M.gguf I did try out other models but not with any real good results.&lt;br /&gt; But it all comes down to the fact that they LLM really like things fast and for them to having to share over a &amp;quot;slow&amp;quot; 1Gb ethernet connection between each other was one of the factors that slowed everything down. &lt;/p&gt; &lt;p&gt;So I wanted to try something else.&lt;br /&gt; I loaded up the same model all 3 Jetsons and started a llama-server on each node but on different ports.&lt;br /&gt; Then setting up a Raspberry pi 5 4GB with Nginx as a load balancer and having a docker container run open webUI I then got all 3 Jetsons with llama.cpp feeding into the same UI, I still only get about 20-22 tokens/sec pr node, but adding the same model 3 times in one chat then all 3 nodes starts working on the prompt at the same time, then I can either merge the result or have 3 separate results.&lt;br /&gt; So all in all as for a first real try, not great but also not bad and just happy I got it running.&lt;/p&gt; &lt;p&gt;Now I think I will be looking into getting a larger model running to maximize the use of the jetsons.&lt;br /&gt; Still a lot to learn..&lt;/p&gt; &lt;p&gt;&lt;em&gt;The bottom part of the rack has the 3 x Nvidia Jetson Orin nano's and the Raspberry pi 5 for load balancing and running the webUI.&lt;/em&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Von_plaf"&gt; /u/Von_plaf &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/aseoosei84xf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1of8f1i/first_attempt_at_building_a_local_llm_setup_in_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1of8f1i/first_attempt_at_building_a_local_llm_setup_in_my/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-24T20:23:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1ofezsz</id>
    <title>If you could have one LLM distilled to a smaller size, which would model would you pick and what size(s) would you pick?</title>
    <updated>2025-10-25T01:23:12+00:00</updated>
    <author>
      <name>/u/silenceimpaired</name>
      <uri>https://old.reddit.com/user/silenceimpaired</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Really the question is… what larger open weight model do you wish you could run on your hardware with some reduced capacity: something large enough where quantization isn’t an option.&lt;/p&gt; &lt;p&gt;This is a tough choice for me, as I’ve wanted to have a true distillation of Deepseek for the longest time, but I think Kimi-K2 has changed my mind. &lt;/p&gt; &lt;p&gt;I would love to have Kimi-K2 distilled to a 70b dense model… a more likely size someone might attempt would be 106 billion total parameters and 12 billion active parameters, the same size as GLM 4.5 Air… though maybe I would even go so large as GLM-4.5 which has 355 billion total parameters with 32 billion active parameters.&lt;/p&gt; &lt;p&gt;I completely forgot about the larger Qwen model! That would be great as well.&lt;/p&gt; &lt;p&gt;How about you? What model would you pick and at what size?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/silenceimpaired"&gt; /u/silenceimpaired &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofezsz/if_you_could_have_one_llm_distilled_to_a_smaller/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofezsz/if_you_could_have_one_llm_distilled_to_a_smaller/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ofezsz/if_you_could_have_one_llm_distilled_to_a_smaller/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-25T01:23:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ofh2ia</id>
    <title>Woke up whole night and still couldn't resolve this one issue</title>
    <updated>2025-10-25T03:10:46+00:00</updated>
    <author>
      <name>/u/thenew_Alex_Bawden</name>
      <uri>https://old.reddit.com/user/thenew_Alex_Bawden</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofh2ia/woke_up_whole_night_and_still_couldnt_resolve/"&gt; &lt;img alt="Woke up whole night and still couldn't resolve this one issue" src="https://preview.redd.it/1v6zgu92d6xf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d0c90f63896b1c5ead31fa8fc2b09e3e0261e75a" title="Woke up whole night and still couldn't resolve this one issue" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Google Collab link :- &lt;a href="https://colab.research.google.com/drive/1gutbsKAiS46PsSoqPG51fHt8VNRrUNB3?usp=sharing#scrollTo=xIPudkKcQeyD"&gt;https://colab.research.google.com/drive/1gutbsKAiS46PsSoqPG51fHt8VNRrUNB3?usp=sharing#scrollTo=xIPudkKcQeyD&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I was fine tuning gpt oss 20B using unsloth on Google Colab and this error kept coming...&lt;/p&gt; &lt;p&gt;I feel i changed my dataset structure many times and still wasnot about to proceed.....&lt;/p&gt; &lt;p&gt;Also i think it is something to which harmony 1&lt;/p&gt; &lt;p&gt;Like do i need build a good json file but everything failed or the error is something else &lt;/p&gt; &lt;p&gt;Please please help me&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thenew_Alex_Bawden"&gt; /u/thenew_Alex_Bawden &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1v6zgu92d6xf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofh2ia/woke_up_whole_night_and_still_couldnt_resolve/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ofh2ia/woke_up_whole_night_and_still_couldnt_resolve/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-25T03:10:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1oes4ez</id>
    <title>Qwen3 Next support in llama.cpp ready for review</title>
    <updated>2025-10-24T08:18:49+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oes4ez/qwen3_next_support_in_llamacpp_ready_for_review/"&gt; &lt;img alt="Qwen3 Next support in llama.cpp ready for review" src="https://external-preview.redd.it/JWuwM-H5pHYaaKPNtY_8U3LHlrsSjJTNAjLHRGwU5o0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=016d876487cc90150078e6b226c52b29735d5532" title="Qwen3 Next support in llama.cpp ready for review" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Congratulations to Piotr for his hard work, the code is now ready for review.&lt;/p&gt; &lt;p&gt;Please note that this is not the final version, and if you download some quantized models, you will probably need to download them again later. Also, it's not yet optimized for speed.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/16095"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oes4ez/qwen3_next_support_in_llamacpp_ready_for_review/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oes4ez/qwen3_next_support_in_llamacpp_ready_for_review/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-24T08:18:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1oeuiev</id>
    <title>Is OpenAI afraid of Kimi?</title>
    <updated>2025-10-24T10:50:59+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oeuiev/is_openai_afraid_of_kimi/"&gt; &lt;img alt="Is OpenAI afraid of Kimi?" src="https://b.thumbs.redditmedia.com/apnbgQLHwz7nx79BJoKIRI6eUEamaMguXqyI2K8LM8M.jpg" title="Is OpenAI afraid of Kimi?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;roon from OpenAI posted this earlier&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5hqotg83i1xf1.jpg?width=1190&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=f1396023a25350b27a94a3e4225bf38eb4ae86c3"&gt;https://preview.redd.it/5hqotg83i1xf1.jpg?width=1190&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=f1396023a25350b27a94a3e4225bf38eb4ae86c3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Then he instantly deleted the tweet&lt;/strong&gt; lol&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oeuiev/is_openai_afraid_of_kimi/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oeuiev/is_openai_afraid_of_kimi/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oeuiev/is_openai_afraid_of_kimi/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-24T10:50:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1ofboir</id>
    <title>Strix Halo + RTX 3090 Achieved! Interesting Results...</title>
    <updated>2025-10-24T22:42:49+00:00</updated>
    <author>
      <name>/u/JayTheProdigy16</name>
      <uri>https://old.reddit.com/user/JayTheProdigy16</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofboir/strix_halo_rtx_3090_achieved_interesting_results/"&gt; &lt;img alt="Strix Halo + RTX 3090 Achieved! Interesting Results..." src="https://b.thumbs.redditmedia.com/b6nRUaAihZDoN0MlWEmQXboPVf1abHo_e9s727G4WNU.jpg" title="Strix Halo + RTX 3090 Achieved! Interesting Results..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Specs: Fedora 43 Server (bare metal, tried via Proxmox but to reduce complexity went BM, will try again), Bosgame M5 128gb AI Max+ 395 (identical board to GMKtek EVO-X2), EVGA FTW3 3090, MinisForum DEG1 eGPU dock with generic m.2 to Oculink adapter + 850w PSU.&lt;/p&gt; &lt;p&gt;Compiled the latest version of llama.cpp with Vulkan RADV (NO CUDA), things are still very wonky but it does work. I was able to get GPT OSS 120b to run on llama-bench but running into weird OOM and VlkDeviceLost errors specifically in llama-bench when trying GLM 4.5 Air even though the rig has served all models perfectly fine thus far. KV cache quant also seems to be bugged out and throws context errors with llama-bench but again works fine with llama-server. Tried the strix-halo-toolbox build of llama.cpp but could never get memory allocation to function properly with the 3090.&lt;/p&gt; &lt;p&gt;Saw a ~30% increase in PP at 12k context no quant going from 312 TPS on Strix Halo only to 413 TPS with SH + 3090, but a ~20% decrease in TG from 50 TPS on SH only to 40 on SH + 3090 which i thought was pretty interesting, and a part of me wonders if that was an anomaly or not but will confirm at a later date with more data.&lt;/p&gt; &lt;p&gt;Going to do more testing with it but after banging my head into a wall for 4 days to get it serving properly im taking a break and enjoying my vette. Let me know if yall have any ideas or benchmarks yall might be interested in&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ly9ey0wr05xf1.jpg?width=3060&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=cc073c67f6d2bd5f976f53679d8de83215fb4697"&gt;https://preview.redd.it/ly9ey0wr05xf1.jpg?width=3060&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=cc073c67f6d2bd5f976f53679d8de83215fb4697&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/gv0terms05xf1.jpg?width=3060&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=8ccc70fb59e9e7a0771274e15e67bc18a36ac624"&gt;https://preview.redd.it/gv0terms05xf1.jpg?width=3060&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=8ccc70fb59e9e7a0771274e15e67bc18a36ac624&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/0ohsyz23z4xf1.png?width=1654&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9e0d122713096d181026c3a160b381e2db1333c6"&gt;https://preview.redd.it/0ohsyz23z4xf1.png?width=1654&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9e0d122713096d181026c3a160b381e2db1333c6&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JayTheProdigy16"&gt; /u/JayTheProdigy16 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofboir/strix_halo_rtx_3090_achieved_interesting_results/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofboir/strix_halo_rtx_3090_achieved_interesting_results/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ofboir/strix_halo_rtx_3090_achieved_interesting_results/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-24T22:42:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1of7gcb</id>
    <title>MiniMax-M2 Info (from OpenRouter discord)</title>
    <updated>2025-10-24T19:45:21+00:00</updated>
    <author>
      <name>/u/nuclearbananana</name>
      <uri>https://old.reddit.com/user/nuclearbananana</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1of7gcb/minimaxm2_info_from_openrouter_discord/"&gt; &lt;img alt="MiniMax-M2 Info (from OpenRouter discord)" src="https://b.thumbs.redditmedia.com/xndOTEovtiDsZXO2jNXpyX138_t8q84HFR2H6Bffzts.jpg" title="MiniMax-M2 Info (from OpenRouter discord)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/6s6d9ykc54xf1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=51b274177a62bfec585d7f06c2fe7649bc9aa5c9"&gt;https://preview.redd.it/6s6d9ykc54xf1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=51b274177a62bfec585d7f06c2fe7649bc9aa5c9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;MiniMax M2 — A Gift for All Developers on the 1024 Festival&amp;quot;&lt;/p&gt; &lt;p&gt;Top 5 globally, surpassing Claude Opus 4.1 and second only to Sonnet 4.5; state-of-the-art among open-source models. Reengineered for coding and agentic use—open-source SOTA, highly intelligent, with low latency and cost. We believe it's one of the best choices for agent products and the most suitable open-source alternative to Claude Code.&lt;/p&gt; &lt;p&gt;We are very proud to have participated in the model’s development; this is our gift to all developers.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;MiniMax-M2 is coming on Oct 27&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6s6d9ykc54xf1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=51b274177a62bfec585d7f06c2fe7649bc9aa5c9"&gt;https://preview.redd.it/6s6d9ykc54xf1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=51b274177a62bfec585d7f06c2fe7649bc9aa5c9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xfjadh5e54xf1.png?width=1380&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=21b792491113f10c43e492b252bc87176a3f7f53"&gt;https://preview.redd.it/xfjadh5e54xf1.png?width=1380&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=21b792491113f10c43e492b252bc87176a3f7f53&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/gb7mat4f54xf1.png?width=636&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=59eed9df7b9c485ce21b5cb41d9707e72ba5d39b"&gt;https://preview.redd.it/gb7mat4f54xf1.png?width=636&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=59eed9df7b9c485ce21b5cb41d9707e72ba5d39b&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nuclearbananana"&gt; /u/nuclearbananana &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1of7gcb/minimaxm2_info_from_openrouter_discord/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1of7gcb/minimaxm2_info_from_openrouter_discord/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1of7gcb/minimaxm2_info_from_openrouter_discord/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-24T19:45:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1ofb7mu</id>
    <title>4B fp16 or 8B q4?</title>
    <updated>2025-10-24T22:21:45+00:00</updated>
    <author>
      <name>/u/Ok-Internal9317</name>
      <uri>https://old.reddit.com/user/Ok-Internal9317</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofb7mu/4b_fp16_or_8b_q4/"&gt; &lt;img alt="4B fp16 or 8B q4?" src="https://preview.redd.it/ukn1akunw4xf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dd900eb86aa1c749649781d6ebd92067020e1209" title="4B fp16 or 8B q4?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys, &lt;/p&gt; &lt;p&gt;For my 8GB GPU schould I go for &lt;strong&gt;fp16 but 4B&lt;/strong&gt; or &lt;strong&gt;q4 version of 8B&lt;/strong&gt;? Any model you particularly want to recommend me? Requirement: basic ChatGPT replacement&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Internal9317"&gt; /u/Ok-Internal9317 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ukn1akunw4xf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofb7mu/4b_fp16_or_8b_q4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ofb7mu/4b_fp16_or_8b_q4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-24T22:21:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1of769j</id>
    <title>You can turn off the cloud, this + solar panel will suffice:</title>
    <updated>2025-10-24T19:34:13+00:00</updated>
    <author>
      <name>/u/JLeonsarmiento</name>
      <uri>https://old.reddit.com/user/JLeonsarmiento</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1of769j/you_can_turn_off_the_cloud_this_solar_panel_will/"&gt; &lt;img alt="You can turn off the cloud, this + solar panel will suffice:" src="https://preview.redd.it/a0svyfed34xf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f50f6ea7d9b1152c45428baa79fd791529dae389" title="You can turn off the cloud, this + solar panel will suffice:" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JLeonsarmiento"&gt; /u/JLeonsarmiento &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/a0svyfed34xf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1of769j/you_can_turn_off_the_cloud_this_solar_panel_will/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1of769j/you_can_turn_off_the_cloud_this_solar_panel_will/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-24T19:34:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ofggny</id>
    <title>Kimi k2 image generation</title>
    <updated>2025-10-25T02:38:13+00:00</updated>
    <author>
      <name>/u/Used-Nectarine5541</name>
      <uri>https://old.reddit.com/user/Used-Nectarine5541</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofggny/kimi_k2_image_generation/"&gt; &lt;img alt="Kimi k2 image generation" src="https://preview.redd.it/vimkvz5976xf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=202c4cfa6cd438c6e2e103ec6ec094a5b596d49a" title="Kimi k2 image generation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am so confused because I can’t find any information on Kimi k2 image generation abilities. When I asked Kimi to generate an image it said it couldn’t. But I’m having it code a tarot reading project and it’s generating all these images…when I asked about it Kimi still said it couldn’t generate images. What’s going on and how are these images being generated??&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Used-Nectarine5541"&gt; /u/Used-Nectarine5541 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vimkvz5976xf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofggny/kimi_k2_image_generation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ofggny/kimi_k2_image_generation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-25T02:38:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1oextwc</id>
    <title>GLM-4.6-Air is not forgotten!</title>
    <updated>2025-10-24T13:31:12+00:00</updated>
    <author>
      <name>/u/codys12</name>
      <uri>https://old.reddit.com/user/codys12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oextwc/glm46air_is_not_forgotten/"&gt; &lt;img alt="GLM-4.6-Air is not forgotten!" src="https://preview.redd.it/z5dduynua2xf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b43f43a244e84de5bb07a0bc9e4c16127860c9a4" title="GLM-4.6-Air is not forgotten!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/codys12"&gt; /u/codys12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/z5dduynua2xf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oextwc/glm46air_is_not_forgotten/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oextwc/glm46air_is_not_forgotten/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-24T13:31:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1of8xl2</id>
    <title>Apple Foundation is dumb</title>
    <updated>2025-10-24T20:45:00+00:00</updated>
    <author>
      <name>/u/PM_ME_UR_COFFEE_CUPS</name>
      <uri>https://old.reddit.com/user/PM_ME_UR_COFFEE_CUPS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1of8xl2/apple_foundation_is_dumb/"&gt; &lt;img alt="Apple Foundation is dumb" src="https://b.thumbs.redditmedia.com/YZ_noIfhIcr5Qu2dEET4ho0GRm1U2R0wUE1Mn8YjpWQ.jpg" title="Apple Foundation is dumb" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Like the other poster, I’ve found Apple Foundational model to disapprove of lots of content. It’s too safe. Too corporate.&lt;/p&gt; &lt;p&gt;This is the most innocuous example I could come up with. Also attached proof that it even indirectly avoids the word. Google’s model gives me accurate info. &lt;/p&gt; &lt;p&gt;(FYI in case you are not in a region that has chiggers… they are little red bugs that bite you, no relation to a word that it rhymes with at all)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PM_ME_UR_COFFEE_CUPS"&gt; /u/PM_ME_UR_COFFEE_CUPS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1of8xl2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1of8xl2/apple_foundation_is_dumb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1of8xl2/apple_foundation_is_dumb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-24T20:45:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1ofasus</id>
    <title>MiniMax M2 is 230B-A10B</title>
    <updated>2025-10-24T22:03:31+00:00</updated>
    <author>
      <name>/u/codys12</name>
      <uri>https://old.reddit.com/user/codys12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofasus/minimax_m2_is_230ba10b/"&gt; &lt;img alt="MiniMax M2 is 230B-A10B" src="https://preview.redd.it/f45v1dx7u4xf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9c77e931b8bddbffa454a9542abfb528e2be6fc4" title="MiniMax M2 is 230B-A10B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/codys12"&gt; /u/codys12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/f45v1dx7u4xf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofasus/minimax_m2_is_230ba10b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ofasus/minimax_m2_is_230ba10b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-24T22:03:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1ofn0nb</id>
    <title>meituan-longcat/LongCat-Video · Hugging Face</title>
    <updated>2025-10-25T09:11:57+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofn0nb/meituanlongcatlongcatvideo_hugging_face/"&gt; &lt;img alt="meituan-longcat/LongCat-Video · Hugging Face" src="https://external-preview.redd.it/Yt-ii7zJ14OIVDQLAiu4mswoqHj6Da2dRjpAXgOm1a4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a68e500343ff77272afd8ad706e2ac971ab2d083" title="meituan-longcat/LongCat-Video · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A foundational video generation model with 13.6B parameters, delivering strong performance across Text-to-Video, Image-to-Video, and Video-Continuation generation tasks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/meituan-longcat/LongCat-Video"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofn0nb/meituanlongcatlongcatvideo_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ofn0nb/meituanlongcatlongcatvideo_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-25T09:11:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1of5ywl</id>
    <title>What’s even the goddamn point?</title>
    <updated>2025-10-24T18:47:20+00:00</updated>
    <author>
      <name>/u/ChockyBlox</name>
      <uri>https://old.reddit.com/user/ChockyBlox</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1of5ywl/whats_even_the_goddamn_point/"&gt; &lt;img alt="What’s even the goddamn point?" src="https://preview.redd.it/9fjtexb9v3xf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0d398d419e5a3d539c3f2c82b07408ef22f90899" title="What’s even the goddamn point?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;To be fair I will probably never use this model for any real use cases, but these corporations do need to go a little easy on the restrictions and be less paranoid.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ChockyBlox"&gt; /u/ChockyBlox &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9fjtexb9v3xf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1of5ywl/whats_even_the_goddamn_point/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1of5ywl/whats_even_the_goddamn_point/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-24T18:47:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1obqkpe</id>
    <title>Best Local LLMs - October 2025</title>
    <updated>2025-10-20T19:06:06+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Welcome to the first monthly &amp;quot;Best Local LLMs&amp;quot; post!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Share what your favorite models are right now &lt;strong&gt;and why.&lt;/strong&gt; Given the nature of the beast in evaluating LLMs (untrustworthiness of benchmarks, immature tooling, intrinsic stochasticity), please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Applications&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;General&lt;/li&gt; &lt;li&gt;Agentic/Tool Use&lt;/li&gt; &lt;li&gt;Coding&lt;/li&gt; &lt;li&gt;Creative Writing/RP&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;(look for the top level comments for each Application and please thread your responses under that)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obqkpe/best_local_llms_october_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obqkpe/best_local_llms_october_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1obqkpe/best_local_llms_october_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T19:06:06+00:00</published>
  </entry>
</feed>
