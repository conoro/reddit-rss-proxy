<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-17T11:06:17+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1o81rvs</id>
    <title>Google C2S-Scale 27B (based on Gemma) built with Yale generated a novel hypothesis about cancer cellular behavior - Model + resources are now on Hugging Face and GitHub</title>
    <updated>2025-10-16T09:41:04+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o81rvs/google_c2sscale_27b_based_on_gemma_built_with/"&gt; &lt;img alt="Google C2S-Scale 27B (based on Gemma) built with Yale generated a novel hypothesis about cancer cellular behavior - Model + resources are now on Hugging Face and GitHub" src="https://b.thumbs.redditmedia.com/h0BE1gNO8S-6xv6b1X5IIoHb8CSHWZoS7YxS0LFbuxA.jpg" title="Google C2S-Scale 27B (based on Gemma) built with Yale generated a novel hypothesis about cancer cellular behavior - Model + resources are now on Hugging Face and GitHub" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Blog post: How a Gemma model helped discover a new potential cancer therapy pathway - We‚Äôre launching a new 27 billion parameter foundation model for single-cell analysis built on the Gemma family of open models.: &lt;a href="https://blog.google/technology/ai/google-gemma-ai-cancer-therapy-discovery/"&gt;https://blog.google/technology/ai/google-gemma-ai-cancer-therapy-discovery/&lt;/a&gt;&lt;br /&gt; Hugging Face: &lt;a href="https://huggingface.co/vandijklab/C2S-Scale-Gemma-2-27B"&gt;https://huggingface.co/vandijklab/C2S-Scale-Gemma-2-27B&lt;/a&gt;&lt;br /&gt; Scientific preprint on bioRxiv: &lt;a href="https://www.biorxiv.org/content/10.1101/2025.04.14.648850v2"&gt;https://www.biorxiv.org/content/10.1101/2025.04.14.648850v2&lt;/a&gt;&lt;br /&gt; Code on GitHub: &lt;a href="https://github.com/vandijklab/cell2sentence"&gt;https://github.com/vandijklab/cell2sentence&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1o81rvs"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o81rvs/google_c2sscale_27b_based_on_gemma_built_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o81rvs/google_c2sscale_27b_based_on_gemma_built_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T09:41:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8lcpc</id>
    <title>New NVIDIA Project G-Assist Plug-in Hackathon - Win a GeForce RTX 5090</title>
    <updated>2025-10-16T23:13:34+00:00</updated>
    <author>
      <name>/u/NV_Cory</name>
      <uri>https://old.reddit.com/user/NV_Cory</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, hope you don't mind if I share a project we're working on at NVIDIA. &lt;/p&gt; &lt;p&gt;We recently launched a new plug-in hackathon contest around Project G-Assist, with a theme for ‚Äúhome control.‚Äù Think smart lights, adjusting thermostat temperature, managing devices &amp;amp; more. &lt;/p&gt; &lt;p&gt;Project G-Assist is an experimental AI assistant for GeForce RTX-powered PCs that lets you call a variety of NVIDIA and third-party PC APIs to execute actions. It uses a specially tuned Small Language Model (SLM) to efficiently interpret natural language instructions, and users can make plugins (in C++ or Python) to add new features.&lt;/p&gt; &lt;p&gt;The top 3 entries will win RTX 50 Series GPUs, including a GeForce RTX 5090. Full details are &lt;a href="https://developer.nvidia.com/g-assist-hackathon"&gt;here&lt;/a&gt;. &lt;/p&gt; &lt;p&gt;This is the second hackathon we've run for G-Assist, and the winners in the first event were pretty impressive. Our first-place &lt;a href="https://github.com/briancaffey/flux-plugin-for-g-assist"&gt;winner&lt;/a&gt; last time enabled real-time image generation with voice commands through FLUX.1 running locally. I'd love to see what LocalLLaMA can do.&lt;/p&gt; &lt;p&gt;Let us know what you think, and I'm happy to answer any questions. Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NV_Cory"&gt; /u/NV_Cory &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8lcpc/new_nvidia_project_gassist_plugin_hackathon_win_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8lcpc/new_nvidia_project_gassist_plugin_hackathon_win_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8lcpc/new_nvidia_project_gassist_plugin_hackathon_win_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T23:13:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8x4ta</id>
    <title>just added Qwen3-VL support for MNN Chat android</title>
    <updated>2025-10-17T09:55:15+00:00</updated>
    <author>
      <name>/u/Juude89</name>
      <uri>https://old.reddit.com/user/Juude89</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8x4ta/just_added_qwen3vl_support_for_mnn_chat_android/"&gt; &lt;img alt="just added Qwen3-VL support for MNN Chat android" src="https://external-preview.redd.it/zNY5eQnZ-x8ORX22UUI4aGsd0-StGUHm-Z-wi5X4Vb4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9d5a7f8b045bac80ec011d43c579c53bf810f8f7" title="just added Qwen3-VL support for MNN Chat android" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1o8x4ta/video/juu7ycgm9nvf1/player"&gt;https://reddit.com/link/1o8x4ta/video/juu7ycgm9nvf1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also support qwen3-vl-4b and qwen3-vl-8b &lt;/p&gt; &lt;p&gt;Download the 0.7.5version to experience: &lt;a href="https://github.com/alibaba/MNN/blob/master/apps/Android/MnnLlmChat/README.md#version-075"&gt;https://github.com/alibaba/MNN/blob/master/apps/Android/MnnLlmChat/README.md#version-075&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Juude89"&gt; /u/Juude89 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8x4ta/just_added_qwen3vl_support_for_mnn_chat_android/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8x4ta/just_added_qwen3vl_support_for_mnn_chat_android/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8x4ta/just_added_qwen3vl_support_for_mnn_chat_android/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T09:55:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8st45</id>
    <title>Built Overtab: An On-device AI browsing assistant powered by Gemini Nano (no cloud, no data sent out)!</title>
    <updated>2025-10-17T05:20:11+00:00</updated>
    <author>
      <name>/u/Consistent_One7493</name>
      <uri>https://old.reddit.com/user/Consistent_One7493</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone üëã&lt;/p&gt; &lt;p&gt;I‚Äôve been obsessed with making browsing smarter, so I built what I wished existed: &lt;strong&gt;Overtab&lt;/strong&gt;, an on-device AI Chrome assistant I created for the &lt;strong&gt;Google Chrome Built-in AI Challenge 2025&lt;/strong&gt; that gives instant insights right in your browser.&lt;/p&gt; &lt;p&gt;Highlight text, ask by voice, or right-click images: all processed locally with &lt;strong&gt;Gemini Nano&lt;/strong&gt;!&lt;br /&gt; (And if you don‚Äôt have Nano set up yet, there‚Äôs an OpenAI fallback!)&lt;/p&gt; &lt;p&gt;üé¨ &lt;a href="https://www.youtube.com/watch?v=Wq5pnpnK9r0"&gt;Demo Video&lt;/a&gt; | üåê &lt;a href="https://chromewebstore.google.com/detail/overtab/oloejollcmhnbacdkfgbdlgcgbeegcje"&gt;Chrome Web Store&lt;/a&gt; | üíª &lt;a href="https://github.com/riyanshibohra/Overtab"&gt;GitHub&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Consistent_One7493"&gt; /u/Consistent_One7493 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8st45/built_overtab_an_ondevice_ai_browsing_assistant/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8st45/built_overtab_an_ondevice_ai_browsing_assistant/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8st45/built_overtab_an_ondevice_ai_browsing_assistant/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T05:20:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8d3gt</id>
    <title>Internship with local LLMs at AMD!</title>
    <updated>2025-10-16T17:50:36+00:00</updated>
    <author>
      <name>/u/dholanda_amd</name>
      <uri>https://old.reddit.com/user/dholanda_amd</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi folks! &lt;/p&gt; &lt;p&gt;My team and I at AMD have been having a lot of fun developing agents, building next-gen apps for local LLMs, fine-tuning models, and posting a lot of that here on &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;) . We‚Äôre now looking for a (ideally grad) student who loves hands-on local AI for an internship on our team.&lt;/p&gt; &lt;p&gt;Our team really tries to contribute quite a bit to the open source community. One of our key projects is &lt;a href="https://github.com/lemonade-sdk/lemonade"&gt;Lemonade &lt;/a&gt;(Ollama-like local app with a really cool Discord community).&lt;/p&gt; &lt;p&gt;Here is the rough description of what we envision for this position:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Develop an agentic LLM framework, designed to operate effectively on client devices&lt;/li&gt; &lt;li&gt;Build and refine the framework by developing a focused application (from computer use to database reasoning - your choice!)&lt;/li&gt; &lt;li&gt;Experiment with fine-tuning, LoRAs, RAG, and agent architectures&lt;/li&gt; &lt;li&gt;Work side-by-side with the Lemonade team =D&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Experience with some of the above (e.g., fine-tuning) is a huge bonus. We also love people who are active on open-source GitHub projects, Hugging Face, and of course &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; ;)&lt;/p&gt; &lt;p&gt;If you‚Äôre excited about this opportunity with local AI, let‚Äôs chat! Please apply using the link below. Please also feel free to ask questions here or DM me on Discord (look for Daniel H).&lt;/p&gt; &lt;p&gt;Excited to hear from this community!&lt;/p&gt; &lt;p&gt;Details here: careers (dot) amd (dot) com/careers-home/jobs/70208&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dholanda_amd"&gt; /u/dholanda_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8d3gt/internship_with_local_llms_at_amd/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8d3gt/internship_with_local_llms_at_amd/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8d3gt/internship_with_local_llms_at_amd/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T17:50:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8jocc</id>
    <title>Improving low VRAM performance for dense models using MoE offload technique</title>
    <updated>2025-10-16T22:02:16+00:00</updated>
    <author>
      <name>/u/eloquentemu</name>
      <uri>https://old.reddit.com/user/eloquentemu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;MoE partial offload, i.e. keeping experts on CPU and the context, attention, etc on GPU, has two benefits:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The non-sparse data is kept on fast VRAM&lt;/li&gt; &lt;li&gt;Everything needed to handle context computations is on GPU&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For dense models the first point is fairly irrelevant since, well, it's all dense so how you offload isn't really going to change bandwidth needs. However the second still applies and, MoE or not, compute for attention scales with context size but doesn't for the feed forward network (FFN). Thus, in theory, given the same VRAM we should be able to get much better scaling by offloading non-ffn tensors first to the GPU, rather than just whole layers.&lt;/p&gt; &lt;p&gt;There is no handy &lt;code&gt;--n-cpu-moe&lt;/code&gt; for this, but we can use the old &lt;code&gt;-ot exps=CPU&lt;/code&gt; tool to make it work. For MoE models the tensors look like &lt;code&gt;blk.2.ffn_down_exps.weight&lt;/code&gt; (note the &amp;quot;exps&amp;quot;) whereas a dense model has names like &lt;code&gt;blk.2.ffn_down.weight&lt;/code&gt; so here we just match all the FFN tensors and put them on CPU with &lt;code&gt;-ot ffn=CPU&lt;/code&gt;. &lt;code&gt;-ngl 99&lt;/code&gt; then offloads everything else:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;model&lt;/th&gt; &lt;th align="right"&gt;size&lt;/th&gt; &lt;th align="right"&gt;params&lt;/th&gt; &lt;th&gt;backend&lt;/th&gt; &lt;th align="right"&gt;ngl&lt;/th&gt; &lt;th align="right"&gt;fa&lt;/th&gt; &lt;th&gt;ot&lt;/th&gt; &lt;th align="right"&gt;context&lt;/th&gt; &lt;th align="right"&gt;test&lt;/th&gt; &lt;th align="right"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;llama 70B Q4_K_M&lt;/td&gt; &lt;td align="right"&gt;39.59 GiB&lt;/td&gt; &lt;td align="right"&gt;70.55 B&lt;/td&gt; &lt;td&gt;CUDA&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td&gt;ffn=CPU&lt;/td&gt; &lt;td align="right"&gt;0&lt;/td&gt; &lt;td align="right"&gt;pp512&lt;/td&gt; &lt;td align="right"&gt;273.22&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;llama 70B Q4_K_M&lt;/td&gt; &lt;td align="right"&gt;39.59 GiB&lt;/td&gt; &lt;td align="right"&gt;70.55 B&lt;/td&gt; &lt;td&gt;CUDA&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td&gt;ffn=CPU&lt;/td&gt; &lt;td align="right"&gt;4096&lt;/td&gt; &lt;td align="right"&gt;pp512&lt;/td&gt; &lt;td align="right"&gt;272.13&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;llama 70B Q4_K_M&lt;/td&gt; &lt;td align="right"&gt;39.59 GiB&lt;/td&gt; &lt;td align="right"&gt;70.55 B&lt;/td&gt; &lt;td&gt;CUDA&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td&gt;ffn=CPU&lt;/td&gt; &lt;td align="right"&gt;16384&lt;/td&gt; &lt;td align="right"&gt;pp512&lt;/td&gt; &lt;td align="right"&gt;253.86&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;llama 70B Q4_K_M&lt;/td&gt; &lt;td align="right"&gt;39.59 GiB&lt;/td&gt; &lt;td align="right"&gt;70.55 B&lt;/td&gt; &lt;td&gt;CUDA&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td&gt;ffn=CPU&lt;/td&gt; &lt;td align="right"&gt;65536&lt;/td&gt; &lt;td align="right"&gt;pp512&lt;/td&gt; &lt;td align="right"&gt;188.39&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;llama 70B Q4_K_M&lt;/td&gt; &lt;td align="right"&gt;39.59 GiB&lt;/td&gt; &lt;td align="right"&gt;70.55 B&lt;/td&gt; &lt;td&gt;CUDA&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td&gt;ffn=CPU&lt;/td&gt; &lt;td align="right"&gt;0&lt;/td&gt; &lt;td align="right"&gt;tg128&lt;/td&gt; &lt;td align="right"&gt;8.40&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;llama 70B Q4_K_M&lt;/td&gt; &lt;td align="right"&gt;39.59 GiB&lt;/td&gt; &lt;td align="right"&gt;70.55 B&lt;/td&gt; &lt;td&gt;CUDA&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td&gt;ffn=CPU&lt;/td&gt; &lt;td align="right"&gt;4096&lt;/td&gt; &lt;td align="right"&gt;tg128&lt;/td&gt; &lt;td align="right"&gt;7.99&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;llama 70B Q4_K_M&lt;/td&gt; &lt;td align="right"&gt;39.59 GiB&lt;/td&gt; &lt;td align="right"&gt;70.55 B&lt;/td&gt; &lt;td&gt;CUDA&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td&gt;ffn=CPU&lt;/td&gt; &lt;td align="right"&gt;16384&lt;/td&gt; &lt;td align="right"&gt;tg128&lt;/td&gt; &lt;td align="right"&gt;7.87&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;llama 70B Q4_K_M&lt;/td&gt; &lt;td align="right"&gt;39.59 GiB&lt;/td&gt; &lt;td align="right"&gt;70.55 B&lt;/td&gt; &lt;td&gt;CUDA&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td&gt;ffn=CPU&lt;/td&gt; &lt;td align="right"&gt;65536&lt;/td&gt; &lt;td align="right"&gt;tg128&lt;/td&gt; &lt;td align="right"&gt;7.17&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;llama 70B Q4_K_M&lt;/td&gt; &lt;td align="right"&gt;39.59 GiB&lt;/td&gt; &lt;td align="right"&gt;70.55 B&lt;/td&gt; &lt;td&gt;CUDA&lt;/td&gt; &lt;td align="right"&gt;21&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td&gt;N/A&lt;/td&gt; &lt;td align="right"&gt;0&lt;/td&gt; &lt;td align="right"&gt;pp512&lt;/td&gt; &lt;td align="right"&gt;291.84&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;llama 70B Q4_K_M&lt;/td&gt; &lt;td align="right"&gt;39.59 GiB&lt;/td&gt; &lt;td align="right"&gt;70.55 B&lt;/td&gt; &lt;td&gt;CUDA&lt;/td&gt; &lt;td align="right"&gt;21&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td&gt;N/A&lt;/td&gt; &lt;td align="right"&gt;4096&lt;/td&gt; &lt;td align="right"&gt;pp512&lt;/td&gt; &lt;td align="right"&gt;280.37&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;llama 70B Q4_K_M&lt;/td&gt; &lt;td align="right"&gt;39.59 GiB&lt;/td&gt; &lt;td align="right"&gt;70.55 B&lt;/td&gt; &lt;td&gt;CUDA&lt;/td&gt; &lt;td align="right"&gt;21&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td&gt;N/A&lt;/td&gt; &lt;td align="right"&gt;16384&lt;/td&gt; &lt;td align="right"&gt;pp512&lt;/td&gt; &lt;td align="right"&gt;246.97&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;llama 70B Q4_K_M&lt;/td&gt; &lt;td align="right"&gt;39.59 GiB&lt;/td&gt; &lt;td align="right"&gt;70.55 B&lt;/td&gt; &lt;td&gt;CUDA&lt;/td&gt; &lt;td align="right"&gt;21&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td&gt;N/A&lt;/td&gt; &lt;td align="right"&gt;65536&lt;/td&gt; &lt;td align="right"&gt;pp512&lt;/td&gt; &lt;td align="right"&gt;155.81&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;llama 70B Q4_K_M&lt;/td&gt; &lt;td align="right"&gt;39.59 GiB&lt;/td&gt; &lt;td align="right"&gt;70.55 B&lt;/td&gt; &lt;td&gt;CUDA&lt;/td&gt; &lt;td align="right"&gt;21&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td&gt;N/A&lt;/td&gt; &lt;td align="right"&gt;0&lt;/td&gt; &lt;td align="right"&gt;tg128&lt;/td&gt; &lt;td align="right"&gt;8.84&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;llama 70B Q4_K_M&lt;/td&gt; &lt;td align="right"&gt;39.59 GiB&lt;/td&gt; &lt;td align="right"&gt;70.55 B&lt;/td&gt; &lt;td&gt;CUDA&lt;/td&gt; &lt;td align="right"&gt;21&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td&gt;N/A&lt;/td&gt; &lt;td align="right"&gt;4096&lt;/td&gt; &lt;td align="right"&gt;tg128&lt;/td&gt; &lt;td align="right"&gt;5.22&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;llama 70B Q4_K_M&lt;/td&gt; &lt;td align="right"&gt;39.59 GiB&lt;/td&gt; &lt;td align="right"&gt;70.55 B&lt;/td&gt; &lt;td&gt;CUDA&lt;/td&gt; &lt;td align="right"&gt;21&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td&gt;N/A&lt;/td&gt; &lt;td align="right"&gt;16384&lt;/td&gt; &lt;td align="right"&gt;tg128&lt;/td&gt; &lt;td align="right"&gt;2.42&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;llama 70B Q4_K_M&lt;/td&gt; &lt;td align="right"&gt;39.59 GiB&lt;/td&gt; &lt;td align="right"&gt;70.55 B&lt;/td&gt; &lt;td&gt;CUDA&lt;/td&gt; &lt;td align="right"&gt;21&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td&gt;N/A&lt;/td&gt; &lt;td align="right"&gt;65536&lt;/td&gt; &lt;td align="right"&gt;tg128&lt;/td&gt; &lt;td align="right"&gt;0.76&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;We can see that using &lt;code&gt;-ot ffn=CPU&lt;/code&gt; scales dramatically better with context than &lt;code&gt;-ngl ??&lt;/code&gt;. The value of &lt;code&gt;-ngl 21&lt;/code&gt; here was chosen to match the VRAM utilization of &lt;code&gt;-ot ffn=CPU -c 16384&lt;/code&gt; which is about 13.7GB (note that I didn't quantize context!). The one tradeoff in terms of VRAM utilization is that this puts all the context on the GPU rather than splitting it based on &lt;code&gt;-ngl&lt;/code&gt;. As a result the fraction of model you can fit into VRAM is reduced and thus you'd expect worse performance at short context lengths. This is generally quite minor, but as always, test on your hardware. (Note that the test system is an Epyc + 6000 Blackwell so quite chonky with a lot of compute but see my laptop below test below for the opposite.)&lt;/p&gt; &lt;p&gt;Tuning for your system: - Quantize your context (e.g. &lt;code&gt;-ctk q8_0 -ctv q8_0&lt;/code&gt;) if you want/can: As mentioned, pretty much the point of this is to put the context on GPU so it'll use more VRAM than it would with &lt;code&gt;-ngl&lt;/code&gt; where some fraction of the context would be on CPU with the CPU layers. - Offloading less: If you don't have enough VRAM to handle &lt;code&gt;-ngl 99 -ot ffn=CPU&lt;/code&gt; then just use &lt;code&gt;-ngl 50&lt;/code&gt; or whatever. You'll still get better context length scaling, but obviously it won't be perfect. - Offloading more: If you have leftover VRAM after your &lt;code&gt;-ngl 99 -ot ffn=CPU -c ????&lt;/code&gt; then you can offload some of the ffn layers by doing &lt;code&gt;blk.(0|1|2|3|4).ffn=CPU&lt;/code&gt; or &lt;code&gt;blk.[2-9][0-9].ffn=CPU&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Here's a test on my laptop with a &amp;quot;can't believe it's not a 4070&amp;quot; GPU (8GB w/ ~6GB free) and 2ch 6400MHz DDR5. I only go to 10k context (quantized q8_0) and the difference isn't as quite as dramatic but it's still a ~80% improvement at full context length which is nothing to scoff at:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="right"&gt;size&lt;/th&gt; &lt;th align="right"&gt;params&lt;/th&gt; &lt;th&gt;backend&lt;/th&gt; &lt;th align="right"&gt;ngl&lt;/th&gt; &lt;th&gt;ot&lt;/th&gt; &lt;th align="right"&gt;context&lt;/th&gt; &lt;th align="right"&gt;test&lt;/th&gt; &lt;th align="right"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="right"&gt;13.34 GiB&lt;/td&gt; &lt;td align="right"&gt;23.57 B&lt;/td&gt; &lt;td&gt;CUDA&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td&gt;blk.([8-9]&amp;#124;[1-9][0-9]).ffn=CPU&lt;/td&gt; &lt;td align="right"&gt;0&lt;/td&gt; &lt;td align="right"&gt;pp512&lt;/td&gt; &lt;td align="right"&gt;428.51&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="right"&gt;13.34 GiB&lt;/td&gt; &lt;td align="right"&gt;23.57 B&lt;/td&gt; &lt;td&gt;CUDA&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td&gt;blk.([8-9]&amp;#124;[1-9][0-9]).ffn=CPU&lt;/td&gt; &lt;td align="right"&gt;10000&lt;/td&gt; &lt;td align="right"&gt;pp512&lt;/td&gt; &lt;td align="right"&gt;375.32&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="right"&gt;13.34 GiB&lt;/td&gt; &lt;td align="right"&gt;23.57 B&lt;/td&gt; &lt;td&gt;CUDA&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td&gt;blk.([8-9]&amp;#124;[1-9][0-9]).ffn=CPU&lt;/td&gt; &lt;td align="right"&gt;0&lt;/td&gt; &lt;td align="right"&gt;tg128&lt;/td&gt; &lt;td align="right"&gt;4.31&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="right"&gt;13.34 GiB&lt;/td&gt; &lt;td align="right"&gt;23.57 B&lt;/td&gt; &lt;td&gt;CUDA&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td&gt;blk.([8-9]&amp;#124;[1-9][0-9]).ffn=CPU&lt;/td&gt; &lt;td align="right"&gt;10000&lt;/td&gt; &lt;td align="right"&gt;tg128&lt;/td&gt; &lt;td align="right"&gt;4.16&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="right"&gt;13.34 GiB&lt;/td&gt; &lt;td align="right"&gt;23.57 B&lt;/td&gt; &lt;td&gt;CUDA&lt;/td&gt; &lt;td align="right"&gt;13&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td align="right"&gt;0&lt;/td&gt; &lt;td align="right"&gt;pp512&lt;/td&gt; &lt;td align="right"&gt;429.88&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="right"&gt;13.34 GiB&lt;/td&gt; &lt;td align="right"&gt;23.57 B&lt;/td&gt; &lt;td&gt;CUDA&lt;/td&gt; &lt;td align="right"&gt;13&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td align="right"&gt;10000&lt;/td&gt; &lt;td align="right"&gt;pp512&lt;/td&gt; &lt;td align="right"&gt;367.12&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="right"&gt;13.34 GiB&lt;/td&gt; &lt;td align="right"&gt;23.57 B&lt;/td&gt; &lt;td&gt;CUDA&lt;/td&gt; &lt;td align="right"&gt;13&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td align="right"&gt;0&lt;/td&gt; &lt;td align="right"&gt;tg128&lt;/td&gt; &lt;td align="right"&gt;4.46&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="right"&gt;13.34 GiB&lt;/td&gt; &lt;td align="right"&gt;23.57 B&lt;/td&gt; &lt;td&gt;CUDA&lt;/td&gt; &lt;td align="right"&gt;13&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td align="right"&gt;10000&lt;/td&gt; &lt;td align="right"&gt;tg128&lt;/td&gt; &lt;td align="right"&gt;2.34&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eloquentemu"&gt; /u/eloquentemu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8jocc/improving_low_vram_performance_for_dense_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8jocc/improving_low_vram_performance_for_dense_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8jocc/improving_low_vram_performance_for_dense_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T22:02:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8m1v0</id>
    <title>I got Kokoro TTS running natively on iOS! üéâ Natural-sounding speech synthesis entirely on-device</title>
    <updated>2025-10-16T23:44:22+00:00</updated>
    <author>
      <name>/u/Living_Commercial_10</name>
      <uri>https://old.reddit.com/user/Living_Commercial_10</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! Just wanted to share something cool I built this weekend.&lt;/p&gt; &lt;p&gt;I managed to get Kokoro TTS (the high-quality open-source text-to-speech model) running completely natively on iOS - no server, no API calls, 100% on-device inference!&lt;/p&gt; &lt;p&gt;What it does:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Converts text to natural-sounding speech directly on your iPhone/iPad&lt;/li&gt; &lt;li&gt;Uses the full ONNX model (325MB) with real voice embeddings&lt;/li&gt; &lt;li&gt;50+ voices in multiple languages (English, Spanish, French, Japanese, Chinese, etc.)&lt;/li&gt; &lt;li&gt;24kHz audio output at ~4 seconds generation time for a sentence&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The audio quality is surprisingly good! It's not real-time yet (takes a few seconds per sentence), but for a 325MB model running entirely on a phone with no quantization, I'm pretty happy with it. &lt;/p&gt; &lt;p&gt;Planning on integrating it in my iOS apps. &lt;/p&gt; &lt;p&gt;Has anyone else tried running TTS models locally on mobile? Would love to hear about your experiences!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Living_Commercial_10"&gt; /u/Living_Commercial_10 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8m1v0/i_got_kokoro_tts_running_natively_on_ios/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8m1v0/i_got_kokoro_tts_running_natively_on_ios/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8m1v0/i_got_kokoro_tts_running_natively_on_ios/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T23:44:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8x2w0</id>
    <title>What in the Black Friday hell is happening with the DDR5-5600 128GB SODIMM kits ?</title>
    <updated>2025-10-17T09:51:44+00:00</updated>
    <author>
      <name>/u/HumanDrone8721</name>
      <uri>https://old.reddit.com/user/HumanDrone8721</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In summer Amazon was selling them with something like 320‚Ç¨, not they are almost 500‚Ç¨ and increasing, I wanted to update my 64GB to 128, but this is obscene :(&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HumanDrone8721"&gt; /u/HumanDrone8721 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8x2w0/what_in_the_black_friday_hell_is_happening_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8x2w0/what_in_the_black_friday_hell_is_happening_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8x2w0/what_in_the_black_friday_hell_is_happening_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T09:51:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8xd0p</id>
    <title>How do you define acceptance criteria when delivering LLM projects for companies?</title>
    <updated>2025-10-17T10:09:10+00:00</updated>
    <author>
      <name>/u/piske_usagi</name>
      <uri>https://old.reddit.com/user/piske_usagi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, I‚Äôd like to ask‚Äîwhen you take on large language model (LLM) projects for companies, how do you usually discuss and agree on acceptance criteria?&lt;/p&gt; &lt;p&gt;My initial idea was to collaborate with the client to build an evaluation set (perhaps in the form of multiple-choice questions), and once the model achieves a mutually agreed score, it would be considered successful.&lt;/p&gt; &lt;p&gt;However, I‚Äôve found that most companies that commission these projects have trouble accepting this approach. First, they often struggle to translate their internal knowledge into concrete evaluation steps. Second, they tend to rely more on subjective impressions to judge whether the model performs well or not.&lt;/p&gt; &lt;p&gt;I‚Äôm wondering how others handle this situation‚Äîany experiences or frameworks you can share? Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/piske_usagi"&gt; /u/piske_usagi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8xd0p/how_do_you_define_acceptance_criteria_when/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8xd0p/how_do_you_define_acceptance_criteria_when/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8xd0p/how_do_you_define_acceptance_criteria_when/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T10:09:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8n9ym</id>
    <title>We built an open-source coding agent CLI that can be run locally</title>
    <updated>2025-10-17T00:41:19+00:00</updated>
    <author>
      <name>/u/SmilingGen</name>
      <uri>https://old.reddit.com/user/SmilingGen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8n9ym/we_built_an_opensource_coding_agent_cli_that_can/"&gt; &lt;img alt="We built an open-source coding agent CLI that can be run locally" src="https://preview.redd.it/k22utme4jkvf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7cd5da76109133d6785e5504cde25419f8caf5ff" title="We built an open-source coding agent CLI that can be run locally" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Basically, it‚Äôs like Claude Code but with native support for local LLMs and a universal tool parser that works even on inference platforms without built-in tool call support.&lt;/p&gt; &lt;p&gt;Kolosal CLI is an open-source, cross-platform agentic command-line tool that lets you discover, download, and run models locally using an ultra-lightweight inference server. It supports coding agents, Hugging Face model integration, and a memory calculator to estimate model memory requirements.&lt;/p&gt; &lt;p&gt;It‚Äôs a fork of Qwen Code, and we also host GLM 4.6 and Kimi K2 if you prefer to use them without running them yourself.&lt;/p&gt; &lt;p&gt;You can try it at kolosal.ai and check out the source code on GitHub: github.com/KolosalAI/kolosal-cli&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SmilingGen"&gt; /u/SmilingGen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/k22utme4jkvf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8n9ym/we_built_an_opensource_coding_agent_cli_that_can/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8n9ym/we_built_an_opensource_coding_agent_cli_that_can/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T00:41:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8pj1k</id>
    <title>Waiting on Ryzen Max 395+ w/ 128gb RAM to be delivered. How should I set it up for AI?</title>
    <updated>2025-10-17T02:28:29+00:00</updated>
    <author>
      <name>/u/atomicpapa210</name>
      <uri>https://old.reddit.com/user/atomicpapa210</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The title pretty much says it all.&lt;/p&gt; &lt;p&gt;Beelink GTR9 Pro&lt;br /&gt; Ryzen Max AI 395+&lt;br /&gt; 128 gb LPDDR5x-8000&lt;br /&gt; 2TB SSD&lt;br /&gt; Radeon 8060S iGPU&lt;/p&gt; &lt;p&gt;Comes with Windows 11&lt;/p&gt; &lt;p&gt;Planning on using it for Home Assistant and learning more about AI&lt;/p&gt; &lt;p&gt;Should I switch to Linux? This is of course what I am leaning toward.&lt;br /&gt; What should I run for AI? Lemonade Server? Something else?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/atomicpapa210"&gt; /u/atomicpapa210 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8pj1k/waiting_on_ryzen_max_395_w_128gb_ram_to_be/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8pj1k/waiting_on_ryzen_max_395_w_128gb_ram_to_be/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8pj1k/waiting_on_ryzen_max_395_w_128gb_ram_to_be/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T02:28:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8c9ta</id>
    <title>new 1B LLM by meta</title>
    <updated>2025-10-16T17:20:06+00:00</updated>
    <author>
      <name>/u/BreakfastFriendly728</name>
      <uri>https://old.reddit.com/user/BreakfastFriendly728</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8c9ta/new_1b_llm_by_meta/"&gt; &lt;img alt="new 1B LLM by meta" src="https://external-preview.redd.it/jpWPa_miOIzMa-yE2f-cKYYuEKEP1Bglx0QJxru_lGs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7e2417b6932d5a1fcaf49ac7d2561558b6a9873e" title="new 1B LLM by meta" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/5adjo04ecivf1.png?width=825&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c09b47aee7f7597d580d6dbe1eec7e6429504d41"&gt;https://preview.redd.it/5adjo04ecivf1.png?width=825&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c09b47aee7f7597d580d6dbe1eec7e6429504d41&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/facebook/MobileLLM-Pro"&gt;facebook/MobileLLM-Pro ¬∑ Hugging Face&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BreakfastFriendly728"&gt; /u/BreakfastFriendly728 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8c9ta/new_1b_llm_by_meta/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8c9ta/new_1b_llm_by_meta/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8c9ta/new_1b_llm_by_meta/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T17:20:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8q4xt</id>
    <title>North Dakota using Llama3.2 1B with Ollama to summarize bills</title>
    <updated>2025-10-17T02:57:56+00:00</updated>
    <author>
      <name>/u/SM8085</name>
      <uri>https://old.reddit.com/user/SM8085</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Didn't see this posted here yet.&lt;/p&gt; &lt;p&gt;Apparently North Dakota has been using &lt;a href="https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct"&gt;Llama3.2 1B&lt;/a&gt; with Ollama to summarize their bills and are seeing positive results.&lt;/p&gt; &lt;p&gt;Video: &lt;a href="https://www.youtube.com/watch?v=PYqH1aYhLY0"&gt;North Dakota Legislature innovates with AI - KX News (Youtube)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm surprised they went with Llama3.2 1B, but I think it's interesting they're using a local model.&lt;/p&gt; &lt;p&gt;Somebody in ND had a spare raspberry pi 5 to give the state an AI system?&lt;/p&gt; &lt;p&gt;When I mention summarizing things with small models 4B and under people will ask what kind of accuracy I get and I'm never sure how to quantify it. I get nervous with bots under 2B, but maybe less is more when you're asking them to simply summarize things without injecting what they may or may not know on the subject?&lt;/p&gt; &lt;p&gt;I'll have to check how many bills are over 128k tokens long. I wonder what their plan is at that point? I suppose just do it the old fashioned way.&lt;/p&gt; &lt;p&gt;What does &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; think about this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SM8085"&gt; /u/SM8085 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://markets.financialcontent.com/stocks/article/tokenring-2025-10-15-north-dakota-pioneers-ai-in-government-legislative-council-adopts-meta-ai-to-revolutionize-bill-summarization"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8q4xt/north_dakota_using_llama32_1b_with_ollama_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8q4xt/north_dakota_using_llama32_1b_with_ollama_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T02:57:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1o866vl</id>
    <title>PaddleOCR-VL, is better than private models</title>
    <updated>2025-10-16T13:29:48+00:00</updated>
    <author>
      <name>/u/Illustrious-Swim9663</name>
      <uri>https://old.reddit.com/user/Illustrious-Swim9663</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o866vl/paddleocrvl_is_better_than_private_models/"&gt; &lt;img alt="PaddleOCR-VL, is better than private models" src="https://b.thumbs.redditmedia.com/X4U2z8D2mUefEIEuBe11hVIGCJrPT-oF1EOiA8c0dyw.jpg" title="PaddleOCR-VL, is better than private models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/PaddlePaddle/status/1978809999263781290?t=mcHYAF7osq3MmicjMLi0IQ&amp;amp;s=19"&gt;https://x.com/PaddlePaddle/status/1978809999263781290?t=mcHYAF7osq3MmicjMLi0IQ&amp;amp;s=19&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Illustrious-Swim9663"&gt; /u/Illustrious-Swim9663 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1o866vl"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o866vl/paddleocrvl_is_better_than_private_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o866vl/paddleocrvl_is_better_than_private_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T13:29:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8sbv1</id>
    <title>üöÄ HuggingFaceChat Omni: Dynamic policy-baed routing to 115+ LLMs</title>
    <updated>2025-10-17T04:52:56+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8sbv1/huggingfacechat_omni_dynamic_policybaed_routing/"&gt; &lt;img alt="üöÄ HuggingFaceChat Omni: Dynamic policy-baed routing to 115+ LLMs" src="https://preview.redd.it/tmc0nl14pjvf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f6edd535c0dbc7303cd47479e890c56a0f66c45b" title="üöÄ HuggingFaceChat Omni: Dynamic policy-baed routing to 115+ LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Introducing: &lt;a href="https://github.com/huggingface/chat-ui"&gt;HuggingChat Omni &lt;/a&gt; &lt;/p&gt; &lt;p&gt;Select the best model for every prompt automatically &lt;/p&gt; &lt;p&gt;- Automatic model selection for your queries&lt;br /&gt; - 115 models available across 15 providers &lt;/p&gt; &lt;p&gt;Available now all Hugging Face users. 100% open source.&lt;/p&gt; &lt;p&gt;Omni uses a policy-based approach to model selection (after experimenting with different methods). Credits to &lt;a href="https://huggingface.co/katanemo"&gt;Katanemo&lt;/a&gt; for their small routing model: &lt;a href="https://huggingface.co/katanemo/Arch-Router-1.5B"&gt;katanemo/Arch-Router-1.5B&lt;/a&gt;. The model is natively integrated in &lt;a href="https://github.com/katanemo/archgw"&gt;archgw&lt;/a&gt; for those who want to build their own chat experiences with policy-based dynamic routing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/tmc0nl14pjvf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8sbv1/huggingfacechat_omni_dynamic_policybaed_routing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8sbv1/huggingfacechat_omni_dynamic_policybaed_routing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T04:52:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8k4gc</id>
    <title>DGX Spark is here, give me your non-inference workloads</title>
    <updated>2025-10-16T22:20:50+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8k4gc/dgx_spark_is_here_give_me_your_noninference/"&gt; &lt;img alt="DGX Spark is here, give me your non-inference workloads" src="https://preview.redd.it/17hmoj42ujvf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aca471462e384551e5b0d09c075c93f99e741ab1" title="DGX Spark is here, give me your non-inference workloads" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just received my DGX Spark. We all know it's trash for inference, so give me your non-inference test ideas (e.g., RL) to see what else it's trash at. I can also compare the numbers with my 4090 and H100.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/17hmoj42ujvf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8k4gc/dgx_spark_is_here_give_me_your_noninference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8k4gc/dgx_spark_is_here_give_me_your_noninference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T22:20:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8no7i</id>
    <title>Best Open Source TTS That Sounds Most Natural Voice For Storytelling? That You Can Run With 12GB Vram?</title>
    <updated>2025-10-17T01:00:05+00:00</updated>
    <author>
      <name>/u/Head-Investigator540</name>
      <uri>https://old.reddit.com/user/Head-Investigator540</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Last I heard Higgs was great - but have heard it takes 24gb vram (and I only have 12GB on my machine). So wanted to see if anyone had suggested on the best free to use (commercial or otherwise) that I can run from my own machine.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Head-Investigator540"&gt; /u/Head-Investigator540 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8no7i/best_open_source_tts_that_sounds_most_natural/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8no7i/best_open_source_tts_that_sounds_most_natural/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8no7i/best_open_source_tts_that_sounds_most_natural/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T01:00:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8hacw</id>
    <title>Since DGX Spark is a disappointment... What is the best value for money hardware today?</title>
    <updated>2025-10-16T20:26:47+00:00</updated>
    <author>
      <name>/u/goto-ca</name>
      <uri>https://old.reddit.com/user/goto-ca</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My current compute box (2√ó1080 Ti) is failing, so I‚Äôve been renting GPUs by the hour. I‚Äôd been waiting for DGX Spark, but early reviews look disappointing for the price/perf.&lt;/p&gt; &lt;p&gt;I‚Äôm ready to build a new PC and I‚Äôm torn between a &lt;strong&gt;single&lt;/strong&gt; high-end GPU or &lt;strong&gt;dual&lt;/strong&gt; mid/high GPUs. What‚Äôs the &lt;strong&gt;best price/performance configuration&lt;/strong&gt; I can build &lt;strong&gt;for ‚â§ $3,999&lt;/strong&gt; (tower, not a rack server)?&lt;/p&gt; &lt;p&gt;I don't care about RGBs and things like that - it will be kept in the basement and not looked at. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/goto-ca"&gt; /u/goto-ca &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8hacw/since_dgx_spark_is_a_disappointment_what_is_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8hacw/since_dgx_spark_is_a_disappointment_what_is_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8hacw/since_dgx_spark_is_a_disappointment_what_is_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T20:26:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8b00e</id>
    <title>GLM 4.6 is hilarious, I wish I could run this on my own PC lol</title>
    <updated>2025-10-16T16:34:48+00:00</updated>
    <author>
      <name>/u/Cool-Chemical-5629</name>
      <uri>https://old.reddit.com/user/Cool-Chemical-5629</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Prompt:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;I have a fun task for you! Search the web to find the most (infamously) popular annoying LLM clich√©s (aka AI slop) in roleplay and creative writing and generate a sentence using only those phrases. Make the sentence as long as possible to cover them all.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;GLM 4.6 response:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Based on the search results, I've compiled the most notoriously annoying LLM clich√©s in roleplay and creative writing. Here's a sentence that combines as many of these overused phrases as possible:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&amp;quot;In a world where the tapestry of human emotions unfolds like a delicate symphony, I'm sorry, but as an AI language model, I cannot delve into unlocking your full potential without first embarking on a journey to harness the vast landscape of creativity, though I'm just an AI and don't have feelings, but I'm here to help you explore the intricate dance of storytelling where every thread weaves a new chapter in the grand narrative of imagination.&amp;quot;&lt;/strong&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cool-Chemical-5629"&gt; /u/Cool-Chemical-5629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8b00e/glm_46_is_hilarious_i_wish_i_could_run_this_on_my/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8b00e/glm_46_is_hilarious_i_wish_i_could_run_this_on_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8b00e/glm_46_is_hilarious_i_wish_i_could_run_this_on_my/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T16:34:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8h9ye</id>
    <title>China's GPU Competition: 96GB Huawei Atlas 300I Duo Dual-GPU Tear-Down</title>
    <updated>2025-10-16T20:26:18+00:00</updated>
    <author>
      <name>/u/sub_RedditTor</name>
      <uri>https://old.reddit.com/user/sub_RedditTor</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8h9ye/chinas_gpu_competition_96gb_huawei_atlas_300i_duo/"&gt; &lt;img alt="China's GPU Competition: 96GB Huawei Atlas 300I Duo Dual-GPU Tear-Down" src="https://external-preview.redd.it/h0AXu77KNbLZvwy9Mngk-UUEgeAjkwRBjGOdlujFn7U.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=436c9e5446c362e3900f824dd945b76349b991ff" title="China's GPU Competition: 96GB Huawei Atlas 300I Duo Dual-GPU Tear-Down" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We need benchmarks ..&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sub_RedditTor"&gt; /u/sub_RedditTor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/qGe_fq68x-Q?si=0w_3qjLvmHTXPWxI"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8h9ye/chinas_gpu_competition_96gb_huawei_atlas_300i_duo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8h9ye/chinas_gpu_competition_96gb_huawei_atlas_300i_duo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T20:26:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8ay34</id>
    <title>GLM 4.6 air when?</title>
    <updated>2025-10-16T16:32:48+00:00</updated>
    <author>
      <name>/u/gamblingapocalypse</name>
      <uri>https://old.reddit.com/user/gamblingapocalypse</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8ay34/glm_46_air_when/"&gt; &lt;img alt="GLM 4.6 air when?" src="https://preview.redd.it/1bhgri4w3ivf1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=660b0d931d85bfb508e26c3e8647b3b8a75f8708" title="GLM 4.6 air when?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gamblingapocalypse"&gt; /u/gamblingapocalypse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1bhgri4w3ivf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8ay34/glm_46_air_when/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8ay34/glm_46_air_when/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T16:32:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8m0ti</id>
    <title>We built 3B and 8B models that rival GPT-5 at HTML extraction while costing 40-80x less - fully open source</title>
    <updated>2025-10-16T23:43:03+00:00</updated>
    <author>
      <name>/u/TerrificMist</name>
      <uri>https://old.reddit.com/user/TerrificMist</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8m0ti/we_built_3b_and_8b_models_that_rival_gpt5_at_html/"&gt; &lt;img alt="We built 3B and 8B models that rival GPT-5 at HTML extraction while costing 40-80x less - fully open source" src="https://b.thumbs.redditmedia.com/m8yoOZ6gwR1CBn-AO5mIRDQSzZ5G3sXEIoAl_NdSUMk.jpg" title="We built 3B and 8B models that rival GPT-5 at HTML extraction while costing 40-80x less - fully open source" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;em&gt;Disclaimer: I work for&lt;/em&gt; &lt;a href="http://Inference.net"&gt;&lt;em&gt;Inference.net&lt;/em&gt;&lt;/a&gt;&lt;em&gt;, creator of the Schematron model family&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Hey everyone, wanted to share something we've been working on at Inference.net: Schematron, a family of small models for web extraction.&lt;/p&gt; &lt;p&gt;Our goal was to make a small, fast model for taking HTML from website and extracting JSON that perfectly adheres to a schema.&lt;/p&gt; &lt;p&gt;We distilled a frontier model down to 8B params and managed to keep basically all the output quality for this task. Schematron-8B scores 4.64 on LLM-as-a-judge evals vs GPT-4.1's 4.74 and Gemma 3B's 2.24. Schematron-3B scores 4.41 while being even faster. The main benefit of this model is that it costs 40-80x less than GPT-5 at comparable quality (slightly worse than GPT-5, better than Gemini 2-5 Flash).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Technical details:&lt;/strong&gt; We fine-tuned &lt;a href="https://huggingface.co/meta-llama/Llama-3.1-8B"&gt;Llama-3.1-8B&lt;/a&gt;, expanded it to a 128K context window, quantized to FP8 without quality loss, and trained until it outputted strict JSON with 100% schema compliance. We also built a smaller 3B variant that's even cheaper and faster, but still maintains most of the accuracy of the 8B variant. We recommend using the 3B for most tasks, and trying 8B if it fails or most of your documents are pushing the context limit.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How we trained it:&lt;/strong&gt; We started with 1M real web pages from Common Crawl and built a synthetic dataset by clustering websites and generating schemas that mirror real-world usage patterns. We used a frontier model as a teacher and applied curriculum learning to progressively train on longer context lengths--training with context parallelism and FSDP to scale efficiently--which is why the models stay accurate even at the 128K token limit.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why this matters:&lt;/strong&gt; Processing 1 million pages daily with GPT-5 would cost you around $20,000. With Schematron-8B, that same workload runs about $480. With Schematron-3B, it's $240.&lt;/p&gt; &lt;p&gt;The speed matters too. Schematron processes pages 10x faster than frontier models. On average, Schamatron can scrape a page in 0.54 seconds, compared to 6 seconds for GPT-5. These latency gains compound very quickly for something like a browser-use agent.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Real-world impact on LLM factuality:&lt;/strong&gt; We tested this on SimpleQA to see how much it improves accuracy when paired with web search. When GPT-5 Nano was paired with Schematron-8B to extract structured data from search results provided by Exa, it went from answering barely any questions correctly (8.54% on SimpleQA) to getting over 85% right. The structured extraction approach means this was done processing lean, clean JSON (very little additional cost) instead of dumping ~8k tokens of raw HTML into your context window per page retrieved (typically LLMs are grounded with 5-10 pages/search).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Getting started:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;If you're using our &lt;a href="https://inference.net/models/schematron-3b"&gt;serverless API&lt;/a&gt;, you only need to pass your Pydantic, Zod, or JSON Schema and the HTML. We handle all the prompting in the backend for you in the backend. You get $10 in free credits to start.&lt;/p&gt; &lt;p&gt;If you're running locally, there are a few things to watch out for. You need to follow the prompting guidelines carefully and make sure you're using structured extraction properly, otherwise the model won't perform as well. &lt;/p&gt; &lt;p&gt;The models are on &lt;a href="https://huggingface.co/inference-net/Schematron-3B"&gt;HuggingFace&lt;/a&gt; and &lt;a href="https://ollama.com/Inference/Schematron"&gt;Ollama&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Full benchmarks and code examples are in our blog post: &lt;a href="https://inference.net/blog/schematron"&gt;https://inference.net/blog/schematron&lt;/a&gt;, &lt;a href="https://docs.inference.net/workhorse-models/schematron"&gt;docs&lt;/a&gt;, and &lt;a href="https://github.com/context-labs/inference-samples/blob/main/examples/schematron-scrape-companies/schematron-scrape-companies.ipynb"&gt;samples repo&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Happy to answer any technical questions about the training process or architecture. Also interested in how this would be helpful in your current scraping workflows!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TerrificMist"&gt; /u/TerrificMist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1o8m0ti"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8m0ti/we_built_3b_and_8b_models_that_rival_gpt5_at_html/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8m0ti/we_built_3b_and_8b_models_that_rival_gpt5_at_html/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T23:43:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8wuyj</id>
    <title>Valve Developer Contributes Major Improvement To RADV Vulkan For Llama.cpp AI</title>
    <updated>2025-10-17T09:37:37+00:00</updated>
    <author>
      <name>/u/FastDecode1</name>
      <uri>https://old.reddit.com/user/FastDecode1</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FastDecode1"&gt; /u/FastDecode1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.phoronix.com/news/RADV-Valve-Boost-Llama.cpp"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8wuyj/valve_developer_contributes_major_improvement_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8wuyj/valve_developer_contributes_major_improvement_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T09:37:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8m5ua</id>
    <title>Meta just dropped MobileLLM-Pro, a new 1B foundational language model on Huggingface</title>
    <updated>2025-10-16T23:49:16+00:00</updated>
    <author>
      <name>/u/Sad_Consequence5629</name>
      <uri>https://old.reddit.com/user/Sad_Consequence5629</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Meta just published MobileLLM-Pro, a new 1B parameter foundational language model (pre-trained and instruction fine-tuned) on Huggingface&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/facebook/MobileLLM-Pro"&gt;https://huggingface.co/facebook/MobileLLM-Pro&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The model seems to outperform Gemma 3-1B and Llama 3-1B by quite a large margin in pre-training and shows decent performance after instruction-tuning (Looks like it works pretty well for API calling, rewriting, coding and summarization).&lt;br /&gt; The model is already in GradIO and can be directly chatted with in the browser:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/spaces/akhaliq/MobileLLM-Pro"&gt;https://huggingface.co/spaces/akhaliq/MobileLLM-Pro&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(Tweet source: &lt;a href="https://x.com/_akhaliq/status/1978916251456925757"&gt;https://x.com/_akhaliq/status/1978916251456925757&lt;/a&gt; )&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sad_Consequence5629"&gt; /u/Sad_Consequence5629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8m5ua/meta_just_dropped_mobilellmpro_a_new_1b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8m5ua/meta_just_dropped_mobilellmpro_a_new_1b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8m5ua/meta_just_dropped_mobilellmpro_a_new_1b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T23:49:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8uxh6</id>
    <title>Write three times the word potato</title>
    <updated>2025-10-17T07:32:29+00:00</updated>
    <author>
      <name>/u/TooManyPascals</name>
      <uri>https://old.reddit.com/user/TooManyPascals</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8uxh6/write_three_times_the_word_potato/"&gt; &lt;img alt="Write three times the word potato" src="https://b.thumbs.redditmedia.com/VWK4WzyVVfvV7xJuANrLzK-bH1UfvcQckXM3kS4Llno.jpg" title="Write three times the word potato" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was testing how well Qwen3-0.6B could follow simple instructions... &lt;/p&gt; &lt;p&gt;and it accidentally created a trolling masterpiece.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TooManyPascals"&gt; /u/TooManyPascals &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1o8uxh6"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8uxh6/write_three_times_the_word_potato/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8uxh6/write_three_times_the_word_potato/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T07:32:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
