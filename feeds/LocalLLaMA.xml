<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-12T14:08:08+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1pkr0x0</id>
    <title>Building an offline legal compliance AI on RTX 3090 ‚Äì am I doing this right or completely overengineering it?</title>
    <updated>2025-12-12T12:47:30+00:00</updated>
    <author>
      <name>/u/Motijani28</name>
      <uri>https://old.reddit.com/user/Motijani28</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;I'm building an AI system for insurance policy compliance that needs to run &lt;strong&gt;100% offline&lt;/strong&gt; for legal/privacy reasons. Think: processing payslips, employment contracts, medical records, and cross-referencing them against 300+ pages of insurance regulations to auto-detect claim discrepancies.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What's working so far:&lt;/strong&gt; - Ryzen 9 9950X, 96GB DDR5, RTX 3090 24GB, Windows 11 + Docker + WSL2 - Python 3.11 + Ollama + Tesseract OCR - Built a payslip extractor (OCR + regex) that pulls employee names, national registry numbers, hourly wage (‚Ç¨16.44/hr baseline), sector codes, and hours worked ‚Üí &lt;strong&gt;70-80% accuracy, good enough for PoC&lt;/strong&gt; - Tested Qwen 2.5 14B/32B models locally - Got structured test dataset ready: 13 docs (payslips, contracts, work schedules) from a real anonymized case&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What didn't work:&lt;/strong&gt; - Open WebUI didn't cut it for this use case ‚Äì too generic, not flexible enough for legal document workflows&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What I'm building next:&lt;/strong&gt; - RAG pipeline (LlamaIndex) to index legal sources (insurance regulation PDFs) - Auto-validation: extract payslip data ‚Üí query RAG ‚Üí check compliance ‚Üí generate report with legal citations - Multi-document comparison (contract ‚Üî payslip ‚Üî work hours) - Demo ready by March 2026&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My questions:&lt;/strong&gt; 1. &lt;strong&gt;Model choice:&lt;/strong&gt; Currently eyeing &lt;strong&gt;Qwen 3 30B-A3B (MoE)&lt;/strong&gt; ‚Äì is this the right call for legal reasoning on 24GB VRAM, or should I go with dense 32B? Thinking mode seems clutch for compliance checks. 2. &lt;strong&gt;RAG chunking:&lt;/strong&gt; Fixed-size (1000 tokens) vs section-aware splitting for legal docs? What actually works in production? 3. &lt;strong&gt;Anyone done similar compliance/legal document AI locally?&lt;/strong&gt; What were your pain points? Did it actually work or just benchmarketing bullshit? 4. &lt;strong&gt;Better alternatives to LlamaIndex for this?&lt;/strong&gt; Or am I on the right track?&lt;/p&gt; &lt;p&gt;I'm targeting 70-80% automation for document analysis ‚Äì still needs human review, AI just flags potential issues and cross-references regulations. Not trying to replace legal experts, just speed up the tedious document processing work.&lt;/p&gt; &lt;p&gt;Any tips, similar projects, or &amp;quot;you're doing it completely wrong&amp;quot; feedback welcome. Tight deadline, don't want to waste 3 months going down the wrong path.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; Building offline legal compliance AI (insurance claims) on RTX 3090. Payslip extraction works (70-80%), now adding RAG for legal validation. Qwen 3 30B-A3B good choice? Anyone done similar projects that actually worked? Need it done by March 2026.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Motijani28"&gt; /u/Motijani28 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkr0x0/building_an_offline_legal_compliance_ai_on_rtx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkr0x0/building_an_offline_legal_compliance_ai_on_rtx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pkr0x0/building_an_offline_legal_compliance_ai_on_rtx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T12:47:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1pk3znw</id>
    <title>Microsoft analyzed 37.5 million AI conversations in 2025.</title>
    <updated>2025-12-11T17:50:31+00:00</updated>
    <author>
      <name>/u/Karam1234098</name>
      <uri>https://old.reddit.com/user/Karam1234098</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pk3znw/microsoft_analyzed_375_million_ai_conversations/"&gt; &lt;img alt="Microsoft analyzed 37.5 million AI conversations in 2025." src="https://b.thumbs.redditmedia.com/I0gHntPEl9oW4aCmQGgV1BpHx_Jt0xMYNUHzNOY1Pys.jpg" title="Microsoft analyzed 37.5 million AI conversations in 2025." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Microsoft just released their &amp;quot;Copilot Usage Report 2025,&amp;quot; analyzing de-identified data to see how people actually use AI in their daily lives. The results are surprisingly human. Here are the most interesting graphs and takeaways from the report:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;The &amp;quot;Work Hard, Play Hard&amp;quot; Split&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;People have distinct modes for the week vs. the weekend.&lt;/p&gt; &lt;p&gt;View Graph: Programming vs. Gaming&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The Insight: In August, there was a perfect crossover. &amp;quot;Programming&amp;quot; queries rise steadily from Monday to Friday, then tank on Saturday/Sunday. &amp;quot;Gaming&amp;quot; does the exact opposite, dominating the weekends.&lt;/li&gt; &lt;/ul&gt; &lt;ol&gt; &lt;li&gt;The 2 AM Philosophy Club&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The topics we talk about change drastically depending on the time of day.&lt;/p&gt; &lt;p&gt;View Graph: Topic by Hour of Day&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The Insight: This radial chart shows that &amp;quot;Travel&amp;quot; queries peak during standard commuting hours. However, &amp;quot;Religion and Philosophy&amp;quot; sees a massive spike in the early morning hours. If you're asking AI about the nature of existence at 3 AM, you aren't alone.&lt;/li&gt; &lt;/ul&gt; &lt;ol&gt; &lt;li&gt;The Valentine's Day Panic&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;February data shows a very specific narrative arc.&lt;/p&gt; &lt;p&gt;View Graph: February Topic Trends&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The Insight: &amp;quot;Personal Growth&amp;quot; topics peak in the days leading up to Valentine's Day (people trying to improve themselves?), while &amp;quot;Relationship&amp;quot; queries spike on the day itself (people needing immediate advice).&lt;/li&gt; &lt;/ul&gt; &lt;ol&gt; &lt;li&gt;Health is King on Mobile&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;When we are on our phones, we are almost always worried about our health.&lt;/p&gt; &lt;p&gt;View Graph: Top Mobile Topics&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The Insight: No matter the month, &amp;quot;Health&amp;quot; is consistently the #1 topic for mobile users, far outpacing entertainment or productivity. TL;DR: We use AI to code during the week, survive relationships in February, and serve as a therapist/philosopher late at night.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Source: &lt;a href="https://microsoft.ai/news/its-about-time-the-copilot-usage-report-2025/?utm_source=alphasignal&amp;amp;utm_campaign=2025-12-11&amp;amp;lid=bpzfIvhThUltNeQ9&amp;amp;hl=en-GB"&gt;Microsoft AI - The Copilot Usage Report 2025 &lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Karam1234098"&gt; /u/Karam1234098 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pk3znw"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pk3znw/microsoft_analyzed_375_million_ai_conversations/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pk3znw/microsoft_analyzed_375_million_ai_conversations/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-11T17:50:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkrnpo</id>
    <title>4x AMD R9700 vllm System</title>
    <updated>2025-12-12T13:18:09+00:00</updated>
    <author>
      <name>/u/NunzeCs</name>
      <uri>https://old.reddit.com/user/NunzeCs</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I am new to Reddit, I started testing with local LLMs using a Xeon W2255, 128GB RAM, and 2x RTX 3080s, and everything ran smoothly. Since my primary goal was inference, I initially upgraded to two AMD R9700s to get more VRAM.&lt;/p&gt; &lt;p&gt;The project is working well so far, so I'm moving to the next step with new hardware. My pipeline requires an LLM, a VLM, and a RAG system (including Embeddings and Reranking).&lt;/p&gt; &lt;p&gt;I have now purchased two additional R9700s and plan to build a Threadripper 9955WX Pro system with 128GB DDR5 housing the four R9700s, which will be dedicated exclusively to running vLLM. My old Xeon W2255 system would remain in service to handle the VLM and the rest of the workload, with both systems connected directly via a 10Gb network.&lt;/p&gt; &lt;p&gt;My original plan was to put everything into the Threadripper build and run 6x R9700s, but it feels like going beyond 4 GPUs in one system introduces too many extra problems.&lt;/p&gt; &lt;p&gt;I just wanted to hear your thoughts on this plan. Also, since I haven't found much info on 4x R9700 systems yet, let me know if there are specific models you'd like me to test. Currently, I‚Äôm planning to run gpt-oss 120b.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NunzeCs"&gt; /u/NunzeCs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkrnpo/4x_amd_r9700_vllm_system/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkrnpo/4x_amd_r9700_vllm_system/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pkrnpo/4x_amd_r9700_vllm_system/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T13:18:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkg4iy</id>
    <title>Typical performance of gpt-oss-120b on consumer hardware?</title>
    <updated>2025-12-12T02:20:52+00:00</updated>
    <author>
      <name>/u/Diligent-Culture-432</name>
      <uri>https://old.reddit.com/user/Diligent-Culture-432</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is this typical performance, or are there ways to optimize tps even further?&lt;/p&gt; &lt;p&gt;11-12 tps on gpt-oss-120b on 32GB VRAM (2x5060Ti) &amp;amp; 128GB DDR4 RAM&lt;/p&gt; &lt;p&gt;- Intel i7-11700&lt;/p&gt; &lt;p&gt;- 1x 5060Ti 16gb on PCIe x16&lt;/p&gt; &lt;p&gt;- 1x 5060Ti 16gb on PCIe x4&lt;/p&gt; &lt;p&gt;- 4x 32 GB DDR4-3200 RAM (actually appears to be running at 2400 on checking task manager)&lt;/p&gt; &lt;p&gt;- Running on LM Studio&lt;/p&gt; &lt;p&gt;- 32k context&lt;/p&gt; &lt;p&gt;- experts offloaded to CPU&lt;/p&gt; &lt;p&gt;- 36/36 GPU offloaded&lt;/p&gt; &lt;p&gt;- flash attention enabled&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Diligent-Culture-432"&gt; /u/Diligent-Culture-432 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkg4iy/typical_performance_of_gptoss120b_on_consumer/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkg4iy/typical_performance_of_gptoss120b_on_consumer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pkg4iy/typical_performance_of_gptoss120b_on_consumer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T02:20:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkhx0l</id>
    <title>whats everyones thoughts on devstral small 24b?</title>
    <updated>2025-12-12T03:46:24+00:00</updated>
    <author>
      <name>/u/Odd-Ordinary-5922</name>
      <uri>https://old.reddit.com/user/Odd-Ordinary-5922</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Idk if llamacpp is broken for it but my experience is not too great. &lt;/p&gt; &lt;p&gt;Tried creating a snake game and it failed to even start. Considered that maybe the model is more focused on solving problems so I gave it a hard leetcode problem that imo it shouldve been trained on but when it tried to solve it, failed...which gptoss 20b and qwen30b a3b both completed successfully. &lt;/p&gt; &lt;p&gt;lmk if theres a bug the quant I used was unsloth dynamic 4bit&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Odd-Ordinary-5922"&gt; /u/Odd-Ordinary-5922 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkhx0l/whats_everyones_thoughts_on_devstral_small_24b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkhx0l/whats_everyones_thoughts_on_devstral_small_24b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pkhx0l/whats_everyones_thoughts_on_devstral_small_24b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T03:46:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkqzmf</id>
    <title>I cooked MPOA abliterated Seed-OSS-36B-Instruct</title>
    <updated>2025-12-12T12:45:43+00:00</updated>
    <author>
      <name>/u/Perfect_Biscotti_476</name>
      <uri>https://old.reddit.com/user/Perfect_Biscotti_476</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi community,&lt;/p&gt; &lt;p&gt;I cooked up a new abliterated version of Seed-OSS-36B-Instruct using the norm-preserving biprojected abliteration technique.&lt;/p&gt; &lt;p&gt;Although I used to use the &amp;quot;Norm-Preserving Abliterated&amp;quot; tag, I am switching to the MPOA tag (Magnitude-Preserving Orthogonalized Ablation, a.k.a. norm-preserving biprojected abliteration) to stay consistent with grimjim, who proposed this technique.&lt;/p&gt; &lt;p&gt;Model card: &lt;a href="https://huggingface.co/YanLabs/Seed-OSS-36B-Instruct-MPOA"&gt;https://huggingface.co/YanLabs/Seed-OSS-36B-Instruct-MPOA&lt;/a&gt;&lt;br /&gt; Model: YanLabs/Seed-OSS-36B-Instruct-MPOA&lt;br /&gt; Technique: jim-plus/llm-abliteration&lt;br /&gt; Hardware: one A100 GPU via RunPod&lt;/p&gt; &lt;p&gt;GGUF files are now available at:&lt;br /&gt; &lt;a href="https://huggingface.co/YanLabs/Seed-OSS-36B-Instruct-MPOA-GGUF"&gt;https://huggingface.co/YanLabs/Seed-OSS-36B-Instruct-MPOA-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Please give it a try ‚Äî any feedback is appreciated!&lt;/p&gt; &lt;p&gt;By the way, I also uploaded&lt;br /&gt; &lt;a href="https://huggingface.co/YanLabs/gemma-3-4b-it-abliterated-normpreserve"&gt;https://huggingface.co/YanLabs/gemma-3-4b-it-abliterated-normpreserve&lt;/a&gt;&lt;br /&gt; and the corresponding GGUF files&lt;br /&gt; (&lt;a href="https://huggingface.co/YanLabs/gemma-3-4b-it-abliterated-normpreserve-GGUF"&gt;https://huggingface.co/YanLabs/gemma-3-4b-it-abliterated-normpreserve-GGUF&lt;/a&gt;)&lt;br /&gt; to my HF repository. Since this is a smaller model, I‚Äôm saving myself some time by not making a dedicated release post.&lt;/p&gt; &lt;h1&gt;Disclaimer&lt;/h1&gt; &lt;p&gt;This model has safety guardrails removed. It is for research purposes only.&lt;br /&gt; Use responsibly and in compliance with applicable laws.&lt;/p&gt; &lt;h1&gt;About Me&lt;/h1&gt; &lt;p&gt;I'm an LLM enthusiast and practicing lawyer based in Shanghai.&lt;br /&gt; If your AI company needs legal services (domestic or international), feel free to reach out:&lt;/p&gt; &lt;p&gt;üìß [&lt;a href="mailto:ruiqingyan@outlook.com"&gt;ruiqingyan@outlook.com&lt;/a&gt;](mailto:&lt;a href="mailto:ruiqingyan@outlook.com"&gt;ruiqingyan@outlook.com&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;Happy experimenting! üöÄ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Perfect_Biscotti_476"&gt; /u/Perfect_Biscotti_476 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkqzmf/i_cooked_mpoa_abliterated_seedoss36binstruct/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkqzmf/i_cooked_mpoa_abliterated_seedoss36binstruct/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pkqzmf/i_cooked_mpoa_abliterated_seedoss36binstruct/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T12:45:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1pjvtgn</id>
    <title>Leaked footage from Meta's post-training strategy meeting.</title>
    <updated>2025-12-11T12:02:11+00:00</updated>
    <author>
      <name>/u/YouCanMake1t</name>
      <uri>https://old.reddit.com/user/YouCanMake1t</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pjvtgn/leaked_footage_from_metas_posttraining_strategy/"&gt; &lt;img alt="Leaked footage from Meta's post-training strategy meeting." src="https://preview.redd.it/2cbgowoj0i6g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8274908702ea2b4e3ee76f7741b54aa24bef73d7" title="Leaked footage from Meta's post-training strategy meeting." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/YouCanMake1t"&gt; /u/YouCanMake1t &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2cbgowoj0i6g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pjvtgn/leaked_footage_from_metas_posttraining_strategy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pjvtgn/leaked_footage_from_metas_posttraining_strategy/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-11T12:02:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkr50a</id>
    <title>vibevoice real time swift port</title>
    <updated>2025-12-12T12:53:19+00:00</updated>
    <author>
      <name>/u/Tiny_Judge_2119</name>
      <uri>https://old.reddit.com/user/Tiny_Judge_2119</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The stream input works great with the LLM stream output. Just had to try piping it with mlx_lm.generate, and it works great.&lt;br /&gt; &lt;a href="https://x.com/LiMzba/status/1999457581228785875?s=20"&gt;https://x.com/LiMzba/status/1999457581228785875?s=20&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tiny_Judge_2119"&gt; /u/Tiny_Judge_2119 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkr50a/vibevoice_real_time_swift_port/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkr50a/vibevoice_real_time_swift_port/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pkr50a/vibevoice_real_time_swift_port/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T12:53:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1pko44g</id>
    <title>Benchmark Fatigue - How do you evaluate new models for yourself?</title>
    <updated>2025-12-12T09:55:10+00:00</updated>
    <author>
      <name>/u/Funny-Clock1582</name>
      <uri>https://old.reddit.com/user/Funny-Clock1582</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am getting more and more the impression that the benchmark results published for new models are not even close to the experience i make with models.&lt;br /&gt; Maybe its time for me to create some standard questions for a first quick evaluation of new models just for myself.&lt;br /&gt; Do you guys do this and do you have prompts you feel are helpful in your experience?&lt;/p&gt; &lt;p&gt;Cheers Wolfram &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Funny-Clock1582"&gt; /u/Funny-Clock1582 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pko44g/benchmark_fatigue_how_do_you_evaluate_new_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pko44g/benchmark_fatigue_how_do_you_evaluate_new_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pko44g/benchmark_fatigue_how_do_you_evaluate_new_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T09:55:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkphs3</id>
    <title>Undo for destructive shell commands used by AI agents (SafeShell)</title>
    <updated>2025-12-12T11:22:14+00:00</updated>
    <author>
      <name>/u/qhkmdev90</name>
      <uri>https://old.reddit.com/user/qhkmdev90</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As local AI agents start running shell commands directly, we probably need a better way to protect the filesystem than sandboxes or confirmation prompts.&lt;/p&gt; &lt;p&gt;I built a small open source tool called SafeShell that makes destructive commands reversible (rm, mv, cp, chmod, chown).&lt;/p&gt; &lt;p&gt;It automatically checkpoints before a command runs, so if an agent deletes or mutates the wrong files, you can roll back instantly.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;rm -rf ./build safeshell rollback --last &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;No sandbox, VM, or root&lt;/p&gt; &lt;p&gt;Hard-link snapshots (minimal overhead)&lt;/p&gt; &lt;p&gt;Single Go binary (macOS + Linux)&lt;/p&gt; &lt;p&gt;MCP support&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/qhkm/safeshell"&gt;https://github.com/qhkm/safeshell&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Curious how others are handling filesystem safety for local agents.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/qhkmdev90"&gt; /u/qhkmdev90 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkphs3/undo_for_destructive_shell_commands_used_by_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkphs3/undo_for_destructive_shell_commands_used_by_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pkphs3/undo_for_destructive_shell_commands_used_by_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T11:22:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkr9ak</id>
    <title>Emoji Translator: Convert English to Expressive Emoji Sequences üé≠ (Fun Side Project)</title>
    <updated>2025-12-12T12:59:08+00:00</updated>
    <author>
      <name>/u/ReplacementMoney2484</name>
      <uri>https://old.reddit.com/user/ReplacementMoney2484</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I built a fun open-source tool called the Emoji Translator that converts English sentences into expressive emoji sequences, instead of a simple dictionary lookup (like replacing &amp;quot;cat&amp;quot; with üê±), I fine-tuned BART-Large using LoRA so it actually understands context and sentiment.&lt;/p&gt; &lt;h1&gt;Some funny/interesting results:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&amp;quot;I feel misunderstood.&amp;quot; ‚Üí ü§¨üò¨&lt;/li&gt; &lt;li&gt;&amp;quot;I am happy.&amp;quot; ‚Üí üòÅü§ò&lt;/li&gt; &lt;li&gt;&amp;quot;My parents want to have a new baby&amp;quot; ‚Üí üë∂üë™ü§∞&lt;/li&gt; &lt;li&gt;&amp;quot;I tweeted the news to my followers.&amp;quot; ‚Üí ü§≥ü§†ü§≥&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Technicals for the nerds:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Dataset:&lt;/strong&gt; I used &lt;em&gt;Gemini 3 Pro&lt;/em&gt; to generate a synthetic dataset because scraping clean emoji data is hard.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Training:&lt;/strong&gt; I implemented Curriculum Learning with 6 stages of difficulty. I started by teaching the model simple object-emoji pairs and progressively introduced complex sentences and abstract concepts. This helped stabilize convergence significantly compared to throwing all the data at it at once.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Try it out:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Live Demo:&lt;/strong&gt; &lt;a href="https://huggingface.co/spaces/mohamedmostafa259/emoji-translator-demo"&gt;HuggingFace Space&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/mohamedmostafa259/emoji-translator"&gt;mohamedmostafa259/emoji-translator&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model:&lt;/strong&gt; &lt;a href="https://huggingface.co/mohamedmostafa259/bart-emoji-translator"&gt;HuggingFace Hub&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Dataset:&lt;/strong&gt; &lt;a href="https://www.kaggle.com/datasets/mohamedmostafa259/english-to-emoji"&gt;Kaggle Dataset&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Training notebook:&lt;/strong&gt; &lt;a href="https://www.kaggle.com/code/mohamedmostafa259/emoji-translator-curriculum-learning"&gt;Kaggle Notebook&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It's completely open source. Would love to see what weird translations you can get it to generate!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ReplacementMoney2484"&gt; /u/ReplacementMoney2484 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkr9ak/emoji_translator_convert_english_to_expressive/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkr9ak/emoji_translator_convert_english_to_expressive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pkr9ak/emoji_translator_convert_english_to_expressive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T12:59:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkbwco</id>
    <title>EQ-Bench updates: Gpt-5.2, Opus 4.5, Mistral Large 3 and Nanbeige4-3B</title>
    <updated>2025-12-11T23:06:43+00:00</updated>
    <author>
      <name>/u/_sqrkl</name>
      <uri>https://old.reddit.com/user/_sqrkl</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkbwco/eqbench_updates_gpt52_opus_45_mistral_large_3_and/"&gt; &lt;img alt="EQ-Bench updates: Gpt-5.2, Opus 4.5, Mistral Large 3 and Nanbeige4-3B" src="https://b.thumbs.redditmedia.com/jSd0TbO_srnflwRt8ojBram2pVNjfZ3rVlBCwbCJcHQ.jpg" title="EQ-Bench updates: Gpt-5.2, Opus 4.5, Mistral Large 3 and Nanbeige4-3B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://eqbench.com"&gt;https://eqbench.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;gpt-5.2 writing samples: &lt;/p&gt; &lt;p&gt;&lt;a href="https://eqbench.com/results/creative-writing-v3/gpt-5.2.html"&gt;https://eqbench.com/results/creative-writing-v3/gpt-5.2.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;opus-4.5 writing samples:&lt;/p&gt; &lt;p&gt;&lt;a href="https://eqbench.com/results/creative-writing-v3/claude-opus-4-5-20251101.html"&gt;https://eqbench.com/results/creative-writing-v3/claude-opus-4-5-20251101.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;mistral-large-3 writing samples:&lt;/p&gt; &lt;p&gt;&lt;a href="https://eqbench.com/results/creative-writing-v3/mistralai__Mistral-Large-3-675B-Instruct-2512.html"&gt;https://eqbench.com/results/creative-writing-v3/mistralai__Mistral-Large-3-675B-Instruct-2512.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;nanbeige4-3b writing samples:&lt;/p&gt; &lt;p&gt;&lt;a href="https://eqbench.com/results/creative-writing-v3/Nanbeige__Nanbeige4-3B-Thinking-2511.html"&gt;https://eqbench.com/results/creative-writing-v3/Nanbeige__Nanbeige4-3B-Thinking-2511.html&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_sqrkl"&gt; /u/_sqrkl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pkbwco"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkbwco/eqbench_updates_gpt52_opus_45_mistral_large_3_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pkbwco/eqbench_updates_gpt52_opus_45_mistral_large_3_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-11T23:06:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1pjw7rj</id>
    <title>Mistral‚Äôs Vibe CLI now supports a 200K token context window (previously 100K)</title>
    <updated>2025-12-11T12:23:44+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pjw7rj/mistrals_vibe_cli_now_supports_a_200k_token/"&gt; &lt;img alt="Mistral‚Äôs Vibe CLI now supports a 200K token context window (previously 100K)" src="https://external-preview.redd.it/ZnNsb2d0dzFpazZnMZt0kKC274AvCvOpM9k0UQCIyB1BQvPjsN5T3o1kO8eQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=841cfe71df83eebc90bd5a8915c65e4a8693db6c" title="Mistral‚Äôs Vibe CLI now supports a 200K token context window (previously 100K)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/4nxnq6w1ik6g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pjw7rj/mistrals_vibe_cli_now_supports_a_200k_token/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pjw7rj/mistrals_vibe_cli_now_supports_a_200k_token/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-11T12:23:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkjx5y</id>
    <title>Agentic coding with 32GB of VRAM.. is it doable?</title>
    <updated>2025-12-12T05:29:18+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Theres some solid models that run at this size, but for agentic coding I consider 60K context the bare minimum to get a good number of iterations in on a microservice.&lt;/p&gt; &lt;p&gt;Assuming I can tolerate Q8/Q8 kv cache quantization.. what's the best model I can run that'll fit 60K confidently?&lt;/p&gt; &lt;p&gt;Qwen3-VL-32B runs, but to hit 60K I need to drop down to iq4_xs, and that's introducing frequent errors that Q5 and Q6 don't encounter.&lt;/p&gt; &lt;p&gt;Qwen3-30B-Coder is in a somewhat similar spot only it's faster and works slightly worse with these tools.&lt;/p&gt; &lt;p&gt;Qwen3-Next works great but since I need CPU offloading to start with, prompt processing quickly becomes unacceptably slow.&lt;/p&gt; &lt;p&gt;Anything smaller I've tried fails to adhere to the lengthy 10k token system prompts or enters an infinite loop.&lt;/p&gt; &lt;p&gt;Any suggestions? Is it doable?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkjx5y/agentic_coding_with_32gb_of_vram_is_it_doable/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkjx5y/agentic_coding_with_32gb_of_vram_is_it_doable/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pkjx5y/agentic_coding_with_32gb_of_vram_is_it_doable/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T05:29:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkndwc</id>
    <title>Chat bots up to 24B</title>
    <updated>2025-12-12T09:06:00+00:00</updated>
    <author>
      <name>/u/PsychologicalMud210</name>
      <uri>https://old.reddit.com/user/PsychologicalMud210</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I like to chat about random subjects with AI. It serves more as an aid to thought and sometimes they are really helpful. Subjects may be sensitive, so I like to run local. &lt;/p&gt; &lt;p&gt;What are the best models up to about 24B that I can use? In your experience, what exactly this model does best?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PsychologicalMud210"&gt; /u/PsychologicalMud210 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkndwc/chat_bots_up_to_24b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkndwc/chat_bots_up_to_24b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pkndwc/chat_bots_up_to_24b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T09:06:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkflfw</id>
    <title>Run Mistral Devstral 2 locally Guide + Fixes! (25GB RAM) - Unsloth</title>
    <updated>2025-12-12T01:56:20+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkflfw/run_mistral_devstral_2_locally_guide_fixes_25gb/"&gt; &lt;img alt="Run Mistral Devstral 2 locally Guide + Fixes! (25GB RAM) - Unsloth" src="https://preview.redd.it/1f2wim2zgl6g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=884f4f99a6d0cca98f924c0f9b62f3ea56ac95cb" title="Run Mistral Devstral 2 locally Guide + Fixes! (25GB RAM) - Unsloth" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1f2wim2zgl6g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkflfw/run_mistral_devstral_2_locally_guide_fixes_25gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pkflfw/run_mistral_devstral_2_locally_guide_fixes_25gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T01:56:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1pko16f</id>
    <title>7B MoE with 1B active</title>
    <updated>2025-12-12T09:49:48+00:00</updated>
    <author>
      <name>/u/lossless-compression</name>
      <uri>https://old.reddit.com/user/lossless-compression</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I found that models in that range are relatively rare,I found some models such as (may not be exactly 7B and exactly 1B activated but in that range) are&lt;/p&gt; &lt;ul&gt; &lt;li&gt;1- Granite-4-tiny&lt;/li&gt; &lt;li&gt;2- LFM2-8B-A1B&lt;/li&gt; &lt;li&gt;3- Trinity-nano 6B&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Most of SLMs that are in that range are made of high amount of experts (tiny experts) where larger amount of experts gets activated but the overall parameters activated are ~1B so the model can specialize well.&lt;/p&gt; &lt;p&gt;I really wonder why that range isn't popular,I tried those models and Trinity nano is a very good researcher and it got a good character too and I asked a few general question it answered well,LFM feels like a RAG model even the standard one,it feels so robotic and answers are not the best,even the 350M can be coherent but it still feels like a RAG model, didn't test Granite 4 tiny yet.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lossless-compression"&gt; /u/lossless-compression &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pko16f/7b_moe_with_1b_active/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pko16f/7b_moe_with_1b_active/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pko16f/7b_moe_with_1b_active/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T09:49:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkhudf</id>
    <title>US Administration Issues Executive Order Opposing State-Level Regulation of AI Industry</title>
    <updated>2025-12-12T03:42:45+00:00</updated>
    <author>
      <name>/u/ttkciar</name>
      <uri>https://old.reddit.com/user/ttkciar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The EO:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.whitehouse.gov/presidential-actions/2025/12/eliminating-state-law-obstruction-of-national-artificial-intelligence-policy/"&gt;https://www.whitehouse.gov/presidential-actions/2025/12/eliminating-state-law-obstruction-of-national-artificial-intelligence-policy/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;My take: The EO orders the US AG to set up a task force to sue states which have legislated their own AI industry regulations, orders other agencies to prepare a report on how states might be denied federal funds, and orders that a set of recommendations be made to Congress to draft and pass new laws.&lt;/p&gt; &lt;p&gt;It seems like Christmas came early for commercial inference services, this year.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ttkciar"&gt; /u/ttkciar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkhudf/us_administration_issues_executive_order_opposing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkhudf/us_administration_issues_executive_order_opposing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pkhudf/us_administration_issues_executive_order_opposing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T03:42:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkhzf0</id>
    <title>Reverse-Engineering the RK3588 NPU: Hacking Memory Limits to run massive Vision Transformers</title>
    <updated>2025-12-12T03:49:27+00:00</updated>
    <author>
      <name>/u/one_does_not_just</name>
      <uri>https://old.reddit.com/user/one_does_not_just</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I worked on a &amp;quot;fun&amp;quot; project for my grad school class. I decided to write a blog post about it, maybe its useful to someone who is dealing with problems deploying vision transformers on edge devices&lt;/p&gt; &lt;p&gt;&lt;a href="https://amohan.dev/blog/2025/shard-optimizing-vision-transformers-edge-npu/"&gt;https://amohan.dev/blog/2025/shard-optimizing-vision-transformers-edge-npu/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Edit: Removed massive from title, but reddit won't let me change title, sorry about that&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/one_does_not_just"&gt; /u/one_does_not_just &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkhzf0/reverseengineering_the_rk3588_npu_hacking_memory/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkhzf0/reverseengineering_the_rk3588_npu_hacking_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pkhzf0/reverseengineering_the_rk3588_npu_hacking_memory/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T03:49:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1pk0ubn</id>
    <title>New in llama.cpp: Live Model Switching</title>
    <updated>2025-12-11T15:49:43+00:00</updated>
    <author>
      <name>/u/paf1138</name>
      <uri>https://old.reddit.com/user/paf1138</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pk0ubn/new_in_llamacpp_live_model_switching/"&gt; &lt;img alt="New in llama.cpp: Live Model Switching" src="https://external-preview.redd.it/8Hy799ws5wvJKYaRb__KN0TGXYxiPxKG6PuG-1SlIWg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2a43f5804fb810225237c9c37046b91c9bbb6451" title="New in llama.cpp: Live Model Switching" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paf1138"&gt; /u/paf1138 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/blog/ggml-org/model-management-in-llamacpp"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pk0ubn/new_in_llamacpp_live_model_switching/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pk0ubn/new_in_llamacpp_live_model_switching/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-11T15:49:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkdkjo</id>
    <title>Agentic Local AI on CPU = Mistral Vibe + Granite-4-h-1b</title>
    <updated>2025-12-12T00:22:10+00:00</updated>
    <author>
      <name>/u/PotentialFunny7143</name>
      <uri>https://old.reddit.com/user/PotentialFunny7143</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkdkjo/agentic_local_ai_on_cpu_mistral_vibe_granite4h1b/"&gt; &lt;img alt="Agentic Local AI on CPU = Mistral Vibe + Granite-4-h-1b" src="https://external-preview.redd.it/NDYwbGgydmYybzZnMf2LvdJmBzIyNzEDfN0eOt2yDrF46dRxJq4WcX4O0NUM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4363f44505584728345cc48958c232a7ab91036f" title="Agentic Local AI on CPU = Mistral Vibe + Granite-4-h-1b" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A a3b LLM is all you need :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PotentialFunny7143"&gt; /u/PotentialFunny7143 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/vewmcluf2o6g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkdkjo/agentic_local_ai_on_cpu_mistral_vibe_granite4h1b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pkdkjo/agentic_local_ai_on_cpu_mistral_vibe_granite4h1b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T00:22:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkpsee</id>
    <title>Training an LLM only on 1800s London texts - 90GB dataset</title>
    <updated>2025-12-12T11:40:11+00:00</updated>
    <author>
      <name>/u/Remarkable-Trick-177</name>
      <uri>https://old.reddit.com/user/Remarkable-Trick-177</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, you may have seen a few of my posts here a couple months ago. If not, hi. I‚Äôm working on an open source project called TimeCapsuleLLM, where I train LLMs from scratch using only 1800-1875 London texts.&lt;/p&gt; &lt;p&gt;Until recently most of my work has been done on a small scale but over the past 3 months I‚Äôve been working on a much larger dataset for the next model. My newest dataset is 90GB with 135,000 documents, it contains basically every usable document that I could find on the Internet Archive for that time period. &lt;/p&gt; &lt;p&gt;Before doing any training, I ran an inspection across every file and generated a bias report covering temporal bias, gender/pronoun bias and geographic bias. Given the time period it‚Äôs strongly biased, but it‚Äôs important to study this. You can find the report on my GitHub if anyone wants to take a look. I‚Äôve also trained a small evaluation model on a 15GB subset to evaluate the dataset before I scale up to all 90GB. It‚Äôs a LlaMA style model (300M parameters) trained to 10K steps. Example output: &lt;/p&gt; &lt;p&gt;Prompt: Who is Charles Dickens?&lt;/p&gt; &lt;p&gt;Output with fixed spacing: ‚ÄúWho is Charles Dickens? Does that work more of his excellent stirring, in his plays, in the Great Company's farm? What I have yet to quote from Jack Pickett? Do you not know that they were a species of galloping, or sawing of their breasts, or what was to be done about the time when Jackson was looking on the window? What is the success of an Englishman, and which his son has not been discovering to me, whereby to accomplish such a weight? Did you ever make a passage into the old roadway, or to an anchor-breeze at the foot of our boat, which you must leave us? The fact is, that whether the wind would rise up from the plain on Saturday night or noontide, or till the north, or otherwise, we shall be compelled to describe a formidable barrier, with the same effects as the present. In this situation, at least, it is not too much to say that we have left that room. I believe there are three copies in the 'Five Hundred-fold,' to be referred to, as the first number of our readers who wish to.‚Äù&lt;/p&gt; &lt;p&gt;This type of output is expected since 10,000 steps is very early and it‚Äôs not a QA model. The model has already learned long, winding sentence structures, but can‚Äôt connect ideas logically yet. The main goal here was to see how clean the output would be. &lt;/p&gt; &lt;p&gt;One issue that came up was with the tokenizer, it over-split the text, splitting words into individual characters and subparts. So the model by default gives output like this: &lt;/p&gt; &lt;p&gt;Original output: ‚ÄúW ho is Charles D ic ens ? D oes that work more of h ise x cell ent st ir ring , in his pl ays , int he G reat C omp any 's f arm ? What I have y et to qu ote from J ack P ick ett ?‚Äù&lt;/p&gt; &lt;p&gt;It doubled the tokens for the same amount of data, making learning harder. Next steps are training another eval model and then scaling to the full 90GB dataset for a 1.2B parameter model. The eval model is already on Hugging Face and you can find a run script for it on my GitHub. I‚Äôll upload the 15GB subset to Hugging Face once the tokenizer is corrected.&lt;/p&gt; &lt;p&gt;I also want to thank everyone in this subreddit. This is the only place I‚Äôve shared the project other than github, and a lot of the early guidance came directly from here. I really appreciate how generous people here have been with advice. More updates soon.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/haykgrigo3/TimeCapsuleLLM"&gt;haykgrigo3/TimeCapsuleLLM: A LLM trained only on data from certain time periods to reduce modern bias&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/haykgrigorian/v2mini-eval1"&gt;haykgrigorian/v2mini-eval1 ¬∑ Hugging Face&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Remarkable-Trick-177"&gt; /u/Remarkable-Trick-177 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkpsee/training_an_llm_only_on_1800s_london_texts_90gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkpsee/training_an_llm_only_on_1800s_london_texts_90gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pkpsee/training_an_llm_only_on_1800s_london_texts_90gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T11:40:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkidf6</id>
    <title>What is the smartest uncensored nsfw LLM you can run with 12GB VRAM and 32GB RAM?</title>
    <updated>2025-12-12T04:08:01+00:00</updated>
    <author>
      <name>/u/Dex921</name>
      <uri>https://old.reddit.com/user/Dex921</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I don't know if it's allowed, but I am asking about ALL available LLMs including ones that are closed source and cannot be run locally (like chatgpt or gemini, and in that case obviously the ram limit doesn't apply)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dex921"&gt; /u/Dex921 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkidf6/what_is_the_smartest_uncensored_nsfw_llm_you_can/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkidf6/what_is_the_smartest_uncensored_nsfw_llm_you_can/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pkidf6/what_is_the_smartest_uncensored_nsfw_llm_you_can/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T04:08:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkpxss</id>
    <title>Someone from NVIDIA made a big mistake and uploaded the parent folder of their upcoming model on Hugging Face</title>
    <updated>2025-12-12T11:49:10+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkpxss/someone_from_nvidia_made_a_big_mistake_and/"&gt; &lt;img alt="Someone from NVIDIA made a big mistake and uploaded the parent folder of their upcoming model on Hugging Face" src="https://preview.redd.it/7r3bnj5ugr6g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0c3d5909063dd5ce912e8ebc203168db53b765be" title="Someone from NVIDIA made a big mistake and uploaded the parent folder of their upcoming model on Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From Xeophon on ùïè: &lt;a href="https://x.com/xeophon_/status/1999394570967089630"&gt;https://x.com/xeophon_/status/1999394570967089630&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7r3bnj5ugr6g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkpxss/someone_from_nvidia_made_a_big_mistake_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pkpxss/someone_from_nvidia_made_a_big_mistake_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T11:49:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
