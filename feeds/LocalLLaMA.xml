<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-18T16:25:00+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1o9wn6x</id>
    <title>Codex-Cli with Qwen3-Coder</title>
    <updated>2025-10-18T14:02:23+00:00</updated>
    <author>
      <name>/u/chibop1</name>
      <uri>https://old.reddit.com/user/chibop1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was able to &lt;a href="https://github.com/openai/codex/blob/main/docs/config.md"&gt;add Ollama as a model provider&lt;/a&gt;, and Codex-CLI was successfully able to talk to Ollama.&lt;/p&gt; &lt;p&gt;When I use GPT-OSS-20b, it goes back and forth until completing the task.&lt;/p&gt; &lt;p&gt;I was hoping to use Qwen3-Coder-30b for better quality, but often it stops after a few turns—it’ll say something like “let me do X,” but then doesn’t execute it.&lt;/p&gt; &lt;p&gt;The repo only has a few files, and I’ve set the context size to 65k. It should have plenty room to keep going.&lt;/p&gt; &lt;p&gt;My guess is that Qwen3-Coder often responds without actually invoking tool calls to proceed?&lt;/p&gt; &lt;p&gt;Any thoughts would be appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chibop1"&gt; /u/chibop1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9wn6x/codexcli_with_qwen3coder/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9wn6x/codexcli_with_qwen3coder/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o9wn6x/codexcli_with_qwen3coder/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-18T14:02:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1o9agyg</id>
    <title>New model from inclusionAI - LLaDA2.0-mini-preview</title>
    <updated>2025-10-17T19:17:48+00:00</updated>
    <author>
      <name>/u/edward-dev</name>
      <uri>https://old.reddit.com/user/edward-dev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9agyg/new_model_from_inclusionai_llada20minipreview/"&gt; &lt;img alt="New model from inclusionAI - LLaDA2.0-mini-preview" src="https://external-preview.redd.it/xv_Z1skcqtXjop4d-0l1Usyn5XM0UbKgNLHO0wCID8I.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f43fd9b3305330d175918ca63404b046858d8a03" title="New model from inclusionAI - LLaDA2.0-mini-preview" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;LLaDA2-mini-preview is a diffusion language model featuring a 16BA1B Mixture-of-Experts (MoE) architecture. As an enhanced, instruction-tuned iteration of the LLaDA series, it is optimized for practical applications.&lt;/p&gt; &lt;p&gt;From the benchmarks the preview looks 'not as good' as ling mini 2.0, but it's still a preview, not the final model, and this is a diffusion language model which makes it interesting &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/edward-dev"&gt; /u/edward-dev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/inclusionAI/LLaDA2.0-mini-preview"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9agyg/new_model_from_inclusionai_llada20minipreview/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o9agyg/new_model_from_inclusionai_llada20minipreview/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T19:17:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1o99s2u</id>
    <title>ROCm 7.0 Install for Mi50 32GB | Ubuntu 24.04 LTS</title>
    <updated>2025-10-17T18:51:45+00:00</updated>
    <author>
      <name>/u/legit_split_</name>
      <uri>https://old.reddit.com/user/legit_split_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o99s2u/rocm_70_install_for_mi50_32gb_ubuntu_2404_lts/"&gt; &lt;img alt="ROCm 7.0 Install for Mi50 32GB | Ubuntu 24.04 LTS" src="https://external-preview.redd.it/QZgvW0JuHPNTo3BakCvCal_DO30UZr-SjSZAX09mNCA.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6e9ed4a0e6428b54b70c9ffefc80e3952a1b20e2" title="ROCm 7.0 Install for Mi50 32GB | Ubuntu 24.04 LTS" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I shared a comment on how to do this &lt;a href="https://www.reddit.com/r/linux4noobs/comments/1ly8rq6/comment/nb9uiye/"&gt;here&lt;/a&gt;, but I still see people asking for help so I decided to make a video tutorial.&lt;/p&gt; &lt;h1&gt;Text guide:&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;Copy &amp;amp; paste all the commands from the quick install &lt;a href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/quick-start.html"&gt;https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/quick-start.html&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Before rebooting to complete the install, download the 6.4 rocblas from the AUR: &lt;a href="https://archlinux.org/packages/extra/x86_64/rocblas/"&gt;https://archlinux.org/packages/extra/x86_64/rocblas/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Extract it &lt;/li&gt; &lt;li&gt;Copy all tensor files that contain gfx906 in &lt;code&gt;rocblas-6.4.3-3-x86_64.pkg/opt/rocm/lib/rocblas/library&lt;/code&gt; to &lt;code&gt;/opt/rocm/lib/rocblas/library&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Reboot&lt;/li&gt; &lt;li&gt;Check if it worked by running sudo update-alternatives --display rocm&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&amp;#8203;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# To build llama.cpp with ROCm + flash attention (adjust j value according to number of threads): HIPCXX=&amp;quot;$(hipconfig -l)/clang&amp;quot; HIP_PATH=&amp;quot;$(hipconfig -R)&amp;quot; \ cmake -S . -B build -DGGML_HIP=ON -DAMDGPU_TARGETS=gfx906 -DGGML_HIP_ROCWMMA_FATTN=ON -DCMAKE_BUILD_TYPE=Release \ &amp;amp;&amp;amp; cmake --build build --config Release -- -j 16 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note: This guide can be adapted for 6.4 if more stability is needed when working with PyTorch or vllm. Most performance improvements were already present in 6.4 (roughly 20-30% over 6.3), so 7.0.2 serves to offer more compatibility together with the latest AMD cards :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/legit_split_"&gt; /u/legit_split_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=xcI0pyE8VN8"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o99s2u/rocm_70_install_for_mi50_32gb_ubuntu_2404_lts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o99s2u/rocm_70_install_for_mi50_32gb_ubuntu_2404_lts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T18:51:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1o9g4if</id>
    <title>Ling-1T-GGUF on ik_llama.cpp</title>
    <updated>2025-10-17T23:09:36+00:00</updated>
    <author>
      <name>/u/VoidAlchemy</name>
      <uri>https://old.reddit.com/user/VoidAlchemy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9g4if/ling1tgguf_on_ik_llamacpp/"&gt; &lt;img alt="Ling-1T-GGUF on ik_llama.cpp" src="https://external-preview.redd.it/R-9hsMR8vUhTujexAftojKeBUYgoMQ0LKYIfxmVtxHE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=98540d3830dcf459c8003a9578801440c349b88b" title="Ling-1T-GGUF on ik_llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'll try to fixup the namespace ASAP but wanted to rush out some test quants of Ling-1T 1000B model. For now you'll need roughly 256GiB RAM + 24-32+ GiB VRAM to fit the available quants. Hope to release more after fixing up the 403 uploading issues.&lt;/p&gt; &lt;p&gt;Big thanks to ik and CISC for all the help figuring out how to quantize this beast, and of course thanks to Wendell at level1techs for the hardware support and also the aifoundry folks supporting me to come out to SF for the upcoming AI Plumbers Unconference next week!&lt;/p&gt; &lt;p&gt;In early testing I got out to roughly 40k context depth in ~6 turns of chat and it was doing okay reading some papers and generating diff patches without going off the rails at least.&lt;/p&gt; &lt;p&gt;Please give it a test and lemme know what you find!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VoidAlchemy"&gt; /u/VoidAlchemy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/ubergarm2/Ling-1T-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9g4if/ling1tgguf_on_ik_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o9g4if/ling1tgguf_on_ik_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T23:09:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1o9glvt</id>
    <title>Diagnosing layer sensitivity during post training quantization</title>
    <updated>2025-10-17T23:31:57+00:00</updated>
    <author>
      <name>/u/elinaembedl</name>
      <uri>https://old.reddit.com/user/elinaembedl</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9glvt/diagnosing_layer_sensitivity_during_post_training/"&gt; &lt;img alt="Diagnosing layer sensitivity during post training quantization" src="https://preview.redd.it/ibr4vouwarvf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=79ed43ec6d43bdb26d7c463a7a28500778a67f70" title="Diagnosing layer sensitivity during post training quantization" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have written a blog post on using layerwise PSNR to diagnose where models break during post-training quantization.&lt;/p&gt; &lt;p&gt;Instead of only checking output accuracy, layerwise metrics let you spot exactly which layers are sensitive (e.g. softmax, SE blocks), making it easier to debug and decide what to keep in higher precision.&lt;/p&gt; &lt;p&gt;If you’re experimenting with quantization for local or edge inference, you might find this interesting:&lt;br /&gt; &lt;a href="https://hub.embedl.com/blog/diagnosing-layer-sensitivity"&gt;https://hub.embedl.com/blog/diagnosing-layer-sensitivity&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to hear if anyone has tried similar layerwise diagnostics.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/elinaembedl"&gt; /u/elinaembedl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ibr4vouwarvf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9glvt/diagnosing_layer_sensitivity_during_post_training/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o9glvt/diagnosing_layer_sensitivity_during_post_training/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T23:31:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1o96mwq</id>
    <title>Using llamacpp and RCP, managed to improve promt processing by 4x times (160 t/s to 680 t/s) and text generation by 2x times (12.67 t/s to 22.52 t/s) by changing the device order including RPC. GLM 4.6 IQ4_XS multiGPU + RPC.</title>
    <updated>2025-10-17T16:52:21+00:00</updated>
    <author>
      <name>/u/panchovix</name>
      <uri>https://old.reddit.com/user/panchovix</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello guys, hoping you're having a good day.&lt;/p&gt; &lt;p&gt;As you know, llamacpp has RPC since time ago.&lt;/p&gt; &lt;p&gt;I have 2 PCs in my home:&lt;/p&gt; &lt;p&gt;My &amp;quot;Server&amp;quot;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;AM5 MSI X670E Carbon&lt;/li&gt; &lt;li&gt;AMD Ryzen 9 9900X&lt;/li&gt; &lt;li&gt;192GB DDR5 6000Mhz CL32&lt;/li&gt; &lt;li&gt;7 GPUs &lt;ul&gt; &lt;li&gt;5090x2&lt;/li&gt; &lt;li&gt;4090x2&lt;/li&gt; &lt;li&gt;A6000&lt;/li&gt; &lt;li&gt;3090x2&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;MCX314A-BCCT 40Gbps NIC (totally overkill, prob 10Gbps is fine)&lt;/li&gt; &lt;li&gt;OS: Fedora 42&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;And my &amp;quot;Gaming&amp;quot; PC:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;AM5 Gigabyte X670 Aorus Master (I wouldn't recommend this board btw)&lt;/li&gt; &lt;li&gt;AMD Ryzen 7 7800X3D&lt;/li&gt; &lt;li&gt;64GB DDR5 6000Mhz CL30&lt;/li&gt; &lt;li&gt;RTX 5090&lt;/li&gt; &lt;li&gt;MCX314A-BCCT 40Gbps NIC&lt;/li&gt; &lt;li&gt;OS: Windows 11&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;PC1 and PC2 (Server and Gaming) are connected via the MCX314A-BCCT 40Gbps NIC. As info, the max bandwidth used I have seen on llamacpp was about 10-11 Gbps when loading the model (I think here I'm either SSD bound or CPU bound) and about 3-4 Gbps on first prompt processing.&lt;/p&gt; &lt;p&gt;So for the test, I &amp;quot;disabled&amp;quot; one 3090 and replaced it layers with my 5090 via RPC.&lt;/p&gt; &lt;p&gt;I'm running GLM 4.6 IQ4_XS (~180GB) with (very complex, don't judge me):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;LLAMA_SET_ROWS=1 ./llama-server \ -m '/models/GLM-4.6-IQ4_XS.gguf' \ -c 32768 \ --no-mmap \ --rpc 192.168.50.2:50052 \ -ngl 999 \ -ot &amp;quot;blk.(0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15).ffn.=CUDA0&amp;quot; \ -ot &amp;quot;blk.(16|17|18|19|20|21|22|23|24|25).ffn.=CUDA1&amp;quot; \ -ot &amp;quot;blk.(27|28|29|30|31|32|33|34|35|36).ffn.=CUDA2&amp;quot; \ -ot &amp;quot;blk.(38|39|40|41|42|43|44|45|46|47|48|49|50).ffn.=CUDA3&amp;quot; \ -ot &amp;quot;blk.(51|52|53|54|55|56|57|58|59).ffn.=CUDA4&amp;quot; \ -ot &amp;quot;blk.(61|62|63|64|65|66|67|68|69|70).ffn.=RPC0[192.168.50.2:50052]&amp;quot; \ -ot &amp;quot;blk.(72|73|74|75|76|77|78|79|80|81|82|83|84|85|86|87|88|89|90|91).ffn.=CUDA5&amp;quot; \ -ot &amp;quot;blk.26.ffn_(norm|gate_inp|gate_shexp|down_shexp|up_shexp).weight=CUDA1&amp;quot; \ -ot &amp;quot;blk.26.ffn_gate_exps.weight=CUDA1&amp;quot; \ -ot &amp;quot;blk.26.ffn_(down_exps|up_exps).weight=CUDA0&amp;quot; \ -ot &amp;quot;blk.37.ffn_(norm|gate_inp|gate_shexp|down_shexp|up_shexp).weight=CUDA2&amp;quot; \ -ot &amp;quot;blk.37.ffn_gate_exps.weight=CUDA2&amp;quot; \ -ot &amp;quot;blk.37.ffn_(down_exps|up_exps).weight=CUDA3&amp;quot; \ -ot &amp;quot;blk.60.ffn_(norm|gate_inp|gate_shexp|down_shexp|up_shexp).weight=CUDA4&amp;quot; \ -ot &amp;quot;blk.60.ffn_gate_exps.weight=CUDA4&amp;quot; \ -ot &amp;quot;blk.60.ffn_(down_exps|up_exps).weight=CUDA5&amp;quot; \ -ot &amp;quot;blk.71.ffn_(norm|gate_inp|gate_shexp|down_shexp|up_shexp).weight=RPC0[192.168.50.2:50052]&amp;quot; \ -ot &amp;quot;blk.71.ffn_gate_exps.weight=RPC0[192.168.50.2:50052]&amp;quot; \ -ot &amp;quot;blk.71.ffn_(down_exps|up_exps).weight=CUDA5&amp;quot; \ -fa on \ -mg 0 \ -ub 1792 \ &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;By default, llamacpp assigns RPC devices as &lt;strong&gt;the first device,&lt;/strong&gt; this means that the RPC device has the bigger buffers and also has to do more processing that the server itself.&lt;/p&gt; &lt;p&gt;So it is like, by the --devices parameters in this case, use:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;--device RPC0,CUDA0,CUDA1,CUDA2,CUDA3,CUDA4,CUDA5&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;And I was getting these speeds:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;prompt eval time = 27661.35 ms / 4410 tokens ( 6.27 ms per token, 159.43 tokens per second) eval time = 140832.84 ms / 1784 tokens ( 78.94 ms per token, 12.67 tokens per second) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;So, I started a question on github here &lt;a href="https://github.com/ggml-org/llama.cpp/discussions/16625"&gt;https://github.com/ggml-org/llama.cpp/discussions/16625&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And &lt;a href="https://github.com/abc-nix"&gt;abc-nix&lt;/a&gt; did the great suggestion to move it.&lt;/p&gt; &lt;p&gt;So then, used&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;--device CUDA0,CUDA1,CUDA2,CUDA3,CUDA4,RPC0,CUDA5&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;And got&lt;/p&gt; &lt;pre&gt;&lt;code&gt;prompt eval time = 6483.46 ms / 4410 tokens ( 1.47 ms per token, 680.19 tokens per second) eval time = 78029.06 ms / 1757 tokens ( 44.41 ms per token, 22.52 tokens per second) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Which is an absolutely insane performance bump.&lt;/p&gt; &lt;p&gt;Now I want to try to dual boot the &amp;quot;Gaming&amp;quot; PC to Linux to see if there's an improvement. As multiGPU by itself is really bad on Windows, not sure if that also affects RPC.&lt;/p&gt; &lt;p&gt;EDIT: If you wonder how do I connect so much on a consumer CPU:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;X16 split into X8/X4/X4 5.0 from CPU (5090 at X8 5.0, 4090/4090 at X4 4.0)&lt;/li&gt; &lt;li&gt;X4/X4 5.0 from CPU from top 2 M2 slots, to PCIe adapters (RTX 5090 at X4 5.0 and Cx314a NIC X4 3.0)&lt;/li&gt; &lt;li&gt;X4 4.0 from Chipset from bottom PCIe slot (RTX A6000)&lt;/li&gt; &lt;li&gt;X4/X4 4.0 from Chipset from bottom M2 slots, to PCIe adapters (3090/3090)&lt;/li&gt; &lt;li&gt;X1 3.0 from NFF Wifi to PCIe adapter (for now it's open, thinking what can I put there).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;EDIT2: For those wondering, I get no money return for this. I haven't rented and I haven't sold anything related to AI either. So just expenses.&lt;/p&gt; &lt;p&gt;EDIT3: I have confirmed this also works perfectly when offloading to CPU.&lt;/p&gt; &lt;p&gt;I.e. for DeepSeek V3, I ran:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;LLAMA_SET_ROWS=1 ./llama-server -m '/models_llm_2tb/DeepSeek-V3-0324-UD-Q3_K_XL.gguf' -c 32768 --no-mmap -ngl 999 \ --rpc 192.168.50.2:50052 \ -ot &amp;quot;blk.(0|1|2|3|4|5|6|7).ffn.=CUDA0&amp;quot; \ -ot &amp;quot;blk.(8|9|10).ffn.=CUDA1&amp;quot; \ -ot &amp;quot;blk.(11|12|13).ffn.=CUDA2&amp;quot; \ -ot &amp;quot;blk.(14|15|16|17|18).ffn.=CUDA3&amp;quot; \ -ot &amp;quot;blk.(19|20|21).ffn.=CUDA4&amp;quot; \ -ot &amp;quot;blk.(22|23|24).ffn.=RPC0[192.168.50.2:50052]&amp;quot; \ -ot &amp;quot;blk.(25|26|27|28|29|30|31).ffn.=CUDA5&amp;quot; \ -ot &amp;quot;blk.32.ffn_(norm|gate_inp|gate_shexp|down_shexp|up_shexp).weight=CUDA1&amp;quot; \ -ot &amp;quot;blk.32.ffn_gate_exps.weight=CUDA1&amp;quot; \ -ot &amp;quot;blk.32.ffn_down_exps.weight=CUDA1&amp;quot; \ -ot &amp;quot;blk.32.ffn_up_exps.weight=CUDA1&amp;quot; \ -ot &amp;quot;blk.33.ffn_(norm|gate_inp|gate_shexp|down_shexp|up_shexp).weight=CUDA2&amp;quot; \ -ot &amp;quot;blk.33.ffn_gate_exps.weight=CUDA2&amp;quot; \ -ot &amp;quot;blk.33.ffn_down_exps.weight=CUDA2&amp;quot; \ -ot &amp;quot;blk.33.ffn_up_exps.weight=CUDA2&amp;quot; \ -ot &amp;quot;blk.34.ffn_(norm|gate_inp|gate_shexp|down_shexp|up_shexp).weight=CUDA5&amp;quot; \ -ot &amp;quot;blk.34.ffn_gate_exps.weight=CUDA5&amp;quot; \ -ot &amp;quot;blk.34.ffn_down_exps.weight=CUDA5&amp;quot; \ -ot &amp;quot;blk.35.ffn_gate_exps.weight=CUDA3&amp;quot; \ -ot &amp;quot;blk.35.ffn_down_exps.weight=CUDA3&amp;quot; \ -ot &amp;quot;exps=CPU&amp;quot; \ -fa on -mg 0 -ub 2560 -b 2560 --device CUDA0,CUDA1,CUDA2,CUDA3,CUDA4,RPC0,CUDA5 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And got about ~10% less perf than connecting the 5090 directly into the server PC.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/panchovix"&gt; /u/panchovix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o96mwq/using_llamacpp_and_rcp_managed_to_improve_promt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o96mwq/using_llamacpp_and_rcp_managed_to_improve_promt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o96mwq/using_llamacpp_and_rcp_managed_to_improve_promt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T16:52:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1o98f57</id>
    <title>New from Cerebras: REAP the Experts: Why Pruning Prevails for One-Shot MoE compression</title>
    <updated>2025-10-17T17:59:47+00:00</updated>
    <author>
      <name>/u/ilzrvch</name>
      <uri>https://old.reddit.com/user/ilzrvch</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o98f57/new_from_cerebras_reap_the_experts_why_pruning/"&gt; &lt;img alt="New from Cerebras: REAP the Experts: Why Pruning Prevails for One-Shot MoE compression" src="https://external-preview.redd.it/WYinqoDP9OerKm8ljzpFUp26G03RA6wo-9izylOPBeM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6b54dc6d0fc61b0246d5aa15915fc1c876cfd68a" title="New from Cerebras: REAP the Experts: Why Pruning Prevails for One-Shot MoE compression" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TLDR: We show that one-shot pruning of experts in large MoEs is better than expert merging when looking at realistic benchmarks, not just perplexity measures. &lt;/p&gt; &lt;p&gt;Using a saliency criterion that measures expected routed contribution of each expert (REAP), we pruned Qwen3-Coder-480B to 363B (25% pruning) and 246B (50% pruning), all in FP8. At 25%, accuracy degradation is minimal across a suite of benchmarks.&lt;/p&gt; &lt;p&gt;Checkpoints on HF:&lt;br /&gt; &lt;a href="https://huggingface.co/cerebras/Qwen3-Coder-REAP-363B-A35B-FP8"&gt;https://huggingface.co/cerebras/Qwen3-Coder-REAP-363B-A35B-FP8&lt;/a&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/cerebras/Qwen3-Coder-REAP-246B-A35B-FP8"&gt;https://huggingface.co/cerebras/Qwen3-Coder-REAP-246B-A35B-FP8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;These can be run with vanilla vLLM, no patches required. &lt;/p&gt; &lt;p&gt;More evals and pruned models on the way!&lt;/p&gt; &lt;p&gt;Link to the paper: &lt;a href="https://arxiv.org/abs/2510.13999"&gt;https://arxiv.org/abs/2510.13999&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6zdkycxjnpvf1.png?width=6884&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ef2e6f9f61b89de730fa9c01d6774998dedee9d8"&gt;https://preview.redd.it/6zdkycxjnpvf1.png?width=6884&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ef2e6f9f61b89de730fa9c01d6774998dedee9d8&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ilzrvch"&gt; /u/ilzrvch &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o98f57/new_from_cerebras_reap_the_experts_why_pruning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o98f57/new_from_cerebras_reap_the_experts_why_pruning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o98f57/new_from_cerebras_reap_the_experts_why_pruning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T17:59:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1o9zgww</id>
    <title>Paper Share: Under Large Batches and High Concurrency, I’d Rather Try CISPO First</title>
    <updated>2025-10-18T15:55:47+00:00</updated>
    <author>
      <name>/u/Hairy-Librarian3796</name>
      <uri>https://old.reddit.com/user/Hairy-Librarian3796</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I saw people in the community mention Meta’s recent paper “The Art of Scaling Reinforcement Learning Compute for LLMs.” I had time to read it over the past two days, and one point really caught my eye: they discuss GRPO/DAPO/GSPO/CISPO along a single axis, with the focus largely on how to suppress variance and instability under large batches and high concurrency. My rough take:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;GRPO: simple to implement with low engineering overhead; but in highly off policy, large batch settings, its stability margin is more sensitive.&lt;/li&gt; &lt;li&gt;DAPO: some implementations introduce token level filtering or suppression, which does clean up some bad gradients; but on reasoning heavy samples, if thresholds or masking are set poorly, it may affect chain of thought continuity (implementation dependent, not inherent).&lt;/li&gt; &lt;li&gt;CISPO: following the minimal change route of PPO or GRPO, it applies clipped and normalized importance sampling weights, balancing scalability and steady state behavior. Under the configurations we have observed, it is more friendly in terms of controllability and reproducibility at large compute scales.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The difference with CISPO is that it does not drop tokens; instead, it applies clipping and normalization to the importance sampling weights. This compresses the long tail of extreme weights while keeping all samples on the gradient path. In practice, this tends to be friendlier to complex reasoning and yields more controllable stability; it is also easier to reproduce comparable results under high concurrency. More pragmatically, CISPO is very low intrusion. It addresses the source of instability and leaves the rest to the usual recipe: KL control, advantage normalization, weight normalization, and gradient clipping. For those running large scale training pipelines, this approach of not rewriting everything but instead polishing the critical parts is indeed more convenient.&lt;/p&gt; &lt;p&gt;To be frank, I am once again impressed by how quickly other teams are advancing along this line; the paper’s final scheme also adopts Minimax’s original algorithm. Tracing it back, they had in fact systematized the idea of clipped IS weights with normalization in their early M1 model. As to whether it is the optimal solution, I do not think we need to rush to a verdict. More importantly, it tackles the practical question of how RL scales compute and offers a low barrier, reproducible path.&lt;/p&gt; &lt;p&gt;Meta paper: &lt;a href="https://arxiv.org/abs/2510.13786"&gt;arXiv:2510.13786&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Minimax M1 model technical report: &lt;a href="https://arxiv.org/abs/2506.13585"&gt;arXiv:2506.13585&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hairy-Librarian3796"&gt; /u/Hairy-Librarian3796 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9zgww/paper_share_under_large_batches_and_high/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9zgww/paper_share_under_large_batches_and_high/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o9zgww/paper_share_under_large_batches_and_high/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-18T15:55:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8uxh6</id>
    <title>Write three times the word potato</title>
    <updated>2025-10-17T07:32:29+00:00</updated>
    <author>
      <name>/u/TooManyPascals</name>
      <uri>https://old.reddit.com/user/TooManyPascals</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8uxh6/write_three_times_the_word_potato/"&gt; &lt;img alt="Write three times the word potato" src="https://b.thumbs.redditmedia.com/VWK4WzyVVfvV7xJuANrLzK-bH1UfvcQckXM3kS4Llno.jpg" title="Write three times the word potato" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was testing how well Qwen3-0.6B could follow simple instructions... &lt;/p&gt; &lt;p&gt;and it accidentally created a trolling masterpiece.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TooManyPascals"&gt; /u/TooManyPascals &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1o8uxh6"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8uxh6/write_three_times_the_word_potato/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8uxh6/write_three_times_the_word_potato/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T07:32:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1o9i5rm</id>
    <title>Guys my Sparx Station won't start it just beeps. I can hear the 4.3gb hard drive spinning but nothing else.</title>
    <updated>2025-10-18T00:46:06+00:00</updated>
    <author>
      <name>/u/oodelay</name>
      <uri>https://old.reddit.com/user/oodelay</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9i5rm/guys_my_sparx_station_wont_start_it_just_beeps_i/"&gt; &lt;img alt="Guys my Sparx Station won't start it just beeps. I can hear the 4.3gb hard drive spinning but nothing else." src="https://external-preview.redd.it/OGM5NDY4aXZvcnZmMYsjotqartWvywDpSqT_oNbLP9E-6VMssWCFXA3Vaqho.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2f68845d8b086982b234c0efc5d38a18c4148581" title="Guys my Sparx Station won't start it just beeps. I can hear the 4.3gb hard drive spinning but nothing else." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/oodelay"&gt; /u/oodelay &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/fmvhojfvorvf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9i5rm/guys_my_sparx_station_wont_start_it_just_beeps_i/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o9i5rm/guys_my_sparx_station_wont_start_it_just_beeps_i/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-18T00:46:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1o96o9o</id>
    <title>RTX Pro 6000 Blackwell vLLM Benchmark: 120B Model Performance Analysis</title>
    <updated>2025-10-17T16:53:42+00:00</updated>
    <author>
      <name>/u/notaDestroyer</name>
      <uri>https://old.reddit.com/user/notaDestroyer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o96o9o/rtx_pro_6000_blackwell_vllm_benchmark_120b_model/"&gt; &lt;img alt="RTX Pro 6000 Blackwell vLLM Benchmark: 120B Model Performance Analysis" src="https://external-preview.redd.it/12ojQ9khZuJRm7jqdMaOtnKaFtBC6Yo7dfwq4qKZ3jA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4ae7c659a21f868f6dba51b958c810a90c5bfe24" title="RTX Pro 6000 Blackwell vLLM Benchmark: 120B Model Performance Analysis" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Hardware:&lt;/strong&gt; NVIDIA RTX Pro 6000 Blackwell Workstation Edition (96GB VRAM)&lt;br /&gt; &lt;strong&gt;Software:&lt;/strong&gt; vLLM 0.11.0 | CUDA 13.0 | Driver 580.82.09 | FP16/BF16&lt;br /&gt; &lt;strong&gt;Model:&lt;/strong&gt; openai/gpt-oss-120b source: &lt;a href="https://huggingface.co/openai/gpt-oss-120b"&gt;https://huggingface.co/openai/gpt-oss-120b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Ran two test scenarios with 500-token and 1000-2000-token outputs across varying context lengths (1K-128K) and concurrency levels (1-20 users).&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5io6r8cfcpvf1.png?width=6907&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=dfa2bf6d638fcf36f75be97745f4be59c5f5cade"&gt;500 tokens&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/1i5c8lcgcpvf1.png?width=6907&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f6949af68c70e0b95f2462dab8dc6c3a5be7943a"&gt;1000-2000 tokens&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Key Findings&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Peak Performance (500-token output):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;1051 tok/s&lt;/strong&gt; at 20 users, 1K context&lt;/li&gt; &lt;li&gt;Maintains &lt;strong&gt;300-476 tok/s&lt;/strong&gt; at 20 concurrent users across context lengths&lt;/li&gt; &lt;li&gt;TTFT: 200-400ms at low concurrency, scales to 2000-3000ms at 20 users&lt;/li&gt; &lt;li&gt;Average latency: 2.6s (1 user) → 30.2s (20 users) at 128K context&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Extended Output (1000-2000 tokens):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;1016 tok/s&lt;/strong&gt; peak throughput (minimal degradation vs 500-token)&lt;/li&gt; &lt;li&gt;Slightly higher latencies due to longer decode phases&lt;/li&gt; &lt;li&gt;Power draw: 300-600W depending on load&lt;/li&gt; &lt;li&gt;Batch scaling efficiency: &lt;strong&gt;EXCELLENT&lt;/strong&gt; at 2-5 users, still good up to 10 users&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Observations&lt;/h1&gt; &lt;p&gt;The Blackwell architecture handles this 120B model impressively well:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Linear scaling up to ~5 concurrent users&lt;/li&gt; &lt;li&gt;GPU clocks remain stable at 2800+ MHz under load&lt;/li&gt; &lt;li&gt;Inter-token latency stays in the &amp;quot;INSTANT&amp;quot; zone (&amp;lt;50ms) for most configurations&lt;/li&gt; &lt;li&gt;Context length scaling is predictable—throughput halves roughly every 32K context increase&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The 96GB VRAM headroom means no swapping even at 128K context with max concurrency.&lt;/p&gt; &lt;p&gt;Used: &lt;a href="https://github.com/notaDestroyer/vllm-benchmark-suite"&gt;https://github.com/notaDestroyer/vllm-benchmark-suite&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; If you're running 100B+ models locally, the RTX Pro 6000 Blackwell delivers production-grade throughput with excellent multi-user scaling. Power efficiency is reasonable given the compute density.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/notaDestroyer"&gt; /u/notaDestroyer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o96o9o/rtx_pro_6000_blackwell_vllm_benchmark_120b_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o96o9o/rtx_pro_6000_blackwell_vllm_benchmark_120b_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o96o9o/rtx_pro_6000_blackwell_vllm_benchmark_120b_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T16:53:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1o98m76</id>
    <title>NVIDIA sent me a 5090 so I can demo Qwen3-VL GGUF</title>
    <updated>2025-10-17T18:06:56+00:00</updated>
    <author>
      <name>/u/AlanzhuLy</name>
      <uri>https://old.reddit.com/user/AlanzhuLy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o98m76/nvidia_sent_me_a_5090_so_i_can_demo_qwen3vl_gguf/"&gt; &lt;img alt="NVIDIA sent me a 5090 so I can demo Qwen3-VL GGUF" src="https://b.thumbs.redditmedia.com/pBsmzNoWTR_PcJ6HlkTBSSZmdKgKd285MYT5zK95iGA.jpg" title="NVIDIA sent me a 5090 so I can demo Qwen3-VL GGUF" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;3 days ago. We &lt;a href="https://x.com/Alibaba_Qwen/status/1978154384098754943"&gt;partnered with the Qwen team&lt;/a&gt; so the new Qwen3-VL 4B &amp;amp; 8B models run &lt;strong&gt;day-0&lt;/strong&gt; with GGUF, MLX inside &lt;a href="https://github.com/NexaAI/nexa-sdk"&gt;NexaSDK&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; powered by our NexaML Engine — the first and only framework that supports Qwen3-VL GGUF right now. We just received a 5090 from the NVIDIA team and I want to show you how it runs on a 5090&lt;/p&gt; &lt;p&gt;Today, we also made it run &lt;em&gt;locally&lt;/em&gt; inside our desktop UI app Hyperlink, so everyone can try Qwen3VL on their device easily&lt;/p&gt; &lt;p&gt;I tried the same demo examples from the &lt;a href="https://qwen.ai/blog?id=250aaecfcd4828d55be2b2437a76d66a099860da&amp;amp;from=research.research-list"&gt;Qwen2.5-32B blog&lt;/a&gt;, and the new &lt;strong&gt;Qwen3-VL 4B &amp;amp; 8B&lt;/strong&gt; are &lt;em&gt;insane.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Benchmarks on the 5090 (Q4):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Qwen3VL-8B → &lt;strong&gt;187 tok/s, ~8GB VRAM&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Qwen3VL-4B → &lt;strong&gt;267 tok/s, ~6GB VRAM&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Demo:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1o98m76/video/mvvtazwropvf1/player"&gt;https://reddit.com/link/1o98m76/video/mvvtazwropvf1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How to try:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Install Hyperlink with one click: &lt;a href="https://hyperlink.nexa.ai/"&gt;hyperlink.nexa.ai&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Then go to &lt;em&gt;Discover Models → download Qwen3-VL GGUF to test.&lt;/em&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;How does it do on your setup? Do you see similar performance between Qwen3VL 8B and Qwen2.5-32B?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlanzhuLy"&gt; /u/AlanzhuLy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o98m76/nvidia_sent_me_a_5090_so_i_can_demo_qwen3vl_gguf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o98m76/nvidia_sent_me_a_5090_so_i_can_demo_qwen3vl_gguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o98m76/nvidia_sent_me_a_5090_so_i_can_demo_qwen3vl_gguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T18:06:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1o9xt7v</id>
    <title>Benchmark Request (MAX+ 395)</title>
    <updated>2025-10-18T14:49:46+00:00</updated>
    <author>
      <name>/u/bmayer0122</name>
      <uri>https://old.reddit.com/user/bmayer0122</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am considering buying a Ryzen AI MAX+ 395 based system. I wonder if someone could run a couple of quick benchmarks for me? You just need to copy and paste a command.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.localscore.ai/download"&gt;https://www.localscore.ai/download&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bmayer0122"&gt; /u/bmayer0122 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9xt7v/benchmark_request_max_395/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9xt7v/benchmark_request_max_395/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o9xt7v/benchmark_request_max_395/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-18T14:49:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1o9oqa2</id>
    <title>After treating RL training like an SRE project, I see why they chose CISPO</title>
    <updated>2025-10-18T06:42:50+00:00</updated>
    <author>
      <name>/u/chenqian615</name>
      <uri>https://old.reddit.com/user/chenqian615</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I mainly do operations and monitoring for long running RL training. In reality the scariest things are metric jitter, extrapolation mismatch, and hypers that are so sensitive they destabilize production. Two parts of The Art of Scaling RL Compute resonate with me. First, they use Sigmoid fitting and extrapolation to make what happens after one hundred thousand GPU hours predictable. Second, they pick CISPO for the loss because it is more stable, more linear, continues to yield gains in later stages, and is insensitive to IS clipping choices.&lt;/p&gt; &lt;p&gt;We reproduced similar trends on a small cluster. When training enters the latter phase, CISPO’s gains are easier to retain instead of letting the reward curve swing up and down. Combined with prompt level aggregation, batch advantage normalization, logits in FP32, and zero variance filtering in ScaleRL, the overall signal to noise ratio is higher and monitoring feels steadier.&lt;/p&gt; &lt;p&gt;Regarding the contribution of MiniMax as the originator of the algorithm, my sense is they distilled CISPO in an engineering oriented way so front line teams can land it. Things like hyperparameter ranges, clipping policies, and alignment with existing pipeline RL are explicit. Being selected by Meta in systematic experiments is a kind of cross environment reproduction.&lt;/p&gt; &lt;p&gt;Two small suggestions for local and open source friends:&lt;/p&gt; &lt;p&gt;(1) First run short sprints to find your CISPO sweet spot and set epsilon max and advantage normalization to a stable zone.&lt;/p&gt; &lt;p&gt;(2) When expanding budget prioritize axes that translate into Pass at K or Mean at K for your task rather than simply increasing model size.&lt;/p&gt; &lt;p&gt;(3) Add a late stage gain slope alert to monitoring. In theory CISPO gives a more linear slope, so if it deviates intervene early.If anyone has run CISPO on a local MoE for more than ten thousand GPU hours please share your epsilon max and normalization configurations and incident handling experience. I am happy to exchange lessons.&lt;/p&gt; &lt;p&gt;Paper: &lt;a href="https://arxiv.org/abs/2510.13786"&gt;https://arxiv.org/abs/2510.13786&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chenqian615"&gt; /u/chenqian615 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9oqa2/after_treating_rl_training_like_an_sre_project_i/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9oqa2/after_treating_rl_training_like_an_sre_project_i/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o9oqa2/after_treating_rl_training_like_an_sre_project_i/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-18T06:42:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1o9lv2k</id>
    <title>Free Wilderness Survival AI App w/ WebLLM Qwen</title>
    <updated>2025-10-18T03:55:36+00:00</updated>
    <author>
      <name>/u/vesudeva</name>
      <uri>https://old.reddit.com/user/vesudeva</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9lv2k/free_wilderness_survival_ai_app_w_webllm_qwen/"&gt; &lt;img alt="Free Wilderness Survival AI App w/ WebLLM Qwen" src="https://a.thumbs.redditmedia.com/hS27H4sCPxm-RdtWXpjwSqcgd3PT3m3EoYI3EA5os48.jpg" title="Free Wilderness Survival AI App w/ WebLLM Qwen" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm excited to share a free app I built called Flint, your AI-powered companion for wilderness survival. I created it for my wife and me for our trips to National Parks and backcountry adventures, and it's been a fun and useful tool. Now, I want to share it with anyone who loves the outdoors.&lt;/p&gt; &lt;p&gt;Flint is designed to be a comprehensive emergency tool that works entirely offline. It's a Progressive Web App (PWA), so you can easily add it to your phone's home screen and have it ready whenever you need it, even with zero cell service.&lt;/p&gt; &lt;p&gt;It was built from real-world guidelines and resources to ensure facts and truly helpful knowledge. Every aspect was researched by me before it went into the app. Here’s a look at what Flint can do:&lt;/p&gt; &lt;p&gt;-Offline AI Assistant: Get answers to your survival questions without needing an internet connection. The app uses a local LLM (Qwen2-1.5B-Instruct-q4f16_1-MLC) to provide guidance on the fly.&lt;/p&gt; &lt;p&gt;-Comprehensive Knowledge Base: Access a wealth of information on essential survival topics, including:&lt;/p&gt; &lt;p&gt;-First Aid: Handle medical emergencies with guides for treating burns, severe bleeding, and other injuries.&lt;/p&gt; &lt;p&gt;-Shelter: Learn how to build crisis shelters and calculate the materials you'll need.&lt;/p&gt; &lt;p&gt;-Water: Find and purify water with detailed guides on collection and filtration.&lt;/p&gt; &lt;p&gt;-Foraging: Identify edible plants and other natural resources.&lt;/p&gt; &lt;p&gt;-Powerful Survival Tools: Flint is packed with over 30 interactive tools to help you navigate and survive in the wild:&lt;/p&gt; &lt;p&gt;-Navigation: Use the Compass, Dead Reckoning Calculator, and Triangulation Calculator to find your way.&lt;/p&gt; &lt;p&gt;-Signaling: Practice Morse code with the trainer and learn how to use a signal mirror effectively.&lt;/p&gt; &lt;p&gt;-Resource Management: Estimate firewood needs, calculate water purification requirements, and track your supplies.&lt;/p&gt; &lt;p&gt;-Practical Skills: Learn essential knots with the interactive Knot Guide and identify animal tracks with the Track Identifier.&lt;/p&gt; &lt;p&gt;-Scenario-Based Guidance: Prepare for emergencies with pre-loaded scenarios for situations like wildfire evacuations, flash floods, and getting lost.&lt;/p&gt; &lt;p&gt;Check it out here: &lt;a href="https://flint-wilderness-survival-ai.vercel.app/"&gt;https://flint-wilderness-survival-ai.vercel.app/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vesudeva"&gt; /u/vesudeva &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1o9lv2k"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9lv2k/free_wilderness_survival_ai_app_w_webllm_qwen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o9lv2k/free_wilderness_survival_ai_app_w_webllm_qwen/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-18T03:55:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1o9vltx</id>
    <title>Added PyTorch trace + CUDA memory profiling support to Andrej Karpathy's nanochat</title>
    <updated>2025-10-18T13:19:04+00:00</updated>
    <author>
      <name>/u/aospan</name>
      <uri>https://old.reddit.com/user/aospan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9vltx/added_pytorch_trace_cuda_memory_profiling_support/"&gt; &lt;img alt="Added PyTorch trace + CUDA memory profiling support to Andrej Karpathy's nanochat" src="https://external-preview.redd.it/Yxmi9xQsq3Lf3YBYyU7tPqHBqkEffJmrRMDYgY5g0DM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=49a868f3a4af2d254e68636ef1837decfbcc9bfc" title="Added PyTorch trace + CUDA memory profiling support to Andrej Karpathy's nanochat" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hope it helps those curious to see how things work under the hood :)&lt;br /&gt; Pull request here: &lt;a href="https://github.com/karpathy/nanochat/pull/105"&gt;https://github.com/karpathy/nanochat/pull/105&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here’s a neat visualization from my test runs:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ffub7nm3evvf1.jpg?width=1895&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=a6e6cd7b9beee5bfc8f595b66784734e23aaf3a4"&gt;https://preview.redd.it/ffub7nm3evvf1.jpg?width=1895&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=a6e6cd7b9beee5bfc8f595b66784734e23aaf3a4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Nanochat profiling results: Training microsteps trace showing CPU/CUDA activity timeline down to individual CUDA kernel calls&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/q0zslci6evvf1.jpg?width=1737&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=285f09e9628a09e506e48bf19a5eb9afaa857c24"&gt;https://preview.redd.it/q0zslci6evvf1.jpg?width=1737&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=285f09e9628a09e506e48bf19a5eb9afaa857c24&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Nanochat profiling results: Memory timeline visualization showing allocation patterns across training micro-steps&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ok1ngbv7evvf1.jpg?width=1888&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=4f74648534aef65305434ae5afd4d357ccad88ec"&gt;https://preview.redd.it/ok1ngbv7evvf1.jpg?width=1888&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=4f74648534aef65305434ae5afd4d357ccad88ec&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Nanochat profiling results: CUDA memory snapshot showing detailed memory allocations by category&lt;/em&gt;&lt;/p&gt; &lt;p&gt;The image below isn’t part of the pull request - it just shows GPU utilization in Grafana from my overnight run of nanochat:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/rvr3zmnlevvf1.png?width=2170&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f16e138a9ef3db0d8972933e310db7f664d9a113"&gt;https://preview.redd.it/rvr3zmnlevvf1.png?width=2170&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f16e138a9ef3db0d8972933e310db7f664d9a113&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy hacking! :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aospan"&gt; /u/aospan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9vltx/added_pytorch_trace_cuda_memory_profiling_support/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9vltx/added_pytorch_trace_cuda_memory_profiling_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o9vltx/added_pytorch_trace_cuda_memory_profiling_support/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-18T13:19:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1o9txdt</id>
    <title>Is anyone else using Home-Cook-Mistral-Small-Omni? This is an hidden gem!</title>
    <updated>2025-10-18T12:02:15+00:00</updated>
    <author>
      <name>/u/no_no_no_oh_yes</name>
      <uri>https://old.reddit.com/user/no_no_no_oh_yes</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;gguf: &lt;a href="https://huggingface.co/ngxson/Home-Cook-Mistral-Small-Omni-24B-2507-GGUF"&gt;https://huggingface.co/ngxson/Home-Cook-Mistral-Small-Omni-24B-2507-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It is supported on latest Llama.cpp.&lt;/p&gt; &lt;p&gt;For technical stuff, tables, charts, transcriptions (somehow it is identifying multiple speakers too), changed my workflow from multi-model to single model. &lt;/p&gt; &lt;p&gt;My question for Reddit (and I did it also in the HF) is my experience with Q4 seems to miss details here and there, subtle stuff. But Q6 and Q8 do the job perfectly. Should a Q6 be that much better especially with Voice and Image in the mix? &lt;/p&gt; &lt;p&gt;Thanks! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/no_no_no_oh_yes"&gt; /u/no_no_no_oh_yes &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9txdt/is_anyone_else_using_homecookmistralsmallomni/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9txdt/is_anyone_else_using_homecookmistralsmallomni/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o9txdt/is_anyone_else_using_homecookmistralsmallomni/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-18T12:02:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1o9it7v</id>
    <title>[Benchmark Visualization] RTX Pro 6000 vs DGX Spark - I visualized the LMSYS data and the results are interesting</title>
    <updated>2025-10-18T01:18:03+00:00</updated>
    <author>
      <name>/u/Spare-Solution-787</name>
      <uri>https://old.reddit.com/user/Spare-Solution-787</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9it7v/benchmark_visualization_rtx_pro_6000_vs_dgx_spark/"&gt; &lt;img alt="[Benchmark Visualization] RTX Pro 6000 vs DGX Spark - I visualized the LMSYS data and the results are interesting" src="https://external-preview.redd.it/o-FGzNvhg8l8VVDovZxd73JtKM597iNiithcrypGneg.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=83cef7040edd7231023da7fa0f5e5f6ccb36d889" title="[Benchmark Visualization] RTX Pro 6000 vs DGX Spark - I visualized the LMSYS data and the results are interesting" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/pdrox73furvf1.png?width=1346&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=55c2ef1d2d887273a5f37747b6834d7a4bcb441b"&gt;https://preview.redd.it/pdrox73furvf1.png?width=1346&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=55c2ef1d2d887273a5f37747b6834d7a4bcb441b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I was curious how the RTX Pro 6000 Workstation Edition compares to the new DGX Spark (experimental results, not just the theoretical difference), so I dove into the &lt;a href="https://lmsys.org/blog/2025-10-13-nvidia-dgx-spark/"&gt;LMSYS benchmark data&lt;/a&gt; (which tested both sglang and ollama). The results were so interesting I created visualizations for it.&lt;/p&gt; &lt;p&gt;GitHub repo with charts: &lt;a href="https://github.com/casualcomputer/rtx_pro_6000_vs_dgx_spark"&gt;https://github.com/casualcomputer/rtx_pro_6000_vs_dgx_spark&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;TL;DR&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;RTX Pro 6000 is 6-7x faster&lt;/strong&gt; for LLM inference across every batch size and model tested. This isn't a small difference - we're talking 100 seconds vs 14 seconds for a 4k token conversation with Llama 3.1 8B.&lt;/p&gt; &lt;h1&gt;The Numbers (FP8, SGLang, 2k in/2k out)&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Llama 3.1 8B - Batch Size 1:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;DGX Spark: 100.1s end-to-end&lt;/li&gt; &lt;li&gt;RTX Pro 6000: 14.3s end-to-end&lt;/li&gt; &lt;li&gt;&lt;strong&gt;7.0x faster&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Llama 3.1 70B - Batch Size 1:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;DGX Spark: 772s (almost 13 minutes!)&lt;/li&gt; &lt;li&gt;RTX Pro 6000: 100s&lt;/li&gt; &lt;li&gt;&lt;strong&gt;7.7x faster&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Performance stays consistent across batch sizes 1-32.&lt;/strong&gt; The RTX just keeps winning by ~6x regardless of whether you're running single user or multi-tenant.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why Though?&lt;/strong&gt; LLM inference is memory-bound. You're constantly loading model weights from memory for every token generation. The RTX Pro 6000 has 6.5x more memory bandwidth (1,792 GB/s) than DGX-Spark (273 GB/s), and surprise - it's 6x faster. The math seems to check out.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Spare-Solution-787"&gt; /u/Spare-Solution-787 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9it7v/benchmark_visualization_rtx_pro_6000_vs_dgx_spark/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9it7v/benchmark_visualization_rtx_pro_6000_vs_dgx_spark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o9it7v/benchmark_visualization_rtx_pro_6000_vs_dgx_spark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-18T01:18:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1o9uw49</id>
    <title>Stress Testing Embedding Models with adversarial examples</title>
    <updated>2025-10-18T12:47:58+00:00</updated>
    <author>
      <name>/u/GullibleEngineer4</name>
      <uri>https://old.reddit.com/user/GullibleEngineer4</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After hitting performance walls on several RAG projects, I'm starting to think the real problem isn't our retrieval logic. It's the embedding models themselves. My theory is that even the top models are still way too focused on keyword matching and actually don't capture sentence level semantic similarity.&lt;/p&gt; &lt;p&gt;Here's a test I've been running. Which sentence is closer to the Anchor?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Anchor:&lt;/strong&gt; &amp;quot;A background service listens to a task queue and processes incoming data payloads using a custom rules engine before persisting output to a local SQLite database.&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Option A (Lexical Match):&lt;/strong&gt; &amp;quot;A background service listens to a message queue and processes outgoing authentication tokens using a custom hash function before transmitting output to a local SQLite database.&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Option B (Semantic Match):&lt;/strong&gt; &amp;quot;An asynchronous worker fetches jobs from a scheduling channel, transforms each record according to a user-defined logic system, and saves the results to an embedded relational data store on disk.&amp;quot;&lt;/p&gt; &lt;p&gt;If you ask an LLM like Gemini 2.5 Pro, it correctly identifies that the Anchor and Option B are describing the same core concept - just with different words.&lt;/p&gt; &lt;p&gt;But when I tested this with gemini-embedding-001 (currently #1 on MTEB), it consistently scores Option A as more similar. It gets completely fooled by surface-level vocabulary overlap.&lt;/p&gt; &lt;p&gt;I put together a small GitHub project that uses ChatGPT to generate and test these &amp;quot;semantic triplets&amp;quot;: &lt;a href="https://github.com/semvec/embedstresstest"&gt;https://github.com/semvec/embedstresstest&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The README walks through the whole methodology if anyone wants to dig in.&lt;/p&gt; &lt;p&gt;Has anyone else noticed this? Where embeddings latch onto surface-level patterns instead of understanding what a sentence is actually &lt;em&gt;about&lt;/em&gt;?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GullibleEngineer4"&gt; /u/GullibleEngineer4 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9uw49/stress_testing_embedding_models_with_adversarial/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9uw49/stress_testing_embedding_models_with_adversarial/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o9uw49/stress_testing_embedding_models_with_adversarial/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-18T12:47:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1o9q01c</id>
    <title>Medical model: Bio-Medical-ContactDoctorVLLM</title>
    <updated>2025-10-18T08:02:08+00:00</updated>
    <author>
      <name>/u/beneath_steel_sky</name>
      <uri>https://old.reddit.com/user/beneath_steel_sky</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;Bio-Medical-ContactDoctorVLLM-14B-V1-102025 is a specialized vision-language model designed for comprehensive biomedical image analysis.&lt;/p&gt; &lt;p&gt;Built on a novel architecture combining Qwen3-14B language model with Google's MedSigLIP-448 vision encoder, this model excels at analyzing diverse medical imaging modalities including X-rays, CT scans, MRI, ultrasound, histopathology, and clinical photography.&amp;quot;&lt;/p&gt; &lt;p&gt;Couldn't find any benchmark, I wonder how does it compare to medgemma...&lt;/p&gt; &lt;p&gt;Link: &lt;a href="https://huggingface.co/ContactDoctor/Bio-Medical-ContactDoctorVLLM-14B-V1-102025"&gt;https://huggingface.co/ContactDoctor/Bio-Medical-ContactDoctorVLLM-14B-V1-102025&lt;/a&gt; (8B also available)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/beneath_steel_sky"&gt; /u/beneath_steel_sky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9q01c/medical_model_biomedicalcontactdoctorvllm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9q01c/medical_model_biomedicalcontactdoctorvllm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o9q01c/medical_model_biomedicalcontactdoctorvllm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-18T08:02:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1o9xf4q</id>
    <title>[Experiment] Qwen3-VL-8B VS Qwen2.5-VL-7B test results</title>
    <updated>2025-10-18T14:33:56+00:00</updated>
    <author>
      <name>/u/Unbreakable_ryan</name>
      <uri>https://old.reddit.com/user/Unbreakable_ryan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9xf4q/experiment_qwen3vl8b_vs_qwen25vl7b_test_results/"&gt; &lt;img alt="[Experiment] Qwen3-VL-8B VS Qwen2.5-VL-7B test results" src="https://external-preview.redd.it/MWdpMThpMmdzdnZmMSlMdchWVQEhhJbqcRpTSv2Il4U_vTm8zPYXwK-Tk6g_.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=63ce1d6d2b0cd265b89c2d1f2c44c8a926ac5d47" title="[Experiment] Qwen3-VL-8B VS Qwen2.5-VL-7B test results" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt;&lt;br /&gt; I tested the brand-new &lt;strong&gt;Qwen3-VL-8B&lt;/strong&gt; against &lt;strong&gt;Qwen2.5-VL-7B&lt;/strong&gt; on the same set of visual reasoning tasks — OCR, chart analysis, multimodal QA, and instruction following.&lt;br /&gt; Despite being only 1B parameters larger, Qwen3-VL shows a &lt;strong&gt;&lt;em&gt;clear generation-to-generation leap&lt;/em&gt;&lt;/strong&gt; and delivers more accurate, nuanced, and faster multimodal reasoning.&lt;/p&gt; &lt;h1&gt;1. Setup&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Environment:&lt;/strong&gt; Local inference&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hardware:&lt;/strong&gt; Mac Air M4, 8-core GPU, 24 GB VRAM&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model format:&lt;/strong&gt; gguf, Q4&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Tasks tested:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Visual perception (receipts, invoice)&lt;/li&gt; &lt;li&gt;Visual captioning (photos)&lt;/li&gt; &lt;li&gt;Visual reasoning (business data)&lt;/li&gt; &lt;li&gt;Multimodal Fusion (does paragraph match figure)&lt;/li&gt; &lt;li&gt;Instruction following (structured answers)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Each prompt + image pair was fed to both models, using identical context.&lt;/p&gt; &lt;h1&gt;2. Evaluation Criteria&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Visual Perception&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Metric&lt;/strong&gt;: Correctly identifies text, objects, and layout.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Why It Matters&lt;/strong&gt;: This reflects the model’s baseline visual IQ.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Visual Captioning&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Metric&lt;/strong&gt;: Generates natural language descriptions of images.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Why It Matters&lt;/strong&gt;: Bridges vision and language, showing the model can translate what it sees into coherent text.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Visual Reasoning&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Metric&lt;/strong&gt;: Reads chart trends and applies numerical logic.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Why It Matters&lt;/strong&gt;: Tests true multimodal reasoning ability, beyond surface-level recognition.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Multimodal Fusion&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Metric&lt;/strong&gt;: Connects image content with text context.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Why It Matters&lt;/strong&gt;: Demonstrates cross-attention strength—how well the model integrates multiple modalities.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Instruction Following&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Metric&lt;/strong&gt;: Obeys structured prompts, such as “answer in 3 bullets.”&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Why It Matters&lt;/strong&gt;: Reflects alignment quality and the ability to produce controllable outputs.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Efficiency&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Metric&lt;/strong&gt;: TTFT (time to first token) and decoding speed.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Why It Matters&lt;/strong&gt;: Determines local usability and user experience.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Note: all answers are verified by humans and ChatGPT5.&lt;/p&gt; &lt;h1&gt;3. Test Results Summary&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Visual Perception&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;ul&gt; &lt;li&gt;Qwen2.5-VL-7B: Score 5&lt;/li&gt; &lt;li&gt;Qwen3-VL-8B: Score 8&lt;/li&gt; &lt;li&gt;Winner: Qwen3-VL-8B&lt;/li&gt; &lt;li&gt;Notes: Qwen3-VL-8B identify all the elements in the pic but fail the first and final calculation (the answer is 480.96 and 976.94). In comparison, Qwen2.5-VL-7B could not even understand the meaning of all the elements in the pic (there are two tourists) though the calculation is correct.&lt;/li&gt; &lt;/ul&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Visual Captioning&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;ul&gt; &lt;li&gt;Qwen2.5-VL-7B: Score 6.5&lt;/li&gt; &lt;li&gt;Qwen3-VL-8B: Score 9&lt;/li&gt; &lt;li&gt;Winner: Qwen3-VL-8B&lt;/li&gt; &lt;li&gt;Notes: Qwen3-VL-8B is more accurate, detailed, and has better scene understanding. (for example, identify Christmas Tree and Milkis). In contrary, Qwen2.5-VL-7B Gets the gist, but makes several misidentifications and lacks nuance.&lt;/li&gt; &lt;/ul&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Visual Reasoning&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;ul&gt; &lt;li&gt;Qwen2.5-VL-7B: Score 8&lt;/li&gt; &lt;li&gt;Qwen3-VL-8B: Score 9&lt;/li&gt; &lt;li&gt;Winner: Qwen3-VL-8B&lt;/li&gt; &lt;li&gt;Notes: Both models show the basically correct reasoning of the charts and one or two numeric errors. Qwen3-VL-8B is better at analysis/insight which indicates the key shifts while Qwen2.5-VL-7B has a clearer structure.&lt;/li&gt; &lt;/ul&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Multimodal Fusion&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;ul&gt; &lt;li&gt;Qwen2.5-VL-7B: Score 7&lt;/li&gt; &lt;li&gt;Qwen3-VL-8B: Score 9&lt;/li&gt; &lt;li&gt;Winner: Qwen3-VL-8B&lt;/li&gt; &lt;li&gt;Notes: The reasoning of Qwen3-VL-8B is correct, well-supported, and compelling with slight round up for some percentages, while that of Qwen2.5-VL-7B shows Incorrect data reference.&lt;/li&gt; &lt;/ul&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Instruction Following&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;ul&gt; &lt;li&gt;Qwen2.5-VL-7B: Score 8&lt;/li&gt; &lt;li&gt;Qwen3-VL-8B: Score 8.5&lt;/li&gt; &lt;li&gt;Winner: Qwen3-VL-8B&lt;/li&gt; &lt;li&gt;Notes: The summary from Qwen3-VL-8B is more faithful and nuanced, but more wordy. The suammry of Qwen2.5-VL-7B is cleaner and easier to read but misses some details.&lt;/li&gt; &lt;/ul&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Decode Speed&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;ul&gt; &lt;li&gt;Qwen2.5-VL-7B: 11.7–19.9t/s&lt;/li&gt; &lt;li&gt;Qwen3-VL-8B: 15.2–20.3t/s&lt;/li&gt; &lt;li&gt;Winner: Qwen3-VL-8B&lt;/li&gt; &lt;li&gt;Notes: 15–60% faster.&lt;/li&gt; &lt;/ul&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;TTFT&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;ul&gt; &lt;li&gt;Qwen2.5-VL-7B: 5.9–9.9s&lt;/li&gt; &lt;li&gt;Qwen3-VL-8B: 4.6–7.1s&lt;/li&gt; &lt;li&gt;Winner: Qwen3-VL-8B&lt;/li&gt; &lt;li&gt;Notes: 20–40% faster.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;4. Example Prompts&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Visual perception:&lt;/strong&gt; “Extract the total amount and payment date from this invoice.”&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Visual captioning&lt;/strong&gt;: &amp;quot;Describe this photo&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Visual reasoning:&lt;/strong&gt; “From this chart, what’s the trend from 1963 to 1990?”&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multimodal Fusion:&lt;/strong&gt; “Does the table in the image support the written claim: Europe is the dominant market for Farmed Caviar?”&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Instruction following&lt;/strong&gt; “Summarize this poster in exactly 3 bullet points.”&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;5. Summary &amp;amp; Takeaway&lt;/h1&gt; &lt;p&gt;The comparison does not demonstrate just a minor version bump, but a generation leap:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Qwen3-VL-8B consistently outperforms in &lt;strong&gt;Visual reasoning&lt;/strong&gt;, &lt;strong&gt;Multimodal fusion, Instruction following,&lt;/strong&gt; and especially &lt;strong&gt;Visual perception and Visual captioning.&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Qwen3-VL-8B produces more &lt;strong&gt;faithful and nuanced answers&lt;/strong&gt;, often giving richer context and insights. (however, conciseness is the tradeoff). Thus, users who value &lt;strong&gt;accuracy and depth&lt;/strong&gt; should prefer Qwen3, while those who want &lt;strong&gt;conciseness with less cognitive load&lt;/strong&gt; might tolerate Qwen2.5.&lt;/li&gt; &lt;li&gt;Qwen3’s mistakes are easier for humans to correct (eg, some numeric errors), whereas Qwen2.5 can mislead due to &lt;strong&gt;deeper misunderstandings&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Qwen3 not only &lt;strong&gt;improves quality but also reduces latency&lt;/strong&gt;, improving user experience.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Unbreakable_ryan"&gt; /u/Unbreakable_ryan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/t0mzpl2gsvvf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9xf4q/experiment_qwen3vl8b_vs_qwen25vl7b_test_results/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o9xf4q/experiment_qwen3vl8b_vs_qwen25vl7b_test_results/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-18T14:33:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1o9x4ky</id>
    <title>Qwen3VL-30b-a3b Image Caption Performance - Thinking vs Instruct (FP8) using vLLM and 2x RTX 5090</title>
    <updated>2025-10-18T14:22:05+00:00</updated>
    <author>
      <name>/u/reto-wyss</name>
      <uri>https://old.reddit.com/user/reto-wyss</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9x4ky/qwen3vl30ba3b_image_caption_performance_thinking/"&gt; &lt;img alt="Qwen3VL-30b-a3b Image Caption Performance - Thinking vs Instruct (FP8) using vLLM and 2x RTX 5090" src="https://a.thumbs.redditmedia.com/2NLvlnTIwrK_v4qLdjPp5FuWDH46C_f7lvMjl0LblG4.jpg" title="Qwen3VL-30b-a3b Image Caption Performance - Thinking vs Instruct (FP8) using vLLM and 2x RTX 5090" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here to report some performance numbers, hope someone can comment whether that looks in-line.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;System&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;2x RTX 5090 (450W, PCIe 4 x16)&lt;/li&gt; &lt;li&gt;Threadripper 5965WX&lt;/li&gt; &lt;li&gt;512GB RAM&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Command&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;There may be a little bit of headroom for --max-model-len&lt;/p&gt; &lt;pre&gt;&lt;code&gt;vllm serve Qwen/Qwen3-VL-30B-A3B-Thinking-FP8 --async-scheduling --tensor-parallel-size 2 --mm-encoder-tp-mode data --limit-mm-per- prompt.video 0 --max-model-len 128000 vllm serve Qwen/Qwen3-VL-30B-A3B-Instruct-FP8 --async-scheduling --tensor-parallel-size 2 --mm-encoder-tp-mode data --limit-mm-per- prompt.video 0 --max-model-len 128000 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Payload&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;512 Images (max concurrent 256)&lt;/li&gt; &lt;li&gt;1024x1024&lt;/li&gt; &lt;li&gt;Prompt: &amp;quot;Write a very long and detailed description. Do not mention the style.&amp;quot;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/zswllkf5pvvf1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=79edc002bcc13ae1e6177909ab9667dffb142aa5"&gt;Sample Image&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Instruct Model Total time: 162.61s Throughput: 188.9 images/minute Average time per request: 55.18s Fastest request: 23.27s Slowest request: 156.14s Total tokens processed: 805,031 Average prompt tokens: 1048.0 Average completion tokens: 524.3 Token throughput: 4950.6 tokens/second Tokens per minute: 297033 Thinking Model Total time: 473.49s Throughput: 64.9 images/minute Average time per request: 179.79s Fastest request: 57.75s Slowest request: 321.32s Total tokens processed: 1,497,862 Average prompt tokens: 1051.0 Average completion tokens: 1874.5 Token throughput: 3163.4 tokens/second Tokens per minute: 189807 &lt;/code&gt;&lt;/pre&gt; &lt;ul&gt; &lt;li&gt;The Thinking Model typically has around 65 - 75 requests active and the Instruct Model around 100 - 120.&lt;/li&gt; &lt;li&gt;Peak PP is over 10k t/s&lt;/li&gt; &lt;li&gt;Peak generation is over 2.5k t/s&lt;/li&gt; &lt;li&gt;Non-Thinking Model is about 3x faster (189 images per minute) on this task than the Thinking Model (65 images per minute).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Do these numbers look fine?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/reto-wyss"&gt; /u/reto-wyss &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9x4ky/qwen3vl30ba3b_image_caption_performance_thinking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9x4ky/qwen3vl30ba3b_image_caption_performance_thinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o9x4ky/qwen3vl30ba3b_image_caption_performance_thinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-18T14:22:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1o9z1jc</id>
    <title>Why does Jensen keep telling ASICs aren't worth it and most of them will fail despite Groq/Cerebras achieving decent success?</title>
    <updated>2025-10-18T15:38:39+00:00</updated>
    <author>
      <name>/u/Ok-Elevator5091</name>
      <uri>https://old.reddit.com/user/Ok-Elevator5091</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I'm a beginner here, and my Q itself may be elementary. Groq/Cerebras often claim record breaking inference speeds and beating NVIDIA...and Cerebras now has gotten into training to afaik. And they are reasonoably successful, given how Cerebras raised funding too.&lt;/p&gt; &lt;p&gt;Ofc they're not gonna replace GPUs, but at least for inference, they'll survive...and maybe not as bad as Jensen says? I heard one dev tell me Cerebras/Groq arent just as flexible like GPUs for any task in AI workloads. Would really appreciate if someone can help me understand this, with some supporting technical details as to why GPUs are still the best for any task..&lt;/p&gt; &lt;p&gt;Thanks as always to the lovely folks at LocalLLama in advance. Learnt so much from this community&lt;/p&gt; &lt;p&gt;Edit: I want to know the reasons besides the fact that Jensen will not openly praise ASICs and it is in his best intertests to make public comments in favour if GPUs...hehe&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Elevator5091"&gt; /u/Ok-Elevator5091 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9z1jc/why_does_jensen_keep_telling_asics_arent_worth_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9z1jc/why_does_jensen_keep_telling_asics_arent_worth_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o9z1jc/why_does_jensen_keep_telling_asics_arent_worth_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-18T15:38:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1o9s60k</id>
    <title>Bee-8B, "fully open 8B Multimodal LLM designed to close the performance gap with proprietary models"</title>
    <updated>2025-10-18T10:21:03+00:00</updated>
    <author>
      <name>/u/beneath_steel_sky</name>
      <uri>https://old.reddit.com/user/beneath_steel_sky</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9s60k/bee8b_fully_open_8b_multimodal_llm_designed_to/"&gt; &lt;img alt="Bee-8B, &amp;quot;fully open 8B Multimodal LLM designed to close the performance gap with proprietary models&amp;quot;" src="https://external-preview.redd.it/Ncd1u3KqHI3q8cl_bGxSrWsYruQjV7vJ0ceKV7GWN6Y.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ae9ffa18e4d3b03e94f5458c4c02b0fc641812e8" title="Bee-8B, &amp;quot;fully open 8B Multimodal LLM designed to close the performance gap with proprietary models&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/beneath_steel_sky"&gt; /u/beneath_steel_sky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Open-Bee/Bee-8B-RL"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9s60k/bee8b_fully_open_8b_multimodal_llm_designed_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o9s60k/bee8b_fully_open_8b_multimodal_llm_designed_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-18T10:21:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1o9xiza</id>
    <title>dgx, it's useless , High latency</title>
    <updated>2025-10-18T14:38:06+00:00</updated>
    <author>
      <name>/u/Illustrious-Swim9663</name>
      <uri>https://old.reddit.com/user/Illustrious-Swim9663</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9xiza/dgx_its_useless_high_latency/"&gt; &lt;img alt="dgx, it's useless , High latency" src="https://preview.redd.it/wwroq3nbtvvf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d419416ab7812d4f7c564531795007c015a4c85f" title="dgx, it's useless , High latency" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ahmad posted a tweet where DGX latency is high : &lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/TheAhmadOsman/status/1979408446534398403?t=COH4pw0-8Za4kRHWa2ml5A&amp;amp;s=19"&gt;https://x.com/TheAhmadOsman/status/1979408446534398403?t=COH4pw0-8Za4kRHWa2ml5A&amp;amp;s=19&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Illustrious-Swim9663"&gt; /u/Illustrious-Swim9663 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wwroq3nbtvvf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9xiza/dgx_its_useless_high_latency/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o9xiza/dgx_its_useless_high_latency/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-18T14:38:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
