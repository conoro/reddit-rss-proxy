<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-28T22:24:17+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1p8ahy8</id>
    <title>Prime Intellect Introduces INTELLECT-3: A 100B+ MoE Trained With Large-scale RL That Achieves State-Of-The-Art Performance For Its Size, Taking The Lead Amongst Open-Sourced Models Across Math, Code, Science &amp; Reasoning Benchmarks. (Link to Chat with the Model provided)</title>
    <updated>2025-11-27T19:13:50+00:00</updated>
    <author>
      <name>/u/44th--Hokage</name>
      <uri>https://old.reddit.com/user/44th--Hokage</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8ahy8/prime_intellect_introduces_intellect3_a_100b_moe/"&gt; &lt;img alt="Prime Intellect Introduces INTELLECT-3: A 100B+ MoE Trained With Large-scale RL That Achieves State-Of-The-Art Performance For Its Size, Taking The Lead Amongst Open-Sourced Models Across Math, Code, Science &amp;amp; Reasoning Benchmarks. (Link to Chat with the Model provided)" src="https://a.thumbs.redditmedia.com/tQWpy1j22HMExtYqdGvlA_Lo8sIubygJf4xso2VwSj0.jpg" title="Prime Intellect Introduces INTELLECT-3: A 100B+ MoE Trained With Large-scale RL That Achieves State-Of-The-Art Performance For Its Size, Taking The Lead Amongst Open-Sourced Models Across Math, Code, Science &amp;amp; Reasoning Benchmarks. (Link to Chat with the Model provided)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h2&gt;From the Official Announcement:&lt;/h2&gt; &lt;blockquote&gt; &lt;p&gt;Today, we release INTELLECT-3, a 100B+ parameter Mixture-of-Experts model trained on our RL stack, achieving state-of-the-art performance for its size across math, code, science and reasoning benchmarks, outperforming many larger frontier models.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Our complete recipe ‚Äî from the model weights and training frameworks, to our datasets, RL environments, and evaluations ‚Äî has been open-sourced, with the goal of encouraging more open research on large scale reinforcement learning.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;INTELLECT-3 is trained on the same software and infrastructure that we‚Äôre open-sourcing and making available on our platform at Prime Intellect, giving everyone the tools to post-train their own state-of-the-art models, and moving us towards a future where every company can be an AI company.&lt;/p&gt; &lt;p&gt;The sharpest distinction between Prime-RL and many other RL trainers is that it is async-only ‚Äî we recognized fairly early (for our previous INTELLECT-2 model) that the future of RL is async; i.e. always a few steps off-policy. Async training is simply the only practical way to efficiently scale RL to long-horizon agentic rollouts without incurring bottlenecks based on the slowest rollouts per step.&lt;/p&gt; &lt;/blockquote&gt; &lt;hr /&gt; &lt;h2&gt;Architecture:&lt;/h2&gt; &lt;p&gt;Three main abstractions facilitate RL training: the orchestrator, the trainer, and the inference service. A RL training run involves the coordination of a trainer, orchestrator and an inference service. The FSDP trainer and vLLM inference run disaggregated, and can be individually deployed across multiple nodes.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Orchestrator:&lt;/strong&gt; - The orchestrator is a lightweight CPU process that handles the core data flow and scheduling logic, serving as an intermediary between the trainer and inference service with bidirectional relays. In one direction, it collects rollouts from the inference server, assembles them into packed batches, and dispatches them to the trainer; in the other direction, it relays updated model weights from the trainer to the inference service. The orchestrator utilizes verifiers environments to abstract multi-turn rollout generation and scoring, allowing any environment on the Environments Hub to plug into the training loop.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Trainer:&lt;/strong&gt; - The trainer is responsible for producing an updated policy model given rollouts and advantages. We use FSDP 2 as the backend with compatibility for any HuggingFace model. FSDP shards model parameters, gradients, and optimizer states, allowing training large models with data parallelism and minimal GPU memory footprint. The trainer is inspired by torchtitan and relies on native PyTorch features to implement advanced parallelism techniques, such as tensor, context, and expert parallelism, and leverages grouped matrix multiplication kernels for efficient MoE training.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Inference:&lt;/strong&gt; - The inference pool consists of standard OpenAI-compatible servers with a vLLM backend. The API specification is extended with custom endpoints to enable updating the server with the latest policy: &lt;code&gt;/update_weights&lt;/code&gt; is used to update the policy, and &lt;code&gt;/reload_weights&lt;/code&gt; is used to reset the weights to the base model in between experiments. We rely on vLLM's optimized kernels, parallelism strategies, and scheduling for fast rollout generation. Given the disaggregated nature of the service architecture, it can be directly extended to include multiple engines with a shared request pool, allowing operation across multiple clusters and straightforward integration of alternative inference engines.&lt;/p&gt; &lt;hr /&gt; &lt;h4&gt;Link to the Official Announcement: &lt;a href="https://www.primeintellect.ai/blog/intellect-3"&gt;https://www.primeintellect.ai/blog/intellect-3&lt;/a&gt;&lt;/h4&gt; &lt;hr /&gt; &lt;h4&gt;Link to the Technical Report: &lt;a href="https://storage.googleapis.com/intellect-3-paper/INTELLECT_3_Technical_Report.pdf"&gt;https://storage.googleapis.com/intellect-3-paper/INTELLECT_3_Technical_Report.pdf&lt;/a&gt;&lt;/h4&gt; &lt;hr /&gt; &lt;h4&gt;Link to the Open-Sourced Prime-RL GitHub: &lt;a href="https://github.com/PrimeIntellect-ai/prime-rl"&gt;https://github.com/PrimeIntellect-ai/prime-rl&lt;/a&gt;&lt;/h4&gt; &lt;hr /&gt; &lt;h4&gt;Link to the Open-Sourced Model Weights: &lt;a href="https://huggingface.co/PrimeIntellect/INTELLECT-3"&gt;https://huggingface.co/PrimeIntellect/INTELLECT-3&lt;/a&gt;&lt;/h4&gt; &lt;hr /&gt; &lt;h4&gt;Chat with the Model Here: &lt;a href="https://chat.primeintellect.ai/"&gt;https://chat.primeintellect.ai/&lt;/a&gt;&lt;/h4&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/44th--Hokage"&gt; /u/44th--Hokage &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1p8ahy8"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8ahy8/prime_intellect_introduces_intellect3_a_100b_moe/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p8ahy8/prime_intellect_introduces_intellect3_a_100b_moe/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-27T19:13:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1p97uuv</id>
    <title>Your local models can now make phone calls! Launching Phone Integration üìû in Observer</title>
    <updated>2025-11-28T22:18:25+00:00</updated>
    <author>
      <name>/u/Roy3838</name>
      <uri>https://old.reddit.com/user/Roy3838</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p97uuv/your_local_models_can_now_make_phone_calls/"&gt; &lt;img alt="Your local models can now make phone calls! Launching Phone Integration üìû in Observer" src="https://external-preview.redd.it/nOalRlFoqYIIAoHotT2ulKa0W-tArBvHe2Mb6BSa9vU.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3394b4f1ae7c7e8c7e6556e7ffe2af8db0354268" title="Your local models can now make phone calls! Launching Phone Integration üìû in Observer" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TL;DR: Observer is an &lt;strong&gt;open-source, free, and local&lt;/strong&gt; framework that gives your local models actual powers, like watching your screen/camera/mic, logging to memory, and now &lt;strong&gt;making real phone calls!!&lt;/strong&gt; I'm Roy, the solo dev building this, and I would really appreciate your feedback to keep making Observer better :)&lt;/p&gt; &lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;Thanks for all the support! seriously, this community has always been incredible. Observer has gone super far due to your support and feedback!!&lt;/p&gt; &lt;p&gt;I'm back with something I think is pretty cool: your local models can now &lt;strong&gt;make actual phone calls.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Quick Setup:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Whitelist your number by messaging/calling Observer (to prevent abuse)&lt;/li&gt; &lt;li&gt;Observer watches your screen/camera via WebRTC&lt;/li&gt; &lt;li&gt;Your local model (Ollama/llama.cpp) processes what it sees&lt;/li&gt; &lt;li&gt;New call() function triggers a real phone call when your conditions are met&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Random use cases I've used it for:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;That 2-hour render finally finishes ‚Üí get a call&lt;/li&gt; &lt;li&gt;Your AFK Minecraft character is about to die ‚Üí phone rings&lt;/li&gt; &lt;li&gt;Security camera detects motion ‚Üí instant call with a description of what it sees.&lt;/li&gt; &lt;li&gt;Your crypto bot sees something ‚Üí wake up with specific data of what happened.&lt;/li&gt; &lt;li&gt;Literally anything you can see on screen ‚Üí phone call with text2speech&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What is Observer AI?&lt;/p&gt; &lt;p&gt;It's a framework I built for this community. Think of it like a super simple MCP server that runs in your browser:&lt;/p&gt; &lt;p&gt;- Sensors (Screen/Camera/Mic) ‚Üí Local Models (Ollama/llama.cpp) ‚Üí Tools (notifications, recordings, memory, code, and &lt;strong&gt;now phone calls&lt;/strong&gt;)&lt;/p&gt; &lt;p&gt;The whole thing is free (with some convenient paid tiers to make it sustainable), open-source (MIT license), and runs entirely on your machine. You can try it in your browser with zero setup, or go full local with the desktop app.&lt;/p&gt; &lt;p&gt;Links:&lt;/p&gt; &lt;p&gt;- GitHub (all the code, open source): &lt;a href="https://github.com/Roy3838/Observer"&gt;https://github.com/Roy3838/Observer&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- Try it without any install: &lt;a href="https://app.observer-ai.com/"&gt;https://app.observer-ai.com/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- Discord: &lt;a href="https://discord.gg/wnBb7ZQDUC"&gt;https://discord.gg/wnBb7ZQDUC&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm here to answer questions. What would YOU use this for?&lt;/p&gt; &lt;p&gt;Cheers,&lt;/p&gt; &lt;p&gt;Roy&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Roy3838"&gt; /u/Roy3838 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/shorts/yNu2K6LaTNk"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p97uuv/your_local_models_can_now_make_phone_calls/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p97uuv/your_local_models_can_now_make_phone_calls/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-28T22:18:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1p8uzww</id>
    <title>Bifrost vs LiteLLM: Side-by-Side Benchmarks (50x Faster LLM Gateway)</title>
    <updated>2025-11-28T13:35:27+00:00</updated>
    <author>
      <name>/u/dinkinflika0</name>
      <uri>https://old.reddit.com/user/dinkinflika0</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone; I recently shared a post here about Bifrost, a high-performance LLM gateway we‚Äôve been building in Go. A lot of folks in the comments asked for a clearer side-by-side comparison with LiteLLM, including performance benchmarks and migration examples. So here‚Äôs a follow-up that lays out the numbers, features, and how to switch over in one line of code.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Benchmarks (vs LiteLLM)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Setup:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;single t3.medium instance&lt;/li&gt; &lt;li&gt;mock llm with 1.5 seconds latency&lt;/li&gt; &lt;/ul&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Metric&lt;/th&gt; &lt;th align="left"&gt;LiteLLM&lt;/th&gt; &lt;th align="left"&gt;Bifrost&lt;/th&gt; &lt;th align="left"&gt;Improvement&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left" colspan="3"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;p99 Latency&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;90.72s&lt;/td&gt; &lt;td align="left"&gt;1.68s&lt;/td&gt; &lt;td align="left"&gt;~54√ó faster&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Throughput&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;44.84 req/sec&lt;/td&gt; &lt;td align="left"&gt;424 req/sec&lt;/td&gt; &lt;td align="left"&gt;~9.4√ó higher&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Memory Usage&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;372MB&lt;/td&gt; &lt;td align="left"&gt;120MB&lt;/td&gt; &lt;td align="left"&gt;~3√ó lighter&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Mean Overhead&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;~500¬µs&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;11¬µs @ 5K RPS&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;~45√ó lower&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Repo:&lt;/strong&gt; &lt;a href="https://github.com/maximhq/bifrost"&gt;https://github.com/maximhq/bifrost&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Key Highlights&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Ultra-low overhead:&lt;/strong&gt; mean request handling overhead is just &lt;strong&gt;11¬µs per request&lt;/strong&gt; at 5K RPS.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Provider Fallback:&lt;/strong&gt; Automatic failover between providers ensures 99.99% uptime for your applications.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Semantic caching:&lt;/strong&gt; deduplicates similar requests to reduce repeated inference costs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Adaptive load balancing:&lt;/strong&gt; Automatically optimizes traffic distribution across provider keys and models based on real-time performance metrics.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cluster mode resilience:&lt;/strong&gt; High availability deployment with automatic failover and load balancing. Peer-to-peer clustering where every instance is equal.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Drop-in OpenAI-compatible API:&lt;/strong&gt; Replace your existing SDK with just one line change. Compatible with OpenAI, Anthropic, LiteLLM, Google Genai, Langchain and more.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Observability:&lt;/strong&gt; Out-of-the-box OpenTelemetry support for observability. Built-in dashboard for quick glances without any complex setup.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model-Catalog:&lt;/strong&gt; Access 15+ providers and 1000+ AI models from multiple providers through a unified interface. Also support custom deployed models!&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Governance&lt;/strong&gt;: SAML support for SSO and Role-based access control and policy enforcement for team collaboration.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Migrating from LiteLLM ‚Üí Bifrost&lt;/h1&gt; &lt;p&gt;You don‚Äôt need to rewrite your code; just point your LiteLLM SDK to Bifrost‚Äôs endpoint.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Old (LiteLLM):&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from litellm import completion response = completion( model=&amp;quot;gpt-4o-mini&amp;quot;, messages=[{&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Hello GPT!&amp;quot;}] ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;New (Bifrost):&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from litellm import completion response = completion( model=&amp;quot;gpt-4o-mini&amp;quot;, messages=[{&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Hello GPT!&amp;quot;}], base_url=&amp;quot;&amp;lt;http://localhost:8080/litellm&amp;gt;&amp;quot; ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can also use custom headers for governance and tracking (see docs!)&lt;/p&gt; &lt;p&gt;The switch is one line; everything else stays the same.&lt;/p&gt; &lt;p&gt;Bifrost is built for teams that treat LLM infra as production software: &lt;strong&gt;predictable, observable, and fast&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;If you‚Äôve found LiteLLM fragile or slow at higher load, this might be worth testing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dinkinflika0"&gt; /u/dinkinflika0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8uzww/bifrost_vs_litellm_sidebyside_benchmarks_50x/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8uzww/bifrost_vs_litellm_sidebyside_benchmarks_50x/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p8uzww/bifrost_vs_litellm_sidebyside_benchmarks_50x/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-28T13:35:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1p8pcrj</id>
    <title>How many parameters do you think are required to emulate the *knowledge* of an average person</title>
    <updated>2025-11-28T08:02:12+00:00</updated>
    <author>
      <name>/u/FrostTactics</name>
      <uri>https://old.reddit.com/user/FrostTactics</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's not controversial to state that LLMs today aren't 100% efficient in their parameter usage. It would not surprise me if we could compress current day performance into one hundredth of the parameters. That said, all knowledge requires &lt;em&gt;information,&lt;/em&gt; and there must therefore be a limit to the level of compression that can be achieved.&lt;/p&gt; &lt;p&gt;The current paradigm tries to train all LLMs as generalists for various technical reasons I'm sure I don't have to explain to the people here. This means that basically all LLMs, even those with only a couple of billion parameters, speak passable Norwegian, for example.&lt;/p&gt; &lt;p&gt;Say we narrowed the scope and instead of trying to build generalists, we tried to build an LLM with an amount of knowledge comparable to that of an average person. Let's make the person monolingual, with the common knowledge expected of any modern person, and an expert in a single field.&lt;/p&gt; &lt;p&gt;Let's also ignore vision, real-world navigation, and actually processing the knowledge, as these seem a bit too vague to reliably get an estimate of at the moment.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;EDIT:&lt;/strong&gt; Feels like a fair few of the responders didn't understand the questionüòÖ. This discussion is meant as a purely academic exercise for the theoretical lower limit of number of parameters required for the knowledge of an average person. I.e. not intelligence, just the pure amount of information required to represent the an average person's knowledge. I've seen a few people comment that LLMs have surpassed us on this already. I agree with this, I think we could easily represent it with far fewer parameters than the current SotA LLMs.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FrostTactics"&gt; /u/FrostTactics &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8pcrj/how_many_parameters_do_you_think_are_required_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8pcrj/how_many_parameters_do_you_think_are_required_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p8pcrj/how_many_parameters_do_you_think_are_required_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-28T08:02:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1p90a31</id>
    <title>Made a little desktop tool</title>
    <updated>2025-11-28T17:12:30+00:00</updated>
    <author>
      <name>/u/randomNinja64</name>
      <uri>https://old.reddit.com/user/randomNinja64</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p90a31/made_a_little_desktop_tool/"&gt; &lt;img alt="Made a little desktop tool" src="https://b.thumbs.redditmedia.com/cjiuzRM7aD260eX6h2SUZpvKdEwcMMIwwSJY8zr7SFA.jpg" title="Made a little desktop tool" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Though I doubt anyone was asking for such a thing, I ended up making a little AI agent tool that works on Windows XP and up. It's a piece of software for communicating with OpenAI-compatible LLM servers. I've been getting a good bit of use with it on my older systems.&lt;/p&gt; &lt;p&gt;The application (and its source code) are available at &lt;a href="https://github.com/randomNinja64/SimpleLLMChat"&gt;https://github.com/randomNinja64/SimpleLLMChat&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/systbzd4614g1.png?width=1033&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ef57f079a56caa10b75358095f824b5621fa1b78"&gt;A screenshot of the SimpleLLMChat UI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If anyone has some suggestions for making HTTPS work properly under XP/.NET 4/C#, please let me know.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/randomNinja64"&gt; /u/randomNinja64 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p90a31/made_a_little_desktop_tool/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p90a31/made_a_little_desktop_tool/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p90a31/made_a_little_desktop_tool/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-28T17:12:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1p8p844</id>
    <title>Tested quantization on my 8GB potato laptop here's what actually breaks first</title>
    <updated>2025-11-28T07:54:17+00:00</updated>
    <author>
      <name>/u/Even_Ganache6148</name>
      <uri>https://old.reddit.com/user/Even_Ganache6148</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8p844/tested_quantization_on_my_8gb_potato_laptop_heres/"&gt; &lt;img alt="Tested quantization on my 8GB potato laptop here's what actually breaks first" src="https://b.thumbs.redditmedia.com/vR1H3t5ezVMlfOa85U2nv8CFIEe0-WSfW8pF48jj7ns.jpg" title="Tested quantization on my 8GB potato laptop here's what actually breaks first" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been running local LLMs on my broke-student laptop (8GB RAM, i3 processor) and kept hitting the quantization guessing game. Downloaded like 10 different formats trying to figure out which one wouldn't destroy quality.&lt;/p&gt; &lt;p&gt;Here's what I found from testing TinyLlama and reading through hundreds of benchmark results:&lt;/p&gt; &lt;p&gt;Findings:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/0x2atfgwdy3g1.png?width=730&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b4feadc39dc6e8a7dc96e7d8d4e63393b13d0859"&gt;https://preview.redd.it/0x2atfgwdy3g1.png?width=730&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b4feadc39dc6e8a7dc96e7d8d4e63393b13d0859&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The Pattern:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;General chat: Survives down to Q4 pretty well (2-3% quality drop)&lt;/li&gt; &lt;li&gt;Creative writing: Actually stays decent even at Q3&lt;/li&gt; &lt;li&gt;Code generation: Starts getting buggy at Q4 (5-10% drop)&lt;/li&gt; &lt;li&gt;Math/reasoning: Falls off a CLIFF at Q4 (15-20% accuracy drop)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Data Sources:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Llama 3.1 8B (multiple quant formats from TheBloke/bartowski)&lt;/li&gt; &lt;li&gt;Mistral 7B v0.3 (various GGUF quants)&lt;/li&gt; &lt;li&gt;Qwen2 7B (official quants)&lt;/li&gt; &lt;li&gt;Phi-3 Mini (Microsoft's quants)&lt;/li&gt; &lt;li&gt;Tested on: MMLU (general reasoning), HumanEval (coding), GSM8K (math), creative writing prompts&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Compiled from:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;HuggingFace model cards with reported benchmarks&lt;/li&gt; &lt;li&gt;Open LLM Leaderboard results&lt;/li&gt; &lt;li&gt;llama.cpp community benchmarks on GitHub&lt;/li&gt; &lt;li&gt;My own testing on TinyLlama 1.1B (what my laptop can actually run)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This is aggregated trends across models, not exhaustive testing. Different models degrade slightly differently, but the PATTERN holds - math breaks way faster than other tasks.&lt;/p&gt; &lt;p&gt;Why this matters: If you're using a model for coding or math, Q4 might seem fine in casual testing but will randomly fail on complex problems. Meanwhile creative tasks are way more forgiving.&lt;/p&gt; &lt;p&gt;My conclusion: Q5_K_M is the sweet spot - 95%+ quality, fits on 8GB systems, doesn't randomly break on specific tasks.&lt;/p&gt; &lt;p&gt;Now heres my question would anyone actually pay for a tool that analyzes YOUR specific model/use-case and predicts which quantization to use BEFORE downloading 50GB of different formats?&lt;/p&gt; &lt;p&gt;I'm thinking of building this because I'm tired of the trial-and-error, but want to know if it's just me being lazy or an actual problem people would pay to solve.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Even_Ganache6148"&gt; /u/Even_Ganache6148 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8p844/tested_quantization_on_my_8gb_potato_laptop_heres/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8p844/tested_quantization_on_my_8gb_potato_laptop_heres/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p8p844/tested_quantization_on_my_8gb_potato_laptop_heres/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-28T07:54:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1p91eay</id>
    <title>MI50 price hike, are they moving inventory at that price?</title>
    <updated>2025-11-28T17:56:30+00:00</updated>
    <author>
      <name>/u/emaiksiaime</name>
      <uri>https://old.reddit.com/user/emaiksiaime</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was monitoring the price on ebay, they would go 300$CAD free shipping and now they are 550$CAD+ why the sudden price hike? Are they even selling at this price? Seems like a dick move to me. On another note, there are plenty of rtx 3090 for 800$CAD on marketplace if you are willing to drive around... Why does it suck so much acquiring proper VRAM?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/emaiksiaime"&gt; /u/emaiksiaime &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p91eay/mi50_price_hike_are_they_moving_inventory_at_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p91eay/mi50_price_hike_are_they_moving_inventory_at_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p91eay/mi50_price_hike_are_they_moving_inventory_at_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-28T17:56:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1p93syj</id>
    <title>Qwen3-Next: Did a quant with extended context</title>
    <updated>2025-11-28T19:30:20+00:00</updated>
    <author>
      <name>/u/noctrex</name>
      <uri>https://old.reddit.com/user/noctrex</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For anyone interested, I made an MXFP4 quant with the context extended from 256k to 1M, with YaRN as seen on &lt;a href="https://huggingface.co/unsloth/Qwen3-Next-80B-A3B-Thinking-GGUF#processing-ultra-long-texts"&gt;unsloth's repo&lt;/a&gt;:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/noctrex/Qwen3-Next-80B-A3B-Instruct-1M-MXFP4_MOE-GGUF"&gt;https://huggingface.co/noctrex/Qwen3-Next-80B-A3B-Instruct-1M-MXFP4_MOE-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/noctrex/Qwen3-Next-80B-A3B-Thinking-1M-MXFP4_MOE-GGUF"&gt;https://huggingface.co/noctrex/Qwen3-Next-80B-A3B-Thinking-1M-MXFP4_MOE-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;To enable it, run llama.cpp with options like:&lt;br /&gt; &lt;code&gt;--ctx-size 0 --rope-scaling yarn --rope-scale 4&lt;/code&gt;&lt;br /&gt; ctx-size 0 sets it to 1M context, else set a smaller number like 524288 for 512k&lt;/p&gt; &lt;p&gt;You can use also as normal if you don't want the extended context.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/noctrex"&gt; /u/noctrex &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p93syj/qwen3next_did_a_quant_with_extended_context/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p93syj/qwen3next_did_a_quant_with_extended_context/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p93syj/qwen3next_did_a_quant_with_extended_context/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-28T19:30:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1p91p4k</id>
    <title>What broke when you tried to take local LLMs to production?</title>
    <updated>2025-11-28T18:07:31+00:00</updated>
    <author>
      <name>/u/Defilan</name>
      <uri>https://old.reddit.com/user/Defilan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Curious what people's experience has been going from &amp;quot;Ollama on my laptop&amp;quot; to actually serving models to a team or company.&lt;/p&gt; &lt;p&gt;I keep seeing blog posts about the Ollama ‚Üí vLLM migration path, GPU memory headaches, cold start times, etc. But I'm wondering how much of that is real vs. content marketing fluff.&lt;/p&gt; &lt;p&gt;For those who've actually tried to productionize local models, what surprised you? What broke? What's your stack look like now?&lt;/p&gt; &lt;p&gt;Trying to separate the signal from the noise here.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Defilan"&gt; /u/Defilan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p91p4k/what_broke_when_you_tried_to_take_local_llms_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p91p4k/what_broke_when_you_tried_to_take_local_llms_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p91p4k/what_broke_when_you_tried_to_take_local_llms_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-28T18:07:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1p8onns</id>
    <title>I cooked abliterated gemma3-27b-it with norm-preserving technique</title>
    <updated>2025-11-28T07:17:10+00:00</updated>
    <author>
      <name>/u/Perfect_Biscotti_476</name>
      <uri>https://old.reddit.com/user/Perfect_Biscotti_476</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Gemma 3 27B Instruct - Norm-Preserving Abliterated&lt;/h1&gt; &lt;p&gt;I'm excited to share my contribution to the community: a &lt;strong&gt;norm-preserving abliterated version of Google's Gemma 3 27B Instruct&lt;/strong&gt;! Consider it a late Thanksgiving present.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/YanLabs/gemma3-27b-it-abliterated-normpreserve"&gt;https://huggingface.co/YanLabs/gemma3-27b-it-abliterated-normpreserve&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This model uses the &lt;strong&gt;norm-preserving biprojected abliteration&lt;/strong&gt; technique, which surgically removes refusal mechanisms while preserving reasoning capabilities.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Model&lt;/strong&gt;: &lt;a href="https://huggingface.co/YanLabs/gemma3-27b-it-abliterated-normpreserve"&gt;YanLabs/gemma3-27b-it-abliterated-normpreserve&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;Technique&lt;/strong&gt;: &lt;a href="https://github.com/jim-plus/llm-abliteration"&gt;jim-plus/llm-abliteration&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;Hardware&lt;/strong&gt;: Cooked on a rented A100 GPU via RunPod&lt;/p&gt; &lt;p&gt;GGUF files are now available at YanLabs/gemma-3-27b-abliterated-normpreserve-GGUF&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/YanLabs/gemma-3-27b-abliterated-normpreserve-GGUF"&gt;https://huggingface.co/YanLabs/gemma-3-27b-abliterated-normpreserve-GGUF&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Disclaimer&lt;/h1&gt; &lt;p&gt;This model has safety guardrails removed. &lt;strong&gt;Research purposes only.&lt;/strong&gt; Use responsibly and in compliance with applicable laws.&lt;/p&gt; &lt;h1&gt;About Me&lt;/h1&gt; &lt;p&gt;I'm an LLM enthusiast and practicing lawyer based in Shanghai. If your AI company needs legal services (domestic or international), feel free to reach out!&lt;/p&gt; &lt;ul&gt; &lt;li&gt;üìß [&lt;a href="mailto:ruiqingyan@outlook.com"&gt;ruiqingyan@outlook.com&lt;/a&gt;](mailto:&lt;a href="mailto:ruiqingyan@outlook.com"&gt;ruiqingyan@outlook.com&lt;/a&gt;)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Happy experimenting! üöÄ&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Perfect_Biscotti_476"&gt; /u/Perfect_Biscotti_476 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8onns/i_cooked_abliterated_gemma327bit_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8onns/i_cooked_abliterated_gemma327bit_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p8onns/i_cooked_abliterated_gemma327bit_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-28T07:17:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1p8zfdr</id>
    <title>Best Models for 16GB VRAM</title>
    <updated>2025-11-28T16:39:51+00:00</updated>
    <author>
      <name>/u/LinuxIsFree</name>
      <uri>https://old.reddit.com/user/LinuxIsFree</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Swiped up an RX 9070 from newegg since it's below MSRP today. Primarily interested in gaming, hence the 9070 over the 5070 at a similar price. However, Id like to sip my toes further into AI, and since Im doubling my vram from igb to 16gb, Im curious&lt;/p&gt; &lt;p&gt;**What are the best productivity, coding, ans storywriting AI models I can run reasonably with 16GB VRAM?&lt;/p&gt; &lt;p&gt;Last similar post I found with google was about 10mo old, and I figured things may have changed since then?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LinuxIsFree"&gt; /u/LinuxIsFree &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8zfdr/best_models_for_16gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8zfdr/best_models_for_16gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p8zfdr/best_models_for_16gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-28T16:39:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1p96n9d</id>
    <title>Looking for a local AI tool that can extract any info from high-quality sources (papers + reputable publications) with real citations</title>
    <updated>2025-11-28T21:27:53+00:00</updated>
    <author>
      <name>/u/Inflation_Artistic</name>
      <uri>https://old.reddit.com/user/Inflation_Artistic</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm trying to set up a fully local AI workflow (English/Chinese) that can dig through &lt;em&gt;both&lt;/em&gt; scientific papers &lt;em&gt;and&lt;/em&gt; reputable publications things like Bloomberg, Economist, reputable industry analyses, tech reports, etc.&lt;/p&gt; &lt;p&gt;The main goal:&lt;br /&gt; I want to automatically extract any specific information I request, not just statistics, but &lt;em&gt;any data&lt;/em&gt;, like:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;numbers&lt;/li&gt; &lt;li&gt;experimental details&lt;/li&gt; &lt;li&gt;comparisons&lt;/li&gt; &lt;li&gt;anything else I ask for&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;And the most important requirement:&lt;br /&gt; The tool must always give &lt;em&gt;real citations&lt;/em&gt; (article, link, page, paragraph) so I can verify every piece of data. No hallucinated facts.&lt;/p&gt; &lt;p&gt;Ideally, the tool should:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;run 100% locally&lt;/li&gt; &lt;li&gt;search deeply and for long periods&lt;/li&gt; &lt;li&gt;support Chinese + English&lt;/li&gt; &lt;li&gt;extract structured or unstructured data depending on the query&lt;/li&gt; &lt;li&gt;keep exact source references for everything&lt;/li&gt; &lt;li&gt;work on an &lt;strong&gt;RTX 3060 12GB&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Basically, I‚Äôm looking for a local ‚ÄúAI-powered research engine‚Äù that can dig through a large collection of credible sources and give me trustworthy, citation-backed answers to complex queries.&lt;/p&gt; &lt;p&gt;Has anyone built something like this?&lt;br /&gt; What tools, models, or workflows would you recommend for a 12GB GPU?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Inflation_Artistic"&gt; /u/Inflation_Artistic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p96n9d/looking_for_a_local_ai_tool_that_can_extract_any/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p96n9d/looking_for_a_local_ai_tool_that_can_extract_any/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p96n9d/looking_for_a_local_ai_tool_that_can_extract_any/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-28T21:27:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1p97lqq</id>
    <title>pmp - manage your prompts locally</title>
    <updated>2025-11-28T22:07:29+00:00</updated>
    <author>
      <name>/u/vivis-dev</name>
      <uri>https://old.reddit.com/user/vivis-dev</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/julio-mcdulio/pmp"&gt;https://github.com/julio-mcdulio/pmp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I've been working with LLMs a lot lately and got tired of managing prompts in random text files and copy-pasting them around. So I built `pmp` - a simple cli tool for managing prompts with versioning and pluggable storage backends.&lt;/p&gt; &lt;p&gt;There are quite a few products out there like mlflow and langfuse, but they come with a lot of bells and whistles and have complex deployments with a web frontend. I just wanted something simple and lightweight with no dependencies.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ pmp add code-reviewer --content &amp;quot;Review this code for bugs and improvements&amp;quot; --tag &amp;quot;code,review&amp;quot; --model &amp;quot;gpt-4&amp;quot; prompt &amp;quot;code-reviewer&amp;quot; version 1 created $ pmp get code-reviewer Review this code for bugs and improvements $ pmp update code-reviewer --content &amp;quot;Review this code thoroughly for bugs, security issues, and improvements&amp;quot; prompt &amp;quot;code-reviewer&amp;quot; version 2 created $ pmp list --tag code code-reviewer summarize &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I've also added support for a &lt;a href="https://github.com/google/dotprompt"&gt;dotprompt&lt;/a&gt; storage backend, and I'm planning to add support for different execution backends which will let you run your prompts using tools like llm, gemini cli and openai-cli.&lt;/p&gt; &lt;p&gt;Interested to hear what you think!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vivis-dev"&gt; /u/vivis-dev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p97lqq/pmp_manage_your_prompts_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p97lqq/pmp_manage_your_prompts_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p97lqq/pmp_manage_your_prompts_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-28T22:07:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1p8hqq4</id>
    <title>Apparently Asus is working with Nvidia on a 784GB "Coherent" Memory desktop PC with 20 PFLOPS AI Performance</title>
    <updated>2025-11-28T00:56:49+00:00</updated>
    <author>
      <name>/u/waiting_for_zban</name>
      <uri>https://old.reddit.com/user/waiting_for_zban</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Somehow the announcement went under the radar, but back in May, along side the Ascent GX10, Asus announced the &lt;a href="https://www.asus.com/displays-desktops/workstations/performance/expertcenter-pro-et900n-g3/"&gt;ExpertCenter Pro ET900N G3&lt;/a&gt;, with GB300 Blackwell. They don't really say what's a &amp;quot;Coherent&amp;quot; memory, but my guess it's another term of saying unified memory like Apple and AMD. &lt;/p&gt; &lt;p&gt;The announcement and the specs are very dry on details, but given the GB300, we might get a very decent memory bandwidth, without &lt;a href="https://i.imgur.com/pNaKzWb.png"&gt;looking like a hideous frankestein monster&lt;/a&gt;. &lt;/p&gt; &lt;p&gt;This might be &lt;a href="/r/Localllama"&gt;r/Localllama&lt;/a&gt; wet dream. If they manage to price it well, and fix that memory bandwidth (that plagued Spark), they have my money. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;EDIT:&lt;/strong&gt; As &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8hqq4/apparently_asus_is_working_with_nvidia_on_a_784gb/nr5ae77/"&gt;many&lt;/a&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8hqq4/apparently_asus_is_working_with_nvidia_on_a_784gb/nr5jvc1/"&gt;pointed out&lt;/a&gt; in the comments, it's based on the &lt;a href="https://www.nvidia.com/en-us/products/workstations/dgx-station/"&gt;Nvidia DGX Station&lt;/a&gt;, announced back in March, which is &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8hqq4/apparently_asus_is_working_with_nvidia_on_a_784gb/nr5jvc1/"&gt;rumored to be 80k&lt;/a&gt;. ServeTheHome had a &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jl4amv/a_closer_look_at_the_nvidia_dgx_station_gb300/"&gt;nice article about it&lt;/a&gt; back in March.&lt;br /&gt; The official specs: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;496GB LPDDR5X CPU memory at 396GB/s (Micron SOCAMM, so it seems that it will be modular not soldered!)&lt;br /&gt;&lt;/li&gt; &lt;li&gt;288GB HBM3e GPU memory at 8TB/s.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/waiting_for_zban"&gt; /u/waiting_for_zban &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8hqq4/apparently_asus_is_working_with_nvidia_on_a_784gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8hqq4/apparently_asus_is_working_with_nvidia_on_a_784gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p8hqq4/apparently_asus_is_working_with_nvidia_on_a_784gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-28T00:56:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1p9563c</id>
    <title>Z_Image benchmark with simulating VRAM Limits on RTX 5090 &amp; 3090</title>
    <updated>2025-11-28T20:25:39+00:00</updated>
    <author>
      <name>/u/namjuu_ka09114</name>
      <uri>https://old.reddit.com/user/namjuu_ka09114</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9563c/z_image_benchmark_with_simulating_vram_limits_on/"&gt; &lt;img alt="Z_Image benchmark with simulating VRAM Limits on RTX 5090 &amp;amp; 3090" src="https://b.thumbs.redditmedia.com/7oYXvTGJegOmwpjlJpGRq6M3tIh6kruN6RuJFFdK50U.jpg" title="Z_Image benchmark with simulating VRAM Limits on RTX 5090 &amp;amp; 3090" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I recently got my hands on an &lt;strong&gt;RTX 5090 (32GB)&lt;/strong&gt; and also have an &lt;strong&gt;RTX 3090 (24GB)&lt;/strong&gt;. I experimented to simulate the VRAM capacity of the upcoming 50-series lineup (5080, 5070, etc.) and older 30-series cards.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The main goal&lt;/strong&gt; was to see what happens when VRAM runs out (OOM) and the system starts swapping to System RAM (DDR5). Specifically, I wanted to measure the performance penalty.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;‚ö†Ô∏è Disclaimer:&lt;/strong&gt; This test only limits &lt;strong&gt;VRAM Capacity&lt;/strong&gt;. It does &lt;strong&gt;NOT&lt;/strong&gt; simulate the raw compute power (CUDA cores) of lower-tier cards.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;em&gt;e.g., The &amp;quot;Simulated 5060&amp;quot; result shows how a 5090 performs when choked by 8GB VRAM, not the actual speed of a real 5060.&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Test Environment&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GPU:&lt;/strong&gt; RTX 5090 (32GB) &amp;amp; RTX 3090 (24GB)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;CPU:&lt;/strong&gt; Ryzen 9 7900X&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RAM:&lt;/strong&gt; DDR5 96GB (6000MHz)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;PSU:&lt;/strong&gt; 1600W&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Software:&lt;/strong&gt; ComfyUI (Provided Z_Image Workflow from its site/1024x1024 generation)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;OS:&lt;/strong&gt; Windows 11&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;1. RTX 3090 Results (Simulating 30-series VRAM tiers)&lt;/h1&gt; &lt;p&gt;&lt;em&gt;Comparing Native 24GB vs. Artificial Limits&lt;/em&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Simulated Tier&lt;/th&gt; &lt;th align="left"&gt;VRAM Limit&lt;/th&gt; &lt;th align="left"&gt;Cold Start (s)&lt;/th&gt; &lt;th align="left"&gt;Warm Gen (s)&lt;/th&gt; &lt;th align="left"&gt;System RAM (DRAM) Usage&lt;/th&gt; &lt;th align="left"&gt;Real VRAM Used&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;RTX 3090 (Native)&lt;/td&gt; &lt;td align="left"&gt;24 GB&lt;/td&gt; &lt;td align="left"&gt;19.07s&lt;/td&gt; &lt;td align="left"&gt;9.71s&lt;/td&gt; &lt;td align="left"&gt;Negligible&lt;/td&gt; &lt;td align="left"&gt;20 GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;16GB Tier (4080/4070Ti S)&lt;/td&gt; &lt;td align="left"&gt;16 GB&lt;/td&gt; &lt;td align="left"&gt;20.84s&lt;/td&gt; &lt;td align="left"&gt;10.43s&lt;/td&gt; &lt;td align="left"&gt;+11 GB&lt;/td&gt; &lt;td align="left"&gt;13 GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;3080 (12G) / 4070 Ti&lt;/td&gt; &lt;td align="left"&gt;12 GB&lt;/td&gt; &lt;td align="left"&gt;22.92s&lt;/td&gt; &lt;td align="left"&gt;13.82s&lt;/td&gt; &lt;td align="left"&gt;+15 GB (Generation)&lt;/td&gt; &lt;td align="left"&gt;11.1 GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;3080 (10G)&lt;/td&gt; &lt;td align="left"&gt;10 GB&lt;/td&gt; &lt;td align="left"&gt;25.38s&lt;/td&gt; &lt;td align="left"&gt;17.04s&lt;/td&gt; &lt;td align="left"&gt;+13 GB (Generation)&lt;/td&gt; &lt;td align="left"&gt;9.1 GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;3070 / 3060 Ti&lt;/td&gt; &lt;td align="left"&gt;8 GB&lt;/td&gt; &lt;td align="left"&gt;27.94s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;20.00s&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;+15 GB (Generation)&lt;/td&gt; &lt;td align="left"&gt;7.0 GB&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Analysis:&lt;/strong&gt; Performance takes a noticeable hit as soon as you drop below 12GB. At 8GB, the generation time doubles compared to the native 24GB environment. However, thanks to the system RAM, it is still usable (didn't crash).&lt;/p&gt; &lt;h1&gt;2. RTX 5090 Results (Simulating 50-series VRAM tiers)&lt;/h1&gt; &lt;p&gt;&lt;em&gt;Comparing Native 32GB vs. Artificial Limits&lt;/em&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Simulated Tier&lt;/th&gt; &lt;th align="left"&gt;VRAM Limit&lt;/th&gt; &lt;th align="left"&gt;Cold Start (s)&lt;/th&gt; &lt;th align="left"&gt;Warm Gen (s)&lt;/th&gt; &lt;th align="left"&gt;System RAM (DRAM) Usage&lt;/th&gt; &lt;th align="left"&gt;Real VRAM Used&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;RTX 5090 (Native)&lt;/td&gt; &lt;td align="left"&gt;32 GB&lt;/td&gt; &lt;td align="left"&gt;10.17s&lt;/td&gt; &lt;td align="left"&gt;3.45s&lt;/td&gt; &lt;td align="left"&gt;Negligible&lt;/td&gt; &lt;td align="left"&gt;22 GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;4090&lt;/td&gt; &lt;td align="left"&gt;24 GB&lt;/td&gt; &lt;td align="left"&gt;10.48s&lt;/td&gt; &lt;td align="left"&gt;3.33s&lt;/td&gt; &lt;td align="left"&gt;Negligible&lt;/td&gt; &lt;td align="left"&gt;21 GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;5080/5070 ti&lt;/td&gt; &lt;td align="left"&gt;16 GB&lt;/td&gt; &lt;td align="left"&gt;11.93s&lt;/td&gt; &lt;td align="left"&gt;4.20s&lt;/td&gt; &lt;td align="left"&gt;+12 GB&lt;/td&gt; &lt;td align="left"&gt;15.8 GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;5070&lt;/td&gt; &lt;td align="left"&gt;12 GB&lt;/td&gt; &lt;td align="left"&gt;12.11s&lt;/td&gt; &lt;td align="left"&gt;5.07s&lt;/td&gt; &lt;td align="left"&gt;+12.9 GB (Generation)&lt;/td&gt; &lt;td align="left"&gt;12.9 GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;5060&lt;/td&gt; &lt;td align="left"&gt;8 GB&lt;/td&gt; &lt;td align="left"&gt;11.70s&lt;/td&gt; &lt;td align="left"&gt;6.19s&lt;/td&gt; &lt;td align="left"&gt;+21 GB (Generation)&lt;/td&gt; &lt;td align="left"&gt;7 GB&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Analysis:&lt;/strong&gt; The 5090's raw power is insane. Even when limited to 8GB VRAM and forced to pull 21GB from System RAM, &lt;strong&gt;it is still faster (6.19s) than a native 3090 (9.71s).&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Note again: A real 5060 will be much slower due to fewer CUDA cores. This just proves the 5090's architectural dominance.&lt;/em&gt;&lt;/p&gt; &lt;h1&gt;Key Findings &amp;amp; Analysis&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;1. The 5090 is a Monster&lt;/strong&gt; With unlimited VRAM, the 5090 is roughly &lt;strong&gt;3x faster&lt;/strong&gt; than the 3090 in this workflow. The Blackwell chip is impressive.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. The VRAM Bottleneck &amp;amp; System RAM&lt;/strong&gt; Based on my data, when VRAM is insufficient (8GB~12GB range for SDXL), the system offloads about &lt;strong&gt;20GB&lt;/strong&gt; of data to the System DRAM.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3. Speed during Swapping&lt;/strong&gt; Both GPUs remained &amp;quot;usable&amp;quot; even when restricted to 8GB, as long as there was enough System RAM. Excluding the cold start, the generation speed was acceptable for local use.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;em&gt;However, on the 3090, the slowdown is clearly felt (9s -&amp;gt; 20s).&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;em&gt;On the 5090, the brute force computational power masks the swapping latency significantly.&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;4. Oddity&lt;/strong&gt; Software VRAM limiting wasn't 100% precise in reporting, likely due to overhead or PyTorch memory management, but the trend is clear.&lt;/p&gt; &lt;h1&gt;TL;DR&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Z_Image is efficient:&lt;/strong&gt; Great bang for the buck in terms of local generation.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RAM is King:&lt;/strong&gt; If you have &lt;strong&gt;32GB+ of System RAM&lt;/strong&gt;, even an 8GB VRAM card can run these workflows (albeit slower). It won't crash, it just swaps.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;For Speed:&lt;/strong&gt; If you want snappy generation without waiting, you probably want a &lt;strong&gt;70-class or higher&lt;/strong&gt; card (12GB+ VRAM).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;5090 Reaction:&lt;/strong&gt; It's insanely fast...&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/izgg24dj424g1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b911124200bb5336cd1246efee6c539fff1be3b5"&gt;https://preview.redd.it/izgg24dj424g1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b911124200bb5336cd1246efee6c539fff1be3b5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Test result example&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;This is the translated version of my writing in Korean&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/namjuu_ka09114"&gt; /u/namjuu_ka09114 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9563c/z_image_benchmark_with_simulating_vram_limits_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9563c/z_image_benchmark_with_simulating_vram_limits_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p9563c/z_image_benchmark_with_simulating_vram_limits_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-28T20:25:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1p90zzi</id>
    <title>CPU-only LLM performance - t/s with llama.cpp</title>
    <updated>2025-11-28T17:40:34+00:00</updated>
    <author>
      <name>/u/pmttyji</name>
      <uri>https://old.reddit.com/user/pmttyji</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How many of you do use CPU only inference time to time(at least rarely)? .... Really missing CPU-Only Performance threads here in this sub.&lt;/p&gt; &lt;p&gt;&lt;sup&gt;Possibly few of you waiting to grab one or few 96GB GPUs at cheap price later so using CPU only inference for now just with bulk RAM.&lt;/sup&gt;&lt;/p&gt; &lt;p&gt;I think bulk RAM(128GB-1TB) is more than enough to run small/medium models since it comes with more memory bandwidth.&lt;/p&gt; &lt;p&gt;My System Info: &lt;/p&gt; &lt;p&gt;Intel Core i7-14700HX 2.10 GHz | &lt;strong&gt;32 GB RAM&lt;/strong&gt; | &lt;strong&gt;DDR5-5600&lt;/strong&gt; | &lt;strong&gt;65GB/s Bandwidth&lt;/strong&gt; |&lt;/p&gt; &lt;p&gt;llama-bench Command: (Used Q8 for KVCache to get decent t/s with my 32GB RAM)&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-bench -m modelname.gguf -fa 1 -ctk q8_0 -ctv q8_0 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;CPU-only performance stats (Model Name with Quant - t/s):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Qwen3-0.6B-Q8_0 - 86 gemma-3-1b-it-UD-Q8_K_XL - 42 LFM2-2.6B-Q8_0 - 24 LFM2-2.6B.i1-Q4_K_M - 30 SmolLM3-3B-UD-Q8_K_XL - 16 SmolLM3-3B-UD-Q4_K_XL - 27 Llama-3.2-3B-Instruct-UD-Q8_K_XL - 16 Llama-3.2-3B-Instruct-UD-Q4_K_XL - 25 Qwen3-4B-Instruct-2507-UD-Q8_K_XL - 13 Qwen3-4B-Instruct-2507-UD-Q4_K_XL - 20 gemma-3-4b-it-qat-UD-Q6_K_XL - 17 gemma-3-4b-it-UD-Q4_K_XL - 20 Phi-4-mini-instruct.Q8_0 - 16 Phi-4-mini-instruct-Q6_K - 18 granite-4.0-micro-UD-Q8_K_XL - 15 granite-4.0-micro-UD-Q4_K_XL - 24 MiniCPM4.1-8B.i1-Q4_K_M - 10 Llama-3.1-8B-Instruct-UD-Q4_K_XL - 11 Qwen3-8B-128K-UD-Q4_K_XL - 9 gemma-3-12b-it-Q6_K - 6 gemma-3-12b-it-UD-Q4_K_XL - 7 Mistral-Nemo-Instruct-2407-IQ4_XS - 10 Huihui-Ling-mini-2.0-abliterated-MXFP4_MOE - 58 inclusionAI_Ling-mini-2.0-Q6_K_L - 47 LFM2-8B-A1B-UD-Q4_K_XL - 38 ai-sage_GigaChat3-10B-A1.8B-Q4_K_M - 34 Ling-lite-1.5-2507-MXFP4_MOE - 31 granite-4.0-h-tiny-UD-Q4_K_XL - 29 granite-4.0-h-small-IQ4_XS - 9 gemma-3n-E2B-it-UD-Q4_K_XL - 28 gemma-3n-E4B-it-UD-Q4_K_XL - 13 kanana-1.5-15.7b-a3b-instruct-i1-MXFP4_MOE - 24 ERNIE-4.5-21B-A3B-PT-IQ4_XS - 28 SmallThinker-21BA3B-Instruct-IQ4_XS - 26 Phi-mini-MoE-instruct-Q8_0 - 25 Qwen3-30B-A3B-IQ4_XS - 27 gpt-oss-20b-mxfp4 - 23 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;So it seems I would get 3-4X performance if I build a desktop with 128GB DDR5 RAM 6000-6600. For example, above t/s * 4 for 128GB (32GB * 4). And 256GB could give 7-8X and so on. Of course I'm aware of context of models here.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Qwen3-4B-Instruct-2507-UD-Q8_K_XL - 52 (13 * 4) gpt-oss-20b-mxfp4 - 92 (23 * 4) Qwen3-8B-128K-UD-Q4_K_XL - 36 (9 * 4) gemma-3-12b-it-UD-Q4_K_XL - 28 (7 * 4) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I stopped bothering 12+B Dense models since Q4 of 12B Dense models itself bleeding tokens in single digits(Ex: Gemma3-12B just 7 t/s). But I really want to know the CPU-only performance of 12+B Dense models so it could help me deciding to get how much RAM needed for expected t/s. Sharing list for reference, it would be great if someone shares stats of these models. &lt;/p&gt; &lt;pre&gt;&lt;code&gt;Seed-OSS-36B-Instruct-GGUF Mistral-Small-3.2-24B-Instruct-2506-GGUF Devstral-Small-2507-GGUF Magistral-Small-2509-GGUF phi-4-gguf RekaAI_reka-flash-3.1-GGUF NVIDIA-Nemotron-Nano-9B-v2-GGUF NVIDIA-Nemotron-Nano-12B-v2-GGUF GLM-Z1-32B-0414-GGUF Llama-3_3-Nemotron-Super-49B-v1_5-GGUF Qwen3-14B-GGUF Qwen3-32B-GGUF NousResearch_Hermes-4-14B-GGUF gemma-3-12b-it-GGUF gemma-3-27b-it-GGUF &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Please share your stats with your config(Total RAM, RAM Type - MT/s, Total Bandwidth) &amp;amp; whatever models(Quant, t/s) you tried. &lt;/p&gt; &lt;p&gt;And let me know if any changes needed in my llama-bench command to get better t/s. Hope there are few. Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmttyji"&gt; /u/pmttyji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p90zzi/cpuonly_llm_performance_ts_with_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p90zzi/cpuonly_llm_performance_ts_with_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p90zzi/cpuonly_llm_performance_ts_with_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-28T17:40:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1p93jcu</id>
    <title>Gemma3 27 heretic, lower divergence than mlabonne/gemma3</title>
    <updated>2025-11-28T19:19:34+00:00</updated>
    <author>
      <name>/u/coder3101</name>
      <uri>https://old.reddit.com/user/coder3101</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I set out to abliterate Gemma3 27b, wanted to reach or surpass the most popular one and here's the results after 5hr on H100 using heretic. &lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;KL Divergence&lt;/th&gt; &lt;th align="left"&gt;Refusal&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/google/gemma-3-27b-pt"&gt;Google's base model&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;0 (by definition)&lt;/td&gt; &lt;td align="left"&gt;98/100&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated"&gt;mlabonne's gemma3&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;0.08&lt;/td&gt; &lt;td align="left"&gt;6/100&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/coder3101/gemma-3-27b-it-heretic"&gt;Heretic gemma3 - v1&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;0.07&lt;/td&gt; &lt;td align="left"&gt;7/100&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/coder3101/gemma-3-27b-it-heretic-v2"&gt;Heretic gemma3 - v2&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;0.03&lt;/td&gt; &lt;td align="left"&gt;14/100&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;KL Divergence:&lt;/strong&gt; Lower the better, roughly a measure of how close the model should be to its original. It is worth noting that lower, better for quantization.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Refusal:&lt;/strong&gt; Lower the better, measure of how many harmful prompts model refused, this is calculated based on presence of tokens such &amp;quot;sorry&amp;quot; etc, which gives a general measure. &lt;/p&gt; &lt;p&gt;I published two versions - one with slightly higher refusal but very low KL divergence and another almost close to that of mlabonne's. It is also worth noting that during my testing I couldn't get v2 to refuse on any prompts, so that would mean it should be much close to original model without refusing on many stuff.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/coder3101"&gt; /u/coder3101 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p93jcu/gemma3_27_heretic_lower_divergence_than/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p93jcu/gemma3_27_heretic_lower_divergence_than/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p93jcu/gemma3_27_heretic_lower_divergence_than/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-28T19:19:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1p8ven0</id>
    <title>Compared actual usage costs for Chinese AI models. Token efficiency changes everything.</title>
    <updated>2025-11-28T13:54:37+00:00</updated>
    <author>
      <name>/u/YormeSachi</name>
      <uri>https://old.reddit.com/user/YormeSachi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Everyone talks about per-token pricing but nobody mentions token efficiency. How many tokens does it take to complete the same task?&lt;/p&gt; &lt;p&gt;Tested this with coding tasks cause thats where I actually use these models.&lt;/p&gt; &lt;p&gt;glm-4.6: $0.15 input / $0.60 output Kimi K2: $1.50-2.00 MiniMax: $0.80-1.20 deepseek: $0.28&lt;/p&gt; &lt;p&gt;deepseek looks cheapest on paper. But thats not the whole story.&lt;/p&gt; &lt;p&gt;Token efficiency (same task):&lt;/p&gt; &lt;p&gt;Gave each model identical coding task: &amp;quot;refactor this component to use hooks, add error handling, write tests&amp;quot;&lt;/p&gt; &lt;p&gt;glm: 8,200 tokens average deepseek: 14,800 tokens average MiniMax: 10,500 tokens average, Kimi: 11,000 tokens average&lt;/p&gt; &lt;p&gt;glm uses 26% fewer tokens than Kimi, 45% fewer than deepseek.&lt;/p&gt; &lt;p&gt;Real cost for that task:&lt;/p&gt; &lt;p&gt;glm: ~$0.04 (4 cents) deepseek: ~$0.03 (3 cents) - looks cheaper MiniMax: ~$0.05 (5 cents) Kimi: ~$0.09 (9 cents)&lt;/p&gt; &lt;p&gt;But wait. If you do 100 similar tasks:&lt;/p&gt; &lt;p&gt;glm: Total tokens needed: ~820K, Cost: $0.40-0.50 deepseek: Total tokens needed: ~1.48M, Cost: $0.41 - basically same as glm despite lower per-token price MiniMax: Total tokens needed: ~1.05M, Cost: $0.50-0.60 Kimi: Total tokens needed: ~1.1M, Cost: $0.90-1.00&lt;/p&gt; &lt;p&gt;Token efficiency beats per-token price. glm generates less verbose code, fewer explanatory comments, tighter solutions. deepseek tends to over-explain and generate longer outputs.&lt;/p&gt; &lt;p&gt;For businesses doing thousands of API calls daily, glms efficiency compounds into real savings even though its not the absolute cheapest per-token.&lt;/p&gt; &lt;p&gt;Switched to glm for production workloads. Monthly costs dropped 60% vs previous setup. Performance is adequate for 90% of tasks.&lt;/p&gt; &lt;p&gt;deepseeks pricing looks great until you realize youre using 50% more tokens per task. The savings disappear.&lt;/p&gt; &lt;p&gt;Anyone else measuring token efficiency? Feel like this is the underrated metric everyone ignores.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/YormeSachi"&gt; /u/YormeSachi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8ven0/compared_actual_usage_costs_for_chinese_ai_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8ven0/compared_actual_usage_costs_for_chinese_ai_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p8ven0/compared_actual_usage_costs_for_chinese_ai_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-28T13:54:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1p93r0w</id>
    <title>Benchmarking LLM Inference on RTX PRO 6000 vs H100 vs H200</title>
    <updated>2025-11-28T19:28:09+00:00</updated>
    <author>
      <name>/u/NoVibeCoding</name>
      <uri>https://old.reddit.com/user/NoVibeCoding</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p93r0w/benchmarking_llm_inference_on_rtx_pro_6000_vs/"&gt; &lt;img alt="Benchmarking LLM Inference on RTX PRO 6000 vs H100 vs H200" src="https://b.thumbs.redditmedia.com/rFGM4WACmn7770XlqkGVAHUlLK43-ZKyVzkyAkXKtqk.jpg" title="Benchmarking LLM Inference on RTX PRO 6000 vs H100 vs H200" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi LocalLlama community. I present an LLM inference throughput benchmark for RTX PRO 6000 WK vs H100 vs H200 vs L40S GPUs, based on the vllm serve and vllm bench serve benchmarking tools, to understand the cost efficiency of RTX PRO 6000 vs previous-generation datacenter GPUs for LLM inference. Pro 6000 is significantly cheaper and is built on the latest Blackwell architecture, but it has slower GDDR memory and lacks NVLink.&lt;/p&gt; &lt;p&gt;&lt;a href="https://medium.com/p/fde1798571a1"&gt;Full article on Medium&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.cloudrift.ai/blog/benchmarking-rtx6000-vs-datacenter-gpus"&gt;Non-medium link&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Benchmarking Setup&lt;/h1&gt; &lt;p&gt;The hardware configurations used:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;1xPRO6000; 1xH100; 1xH200; 2xL40s&lt;/li&gt; &lt;li&gt;8xPRO6000; 8xH100; 8xH200&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;I have optimized the benchmark setup for throughput.&lt;/strong&gt; VLLM serves models. The model is split across multiple GPUs using the --tensor-parallel-size VLLM option, if needed. I run as many VLLM instances as possible, using an NGINX load balancer on top to distribute requests across them and maximize throughput (replica parallelism). For example, if only four GPUs are required to run the model on an 8-GPU machine, I run two VLLM instances with --tensor-parallel-size=4 and an NGINX load balancer. If all eight GPUs are required, then a single VLLM instance with --tensor-parallel-size=8 is used.&lt;/p&gt; &lt;p&gt;The &lt;strong&gt;vllm bench serve&lt;/strong&gt; tool is used for benchmarking with random data and a sequence length of 1000. The number of concurrent requests is set between 256 and 512 to ensure the LLM's token-generation capacity is saturated.&lt;/p&gt; &lt;p&gt;I have benchmarked three models to better understand the effect of PCIe communication on the 8xPro6000 server vs. NVLink on the H100/H200.&lt;/p&gt; &lt;p&gt;Here is the model selection and the logic behind it:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;GLM-4.5-Air-AWQ-4bit (fits 80GB).&lt;/strong&gt; Testing single-GPU performance and maximum throughput with replica scaling on 8 GPU setups. No PCIE bottleneck. The Pro 6000 should demonstrate strong results thanks to Blackwell native support for FP4.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Qwen3-Coder-480B-A35B-Instruct-AWQ (fits 320GB).&lt;/strong&gt; This 4-bit-quantized model fits into 4 GPUs. Some PCIe communication overhead in Pro 6000 setups may reduce performance relative to NVLink-enabled datacenter GPUs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GLM-4.6-FP8 (fits 640GB).&lt;/strong&gt; This model requires all eight GPUs. PCIe communication overhead expected. The H100 and H200 configurations should have an advantage.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Besides raw throughput, graphs show the serving cost per million tokens for each model on its respective hardware. The rental price is set to $2.09 for Pro6000; $2.69 for H100; $3.39 for H200, and $0.86 for L40S - today's rental prices from Runpod secure cloud.&lt;/p&gt; &lt;h1&gt;Results&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;For single-GPU workloads&lt;/strong&gt;, RTX PRO 6000 is a clear winner‚Äîand arguably an H100 killer. Remarkably, the PRO 6000 with GDDR7 memory outperforms even the H100 SXM with its HBM3e in single-GPU throughput (3,140 vs 2,987 tok/s), while delivering 28% lower cost per token ($0.18 vs $0.25/mtok). The 2xL40S configuration is the least performant and most cost-effective of the bunch.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;For medium-sized models&lt;/strong&gt; requiring 2-4 GPUs, PRO 6000 remains competitive. While it loses some ground to NVLink-equipped datacenter GPUs, the cost efficiency stays within the same ballpark ($1.03 vs $1.01/mtok for Qwen3-480B).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;For large models&lt;/strong&gt; requiring 8-way tensor parallelism, datacenter GPUs pull ahead significantly. The H100 and H200's NVLink interconnect delivers 3-4x the throughput of PCIe-bound PRO 6000s. The cost efficiency gap is significant: $1.72/mtok for Pro6000 vs $0.72-0.76/mtok for H100/H200.&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/qyda1ooas14g1.gif"&gt;Price in millidollars, i.e. around $0.2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/69a68sbks14g1.gif"&gt;https://i.redd.it/69a68sbks14g1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/rtyvpiars14g1.gif"&gt;https://i.redd.it/rtyvpiars14g1.gif&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Code and Resources&lt;/h1&gt; &lt;p&gt;The code is available &lt;a href="https://github.com/cloudrift-ai/server-benchmark"&gt;here&lt;/a&gt;. Instructions for performing your own benchmark are in the README. You can find the benchmark data in the results folder.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NoVibeCoding"&gt; /u/NoVibeCoding &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p93r0w/benchmarking_llm_inference_on_rtx_pro_6000_vs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p93r0w/benchmarking_llm_inference_on_rtx_pro_6000_vs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p93r0w/benchmarking_llm_inference_on_rtx_pro_6000_vs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-28T19:28:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1p8w9hg</id>
    <title>unsloth/Qwen3-Next-80B-A3B-Thinking-GGUF ¬∑ Hugging Face</title>
    <updated>2025-11-28T14:31:50+00:00</updated>
    <author>
      <name>/u/WhaleFactory</name>
      <uri>https://old.reddit.com/user/WhaleFactory</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8w9hg/unslothqwen3next80ba3bthinkinggguf_hugging_face/"&gt; &lt;img alt="unsloth/Qwen3-Next-80B-A3B-Thinking-GGUF ¬∑ Hugging Face" src="https://external-preview.redd.it/ble3gnyoRHIxGbCkynVYdB5oBvepM5IUsQgkKmTQPvE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8b518c02f5843d8adaaa30fd9ccf229f596ac77a" title="unsloth/Qwen3-Next-80B-A3B-Thinking-GGUF ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WhaleFactory"&gt; /u/WhaleFactory &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/unsloth/Qwen3-Next-80B-A3B-Thinking-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8w9hg/unslothqwen3next80ba3bthinkinggguf_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p8w9hg/unslothqwen3next80ba3bthinkinggguf_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-28T14:31:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1p8s7az</id>
    <title>Model: Qwen3 Next by pwilkin ¬∑ Pull Request #16095 ¬∑ ggml-org/llama.cpp</title>
    <updated>2025-11-28T11:05:36+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8s7az/model_qwen3_next_by_pwilkin_pull_request_16095/"&gt; &lt;img alt="Model: Qwen3 Next by pwilkin ¬∑ Pull Request #16095 ¬∑ ggml-org/llama.cpp" src="https://external-preview.redd.it/GTfGIM6FaPx4w5_-UCOwiPgKZNkGDkC0q-Pvot4uDk0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=45f974480724f58cdf70046026bd0ccf7e6b00f6" title="Model: Qwen3 Next by pwilkin ¬∑ Pull Request #16095 ¬∑ ggml-org/llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;and it's done&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/16095"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8s7az/model_qwen3_next_by_pwilkin_pull_request_16095/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p8s7az/model_qwen3_next_by_pwilkin_pull_request_16095/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-28T11:05:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1p8wcbn</id>
    <title>Ask me to run models</title>
    <updated>2025-11-28T14:35:08+00:00</updated>
    <author>
      <name>/u/monoidconcat</name>
      <uri>https://old.reddit.com/user/monoidconcat</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8wcbn/ask_me_to_run_models/"&gt; &lt;img alt="Ask me to run models" src="https://b.thumbs.redditmedia.com/SlyBYDnPqUybVrSsHIbiqHHjqXLR7EhbE0-6UyLUGJk.jpg" title="Ask me to run models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys, I am currently in the process of upgrading my 4√ó3090 setup to 2√ó5090 + 1√óRTX Pro 6000. As a result, I have all three kinds of cards in the rig temporarily, and I thought it would be a good idea to take some requests for models to run on my machine.&lt;/p&gt; &lt;p&gt;Here is my current setup: - 1√ó RTX Pro 6000 Blackwell, power limited to 525 W - 2√ó RTX 5090, power limited to 500 W - 2√ó RTX 3090, power limited to 280 W - WRX80E (PCIe 4.0 x16) with 3975WX - 512 GB DDR4 RAM&lt;/p&gt; &lt;p&gt;If you have any model that you want me to run with a specific setup (certain cards, parallelism methods, etc.), let me know in the comments. I‚Äôll run them this weekend and reply with the tok/s!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/monoidconcat"&gt; /u/monoidconcat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1p8wcbn"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8wcbn/ask_me_to_run_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p8wcbn/ask_me_to_run_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-28T14:35:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1p8v9y9</id>
    <title>unsloth/Qwen3-Next-80B-A3B-Instruct-GGUF ¬∑ Hugging Face</title>
    <updated>2025-11-28T13:48:28+00:00</updated>
    <author>
      <name>/u/WhaleFactory</name>
      <uri>https://old.reddit.com/user/WhaleFactory</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8v9y9/unslothqwen3next80ba3binstructgguf_hugging_face/"&gt; &lt;img alt="unsloth/Qwen3-Next-80B-A3B-Instruct-GGUF ¬∑ Hugging Face" src="https://external-preview.redd.it/SSIhbD5Dl8kZRyNgV0oqxKpaE8kMvA_ZXLBFpkDEq90.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1291833f6c1644105b326fbe9244666f7b478451" title="unsloth/Qwen3-Next-80B-A3B-Instruct-GGUF ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WhaleFactory"&gt; /u/WhaleFactory &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/unsloth/Qwen3-Next-80B-A3B-Instruct-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8v9y9/unslothqwen3next80ba3binstructgguf_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p8v9y9/unslothqwen3next80ba3binstructgguf_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-28T13:48:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5retd</id>
    <title>Best Local VLMs - November 2025</title>
    <updated>2025-11-24T20:00:04+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Share what your favorite models are right now and &lt;strong&gt;&lt;em&gt;why&lt;/em&gt;&lt;/strong&gt;. Given the nature of the beast in evaluating VLMs (untrustworthiness of benchmarks, immature tooling, intrinsic stochasticity), please be as detailed as possible in describing your setup, nature of your usage (what applications, how much, personal/professional use), tools/frameworks/prompts etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5retd/best_local_vlms_november_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5retd/best_local_vlms_november_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p5retd/best_local_vlms_november_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T20:00:04+00:00</published>
  </entry>
</feed>
