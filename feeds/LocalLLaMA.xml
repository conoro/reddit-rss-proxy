<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-01T12:11:49+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1nunq7s</id>
    <title>GPT-OSS-120B Performance on 4 x 3090</title>
    <updated>2025-09-30T20:07:59+00:00</updated>
    <author>
      <name>/u/Mr_Moonsilver</name>
      <uri>https://old.reddit.com/user/Mr_Moonsilver</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Have been running a task for synthetic datageneration on a 4 x 3090 rig.&lt;/p&gt; &lt;p&gt;Input sequence length: 250-750 tk&lt;br /&gt; Output sequence lenght: 250 tk&lt;/p&gt; &lt;p&gt;Concurrent requests: 120&lt;/p&gt; &lt;p&gt;Avg. Prompt Throughput: 1.7k tk/s&lt;br /&gt; Avg. Generation Throughput: 1.3k tk/s&lt;/p&gt; &lt;p&gt;Power usage per GPU: Avg 280W&lt;/p&gt; &lt;p&gt;Maybe someone finds this useful.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mr_Moonsilver"&gt; /u/Mr_Moonsilver &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nunq7s/gptoss120b_performance_on_4_x_3090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nunq7s/gptoss120b_performance_on_4_x_3090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nunq7s/gptoss120b_performance_on_4_x_3090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-30T20:07:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1nusi98</id>
    <title>Demo: I made an open-source version of Imagine by Claude (released yesterday)</title>
    <updated>2025-09-30T23:21:41+00:00</updated>
    <author>
      <name>/u/Jebick</name>
      <uri>https://old.reddit.com/user/Jebick</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nusi98/demo_i_made_an_opensource_version_of_imagine_by/"&gt; &lt;img alt="Demo: I made an open-source version of Imagine by Claude (released yesterday)" src="https://external-preview.redd.it/azFrenE2YnV4ZHNmMcTEn_BPcdeQBB2xj86P8Iu8MwpzWMTuCSw-sDoAbeKM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=569644110fedf559a7fcc7d217d439a9dfdd1466" title="Demo: I made an open-source version of Imagine by Claude (released yesterday)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Yesterday, Anthropic launched Imagine with Claude to Max users. &lt;/p&gt; &lt;p&gt;I created an open-source version for anyone to try that leverages the Gemini-CLI agent to generate the UI content. &lt;/p&gt; &lt;p&gt;I'm calling it Generative Computer, GitHub link: &lt;a href="https://github.com/joshbickett/generative-computer"&gt;https://github.com/joshbickett/generative-computer&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'd love any thoughts or contributions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Jebick"&gt; /u/Jebick &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/zdt9i5buxdsf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nusi98/demo_i_made_an_opensource_version_of_imagine_by/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nusi98/demo_i_made_an_opensource_version_of_imagine_by/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-30T23:21:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1nusali</id>
    <title>Open-source Video-to-Video Minecraft Mod</title>
    <updated>2025-09-30T23:12:08+00:00</updated>
    <author>
      <name>/u/decartai</name>
      <uri>https://old.reddit.com/user/decartai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nusali/opensource_videotovideo_minecraft_mod/"&gt; &lt;img alt="Open-source Video-to-Video Minecraft Mod" src="https://external-preview.redd.it/bzNhb3MzOWx3ZHNmMa3QwiO5wxj6sjoVqnijYMVhdAngVZj0rCfnhHGJ_xJq.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f94a405d23c2302bfc8894be32b043722df7c47e" title="Open-source Video-to-Video Minecraft Mod" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;we released a Minecraft Mod (link: &lt;a href="https://modrinth.com/mod/oasis2"&gt;https://modrinth.com/mod/oasis2&lt;/a&gt;) several weeks ago and today we are open-sourcing it!&lt;/p&gt; &lt;p&gt;It uses our WebRTC API, and we hope this can provide a blueprint for deploying vid2vid models inside Minecraft as well as a fun example of how to use our API.We'd love to see what you build with it! &lt;/p&gt; &lt;p&gt;Now that our platform is officially live (learn more in our announcement: &lt;a href="https://x.com/DecartAI/status/1973125817631908315"&gt;https://x.com/DecartAI/status/1973125817631908315&lt;/a&gt;), we will be releasing numerous open-source starting templates for both our hosted models and open-weights releases. &lt;/p&gt; &lt;p&gt;Leave a comment with what you’d like to see next!&lt;/p&gt; &lt;p&gt;Code: &lt;a href="https://github.com/DecartAI/mirage-minecraft-mod"&gt;https://github.com/DecartAI/mirage-minecraft-mod&lt;/a&gt;&lt;br /&gt; Article: &lt;a href="https://cookbook.decart.ai/mirage-minecraft-mod"&gt;https://cookbook.decart.ai/mirage-minecraft-mod&lt;/a&gt;&lt;br /&gt; Platform details: &lt;a href="https://x.com/DecartAI/status/1973125817631908315"&gt;https://x.com/DecartAI/status/1973125817631908315&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Decart Team&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/decartai"&gt; /u/decartai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/qcgqv39lwdsf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nusali/opensource_videotovideo_minecraft_mod/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nusali/opensource_videotovideo_minecraft_mod/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-30T23:12:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1nupsgp</id>
    <title>No GLM 4.6-Air</title>
    <updated>2025-09-30T21:27:00+00:00</updated>
    <author>
      <name>/u/festr2</name>
      <uri>https://old.reddit.com/user/festr2</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nupsgp/no_glm_46air/"&gt; &lt;img alt="No GLM 4.6-Air" src="https://b.thumbs.redditmedia.com/yn-g4_XuI0bLOv3QEgjeBe59MVVq9pvBYS_OKDOAN7w.jpg" title="No GLM 4.6-Air" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/lb3lyo9tddsf1.png?width=595&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=43c558f92d47949834e74e2f776f8a4a2b744811"&gt;https://preview.redd.it/lb3lyo9tddsf1.png?width=595&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=43c558f92d47949834e74e2f776f8a4a2b744811&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/Zai_org/status/1973134943158141421"&gt;https://x.com/Zai_org/status/1973134943158141421&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/festr2"&gt; /u/festr2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nupsgp/no_glm_46air/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nupsgp/no_glm_46air/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nupsgp/no_glm_46air/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-30T21:27:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1nv5el5</id>
    <title>InfiniteGPU - Open source Distributed AI Inference Platform</title>
    <updated>2025-10-01T11:17:14+00:00</updated>
    <author>
      <name>/u/franklbt</name>
      <uri>https://old.reddit.com/user/franklbt</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey! I've been working on a platform that addresses a problem many of us face: needing more compute power for AI inference without breaking the bank on cloud GPUs.&lt;/p&gt; &lt;p&gt;What is InfiniteGPU?&lt;/p&gt; &lt;p&gt;It's a distributed compute marketplace where people can:&lt;/p&gt; &lt;p&gt;As Requestors: Run ONNX models on a distributed network of providers' hardware at an interesting price&lt;/p&gt; &lt;p&gt;As Providers: Monetize idle GPU/CPU/NPU time by running inference tasks in the background&lt;/p&gt; &lt;p&gt;Think of it as &amp;quot;Uber for AI compute&amp;quot; - but actually working and with real money involved.&lt;/p&gt; &lt;p&gt;The platform is functional for ONNX model inference tasks. Perfect for:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Running inference when your local GPU is maxed out&lt;/li&gt; &lt;li&gt;Distributed batch processing of images/data&lt;/li&gt; &lt;li&gt;Earning passive income from idle hardware&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;How It Works&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Requestors upload ONNX models and input data&lt;/li&gt; &lt;li&gt;Platform splits work into subtasks and distributes to available providers&lt;/li&gt; &lt;li&gt;Providers (desktop clients) automatically claim and execute subtasks&lt;/li&gt; &lt;li&gt;Results stream back in real-time&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What Makes This Different?&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Real money: Not crypto tokens&lt;/li&gt; &lt;li&gt;Native performance optimized with access to neural processing unit or gpu when available&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Try It Out&lt;/p&gt; &lt;p&gt;GitHub repo: &lt;a href="https://github.com/Scalerize/Scalerize.InfiniteGpu"&gt;https://github.com/Scalerize/Scalerize.InfiniteGpu&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The entire codebase is available - backend API, React frontend, and Windows desktop client.&lt;/p&gt; &lt;p&gt;Happy to answer any technical questions about the project!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/franklbt"&gt; /u/franklbt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nv5el5/infinitegpu_open_source_distributed_ai_inference/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nv5el5/infinitegpu_open_source_distributed_ai_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nv5el5/infinitegpu_open_source_distributed_ai_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-01T11:17:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1nu6kjc</id>
    <title>Hot take: ALL Coding tools are bullsh*t</title>
    <updated>2025-09-30T07:14:23+00:00</updated>
    <author>
      <name>/u/Adventurous-Slide776</name>
      <uri>https://old.reddit.com/user/Adventurous-Slide776</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Let me tell you about the dumbest fucking trend in software development: taking the most powerful reasoning engines humanity has ever created and lobotomizing them with middleware.&lt;/p&gt; &lt;p&gt;We have these incredible language models—DeepSeek 3.2, GLM-4.5, Qwen 3 Coder—that can understand complex problems, reason through edge cases, and generate genuinely good code. And what did we do? We wrapped them in so many layers of bullshit that they can barely function.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Scam:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Every coding tool follows the same playbook:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Inject a 20,000 token system prompt explaining how to use tools&lt;/li&gt; &lt;li&gt;Add tool-calling ceremonies for every filesystem operation&lt;/li&gt; &lt;li&gt;Send timezone, task lists, environment info with EVERY request&lt;/li&gt; &lt;li&gt;Read the same files over and over and over&lt;/li&gt; &lt;li&gt;Make tiny edits one at a time&lt;/li&gt; &lt;li&gt;Re-read everything to &amp;quot;verify&amp;quot;&lt;/li&gt; &lt;li&gt;Repeat until you've burned 50,000 tokens&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;And then they market this as &amp;quot;agentic&amp;quot; and &amp;quot;autonomous&amp;quot; and charge you $20/month.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Reality:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The model spends 70% of its context window reading procedural garbage it's already seen five times. It's not thinking about your problem—it's playing filesystem navigator. It's not reasoning deeply—it's pattern matching through the noise because it's cognitively exhausted.&lt;/p&gt; &lt;p&gt;You ask it to fix a bug. It reads the file (3k tokens). Checks the timezone (why?). Reviews the task list (who asked?). Makes a one-line change. Reads the file AGAIN to verify. Runs a command. Reads the output. And somehow the bug still isn't fixed because the model never had enough clean context to actually understand the problem.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Insanity:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;What you can accomplish in 15,000 tokens with a direct conversation—problem explained, context provided, complete solution generated—these tools spread across 50,000 tokens of redundant slop.&lt;/p&gt; &lt;p&gt;The model generates the same code snippets again and again. It sees the same file contents five times in one conversation. It's drowning in its own output, suffocating under layers of middleware-generated vomit.&lt;/p&gt; &lt;p&gt;And the worst part? &lt;strong&gt;It gives worse results.&lt;/strong&gt; The solutions are half-assed because the model is working with a fraction of its actual reasoning capacity. Everything else is burned on ceremonial bullshit.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Market Dynamics:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;VCs threw millions at &amp;quot;AI coding agents.&amp;quot; Companies rushed to ship agentic frameworks. Everyone wanted to be the &amp;quot;autonomous&amp;quot; solution. So they added more tools, more features, more automation.&lt;/p&gt; &lt;p&gt;More context r*pe.&lt;/p&gt; &lt;p&gt;They optimized for demos, not for actual utility. Because in a demo, watching the tool &amp;quot;autonomously&amp;quot; read files and run commands looks impressive. In reality, you're paying 3x the API costs for 0.5x the quality.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Simple Truth:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Just upload your fucking files to a local chat interface like LobeHub (Open Source). Explain the problem. Let the model think. Get your code in one artifact. Copy it. Done.&lt;/p&gt; &lt;p&gt;No tool ceremonies. No context pollution. No reading the same file seven times. No timezone updates nobody asked for.&lt;/p&gt; &lt;p&gt;The model's full intelligence goes toward your problem, not toward navigating a filesystem through an API. You get better code, faster, for less money.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Irony:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;We spent decades making programming languages more expressive so humans could think at a higher level. Then we built AI that can understand natural language and reason about complex systems.&lt;/p&gt; &lt;p&gt;And then we forced it back down into the machine-level bullsh*t of &amp;quot;read file, edit line 47, write file, run command, read output.&amp;quot;&lt;/p&gt; &lt;p&gt;We took reasoning engines and turned them into glorified bash scripts.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Future:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I hope we look back at this era and laugh. The &amp;quot;agentic coding tool&amp;quot; phase where everyone was convinced that more automation meant better results. Where we drowned AI in context pollution and called it progress.&lt;/p&gt; &lt;p&gt;The tools that will win aren't the ones with the most features or the most autonomy. They're the ones that get out of the model's way and let it do what it's actually good at: thinking.&lt;/p&gt; &lt;p&gt;Until then, I'll be over here using the chat interface like a sane person, getting better results for less money, while the rest of you pay for the privilege of context r*pe.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Adventurous-Slide776"&gt; /u/Adventurous-Slide776 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nu6kjc/hot_take_all_coding_tools_are_bullsht/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nu6kjc/hot_take_all_coding_tools_are_bullsht/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nu6kjc/hot_take_all_coding_tools_are_bullsht/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-30T07:14:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1nv1spp</id>
    <title>Uncensored models providers</title>
    <updated>2025-10-01T07:27:58+00:00</updated>
    <author>
      <name>/u/SnooPaintings8639</name>
      <uri>https://old.reddit.com/user/SnooPaintings8639</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there any LLM API provider, like OpenRouter, but with uncensored/abliterated models? I use them locally, but for my project I need something more reliable, so I either have to rent GPUs and manage them myself, or preferably find an API with these models.&lt;/p&gt; &lt;p&gt;Any API you can suggest? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SnooPaintings8639"&gt; /u/SnooPaintings8639 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nv1spp/uncensored_models_providers/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nv1spp/uncensored_models_providers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nv1spp/uncensored_models_providers/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-01T07:27:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1nv44g9</id>
    <title>Step by Step installation vllm or llama.cpp under unbuntu / strix halo - AMD Ryzen AI Max</title>
    <updated>2025-10-01T10:02:43+00:00</updated>
    <author>
      <name>/u/Impossible_Art9151</name>
      <uri>https://old.reddit.com/user/Impossible_Art9151</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'd appreciate any help since I am hanging in the installation on my brand new strix halo 128GB RAM.&lt;/p&gt; &lt;p&gt;Two days ago I installed the actual ubuntu 24.04 in dual boot mode with windows.&lt;br /&gt; I configured the bios according to:&lt;br /&gt; &lt;a href="https://github.com/technigmaai/technigmaai-wiki/wiki/AMD-Ryzen-AI-Max--395:-GTT--Memory-Step%E2%80%90by%E2%80%90Step-Instructions-%28Ubuntu-24.04%29"&gt;https://github.com/technigmaai/technigmaai-wiki/wiki/AMD-Ryzen-AI-Max--395:-GTT--Memory-Step%E2%80%90by%E2%80%90Step-Instructions-%28Ubuntu-24.04%29&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Then I followed a step by step instruction to install vllm, installing the actual rocm verson 7 (do not find the link right now) - but faild at one point and decided to try llama.cpp instead,&lt;br /&gt; following this instruction:&lt;br /&gt; &lt;a href="https://github.com/kyuz0/amd-strix-halo-toolboxes?tab=readme-ov-file"&gt;https://github.com/kyuz0/amd-strix-halo-toolboxes?tab=readme-ov-file&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I am hanging at this step:&lt;br /&gt; ----------------------------------------------&lt;/p&gt; &lt;p&gt;&lt;em&gt;toolbox create llama-rocm-6.4.4-rocwmma \&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;--image&lt;/em&gt; &lt;a href="http://docker.io/kyuz0/amd-strix-halo-toolboxes:rocm-6.4.4-rocwmma"&gt;&lt;em&gt;docker.io/kyuz0/amd-strix-halo-toolboxes:rocm-6.4.4-rocwmma&lt;/em&gt;&lt;/a&gt; &lt;em&gt;\&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;-- --device /dev/dri --device /dev/kfd \&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;--group-add video --group-add render --group-add sudo --security-opt seccomp=unconfined&lt;/em&gt; &lt;/p&gt; &lt;p&gt;----------------------------------------------&lt;/p&gt; &lt;p&gt;What does it mean? There is no toolbox command. What am I missing?&lt;/p&gt; &lt;p&gt;Otherwise - maybe s.o. can help me with a more detailed instruction?&lt;/p&gt; &lt;p&gt;background: I just worked with ollama/linux up to know and would like to get 1st experience with vllm or llama.cpp&lt;br /&gt; We are a small company, a handful of users started working with coder models.&lt;br /&gt; With llama.cpp or vllm on strix halo I'd like to provide more local AI-ressources for qwen3-coder in 8-quant or higher. hopefully I can free ressources from my main AI-server.&lt;/p&gt; &lt;p&gt;thx in advance&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impossible_Art9151"&gt; /u/Impossible_Art9151 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nv44g9/step_by_step_installation_vllm_or_llamacpp_under/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nv44g9/step_by_step_installation_vllm_or_llamacpp_under/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nv44g9/step_by_step_installation_vllm_or_llamacpp_under/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-01T10:02:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1nuebie</id>
    <title>Some mad lads at Aperture Science got a quantized AGI running on a potato BTW.</title>
    <updated>2025-09-30T14:13:29+00:00</updated>
    <author>
      <name>/u/Technical-Drag-255</name>
      <uri>https://old.reddit.com/user/Technical-Drag-255</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nuebie/some_mad_lads_at_aperture_science_got_a_quantized/"&gt; &lt;img alt="Some mad lads at Aperture Science got a quantized AGI running on a potato BTW." src="https://preview.redd.it/g2g5h9qi8bsf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9966f2251d399b479c9d0ad3e45206689b4b5980" title="Some mad lads at Aperture Science got a quantized AGI running on a potato BTW." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Technical-Drag-255"&gt; /u/Technical-Drag-255 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/g2g5h9qi8bsf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nuebie/some_mad_lads_at_aperture_science_got_a_quantized/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nuebie/some_mad_lads_at_aperture_science_got_a_quantized/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-30T14:13:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1numsuq</id>
    <title>DeepSeek-R1 performance with 15B parameters</title>
    <updated>2025-09-30T19:33:12+00:00</updated>
    <author>
      <name>/u/lewtun</name>
      <uri>https://old.reddit.com/user/lewtun</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ServiceNow just released a new 15B reasoning model on the Hub which is pretty interesting for a few reasons:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Similar perf as DeepSeek-R1 and Gemini Flash, but fits on a single GPU&lt;/li&gt; &lt;li&gt;No RL was used to train the model, just high-quality mid-training&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;They also made a demo so you can vibe check it: &lt;a href="https://huggingface.co/spaces/ServiceNow-AI/Apriel-Chat"&gt;https://huggingface.co/spaces/ServiceNow-AI/Apriel-Chat&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm pretty curious to see what the community thinks about it!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lewtun"&gt; /u/lewtun &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1numsuq/deepseekr1_performance_with_15b_parameters/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1numsuq/deepseekr1_performance_with_15b_parameters/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1numsuq/deepseekr1_performance_with_15b_parameters/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-30T19:33:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1nv17bt</id>
    <title>Interesting article, looks promising</title>
    <updated>2025-10-01T06:49:32+00:00</updated>
    <author>
      <name>/u/Wooden_Yam1924</name>
      <uri>https://old.reddit.com/user/Wooden_Yam1924</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is this our way to AGI?&lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/abs/2509.26507v1"&gt;https://arxiv.org/abs/2509.26507v1&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wooden_Yam1924"&gt; /u/Wooden_Yam1924 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nv17bt/interesting_article_looks_promising/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nv17bt/interesting_article_looks_promising/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nv17bt/interesting_article_looks_promising/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-01T06:49:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1nv5uw8</id>
    <title>don't sleep on Apriel-1.5-15b-Thinker and Snowpiercer</title>
    <updated>2025-10-01T11:41:17+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Apriel-1.5-15b-Thinker&lt;/strong&gt; is a multimodal reasoning model in ServiceNow’s Apriel SLM series which achieves competitive performance against models 10 times it's size. Apriel-1.5 is the second model in the reasoning series. It introduces enhanced textual reasoning capabilities and adds image reasoning support to the previous text model. It has undergone extensive continual pretraining across both text and image domains. In terms of post-training this model has &lt;strong&gt;undergone text-SFT only&lt;/strong&gt;. Our research demonstrates that with a strong mid-training regimen, we are able to achive SOTA performance on text and image reasoning tasks without having any image SFT training or RL.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Highlights&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Achieves a score of &lt;strong&gt;52&lt;/strong&gt; on the Artificial Analysis index and is competitive with Deepseek R1 0528, Gemini-Flash etc.&lt;/li&gt; &lt;li&gt;It is &lt;strong&gt;AT LEAST 1 / 10&lt;/strong&gt; the size of any other model that scores &amp;gt; 50 on the Artificial Analysis index.&lt;/li&gt; &lt;li&gt;Scores &lt;strong&gt;68&lt;/strong&gt; on Tau2 Bench Telecom and &lt;strong&gt;62&lt;/strong&gt; on IFBench, which are key benchmarks for the enterprise domain.&lt;/li&gt; &lt;li&gt;At 15B parameters, the model fits on a single GPU, making it highly memory-efficient.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;it was published yesterday&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/ServiceNow-AI/Apriel-1.5-15b-Thinker"&gt;https://huggingface.co/ServiceNow-AI/Apriel-1.5-15b-Thinker&lt;/a&gt;&lt;/p&gt; &lt;p&gt;their previous model was&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/ServiceNow-AI/Apriel-Nemotron-15b-Thinker"&gt;https://huggingface.co/ServiceNow-AI/Apriel-Nemotron-15b-Thinker&lt;/a&gt;&lt;/p&gt; &lt;p&gt;which is a base model for&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/TheDrummer/Snowpiercer-15B-v3"&gt;https://huggingface.co/TheDrummer/Snowpiercer-15B-v3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;which was published earlier this week :)&lt;/p&gt; &lt;p&gt;let's hope mr &lt;a href="/u/TheLocalDrummer"&gt;u/TheLocalDrummer&lt;/a&gt; will continue Snowpiercing &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nv5uw8/dont_sleep_on_apriel1515bthinker_and_snowpiercer/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nv5uw8/dont_sleep_on_apriel1515bthinker_and_snowpiercer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nv5uw8/dont_sleep_on_apriel1515bthinker_and_snowpiercer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-01T11:41:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1nuhe8m</id>
    <title>GLM 4.6 one-shot aquarium simulator with the best looking fishes I've ever seen created by open weight models.</title>
    <updated>2025-09-30T16:10:39+00:00</updated>
    <author>
      <name>/u/Cool-Chemical-5629</name>
      <uri>https://old.reddit.com/user/Cool-Chemical-5629</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nuhe8m/glm_46_oneshot_aquarium_simulator_with_the_best/"&gt; &lt;img alt="GLM 4.6 one-shot aquarium simulator with the best looking fishes I've ever seen created by open weight models." src="https://preview.redd.it/i68ebwe6rbsf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=59d3e23dec72571f34d1e0bef84e936438849af3" title="GLM 4.6 one-shot aquarium simulator with the best looking fishes I've ever seen created by open weight models." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Fish tails actually wave around while they swim. I admit the rest of the scene is not extremely detailed, but overall this is better that what you get from for example DeepSeek models which are nearly twice as big. Qwen models are usually fairly good at this too, except the buttons all work here which is kinda something note worthy given my previous experience with other models which generate beautiful (and very often ridiculously useless) buttons which don't even work. Here everything works out of the box. No bugs or errors. I said it with GLM 4.5 and I can only say it again with GLM 4.6. GLM is the real deal alternative to closed source proprietary models, guys.&lt;/p&gt; &lt;p&gt;Demo: &lt;a href="https://jsfiddle.net/n52smvkr/"&gt;Jsfiddle&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cool-Chemical-5629"&gt; /u/Cool-Chemical-5629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/i68ebwe6rbsf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nuhe8m/glm_46_oneshot_aquarium_simulator_with_the_best/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nuhe8m/glm_46_oneshot_aquarium_simulator_with_the_best/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-30T16:10:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1nujx4x</id>
    <title>GLM 4.6 already runs on MLX</title>
    <updated>2025-09-30T17:45:32+00:00</updated>
    <author>
      <name>/u/No_Conversation9561</name>
      <uri>https://old.reddit.com/user/No_Conversation9561</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nujx4x/glm_46_already_runs_on_mlx/"&gt; &lt;img alt="GLM 4.6 already runs on MLX" src="https://preview.redd.it/jcb16mqcacsf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0df9237936852365f57d5b9dcba46dd846a877fe" title="GLM 4.6 already runs on MLX" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Conversation9561"&gt; /u/No_Conversation9561 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jcb16mqcacsf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nujx4x/glm_46_already_runs_on_mlx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nujx4x/glm_46_already_runs_on_mlx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-30T17:45:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1nutjlm</id>
    <title>Qwen3-Next-80B-GGUF, Any Update?</title>
    <updated>2025-10-01T00:09:23+00:00</updated>
    <author>
      <name>/u/Iory1998</name>
      <uri>https://old.reddit.com/user/Iory1998</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;I am wondering what's the update on this model's support in llama.cpp?&lt;/p&gt; &lt;p&gt;Does anyone of you have any idea?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Iory1998"&gt; /u/Iory1998 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nutjlm/qwen3next80bgguf_any_update/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nutjlm/qwen3next80bgguf_any_update/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nutjlm/qwen3next80bgguf_any_update/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-01T00:09:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1nuerql</id>
    <title>zai-org/GLM-4.6 · Hugging Face</title>
    <updated>2025-09-30T14:31:09+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nuerql/zaiorgglm46_hugging_face/"&gt; &lt;img alt="zai-org/GLM-4.6 · Hugging Face" src="https://external-preview.redd.it/PGKpaG-61JC7z-y_F2XkhwKzdcpyb99tvV79_JhB320.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3b34f4363d1d490762c5a458490a60b87ed1e125" title="zai-org/GLM-4.6 · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Model Introduction&lt;/h1&gt; &lt;p&gt;Compared with GLM-4.5, &lt;strong&gt;GLM-4.6&lt;/strong&gt; brings several key improvements:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Longer context window:&lt;/strong&gt; The context window has been expanded from 128K to 200K tokens, enabling the model to handle more complex agentic tasks.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Superior coding performance:&lt;/strong&gt; The model achieves higher scores on code benchmarks and demonstrates better real-world performance in applications such as Claude Code、Cline、Roo Code and Kilo Code, including improvements in generating visually polished front-end pages.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Advanced reasoning:&lt;/strong&gt; GLM-4.6 shows a clear improvement in reasoning performance and supports tool use during inference, leading to stronger overall capability.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;More capable agents:&lt;/strong&gt; GLM-4.6 exhibits stronger performance in tool using and search-based agents, and integrates more effectively within agent frameworks.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Refined writing:&lt;/strong&gt; Better aligns with human preferences in style and readability, and performs more naturally in role-playing scenarios.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We evaluated GLM-4.6 across eight public benchmarks covering agents, reasoning, and coding. Results show clear gains over GLM-4.5, with GLM-4.6 also holding competitive advantages over leading domestic and international models such as &lt;strong&gt;DeepSeek-V3.1-Terminus&lt;/strong&gt; and &lt;strong&gt;Claude Sonnet 4&lt;/strong&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/zai-org/GLM-4.6"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nuerql/zaiorgglm46_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nuerql/zaiorgglm46_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-30T14:31:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1nufu17</id>
    <title>AMD tested 20+ local models for coding &amp; only 2 actually work (testing linked)</title>
    <updated>2025-09-30T15:11:54+00:00</updated>
    <author>
      <name>/u/nick-baumann</name>
      <uri>https://old.reddit.com/user/nick-baumann</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nufu17/amd_tested_20_local_models_for_coding_only_2/"&gt; &lt;img alt="AMD tested 20+ local models for coding &amp;amp; only 2 actually work (testing linked)" src="https://external-preview.redd.it/eXRyOW5lM2JpYnNmMRoy4uRFePoUQZKQzw3MqAlRHs-miZIp3JL6ldgQ6nGR.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=121a931aab3efc98ee57ab2e3519bf172950e05f" title="AMD tested 20+ local models for coding &amp;amp; only 2 actually work (testing linked)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;tldr&lt;/strong&gt;; qwen3-coder (4-bit, 8-bit) is really the only viable local model for coding, if you have 128gb+ of RAM, check out GLM-4.5-air (8-bit)&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;hello hello!&lt;/p&gt; &lt;p&gt;So AMD just dropped their comprehensive testing of local models for AI coding and it pretty much validates what I've been preaching about local models&lt;/p&gt; &lt;p&gt;They tested 20+ models and found exactly what many of us suspected: most of them completely fail at actual coding tasks. Out of everything they tested, only three models consistently worked: Qwen3-Coder 30B, GLM-4.5-Air for those with beefy rigs. Magistral Small is worth an honorable mention in my books.&lt;/p&gt; &lt;p&gt;deepseek/deepseek-r1-0528-qwen3-8b, smaller Llama models, GPT-OSS-20B, Seed-OSS-36B (bytedance) all produce broken outputs or can't handle tool use properly. This isn't a knock on the models themselves, they're just not built for the complex tool-calling that coding agents need.&lt;/p&gt; &lt;p&gt;What's interesting is their RAM findings match exactly what I've been seeing. For 32gb machines, Qwen3-Coder 30B at 4-bit is basically your only option, but an extremely viable one at that.&lt;/p&gt; &lt;p&gt;For those with 64gb RAM, you can run the same model at 8-bit quantization. And if you've got 128gb+, GLM-4.5-Air is apparently incredible (this is AMD's #1)&lt;/p&gt; &lt;p&gt;AMD used Cline &amp;amp; LM Studio for all their testing, which is how they validated these specific configurations. Cline is pretty demanding in terms of tool-calling and context management, so if a model works with Cline, it'll work with pretty much anything.&lt;/p&gt; &lt;p&gt;AMD's blog: &lt;a href="https://www.amd.com/en/blogs/2025/how-to-vibe-coding-locally-with-amd-ryzen-ai-and-radeon.html"&gt;https://www.amd.com/en/blogs/2025/how-to-vibe-coding-locally-with-amd-ryzen-ai-and-radeon.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;setup instructions for coding w/ local models: &lt;a href="https://cline.bot/blog/local-models-amd"&gt;https://cline.bot/blog/local-models-amd&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nick-baumann"&gt; /u/nick-baumann &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/fmg3qe3bibsf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nufu17/amd_tested_20_local_models_for_coding_only_2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nufu17/amd_tested_20_local_models_for_coding_only_2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-30T15:11:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1nuq4tr</id>
    <title>❌Spent ~$3K building the open source models you asked for. Need to abort Art-1-20B and shut down AGI-0. Ideas?❌</title>
    <updated>2025-09-30T21:40:33+00:00</updated>
    <author>
      <name>/u/GuiltyBookkeeper4849</name>
      <uri>https://old.reddit.com/user/GuiltyBookkeeper4849</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Quick update on AGI-0 Labs. Not great news.&lt;/p&gt; &lt;p&gt;A while back I &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nbqa34/what_model_should_we_build_next_you_decide/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button"&gt;posted asking what model you wanted next&lt;/a&gt;. The response was awesome - you voted, gave ideas, and I started building. Art-1-8B is nearly done, and I was working on Art-1-20B plus the community-voted model .&lt;/p&gt; &lt;p&gt;Problem: I've burned through almost $3K of my own money on compute. I'm basically tapped out.&lt;/p&gt; &lt;p&gt;Art-1-8B I can probably finish. Art-1-20B and the community model? Can't afford to complete them. And I definitely can't keep doing this.&lt;/p&gt; &lt;p&gt;So I'm at a decision point: either figure out how to make this financially viable, or just shut it down and move on. I'm not interested in half-doing this as a occasional hobby project.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;I've thought about a few options:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Paid community - early access, vote on models, co-author credits, shared compute pool&lt;/li&gt; &lt;li&gt;Finding sponsors for model releases - logo and website link on the model card, still fully open source&lt;/li&gt; &lt;li&gt;Custom model training / consulting - offering services for a fee&lt;/li&gt; &lt;li&gt;Just donations (Already possible at &lt;a href="https://agi-0.com/donate"&gt;https://agi-0.com/donate&lt;/a&gt; )&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;But honestly? I don't know what makes sense or what anyone would actually pay for.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;So I'm asking: if you want AGI-0 to keep releasing open source models, what's the path here?&lt;/strong&gt; What would you actually support? Is there an obvious funding model I'm missing?&lt;/p&gt; &lt;p&gt;Or should I just accept this isn't sustainable and shut it down?&lt;/p&gt; &lt;p&gt;Not trying to guilt anyone - genuinely asking for ideas. If there's a clear answer in the comments I'll pursue it. If not, I'll wrap up Art-1-8B and call it.&lt;/p&gt; &lt;p&gt;Let me know what you think.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GuiltyBookkeeper4849"&gt; /u/GuiltyBookkeeper4849 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nuq4tr/spent_3k_building_the_open_source_models_you/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nuq4tr/spent_3k_building_the_open_source_models_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nuq4tr/spent_3k_building_the_open_source_models_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-30T21:40:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1nuw0du</id>
    <title>Sonnet 4.5 tops EQ-Bench writing evals. GLM-4.6 sees incremental improvement.</title>
    <updated>2025-10-01T02:05:23+00:00</updated>
    <author>
      <name>/u/_sqrkl</name>
      <uri>https://old.reddit.com/user/_sqrkl</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nuw0du/sonnet_45_tops_eqbench_writing_evals_glm46_sees/"&gt; &lt;img alt="Sonnet 4.5 tops EQ-Bench writing evals. GLM-4.6 sees incremental improvement." src="https://a.thumbs.redditmedia.com/b5CVus4Rz2oOfFenXT9Ujk9S6gtDV3VmYHL6R6JgQ_0.jpg" title="Sonnet 4.5 tops EQ-Bench writing evals. GLM-4.6 sees incremental improvement." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sonnet 4.5 tops both EQ-Bench writing evals!&lt;/p&gt; &lt;p&gt;Anthropic have evidently worked on safety for this release, with much stronger pushback &amp;amp; de-escalation on spiral-bench vs sonnet-4.&lt;/p&gt; &lt;p&gt;GLM-4.6's score is incremental over GLM-4.5 - but personally I like the newer version's writing much better.&lt;/p&gt; &lt;p&gt;&lt;a href="https://eqbench.com/"&gt;https://eqbench.com/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Sonnet-4.5 creative writing samples:&lt;/p&gt; &lt;p&gt;&lt;a href="https://eqbench.com/results/creative-writing-v3/claude-sonnet-4.5.html"&gt;https://eqbench.com/results/creative-writing-v3/claude-sonnet-4.5.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;x-ai/glm-4.6 creative writing samples:&lt;/p&gt; &lt;p&gt;&lt;a href="https://eqbench.com/results/creative-writing-v3/zai-org__GLM-4.6.html"&gt;https://eqbench.com/results/creative-writing-v3/zai-org__GLM-4.6.html&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_sqrkl"&gt; /u/_sqrkl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nuw0du"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nuw0du/sonnet_45_tops_eqbench_writing_evals_glm46_sees/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nuw0du/sonnet_45_tops_eqbench_writing_evals_glm46_sees/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-01T02:05:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1nuyjp9</id>
    <title>LiquidAI bet on small but mighty model LFM2-1.2B-Tool/RAG/Extract</title>
    <updated>2025-10-01T04:13:12+00:00</updated>
    <author>
      <name>/u/dheetoo</name>
      <uri>https://old.reddit.com/user/dheetoo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So LiquidAI just announced their fine-tuned LFM models with different variants - Tool, RAG, and Extract. Each one's built for specific tasks instead of trying to do everything.&lt;/p&gt; &lt;p&gt;This lines up perfectly with that Nvidia whitepaper about how small specialized models are the future of agentic AI. Looks like it's actually happening now.&lt;/p&gt; &lt;p&gt;I'm planning to swap out parts of my current agentic workflow to test these out. Right now I'm running Qwen3-4B for background tasks and Qwen3-235B for answer generation. Gonna try replacing the background task layer with these LFM models since my main use cases are extraction and RAG.&lt;/p&gt; &lt;p&gt;Will report back with results once I've tested them out.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dheetoo"&gt; /u/dheetoo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nuyjp9/liquidai_bet_on_small_but_mighty_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nuyjp9/liquidai_bet_on_small_but_mighty_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nuyjp9/liquidai_bet_on_small_but_mighty_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-01T04:13:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1nulea4</id>
    <title>How can I use this beast to benefit the community? Quantize larger models? It’s a 9985wx, 768 ddr5, 384 gb vram.</title>
    <updated>2025-09-30T18:39:46+00:00</updated>
    <author>
      <name>/u/joninco</name>
      <uri>https://old.reddit.com/user/joninco</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nulea4/how_can_i_use_this_beast_to_benefit_the_community/"&gt; &lt;img alt="How can I use this beast to benefit the community? Quantize larger models? It’s a 9985wx, 768 ddr5, 384 gb vram." src="https://preview.redd.it/78yadl81kcsf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=977db53773f1bf118531422a3b8b0a76e5905352" title="How can I use this beast to benefit the community? Quantize larger models? It’s a 9985wx, 768 ddr5, 384 gb vram." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Any ideas are greatly appreciated to use this beast for good!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/joninco"&gt; /u/joninco &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/78yadl81kcsf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nulea4/how_can_i_use_this_beast_to_benefit_the_community/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nulea4/how_can_i_use_this_beast_to_benefit_the_community/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-30T18:39:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1nuq54g</id>
    <title>No GLM-4.6 Air version is coming out</title>
    <updated>2025-09-30T21:40:52+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nuq54g/no_glm46_air_version_is_coming_out/"&gt; &lt;img alt="No GLM-4.6 Air version is coming out" src="https://preview.redd.it/mfj4sracgdsf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=99a9ab99d0f5ab635fe346eff30880f517f02f02" title="No GLM-4.6 Air version is coming out" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Zhipu-AI just shared on X that there are currently no plans to release an Air version of their newly announced GLM-4.6.&lt;/p&gt; &lt;p&gt;That said, I’m still incredibly excited about what this lab is doing. In my opinion, Zhipu-AI is one of the most promising open-weight AI labs out there right now. I’ve run my own private benchmarks across all major open-weight model releases, and GLM-4.5 stood out significantly, especially for coding and agentic workloads. It’s the closest I’ve seen an open-weight model come to the performance of the closed-weight frontier models.&lt;/p&gt; &lt;p&gt;I’ve also been keeping up with their technical reports, and they’ve been impressively transparent about their training methods. Notably, they even open-sourced their RL post-training framework, Slime, which is a huge win for the community.&lt;/p&gt; &lt;p&gt;I don’t have any insider knowledge, but based on what I’ve seen so far, I’m hopeful they’ll continue approaching/pushing the open-weight frontier and supporting the local LLM ecosystem.&lt;/p&gt; &lt;p&gt;This is an appreciation post.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/mfj4sracgdsf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nuq54g/no_glm46_air_version_is_coming_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nuq54g/no_glm46_air_version_is_coming_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-30T21:40:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1nv4oy9</id>
    <title>Codex is amazing, it can fix code issues without the need of constant approver. my setup: gpt-oss-20b on lm_studio.</title>
    <updated>2025-10-01T10:37:00+00:00</updated>
    <author>
      <name>/u/kyeoh1</name>
      <uri>https://old.reddit.com/user/kyeoh1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nv4oy9/codex_is_amazing_it_can_fix_code_issues_without/"&gt; &lt;img alt="Codex is amazing, it can fix code issues without the need of constant approver. my setup: gpt-oss-20b on lm_studio." src="https://external-preview.redd.it/ODFtbnEzNm45aHNmMTG3bHLe9xXVwwNl3KvP1Qzcgr5dnq8C6Rg-wDqEIF5Y.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=31079d1483d03020c93a99d188076eb10a02002c" title="Codex is amazing, it can fix code issues without the need of constant approver. my setup: gpt-oss-20b on lm_studio." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kyeoh1"&gt; /u/kyeoh1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/1lusu36n9hsf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nv4oy9/codex_is_amazing_it_can_fix_code_issues_without/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nv4oy9/codex_is_amazing_it_can_fix_code_issues_without/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-01T10:37:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1nuxdd4</id>
    <title>[Release] Finally a working 8-bit quantized VibeVoice model (Release 1.8.0)</title>
    <updated>2025-10-01T03:12:08+00:00</updated>
    <author>
      <name>/u/Fabix84</name>
      <uri>https://old.reddit.com/user/Fabix84</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nuxdd4/release_finally_a_working_8bit_quantized/"&gt; &lt;img alt="[Release] Finally a working 8-bit quantized VibeVoice model (Release 1.8.0)" src="https://preview.redd.it/yevipl7e3fsf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4c842c6d1d94b17de6583302b9661250570aab2a" title="[Release] Finally a working 8-bit quantized VibeVoice model (Release 1.8.0)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;br /&gt; first of all, thank you once again for the incredible support... the project just reached &lt;strong&gt;944 stars&lt;/strong&gt; on GitHub. 🙏&lt;/p&gt; &lt;p&gt;In the past few days, several 8-bit quantized models were shared to me, but unfortunately all of them produced only static noise. Since there was clear community interest, I decided to take the challenge and work on it myself. The result is the &lt;strong&gt;first fully working 8-bit quantized model&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;🔗 &lt;a href="https://huggingface.co/FabioSarracino/VibeVoice-Large-Q8"&gt;FabioSarracino/VibeVoice-Large-Q8 on HuggingFace&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Alongside this, the latest VibeVoice-ComfyUI releases bring some major updates:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Dynamic on-the-fly quantization&lt;/strong&gt;: you can now quantize the base model to 4-bit or 8-bit at runtime.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;New manual model management system&lt;/strong&gt;: replaced the old automatic HF downloads (which many found inconvenient). Details here → &lt;a href="https://github.com/Enemyx-net/VibeVoice-ComfyUI/releases/tag/v1.6.0"&gt;Release 1.6.0&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Latest release (1.8.0)&lt;/strong&gt;: &lt;a href="https://github.com/Enemyx-net/VibeVoice-ComfyUI/releases/tag/v1.8.0"&gt;Changelog&lt;/a&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;GitHub repo (custom ComfyUI node):&lt;br /&gt; 👉 &lt;a href="https://github.com/Enemyx-net/VibeVoice-ComfyUI"&gt;Enemyx-net/VibeVoice-ComfyUI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks again to everyone who contributed feedback, testing, and support! This project wouldn’t be here without the community.&lt;/p&gt; &lt;p&gt;&lt;em&gt;(Of course, I’d love if you try it with my node, but it should also work fine with other VibeVoice nodes 😉)&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fabix84"&gt; /u/Fabix84 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/yevipl7e3fsf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nuxdd4/release_finally_a_working_8bit_quantized/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nuxdd4/release_finally_a_working_8bit_quantized/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-01T03:12:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1nv53rb</id>
    <title>GLM-4.6-GGUF is out!</title>
    <updated>2025-10-01T11:00:52+00:00</updated>
    <author>
      <name>/u/TheAndyGeorge</name>
      <uri>https://old.reddit.com/user/TheAndyGeorge</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nv53rb/glm46gguf_is_out/"&gt; &lt;img alt="GLM-4.6-GGUF is out!" src="https://preview.redd.it/kptmc2f0fhsf1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9344c531abf7cb2d05a64a1d2ee461b6106008bb" title="GLM-4.6-GGUF is out!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheAndyGeorge"&gt; /u/TheAndyGeorge &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kptmc2f0fhsf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nv53rb/glm46gguf_is_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nv53rb/glm46gguf_is_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-01T11:00:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
