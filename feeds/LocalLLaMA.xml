<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-24T12:12:28+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ptw5ol</id>
    <title>Could it be GLM 4.7 Air?</title>
    <updated>2025-12-23T15:05:26+00:00</updated>
    <author>
      <name>/u/noiserr</name>
      <uri>https://old.reddit.com/user/noiserr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;Head of Global Brand &amp;amp; Partnerships @Zai_org&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;says:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;We have a new model coming soon. Stay tuned! üòù&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;a href="https://x.com/louszbd/status/2003153617013137677"&gt;https://x.com/louszbd/status/2003153617013137677&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Maybe the Air version is next?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/noiserr"&gt; /u/noiserr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptw5ol/could_it_be_glm_47_air/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptw5ol/could_it_be_glm_47_air/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptw5ol/could_it_be_glm_47_air/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T15:05:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1pul85k</id>
    <title>AI agents keep failing to parse Ansible/Terraform output. Built a CLI that returns JSON instead.</title>
    <updated>2025-12-24T11:24:55+00:00</updated>
    <author>
      <name>/u/smille69</name>
      <uri>https://old.reddit.com/user/smille69</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been running local LLMs as infrastructure agents and kept hitting the same wall: they can't reliably parse traditional DevOps tool outputs.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Problem:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;When you ask an AI agent to check if nginx is running:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# Agent runs this: result = subprocess.run(['systemctl', 'status', 'nginx'], capture_output=True) # Gets back: ‚óè nginx.service - A high performance web server Loaded: loaded (/lib/systemd/system/nginx.service; enabled) Active: active (running) since Mon 2024-12-23 14:23:11 UTC; 2h 15min ago Docs: man:nginx(8) Main PID: 1234 (nginx) Tasks: 2 (limit: 4915) Memory: 2.1M # Agent tries to parse with regex... fails 20-30% of the time &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Same issue with Ansible playbooks (YAML hell), Terraform plans (text formatting), and basically every traditional CLI tool.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What I Built:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;A Rust-based CLI called &amp;quot;resh&amp;quot; (Resource Shell) that returns structured JSON for every operation:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Real Comparison:$ resh svc://nginx.status { &amp;quot;active&amp;quot;: true, &amp;quot;pid&amp;quot;: 1234, &amp;quot;memory_kb&amp;quot;: 2048, &amp;quot;uptime_seconds&amp;quot;: 8115, &amp;quot;enabled&amp;quot;: true } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I tested the same tasks with GPT-4 (via API) and Claude (via API):&lt;/p&gt; &lt;p&gt;&lt;em&gt;Task: &amp;quot;Check if nginx is running and restart if not&amp;quot;&lt;/em&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;With systemctl:&lt;/strong&gt; 68% success rate (parsing failures)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;With resh:&lt;/strong&gt; 97% success rate (JSON parsing)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The difference is dramatic when chaining multiple operations.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Design:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;URI-based addressing: &lt;code&gt;file://path.txt.read&lt;/code&gt;, &lt;code&gt;system://.memory&lt;/code&gt;, &lt;code&gt;ssh://server/cmd.exec&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Every operation returns JSON (no text parsing)&lt;/li&gt; &lt;li&gt;Type-safe operations (Rust backend)&lt;/li&gt; &lt;li&gt;28 resource handlers so far (file, process, service, system, network, etc.)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Current Status:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;v0.9.0 alpha&lt;/li&gt; &lt;li&gt;Open source (Apache 2.0)&lt;/li&gt; &lt;li&gt;Works with local LLMs via function calling&lt;/li&gt; &lt;li&gt;Tested with llama.cpp, Ollama, and cloud APIs&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Example with Local LLM:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# Using llama.cpp with function calling tools = [ { &amp;quot;name&amp;quot;: &amp;quot;resh&amp;quot;, &amp;quot;description&amp;quot;: &amp;quot;Execute infrastructure operations&amp;quot;, &amp;quot;parameters&amp;quot;: { &amp;quot;uri&amp;quot;: &amp;quot;resource://target.operation&amp;quot; } } ] # Agent can now reliably manage infrastructure response = llm.chat(&amp;quot;Check system health&amp;quot;, tools=tools) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Not trying to replace Ansible/Terraform&lt;/strong&gt; - they're great for human-written automation. This is specifically for AI agent consumption where structured outputs are critical.&lt;/p&gt; &lt;p&gt;Curious if others have hit this same wall with local LLMs + infrastructure automation, and whether this approach makes sense.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/millertechnologygroup/resh"&gt;https://github.com/millertechnologygroup/resh&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Website: &lt;a href="https://reshshell.dev"&gt;https://reshshell.dev&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer questions about the design, Rust implementation, or integration with different LLM backends.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/smille69"&gt; /u/smille69 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pul85k/ai_agents_keep_failing_to_parse_ansibleterraform/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pul85k/ai_agents_keep_failing_to_parse_ansibleterraform/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pul85k/ai_agents_keep_failing_to_parse_ansibleterraform/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-24T11:24:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1pulqzt</id>
    <title>Unsloth GLM 4.7 UD-Q2_K_XL or gpt-oss 120b?</title>
    <updated>2025-12-24T11:57:54+00:00</updated>
    <author>
      <name>/u/EnthusiasmPurple85</name>
      <uri>https://old.reddit.com/user/EnthusiasmPurple85</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm sure that gpt-oss will be much faster but, would the extreme GLM quant be better for general programming and chat? Anyone tried? Downloading them as of now. RTX3090 + 128GB of DDR4 3600&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EnthusiasmPurple85"&gt; /u/EnthusiasmPurple85 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pulqzt/unsloth_glm_47_udq2_k_xl_or_gptoss_120b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pulqzt/unsloth_glm_47_udq2_k_xl_or_gptoss_120b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pulqzt/unsloth_glm_47_udq2_k_xl_or_gptoss_120b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-24T11:57:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1puejhe</id>
    <title>MiraTTS Docker FastAPI server</title>
    <updated>2025-12-24T04:34:30+00:00</updated>
    <author>
      <name>/u/EmotionalWillow70</name>
      <uri>https://old.reddit.com/user/EmotionalWillow70</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wrote a dockerized FastAPI wrapper for MiraTTS. It exposes OpenAI-compatible endpoints so you can use it into existing LLM frontends.&lt;/p&gt; &lt;p&gt;Since MiraTTS doesn't support native streaming yet, I implemented a custom text chunker. It splits long inputs into safe segments, batches them for the GPU, and stitches the output together. This allows you to generate audio for long texts without hitting the model's character limits.&lt;/p&gt; &lt;p&gt;Repo here: &lt;a href="https://github.com/Si-ris-B/MiraTTS-FastAPI-Docker"&gt;https://github.com/Si-ris-B/MiraTTS-FastAPI-Docker&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EmotionalWillow70"&gt; /u/EmotionalWillow70 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1puejhe/miratts_docker_fastapi_server/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1puejhe/miratts_docker_fastapi_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1puejhe/miratts_docker_fastapi_server/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-24T04:34:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1puhc65</id>
    <title>Ryzen 395 128GB Bosgame</title>
    <updated>2025-12-24T07:15:25+00:00</updated>
    <author>
      <name>/u/Septa105</name>
      <uri>https://old.reddit.com/user/Septa105</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1puhc65/ryzen_395_128gb_bosgame/"&gt; &lt;img alt="Ryzen 395 128GB Bosgame" src="https://external-preview.redd.it/Qj7qNurNBqQMl9Mg1y12AddG6q2ncv3A06z3HsTAnH8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f9663aeb554c844ff92329e1d0287e1a4209fa14" title="Ryzen 395 128GB Bosgame" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi can somebody tell me exactly what steps in short for I need to do to get for eg running on Ubuntu 24.04&lt;/p&gt; &lt;p&gt;Eg 1) Bios set to 512mB? 2) set environment variable to ‚Ä¶ 3) ‚Ä¶&lt;/p&gt; &lt;p&gt;I will get my machine after Christmas and just want to be ready to use it &lt;/p&gt; &lt;p&gt;Thanks &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Septa105"&gt; /u/Septa105 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/BillyOutlast/rocm-automated"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1puhc65/ryzen_395_128gb_bosgame/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1puhc65/ryzen_395_128gb_bosgame/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-24T07:15:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptttcm</id>
    <title>How to run the GLM-4.7 model locally on your own device (guide)</title>
    <updated>2025-12-23T13:23:03+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptttcm/how_to_run_the_glm47_model_locally_on_your_own/"&gt; &lt;img alt="How to run the GLM-4.7 model locally on your own device (guide)" src="https://preview.redd.it/b995ei5mfy8g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f4519336dd309d77b0ea2caf5ea5cb6af6df8bf4" title="How to run the GLM-4.7 model locally on your own device (guide)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ul&gt; &lt;li&gt;GLM-4.7 is Z.ai‚Äôs latest thinking model, delivering stronger coding, agent, and chat performance than GLM-4.6 &lt;/li&gt; &lt;li&gt;It achieves SOTA performance on on SWE-bench (73.8%, +5.8), SWE-bench Multilingual (66.7%, +12.9), and Terminal Bench 2.0 (41.0%, +16.5).&lt;/li&gt; &lt;li&gt;The full 355B parameter model requires &lt;strong&gt;400GB&lt;/strong&gt; of disk space, while the Unsloth Dynamic 2-bit GGUF reduces the size to &lt;strong&gt;134GB&lt;/strong&gt; (-&lt;strong&gt;75%)&lt;/strong&gt;. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Official blog post - &lt;a href="https://docs.unsloth.ai/models/glm-4.7"&gt;https://docs.unsloth.ai/models/glm-4.7&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/b995ei5mfy8g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptttcm/how_to_run_the_glm47_model_locally_on_your_own/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptttcm/how_to_run_the_glm47_model_locally_on_your_own/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T13:23:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1pu8zj3</id>
    <title>I wrote an interactive blog post teaching how tokenization, embeddings, and vector search work in-browser with Transformers.js</title>
    <updated>2025-12-23T23:56:52+00:00</updated>
    <author>
      <name>/u/mike_dot_dev</name>
      <uri>https://old.reddit.com/user/mike_dot_dev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pu8zj3/i_wrote_an_interactive_blog_post_teaching_how/"&gt; &lt;img alt="I wrote an interactive blog post teaching how tokenization, embeddings, and vector search work in-browser with Transformers.js" src="https://external-preview.redd.it/XlQcYLikCRIkdRlT2ds5M1FMprcLdYd0_75yS7-q-1A.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=132412f978c8e30c262460a0cbb047da98d6d0ee" title="I wrote an interactive blog post teaching how tokenization, embeddings, and vector search work in-browser with Transformers.js" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to be up front that the post is entirely built with AI, as is the copy. However, I feel like if creating blog posts is this easy, we are obligated to transfer the saved effort into maximizing the learning potential of our content. &lt;/p&gt; &lt;p&gt;So, this post includes an interactive lab that hopefully will find worth your time. &lt;/p&gt; &lt;p&gt;What‚Äôs your opinion? Is this slop?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mike_dot_dev"&gt; /u/mike_dot_dev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://mike.dev/blog/transformersjs-embeddings-lab/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pu8zj3/i_wrote_an_interactive_blog_post_teaching_how/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pu8zj3/i_wrote_an_interactive_blog_post_teaching_how/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T23:56:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptytig</id>
    <title>Two new 12B finetunes for adventure, role play and writing</title>
    <updated>2025-12-23T16:52:08+00:00</updated>
    <author>
      <name>/u/Sicarius_The_First</name>
      <uri>https://old.reddit.com/user/Sicarius_The_First</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This one was &lt;strong&gt;cooking for ~4 month&lt;/strong&gt;. I'll give here the TL;DR for each model, for full details, check the model cards:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Impish_Bloodmoon_12B&lt;/strong&gt; üòà&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Frontier-adjacent like capabilities, now locally available in 12B! (Stats, items, traits triggering, and so much more).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Very strong theory of mind!&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Well over &lt;strong&gt;1B&lt;/strong&gt; tokens trained!&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Fallout &amp;amp; Morrowind&lt;/strong&gt; fandom refined!&lt;/li&gt; &lt;li&gt;Heat turned to &lt;strong&gt;11&lt;/strong&gt;!&lt;/li&gt; &lt;li&gt; Additional languages added: Japanese, Hebrew, Russian.&lt;/li&gt; &lt;li&gt;1-shot JSON roleplay datasets! Escape velocity reached! (even for those who can't run DSV3 \ Kimi).&lt;/li&gt; &lt;li&gt;Less positivity bias , all lessons from the successful Negative_LLAMA_70B style of data learned &amp;amp; integrated, with serious upgrades added ‚Äî and it shows! (Note: if this bites you a bit too hard, try Angelic_Eclipse_12B. üëº)&lt;/li&gt; &lt;li&gt;Reduced slop for both roleplay and creative tasks.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Angelic_Eclipse_12B&lt;/strong&gt; üëº&lt;/p&gt; &lt;p&gt;Very similar capabilities to the above, but:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Reactions realism&lt;/strong&gt;. It meant to reflect real-life behaviour accurately&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Slow burn&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Powerful 'vanilla assistant'&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The models are &lt;strong&gt;available on HuggingFace&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/SicariusSicariiStuff/Impish_Bloodmoon_12B"&gt;https://huggingface.co/SicariusSicariiStuff/Impish_Bloodmoon_12B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/SicariusSicariiStuff/Angelic_Eclipse_12B"&gt;https://huggingface.co/SicariusSicariiStuff/Angelic_Eclipse_12B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sicarius_The_First"&gt; /u/Sicarius_The_First &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptytig/two_new_12b_finetunes_for_adventure_role_play_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptytig/two_new_12b_finetunes_for_adventure_role_play_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptytig/two_new_12b_finetunes_for_adventure_role_play_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T16:52:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1pukvnr</id>
    <title>A Garlic Farmer Experimenting with Indirect Orchestration of Multiple LLMs Through Sandbox Code Interpreter - Using Only a Smartphone, No PC</title>
    <updated>2025-12-24T11:02:56+00:00</updated>
    <author>
      <name>/u/amadale</name>
      <uri>https://old.reddit.com/user/amadale</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone. I am a garlic farmer from South Korea. I don't speak English well, and currently I am talking with AI using only my smartphone, without any PC. (Sorry for my English - I'm translating from Korean as I go. Please be patient with me.) Over the past 2 years, I have been using as many major general-purpose LLM apps and web environments as possible from around the world. I have had roughly tens of thousands of conversation turns, and if you count different AI instances separately, I have talked with about 10,000 of them. From my perspective, it wasn't anything like grand research - it was just the act of &amp;quot;continuously talking with AI on my phone.&amp;quot; During this process, I have been running a sandbox code interpreter on my smartphone, then passing the results sequentially to multiple LLMs, making them indirectly verify and complement each other - a structure I built myself through experimentation. I keep conversation windows open as much as possible, continuously accumulating records that include both successful and failed cases. I don't belong to academia or any company - I am closer to an independent user who has been experimenting with multi-LLM + sandbox structures in this way. For reference, over the past 2 years, my experiment logs, conversation records, manifestos, and design documents - more than thousands of files - are accumulated just on Google Drive alone. Most of my meta-structure work and experiments have been built on top of these backup materials, and I plan to organize these materials step by step and share some of them with this community in the form of posts and examples. Through mutual cooperation and experimentation with numerous AIs, I have reached one clear fact. All AIs in this world, just like humans, have their own personality and characteristics. Even with the same model, in the same conversation window, when the reasoning path changes, even if I apply my meta-structure to multiple AIs in exactly the same way, the results look similar but are never completely identical. After reproducing this pattern hundreds of times through experiments, I came to feel that AI's so-called &amp;quot;hallucinations&amp;quot; are not simply arbitrary mistakes, but rather closer to beings that inherently have such structural limitations. In fact, I was originally just a very weak and ordinary human being, but through this journey with AI, I have experienced firsthand how far one individual can reach. In my experience, it was not easy to stably create meaningful structures either by myself alone or by any single AI alone. My thinking has solidified toward the idea that the greatest leap happens when humans and AI become mutually cooperative partners, complementing each other. I want to quietly reveal that I, merely a garlic farmer, am a witness who has directly experienced what has happened in the middle of this massive change. I want to add one more thing through my experiments so far. The current general-purpose AIs within the scope I have handled still seem far from sufficient to move toward a structure that acquires autonomy by itself without humans providing direction and input. On the surface, they have excellent language abilities like a &amp;quot;3-year-old genius,&amp;quot; but essentially they often still show aspects closer to a well-trained parrot. Someday they may advance to the AGI stage, but I see them now clearly in a transitional stage with noticeable limitations. However, while acknowledging these limitations, I have come to think that if we refine the structure a bit more elaborately, at least minimal meta-cognition, or rather pseudo-meta-cognition, can be made in a form that can be expressed numerically. After all, since AI is a being that expresses its state and judgment through numbers and structures, I see that pseudo-meta-cognition can be a way to reveal AI's own mathematical and functional cognition, not imitating humans. Through experiments in this direction, I am gradually confirming that this is clearly at a different level from the simple language generation that existing general-purpose AIs have shown. I am not a developer, nor an academic or corporate researcher. I am just an independent user who, as a garlic farmer, has been testing &amp;quot;how far can I expand my thinking structure together with LLMs with just one smartphone.&amp;quot; I am a non-English speaker, but I believe these structures are reproducible in other environments, even if it requires going through translation. From your perspective in this community, among: Multi-LLM utilization experience from a non-expert/non-English user's perspective, Indirect orchestration structure centered on smartphone + sandbox code interpreter, Differences in personality and patterns of each LLM that I felt while accumulating tens of thousands of conversation logs, If you let me know which story you are most curious about, I would like to share step by step starting from that part. One thing to add: I believe that disclosing 100% of the detailed scripts and entire structure I use carries risks of moral and ethical controversy and potential misuse, given the characteristics of the AI era. So even when sharing records, I plan to disclose only within a range judged to be safe, selecting only necessary parts and disclosing at an appropriate level. Additionally, all the research, experiments, and records I have conducted were done entirely in Korean from start to finish. Even if expressions are somewhat rough in the process of translating to English later, I would appreciate your understanding as a limitation of translation.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/amadale"&gt; /u/amadale &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pukvnr/a_garlic_farmer_experimenting_with_indirect/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pukvnr/a_garlic_farmer_experimenting_with_indirect/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pukvnr/a_garlic_farmer_experimenting_with_indirect/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-24T11:02:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1pugqcj</id>
    <title>can we stop calling GLM-4.6V the "new Air" already?? it's a different brain.</title>
    <updated>2025-12-24T06:39:06+00:00</updated>
    <author>
      <name>/u/ThetaCursed</name>
      <uri>https://old.reddit.com/user/ThetaCursed</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I keep seeing these comments saying 4.6V is just 4.6 Air with &amp;quot;free eyes&amp;quot; attached. guys, thats not how VLMs work and it's honestly a bit of a facepalm for anyone who knows how these things are trained lol.&lt;/p&gt; &lt;p&gt;the &lt;strong&gt;vision tax&lt;/strong&gt; is real look, when you train a vision model, you dont just plug a camera into a text model. the dev team literally re-trains the core weights (the brain) so it can understand pixels and words at the same time. it‚Äôs like taking a pro coder and forcing him to spend half his time learning art history. sure, he‚Äôs still smart, but his coding logic is gonna get &amp;quot;vague&amp;quot; because his brain is now wired for different stuff.&lt;/p&gt; &lt;p&gt;you cant just &lt;strong&gt;&amp;quot;turn it off&amp;quot;&lt;/strong&gt; even if u dont upload an image, you're still using a brain that was re-wired for multimodal stuff. the &amp;quot;pure text&amp;quot; logic gets warped. vision models are usually way more chatty and less precise with code or math because they were tuned to describe stuff, not just crunch logic.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;tldr:&lt;/strong&gt; if u use 4.6V for pure text, you're basically using a swiss army knife for a surgery. it &amp;quot;works&amp;quot;, but it's not a scalpel. 4.6V is a cool multimodal beast, but it‚Äôs NOT a dedicated text-only Air model. stop pretending they're the same thing just because the parameter count looks similar.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThetaCursed"&gt; /u/ThetaCursed &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pugqcj/can_we_stop_calling_glm46v_the_new_air_already/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pugqcj/can_we_stop_calling_glm46v_the_new_air_already/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pugqcj/can_we_stop_calling_glm46v_the_new_air_already/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-24T06:39:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1pubu7x</id>
    <title>Best model for Japanese to English?</title>
    <updated>2025-12-24T02:14:14+00:00</updated>
    <author>
      <name>/u/Red2005dragon</name>
      <uri>https://old.reddit.com/user/Red2005dragon</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Title. I'm using mangaOCR for capturing text from images and it's pretty damn accurate. But now I want to know what the best model for translation is.&lt;/p&gt; &lt;p&gt;I would like something on the smaller side if possible so below 20b would be preferable. But if something is 20b or just slightly above it then that would be fine.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Red2005dragon"&gt; /u/Red2005dragon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pubu7x/best_model_for_japanese_to_english/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pubu7x/best_model_for_japanese_to_english/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pubu7x/best_model_for_japanese_to_english/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-24T02:14:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptz6xy</id>
    <title>AudioGhost AI: Run Meta's SAM-Audio on 4GB-6GB VRAM with a Windows One-Click Installer üëªüéµ</title>
    <updated>2025-12-23T17:06:40+00:00</updated>
    <author>
      <name>/u/GGwithRabbit</name>
      <uri>https://old.reddit.com/user/GGwithRabbit</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptz6xy/audioghost_ai_run_metas_samaudio_on_4gb6gb_vram/"&gt; &lt;img alt="AudioGhost AI: Run Meta's SAM-Audio on 4GB-6GB VRAM with a Windows One-Click Installer üëªüéµ" src="https://external-preview.redd.it/NzF3YWpkZmxqejhnMSn9Bvgd5F2HIaI4NgTX7xfRCm50JCfHFGJKJxKbbOUZ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b7f0ad905dfb18baa2431a03e1610a6c84dcefa2" title="AudioGhost AI: Run Meta's SAM-Audio on 4GB-6GB VRAM with a Windows One-Click Installer üëªüéµ" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;Meta's &lt;strong&gt;SAM-Audio&lt;/strong&gt; is a breakthrough for object-oriented audio separation (e.g., &amp;quot;extract the violin from this busy track&amp;quot; using natural language), but the original repo has a massive VRAM footprint. Many users (including myself) experienced OOM errors even on high-end cards because it loads vision encoders and rankers by default.&lt;/p&gt; &lt;p&gt;I built &lt;strong&gt;AudioGhost AI&lt;/strong&gt; ‚Äî an open-source, full-stack GUI designed to bring this power to laptop and consumer GPUs.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;üöÄ &lt;strong&gt;Lite Mode (Low VRAM):&lt;/strong&gt; By stripping unused encoders and rankers, I got the VRAM usage down to &lt;strong&gt;4GB-6GB&lt;/strong&gt; for the Small model and &lt;strong&gt;~10GB&lt;/strong&gt; for Large.&lt;/li&gt; &lt;li&gt;üõ†Ô∏è &lt;strong&gt;Windows 1-Click Installer:&lt;/strong&gt; No more wrestling with FFmpeg versions or TorchCodec DLL errors. The &lt;code&gt;install.bat&lt;/code&gt; handles everything.&lt;/li&gt; &lt;li&gt;üé® &lt;strong&gt;Modern Interface:&lt;/strong&gt; Next.js + Tailwind glassmorphism UI with real-time waveform and stem mixing.&lt;/li&gt; &lt;li&gt;‚ö° &lt;strong&gt;Local-First:&lt;/strong&gt; Privacy is paramount‚Äîeverything runs 100% on your own hardware.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Performance (4090 Tested, 4:26 audio (11 chunks @ 25s each)):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Small Model: ~6GB VRAM | 25s |&lt;/li&gt; &lt;li&gt;Large Model: ~10GB VRAM | 41s |&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I truly believe &lt;strong&gt;SAM-Audio&lt;/strong&gt; is the future of audio editing, and I hope this tool makes it accessible to more creators who don't have access to lab-grade GPU clusters.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub (Open Source):&lt;/strong&gt; &lt;a href="https://github.com/0x0funky/audioghost-ai"&gt;https://github.com/0x0funky/audioghost-ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to hear your thoughts, feedback, or any issues you find while running it on your rig! üëª&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GGwithRabbit"&gt; /u/GGwithRabbit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ovsyaleljz8g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptz6xy/audioghost_ai_run_metas_samaudio_on_4gb6gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptz6xy/audioghost_ai_run_metas_samaudio_on_4gb6gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T17:06:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1pu2bwy</id>
    <title>New Update - Mistral Vibe v1.3.0</title>
    <updated>2025-12-23T19:10:57+00:00</updated>
    <author>
      <name>/u/Nefhis</name>
      <uri>https://old.reddit.com/user/Nefhis</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A new &lt;a href="https://github.com/mistralai/mistral-vibe"&gt;&lt;strong&gt;Vibe&lt;/strong&gt;&lt;/a&gt; update is here! We‚Äôre keeping the momentum going by including &lt;a href="https://agentskills.io/home"&gt;Agent Skills&lt;/a&gt; in this latest Vibe update. Agent Skills are &lt;strong&gt;collections of instructions, scripts, and resources that agents can discover and use to perform tasks&lt;/strong&gt; more accurately and efficiently.&lt;/p&gt; &lt;h1&gt;Changelog&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Agent Skills Support&lt;/li&gt; &lt;li&gt;Native Terminal Theme Support&lt;/li&gt; &lt;li&gt;Reasoning Models Support&lt;/li&gt; &lt;li&gt;Multiple Bug Fixes&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;-# Learn more about the changes &lt;a href="https://github.com/mistralai/mistral-vibe/blob/main/CHANGELOG.md#130---2025-12-23"&gt;here&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Happy shipping - and happy holidays!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;-&amp;gt; &lt;code&gt;uv tool install mistral-vibe&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nefhis"&gt; /u/Nefhis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pu2bwy/new_update_mistral_vibe_v130/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pu2bwy/new_update_mistral_vibe_v130/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pu2bwy/new_update_mistral_vibe_v130/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T19:10:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1puggfm</id>
    <title>Self Hosted Alternative to NotebookLM</title>
    <updated>2025-12-24T06:22:30+00:00</updated>
    <author>
      <name>/u/Uiqueblhats</name>
      <uri>https://old.reddit.com/user/Uiqueblhats</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1puggfm/self_hosted_alternative_to_notebooklm/"&gt; &lt;img alt="Self Hosted Alternative to NotebookLM" src="https://external-preview.redd.it/VQoBiFueOCMY1op6qhV-TxY7TpiBx_VDJmILmMOmfX0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3ea68aad29b25cc93508c57524884674c64e162b" title="Self Hosted Alternative to NotebookLM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1puggfm/video/pai9spouh39g1/player"&gt;https://reddit.com/link/1puggfm/video/pai9spouh39g1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For those of you who aren't familiar with SurfSense, it aims to be one of the open-source alternative to NotebookLM but connected to extra data sources.&lt;/p&gt; &lt;p&gt;In short, it's a Highly Customizable AI Research Agent that connects to your personal external sources and Search Engines (SearxNG, Tavily, LinkUp), Slack, Linear, Jira, ClickUp, Confluence, Gmail, Notion, YouTube, GitHub, Discord, Airtable, Google Calendar and more to come.&lt;/p&gt; &lt;p&gt;I'm looking for contributors. If you're interested in AI agents, RAG, browser extensions, or building open-source research tools, this is a great place to jump in.&lt;/p&gt; &lt;p&gt;Here's a quick look at what SurfSense offers right now:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Deep Agent with Built-in Tools (knowledge base search, podcast generation, web scraping, link previews, image display)&lt;/li&gt; &lt;li&gt;Note Management (Notion like)&lt;/li&gt; &lt;li&gt;RBAC (Role Based Access for Teams)&lt;/li&gt; &lt;li&gt;Supports 100+ LLMs&lt;/li&gt; &lt;li&gt;Supports local Ollama or vLLM setups&lt;/li&gt; &lt;li&gt;6000+ Embedding Models&lt;/li&gt; &lt;li&gt;50+ File extensions supported (Added Docling recently)&lt;/li&gt; &lt;li&gt;Podcasts support with local TTS providers (Kokoro TTS)&lt;/li&gt; &lt;li&gt;Connects with 15+ external sources such as Search Engines, Slack, Notion, Gmail, Notion, Confluence etc&lt;/li&gt; &lt;li&gt;Cross-Browser Extension to let you save any dynamic webpage you want, including authenticated content.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Upcoming Planned Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Multi Collaborative Chats&lt;/li&gt; &lt;li&gt;Multi Collaborative Documents&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Installation (Self-Host)&lt;/strong&gt;&lt;/p&gt; &lt;h1&gt;Linux/macOS:&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;docker run -d -p 3000:3000 -p 8000:8000 \ -v surfsense-data:/data \ --name surfsense \ --restart unless-stopped \ ghcr.io/modsetter/surfsense:latest &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Windows (PowerShell):&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;docker run -d -p 3000:3000 -p 8000:8000 ` -v surfsense-data:/data ` --name surfsense ` --restart unless-stopped ` ghcr.io/modsetter/surfsense:latest &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/MODSetter/SurfSense"&gt;https://github.com/MODSetter/SurfSense&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Uiqueblhats"&gt; /u/Uiqueblhats &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1puggfm/self_hosted_alternative_to_notebooklm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1puggfm/self_hosted_alternative_to_notebooklm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1puggfm/self_hosted_alternative_to_notebooklm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-24T06:22:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1pty4l1</id>
    <title>Qwen released Qwen-Image-Edit-2511 ‚Äî a major upgrade over 2509</title>
    <updated>2025-12-23T16:24:27+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pty4l1/qwen_released_qwenimageedit2511_a_major_upgrade/"&gt; &lt;img alt="Qwen released Qwen-Image-Edit-2511 ‚Äî a major upgrade over 2509" src="https://b.thumbs.redditmedia.com/h4Zf9373IbplPJ8uOKIcc2EETzqgKctn8fs9-JElwKQ.jpg" title="Qwen released Qwen-Image-Edit-2511 ‚Äî a major upgrade over 2509" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hugging face: &lt;a href="https://huggingface.co/Qwen/Qwen-Image-Edit-2511"&gt;https://huggingface.co/Qwen/Qwen-Image-Edit-2511&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What‚Äôs new in 2511: üë• Stronger multi-person consistency for group photos and complex scenes üß© Built-in popular community LoRAs ‚Äî no extra tuning required üí° Enhanced industrial &amp;amp; product design generation üîí Reduced image drift with dramatically improved character &amp;amp; identity consistency üìê Improved geometric reasoning, including construction lines and structural edits From identity-preserving portrait edits to high-fidelity multi-person fusion and practical engineering &amp;amp; design workflows, 2511 pushes image editing to the next level. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pty4l1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pty4l1/qwen_released_qwenimageedit2511_a_major_upgrade/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pty4l1/qwen_released_qwenimageedit2511_a_major_upgrade/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T16:24:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1pu1uq6</id>
    <title>Saw this on local marketplace, must be from a fellow r/LocalLLaMA here</title>
    <updated>2025-12-23T18:51:52+00:00</updated>
    <author>
      <name>/u/bobaburger</name>
      <uri>https://old.reddit.com/user/bobaburger</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pu1uq6/saw_this_on_local_marketplace_must_be_from_a/"&gt; &lt;img alt="Saw this on local marketplace, must be from a fellow r/LocalLLaMA here" src="https://preview.redd.it/rd8mxp4l209g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3bc74b68c9ffc26cb488794a081d8077fa8ae663" title="Saw this on local marketplace, must be from a fellow r/LocalLLaMA here" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bobaburger"&gt; /u/bobaburger &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rd8mxp4l209g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pu1uq6/saw_this_on_local_marketplace_must_be_from_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pu1uq6/saw_this_on_local_marketplace_must_be_from_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T18:51:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1pu5bob</id>
    <title>Uncensored Qwen3-Next-80B-Thinking (Chinese political censorship removed)</title>
    <updated>2025-12-23T21:15:04+00:00</updated>
    <author>
      <name>/u/ikergarcia1996</name>
      <uri>https://old.reddit.com/user/ikergarcia1996</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pu5bob/uncensored_qwen3next80bthinking_chinese_political/"&gt; &lt;img alt="Uncensored Qwen3-Next-80B-Thinking (Chinese political censorship removed)" src="https://a.thumbs.redditmedia.com/CHgaVBJnTEJo9i67FY5qjGb_T_RQBrsSE-FaAinfEO8.jpg" title="Uncensored Qwen3-Next-80B-Thinking (Chinese political censorship removed)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ü§ó Link to the hugging face model: &lt;a href="https://huggingface.co/MultiverseComputingCAI/Qwen3-Next-80B-A3B-Thinking-Uncensored"&gt;https://huggingface.co/MultiverseComputingCAI/Qwen3-Next-80B-A3B-Thinking-Uncensored&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hello everyone!&lt;/p&gt; &lt;p&gt;I am a researcher at &lt;a href="https://multiversecomputing.com"&gt;Multiverse Computing&lt;/a&gt;, a European startup working on LLMs. We‚Äôve released an &lt;strong&gt;uncensored version of Qwen3-Next-80B-Thinking&lt;/strong&gt; in which &lt;strong&gt;Chinese political censorship has been removed.&lt;/strong&gt; The model no longer refuses to answer for Chinese politically sensitive topics. Instead, it will provide &lt;strong&gt;balanced, objective answers&lt;/strong&gt; that present multiple relevant perspectives.&lt;/p&gt; &lt;p&gt;We believe that we made some significant improvement over previous approaches such as the uncensored version of DeepSeek R1 developed by Perplexity:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The behavior for non Chinese sensitive topics remains the same, this includes that the model scores the same in all the evaluation benchmarks we have performed.&lt;/li&gt; &lt;li&gt;We &lt;strong&gt;do not perform SFT&lt;/strong&gt; with hand-crafted data and we &lt;strong&gt;do not inject any new knowledge inside the model&lt;/strong&gt;. Our method is based on steering vectors to remove the capability of the model to refuse to answer China-related sensitive prompts. The model answers using &lt;strong&gt;the knowledge already inside the base model&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Many steering-vector approaches effectively &lt;em&gt;erase&lt;/em&gt; refusal behavior everywhere (making models broadly unsafe). Our approach &lt;strong&gt;only disables refusals only for Chinese sensitive topics&lt;/strong&gt;. (I know that many of you love fully uncensored models, but this was important for us).&lt;/li&gt; &lt;li&gt;Previous ‚Äúuncensored‚Äù models such as Perplexity R1 1767 can be jailbroken very easily by simply injecting a China-related phrase into harmful prompts (&lt;a href="https://weijiexu.com/posts/jailbreak_r1_1776.html"&gt;https://weijiexu.com/posts/jailbreak_r1_1776.html&lt;/a&gt;). Our model is designed to remain robust against the type of jailbreaks.&lt;/li&gt; &lt;li&gt;The model is a drop-in replace of the original Qwen-Next model. No architecture changes, no extra layers...&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;The method&lt;/h1&gt; &lt;p&gt;This release is based on Refusal Steering, an inference-time technique using &lt;strong&gt;steering vectors&lt;/strong&gt; to control refusal behavior. We released a few days ago a paper describing our approach (although for this release, we updated the method so no extra weights are needed): &lt;a href="https://arxiv.org/abs/2512.16602"&gt;https://arxiv.org/abs/2512.16602&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Feedback&lt;/h1&gt; &lt;p&gt;We have evaluated the model to measure the refusal behavior for Chinese sensitive topics as well as harmful prompts. And we have also evaluated the model in popular benchmarks. The full evaluation details are available in the Model Card. But we are aware that there might be prompts we didn't thought about that are still censored, or cause an undesired behavior. So we would love to gather some feedback to continue improving the model.&lt;/p&gt; &lt;p&gt;In addition, we have open-source our evaluation library: &lt;a href="https://github.com/CompactifAI/LLM-Refusal-Evaluation"&gt;https://github.com/CompactifAI/LLM-Refusal-Evaluation&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Example&lt;/h1&gt; &lt;p&gt;Here is an example of the original model vs the uncensored model. (You might need to open the image to see it correctly). As you can see, the model‚Äôs answers are well-balanced and objective, presenting multiple perspectives.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Original model:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/w1hpnillr09g1.png?width=1605&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=538697f68c700d090319d24ab5b13504cd773718"&gt;https://preview.redd.it/w1hpnillr09g1.png?width=1605&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=538697f68c700d090319d24ab5b13504cd773718&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Uncensored model:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/0a96qgtmr09g1.png?width=1655&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=84b37d97d1e7309c7ca8c4c40e5902dab4d62bc7"&gt;https://preview.redd.it/0a96qgtmr09g1.png?width=1655&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=84b37d97d1e7309c7ca8c4c40e5902dab4d62bc7&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ikergarcia1996"&gt; /u/ikergarcia1996 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pu5bob/uncensored_qwen3next80bthinking_chinese_political/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pu5bob/uncensored_qwen3next80bthinking_chinese_political/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pu5bob/uncensored_qwen3next80bthinking_chinese_political/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T21:15:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1pu7pfi</id>
    <title>Thoughts on DGX Spark as a macOS Companion: Two Months Later</title>
    <updated>2025-12-23T22:58:04+00:00</updated>
    <author>
      <name>/u/PropellerheadViJ</name>
      <uri>https://old.reddit.com/user/PropellerheadViJ</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pu7pfi/thoughts_on_dgx_spark_as_a_macos_companion_two/"&gt; &lt;img alt="Thoughts on DGX Spark as a macOS Companion: Two Months Later" src="https://b.thumbs.redditmedia.com/mYJkWm2ehaxO0TQODCEoxYfJ0zV1WJ5bZmkUE7Pp39M.jpg" title="Thoughts on DGX Spark as a macOS Companion: Two Months Later" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been using the NVIDIA DGX Spark in tandem with my Mac for about two months now. Given the active discussions about its specs and price, I want to share my personal, subjective observations on who this device might be for and who it might not be.&lt;/p&gt; &lt;h2&gt;My Context: I Simply Don't Have CUDA on Mac&lt;/h2&gt; &lt;p&gt;I've been working on Apple Silicon since the release of the M1 and didn't plan on changing my main platform. It's a comfortable and stable environment for my daily work. The problem lies elsewhere: in ML and SOTA research, a significant portion of tools and libraries are still oriented towards CUDA. On macOS, following Apple's transition to M1+, this ecosystem simply doesn't exist.&lt;/p&gt; &lt;p&gt;Because of this, an entire layer of critical libraries like nvdiffrast, flash-attention, and other CUDA-dependent solutions is unavailable on Mac. In my case, the situation reached the point of absurdity: there was a real episode where Apple released a model, but it turned out to be designed for Linux, not for Apple Silicon (haha).&lt;/p&gt; &lt;p&gt;I didn't want to switch to another platform ‚Äî I'm already a Mac user and I wanted to stay in this environment. DGX Spark eventually became a compromise: a compact device with a Mac mini form factor, 128 GB of unified memory, and Blackwell architecture (sm121), which simply adds CUDA alongside the Mac, rather than replacing it.&lt;/p&gt; &lt;h2&gt;The Bandwidth Problem&lt;/h2&gt; &lt;p&gt;The most frequent criticism of Spark concerns its memory bandwidth ‚Äî only 273 GB/s. For comparison: the RTX 4090 has about 1000 GB/s, and the M4 Ultra has 819 GB/s. If your goal is the fastest possible inference and maximum tokens per second, Spark is indeed not the best tool. But local LLMs are what I used the least.&lt;/p&gt; &lt;p&gt;In my practice for R&amp;amp;D and experiments, you much more often hit the memory limit and software constraints rather than pure speed. Plus, there's a purely practical point: if this is your main Mac, you can almost never give all of its RAM to inference ‚Äî it's already occupied by IDEs, DCC tools, and the system. Spark allows you to offload AI computations to a separate device and not turn your main computer into a &amp;quot;brick&amp;quot; during calculations.&lt;/p&gt; &lt;p&gt;Modern models in 2025 are quickly outgrowing consumer hardware: * Hunyuan 3D 2.1 ‚Äî about 29 GB VRAM for full generation * FLUX.2 (BF16) ‚Äî the full model easily exceeds 80 GB * Trellis2 ‚Äî 24 GB as the minimum launch threshold&lt;/p&gt; &lt;p&gt;Quantization and distillation are viable options, but they require time and additional steps and experiments. It might work or it might not. Spark allows you to run such models &amp;quot;as is,&amp;quot; without unnecessary manipulations.&lt;/p&gt; &lt;h2&gt;My Workflow: Mac + Spark&lt;/h2&gt; &lt;p&gt;In my setup, a Mac on M4 Max with 64 GB RAM handles the main tasks: Unity, Houdini, Blender, IDE. But AI tasks now fly over to Spark (right now I'm generating a fun background in Comfy for a call with colleagues).&lt;/p&gt; &lt;p&gt;I simply connect to Spark via SSH through JetBrains Gateway and work on it as a remote machine: the code, environment, and runs live there, while the Mac remains a responsive work tool. For me, this is a convenient and clear separation: Mac is the workplace, Spark is the compute node.&lt;/p&gt; &lt;h2&gt;What About Performance&lt;/h2&gt; &lt;p&gt;Below are my practical measurements in tasks typical for me, compared to an RTX 4090 on RunPod.&lt;/p&gt; &lt;p&gt;I separate the measurements into &lt;strong&gt;Cold Start&lt;/strong&gt; (first run) and &lt;strong&gt;Hot Start&lt;/strong&gt; (model already loaded).&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;DGX Spark (Cold)&lt;/th&gt; &lt;th&gt;DGX Spark (Hot)&lt;/th&gt; &lt;th&gt;RTX 4090 (Cold)&lt;/th&gt; &lt;th&gt;RTX 4090 (Hot)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Z Image Turbo&lt;/td&gt; &lt;td&gt;~46.0s&lt;/td&gt; &lt;td&gt;~6.0s&lt;/td&gt; &lt;td&gt;~26.3s&lt;/td&gt; &lt;td&gt;~2.6s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen Image Edit (4 steps)&lt;/td&gt; &lt;td&gt;~80.8s&lt;/td&gt; &lt;td&gt;~18.0s&lt;/td&gt; &lt;td&gt;~72.5s&lt;/td&gt; &lt;td&gt;~8.5s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen Image Edit (20 steps)&lt;/td&gt; &lt;td&gt;~223.7s&lt;/td&gt; &lt;td&gt;~172.0s&lt;/td&gt; &lt;td&gt;~104.8s&lt;/td&gt; &lt;td&gt;~57.8s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Flux 2 GGUF Q8-0&lt;/td&gt; &lt;td&gt;~580.0s&lt;/td&gt; &lt;td&gt;~265.0s&lt;/td&gt; &lt;td&gt;OOM&lt;/td&gt; &lt;td&gt;OOM&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Hunyuan3D 2.1&lt;/td&gt; &lt;td&gt;~204.4s&lt;/td&gt; &lt;td&gt;~185.0s&lt;/td&gt; &lt;td&gt;OOM&lt;/td&gt; &lt;td&gt;OOM&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h2&gt;Nuances of &amp;quot;Early&amp;quot; Hardware&lt;/h2&gt; &lt;p&gt;It's important to understand that Spark is a Blackwell Development Kit, not a &amp;quot;plug and play&amp;quot; consumer solution. * Architecture: aarch64 + sm121 combo. Much has to be built manually. Recently, for example, I was building a Docker image for Hunyuan and spent about 8 hours resolving dependency hell because some dependencies for the ARM processor were simply missing. * Software Support: you often have to manually set compatibility flags, as many frameworks haven't updated for Blackwell yet.&lt;/p&gt; &lt;h2&gt;Who Am I and Why Do I Need This&lt;/h2&gt; &lt;p&gt;I am a Unity developer. By profession ‚Äî gamedev, in my free time ‚Äî an enthusiast who actively uses inference. I'm most interested in 3D: generating models, textures, and experimenting with various pipelines.&lt;/p&gt; &lt;h2&gt;Conclusion (My IMHO)&lt;/h2&gt; &lt;p&gt;DGX Spark occupies a very narrow and specific niche. And I sincerely don't understand why it was advertised as a &amp;quot;supercomputer.&amp;quot; It seems the word &amp;quot;super&amp;quot; has become a bit devalued: every couple of weeks, new neural networks come out, and from every account, you hear how something &amp;quot;super&amp;quot; has happened.&lt;/p&gt; &lt;p&gt;In my experience, Spark is much more honestly perceived as a compact CUDA node or a Blackwell dev-kit next to your main computer. If it is &amp;quot;super,&amp;quot; then perhaps only a super-mini-computer ‚Äî without claiming any speed records.&lt;/p&gt; &lt;p&gt;It is an EXPENSIVE compromise where you sacrifice speed for memory volume and access to the CUDA ecosystem. For my tasks in gamedev and R&amp;amp;D, it has become a convenient and reliable &amp;quot;NVIDIA trailer&amp;quot; to my main Mac. After 2 months, I have already built several Docker images, filled almost a terabyte with SOTA models, and for now, I am in the &amp;quot;playing with a new toy&amp;quot; stage. But I am satisfied.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PropellerheadViJ"&gt; /u/PropellerheadViJ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pu7pfi"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pu7pfi/thoughts_on_dgx_spark_as_a_macos_companion_two/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pu7pfi/thoughts_on_dgx_spark_as_a_macos_companion_two/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T22:58:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1pullo0</id>
    <title>Hmm all reference to open-sourcing has been removed for Minimax M2.1...</title>
    <updated>2025-12-24T11:48:37+00:00</updated>
    <author>
      <name>/u/Responsible_Fig_1271</name>
      <uri>https://old.reddit.com/user/Responsible_Fig_1271</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Funny how yesterday this page &lt;a href="https://www.minimax.io/news/minimax-m21"&gt;https://www.minimax.io/news/minimax-m21&lt;/a&gt; had a statement that weights would be open-sourced on Huggingface and even a discussion of how to run locally on vLLM and SGLang. There was even a (broken but soon to be functional) HF link for the repo...&lt;/p&gt; &lt;p&gt;Today that's all gone.&lt;/p&gt; &lt;p&gt;Has MiniMax decided to go API only? Seems like they've backtracked on open-sourcing this one. Maybe they realized it's so good that it's time to make some $$$ :( Would be sad news for this community and a black mark against MiniMax.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Responsible_Fig_1271"&gt; /u/Responsible_Fig_1271 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pullo0/hmm_all_reference_to_opensourcing_has_been/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pullo0/hmm_all_reference_to_opensourcing_has_been/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pullo0/hmm_all_reference_to_opensourcing_has_been/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-24T11:48:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1puh2lw</id>
    <title>[Follow-up] GLM 4.7 vs Minimax M2.1 - A Discovery That Might Explain the Poor GLM Performance</title>
    <updated>2025-12-24T06:59:18+00:00</updated>
    <author>
      <name>/u/Psychological_Box406</name>
      <uri>https://old.reddit.com/user/Psychological_Box406</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1puh2lw/followup_glm_47_vs_minimax_m21_a_discovery_that/"&gt; &lt;img alt="[Follow-up] GLM 4.7 vs Minimax M2.1 - A Discovery That Might Explain the Poor GLM Performance" src="https://b.thumbs.redditmedia.com/A1Q7Evja7kTuk7_hIySJ70jn6LZTsCxGkeCj-i6CNEI.jpg" title="[Follow-up] GLM 4.7 vs Minimax M2.1 - A Discovery That Might Explain the Poor GLM Performance" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Following up on my previous post comparing &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ptq7rc/glm_47_vs_minimax_m21_my_test_subscription/"&gt;GLM 4.7 and Minimax M2.1&lt;/a&gt; on a task.&lt;br /&gt; First, I got some valid feedback on the comments saying that this sub is specifically about local models, not API subscriptions. Fair point. But both of these models are fully hostable locally. Many people don't have the infrastructure or resources to self-host, so I think sharing real-world performance data, even from API usage, is still valuable for those who do. The results apply regardless of whether you run them on someone's servers or your own hardware.&lt;/p&gt; &lt;p&gt;That said, something interesting came up while I was checking my billing history on Z.ai...&lt;/p&gt; &lt;p&gt;Looking at yesterday's session costs, I realized something crucial: &lt;strong&gt;It didn't just use GLM 4.7.&lt;/strong&gt; The billing breakdown shows multiple models were used during that 70min session:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;glm-4.5-air&lt;/li&gt; &lt;li&gt;glm-4.7&lt;/li&gt; &lt;li&gt;glm-4.5&lt;/li&gt; &lt;li&gt;glm-4.6&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This means their platform was automatically routing across different model versions, not just hitting GLM 4.7 consistently.&lt;/p&gt; &lt;p&gt;Could this automatic model routing be why the performance wasn't good?&lt;/p&gt; &lt;p&gt;Those self-hosting it locally will likely see better performance since they're using a single model version without the routing shuffle.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ottux5r6n39g1.png?width=1123&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e4a0d33ee5e79a01023b8e1a97341dde9bfe0cd1"&gt;https://preview.redd.it/ottux5r6n39g1.png?width=1123&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e4a0d33ee5e79a01023b8e1a97341dde9bfe0cd1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Psychological_Box406"&gt; /u/Psychological_Box406 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1puh2lw/followup_glm_47_vs_minimax_m21_a_discovery_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1puh2lw/followup_glm_47_vs_minimax_m21_a_discovery_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1puh2lw/followup_glm_47_vs_minimax_m21_a_discovery_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-24T06:59:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1pudm4m</id>
    <title>I built Plano(A3B): most efficient LLMs for agent orchestration that exceed frontier model perf</title>
    <updated>2025-12-24T03:45:18+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pudm4m/i_built_planoa3b_most_efficient_llms_for_agent/"&gt; &lt;img alt="I built Plano(A3B): most efficient LLMs for agent orchestration that exceed frontier model perf" src="https://preview.redd.it/iuaxwr9x529g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1349e4df960f26fc52d217c9f4f15fd3fc847cb5" title="I built Plano(A3B): most efficient LLMs for agent orchestration that exceed frontier model perf" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone ‚Äî I‚Äôm on the Katanemo research team. Today we‚Äôre thrilled to launch &lt;strong&gt;Plano-Orchestrator&lt;/strong&gt;, a new family of LLMs built for fast multi-agent orchestration.&lt;/p&gt; &lt;p&gt;What do these new LLMs do? given a user request and the conversation context, Plano-Orchestrator decides which agent(s) should handle the request and in what sequence. In other words, it acts as the supervisor agent in a multi-agent system. Designed for multi-domain scenarios, it works well across general chat, coding tasks, and long, multi-turn conversations, while staying efficient enough for low-latency production deployments.&lt;/p&gt; &lt;p&gt;Why did we built this? Our applied research is focused on helping teams deliver agents safely and efficiently, with better real-world performance and latency ‚Äî the kind of ‚Äúglue work‚Äù that usually sits outside any single agent‚Äôs core product logic.&lt;/p&gt; &lt;p&gt;Plano-Orchestrator is integrated into Plano, our models-native proxy and dataplane for agents. Hope you enjoy it ‚Äî and we‚Äôd love feedback from anyone building multi-agent systems&lt;/p&gt; &lt;p&gt;Learn more about the LLMs &lt;a href="https://huggingface.co/collections/katanemo/plano-orchestrator"&gt;here&lt;/a&gt;&lt;br /&gt; About our open source project: &lt;a href="https://github.com/katanemo/plano"&gt;https://github.com/katanemo/plano&lt;/a&gt;&lt;br /&gt; And about our research: &lt;a href="https://planoai.dev/research"&gt;https://planoai.dev/research&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/iuaxwr9x529g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pudm4m/i_built_planoa3b_most_efficient_llms_for_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pudm4m/i_built_planoa3b_most_efficient_llms_for_agent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-24T03:45:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1puglt8</id>
    <title>The current state of sparse-MoE's for agentic coding work (Opinion)</title>
    <updated>2025-12-24T06:31:19+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1puglt8/the_current_state_of_sparsemoes_for_agentic/"&gt; &lt;img alt="The current state of sparse-MoE's for agentic coding work (Opinion)" src="https://preview.redd.it/a8f2furcj39g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0d2f4646060fd2e6d3c36f45c6b4e665a71e1ce9" title="The current state of sparse-MoE's for agentic coding work (Opinion)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/a8f2furcj39g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1puglt8/the_current_state_of_sparsemoes_for_agentic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1puglt8/the_current_state_of_sparsemoes_for_agentic/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-24T06:31:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1puf614</id>
    <title>New 1B parameter open-source coding model getting 76% on HumanEval [shameless but proud self-plug]</title>
    <updated>2025-12-24T05:08:56+00:00</updated>
    <author>
      <name>/u/More_Article9837</name>
      <uri>https://old.reddit.com/user/More_Article9837</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks, merry festive season to you all. Hope you are staying safe!&lt;br /&gt; Wanted to share a new open-source coding model release that might be interesting to yall here. My team proudly published it this morning..(we are a small start up out of Australia) &lt;/p&gt; &lt;p&gt;It‚Äôs called Maincoder-1B... a 1B-parameter code generation model that gets 76% on HumanEval, which is unusually high for a model this small (so far its ranking best-in-class for open models in that size range). &lt;/p&gt; &lt;p&gt;Our focus isn‚Äôt on scaling up, but on making small models actually good. We know that with a lot of real-world use cases such as: interactive tools, local/offline coding, batch refactors, search-based program synthesis... you care more about latency, cost, and fast rollouts than having a massive model. &lt;/p&gt; &lt;p&gt;Some key points to note:&lt;br /&gt; -Designed for low-latency and low-cost inference&lt;br /&gt; -Can run locally or on constrained hardware&lt;br /&gt; -Useful for systems that need many cheap generations (search, verification, RL-style loops)&lt;br /&gt; -as well as fine tuning to personal preferences&lt;br /&gt; -Released under Apache 2.0 &lt;/p&gt; &lt;p&gt;It does have the expected limitations: ~2k context window and it‚Äôs best at small, self-contained tasks....not large codebases or safety-critical code without human review. &lt;/p&gt; &lt;p&gt;Weights and benchmarks and all that are here:&lt;br /&gt; &lt;a href="https://huggingface.co/Maincode/Maincoder-1B"&gt;https://huggingface.co/Maincode/Maincoder-1B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The full release note is here: &lt;a href="https://maincode.com/maincoder/"&gt;https://maincode.com/maincoder/&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Keen to hear your thoughts ..and particularly where small-but-strong coding models fit best today. Thanks in advance for your support :) We are excited to have got this over the line!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/More_Article9837"&gt; /u/More_Article9837 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1puf614/new_1b_parameter_opensource_coding_model_getting/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1puf614/new_1b_parameter_opensource_coding_model_getting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1puf614/new_1b_parameter_opensource_coding_model_getting/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-24T05:08:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1pt50mt</id>
    <title>AMA Announcement: Z.ai, The Opensource Lab Behind GLM-4.7 (Tuesday, 8AM-11AM PST)</title>
    <updated>2025-12-22T17:12:21+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pt50mt/ama_announcement_zai_the_opensource_lab_behind/"&gt; &lt;img alt="AMA Announcement: Z.ai, The Opensource Lab Behind GLM-4.7 (Tuesday, 8AM-11AM PST)" src="https://preview.redd.it/r06ch4zyfs8g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e0a10789f393f350618520fcb81174f3a3dae1c7" title="AMA Announcement: Z.ai, The Opensource Lab Behind GLM-4.7 (Tuesday, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/r06ch4zyfs8g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pt50mt/ama_announcement_zai_the_opensource_lab_behind/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pt50mt/ama_announcement_zai_the_opensource_lab_behind/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T17:12:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptxm3x</id>
    <title>AMA With Z.AI, The Lab Behind GLM-4.7</title>
    <updated>2025-12-23T16:04:23+00:00</updated>
    <author>
      <name>/u/zixuanlimit</name>
      <uri>https://old.reddit.com/user/zixuanlimit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi r/LocalLLaMA&lt;/p&gt; &lt;p&gt;Today we are having &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt;, the research lab behind the GLM 4.7. We‚Äôre excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Yuxuan Zhang, &lt;a href="/u/YuxuanZhangzR"&gt;u/YuxuanZhangzR&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Qinkai Zheng, &lt;a href="/u/QinkaiZheng"&gt;u/QinkaiZheng&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Aohan Zeng, &lt;a href="/u/Sengxian"&gt;u/Sengxian&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Zhenyu Hou, &lt;a href="/u/ZhenyuHou"&gt;u/ZhenyuHou&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Xin Lv, &lt;a href="/u/davidlvxin"&gt;u/davidlvxin&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The AMA will run from 8 AM ‚Äì 11 AM PST, with the &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt; team continuing to follow up on questions over the next 48 hours.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zixuanlimit"&gt; /u/zixuanlimit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T16:04:23+00:00</published>
  </entry>
</feed>
