<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-23T19:48:49+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1p46mkt</id>
    <title>Qwen 2.5 vl 72b is the new SOTA model on SpatialBench, beating Gemini 3 pro. A new benchmark to test spatial reasoning on vlms</title>
    <updated>2025-11-22T22:51:53+00:00</updated>
    <author>
      <name>/u/gbomb13</name>
      <uri>https://old.reddit.com/user/gbomb13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p46mkt/qwen_25_vl_72b_is_the_new_sota_model_on/"&gt; &lt;img alt="Qwen 2.5 vl 72b is the new SOTA model on SpatialBench, beating Gemini 3 pro. A new benchmark to test spatial reasoning on vlms" src="https://b.thumbs.redditmedia.com/gkUZjCWCpd_R9pbF5IZULC7D5HjBa1BJsXWaY241hpQ.jpg" title="Qwen 2.5 vl 72b is the new SOTA model on SpatialBench, beating Gemini 3 pro. A new benchmark to test spatial reasoning on vlms" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We looked over its answers, the questions it got correct were the easiest ones but impressive nonetheless compared to other models. &lt;a href="https://spicylemonade.github.io/spatialbench/"&gt;https://spicylemonade.github.io/spatialbench/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gbomb13"&gt; /u/gbomb13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1p46mkt"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p46mkt/qwen_25_vl_72b_is_the_new_sota_model_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p46mkt/qwen_25_vl_72b_is_the_new_sota_model_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-22T22:51:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4ubf0</id>
    <title>Z.AI: GLM 4.6 on Mac Studio 256GB for agentic coding?</title>
    <updated>2025-11-23T18:39:39+00:00</updated>
    <author>
      <name>/u/ThingRexCom</name>
      <uri>https://old.reddit.com/user/ThingRexCom</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I would like to use the Z.AI: GLM 4.6 for agentic coding.&lt;/p&gt; &lt;p&gt;Would it work on a Mac Studio with 256GB RAM?&lt;/p&gt; &lt;p&gt;What performance can I expect?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThingRexCom"&gt; /u/ThingRexCom &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4ubf0/zai_glm_46_on_mac_studio_256gb_for_agentic_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4ubf0/zai_glm_46_on_mac_studio_256gb_for_agentic_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p4ubf0/zai_glm_46_on_mac_studio_256gb_for_agentic_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T18:39:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4nip2</id>
    <title>Best method to create datasets for fine tuning?</title>
    <updated>2025-11-23T14:04:58+00:00</updated>
    <author>
      <name>/u/Adventurous-Gold6413</name>
      <uri>https://old.reddit.com/user/Adventurous-Gold6413</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Let‚Äôs say I have a bunch of txt files about a certain knowledge base/ character info/ or whatever.&lt;/p&gt; &lt;p&gt;How could I convert it into a dataset format?(for unsloth as an example)&lt;/p&gt; &lt;p&gt;Is there some preferably local project or software to do that?&lt;/p&gt; &lt;p&gt;Thanks in advance &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Adventurous-Gold6413"&gt; /u/Adventurous-Gold6413 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4nip2/best_method_to_create_datasets_for_fine_tuning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4nip2/best_method_to_create_datasets_for_fine_tuning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p4nip2/best_method_to_create_datasets_for_fine_tuning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T14:04:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4mv67</id>
    <title>Hephaestus Dev: 5 ready-to-use AI workflows for software development (PRD‚ÜíCode, Bug Fix, Feature Dev, and more)</title>
    <updated>2025-11-23T13:35:03+00:00</updated>
    <author>
      <name>/u/Standard_Excuse7988</name>
      <uri>https://old.reddit.com/user/Standard_Excuse7988</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4mv67/hephaestus_dev_5_readytouse_ai_workflows_for/"&gt; &lt;img alt="Hephaestus Dev: 5 ready-to-use AI workflows for software development (PRD‚ÜíCode, Bug Fix, Feature Dev, and more)" src="https://external-preview.redd.it/YWoxaHZnaHVlMDNnMZc87bODxInUab1QdzAVIKt_p_AEHL5YkJEhQTcdw4CD.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c93846d9323b801571f14e02d673fce52428d85d" title="Hephaestus Dev: 5 ready-to-use AI workflows for software development (PRD‚ÜíCode, Bug Fix, Feature Dev, and more)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! üëã&lt;/p&gt; &lt;p&gt;Quick update on Hephaestus - the open-source framework where AI agents dynamically build workflows based on what they discover.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;For those new here:&lt;/strong&gt; Hephaestus is a &amp;quot;semi-structured&amp;quot; agentic framework. Instead of predefining every task, you define phase types (like &amp;quot;Analyze ‚Üí Implement ‚Üí Test&amp;quot;), and agents create specific tasks across these phases based on what they actually discover. A testing agent finds a bug? It spawns a fix task. Discovers an optimization opportunity? It spawns an investigation task. The workflow builds itself.&lt;/p&gt; &lt;p&gt;Also - everything in Hephaestus can use Open source models! I personally set my coding agents to use &lt;code&gt;GLM-4.6&lt;/code&gt; and the Hephaestus Engine with &lt;code&gt;gpt-oss:120b&lt;/code&gt;&lt;/p&gt; &lt;h2&gt;What's New: Hephaestus Dev&lt;/h2&gt; &lt;p&gt;I've packaged Hephaestus into a ready-to-use development tool with 5 pre-built workflows:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Workflow&lt;/th&gt; &lt;th&gt;What it does&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;PRD to Software Builder&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Give it a Product Requirements Document, get working software&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Bug Fix&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Describe a bug ‚Üí agents reproduce, fix, and verify it&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Index Repository&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Scans your codebase and builds knowledge in memory&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Feature Development&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Add features following your existing code patterns&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Documentation Generation&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Generate comprehensive docs for your codebase&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;One command to start:&lt;/strong&gt; python run_hephaestus_dev.py --path /path/to/project&lt;/p&gt; &lt;p&gt;Then open http://localhost:3000, pick a workflow, fill in a form, and launch. Agents work in parallel, create tickets on a Kanban board, and coordinate through shared memory.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Pro tip:&lt;/strong&gt; Run &amp;quot;Index Repository&amp;quot; first on any existing codebase. It builds semantic knowledge that all other workflows can leverage - agents get rich context about your code's structure, patterns, and conventions.&lt;/p&gt; &lt;h2&gt;What's under the hood:&lt;/h2&gt; &lt;p&gt;üîÑ &lt;strong&gt;Multi-workflow execution&lt;/strong&gt; - Run different workflows, each isolated with its own phases and tickets&lt;/p&gt; &lt;p&gt;üöÄ &lt;strong&gt;Launch templates&lt;/strong&gt; - Customizable forms for each workflow type&lt;/p&gt; &lt;p&gt;üß† &lt;strong&gt;RAG-powered coordination&lt;/strong&gt; - Agents share discoveries through Qdrant vector memory&lt;/p&gt; &lt;p&gt;üéØ &lt;strong&gt;Guardian monitoring&lt;/strong&gt; - Tracks agent trajectories to prevent drift&lt;/p&gt; &lt;p&gt;üìä &lt;strong&gt;Real-time Kanban&lt;/strong&gt; - Watch tickets move from Backlog ‚Üí In Progress ‚Üí Done&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;üîó &lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/Ido-Levi/Hephaestus"&gt;https://github.com/Ido-Levi/Hephaestus&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üìö &lt;strong&gt;Docs:&lt;/strong&gt; &lt;a href="https://ido-levi.github.io/Hephaestus/"&gt;https://ido-levi.github.io/Hephaestus/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üõ†Ô∏è &lt;strong&gt;Hephaestus Dev Guide:&lt;/strong&gt; &lt;a href="https://ido-levi.github.io/Hephaestus/docs/getting-started/hephaestus-dev"&gt;https://ido-levi.github.io/Hephaestus/docs/getting-started/hephaestus-dev&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Still rough around the edges - feedback and issues are welcome! Happy to review contributions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Standard_Excuse7988"&gt; /u/Standard_Excuse7988 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/fi44gghue03g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4mv67/hephaestus_dev_5_readytouse_ai_workflows_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p4mv67/hephaestus_dev_5_readytouse_ai_workflows_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T13:35:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1p3xqsu</id>
    <title>Qwen-image-edit-2511 coming next week</title>
    <updated>2025-11-22T16:41:15+00:00</updated>
    <author>
      <name>/u/abdouhlili</name>
      <uri>https://old.reddit.com/user/abdouhlili</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3xqsu/qwenimageedit2511_coming_next_week/"&gt; &lt;img alt="Qwen-image-edit-2511 coming next week" src="https://preview.redd.it/yeofdp077u2g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ef09997e54c4c481e545ec2dd4183f65163c8a73" title="Qwen-image-edit-2511 coming next week" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdouhlili"&gt; /u/abdouhlili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/yeofdp077u2g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3xqsu/qwenimageedit2511_coming_next_week/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p3xqsu/qwenimageedit2511_coming_next_week/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-22T16:41:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4t15y</id>
    <title>Turned my spare PC into a Local LLaMa box. Need tips for practical use</title>
    <updated>2025-11-23T17:49:21+00:00</updated>
    <author>
      <name>/u/Future_Draw5416</name>
      <uri>https://old.reddit.com/user/Future_Draw5416</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I converted an old PC into a machine dedicated to running local LLMs. It surprised me how well it performs for simple tasks. I want to apply it to real-life scenarios like note taking, automation or personal knowledge management. &lt;/p&gt; &lt;p&gt;What practical use cases do you rely on your local model for? Hoping to pick up ideas that go beyond basic chat. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Future_Draw5416"&gt; /u/Future_Draw5416 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4t15y/turned_my_spare_pc_into_a_local_llama_box_need/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4t15y/turned_my_spare_pc_into_a_local_llama_box_need/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p4t15y/turned_my_spare_pc_into_a_local_llama_box_need/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T17:49:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4jfb4</id>
    <title>ComfyUI Raylight Parallelism Benchmark, 5090 vs Dual 2000 Ada (4060 Ti-ish). Also I enable CFG Parallel, so SDXL and SD1.5 can be parallelized.</title>
    <updated>2025-11-23T10:24:23+00:00</updated>
    <author>
      <name>/u/Altruistic_Heat_9531</name>
      <uri>https://old.reddit.com/user/Altruistic_Heat_9531</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4jfb4/comfyui_raylight_parallelism_benchmark_5090_vs/"&gt; &lt;img alt="ComfyUI Raylight Parallelism Benchmark, 5090 vs Dual 2000 Ada (4060 Ti-ish). Also I enable CFG Parallel, so SDXL and SD1.5 can be parallelized." src="https://preview.redd.it/9z32gcsngz2g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e9b23ff50d19900019d39d7d051f985c46fa61d9" title="ComfyUI Raylight Parallelism Benchmark, 5090 vs Dual 2000 Ada (4060 Ti-ish). Also I enable CFG Parallel, so SDXL and SD1.5 can be parallelized." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Someone asked about 5090 vs dual 5070/5060 16GB perf benchmark for Raylight, so here it is.&lt;/p&gt; &lt;p&gt;Take it with a grain of salt ofc.&lt;br /&gt; &lt;strong&gt;TLDR&lt;/strong&gt;: 5090 had, is, and will demolish dual 4060Ti. That is as true as asking if the sky is blue. But again, my project is for people who can buy a second 4060Ti, not necessarily for people buying a 5090 or 4090.&lt;/p&gt; &lt;p&gt;Runs purely on RunPod. Anyway have a nice day.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/komikndr/raylight/tree/main"&gt;https://github.com/komikndr/raylight/tree/main&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Altruistic_Heat_9531"&gt; /u/Altruistic_Heat_9531 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9z32gcsngz2g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4jfb4/comfyui_raylight_parallelism_benchmark_5090_vs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p4jfb4/comfyui_raylight_parallelism_benchmark_5090_vs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T10:24:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4jvvw</id>
    <title>What's the fastest OCR model / solution for a production grade pipeline ingesting 4M pages per month?</title>
    <updated>2025-11-23T10:52:48+00:00</updated>
    <author>
      <name>/u/DistinctAir8716</name>
      <uri>https://old.reddit.com/user/DistinctAir8716</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We are running an app serving 500k users, where we ingest pdf documents from users, and we have to turn them into markdown format for LLM integration.&lt;/p&gt; &lt;p&gt;Currently, we're using an OCR service that meets our needs, but it doesn't produce the highest quality results.&lt;/p&gt; &lt;p&gt;We want to switch to a VLLM like Deepseek-OCR, LightonOCR, dots.ocr, olmOCR etc.&lt;/p&gt; &lt;p&gt;The only problem is that when we go out and test these models, they're all too slow, with the best one, LightonOCR, peaking at 600 tok/s in generation.&lt;/p&gt; &lt;p&gt;We need a solution that can (e.g.) turn a 40-page PDF into markdown in ideally less than 20 seconds, while costing less than $0.10 per thousand pages.&lt;/p&gt; &lt;p&gt;We have been bashing out head on this problem for well over a month testing various models, is the route of switching to a VLLM worth it?&lt;/p&gt; &lt;p&gt;If not, what are some good alternatives or gaps we're not seeing? What would be the best way to approach this problem?&lt;/p&gt; &lt;p&gt;EDIT:&lt;/p&gt; &lt;p&gt;I have managed to host Deepseek-OCR on a A100 gpu server, and while running inference via vllm on a local pdf I get speeds of around 3000 tok/s (awesome!). The only problem is when I try to serve the model via an API with vllm serve the speed plunges to 50 tok/s. What would be the best way to host it while retaining inference speed?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DistinctAir8716"&gt; /u/DistinctAir8716 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4jvvw/whats_the_fastest_ocr_model_solution_for_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4jvvw/whats_the_fastest_ocr_model_solution_for_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p4jvvw/whats_the_fastest_ocr_model_solution_for_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T10:52:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1p48d7f</id>
    <title>Strix Halo, Debian 13@6.16.12&amp;6.17.8, Qwen3Coder-Q8 CTX&lt;=131k, llama.cpp@Vulkan&amp;ROCm, Power &amp; Efficiency</title>
    <updated>2025-11-23T00:10:33+00:00</updated>
    <author>
      <name>/u/Educational_Sun_8813</name>
      <uri>https://old.reddit.com/user/Educational_Sun_8813</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p48d7f/strix_halo_debian_13616126178_qwen3coderq8/"&gt; &lt;img alt="Strix Halo, Debian 13@6.16.12&amp;amp;6.17.8, Qwen3Coder-Q8 CTX&amp;lt;=131k, llama.cpp@Vulkan&amp;amp;ROCm, Power &amp;amp; Efficiency" src="https://preview.redd.it/hg69ko66fw2g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2fb55f92f9b2aa5fc700b98933b28eb18462d618" title="Strix Halo, Debian 13@6.16.12&amp;amp;6.17.8, Qwen3Coder-Q8 CTX&amp;lt;=131k, llama.cpp@Vulkan&amp;amp;ROCm, Power &amp;amp; Efficiency" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, i wanted to check kernel improvement in support of strix halo under Debian GNU/Linux, while latest minor versions of 6.16.x improved GTT wanted to check if can be even better. So i tested it on Debian 13 with latest kernel from testing &lt;code&gt;6.16.12+deb14+1-amd64&lt;/code&gt;, and one precompiled performance optimized kernel &lt;code&gt;6.17.8-x64v3-xanmod1&lt;/code&gt;. I ran tests agains &lt;code&gt;Qwen3-Coder-Q8&lt;/code&gt; in full context, but i did benchmark up to &lt;code&gt;131k&lt;/code&gt;. Llama.cpp versions i used for tests: &lt;code&gt;Vulkan build: 5be353ec4 (7109)&lt;/code&gt; and &lt;code&gt;ROCm TheROCK precompiled build: 416e7c7 (1)&lt;/code&gt;. Side notice i managed to compile finally llama.cpp with external libs from AMD for HIP support, so from now one i will use same build for Vulkan and ROCM. Since i wanted also to find sweet spot in energy efficiency so i tried to capture also power usage, and compare it with computing performance. So in the end i tested that model with two backends, and kernels, changing context in few steps, to find out.&lt;/p&gt; &lt;p&gt;In the end seems that latest kernel from testing &lt;code&gt;6.16.12&lt;/code&gt; works just great! Performance kernel speed is maybe fraction better (max 2%). Besides stock kernel had 4W in idle (in &lt;code&gt;balanced&lt;/code&gt; mode), while performance kernel had always minimum 9-10W. And i use fans with 0RPM &amp;lt;= PWM 5% so it's completly silent when idle. And audible under heavy load especially with ROCm. Anyway most optimal power setting for computations is &lt;code&gt;latency-performance&lt;/code&gt; and it's not worth to use &lt;code&gt;accelerator-performance&lt;/code&gt; in the long run.&lt;/p&gt; &lt;p&gt;Here just notice for strix halo Debian users (and other distros probably too, but current Arch and Fedora have newer kernel), you need to use at least &lt;code&gt;6.16.x&lt;/code&gt; to have better experience with that platform. For Debian GNU/Linux easiest way is to install newer kernel from backports, or move to testing for the latest one. I just noticed that with &lt;code&gt;apt update&lt;/code&gt; just now that there is &lt;code&gt;6.16.12&lt;/code&gt; in stable, so it's great nothing to for Debian users. :) And testing moved to &lt;code&gt;6.17.8+deb14-amd64&lt;/code&gt; so great, anyway i will have now that kernel, so will test it soon again from debian branch. haha, what an irony, but it took me quite time to write it down. So update: and just tested &lt;code&gt;6.17.8+deb14-amd64&lt;/code&gt; and idle now is 6W in balance mode now, bit more, than before, but less than the custom kernel.&lt;/p&gt; &lt;p&gt;Performance wise Vulkan is faster in TG, while significantly slower in PP especially with long context. On the other hand ROCm is much faster in PP, and bit slower in TG, but overal improvement in PP is so big that it does not matter for long context (it's around &lt;strong&gt;x2.7 faster&lt;/strong&gt; in 131k CTX window). Vulkan is very fast for shorter chats, but over 32k CTX it's getting much slower. Under load (tested with &lt;code&gt;accelerator-performance&lt;/code&gt; profile in &lt;code&gt;tuned&lt;/code&gt;) ROCm can draw around &lt;code&gt;120W&lt;/code&gt; (this backend use also more CPU for PP), while Vulkan peak was around &lt;code&gt;70W&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;I found that best values for &lt;code&gt;-ub&lt;/code&gt; batch size is &lt;code&gt;512&lt;/code&gt;(it's default) for Vulkan, but &lt;code&gt;2048&lt;/code&gt; for ROCm (it's &lt;strong&gt;faster ~16%&lt;/strong&gt; than default). After that you have to increase &lt;code&gt;-b&lt;/code&gt; logical batch size to &lt;code&gt;8192&lt;/code&gt; for best performance with ROCm. For Vulkan just leave default logical batch size.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;BONUS section&lt;/strong&gt;, agent test: After tests i wanted to check &lt;code&gt;Qwen3-coder-Q8&lt;/code&gt; model in some tooling so i tried to install &lt;code&gt;kubectl-ai&lt;/code&gt;, and connect it to my local llama-server, and perform some tasks on local kubernetes (4 nodes). Model was able based on the natural language promp install Jupyter hub from helm charts, using ~50k tokens for that. And one could run notebooks in some 8-10 minutes. That model works really good on strix halo, worth to check if you didn't yet.&lt;/p&gt; &lt;p&gt;I hope someone will find it valuable, and diagram clear enough. :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Educational_Sun_8813"&gt; /u/Educational_Sun_8813 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hg69ko66fw2g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p48d7f/strix_halo_debian_13616126178_qwen3coderq8/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p48d7f/strix_halo_debian_13616126178_qwen3coderq8/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T00:10:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4h88k</id>
    <title>A neat CLI frontend for live AI dialogue!</title>
    <updated>2025-11-23T08:05:08+00:00</updated>
    <author>
      <name>/u/Rektile142</name>
      <uri>https://old.reddit.com/user/Rektile142</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4h88k/a_neat_cli_frontend_for_live_ai_dialogue/"&gt; &lt;img alt="A neat CLI frontend for live AI dialogue!" src="https://preview.redd.it/k8r9p2j5ly2g1.gif?width=640&amp;amp;crop=smart&amp;amp;s=4f57e7f993c90e823af42c5ab0abd97eb34ca02d" title="A neat CLI frontend for live AI dialogue!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Version 1.0.0 of &lt;strong&gt;Local Sage&lt;/strong&gt;, a dialogue-oriented CLI frontend for AI chat, has launched! &lt;/p&gt; &lt;p&gt;It's aimed at local inference (llama.cpp, ollama, vLLM, etc.) and hooks into any OpenAI API endpoint.&lt;/p&gt; &lt;p&gt;It's got some fun stuff!&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Conversations live in your shell&lt;/strong&gt;, rendering directly to standard output.&lt;/li&gt; &lt;li&gt;Fancy prompts with &lt;strong&gt;command completion&lt;/strong&gt; and &lt;strong&gt;in-memory history&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Context-aware file management&lt;/strong&gt;: attach, remove, and replace text-based files.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Session management&lt;/strong&gt;: load, save, delete, reset, and summarize sessions.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Profile management&lt;/strong&gt;: save, delete, and switch model profiles.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Repo is live here: &lt;a href="https://github.com/Kyleg142/localsage"&gt;https://github.com/Kyleg142/localsage&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You can install Local Sage with &lt;strong&gt;uv&lt;/strong&gt; to give it a spin: &lt;code&gt;uv tool install localsage&lt;/code&gt;&lt;/p&gt; &lt;p&gt;The project is MIT open-source as well! Please let me know what you guys think!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rektile142"&gt; /u/Rektile142 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/k8r9p2j5ly2g1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4h88k/a_neat_cli_frontend_for_live_ai_dialogue/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p4h88k/a_neat_cli_frontend_for_live_ai_dialogue/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T08:05:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4pre3</id>
    <title>Experimenting with Multiple LLMs at once?</title>
    <updated>2025-11-23T15:40:27+00:00</updated>
    <author>
      <name>/u/acornPersonal</name>
      <uri>https://old.reddit.com/user/acornPersonal</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been going mad scientist mode lately working on having more than one LLM functioning at a time. Has anyone else experimented like this? I'm sure someone has and I know that they've done some research in MIT about it, but I was curious to know if anyone has had some fun with it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/acornPersonal"&gt; /u/acornPersonal &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4pre3/experimenting_with_multiple_llms_at_once/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4pre3/experimenting_with_multiple_llms_at_once/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p4pre3/experimenting_with_multiple_llms_at_once/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T15:40:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4k9is</id>
    <title>LLMSnap - fast model swapping for vLLM using sleep mode</title>
    <updated>2025-11-23T11:15:48+00:00</updated>
    <author>
      <name>/u/Camvizioneer</name>
      <uri>https://old.reddit.com/user/Camvizioneer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When I saw the release of vLLM sleep mode providing second-ish swap times, I was very intrigued - it was exactly what I needed. Previous non-sleep vLLM model swapping was unusable for frequent model swaps, with startup times around 1 minute each.&lt;/p&gt; &lt;p&gt;I started looking for an existing lightweight model router with vLLM sleep mode support but couldn't find any. I found what seemed like a perfect project to add this functionality - llama-swap. I implemented vLLM sleep support and opened a PR, but it was closed with the reasoning that most llama-swap users use llama.cpp and don't need this feature. That's how &lt;a href="https://github.com/napmany/llmsnap"&gt;llmsnap&lt;/a&gt;, a fork of llama-swap, was born! :)&lt;/p&gt; &lt;p&gt;I'm going to continue working on llmsnap with a focus on making LLM model swapping faster and more resource-effective, without limiting or tight coupling to any one inference server - even though only vLLM took its spot in the title for now :)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/napmany/llmsnap"&gt;https://github.com/napmany/llmsnap&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You can install and use it with brew, docker, release binaries, or from source.&lt;/p&gt; &lt;p&gt;Questions and feedback are very welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Camvizioneer"&gt; /u/Camvizioneer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4k9is/llmsnap_fast_model_swapping_for_vllm_using_sleep/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4k9is/llmsnap_fast_model_swapping_for_vllm_using_sleep/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p4k9is/llmsnap_fast_model_swapping_for_vllm_using_sleep/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T11:15:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4o9lt</id>
    <title>Olmo 3 from scratch</title>
    <updated>2025-11-23T14:38:20+00:00</updated>
    <author>
      <name>/u/seraschka</name>
      <uri>https://old.reddit.com/user/seraschka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4o9lt/olmo_3_from_scratch/"&gt; &lt;img alt="Olmo 3 from scratch" src="https://b.thumbs.redditmedia.com/6yYfJoLy6qtGGUYvc12cn1_9w9dgwwAi7lOyBfzvTxs.jpg" title="Olmo 3 from scratch" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Lots of interesting LLM releases last week. My favorite was actually the Olmo 3 release. (I love the Olmo series because there's always so much useful info in their technical reports.)&lt;/p&gt; &lt;p&gt;I coded the Olmo 3 architecture in a standalone notebook here if you are interested: &lt;a href="https://github.com/rasbt/LLMs-from-scratch/blob/main/ch05/13_olmo3/standalone-olmo3.ipynb"&gt;https://github.com/rasbt/LLMs-from-scratch/blob/main/ch05/13_olmo3/standalone-olmo3.ipynb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And here's the side-by-side architecture comparison with Qwen3: &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/pmeozowxp03g1.jpg?width=5000&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=6a5d3528d32dc999681af9017b3dc00613606b34"&gt;https://preview.redd.it/pmeozowxp03g1.jpg?width=5000&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=6a5d3528d32dc999681af9017b3dc00613606b34&lt;/a&gt;&lt;/p&gt; &lt;p&gt;1) As we can see, the Olmo 3 architecture is relatively similar to Qwen3. However, it's worth noting that this is essentially likely inspired by the Olmo 2 predecessor, not Qwen3. &lt;/p&gt; &lt;p&gt;2) Similar to Olmo 2, Olmo 3 still uses a post-norm flavor instead of pre-norm, as they found in the Olmo 2 paper that it stabilizes the training. &lt;/p&gt; &lt;p&gt;3) Interestingly, the 7B model still uses multi-head attention similar to Olmo 2.&lt;br /&gt; However, to make things more efficient and reduce the KV cache size, they now use sliding-window attention (e.g., similar to Gemma 3). &lt;/p&gt; &lt;p&gt;Next, the 32B model (the figure is not shown here due to space reasons, but you can find it in my &lt;a href="https://magazine.sebastianraschka.com/p/the-big-llm-architecture-comparison"&gt;The Big LLM Architecture Comparison&lt;/a&gt; article or my &lt;a href="https://github.com/rasbt/LLMs-from-scratch/blob/main/ch05/13_olmo3/standalone-olmo3.ipynb"&gt;Olmo 3 from-scratch notebook&lt;/a&gt;): &lt;/p&gt; &lt;p&gt;4) Overall, it's the same architecture but just scaled up. Also, the proportions (e.g., going from the input to the intermediate size in the feed-forward layer, and so on) roughly match the ones in Qwen3. &lt;/p&gt; &lt;p&gt;5) My guess is the architecture was initially somewhat smaller than Qwen3 due to the smaller vocabulary, and they then scaled up the intermediate size expansion from 5x in Qwen3 to 5.4 in Olmo 3 to have a 32B model for a direct comparison. &lt;/p&gt; &lt;p&gt;6) Also, note that the 32B model (finally!) uses grouped query attention. &lt;/p&gt; &lt;p&gt;And yes, I also did a from-scratch implementation. It was still a lot of work, but since I had already implemented Qwen3 from scratch, as well as Gemma 3 (for the sliding-window attention component), it wasn't too bad!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/seraschka"&gt; /u/seraschka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4o9lt/olmo_3_from_scratch/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4o9lt/olmo_3_from_scratch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p4o9lt/olmo_3_from_scratch/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T14:38:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4lovv</id>
    <title>Qwen3-VL Computer Using Agent works extremely well</title>
    <updated>2025-11-23T12:36:50+00:00</updated>
    <author>
      <name>/u/Money-Coast-3905</name>
      <uri>https://old.reddit.com/user/Money-Coast-3905</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4lovv/qwen3vl_computer_using_agent_works_extremely_well/"&gt; &lt;img alt="Qwen3-VL Computer Using Agent works extremely well" src="https://external-preview.redd.it/Sq30vujKBtYvLAsqvjch2NpcJnBTRk8SnxTPeB4cRLU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1199d59ca5d3099ce2eabd236c89b5eaa3bb0080" title="Qwen3-VL Computer Using Agent works extremely well" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://i.redd.it/h4ici013403g1.gif"&gt;https://i.redd.it/h4ici013403g1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hey all,&lt;/p&gt; &lt;p&gt;I‚Äôve been using &lt;strong&gt;Qwen3-VL&lt;/strong&gt; as a real &lt;em&gt;computer-using&lt;/em&gt; agent ‚Äì it moves the mouse, clicks, types, scrolls, and reads the screen from screenshots, pretty much like a human.&lt;/p&gt; &lt;p&gt;I open-sourced a tiny driver that exposes a &lt;code&gt;computer_use&lt;/code&gt; tool over an OpenAI-compatible API and uses &lt;code&gt;pyautogui&lt;/code&gt; to control the desktop. The GIF shows it &lt;strong&gt;resolving a GitHub issue end-to-end fully autonomously&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Repo (code + minimal loop):&lt;br /&gt; üëâ &lt;a href="https://github.com/SeungyounShin/qwen3_computer_use"&gt;https://github.com/SeungyounShin/qwen3_computer_use&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Next I‚Äôm planning to try &lt;strong&gt;RL tuning&lt;/strong&gt; on top of this Would love feedback or ideas‚Äîhappy to discuss in the comments or DMs.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Money-Coast-3905"&gt; /u/Money-Coast-3905 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4lovv/qwen3vl_computer_using_agent_works_extremely_well/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4lovv/qwen3vl_computer_using_agent_works_extremely_well/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p4lovv/qwen3vl_computer_using_agent_works_extremely_well/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T12:36:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4hkaf</id>
    <title>Qwen3-2B-VL for OCR is actually insane. Dockerized Set Up + GitHub</title>
    <updated>2025-11-23T08:26:46+00:00</updated>
    <author>
      <name>/u/exaknight21</name>
      <uri>https://old.reddit.com/user/exaknight21</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been trying to find an efficient model to perform OCR for my use case for a while. I created &lt;a href="https://github.com/ikantkode/exaOCR"&gt;exaOCR&lt;/a&gt; - and when I pushed the code, I can swear on all that is holy that it was working. BUT, for some reason, I simply cannot fix it anymore. It uses OCRMyPDF and the error is literally unsolvable by any models (ChatGPT, DeepSeek, Claude, Grok) and I threw in the towel until I guess I can make enough friends that are actual coders. (If you are able to contribute, please do.)&lt;/p&gt; &lt;p&gt;My entire purpose in using AI to create these crappy streamlit apps is to test the usability for my use case and then essentially go from there. As such, I could never get DeepSeek OCR to work, but someone posted about their project (ocrarena.ai) and I was able to try the models. Not very impressed + the general chatter around it.&lt;/p&gt; &lt;p&gt;I am a huge fan of the Qwen Team and not because they publish everything Open Source, but the fact that they are working towards an efficient AI model that *some* of us peasants can run.&lt;/p&gt; &lt;p&gt;Brings me to the main point. I got a T5610 for $239, I had a 3060 12 GB laying around and I got another for $280 also 12 GB, I threw them both together and they are able to help me experiment. The Qwen3-2B-VL for OCR is actually insane... I mean, deploy it and look for yourself. Just a heads up, my friend tried it on his 10 GB 3080, and vLLM threw an error, you will want to reduce the **--max-model-len from 16384 to probably 8000 **. Remember, I am using dual 3060s giving me more VRAM to play with.&lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/ikantkode/qwen3-2b-ocr-app"&gt;https://github.com/ikantkode/qwen3-2b-ocr-app&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In any event, here is a short video of it working: &lt;a href="https://youtu.be/anjhfOc7RqA"&gt;https://youtu.be/anjhfOc7RqA&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/exaknight21"&gt; /u/exaknight21 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4hkaf/qwen32bvl_for_ocr_is_actually_insane_dockerized/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4hkaf/qwen32bvl_for_ocr_is_actually_insane_dockerized/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p4hkaf/qwen32bvl_for_ocr_is_actually_insane_dockerized/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T08:26:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4ijay</id>
    <title>Making an offline STS (speech to speech) AI that runs under 2GB RAM. But do people even need offline AI now?</title>
    <updated>2025-11-23T09:29:08+00:00</updated>
    <author>
      <name>/u/Automatic_Finish8598</name>
      <uri>https://old.reddit.com/user/Automatic_Finish8598</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm building a full speech to speech AI that runs totally offline. Everything stays on the device. STT, LLM inference and TTS all running locally in under 2GB RAM. I already have most of the architecture working and a basic MVP.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The part I‚Äôm thinking a lot about is the bigger question. With models like Gemini, ChatGPT and Llama becoming cheaper and extremely accessible, why would anyone still want to use something fully offline?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;My reason is simple. I want an AI that can work completely on personal or sensitive data without sending anything outside. Something you can use in hospitals, rural government centers, developer setups, early startups, labs, or places where internet isn‚Äôt stable or cloud isn‚Äôt allowed. Basically an AI you own fully, with no external calls.&lt;/p&gt; &lt;p&gt;My idea is to make a proper offline autonomous assistant that behaves like a personal AI layer. It should handle voice, do local reasoning, search your files, automate stuff, summarize documents, all of that, without depending on the internet or any external service.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;I‚Äôm curious what others think about this direction. Is offline AI still valuable when cloud AI is getting so cheap? Are there use cases I‚Äôm not thinking about or is this something only a niche group will ever care about?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Would love to hear your thoughts.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Automatic_Finish8598"&gt; /u/Automatic_Finish8598 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4ijay/making_an_offline_sts_speech_to_speech_ai_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4ijay/making_an_offline_sts_speech_to_speech_ai_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p4ijay/making_an_offline_sts_speech_to_speech_ai_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T09:29:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4r0ol</id>
    <title>I built an air-gapped AI Security Analyst (Dolphin + Vector DB) on a 1TB SSD because I don't trust the cloud. Here is the demo</title>
    <updated>2025-11-23T16:30:05+00:00</updated>
    <author>
      <name>/u/Glass-Ant-6041</name>
      <uri>https://old.reddit.com/user/Glass-Ant-6041</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4r0ol/i_built_an_airgapped_ai_security_analyst_dolphin/"&gt; &lt;img alt="I built an air-gapped AI Security Analyst (Dolphin + Vector DB) on a 1TB SSD because I don't trust the cloud. Here is the demo" src="https://external-preview.redd.it/bHBhb2J3bDJhMTNnMe_d2tHulvaS54ITJ5YIpl3vKGq8IwT_QpQcAhaljqVu.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=44cfc0fb49b1efd73232aa38ace208b1d2c96406" title="I built an air-gapped AI Security Analyst (Dolphin + Vector DB) on a 1TB SSD because I don't trust the cloud. Here is the demo" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Glass-Ant-6041"&gt; /u/Glass-Ant-6041 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/wfgc0yl2a13g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4r0ol/i_built_an_airgapped_ai_security_analyst_dolphin/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p4r0ol/i_built_an_airgapped_ai_security_analyst_dolphin/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T16:30:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4t5ix</id>
    <title>I created a llama.cpp fork with the Rockchip NPU integration as an accelerator and the results are already looking great!</title>
    <updated>2025-11-23T17:54:06+00:00</updated>
    <author>
      <name>/u/Inv1si</name>
      <uri>https://old.reddit.com/user/Inv1si</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4t5ix/i_created_a_llamacpp_fork_with_the_rockchip_npu/"&gt; &lt;img alt="I created a llama.cpp fork with the Rockchip NPU integration as an accelerator and the results are already looking great!" src="https://external-preview.redd.it/YWZjazBjYjBwMTNnMfl_KE3bRLUmxUrgo6sq7iH5IJtc0qUYB-cQv58tKBaC.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=59e7a57c308ef96e98b98c0b7dd565ffc1a5aa0c" title="I created a llama.cpp fork with the Rockchip NPU integration as an accelerator and the results are already looking great!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Inv1si"&gt; /u/Inv1si &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9r0ixbb0p13g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4t5ix/i_created_a_llamacpp_fork_with_the_rockchip_npu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p4t5ix/i_created_a_llamacpp_fork_with_the_rockchip_npu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T17:54:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4s7nt</id>
    <title>Drummer's Snowpiercer 15B v4 ¬∑ A strong RP model that punches a pack!</title>
    <updated>2025-11-23T17:17:31+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4s7nt/drummers_snowpiercer_15b_v4_a_strong_rp_model/"&gt; &lt;img alt="Drummer's Snowpiercer 15B v4 ¬∑ A strong RP model that punches a pack!" src="https://external-preview.redd.it/QHcTMS_GK1SpPCsYVSA_d521aSr77tuQOduExaTV8io.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=260671c6b499b9df6965e93453782a12c31d98d9" title="Drummer's Snowpiercer 15B v4 ¬∑ A strong RP model that punches a pack!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;While I have your attention, I'd like to ask: Does anyone here honestly bother with models below 12B? Like 8B, 4B, or 2B? I feel like I might have neglected smaller model sizes for far too long.&lt;/p&gt; &lt;p&gt;Also: &amp;quot;Air 4.6 in two weeks!&amp;quot;&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;Snowpiercer v4 is part of the Gen 4.0 series I'm working on that puts more focus on character adherence. YMMV. You might want to check out Gen 3.5/3.0 if Gen 4.0 isn't doing it for you.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/spaces/TheDrummer/directory"&gt;https://huggingface.co/spaces/TheDrummer/directory&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Snowpiercer-15B-v4"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4s7nt/drummers_snowpiercer_15b_v4_a_strong_rp_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p4s7nt/drummers_snowpiercer_15b_v4_a_strong_rp_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T17:17:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4ftd5</id>
    <title>Physical documentation for LLMs in Shenzhen bookstore selling guides for DeepSeek, Doubao, Kimi, and ChatGPT.</title>
    <updated>2025-11-23T06:36:53+00:00</updated>
    <author>
      <name>/u/abdouhlili</name>
      <uri>https://old.reddit.com/user/abdouhlili</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4ftd5/physical_documentation_for_llms_in_shenzhen/"&gt; &lt;img alt="Physical documentation for LLMs in Shenzhen bookstore selling guides for DeepSeek, Doubao, Kimi, and ChatGPT." src="https://preview.redd.it/94nizo5acy2g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b2a7cc2846512c02a0c4119a338567cbcbcc8574" title="Physical documentation for LLMs in Shenzhen bookstore selling guides for DeepSeek, Doubao, Kimi, and ChatGPT." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdouhlili"&gt; /u/abdouhlili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/94nizo5acy2g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4ftd5/physical_documentation_for_llms_in_shenzhen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p4ftd5/physical_documentation_for_llms_in_shenzhen/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T06:36:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4n5v8</id>
    <title>Kimi K2 Thinking maintains 9-month gap to closed models, time-horizon up to 54min</title>
    <updated>2025-11-23T13:48:56+00:00</updated>
    <author>
      <name>/u/ObnoxiouslyVivid</name>
      <uri>https://old.reddit.com/user/ObnoxiouslyVivid</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4n5v8/kimi_k2_thinking_maintains_9month_gap_to_closed/"&gt; &lt;img alt="Kimi K2 Thinking maintains 9-month gap to closed models, time-horizon up to 54min" src="https://preview.redd.it/o6xqe1m8f03g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9e0a0844eae5636eb4bef402b990a82e9f4e8565" title="Kimi K2 Thinking maintains 9-month gap to closed models, time-horizon up to 54min" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Kimi K2 Thinking (Nov 2025) has a similar score to Sonnet 3.7 (Feb 2025) - 9 months gap.&lt;/p&gt; &lt;p&gt;The previous best was gpt-oss-120b (Aug 2025), slightly beating o1 (Dec 2024) - about 8 months.&lt;/p&gt; &lt;p&gt;Source: &lt;a href="https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/"&gt;Measuring AI Ability to Complete Long Tasks - METR&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ObnoxiouslyVivid"&gt; /u/ObnoxiouslyVivid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/o6xqe1m8f03g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4n5v8/kimi_k2_thinking_maintains_9month_gap_to_closed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p4n5v8/kimi_k2_thinking_maintains_9month_gap_to_closed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T13:48:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4ln6s</id>
    <title>Computer Manufacturer threw my $ 20000 rig down the stairs and now says everything is fine</title>
    <updated>2025-11-23T12:34:18+00:00</updated>
    <author>
      <name>/u/phwlarxoc</name>
      <uri>https://old.reddit.com/user/phwlarxoc</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I bought a custom built Threadripper Pro water-cooled dual RTX 4090 workstation from a builder and had it updated a couple of times with new hardware so that finally it became a rig worth about $20000.&lt;/p&gt; &lt;p&gt;Upon picking up the machine last week from the builder after another upgrade I asked staff that we check together the upgrade before paying and confirming the order fulfilled.&lt;/p&gt; &lt;p&gt;They lifted the machine (still in its box and secured with two styrofoam blocks), on a table, but the heavy box (30kg) slipped from their hands, the box fell on the floor and from there down a staircase where it cartwheeled several times until it stopped at the end of the stairs.&lt;/p&gt; &lt;p&gt;They sent a mail saying they checked the machine and everything is fine.&lt;/p&gt; &lt;p&gt;Who wouldn't expect otherwise.&lt;/p&gt; &lt;p&gt;Can anyone comment on possible damages such an incident can have on the electronics, PCIe Slots, GPUs, watercooling, mainboard etc, ‚Äî also on what damages might have occurred that are not immediately evident, but could e.g. impact signal quality and therefore speed? Would you accept back such a machine?&lt;/p&gt; &lt;p&gt;Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/phwlarxoc"&gt; /u/phwlarxoc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4ln6s/computer_manufacturer_threw_my_20000_rig_down_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4ln6s/computer_manufacturer_threw_my_20000_rig_down_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p4ln6s/computer_manufacturer_threw_my_20000_rig_down_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T12:34:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4jyrv</id>
    <title>No way kimi gonna release new model !!</title>
    <updated>2025-11-23T10:57:51+00:00</updated>
    <author>
      <name>/u/Independent-Wind4462</name>
      <uri>https://old.reddit.com/user/Independent-Wind4462</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4jyrv/no_way_kimi_gonna_release_new_model/"&gt; &lt;img alt="No way kimi gonna release new model !!" src="https://preview.redd.it/1ezldlbumz2g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8ad6c7d5d0f6a6e2b160c885a08a82d80d71ef81" title="No way kimi gonna release new model !!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Wind4462"&gt; /u/Independent-Wind4462 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1ezldlbumz2g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p4jyrv/no_way_kimi_gonna_release_new_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p4jyrv/no_way_kimi_gonna_release_new_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-23T10:57:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1ozopxx</id>
    <title>AMA Announcement: MiniMax, The Opensource Lab Behind MiniMax-M2 + Gifts to Our Community (Wednesday, 8AM-11AM PST)</title>
    <updated>2025-11-17T18:50:44+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozopxx/ama_announcement_minimax_the_opensource_lab/"&gt; &lt;img alt="AMA Announcement: MiniMax, The Opensource Lab Behind MiniMax-M2 + Gifts to Our Community (Wednesday, 8AM-11AM PST)" src="https://preview.redd.it/e3scgr4j5v1g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ca0bf85cb4cf2a2b7e12e8d99d515186275c15e3" title="AMA Announcement: MiniMax, The Opensource Lab Behind MiniMax-M2 + Gifts to Our Community (Wednesday, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/e3scgr4j5v1g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozopxx/ama_announcement_minimax_the_opensource_lab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ozopxx/ama_announcement_minimax_the_opensource_lab/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-17T18:50:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1b550</id>
    <title>AMA with MiniMax ‚Äî Ask Us Anything!</title>
    <updated>2025-11-19T15:46:38+00:00</updated>
    <author>
      <name>/u/OccasionNo6699</name>
      <uri>https://old.reddit.com/user/OccasionNo6699</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;! We‚Äôre really excited to be here, thanks for having us.&lt;/p&gt; &lt;p&gt;I‚Äôm Skyler (&lt;a href="https://www.reddit.com/user/OccasionNo6699/"&gt;u/OccasionNo6699&lt;/a&gt;), head of engineering at MiniMax, the lab behind:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://x.com/MiniMax__AI/status/1982674798649160175?s=20"&gt;MiniMax-M2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983382728343994414"&gt;Hailuo 2.3&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983661667872600296"&gt;MiniMax Speech 2.6&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983964920493568296"&gt;MiniMax Music 2.0&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Joining me today are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Pengyu Zhao, &lt;a href="https://www.reddit.com/user/Wise_Evidence9973/"&gt;u/Wise_Evidence9973&lt;/a&gt; ‚Äî Head of LLM Research&lt;/li&gt; &lt;li&gt;Jade Cai, &lt;a href="https://www.reddit.com/user/srtng/"&gt;u/srtng&lt;/a&gt; ‚Äî Head of Developer Community&lt;/li&gt; &lt;li&gt;midnight_compile , &lt;a href="/u/Top_Cattle_2098"&gt;u/Top_Cattle_2098&lt;/a&gt; ‚Äî LLM Researcher&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 8AM-11AM PST with our core MiniMax tech team continuing to follow up on questions over the next 48 hours.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OccasionNo6699"&gt; /u/OccasionNo6699 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T15:46:38+00:00</published>
  </entry>
</feed>
