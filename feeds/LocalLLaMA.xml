<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-30T11:53:17+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qr1j7b</id>
    <title>Anyone using bitnet.cpp for production apps?</title>
    <updated>2026-01-30T10:09:25+00:00</updated>
    <author>
      <name>/u/4848928883</name>
      <uri>https://old.reddit.com/user/4848928883</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a backend service which does simple text sumarization and clasification (max 5 categories). At the moment I am using Digital Ocean agents (for price reasons) and hosted ollama instance with a 14B model running on a dedicated GPU.&lt;/p&gt; &lt;p&gt;Both solutions come with drawback. &lt;/p&gt; &lt;p&gt;The hosted ollama can process max 2 req/s on average depending on the input size. It is also not really scalable in terms of cost per value generated.&lt;/p&gt; &lt;p&gt;The DO agents are great and scalable. But they are also too expensive for the simple things I need.&lt;/p&gt; &lt;p&gt;For context: My pipeline processes a couple milion documents per day. Each about ~1500 tokens long.&lt;/p&gt; &lt;p&gt;I was reading and playing with bitnet.cpp. But before going too deep, I am curious if you guys can share your. experience and sucess/fail use cases in production systems.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/4848928883"&gt; /u/4848928883 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qr1j7b/anyone_using_bitnetcpp_for_production_apps/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qr1j7b/anyone_using_bitnetcpp_for_production_apps/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qr1j7b/anyone_using_bitnetcpp_for_production_apps/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-30T10:09:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1qqadna</id>
    <title>My humble GLM 4.7 Flash appreciation post</title>
    <updated>2026-01-29T14:42:32+00:00</updated>
    <author>
      <name>/u/Cool-Chemical-5629</name>
      <uri>https://old.reddit.com/user/Cool-Chemical-5629</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qqadna/my_humble_glm_47_flash_appreciation_post/"&gt; &lt;img alt="My humble GLM 4.7 Flash appreciation post" src="https://preview.redd.it/jh83y5tqqagg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=84c7118a2d6ca354bab71459f8fd90766a909deb" title="My humble GLM 4.7 Flash appreciation post" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was impressed by GLM 4.7 Flash performance, but not surprised, because I knew they could make an outstanding model that will leave most of the competitor models around the same size in the dust.&lt;/p&gt; &lt;p&gt;However I was wondering how good it really is, so I got an idea to use Artificial Analysis to put together all the similar sized open weight models I could think of at that time (or at least the ones available there for selection) and check out their benchmarks against each other to see how are they all doing.&lt;/p&gt; &lt;p&gt;To make things more interesting, I decided to throw in some of the best Gemini models for comparison and well... I knew the model was good, but this good? I don't think we can appreciate this little gem enough, just look who's there daring to get so close to the big guys. üòâ&lt;/p&gt; &lt;p&gt;This graph makes me wonder - Could it be that 30B-A3B or similar model sizes might eventually be enough to compete with today's big models? Because to me it looks that way and I have a strong belief that ZAI has what it takes to get us there and I think it's amazing that we have a model of this size and quality at home now.&lt;/p&gt; &lt;p&gt;Thank you, ZAI! ‚ù§&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cool-Chemical-5629"&gt; /u/Cool-Chemical-5629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jh83y5tqqagg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qqadna/my_humble_glm_47_flash_appreciation_post/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qqadna/my_humble_glm_47_flash_appreciation_post/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-29T14:42:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1qr2gas</id>
    <title>Upgrade my rig with a ‚Ç¨3000 budget ‚Äì which setup would you pick?</title>
    <updated>2026-01-30T11:02:35+00:00</updated>
    <author>
      <name>/u/yeswearecoding</name>
      <uri>https://old.reddit.com/user/yeswearecoding</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi folks,&lt;/p&gt; &lt;p&gt;I want to upgrade my rig with a budget of ‚Ç¨3000.&lt;/p&gt; &lt;p&gt;Currently, I have 2√ó RTX 3060 (12 GB VRAM each), 56 GB RAM, and a Ryzen 7 5700G.&lt;/p&gt; &lt;p&gt;My usage: mainly coding with local models. I usually run one model at a time, and I'm looking for a setup that allows a larger context window and better performance with higher quantization levels (q8 or fp16). I use local models to prepare my features (planning mode), then validate them with a SOTA model. The build mode uses either a local model or a small cloud model (like Haiku, Grok Code Fast, etc.).&lt;/p&gt; &lt;p&gt;What setup would you recommend?&lt;/p&gt; &lt;p&gt;1/ Refurbished Mac Studio M2 Max ‚Äì 96 GB RAM (1 TB SSD)&lt;/p&gt; &lt;p&gt;2/ 2√ó RTX 4000 20 GB (360 GB/s) ‚Äî I could keep one RTX 3060 for a total of 52 GB VRAM&lt;/p&gt; &lt;p&gt;3/ 1√ó RTX 4500 32 GB (896 GB/s) ‚Äî I could keep both RTX 3060s for a total of 48 GB VRAM&lt;/p&gt; &lt;p&gt;The Mac probably offers the best capability for larger context sizes, but likely at the lowest raw speed.&lt;/p&gt; &lt;p&gt;Which one would you pick?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yeswearecoding"&gt; /u/yeswearecoding &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qr2gas/upgrade_my_rig_with_a_3000_budget_which_setup/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qr2gas/upgrade_my_rig_with_a_3000_budget_which_setup/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qr2gas/upgrade_my_rig_with_a_3000_budget_which_setup/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-30T11:02:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1qq67io</id>
    <title>OpenMOSS just released MOVA (MOSS-Video-and-Audio) - Fully Open-Source - 18B Active Params (MoE Architecture, 32B in total) - Day-0 support for SGLang-Diffusion</title>
    <updated>2026-01-29T11:35:08+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qq67io/openmoss_just_released_mova_mossvideoandaudio/"&gt; &lt;img alt="OpenMOSS just released MOVA (MOSS-Video-and-Audio) - Fully Open-Source - 18B Active Params (MoE Architecture, 32B in total) - Day-0 support for SGLang-Diffusion" src="https://external-preview.redd.it/anhiOGswbTh5OWdnMQLRfJQU73a9pcHTIBGMkMlYp-rLlT5zwChrnU104y5M.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=75356aa649008b072137cfe5ab634a7b54be4fd5" title="OpenMOSS just released MOVA (MOSS-Video-and-Audio) - Fully Open-Source - 18B Active Params (MoE Architecture, 32B in total) - Day-0 support for SGLang-Diffusion" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GitHub: MOVA: Towards Scalable and Synchronized Video‚ÄìAudio Generation: &lt;a href="https://github.com/OpenMOSS/MOVA"&gt;https://github.com/OpenMOSS/MOVA&lt;/a&gt;&lt;br /&gt; MOVA-360: &lt;a href="https://huggingface.co/OpenMOSS-Team/MOVA-360p"&gt;https://huggingface.co/OpenMOSS-Team/MOVA-360p&lt;/a&gt;&lt;br /&gt; MOVA-720p: &lt;a href="https://huggingface.co/OpenMOSS-Team/MOVA-720p"&gt;https://huggingface.co/OpenMOSS-Team/MOVA-720p&lt;/a&gt;&lt;br /&gt; From OpenMOSS on ùïè: &lt;a href="https://x.com/Open_MOSS/status/2016820157684056172"&gt;https://x.com/Open_MOSS/status/2016820157684056172&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/6n89xfl8y9gg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qq67io/openmoss_just_released_mova_mossvideoandaudio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qq67io/openmoss_just_released_mova_mossvideoandaudio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-29T11:35:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1qq5zdr</id>
    <title>I built an 80M parameter LLM from scratch using the same architecture as Llama 3 - here's what I learned</title>
    <updated>2026-01-29T11:22:46+00:00</updated>
    <author>
      <name>/u/Routine-Thanks-572</name>
      <uri>https://old.reddit.com/user/Routine-Thanks-572</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to share Mini-LLM, a complete implementation of a modern transformer language model built entirely from scratch.&lt;/p&gt; &lt;h1&gt;What makes this different from most educational projects?&lt;/h1&gt; &lt;p&gt;Most tutorials use outdated techniques (learned position embeddings, LayerNorm, character-level tokenization). Mini-LLM implements the &lt;strong&gt;exact same components as Llama 3&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;RoPE&lt;/strong&gt; (Rotary Position Embeddings) - scales to longer sequences&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RMSNorm&lt;/strong&gt; - faster and more stable than LayerNorm&lt;/li&gt; &lt;li&gt;&lt;strong&gt;SwiGLU&lt;/strong&gt; - state-of-the-art activation function&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Grouped Query Attention&lt;/strong&gt; - efficient inference&lt;/li&gt; &lt;li&gt;&lt;strong&gt;SentencePiece BPE&lt;/strong&gt; - real-world tokenization with 32K vocab&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Complete Pipeline&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Custom tokenizer ‚Üí Data processing ‚Üí Training ‚Üí Inference&lt;/li&gt; &lt;li&gt;Memory-mapped data loading (TB-scale ready)&lt;/li&gt; &lt;li&gt;Mixed precision training with gradient accumulation&lt;/li&gt; &lt;li&gt;KV caching for fast generation&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Results&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;80M parameters trained on 361M tokens&lt;/li&gt; &lt;li&gt;5 hours on single A100, final loss ~3.25&lt;/li&gt; &lt;li&gt;Generates coherent text with proper grammar&lt;/li&gt; &lt;li&gt;200-500 tokens/sec inference speed&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Try it yourself&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/Ashx098/Mini-LLM"&gt;https://github.com/Ashx098/Mini-LLM&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;HuggingFace:&lt;/strong&gt; &lt;a href="https://huggingface.co/Ashx098/Mini-LLM"&gt;https://huggingface.co/Ashx098/Mini-LLM&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The code is clean, well-documented, and designed for learning. Every component has detailed explanations of the &amp;quot;why&amp;quot; not just the &amp;quot;how&amp;quot;.&lt;/p&gt; &lt;p&gt;Perfect for students wanting to understand modern LLM architecture without drowning in billion-parameter codebases!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Routine-Thanks-572"&gt; /u/Routine-Thanks-572 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qq5zdr/i_built_an_80m_parameter_llm_from_scratch_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qq5zdr/i_built_an_80m_parameter_llm_from_scratch_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qq5zdr/i_built_an_80m_parameter_llm_from_scratch_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-29T11:22:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1qq8e2x</id>
    <title>Qwen/Qwen3-ASR-1.7B ¬∑ Hugging Face</title>
    <updated>2026-01-29T13:21:49+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qq8e2x/qwenqwen3asr17b_hugging_face/"&gt; &lt;img alt="Qwen/Qwen3-ASR-1.7B ¬∑ Hugging Face" src="https://external-preview.redd.it/7bBjSbi8Jb_ZIxPLdQlxsAX41TayP_Nw4jr5gGuqpXw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=807a024098d7c29e7df6eb6cac205fdc9b7cdeb4" title="Qwen/Qwen3-ASR-1.7B ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The Qwen3-ASR family includes Qwen3-ASR-1.7B and Qwen3-ASR-0.6B, which support language identification and ASR for 52 languages and dialects. Both leverage large-scale speech training data and the strong audio understanding capability of their foundation model, Qwen3-Omni. Experiments show that the 1.7B version achieves state-of-the-art performance among open-source ASR models and is competitive with the strongest proprietary commercial APIs. Here are the main features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;All-in-one&lt;/strong&gt;: Qwen3-ASR-1.7B and Qwen3-ASR-0.6B support language identification and speech recognition for 30 languages and 22 Chinese dialects, so as to English accents from multiple countries and regions.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Excellent and Fast&lt;/strong&gt;: The Qwen3-ASR family ASR models maintains high-quality and robust recognition under complex acoustic environments and challenging text patterns. Qwen3-ASR-1.7B achieves strong performance on both open-sourced and internal benchmarks. While the 0.6B version achieves accuracy-efficient trade-off, it reaches 2000 times throughput at a concurrency of 128. They both achieve streaming / offline unified inference with single model and support transcribe long audio.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Novel and strong forced alignment Solution&lt;/strong&gt;: We introduce Qwen3-ForcedAligner-0.6B, which supports timestamp prediction for arbitrary units within up to 5 minutes of speech in 11 languages. Evaluations show its timestamp accuracy surpasses E2E based forced-alignment models.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Comprehensive inference toolkit&lt;/strong&gt;: In addition to open-sourcing the architectures and weights of the Qwen3-ASR series, we also release a powerful, full-featured inference framework that supports vLLM-based batch inference, asynchronous serving, streaming inference, timestamp prediction, and more.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-ASR-1.7B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qq8e2x/qwenqwen3asr17b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qq8e2x/qwenqwen3asr17b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-29T13:21:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1qr39kx</id>
    <title>Biology PI building multi-agent AI orchestrator - looking for feedback/collaborators</title>
    <updated>2026-01-30T11:47:26+00:00</updated>
    <author>
      <name>/u/Own-Marzipan4488</name>
      <uri>https://old.reddit.com/user/Own-Marzipan4488</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm a biology professor (France/Germany) who spent the last year building an AI development orchestration system:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Multi-agent pipeline: planner ‚Üí executor ‚Üí critic ‚Üí security scan&lt;/li&gt; &lt;li&gt;Local LLM support (Ollama/Qwen) for privacy mode&lt;/li&gt; &lt;li&gt;Multi-executor fallback (cheap models first, escalate if needed)&lt;/li&gt; &lt;li&gt;Quality gates that iterate until code passes&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Working prototype, still rough around the edges. Built it for my own needs.&lt;/p&gt; &lt;p&gt;Now trying to figure out if this is useful to others or just scratching my own itch. Looking for feedback from people who think about this stuff, and potentially collaborators.&lt;/p&gt; &lt;p&gt;Anyone here working on similar problems? What's missing in the current AI dev tooling landscape?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Own-Marzipan4488"&gt; /u/Own-Marzipan4488 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qr39kx/biology_pi_building_multiagent_ai_orchestrator/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qr39kx/biology_pi_building_multiagent_ai_orchestrator/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qr39kx/biology_pi_building_multiagent_ai_orchestrator/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-30T11:47:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1qqzi8r</id>
    <title>Local AI setup</title>
    <updated>2026-01-30T08:05:53+00:00</updated>
    <author>
      <name>/u/Illustrious_Oven2611</name>
      <uri>https://old.reddit.com/user/Illustrious_Oven2611</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I currently have a Ryzen 5 2400G with 16 GB of RAM. Needless to say, it lags ‚Äî it takes a long time to use even small models like Qwen-3 4B. If I install a cheap used graphics card like the Quadro P1000, would that speed up these small models and allow me to have decent responsiveness for interacting with them locally?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Illustrious_Oven2611"&gt; /u/Illustrious_Oven2611 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qqzi8r/local_ai_setup/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qqzi8r/local_ai_setup/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qqzi8r/local_ai_setup/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-30T08:05:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1qr18xk</id>
    <title>Open-source LoongFlow: Bridging LLM-powered Reasoning Agents and Evolutionary Algorithms for Local AI Research</title>
    <updated>2026-01-30T09:53:08+00:00</updated>
    <author>
      <name>/u/EnvironmentTop7077</name>
      <uri>https://old.reddit.com/user/EnvironmentTop7077</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; community! I‚Äôve been exploring tools to make LLM-based autonomous AI research more efficient, and wanted to share an open-source framework that‚Äôs been working well for me‚ÄîLoongFlow. It‚Äôs designed to bridge reasoning agents (powered by LLMs) and evolutionary algorithms, and I think it could be helpful for anyone working on algorithm discovery, ML pipeline optimization, or LLM-based research.&lt;/p&gt; &lt;p&gt;If you‚Äôve ever struggled with inefficient AI research or wasted computing power, you know the pain: Reasoning-based Agents (like AutoGPT, Voyager) are great at understanding tasks but lack large-scale exploration. Evolutionary algorithms (like MAP-Elites, OpenEvolve) excel at diverse search but rely on blind mutation without semantic guidance. LoongFlow merges these two strengths to create a more effective approach to directed cognitive evolution.&lt;/p&gt; &lt;p&gt;The core of LoongFlow is its Plan-Execute-Summarize (PES) cognitive paradigm‚Äînot just a simple combination, but a full closed loop. The Planner uses historical data and semantic reasoning to map the best evolution path, avoiding blind trial and error. The Executor runs parallel population-level optimization to explore diverse solutions. The Summarizer reviews results, learns from successes and failures, and feeds insights back to the Planner. This turns random trial and error into directed thinking, boosting both efficiency and quality.&lt;/p&gt; &lt;p&gt;Here‚Äôs a simple diagram to illustrate the PES cognitive paradigm (helps visualize the closed-loop logic):&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/mqllrhehkggg1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=672e114ad4c45cf5e808fa2182e3e714f7e1d567"&gt;https://preview.redd.it/mqllrhehkggg1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=672e114ad4c45cf5e808fa2182e3e714f7e1d567&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I‚Äôve seen some solid real-world results from it too. In algorithm discovery, it broke baselines in AlphaEvolve tests‚Äîscoring 0.9027 on Autocorrelation II (vs. 0.8962 for traditional frameworks) and advancing the Erd≈ës problem. In ML, its built-in agent won 14 Kaggle/MLEBench gold medals (computer vision, NLP, tabular data) without any manual intervention. All of this is well-documented in its open-source repo, so you can verify the results yourself.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/gjh3jlb7lggg1.png?width=627&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=70ac2ed41b0fbdaf940921e89bcc7c5c919c82af"&gt;https://preview.redd.it/gjh3jlb7lggg1.png?width=627&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=70ac2ed41b0fbdaf940921e89bcc7c5c919c82af&lt;/a&gt;&lt;/p&gt; &lt;p&gt;As an open-source framework, LoongFlow offers a practical tool for LLM-based autonomous research. For years, AI research tools were limited to basic data processing and model training assistance. LoongFlow takes this further, enabling more independent AI-driven research‚Äîespecially useful for those working with local LLMs and looking to avoid unnecessary computing power waste.&lt;/p&gt; &lt;p&gt;Best of all, it‚Äôs completely open-source and accessible to teams of any size, even for local deployment on consumer-grade hardware (no need for high-end GPUs). It comes with full code, pre-built Agents, and detailed documentation, supporting both open-source LLMs (like DeepSeek) and commercial ones (like Gemini). You don‚Äôt need huge R&amp;amp;D costs to access top-tier cognitive evolution capabilities‚Äîjust clone the repo and get started with local testing.&lt;/p&gt; &lt;p&gt;GitHub repo: &lt;a href="https://github.com/baidu-baige/LoongFlow"&gt;https://github.com/baidu-baige/LoongFlow&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I wanted to share this with the community because I think it could help a lot of researchers and developers save time and avoid common pitfalls. Has anyone tried integrating evolutionary algorithms with local LLMs before? What do you think of the PES paradigm? Would you use this for your next research project? Drop your thoughts and questions below‚ÄîI‚Äôm happy to discuss!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EnvironmentTop7077"&gt; /u/EnvironmentTop7077 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qr18xk/opensource_loongflow_bridging_llmpowered/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qr18xk/opensource_loongflow_bridging_llmpowered/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qr18xk/opensource_loongflow_bridging_llmpowered/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-30T09:53:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1qqeudu</id>
    <title>Why don‚Äôt we have more distilled models?</title>
    <updated>2026-01-29T17:23:15+00:00</updated>
    <author>
      <name>/u/GreedyWorking1499</name>
      <uri>https://old.reddit.com/user/GreedyWorking1499</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The Qwen 8B DeepSeek R1 distill genuinely blew me away when it dropped. You had reasoning capabilities that punched way above the parameter count, running on consumer (GPU poor) hardware. &lt;/p&gt; &lt;p&gt;So where are the rest of them? Why aren‚Äôt there more?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GreedyWorking1499"&gt; /u/GreedyWorking1499 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qqeudu/why_dont_we_have_more_distilled_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qqeudu/why_dont_we_have_more_distilled_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qqeudu/why_dont_we_have_more_distilled_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-29T17:23:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1qr1p1u</id>
    <title>SenseTime have launched and open-sourced SenseNova-MARS (8B/32B)!</title>
    <updated>2026-01-30T10:18:39+00:00</updated>
    <author>
      <name>/u/Soggy_Mission3372</name>
      <uri>https://old.reddit.com/user/Soggy_Mission3372</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qr1p1u/sensetime_have_launched_and_opensourced/"&gt; &lt;img alt="SenseTime have launched and open-sourced SenseNova-MARS (8B/32B)!" src="https://b.thumbs.redditmedia.com/gEmngWLjKvXHjaE_3DuqBUDN7h4FLi1ThRmJgvvpdDw.jpg" title="SenseTime have launched and open-sourced SenseNova-MARS (8B/32B)!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;First open-source AgenticVLM with dynamic image reasoning + text/image search&lt;/p&gt; &lt;p&gt;Autonomously plans steps, calls various tools, solves complex tasks&lt;/p&gt; &lt;p&gt;SOTA across benchmarks including MMSearch, HR-MMSearch, FVQA and more ‚Äî surpassing Gemini3Pro &amp;amp; GPT5.2&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/gdm9xsjvoggg1.jpg?width=900&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=62b1690bae6ebe8b4e604d98538ec6e4b72af733"&gt;https://preview.redd.it/gdm9xsjvoggg1.jpg?width=900&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=62b1690bae6ebe8b4e604d98538ec6e4b72af733&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/i8vhm5wq1hgg1.jpg?width=1510&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=3fe24d5d9c963fa58d7373cfdd78e91059032e1f"&gt;https://preview.redd.it/i8vhm5wq1hgg1.jpg?width=1510&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=3fe24d5d9c963fa58d7373cfdd78e91059032e1f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/m0wnl5wq1hgg1.jpg?width=1510&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=ceb5bf9f6ebf5578c7939c32adc4c235e084fb03"&gt;https://preview.redd.it/m0wnl5wq1hgg1.jpg?width=1510&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=ceb5bf9f6ebf5578c7939c32adc4c235e084fb03&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/rbvmh7wq1hgg1.jpg?width=1510&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=ad2ce39099a6b1af79740398c918e1c4f47c749f"&gt;https://preview.redd.it/rbvmh7wq1hgg1.jpg?width=1510&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=ad2ce39099a6b1af79740398c918e1c4f47c749f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/g0drt7wq1hgg1.jpg?width=1510&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=cdbd6cf7305c87ee1f1cbbaeb6dbef3e26646969"&gt;https://preview.redd.it/g0drt7wq1hgg1.jpg?width=1510&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=cdbd6cf7305c87ee1f1cbbaeb6dbef3e26646969&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/h89wd9wq1hgg1.jpg?width=3795&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=56884b757d8ac5a101c81b8fab738a57c216054a"&gt;https://preview.redd.it/h89wd9wq1hgg1.jpg?width=3795&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=56884b757d8ac5a101c81b8fab738a57c216054a&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Soggy_Mission3372"&gt; /u/Soggy_Mission3372 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qr1p1u/sensetime_have_launched_and_opensourced/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qr1p1u/sensetime_have_launched_and_opensourced/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qr1p1u/sensetime_have_launched_and_opensourced/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-30T10:18:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1qqnm9z</id>
    <title>Train your own AI to write like Opus 4.5</title>
    <updated>2026-01-29T22:43:34+00:00</updated>
    <author>
      <name>/u/volious-ka</name>
      <uri>https://old.reddit.com/user/volious-ka</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, I recently trained DASD-4B-Thinking using this as the foundation of the pipeline and it totally works. DASD4B actually sounds like Opus now. You can the dataset I listed on huggingface to do it. &lt;/p&gt; &lt;p&gt;Total api cost: $55.91&lt;br /&gt; &lt;a href="https://huggingface.co/datasets/crownelius/Opus-4.5-WritingStyle-1000x"&gt;https://huggingface.co/datasets/crownelius/Opus-4.5-WritingStyle-1000x&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Works exceptionally well when paired with Gemini 3 Pro distills. &lt;/p&gt; &lt;p&gt;Should I start a kickstarter to make more datasets? lol &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/volious-ka"&gt; /u/volious-ka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qqnm9z/train_your_own_ai_to_write_like_opus_45/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qqnm9z/train_your_own_ai_to_write_like_opus_45/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qqnm9z/train_your_own_ai_to_write_like_opus_45/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-29T22:43:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1qqws3g</id>
    <title>Spent 20 years assessing students. Applied the same framework to LLMs.</title>
    <updated>2026-01-30T05:30:49+00:00</updated>
    <author>
      <name>/u/Adhesiveness_Civil</name>
      <uri>https://old.reddit.com/user/Adhesiveness_Civil</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been an assistive tech instructor for 20 years. Master‚Äôs in special ed. My whole career has been assessing what learners need‚Äînot where they rank.&lt;/p&gt; &lt;p&gt;Applied that to AI models. Built AI-SETT: 600 observable criteria across 13 categories. Diagnostic, not competitive. The +0 list (gaps) matters more than the total.&lt;/p&gt; &lt;p&gt;Grounded in SETT framework, Cognitive Load Theory, Zone of Proximal Development. Tools I‚Äôve used with actual humans for decades.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/crewrelay/AI-SETT"&gt;https://github.com/crewrelay/AI-SETT&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Fair warning: this breaks the moment someone makes it a leaderboard.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Adhesiveness_Civil"&gt; /u/Adhesiveness_Civil &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qqws3g/spent_20_years_assessing_students_applied_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qqws3g/spent_20_years_assessing_students_applied_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qqws3g/spent_20_years_assessing_students_applied_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-30T05:30:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1qr24ml</id>
    <title>Rig for Local LLMs (RTX Pro 6000 vs Halo Strix vs DGX Spark)</title>
    <updated>2026-01-30T10:44:13+00:00</updated>
    <author>
      <name>/u/cysio528</name>
      <uri>https://old.reddit.com/user/cysio528</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;For some time I'm eyeing gear for setting up local LLMs. I've even got 2 3090(with plan to get 4 total) some time ago, but decided that setting up 4 of those would not be feasible for me at that time and I've returned them and I'm looking for different approach. &lt;/p&gt; &lt;p&gt;As for usage, there will probably be only one user at a time, maybe I'll expose it for my family, but I don't expect much concurrency there in general. &lt;/p&gt; &lt;p&gt;I plan to use it at least as some kind of personal assistant - emails and personal messages summary, accessing my private data, maybe private RAG (some clawdbot maybe?). That's the minimum requirement for me, since this may include some sensitive personal information, I can't use external LLMs for this. Other thing I'm interested in is coding - right now using Codex and I'm quite happy with it. I don't expect to get same results, but some coding capabilities would be welcome, but in this area I expect to loose some quality.&lt;/p&gt; &lt;p&gt;Now, I see three options (all the prices are after conversion from my local currency to USD):&lt;/p&gt; &lt;p&gt;- RTX Pro 6000 ($10k)+ utilization of my current PC as server (I would need to get something as replacement for my PC) - best performance, possibility to upgrade in the future. Huge minus is cost of the card itself and having to get rest of the components, which with current ram prices is quite problematic.&lt;/p&gt; &lt;p&gt;- Halo Strix (AI Max+ 395 with 128 GB of ram) ($3100) - way cheaper, but worse performance and also lack of possible upgrades (would running some occulink + RTX Pro 6000 be possible and beneficial as potential upgrade in te future? )&lt;/p&gt; &lt;p&gt;- DGX Spark ($5300) - more expensive than AMD solution, still lack of upgrades. Seems to be way worse option than Halo Strix, but maybe I'm missing something?&lt;/p&gt; &lt;p&gt;I've found some estimations of 30-40 t/s for DGX Spark and Halo Strix and more than 120 t/s - are those realistic values? &lt;/p&gt; &lt;p&gt;Are there other, not obvious potential issues / benefits to consider? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cysio528"&gt; /u/cysio528 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qr24ml/rig_for_local_llms_rtx_pro_6000_vs_halo_strix_vs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qr24ml/rig_for_local_llms_rtx_pro_6000_vs_halo_strix_vs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qr24ml/rig_for_local_llms_rtx_pro_6000_vs_halo_strix_vs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-30T10:44:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1qqidxp</id>
    <title>Why are small models (32b) scoring close to frontier models?</title>
    <updated>2026-01-29T19:27:44+00:00</updated>
    <author>
      <name>/u/Financial-Cap-8711</name>
      <uri>https://old.reddit.com/user/Financial-Cap-8711</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I keep seeing benchmark results where models like Qwen-32B or GLM-4.x Flash score surprisingly good as per their size than larger models like DeepSeek V3, Kimi K2.5 (1T), or GPT-5.x.&lt;/p&gt; &lt;p&gt;Given the huge gap in model size and training compute, I‚Äôd expect a bigger difference.&lt;/p&gt; &lt;p&gt;So what‚Äôs going on?&lt;/p&gt; &lt;p&gt;Are benchmarks basically saturated?&lt;/p&gt; &lt;p&gt;Is this distillation / contamination / inference-time tricks?&lt;/p&gt; &lt;p&gt;Do small models break down on long-horizon or real-world tasks that benchmarks don‚Äôt test?&lt;/p&gt; &lt;p&gt;Curious where people actually see the gap show up in practice.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Financial-Cap-8711"&gt; /u/Financial-Cap-8711 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qqidxp/why_are_small_models_32b_scoring_close_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qqidxp/why_are_small_models_32b_scoring_close_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qqidxp/why_are_small_models_32b_scoring_close_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-29T19:27:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1qr0ubh</id>
    <title>Beginner in RAG, Need help.</title>
    <updated>2026-01-30T09:28:13+00:00</updated>
    <author>
      <name>/u/whatshouldidotoknow</name>
      <uri>https://old.reddit.com/user/whatshouldidotoknow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I have a 400-500 page unstructured PDF document with selectable text filled with Tables. I have been provided Nvidia L40S GPU for a week. I need help in parsing such PDf's to be able to run RAG on this. My task is to make RAG possible on such documents which span anywhere betwee 400 to 1000 pages. I work in pharma so i cant use any paid API's to parse this.&lt;br /&gt; I have tried Camelot - didnt work well,&lt;br /&gt; Tried Docling, works well but takes forever to parse 500 pages.&lt;br /&gt; I thought of converting the PDF to Json, that didnt work so well either. I am new to all this, please help me with some idea on how to go forward.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/whatshouldidotoknow"&gt; /u/whatshouldidotoknow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qr0ubh/beginner_in_rag_need_help/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qr0ubh/beginner_in_rag_need_help/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qr0ubh/beginner_in_rag_need_help/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-30T09:28:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1qq6n3t</id>
    <title>GitHub trending this week: half the repos are agent frameworks. 90% will be dead in 1 week.</title>
    <updated>2026-01-29T11:58:22+00:00</updated>
    <author>
      <name>/u/Distinct-Expression2</name>
      <uri>https://old.reddit.com/user/Distinct-Expression2</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qq6n3t/github_trending_this_week_half_the_repos_are/"&gt; &lt;img alt="GitHub trending this week: half the repos are agent frameworks. 90% will be dead in 1 week." src="https://preview.redd.it/uf2m03ak2agg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=02bc2552e04fbb090e9eb5df2979a536c39ef524" title="GitHub trending this week: half the repos are agent frameworks. 90% will be dead in 1 week." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It this the js framework hell moment of ai?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Distinct-Expression2"&gt; /u/Distinct-Expression2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/uf2m03ak2agg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qq6n3t/github_trending_this_week_half_the_repos_are/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qq6n3t/github_trending_this_week_half_the_repos_are/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-29T11:58:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1qqvb79</id>
    <title>GitHub - TrevorS/qwen3-tts-rs: Pure Rust implementation of Qwen3-TTS speech synthesis</title>
    <updated>2026-01-30T04:18:03+00:00</updated>
    <author>
      <name>/u/adefa</name>
      <uri>https://old.reddit.com/user/adefa</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qqvb79/github_trevorsqwen3ttsrs_pure_rust_implementation/"&gt; &lt;img alt="GitHub - TrevorS/qwen3-tts-rs: Pure Rust implementation of Qwen3-TTS speech synthesis" src="https://external-preview.redd.it/-L5uxiQVcL6ROhDnA4nw0i8GDaCVuNfbMGVzfQpr0OA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=61bb9f2e5549f6fb0e684d0b9a7d8fa88472f0b3" title="GitHub - TrevorS/qwen3-tts-rs: Pure Rust implementation of Qwen3-TTS speech synthesis" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I love pushing these coding platforms to their (my? our?) limits!&lt;/p&gt; &lt;p&gt;This time I ported the new Qwen 3 TTS model to Rust using Candle: &lt;a href="https://github.com/TrevorS/qwen3-tts-rs"&gt;https://github.com/TrevorS/qwen3-tts-rs&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It took a few days to get the first intelligible audio, but eventually voice cloning and voice design were working as well. I was never able to get in context learning (ICL) to work, neither with the original Python code, or with this library.&lt;/p&gt; &lt;p&gt;I've tested that CPU, CUDA, and Metal are all working. Check it out, peek at the code, let me know what you think!&lt;/p&gt; &lt;p&gt;P.S. -- new (to me) Claude Code trick: when working on a TTS speech model, write a skill to run the output through speech to text to verify the results. :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/adefa"&gt; /u/adefa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/TrevorS/qwen3-tts-rs"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qqvb79/github_trevorsqwen3ttsrs_pure_rust_implementation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qqvb79/github_trevorsqwen3ttsrs_pure_rust_implementation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-30T04:18:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1qqfe1k</id>
    <title>Kimi AI team sent me this appreciation mail</title>
    <updated>2026-01-29T17:42:26+00:00</updated>
    <author>
      <name>/u/mehulgupta7991</name>
      <uri>https://old.reddit.com/user/mehulgupta7991</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qqfe1k/kimi_ai_team_sent_me_this_appreciation_mail/"&gt; &lt;img alt="Kimi AI team sent me this appreciation mail" src="https://preview.redd.it/0ztj2mk3sbgg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0cb8fb8a83de79c13b7ca310a250afa85f95fe79" title="Kimi AI team sent me this appreciation mail" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I covered Kimi K2.5 on my YT channel and the team sent me this mail with a premium access to agent swarm&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mehulgupta7991"&gt; /u/mehulgupta7991 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0ztj2mk3sbgg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qqfe1k/kimi_ai_team_sent_me_this_appreciation_mail/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qqfe1k/kimi_ai_team_sent_me_this_appreciation_mail/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-29T17:42:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1qqj51h</id>
    <title>LingBot-World outperforms Genie 3 in dynamic simulation and is fully Open Source</title>
    <updated>2026-01-29T19:54:56+00:00</updated>
    <author>
      <name>/u/Electrical-Shape-266</name>
      <uri>https://old.reddit.com/user/Electrical-Shape-266</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qqj51h/lingbotworld_outperforms_genie_3_in_dynamic/"&gt; &lt;img alt="LingBot-World outperforms Genie 3 in dynamic simulation and is fully Open Source" src="https://external-preview.redd.it/NWM3YmNxOGtlY2dnMazYtBKu82jdVCrAUapl0f29ZcySaNJ_OhsJC51jAkVT.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ec16c92dbc441bee1bbd5f8a0850b37011370867" title="LingBot-World outperforms Genie 3 in dynamic simulation and is fully Open Source" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The newly released LingBot-World framework offers the first high capability world model that is fully open source, directly contrasting with proprietary systems like Genie 3. The technical report highlights that while both models achieve real-time interactivity, LingBot-World surpasses Genie 3 in dynamic degree, meaning it handles complex physics and scene transitions with greater fidelity. It achieves 16 frames per second and features emergent spatial memory where objects remain consistent even after leaving the field of view for 60 seconds. This release effectively breaks the monopoly on interactive world simulation by providing the community with full access to the code and model weights.&lt;/p&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/collections/robbyant/lingbot-world"&gt;https://huggingface.co/collections/robbyant/lingbot-world&lt;/a&gt;&lt;/p&gt; &lt;p&gt;AGI will be very near. Let's talk about it!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Electrical-Shape-266"&gt; /u/Electrical-Shape-266 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/fjyoor8kecgg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qqj51h/lingbotworld_outperforms_genie_3_in_dynamic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qqj51h/lingbotworld_outperforms_genie_3_in_dynamic/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-29T19:54:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1qqw3ov</id>
    <title>GLM 4.7 Flash 30B PRISM + Web Search: Very solid.</title>
    <updated>2026-01-30T04:56:57+00:00</updated>
    <author>
      <name>/u/My_Unbiased_Opinion</name>
      <uri>https://old.reddit.com/user/My_Unbiased_Opinion</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just got this set up yesterday. I have been messing around with it and I am extremely impressed. I find that it is very efficient in reasoning compared to Qwen models. The model is quite uncensored so I'm able to research any topics, it is quite thorough. &lt;/p&gt; &lt;p&gt;The knowledge is definitely less than 120B Derestricted, but once Web Search RAG is involved, I'm finding the 30B model generally superior with far less soft refusals. Since the model has web access, I feel the base knowledge deficit is mitigated. &lt;/p&gt; &lt;p&gt;Running it in the latest LMstudio beta + OpenwebUI. Y'all gotta try it. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/My_Unbiased_Opinion"&gt; /u/My_Unbiased_Opinion &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qqw3ov/glm_47_flash_30b_prism_web_search_very_solid/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qqw3ov/glm_47_flash_30b_prism_web_search_very_solid/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qqw3ov/glm_47_flash_30b_prism_web_search_very_solid/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-30T04:56:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1qqhhtx</id>
    <title>Mistral CEO Arthur Mensch: ‚ÄúIf you treat intelligence as electricity, then you just want to make sure that your access to intelligence cannot be throttled.‚Äù</title>
    <updated>2026-01-29T18:56:08+00:00</updated>
    <author>
      <name>/u/Wonderful-Excuse4922</name>
      <uri>https://old.reddit.com/user/Wonderful-Excuse4922</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qqhhtx/mistral_ceo_arthur_mensch_if_you_treat/"&gt; &lt;img alt="Mistral CEO Arthur Mensch: ‚ÄúIf you treat intelligence as electricity, then you just want to make sure that your access to intelligence cannot be throttled.‚Äù" src="https://external-preview.redd.it/NW03ZGMyazI1Y2dnMWh2gxSpyeR6q2IEmV4jHAJM791DDo_e5MvHim0gQe4g.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=43d352ce0d8764709982770e551c498fa8279ecc" title="Mistral CEO Arthur Mensch: ‚ÄúIf you treat intelligence as electricity, then you just want to make sure that your access to intelligence cannot be throttled.‚Äù" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wonderful-Excuse4922"&gt; /u/Wonderful-Excuse4922 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/wd12dl725cgg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qqhhtx/mistral_ceo_arthur_mensch_if_you_treat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qqhhtx/mistral_ceo_arthur_mensch_if_you_treat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-29T18:56:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1qqpon2</id>
    <title>OpenCode + llama.cpp + GLM-4.7 Flash: Claude Code at home</title>
    <updated>2026-01-30T00:07:33+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qqpon2/opencode_llamacpp_glm47_flash_claude_code_at_home/"&gt; &lt;img alt="OpenCode + llama.cpp + GLM-4.7 Flash: Claude Code at home" src="https://b.thumbs.redditmedia.com/7H9fabOO-Ob3KECLa7f5HGWJVRXrjNM3C7iqKkauNZU.jpg" title="OpenCode + llama.cpp + GLM-4.7 Flash: Claude Code at home" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;command I use (may be suboptimal but it works for me now):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES=0,1,2 llama-server --jinja --host 0.0.0.0 -m /mnt/models1/GLM/GLM-4.7-Flash-Q8_0.gguf --ctx-size 200000 --parallel 1 --batch-size 2048 --ubatch-size 1024 --flash-attn on --cache-ram 61440 --context-shift &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This is probably something I need to use next to make it even faster: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1qpjc4a/add_selfspeculative_decoding_no_draft_model/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1qpjc4a/add_selfspeculative_decoding_no_draft_model/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qqpon2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qqpon2/opencode_llamacpp_glm47_flash_claude_code_at_home/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qqpon2/opencode_llamacpp_glm47_flash_claude_code_at_home/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-30T00:07:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1qp46za</id>
    <title>AMA Announcement: Moonshot AI, The Opensource Frontier Lab Behind Kimi K2.5 SoTA Model (Wednesday, 8AM-11AM PST)</title>
    <updated>2026-01-28T06:54:28+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qp46za/ama_announcement_moonshot_ai_the_opensource/"&gt; &lt;img alt="AMA Announcement: Moonshot AI, The Opensource Frontier Lab Behind Kimi K2.5 SoTA Model (Wednesday, 8AM-11AM PST)" src="https://preview.redd.it/y2qj7ancf1gg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7bb1df11c9d46ca94be0db3438449dc28e2dd48e" title="AMA Announcement: Moonshot AI, The Opensource Frontier Lab Behind Kimi K2.5 SoTA Model (Wednesday, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; üëã&lt;/p&gt; &lt;p&gt;We're excited for Wednesday's guests, &lt;strong&gt;The Moonshot AI Lab Team!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Kicking things off Wednesday, Jan. 28th, 8 AM‚Äì11 AM PST&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;‚ö†Ô∏è &lt;strong&gt;Note:&lt;/strong&gt; The AMA itself will be hosted in a &lt;strong&gt;separate thread,&lt;/strong&gt; please don‚Äôt post questions here.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/y2qj7ancf1gg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qp46za/ama_announcement_moonshot_ai_the_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qp46za/ama_announcement_moonshot_ai_the_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T06:54:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpewj7</id>
    <title>AMA With Kimi, The Open-source Frontier Lab Behind Kimi K2.5 Model</title>
    <updated>2026-01-28T15:46:40+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"&gt; &lt;img alt="AMA With Kimi, The Open-source Frontier Lab Behind Kimi K2.5 Model" src="https://b.thumbs.redditmedia.com/Tdp5mXDQmwe_e0sKIEY1hjWX6FrN679odChZp8YL2Rk.jpg" title="AMA With Kimi, The Open-source Frontier Lab Behind Kimi K2.5 Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Today we are having &lt;strong&gt;Kimi&lt;/strong&gt;, the research lab behind the &lt;strong&gt;Kimi&lt;/strong&gt; &lt;strong&gt;K2.5&lt;/strong&gt;. We‚Äôre excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/ComfortableAsk4494/"&gt;u/ComfortableAsk4494&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/zxytim/"&gt;u/zxytim&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/ppwwyyxx/"&gt;u/ppwwyyxx&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 8 AM ‚Äì 11 AM PST, with the Kimi team continuing to follow up on questions over the next 24 hours.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3yq8msvp24gg1.png?width=2000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=98c89b5d86ee1197799532fead6a84da2223b389"&gt;https://preview.redd.it/3yq8msvp24gg1.png?width=2000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=98c89b5d86ee1197799532fead6a84da2223b389&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Thanks everyone for joining our AMA. The live part has ended and the Kimi team will be following up with more answers sporadically over the next 24 hours.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T15:46:40+00:00</published>
  </entry>
</feed>
