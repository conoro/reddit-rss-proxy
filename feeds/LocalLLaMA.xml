<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-21T13:50:43+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ps6w96</id>
    <title>Dataset quality is not improving much</title>
    <updated>2025-12-21T13:46:50+00:00</updated>
    <author>
      <name>/u/rekriux</name>
      <uri>https://old.reddit.com/user/rekriux</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ps6w96/dataset_quality_is_not_improving_much/"&gt; &lt;img alt="Dataset quality is not improving much" src="https://external-preview.redd.it/1p_Y2zfHWGdGS1n176QenprBBks4UkO2cWuEEHp6f68.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=521a9f1988c888fe9369f5871e2a57530ec8bd94" title="Dataset quality is not improving much" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am checking public dataset often. And while we have RAG and lots of innovation posted here in &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;, there are rarely breakthrough in datasets creation. While I may be lurking in this sub, I doped out of electronics/computing and studied in other fields and obtained my master in something else, I have been dabbling with AI since 2000. So take this as a my rant. But I do hope some people will start more research on dataset quality and it's creation pipelines.&lt;/p&gt; &lt;p&gt;Buckle up (sorry for spelling, no AI proofread and quick typing)&lt;/p&gt; &lt;p&gt;From my perspectives, the most all rounder datasets for instruction following are :&lt;/p&gt; &lt;p&gt;The Tulu from Allenai [allenai/tulu-3-sft-mixture](&lt;a href="https://huggingface.co/datasets/allenai/tulu-3-sft-mixture"&gt;https://huggingface.co/datasets/allenai/tulu-3-sft-mixture&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;The smoltakl from HG [HuggingFaceTB/smoltalk2](&lt;a href="https://huggingface.co/datasets/HuggingFaceTB/smoltalk2"&gt;https://huggingface.co/datasets/HuggingFaceTB/smoltalk2&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;Hermes 3 from NousResearch [NousResearch/Hermes-3-Dataset](&lt;a href="https://huggingface.co/datasets/NousResearch/Hermes-3-Dataset"&gt;https://huggingface.co/datasets/NousResearch/Hermes-3-Dataset&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;That's about it. The other good dataset are those that mix other datasets for good variety. Dolphin could be good, but I found it's quality a bit lacking to be included in the above. OpenHerms was also good for it's time, but now it should be heavily reworked.&lt;/p&gt; &lt;p&gt;Just that ? This is kind of concerning. Every one knows the &amp;quot;**garbage in, garbage out**&amp;quot; phenomena.&lt;/p&gt; &lt;p&gt;I consider 2 dataset breakthrough : **WizzardLM** and **Magpie**.&lt;/p&gt; &lt;p&gt;Since then, we hadn't have any great innovation in dataset or did I miss it ? Yea, deduplication and merging datasets, but that's not brilliant level and over engineered.&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;Lately, NVIDIA released SFT datasets. The first one they released is behind a &amp;quot;ASK AUTH&amp;quot; to access it? Well, guess what, I was denied access.&lt;/p&gt; &lt;p&gt;Then came Nano and they gave access to the the INSTRUCT SFT:&lt;/p&gt; &lt;p&gt;[nvidia/Nemotron-Instruction-Following-Chat-v1](&lt;a href="https://huggingface.co/datasets/nvidia/Nemotron-Instruction-Following-Chat-v1/"&gt;https://huggingface.co/datasets/nvidia/Nemotron-Instruction-Following-Chat-v1/&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;So I went away and check a few examples. There are other parts of the dataset like RL pipeline, but I didn't have time to investigate further.&lt;/p&gt; &lt;p&gt;**Nemotron** are a bit of hit and miss. If you tried it, sometimes it feels brilliant in solving something, then the next it feels dumb in answering something simpler. Do you get that feeling ?&lt;/p&gt; &lt;p&gt;Well I think this is related to the SFT they did in the initial stage.&lt;/p&gt; &lt;p&gt;For a quick round up of what I found :&lt;/p&gt; &lt;p&gt;- Lots of sycophancy thanks to using GPT-OSS 120B&lt;/p&gt; &lt;p&gt;- No use of **system** message&lt;/p&gt; &lt;p&gt;- Wasting precious resources without having the llm learn that the system prompt is prioritized over user request, soft vs hard overwrites handling, like UPPERCASE or directives that could mean priority like ALWAYS, NEVER, if... Handling opposing directives. Implementing directives as code (codeagent?) ...&lt;/p&gt; &lt;p&gt;Aren't most coding agent using very long system messages to give the LLM instructions ?? Well Nemotron is missing out on training on it so there is no way that it will perform well when used by a agent that make MASSIVE list of instructions to follow.&lt;/p&gt; &lt;p&gt;- Poor use of multi-turn conversations&lt;/p&gt; &lt;p&gt;- Recall of something that was used a few turns up, like initial directives (or some sort of AGENT.md)&lt;/p&gt; &lt;p&gt;- Absence of labeling&lt;/p&gt; &lt;p&gt;- Each conversation should have :&lt;/p&gt; &lt;p&gt;&amp;gt; instructions : the specific instructions list to be learned during this conversation&lt;/p&gt; &lt;p&gt;instructions_types : in what major categories does those instructions fit in&lt;/p&gt; &lt;p&gt;constraints : the .. constraints ... learned ...&lt;/p&gt; &lt;p&gt;constraints_types : in what major categories does those constraints fit in&lt;/p&gt; &lt;p&gt;tasks : the specific tasks asked the llm...&lt;/p&gt; &lt;p&gt;task_type : in what type of llm task does this belong to (EDITING, CREATIVE, CODING...)&lt;/p&gt; &lt;p&gt;skills : the specific skills that should be demonstrated ...&lt;/p&gt; &lt;p&gt;skills_types : skills categories&lt;/p&gt; &lt;p&gt;user_intent : what are the user intents in this conversation&lt;/p&gt; &lt;p&gt;user_intent_categories : ... categories&lt;/p&gt; &lt;p&gt;has_context : the user provided the context (RAG, CODE, )&lt;/p&gt; &lt;p&gt;inject_knowledge : this inject knowledge to the model by generating a answer from nothing (ex external source)&lt;/p&gt; &lt;p&gt;context_type : what is it : code, &lt;a href="http://instruction.md"&gt;instruction.md&lt;/a&gt;, pasted text, url to fetch...&lt;/p&gt; &lt;p&gt;domain_knowledge : what are the domains of knowledge that this touch uppon&lt;/p&gt; &lt;p&gt;mode : are we in a chat with a user, a toolcall, a RP session, a persona (coder, writing assistant), interactive vs one shot&lt;/p&gt; &lt;p&gt;tools_provided : did we provide tools to the llm&lt;/p&gt; &lt;p&gt;tools_used : did the llm use the provided tools&lt;/p&gt; &lt;p&gt;tool_summary : tools used, in what order, tool use evaluation (used right tools but many non productive and didn't use the grep tool that should have done it faster)&lt;/p&gt; &lt;p&gt;risks : what are the risks associated with the user request&lt;/p&gt; &lt;p&gt;risk_mitigation : what should the llm do to mitigate the risks ? disclaimer, refusal, providing multiple perspectives to the request, ignore risk as unfounded&lt;/p&gt; &lt;p&gt;intermediary_steps : add additional steps that force the llm to produce plan of action, summary of important information, recall of what was asked the llm to do&lt;/p&gt; &lt;p&gt;system_protection : does the system message ask for it to be protected (no leaks)&lt;/p&gt; &lt;p&gt;system_protection_test : did the system message leak in the assistant responses&lt;/p&gt; &lt;p&gt;...&lt;/p&gt; &lt;p&gt;The labeling of data is the only way to make sure the dataset is balanced in skills, risk management, task types and diversity of knowledge domains etc.&lt;/p&gt; &lt;p&gt;&amp;gt; How many conversations help the llm learn how to efficiently use RAG context in the conversation and make a summary, extract specific information, process it in a coherent json file ? If you don't have your dataset classified, how can you know if this is under-represented and that is why it's not performing well in **YOUR** agentic use ?&lt;/p&gt; &lt;p&gt;Once you have a label dataset, it's easy to spot blind spots. Also it would be easy to test all skills, tasks, risks etc. to evaluate how it performs on more complicated evaluation set and see it some should be augmented in the dataset. This should be done regularly in training phase, **so you could balance things by finer adjustment in ratios between checkpoint snapshot.**&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;From my perspective, Nano will perform poorly in many cases just because the instruction set for initial SFT was bad. They used GPT-OSS-120B, Qwen3-235B-A22B-Thinking-2507, and Qwen3-235B-A22B-Instruct-2507 for generation, and that seems like middle of the LLM size. I would have thought that more large open models would have been used, at least for some tasks like handling multiple instructions/constraints at the same time while performing many tasks and using many skills. Also using those mid range llms, they should have time to do review of the dataset by LLMS. Just produce statistics and ask all other 400B models to evaluate your pipeline, output, reasoning in making the dataset and THEY WILL TELL YOU WHERE YOU MISSED OUT.&lt;/p&gt; &lt;p&gt;Now if you where to ask me how to enhance this dataset, I would say&lt;/p&gt; &lt;ol&gt; &lt;li&gt;classify it to get the idea of current state (the system, user, assistant turns)&lt;/li&gt; &lt;li&gt;make a list of all large categories and plot distributions -&amp;gt; ANALYZE THIS&lt;/li&gt; &lt;li&gt;generate system messages for each conversation, starting with the user requests and looking at user_intent&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;- use a sort of registry to follow and adjust distribution of instructions, constraints, tasks, skills, tools, number of directives in system&lt;/p&gt; &lt;p&gt;- have clear identification of what this conversation is about : you are a chatbot in some company processing complaints, you are a public chat providing answers to help students, engage in roleplay (RP) with user by impersonating, you are a game master/story teller in a interactive, you are a brainstorming assistant that helps produce detailed exploration plans...&lt;/p&gt; &lt;p&gt;- have varying length of system msg, from 10 to 2k tokens&lt;/p&gt; &lt;p&gt;4) Insert RAG content from ultra-fineweb, finepdf, wikipedia, recycling_the_web and ask that answer be based on that context (to prevent too much content injection (that may result in more hallucinations) and work more on skills).&lt;/p&gt; &lt;p&gt;5) For cases where RAG is not used, this should be CREATIVE/PROBLEM_SOLVING/PLANNING types of tasks, and those tasks should be well defined in system message or in user, make sure it is&lt;/p&gt; &lt;p&gt;6) Regenerate set % of user messages using evolve to include more instructions/constraints and complicate things a bit&lt;/p&gt; &lt;p&gt;7) After each change above, update the classification of the conversation, each modification to the conversation should be a json with : what to modify (system, user_#, assistant_#) and classification modification (+instruct, +constraint, +task, -mode, +mode)&lt;/p&gt; &lt;p&gt;8) Review distribution of data, make more adjustments&lt;/p&gt; &lt;p&gt;9) now regenerate the answers, before each assistant turn, produce a intermediary turn, it should be like multiple agents debating about what is the task at hand, what previous information was provided, what are the specific instructions and constraints, enumerate previous conversations that may have content for this, are there any ambiguity or any information missing that could prevent making a informed decision...&lt;/p&gt; &lt;p&gt;10) check that it makes sens, risk management, easy answer or considered multiple angles, did the model consider ambiguity or opposing instructions/constraints... That should use the intermediary_steps.&lt;/p&gt; &lt;p&gt;11) fix any issues in answers&lt;/p&gt; &lt;p&gt;12) evaluate dataset on small model with 100b token budget the model performance to check the impact of the changes to the dataset&lt;/p&gt; &lt;p&gt;----&lt;/p&gt; &lt;p&gt;My gold dataset rule :&lt;/p&gt; &lt;p&gt;Now if you just produce answers without the intermediary steps, this is just distillation and the produced model will never be any better than the reference model (in fact it will be a bit worse, because the model attention is limited and it may have missed something once, then your mode will miss it always). But if you use a few models to reason, explore, summarize, recall previous knowledge and make hypothesis, validate hypothesis beforehand and passing that condensed work to the llm before generating the answer, then you are on the way to developing unique and perhaps enhanced skills for your future model. Simple, generate a distilled response and generate a primed response using the gold intermediary step and compare the 2, you will have your answer.&lt;/p&gt; &lt;p&gt;Every assistant generation should also be checked that it respected the task, that it performed it by following the instructions and constraints, that it stayed in it's 'role' or mode...&lt;/p&gt; &lt;p&gt;This is how we could work on having SOTA datasets to rivalize those held behind closed doors.&lt;/p&gt; &lt;p&gt;Hope this inspire more research and higher quality datasets.&lt;/p&gt; &lt;p&gt;P.S. I would like if you hold datasets that can be anonymized to be shared on HG, this could contribute to more diversity.&lt;/p&gt; &lt;p&gt;Also shout out to Eric Hartford [QuixiAI/VibeCoding](&lt;a href="https://huggingface.co/datasets/QuixiAI/VibeCoding"&gt;https://huggingface.co/datasets/QuixiAI/VibeCoding&lt;/a&gt;) that is trying to make a open dataset for &amp;quot;collect anonymized client ↔ server message logs from popular AI coding tools and interfaces. These logs will form the basis of an open dataset hosted on Hugging Face and GitHub.&amp;quot; So if any of you wish to contribute, please do so !&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rekriux"&gt; /u/rekriux &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/datasets/nvidia/Nemotron-Instruction-Following-Chat-v1/discussions/1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ps6w96/dataset_quality_is_not_improving_much/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ps6w96/dataset_quality_is_not_improving_much/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-21T13:46:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1prjldr</id>
    <title>What's the realistic "entry point" for a good local LLM experience going into 2026?</title>
    <updated>2025-12-20T17:21:59+00:00</updated>
    <author>
      <name>/u/alphatrad</name>
      <uri>https://old.reddit.com/user/alphatrad</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I notice a lot of questions from people asking it they can run LLM's on their 8gb or 12gb GPU's. &lt;/p&gt; &lt;p&gt;But have noticed most builds fall into two camps: the 16GB-24GB crowd making it work with quantized models, or the absolute madlads running 96GB+ setups.&lt;/p&gt; &lt;p&gt;But there's this interesting middle ground between 24-32GB that doesn't get talked about as much.&lt;/p&gt; &lt;p&gt;So I'm curious what this community thinks: &lt;strong&gt;If someone's getting into local LLMs today, wants a genuinely usable experience (not just &amp;quot;it technically runs&amp;quot;), but still has budget constraints—what's the minimum VRAM you'd actually recommend?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Excluding Macs here since they're a whole different value proposition with unified memory.&lt;/p&gt; &lt;p&gt;My take: 24GB feels like the sweet spot for accessibility right now. You can snag a used 3090 for reasonable money, and it opens up a lot of models that just aren't practical at 16GB. If you are willing to go AMD like me, RX 7900 XTX's can be had for under a grand.&lt;/p&gt; &lt;p&gt;But I'm curious if I'm off base. Are people having legitimately good experiences at 16GB with the right model choices? Or is the jump to 24GB as game-changing as it seems?&lt;/p&gt; &lt;p&gt;What's your &amp;quot;minimum viable VRAM&amp;quot; for someone who wants to actually &lt;em&gt;use&lt;/em&gt; local LLMs, not just experiment?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/alphatrad"&gt; /u/alphatrad &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1prjldr/whats_the_realistic_entry_point_for_a_good_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1prjldr/whats_the_realistic_entry_point_for_a_good_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1prjldr/whats_the_realistic_entry_point_for_a_good_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-20T17:21:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1prt5qz</id>
    <title>MiMo-V2-Flash - SGLang - mtp triton attention</title>
    <updated>2025-12-21T00:34:04+00:00</updated>
    <author>
      <name>/u/getfitdotus</name>
      <uri>https://old.reddit.com/user/getfitdotus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Some testing results on 4x 6000 Blackwell workstation cards&lt;/p&gt; &lt;p&gt;Context | Prompt | Output | E2E Speed | Acc Len&lt;br /&gt; 4K | 3,597 | 500 | 100.2 t/s | N/A | 2.40&lt;/p&gt; &lt;p&gt;8K | 7,199 | 500 | 88.2 t/s | N/A | 2.39&lt;/p&gt; &lt;p&gt;16K | 14,401 | 500 | 67.0 t/s | N/A | 2.24&lt;/p&gt; &lt;p&gt;32K | 28,804 | 500 | 54.5 t/s | N/A | 2.50&lt;/p&gt; &lt;p&gt;64K | 57,611 | 500 | 31.7 t/s | N/A | 2.23&lt;/p&gt; &lt;p&gt;100K | 90,019 | 500 | 24.5 t/s | N/A | 2.42&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/getfitdotus"&gt; /u/getfitdotus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1prt5qz/mimov2flash_sglang_mtp_triton_attention/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1prt5qz/mimov2flash_sglang_mtp_triton_attention/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1prt5qz/mimov2flash_sglang_mtp_triton_attention/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-21T00:34:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1pragtf</id>
    <title>Open source LLM tooling is getting eaten by big tech</title>
    <updated>2025-12-20T09:29:03+00:00</updated>
    <author>
      <name>/u/Inevitable_Wear_9107</name>
      <uri>https://old.reddit.com/user/Inevitable_Wear_9107</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was using TGI for inference six months ago. Migrated to vLLM last month. Thought it was just me chasing better performance, then I read the LLM Landscape 2.0 report. Turns out 35% of projects from just three months ago already got replaced. This isn't just my stack. The whole ecosystem is churning.&lt;/p&gt; &lt;p&gt;The deeper I read, the crazier it gets. Manus blew up in March, OpenManus and OWL launched within weeks as open source alternatives, both are basically dead now. TensorFlow has been declining since 2019 and still hasn't hit bottom. The median project age in this space is 30 months.&lt;/p&gt; &lt;p&gt;Then I looked at what's gaining momentum. NVIDIA drops Dynamo, optimized for NVIDIA hardware. Google releases Gemini CLI with Google Cloud baked in. OpenAI ships Codex CLI that funnels you into their API. That's when it clicked.&lt;/p&gt; &lt;p&gt;Two years ago this space was chaotic but independent. Now the open source layer is becoming the customer acquisition layer. We're not choosing tools anymore. We're being sorted into ecosystems.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Inevitable_Wear_9107"&gt; /u/Inevitable_Wear_9107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pragtf/open_source_llm_tooling_is_getting_eaten_by_big/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pragtf/open_source_llm_tooling_is_getting_eaten_by_big/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pragtf/open_source_llm_tooling_is_getting_eaten_by_big/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-20T09:29:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1prcu0t</id>
    <title>Of course it works, in case you are wondering... and it's quite faster.</title>
    <updated>2025-12-20T12:04:22+00:00</updated>
    <author>
      <name>/u/JLeonsarmiento</name>
      <uri>https://old.reddit.com/user/JLeonsarmiento</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1prcu0t/of_course_it_works_in_case_you_are_wondering_and/"&gt; &lt;img alt="Of course it works, in case you are wondering... and it's quite faster." src="https://preview.redd.it/p9tf12m7nc8g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e07a1563bfc8d54447cad9ed61107030cf7aff50" title="Of course it works, in case you are wondering... and it's quite faster." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JLeonsarmiento"&gt; /u/JLeonsarmiento &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/p9tf12m7nc8g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1prcu0t/of_course_it_works_in_case_you_are_wondering_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1prcu0t/of_course_it_works_in_case_you_are_wondering_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-20T12:04:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1prjl7z</id>
    <title>Nvidia Introduces 'NitroGen': A Foundation Model for Generalist Gaming Agents | "This research effectively validates a scalable pipeline for building general-purpose agents that can operate in unknown environments, moving the field closer to universally capable AI."</title>
    <updated>2025-12-20T17:21:48+00:00</updated>
    <author>
      <name>/u/44th--Hokage</name>
      <uri>https://old.reddit.com/user/44th--Hokage</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1prjl7z/nvidia_introduces_nitrogen_a_foundation_model_for/"&gt; &lt;img alt="Nvidia Introduces 'NitroGen': A Foundation Model for Generalist Gaming Agents | &amp;quot;This research effectively validates a scalable pipeline for building general-purpose agents that can operate in unknown environments, moving the field closer to universally capable AI.&amp;quot;" src="https://external-preview.redd.it/aTRkb2lybnI3ZThnMdFL3UUz04QdHLBdqdlbFHYzvAvsN9wNsENNDP9FjT2f.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d6073ba6e1956995a34ece97d824f00cef66f7b9" title="Nvidia Introduces 'NitroGen': A Foundation Model for Generalist Gaming Agents | &amp;quot;This research effectively validates a scalable pipeline for building general-purpose agents that can operate in unknown environments, moving the field closer to universally capable AI.&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h4&gt;TL;DR:&lt;/h4&gt; &lt;p&gt;&lt;strong&gt;NitroGen demonstrates that we can accelerate the development of generalist AI agents by scraping internet-scale data rather than relying on slow, expensive manual labeling.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;This research effectively validates a scalable pipeline for building general-purpose agents that can operate in unknown environments, moving the field closer to universally capable AI.&lt;/strong&gt;&lt;/p&gt; &lt;hr /&gt; &lt;h4&gt;Abstract:&lt;/h4&gt; &lt;blockquote&gt; &lt;p&gt;We introduce NitroGen, a vision-action foundation model for generalist gaming agents that is trained on 40,000 hours of gameplay videos across more than 1,000 games. We incorporate three key ingredients: - (1) An internet-scale video-action dataset constructed by automatically extracting player actions from publicly available gameplay videos, - (2) A multi-game benchmark environment that can measure cross-game generalization, and - (3) A unified vision-action model trained with large-scale behavior cloning. &lt;/p&gt; &lt;p&gt;NitroGen exhibits strong competence across diverse domains, including combat encounters in 3D action games, high-precision control in 2D platformers, and exploration in procedurally generated worlds. &lt;strong&gt;It transfers effectively to unseen games, achieving up to 52% relative improvement in task success rates over models trained from scratch.&lt;/strong&gt; We release the dataset, evaluation suite, and model weights to advance research on generalist embodied agents.&lt;/p&gt; &lt;/blockquote&gt; &lt;hr /&gt; &lt;h4&gt;Layman's Explanation:&lt;/h4&gt; &lt;p&gt;NVIDIA researchers bypassed the data bottleneck in embodied AI by identifying 40,000 hours of gameplay videos where streamers displayed their controller inputs on-screen, effectively harvesting free, high-quality action labels across more than 1,000 games. This approach proves that the &amp;quot;scale is all you need&amp;quot; paradigm, which drove the explosion of Large Language Models, is viable for training agents to act in complex, virtual environments using noisy internet data.&lt;/p&gt; &lt;p&gt;The resulting model &lt;strong&gt;verifies that large-scale pre-training creates transferable skills; the AI can navigate, fight, and solve puzzles in games it has never seen before, performing significantly better than models trained from scratch.&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;By open-sourcing the model weights and the massive video-action dataset, the team has removed a major barrier to entry, allowing the community to immediately fine-tune these foundation models for new tasks instead of wasting compute on training from the ground up. &lt;/p&gt; &lt;hr /&gt; &lt;h5&gt;Link to the Paper: &lt;a href="https://nitrogen.minedojo.org/assets/documents/nitrogen.pdf"&gt;https://nitrogen.minedojo.org/assets/documents/nitrogen.pdf&lt;/a&gt;&lt;/h5&gt; &lt;hr /&gt; &lt;h5&gt;Link to the Project Website: &lt;a href="https://nitrogen.minedojo.org/"&gt;https://nitrogen.minedojo.org/&lt;/a&gt;&lt;/h5&gt; &lt;hr /&gt; &lt;h5&gt;Link to the HuggingFace: &lt;a href="https://huggingface.co/nvidia/NitroGen"&gt;https://huggingface.co/nvidia/NitroGen&lt;/a&gt;&lt;/h5&gt; &lt;hr /&gt; &lt;h5&gt;Link to the Open-Sourced Dataset: &lt;a href="https://huggingface.co/datasets/nvidia/NitroGen"&gt;https://huggingface.co/datasets/nvidia/NitroGen&lt;/a&gt;&lt;/h5&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/44th--Hokage"&gt; /u/44th--Hokage &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/zdp80umr7e8g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1prjl7z/nvidia_introduces_nitrogen_a_foundation_model_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1prjl7z/nvidia_introduces_nitrogen_a_foundation_model_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-20T17:21:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1ps44ye</id>
    <title>Good 3-5B models?</title>
    <updated>2025-12-21T11:08:10+00:00</updated>
    <author>
      <name>/u/SlowFail2433</name>
      <uri>https://old.reddit.com/user/SlowFail2433</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone found good models they like in the 3-5B range?&lt;/p&gt; &lt;p&gt;Is everyone still using the new Qwen 3 4B in this area or are there others?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SlowFail2433"&gt; /u/SlowFail2433 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ps44ye/good_35b_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ps44ye/good_35b_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ps44ye/good_35b_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-21T11:08:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1prm2tq</id>
    <title>TheDrummer models meet heretic</title>
    <updated>2025-12-20T19:08:34+00:00</updated>
    <author>
      <name>/u/coder3101</name>
      <uri>https://old.reddit.com/user/coder3101</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What if I abliterate the drummer's fine tune to make them a bit less censored? So, I did that and here's the collection:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/coder3101/the-drummers"&gt;https://huggingface.co/collections/coder3101/the-drummers&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It includes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Magidonia-24B-v4.3&lt;/li&gt; &lt;li&gt;Cydonia-24B-v4.3&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;There are two variants, one that reduces refusal and another that reduces KLD so as to keep the performance similar.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/coder3101"&gt; /u/coder3101 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1prm2tq/thedrummer_models_meet_heretic/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1prm2tq/thedrummer_models_meet_heretic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1prm2tq/thedrummer_models_meet_heretic/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-20T19:08:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1ps6txq</id>
    <title>RAG that actually works?</title>
    <updated>2025-12-21T13:43:39+00:00</updated>
    <author>
      <name>/u/TheGlobinKing</name>
      <uri>https://old.reddit.com/user/TheGlobinKing</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When I discovered AnythingLLM I thought I could finally create a &amp;quot;knowledge base&amp;quot; for my own use, basically like an expert of a specific field (e.g. engineering, medicine, etc.) I'm not a developer, just a regular user, and AnythingLLM makes this quite easy. I paired it with llama.cpp, added my documents and started to chat.&lt;/p&gt; &lt;p&gt;However, I noticed poor results from all llms I've tried, granite, qwen, gemma, etc. When I finally asked about a specific topic mentioned in a very long pdf included in my rag &amp;quot;library&amp;quot;, it said it couldn't find any mention of that topic anywhere. It seems only part of the available data is actually considered when answering (again, I'm not an expert.) I noticed a few other similar reports from redditors, so it wasn't just matter of using a different model.&lt;/p&gt; &lt;p&gt;Back to my question... is there an easy to use RAG system that &amp;quot;understands&amp;quot; large libraries of complex texts?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheGlobinKing"&gt; /u/TheGlobinKing &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ps6txq/rag_that_actually_works/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ps6txq/rag_that_actually_works/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ps6txq/rag_that_actually_works/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-21T13:43:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1prh5jp</id>
    <title>A Raspberry Pi + eGPU isn't as dumb as I thought</title>
    <updated>2025-12-20T15:38:20+00:00</updated>
    <author>
      <name>/u/geerlingguy</name>
      <uri>https://old.reddit.com/user/geerlingguy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1prh5jp/a_raspberry_pi_egpu_isnt_as_dumb_as_i_thought/"&gt; &lt;img alt="A Raspberry Pi + eGPU isn't as dumb as I thought" src="https://b.thumbs.redditmedia.com/6loQqYyrEG88VaZ_nKHTgCav_dnTkH81E4pJ_NvuO0w.jpg" title="A Raspberry Pi + eGPU isn't as dumb as I thought" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here's a small selection of benchmarks from my &lt;a href="https://www.jeffgeerling.com/blog/2025/big-gpus-dont-need-big-pcs"&gt;blog post&lt;/a&gt;, I tested a variety of AMD and Nvidia cards on a Raspberry Pi CM5 using an eGPU dock (total system cost, cards excluded, around $350).&lt;/p&gt; &lt;p&gt;For larger models, the performance delta between the Pi and an Intel Core Ultra 265K PC build with 64GB of DDR5 RAM and PCIe Gen 5 was less than 5%. For llama 2 13B, the Pi was even faster for many Nvidia cards (why is that?).&lt;/p&gt; &lt;p&gt;For AMD, the Pi was much slower—to the point I'm pretty sure there's a driver issue or something the AMD drivers expect that the Pi isn't providing (yet... like a large BAR).&lt;/p&gt; &lt;p&gt;I publish all the llama-bench data in &lt;a href="https://github.com/geerlingguy/ai-benchmarks/issues?q=is%3Aissue%20state%3Aclosed"&gt;https://github.com/geerlingguy/ai-benchmarks/issues?q=is%3Aissue%20state%3Aclosed&lt;/a&gt; and multi-GPU benchmarks in &lt;a href="https://github.com/geerlingguy/ai-benchmarks/issues/44"&gt;https://github.com/geerlingguy/ai-benchmarks/issues/44&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/geerlingguy"&gt; /u/geerlingguy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1prh5jp"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1prh5jp/a_raspberry_pi_egpu_isnt_as_dumb_as_i_thought/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1prh5jp/a_raspberry_pi_egpu_isnt_as_dumb_as_i_thought/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-20T15:38:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1przir5</id>
    <title>Big training projects appear to be including CoT reasoning traces in their training data.</title>
    <updated>2025-12-21T06:12:25+00:00</updated>
    <author>
      <name>/u/MaggoVitakkaVicaro</name>
      <uri>https://old.reddit.com/user/MaggoVitakkaVicaro</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1przir5/big_training_projects_appear_to_be_including_cot/"&gt; &lt;img alt="Big training projects appear to be including CoT reasoning traces in their training data." src="https://external-preview.redd.it/qY52rLeZM5AmEgPsJDh_3hPjjhv02hbRWUEyDFI3OeE.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=958f00a9e023c8451aac05b0976c46bd89e9e69e" title="Big training projects appear to be including CoT reasoning traces in their training data." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MaggoVitakkaVicaro"&gt; /u/MaggoVitakkaVicaro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://pratyushmaini.substack.com/p/reverse-engineering-a-phase-change-a96"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1przir5/big_training_projects_appear_to_be_including_cot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1przir5/big_training_projects_appear_to_be_including_cot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-21T06:12:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1prxpcx</id>
    <title>NVIDIA Nemotron-3-Nano-30B LLM Benchmarks Vulkan and RPC</title>
    <updated>2025-12-21T04:31:19+00:00</updated>
    <author>
      <name>/u/tabletuser_blogspot</name>
      <uri>https://old.reddit.com/user/tabletuser_blogspot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm running a few benchmarks on Nvidia's new &lt;a href="https://huggingface.co/unsloth/Nemotron-3-Nano-30B-A3B-GGUF"&gt;Nemotron-3-Nano-30B&lt;/a&gt; and will test out RPC-SERVER again.&lt;/p&gt; &lt;p&gt;More details on Mamba2-Transformer Hybrid Mixture of Experts (MoE) model is here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16"&gt;https://huggingface.co/nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16&lt;/a&gt;&lt;/p&gt; &lt;p&gt;4 Systems all running Kubuntu 24.04 to 26.04.&lt;/p&gt; &lt;p&gt;GPUs: &lt;a href="https://www.techpowerup.com/gpu-specs/geforce-gtx-1080-ti.c2877"&gt;Nvidia 1080Ti 11GB&lt;/a&gt;, &lt;a href="https://www.techpowerup.com/gpu-specs/p102-100.c3100"&gt;Nvidia P102-100 &lt;/a&gt;10GB, AMD Ryzen &lt;a href="https://www.techpowerup.com/cpu-specs/ryzen-7-6800h.c2527"&gt;6800H CPU&lt;/a&gt;, 64gb DDR5 RAM with &lt;a href="https://www.techpowerup.com/gpu-specs/radeon-680m.c3871"&gt;iGPU 680M&lt;/a&gt; and AMD&lt;a href="https://www.techpowerup.com/gpu-specs/radeon-rx-7900-gre.c4166"&gt; Radeon 7900 GRE&lt;/a&gt; 16GB.&lt;/p&gt; &lt;p&gt;I also compared AMD vs Intel system, both running DDR4 and no difference in inference speeds.&lt;/p&gt; &lt;p&gt;This model is too big to fit on any of my GPUs Vram, so I used dual Nvidia GPU and RPC to avoid having CPU offloading. Also did some CPU offloading to compare. All system run with Vulkan backend.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-bench -m /Nemotron-3-Nano-30B-A3B-Q4_K_M.gguf -fa 0,1 load_backend: loaded RPC backend from /home/czar33/vulkan/llama-b7476/libggml-rpc.so ggml_vulkan: Found 1 Vulkan devices: ggml_vulkan: 0 = AMD Radeon Graphics (RADV REMBRANDT) (radv) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 32 | shared memory: 65536 | int dot: 1 | matrix cores: none load_backend: loaded Vulkan backend from /home/czar33/vulkan/llama-b7476/libggml-vulkan.so load_backend: loaded CPU backend from /home/czar33/vulkan/llama-b7476/libggml-cpu-haswell.so &lt;/code&gt;&lt;/pre&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;fa&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;nemotron_h_moe 31B.A3.5B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;22.88 GiB&lt;/td&gt; &lt;td align="left"&gt;31.58 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;221.68 ± 0.90&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;nemotron_h_moe 31B.A3.5B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;22.88 GiB&lt;/td&gt; &lt;td align="left"&gt;31.58 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;15.35 ± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;nemotron_h_moe 31B.A3.5B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;22.88 GiB&lt;/td&gt; &lt;td align="left"&gt;31.58 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;214.63 ± 0.78&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;nemotron_h_moe 31B.A3.5B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;22.88 GiB&lt;/td&gt; &lt;td align="left"&gt;31.58 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;15.39 ± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;build: cdbada8d1 (7476) real 2m59.672s&lt;/p&gt; &lt;p&gt;&lt;strong&gt;6800H iGPU 680M&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Nemotron-3-Nano-30B-A3B-Q4_K_M.gguf &lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;221.68 ± 0.90&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;15.35 ± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Nemotron-3-Nano-30B-A3B-IQ4_XS.gguf 6800H iGPU 680M&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;151.09 ± 1.88&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;17.63 ± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Nemotron-3-Nano-30B-A3B-Q4_1.gguf 6800H iGPU 680M&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;241.15 ± 1.06&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;12.77 ± 3.98&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Looks like the iGPU 680M likes Q4_1 quants for best pp512 performance and IQ4_XS for tg128.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;NVIDIA GTX-1080Ti and NVIDIA P102-100&lt;/strong&gt; (21GB of combined VRAM)&lt;/p&gt; &lt;pre&gt;&lt;code&gt;ggml_vulkan: 0 = NVIDIA GeForce GTX 1080 Ti (NVIDIA) | uma: 0 | fp16: 0 | bf16: 0 | warp size: 32 | shared memory: 49152 | int dot: 1 | matrix cores: none ggml_vulkan: 1 = NVIDIA P102-100 (NVIDIA) | uma: 0 | fp16: 0 | bf16: 0 | warp size: 32 | shared memory: 49152 | int dot: 1 | matrix cores: none load_backend: loaded Vulkan backend from /home/czar33/vulkan/llama-b7484/libggml-vulkan.so load_backend: loaded CPU backend from /home/czar33/vulkan/llama-b7484/libggml-cpu-haswell.so | model | size | params | backend | ngl | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: | | nemotron_h_moe 31B.A3.5B IQ4_XS - 4.25 bpw | 16.91 GiB | 31.58 B | Vulkan | 99 | pp512 | 121.23 ± 2.85 | | nemotron_h_moe 31B.A3.5B IQ4_XS - 4.25 bpw | 16.91 GiB | 31.58 B | Vulkan | 99 | tg128 | 64.86 ± 0.15 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;build: ce734a8a2 (7484)&lt;/p&gt; &lt;p&gt;Nemotron-3-Nano-30B-A3B-IQ4_XS.gguf (16.91 GiB)&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;121.23 ± 2.85&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;64.86 ± 0.15&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Nemotron-3-Nano-30B-A3B-Q4_1.gguf (18.67 GiB)&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;133.86 ± 2.44&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;67.99 ± 0.25&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Nemotron-3-Nano-30B-A3B-Q4_K_M.gguf -ngl 44 (22.88 GiB)&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;103.30 ± 0.51&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;34.05 ± 0.92&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Q4_K_M too big for 21GB VRAM so needs &lt;code&gt;-ngl 44&lt;/code&gt; to run and almost a 50% hit for about 1 to 2 GB offload.&lt;/p&gt; &lt;p&gt;Now lets see difference between offload &lt;code&gt;-ngl&lt;/code&gt; and using RPC backend. Using Q4_K_M, Q5_K_M and Q6_K models.&lt;/p&gt; &lt;p&gt;My client is the AMD Radeon 7900 GRE 16GB VRAM GPU:&lt;/p&gt; &lt;p&gt;&lt;code&gt;llama-bench -m /Nemotron-3-Nano-30B-A3B-Q5_K_M.gguf --rpc&lt;/code&gt; &lt;a href="http://10.0.0.173:50054"&gt;&lt;code&gt;10.0.0.173:50054&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;and the RPC-SERVER is running dual GPU GTX-1080Ti/P102-100 on a gigabit network.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-b7491/rpc-server -c --host 0.0.0.0 --port 50054 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;RX 7900GRE (16GB VRAM), GTX1080Ti + P102-100 (21GB VRAM) using RPC&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;time /llama-b7491/llama-bench -m /Nemotron-3-Nano-30B-A3B-Q5_K_M.gguf --rpc &lt;a href="http://10.0.0.173:50054"&gt;10.0.0.173:50054&lt;/a&gt; &lt;/p&gt; &lt;pre&gt;&lt;code&gt;load_backend: loaded RPC backend from /media/czar33/x_2tb/vulkan/llama-b7491/libggml-rpc.so ggml_vulkan: Found 1 Vulkan devices: ggml_vulkan: 0 = AMD Radeon RX 7900 GRE (RADV NAVI31) (radv) | uma: 0 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 1 | matrix c ores: KHR_coopmat load_backend: loaded Vulkan backend from /media/czar33/x_2tb/vulkan/llama-b7491/libggml-vulkan.so load_backend: loaded CPU backend from /media/czar33/x_2tb/vulkan/llama-b7491/libggml-cpu-haswell.so | model | size | params | backend | ngl | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: | | nemotron_h_moe 31B.A3.5B Q5_K - Medium | 24.35 GiB | 31.58 B | Vulkan,RPC | 99 | pp512 | 112.32 ± 1.81 | | nemotron_h_moe 31B.A3.5B Q5_K - Medium | 24.35 GiB | 31.58 B | Vulkan,RPC | 99 | tg128 | 40.79 ± 0.22 | build: 52ab19df6 (7491) real 2m28.029s &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Nemotron-3-Nano-30B-A3B-Q4_K_M.gguf (22.88 GiB)&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;112.04 ± 1.89&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;41.46 ± 0.12&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Nemotron-3-Nano-30B-A3B-Q5_K_M.gguf (24.35 GiB)&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;112.32 ± 1.81&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;40.79 ± 0.22&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Nemotron-3-Nano-30B-A3B-Q6_K.gguf (31.20 GiB)&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;113.58 ± 1.70&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;39.95 ± 0.76&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;COMPARED to -ngl offloading on NVIDIA GTX-1080Ti and P102-100 (21GB VRAM) at Q6_K&lt;/p&gt; &lt;p&gt;Nemotron-3-Nano-30B-A3B-Q6_K.gguf -ngl 30&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;82.68 ± 0.62&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;21.78 ± 0.79&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;I'm impressed on being able to run the Q6_K model at a very respectable speed across 2 system and 3 GPUs.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tabletuser_blogspot"&gt; /u/tabletuser_blogspot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1prxpcx/nvidia_nemotron3nano30b_llm_benchmarks_vulkan_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1prxpcx/nvidia_nemotron3nano30b_llm_benchmarks_vulkan_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1prxpcx/nvidia_nemotron3nano30b_llm_benchmarks_vulkan_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-21T04:31:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ps26tq</id>
    <title>Video2Robot — turn any video (or Veo/Sora prompt) into humanoid robot motion</title>
    <updated>2025-12-21T09:00:34+00:00</updated>
    <author>
      <name>/u/freesysck</name>
      <uri>https://old.reddit.com/user/freesysck</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ps26tq/video2robot_turn_any_video_or_veosora_prompt_into/"&gt; &lt;img alt="Video2Robot — turn any video (or Veo/Sora prompt) into humanoid robot motion" src="https://b.thumbs.redditmedia.com/ngi9XINVJUwQcnKQJtZJJZvsFS_qFXuG2mLEME3bH7w.jpg" title="Video2Robot — turn any video (or Veo/Sora prompt) into humanoid robot motion" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/5r4ippkdvi8g1.png?width=1290&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=30053ed178ae01fbbdd17a24e2d89783283ea13e"&gt;https://preview.redd.it/5r4ippkdvi8g1.png?width=1290&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=30053ed178ae01fbbdd17a24e2d89783283ea13e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;End-to-end pipeline: Video/Prompt → Pose (PromptHMR) → Motion Retargeting (GMR) → Robot. Ships CLI + Web UI, 3D viz, and support for Unitree G1/H1 &amp;amp; Booster T1. &lt;/p&gt; &lt;p&gt;Works with Veo/Sora or your own .mp4&lt;/p&gt; &lt;p&gt;Repo &amp;amp; README: github.com/AIM-Intelligence/video2robot. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/freesysck"&gt; /u/freesysck &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ps26tq/video2robot_turn_any_video_or_veosora_prompt_into/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ps26tq/video2robot_turn_any_video_or_veosora_prompt_into/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ps26tq/video2robot_turn_any_video_or_veosora_prompt_into/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-21T09:00:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1ps5n5l</id>
    <title>Open source library Kreuzberg v4.0.0-rc14 released: optimization phase and v4 release ahead</title>
    <updated>2025-12-21T12:41:08+00:00</updated>
    <author>
      <name>/u/Eastern-Surround7763</name>
      <uri>https://old.reddit.com/user/Eastern-Surround7763</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We’ve released Kreuzberg v4.0.0-rc14, now working across all release channels (language bindings for Rust, Python, Ruby, Go, and TypeScript/Node.js, plus Docker and CLI). As an open-source library, Kreuzberg provides a self-hosted alternative with no per-document API costs, making it suitable for high-volume workloads where cost efficiency matters.&lt;/p&gt; &lt;p&gt;Development focus is now shifting to performance optimization, like profiling and improving bindings, followed by comparative benchmarks and a documentation refresh.&lt;/p&gt; &lt;p&gt;If you have a chance to test rc14, we’d be happy to receive any feedback- bugs, encouragement, design critique, or else- as we prepare for a stable v4 release next month. Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Eastern-Surround7763"&gt; /u/Eastern-Surround7763 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ps5n5l/open_source_library_kreuzberg_v400rc14_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ps5n5l/open_source_library_kreuzberg_v400rc14_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ps5n5l/open_source_library_kreuzberg_v400rc14_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-21T12:41:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1pry2v7</id>
    <title>People using Devstral 2 123b, how has it been working for you? What have you been using it with?</title>
    <updated>2025-12-21T04:51:16+00:00</updated>
    <author>
      <name>/u/maxwell321</name>
      <uri>https://old.reddit.com/user/maxwell321</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;People using Devstral 2 123b, how has it been working for you? What have you been using it with? &lt;/p&gt; &lt;p&gt;I tried it with Claude Code Router and it's not bad! I think just with a few rough tests it seems better at agentic stuff than GPT OSS 120b, however GPT OSS's code quality seems a bit better. HOWEVER, I'm using OSS 120b at Q4 and Devstral at IQ3. &lt;/p&gt; &lt;p&gt;GPT OSS 120b is also faster because it's MoE, but Devstral 2 123b works pretty well with speculative decoding with a heavily quantized Devstral 2 20b. &lt;/p&gt; &lt;p&gt;How is your luck with it? What strengths and weaknesses does it have with your experience?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/maxwell321"&gt; /u/maxwell321 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pry2v7/people_using_devstral_2_123b_how_has_it_been/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pry2v7/people_using_devstral_2_123b_how_has_it_been/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pry2v7/people_using_devstral_2_123b_how_has_it_been/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-21T04:51:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1ps2h9r</id>
    <title>I built an open source voice assistant that runs Whisper + Qwen 2.5 entirely in the browser via WASM</title>
    <updated>2025-12-21T09:19:40+00:00</updated>
    <author>
      <name>/u/muthukrishnan749</name>
      <uri>https://old.reddit.com/user/muthukrishnan749</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been experimenting with running a full voice assistant pipeline in the browser – no server, no API calls, everything local.&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1ps2h9r/video/i4vm3hmnyi8g1/player"&gt;https://reddit.com/link/1ps2h9r/video/i4vm3hmnyi8g1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Live demo: &lt;a href="https://ava.muthu.co"&gt;https://ava.muthu.co&lt;/a&gt;&lt;br /&gt; Source: &lt;a href="https://github.com/muthuspark/ava"&gt;https://github.com/muthuspark/ava&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The stack:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;STT: Whisper tiny-en (q5_1, ~31MB) via whisper-web-transcriber&lt;/li&gt; &lt;li&gt;LLM: Qwen 2.5 0.5B Instruct (q4_k_m, ~350MB) via Wllama (llama.cpp WASM port)&lt;/li&gt; &lt;li&gt;TTS: Native browser SpeechSynthesis API&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;How it works:&lt;br /&gt; The pipeline streams – as the LLM generates tokens, I detect sentence boundaries and queue them for TTS immediately. So it starts speaking before the full response is ready.&lt;/p&gt; &lt;p&gt;Performance (on my machine):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Whisper inference: ~0.3-0.5s&lt;/li&gt; &lt;li&gt;LLM inference: ~1-2s for short responses&lt;/li&gt; &lt;li&gt;End-to-end latency: ~2-3s&lt;/li&gt; &lt;li&gt;Memory: 500MB-1GB during operation&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Limitations:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Doesn't work on mobile yet&lt;/li&gt; &lt;li&gt;Chrome/Edge only (needs SharedArrayBuffer)&lt;/li&gt; &lt;li&gt;0.5B model is pretty limited in capability&lt;/li&gt; &lt;li&gt;English only&lt;/li&gt; &lt;li&gt;First load is ~380MB (cached after)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I chose Qwen 2.5 0.5B because it's the sweet spot between &amp;quot;runs in a browser&amp;quot; and &amp;quot;somewhat coherent responses.&amp;quot; Tried smaller models but they were unusable.&lt;/p&gt; &lt;p&gt;Curious if anyone has suggestions for:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Better small models that work well with llama.cpp WASM&lt;/li&gt; &lt;li&gt;Ways to reduce the initial load time&lt;/li&gt; &lt;li&gt;Improving Whisper accuracy without going to a larger model&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/muthukrishnan749"&gt; /u/muthukrishnan749 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ps2h9r/i_built_an_open_source_voice_assistant_that_runs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ps2h9r/i_built_an_open_source_voice_assistant_that_runs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ps2h9r/i_built_an_open_source_voice_assistant_that_runs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-21T09:19:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1ps1g40</id>
    <title>Benchmark Winners Across 40+ LLM Evaluations: Patterns Without Recommendations</title>
    <updated>2025-12-21T08:11:49+00:00</updated>
    <author>
      <name>/u/abubakkar_s</name>
      <uri>https://old.reddit.com/user/abubakkar_s</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I kept seeing the same question everywhere: &lt;em&gt;“Which LLM is best?”&lt;/em&gt;&lt;/p&gt; &lt;p&gt;So instead of opinions, I went the boring route — &lt;strong&gt;I collected benchmark winners across a wide range of tasks&lt;/strong&gt;: reasoning, math, coding, vision, OCR, multimodal QA, and real-world evaluations. For SLM (3B-25B).&lt;/p&gt; &lt;p&gt;This post is &lt;strong&gt;not a recommendation list&lt;/strong&gt;. It’s simply what the benchmarks show when you look at &lt;strong&gt;task-by-task winners&lt;/strong&gt; instead of a single leaderboard.&lt;/p&gt; &lt;p&gt;You can decide what matters &lt;em&gt;for your use case&lt;/em&gt;.&lt;/p&gt; &lt;h1&gt;Benchmark → Top Scoring Model&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Benchmark&lt;/th&gt; &lt;th align="left"&gt;Best Model&lt;/th&gt; &lt;th align="left"&gt;Score&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;AI2D&lt;/td&gt; &lt;td align="left"&gt;Qwen3-VL-8B-Instruct&lt;/td&gt; &lt;td align="left"&gt;85%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;AIME-2024&lt;/td&gt; &lt;td align="left"&gt;Ministral3-8B-Reasoning-2512&lt;/td&gt; &lt;td align="left"&gt;86%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;ARC-C&lt;/td&gt; &lt;td align="left"&gt;LLaMA-3.1-8B-Instruct&lt;/td&gt; &lt;td align="left"&gt;83%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Arena-Hard&lt;/td&gt; &lt;td align="left"&gt;Phi-4-Reasoning-Plus&lt;/td&gt; &lt;td align="left"&gt;79%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;BFCL-v3&lt;/td&gt; &lt;td align="left"&gt;Qwen3-VL-4B-Thinking&lt;/td&gt; &lt;td align="left"&gt;67%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;BigBench-Hard&lt;/td&gt; &lt;td align="left"&gt;Gemma-3-12B&lt;/td&gt; &lt;td align="left"&gt;85%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;ChartQA&lt;/td&gt; &lt;td align="left"&gt;Qwen2.5-Omni-7B&lt;/td&gt; &lt;td align="left"&gt;85%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;CharXiv-R&lt;/td&gt; &lt;td align="left"&gt;Qwen3-VL-8B-Thinking&lt;/td&gt; &lt;td align="left"&gt;53%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;DocVQA&lt;/td&gt; &lt;td align="left"&gt;Qwen2.5-Omni-7B&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;95%&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;DROP (Reasoning)&lt;/td&gt; &lt;td align="left"&gt;Gemma-3n-E2B&lt;/td&gt; &lt;td align="left"&gt;61%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPQA&lt;/td&gt; &lt;td align="left"&gt;Qwen3-VL-8B-Thinking&lt;/td&gt; &lt;td align="left"&gt;70%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GSM8K&lt;/td&gt; &lt;td align="left"&gt;Gemma-3-12B&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;91%&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;HellaSwag&lt;/td&gt; &lt;td align="left"&gt;Mistral-NeMo-12B-Instruct&lt;/td&gt; &lt;td align="left"&gt;83%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;HumanEval&lt;/td&gt; &lt;td align="left"&gt;Granite-3.3-8B-Instruct&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;89%&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Humanity’s Last Exam&lt;/td&gt; &lt;td align="left"&gt;GPT-OSS-20B&lt;/td&gt; &lt;td align="left"&gt;11%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IfEval&lt;/td&gt; &lt;td align="left"&gt;Nemotron-Nano-9B-v2&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;90%&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;LiveCodeBench&lt;/td&gt; &lt;td align="left"&gt;Nemotron-Nano-9B-v2&lt;/td&gt; &lt;td align="left"&gt;71%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;LiveCodeBench-v6&lt;/td&gt; &lt;td align="left"&gt;Qwen3-VL-8B-Thinking&lt;/td&gt; &lt;td align="left"&gt;58%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Math&lt;/td&gt; &lt;td align="left"&gt;Ministral3-8B&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;90%&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Math-500&lt;/td&gt; &lt;td align="left"&gt;Nemotron-Nano-9B-v2&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;97%&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MathVista&lt;/td&gt; &lt;td align="left"&gt;Qwen2.5-Omni-7B&lt;/td&gt; &lt;td align="left"&gt;68%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MathVista-Mini&lt;/td&gt; &lt;td align="left"&gt;Qwen3-VL-8B-Thinking&lt;/td&gt; &lt;td align="left"&gt;81%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MBPP (Python)&lt;/td&gt; &lt;td align="left"&gt;Qwen2.5-Coder-7B-Instruct&lt;/td&gt; &lt;td align="left"&gt;80%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MGSM&lt;/td&gt; &lt;td align="left"&gt;Gemma-3n-E4B-Instruct&lt;/td&gt; &lt;td align="left"&gt;67%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MM-MT-Bench&lt;/td&gt; &lt;td align="left"&gt;Qwen3-VL-8B-Thinking&lt;/td&gt; &lt;td align="left"&gt;80%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MMLU&lt;/td&gt; &lt;td align="left"&gt;Qwen2.5-Omni-7B&lt;/td&gt; &lt;td align="left"&gt;59%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MMLU-Pro&lt;/td&gt; &lt;td align="left"&gt;Qwen3-VL-8B-Thinking&lt;/td&gt; &lt;td align="left"&gt;77%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MMLU-Pro-X&lt;/td&gt; &lt;td align="left"&gt;Qwen3-VL-8B-Thinking&lt;/td&gt; &lt;td align="left"&gt;70%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MMLU-Redux&lt;/td&gt; &lt;td align="left"&gt;Qwen3-VL-8B-Thinking&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;89%&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MMMLU&lt;/td&gt; &lt;td align="left"&gt;Phi-3.5-Mini-Instruct&lt;/td&gt; &lt;td align="left"&gt;55%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MMMU-Pro&lt;/td&gt; &lt;td align="left"&gt;Qwen3-VL-8B-Thinking&lt;/td&gt; &lt;td align="left"&gt;60%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MMStar&lt;/td&gt; &lt;td align="left"&gt;Qwen3-VL-4B-Thinking&lt;/td&gt; &lt;td align="left"&gt;75%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Multi-IF&lt;/td&gt; &lt;td align="left"&gt;Qwen3-VL-8B-Thinking&lt;/td&gt; &lt;td align="left"&gt;75%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;OCRBench&lt;/td&gt; &lt;td align="left"&gt;Qwen3-VL-8B-Instruct&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;90%&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;RealWorldQA&lt;/td&gt; &lt;td align="left"&gt;Qwen3-VL-8B-Thinking&lt;/td&gt; &lt;td align="left"&gt;73%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;ScreenSpot-Pro&lt;/td&gt; &lt;td align="left"&gt;Qwen3-VL-4B-Instruct&lt;/td&gt; &lt;td align="left"&gt;59%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;SimpleQA&lt;/td&gt; &lt;td align="left"&gt;Qwen3-VL-8B-Thinking&lt;/td&gt; &lt;td align="left"&gt;50%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;SuperGPQA&lt;/td&gt; &lt;td align="left"&gt;Qwen3-VL-8B-Thinking&lt;/td&gt; &lt;td align="left"&gt;51%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;SWE-Bench-Verified&lt;/td&gt; &lt;td align="left"&gt;Devstral-Small-2&lt;/td&gt; &lt;td align="left"&gt;56%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;TAU-Bench-Retail&lt;/td&gt; &lt;td align="left"&gt;GPT-OSS-20B&lt;/td&gt; &lt;td align="left"&gt;55%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;WinoGrande&lt;/td&gt; &lt;td align="left"&gt;Gemma-2-9B&lt;/td&gt; &lt;td align="left"&gt;80%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Patterns I Noticed (Not Conclusions)&lt;/h1&gt; &lt;h1&gt;1. No Single Model Dominates Everything&lt;/h1&gt; &lt;p&gt;Even models that appear frequently don’t win across all categories. Performance is &lt;strong&gt;highly task-dependent&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;If you’re evaluating models based on one benchmark, you’re probably overfitting your expectations.&lt;/p&gt; &lt;h1&gt;2. Mid-Sized Models (7B–9B) Show Up Constantly&lt;/h1&gt; &lt;p&gt;Across math, coding, and multimodal tasks, &lt;strong&gt;sub-10B models appear repeatedly&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;That doesn’t mean they’re “better” — it does suggest &lt;strong&gt;architecture and tuning matter more than raw size&lt;/strong&gt; in many evaluations.&lt;/p&gt; &lt;h1&gt;3. Vision-Language Models Are No Longer “Vision Only”&lt;/h1&gt; &lt;p&gt;Several VL models score competitively on:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;reasoning&lt;/li&gt; &lt;li&gt;OCR&lt;/li&gt; &lt;li&gt;document understanding&lt;/li&gt; &lt;li&gt;multimodal knowledge&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;That gap is clearly shrinking, at least in benchmark settings.&lt;/p&gt; &lt;h1&gt;4. Math, Code, and Reasoning Still Behave Differently&lt;/h1&gt; &lt;p&gt;Models that do extremely well on:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Math (AIME, Math-500) often aren’t the same ones winning:&lt;/li&gt; &lt;li&gt;HumanEval or LiveCodeBench&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So “reasoning” is not one thing — benchmarks expose different failure modes.&lt;/p&gt; &lt;h1&gt;5. Large Parameter Count ≠ Guaranteed Wins&lt;/h1&gt; &lt;p&gt;Some larger models appear rarely or only in narrow benchmarks.&lt;/p&gt; &lt;p&gt;That doesn’t make them bad — it just reinforces that &lt;strong&gt;benchmarks reward specialization&lt;/strong&gt;, not general scale.&lt;/p&gt; &lt;h1&gt;Why I’m Sharing This&lt;/h1&gt; &lt;p&gt;I’m not trying to say &lt;em&gt;“this model is the best”&lt;/em&gt;. I wanted a &lt;strong&gt;task-first view&lt;/strong&gt;, because that’s how most of us actually use models:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Some of you care about math&lt;/li&gt; &lt;li&gt;Some about code&lt;/li&gt; &lt;li&gt;Some about OCR, docs, or UI grounding&lt;/li&gt; &lt;li&gt;Some about overall multimodal behavior&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Benchmarks won’t replace real-world testing — but they &lt;em&gt;do&lt;/em&gt; reveal patterns when you zoom out.&lt;/p&gt; &lt;h1&gt;Open Questions for You&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Which benchmarks do &lt;em&gt;you&lt;/em&gt; trust the most?&lt;/li&gt; &lt;li&gt;Which ones do you think are already being “over-optimized”?&lt;/li&gt; &lt;li&gt;Are there important real-world tasks you feel aren’t reflected here?&lt;/li&gt; &lt;li&gt;Do you trust &lt;strong&gt;single-score leaderboards&lt;/strong&gt;, or do you prefer task-specific evaluations like the breakdown above?&lt;/li&gt; &lt;li&gt;For people running models locally, how much weight do you personally give to &lt;strong&gt;efficiency metrics (latency, VRAM, throughput)&lt;/strong&gt; versus raw benchmark scores? (Currently am with V100, which is cloud based)&lt;/li&gt; &lt;li&gt;If you had to remove &lt;strong&gt;one benchmark entirely&lt;/strong&gt;, which one do you think adds the least signal today?&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abubakkar_s"&gt; /u/abubakkar_s &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ps1g40/benchmark_winners_across_40_llm_evaluations/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ps1g40/benchmark_winners_across_40_llm_evaluations/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ps1g40/benchmark_winners_across_40_llm_evaluations/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-21T08:11:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1prjzoh</id>
    <title>Xiaomi’s MiMo-V2-Flash (309B model) jumping straight to the big leagues</title>
    <updated>2025-12-20T17:39:17+00:00</updated>
    <author>
      <name>/u/98Saman</name>
      <uri>https://old.reddit.com/user/98Saman</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1prjzoh/xiaomis_mimov2flash_309b_model_jumping_straight/"&gt; &lt;img alt="Xiaomi’s MiMo-V2-Flash (309B model) jumping straight to the big leagues" src="https://preview.redd.it/uew3kkt2be8g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c74cf3336896a9944ca78303c3bc0e948e805302" title="Xiaomi’s MiMo-V2-Flash (309B model) jumping straight to the big leagues" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/98Saman"&gt; /u/98Saman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/uew3kkt2be8g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1prjzoh/xiaomis_mimov2flash_309b_model_jumping_straight/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1prjzoh/xiaomis_mimov2flash_309b_model_jumping_straight/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-20T17:39:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1prwhb1</id>
    <title>is it a good deal? 64GB VRAM @ 1,058 USD</title>
    <updated>2025-12-21T03:25:47+00:00</updated>
    <author>
      <name>/u/bohemianLife1</name>
      <uri>https://old.reddit.com/user/bohemianLife1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1prwhb1/is_it_a_good_deal_64gb_vram_1058_usd/"&gt; &lt;img alt="is it a good deal? 64GB VRAM @ 1,058 USD" src="https://preview.redd.it/sr0bemp03h8g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3483188d964c72719dc2a2e2c6a49ccdbdf22371" title="is it a good deal? 64GB VRAM @ 1,058 USD" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This Black Friday, I found an Nvidia Jetson AGX Orin 64GB developer kit for $1,058. It usually goes for $2,000, and if you're in India like I am, it retails around $2,370.61. For comparison, the 5090, which is a 32GB card, costs $2,000 right now.&lt;/p&gt; &lt;p&gt;A little background: in my &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1p9iawm/looking_for_open_source_10b_model_that_is/"&gt;previous post&lt;/a&gt;, I asked the community which open-source model I could use locally to achieve similar performance to GPT-4o-mini with a 16GB VRAM constraint, and the unanimous conclusion was that more VRAM is required.&lt;/p&gt; &lt;p&gt;So I began my search and found this deal (&lt;a href="https://www.amazon.com/NVIDIA-Jetson-Orin-64GB-Developer/dp/B0BYGB3WV4?crid=1Y0GEMVGIT2Y1&amp;amp;dib=eyJ2IjoiMSJ9.FoThX8FZ94bjsnPOOKsYeOU_z7hyFtfGlHRIhkasZV2n3k3fXTbvCidX2BS21F6ho6cCeKibmPpVZ__v6ESMpAPJV0GrTdf9P_Os4hVMzc0ACZbLbOAe6eGI_zkvEeb4kGLxv1F4I1PCp1dryARl0-d4TyqvQtQqJGFMyqcSpEX3yq317tO2ns0-i1_F45_RSj8ia8hONnO1csZjWVJl5MP-QwhkOIy5HVrbgz__9mc.rmCLen5N1BOvx4gErZosaBavA_JDBwagBfDOxG0vdBY&amp;amp;dib_tag=se&amp;amp;keywords=nvidia%2Bjetson%2Bagx%2Borin%2B64gb&amp;amp;qid=1766285654&amp;amp;sprefix=nvidia%2Bjetson%2Bagx%2Caps%2C488&amp;amp;sr=8-1&amp;amp;th=1"&gt;out of stock now&lt;/a&gt;) and asked someone from the US to buy it and bring it to India.&lt;/p&gt; &lt;p&gt;The reason for this purchase: I've built an AI Voice Agent platform that handles pre-sales and post-sales for any company. This voice pipeline runs on three models in a cascading fashion: (VAD + Turn Detection) → STT → LLM → TTS. Since I need to host multiple models, VRAM is a bigger constraint than processing power.&lt;/p&gt; &lt;p&gt;So, instead of a consumer card like the 5090 (32GB), which offers great processing power, I ended up purchasing the Jetson AGX Orin (64GB).&lt;/p&gt; &lt;p&gt;I'll continue the chain of posting with my results of running voice agents specific models on this machine. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bohemianLife1"&gt; /u/bohemianLife1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sr0bemp03h8g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1prwhb1/is_it_a_good_deal_64gb_vram_1058_usd/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1prwhb1/is_it_a_good_deal_64gb_vram_1058_usd/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-21T03:25:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1prw988</id>
    <title>GLM 4.7 imminent?!</title>
    <updated>2025-12-21T03:13:27+00:00</updated>
    <author>
      <name>/u/JuicyLemonMango</name>
      <uri>https://old.reddit.com/user/JuicyLemonMango</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/zRzRzRzRzRzRzR"&gt;https://github.com/zRzRzRzRzRzRzR&lt;/a&gt;, a &lt;a href="http://z.ai"&gt;z.ai&lt;/a&gt; employee, appears hard at work to implement GLM 4.7 support. It's added in vLLM already.&lt;/p&gt; &lt;p&gt;What are your expectations for this, to be announced, new model? I'm both very optimistic and a little cautious at the same time.&lt;/p&gt; &lt;p&gt;Earlier in the year they, GLM itself on twitter, said that version 5.0 would be released this year. Now all i see is 4.7 which kinda gives me a feeling of the model potentially not being as great of an update as they had hoped to be. I don't think they'll top all the SOTA models in the benchmarks but i do think they will come within reach again. Say in the top 10. That's just pure wishful thinking and speculation at this point.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JuicyLemonMango"&gt; /u/JuicyLemonMango &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1prw988/glm_47_imminent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1prw988/glm_47_imminent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1prw988/glm_47_imminent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-21T03:13:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1pruoy7</id>
    <title>How big do we think Gemini 3 flash is</title>
    <updated>2025-12-21T01:51:33+00:00</updated>
    <author>
      <name>/u/davikrehalt</name>
      <uri>https://old.reddit.com/user/davikrehalt</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hopefully the relevance to open models is clear enough. I'm curious about speculations based on speed and other things how big this model is--because it can help us understand just how strong a model something like 512Gb mac ultra can run eventually or something like 128Gb macbook. Do we think it's something that can fit in memory in a 128Gb MacBook for example?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/davikrehalt"&gt; /u/davikrehalt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pruoy7/how_big_do_we_think_gemini_3_flash_is/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pruoy7/how_big_do_we_think_gemini_3_flash_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pruoy7/how_big_do_we_think_gemini_3_flash_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-21T01:51:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ps4jho</id>
    <title>llama.cpp - useful flags - share your thoughts please</title>
    <updated>2025-12-21T11:34:38+00:00</updated>
    <author>
      <name>/u/mossy_troll_84</name>
      <uri>https://old.reddit.com/user/mossy_troll_84</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey Guys, I am new here.&lt;/p&gt; &lt;p&gt;Yesterday I have compiled llama.cpp with flag &lt;strong&gt;GGML_CUDA_ENABLE_UNIFIED_MEMORY=1&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;As a results that increase llm's perormace by aprox &lt;strong&gt;10-15%.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Here is the command I have used:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;cmake -B build -DGGML_CUDA=ON -DCMAKE_CUDA_ARCHITECTURES=&amp;quot;120&amp;quot; GGML_CUDA_ENABLE_UNIFIED_MEMORY=1&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;cmake --build build --config Release -j 32&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I was wondering if you also use some flags which can improve my llama.cpp performance even further.&lt;/p&gt; &lt;p&gt;Just an example:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;gpt-oss-120b - previously 36 tokens/sec to 46 tokens/sec&lt;/li&gt; &lt;li&gt;Qwen3-VL-235B-A22B-Instruct-Q4_K_M - previously 5,3 tokens/sec to 8,9 tokens/sec. All with maximum context window available for each llm model.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please let me know if you have any tricks here which I can use.&lt;/p&gt; &lt;p&gt;FYI - here is my spec: Ryzen 9 9950X3D, RTX 5090, 128 GB DDR 5 - &lt;strong&gt;Arch Linux&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;UPDATE:&lt;/strong&gt; As one of colleagues comments (and he is right): &lt;em&gt;This is he environment variable `GGML_CUDA_ENABLE_UNIFIED_MEMORY=1` can be used to enable unified memory in Linux in command. This allows swapping to system RAM instead of crashing when the GPU VRAM is exhausted. In Windows this setting is available in the NVIDIA control panel as `System Memory Fallback`&lt;/em&gt;- on my side in Arch linux however that worked also during compiling and increased speed (dont know why) then after the comment I have just added to command ind its speed up gpt-oss-120b even more to 56 tokens per second&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mossy_troll_84"&gt; /u/mossy_troll_84 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ps4jho/llamacpp_useful_flags_share_your_thoughts_please/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ps4jho/llamacpp_useful_flags_share_your_thoughts_please/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ps4jho/llamacpp_useful_flags_share_your_thoughts_please/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-21T11:34:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1ps0jnm</id>
    <title>MiniMax 2.1 release?</title>
    <updated>2025-12-21T07:14:03+00:00</updated>
    <author>
      <name>/u/_cttt_</name>
      <uri>https://old.reddit.com/user/_cttt_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ps0jnm/minimax_21_release/"&gt; &lt;img alt="MiniMax 2.1 release?" src="https://preview.redd.it/5rotyw06ci8g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6746b1a77ec85f43c907ff7b7bce5a918251fc32" title="MiniMax 2.1 release?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;new here and just saw the release of MiniMax M2.1, how is it compare to the other models?&lt;/p&gt; &lt;p&gt;github: &lt;a href="https://github.com/vllm-project/recipes/pull/174"&gt;https://github.com/vllm-project/recipes/pull/174&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_cttt_"&gt; /u/_cttt_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5rotyw06ci8g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ps0jnm/minimax_21_release/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ps0jnm/minimax_21_release/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-21T07:14:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1pp9w31</id>
    <title>AMA with the Meta researchers behind SAM 3 + SAM 3D + SAM Audio</title>
    <updated>2025-12-17T22:18:01+00:00</updated>
    <author>
      <name>/u/AIatMeta</name>
      <uri>https://old.reddit.com/user/AIatMeta</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLlama"&gt;r/LocalLlama&lt;/a&gt;! We’re the research team behind the newest members of the Segment Anything collection of models: SAM 3 + SAM 3D + SAM Audio.&lt;/p&gt; &lt;p&gt;We’re excited to be here to talk all things SAM (sorry, we can’t share details on other projects or future work) and have members from across our team participating:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;SAM 3 (&lt;/strong&gt;&lt;a href="https://ai.meta.com/blog/segment-anything-model-3/?utm_source=reddit&amp;amp;utm_medium=organic_social&amp;amp;utm_content=ama&amp;amp;utm_campaign=sam"&gt;&lt;strong&gt;learn more&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Nikhila Ravi&lt;/li&gt; &lt;li&gt;Pengchuan Zhang&lt;/li&gt; &lt;li&gt;Shoubhik Debnath&lt;/li&gt; &lt;li&gt;Chay Ryali&lt;/li&gt; &lt;li&gt;Yuan-Ting Hu&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;SAM 3D (&lt;/strong&gt;&lt;a href="https://ai.meta.com/blog/sam-3d/?utm_source=reddit&amp;amp;utm_medium=organic_social&amp;amp;utm_content=ama&amp;amp;utm_campaign=sam"&gt;&lt;strong&gt;learn more&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Weiyao Wang&lt;/li&gt; &lt;li&gt;Sasha Sax&lt;/li&gt; &lt;li&gt;Xitong Yang&lt;/li&gt; &lt;li&gt;Jinkun Cao&lt;/li&gt; &lt;li&gt;Michelle Guo&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;SAM Audio (&lt;/strong&gt;&lt;a href="https://ai.meta.com/blog/sam-audio/?utm_source=reddit&amp;amp;utm_medium=organic_social&amp;amp;utm_content=ama&amp;amp;utm_campaign=sam"&gt;&lt;strong&gt;learn more&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Bowen Shi&lt;/li&gt; &lt;li&gt;Andros Tjandra&lt;/li&gt; &lt;li&gt;John Hoffman&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can try SAM Audio, SAM 3D, and SAM 3 in the Segment Anything Playground: &lt;a href="https://go.meta.me/87b53b"&gt;https://go.meta.me/87b53b&lt;/a&gt; &lt;/p&gt; &lt;p&gt;PROOF: &lt;a href="https://x.com/AIatMeta/status/2001429429898407977"&gt;https://x.com/AIatMeta/status/2001429429898407977&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;EDIT: Thanks to everyone who joined the AMA and for all the great conversation. We look forward to the next one!&lt;/strong&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AIatMeta"&gt; /u/AIatMeta &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp9w31/ama_with_the_meta_researchers_behind_sam_3_sam_3d/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp9w31/ama_with_the_meta_researchers_behind_sam_3_sam_3d/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pp9w31/ama_with_the_meta_researchers_behind_sam_3_sam_3d/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T22:18:01+00:00</published>
  </entry>
</feed>
