<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-18T03:49:24+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1o9h0yd</id>
    <title>It would be nice to have a super lightweight LM Studio like utility that would let you construct llama-serve command.</title>
    <updated>2025-10-17T23:51:16+00:00</updated>
    <author>
      <name>/u/NoFudge4700</name>
      <uri>https://old.reddit.com/user/NoFudge4700</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, I use LM Studio in Linux but if you run `nvtop` or `nvidia-smi` you will notice LM Studio is a VRAM eater itself. And takes more than a gig for itself. Not everyone is a llama.cpp expert and I am not either but if there existed a utility if only existed a utility that was super lightweight and would help in managing models and remembering parameters and even let us copy generated command for the settings we do via UI that would be awesome.&lt;/p&gt; &lt;p&gt;Maybe someone can vibe code it too as a fun project. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NoFudge4700"&gt; /u/NoFudge4700 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9h0yd/it_would_be_nice_to_have_a_super_lightweight_lm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9h0yd/it_would_be_nice_to_have_a_super_lightweight_lm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o9h0yd/it_would_be_nice_to_have_a_super_lightweight_lm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T23:51:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1o9du7h</id>
    <title>PlayDiffusion finetune for audio inpainting non-verbal tags</title>
    <updated>2025-10-17T21:30:28+00:00</updated>
    <author>
      <name>/u/oezi13</name>
      <uri>https://old.reddit.com/user/oezi13</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;PlayDiffusion is a 7B Apache-licensed diffusion model which can 'inpaint' audio. So you can change existing audio (slightly) by providing new text. I was curious to learn how it works and challenged myself if it was possible to make a small fine-tune which adds support for non-verbal tags such as `&amp;lt;laugh&amp;gt;` or `&amp;lt;cough&amp;gt;`.&lt;/p&gt; &lt;p&gt;After two weeks of tinkering I have support for `&amp;lt;laugh&amp;gt;`, `&amp;lt;pause&amp;gt;` and `&amp;lt;breath&amp;gt;` because there wasn't enough good training data for other tags such as `&amp;lt;cough&amp;gt;` to find easily.&lt;/p&gt; &lt;p&gt;It comes with gradio, docker or runs directly from `uvx`:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Source available here: &lt;a href="https://github.com/coezbek/PlayDiffusion"&gt;https://github.com/coezbek/PlayDiffusion&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Original PlayDiffusion: &lt;a href="https://github.com/PlayHT/playdiffusion"&gt;https://github.com/PlayHT/playdiffusion&lt;/a&gt;&lt;/li&gt; &lt;li&gt;HF Checkpoint: &lt;a href="https://huggingface.co/oezi13/PlayDiffusion-nonverbal"&gt;https://huggingface.co/oezi13/PlayDiffusion-nonverbal&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Datasets used for training: &lt;a href="https://huggingface.co/collections/oezi13/nonverbal-tts-audio-68ec1bee4163e50369424650"&gt;https://huggingface.co/collections/oezi13/nonverbal-tts-audio-68ec1bee4163e50369424650&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Note: PlayDiffusion is english only and doesn't work for all voices.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/oezi13"&gt; /u/oezi13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9du7h/playdiffusion_finetune_for_audio_inpainting/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9du7h/playdiffusion_finetune_for_audio_inpainting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o9du7h/playdiffusion_finetune_for_audio_inpainting/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T21:30:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1o9d13w</id>
    <title>Best hardware and models to get started with local hosting late 2025</title>
    <updated>2025-10-17T20:57:47+00:00</updated>
    <author>
      <name>/u/overloafunderloaf</name>
      <uri>https://old.reddit.com/user/overloafunderloaf</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi Everyone,&lt;/p&gt; &lt;p&gt;I've been curious about getting into hosting local models to mess around with. And maybe to help with my daily coding work, but I'd consider that just as a bonus. Generally, my usecases would be around processing data and coding. &lt;/p&gt; &lt;p&gt;I was wondering what would decent hardware to get started, I don't think I currently own anything that would work. I am happy to spend around $4000 at the absolute max, but less would be very welcome! &lt;/p&gt; &lt;p&gt;I heard about the DGX Spark, Framework Desktop and the M4 Macs/ M5 in the near future. I've heard mixed opinions on which is the best and what the pros and cons of each are. &lt;/p&gt; &lt;p&gt;Aside from performance, what are the benefits and downsides of each from a user perspective. Are any just a pain to get to work? &lt;/p&gt; &lt;p&gt;Finally, I want to learn about this whole world. Any Youtube channels or outlets that are good resources? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/overloafunderloaf"&gt; /u/overloafunderloaf &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9d13w/best_hardware_and_models_to_get_started_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9d13w/best_hardware_and_models_to_get_started_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o9d13w/best_hardware_and_models_to_get_started_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T20:57:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1o9ekh1</id>
    <title>EXO + Mac Studio + DGX Sparks (for prefill tokens) = 2.8x performance gains on AI benchmarks.</title>
    <updated>2025-10-17T22:01:04+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9ekh1/exo_mac_studio_dgx_sparks_for_prefill_tokens_28x/"&gt; &lt;img alt="EXO + Mac Studio + DGX Sparks (for prefill tokens) = 2.8x performance gains on AI benchmarks." src="https://external-preview.redd.it/hyPfbB2-0S9_BpSzWHTEzbVGWtAk9YMYdbHv82PBK64.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bdf78665a24bb3ff417303dab346ea60afc9e667" title="EXO + Mac Studio + DGX Sparks (for prefill tokens) = 2.8x performance gains on AI benchmarks." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I mean, it’s kind of an extremely pricey Frankenstein setup, but still kind of cool that it uses the strengths of both the Mac Studio (wide memory bus) and the DGX (compute for prefill) together to achieve significant performance gains.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.tomshardware.com/software/two-nvidia-dgx-spark-systems-combined-with-m3-ultra-mac-studio-to-create-blistering-llm-system-exo-labs-demonstrates-disaggregated-ai-inference-and-achieves-a-2-8-benchmark-boost"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9ekh1/exo_mac_studio_dgx_sparks_for_prefill_tokens_28x/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o9ekh1/exo_mac_studio_dgx_sparks_for_prefill_tokens_28x/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T22:01:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8x2w0</id>
    <title>What in the Black Friday hell is happening with the DDR5-5600 128GB SODIMM kits ?</title>
    <updated>2025-10-17T09:51:44+00:00</updated>
    <author>
      <name>/u/HumanDrone8721</name>
      <uri>https://old.reddit.com/user/HumanDrone8721</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In summer Amazon was selling them with something like 320€, not they are almost 500€ and increasing, I wanted to update my 64GB to 128, but this is obscene :(&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HumanDrone8721"&gt; /u/HumanDrone8721 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8x2w0/what_in_the_black_friday_hell_is_happening_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8x2w0/what_in_the_black_friday_hell_is_happening_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8x2w0/what_in_the_black_friday_hell_is_happening_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T09:51:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8m0ti</id>
    <title>We built 3B and 8B models that rival GPT-5 at HTML extraction while costing 40-80x less - fully open source</title>
    <updated>2025-10-16T23:43:03+00:00</updated>
    <author>
      <name>/u/TerrificMist</name>
      <uri>https://old.reddit.com/user/TerrificMist</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8m0ti/we_built_3b_and_8b_models_that_rival_gpt5_at_html/"&gt; &lt;img alt="We built 3B and 8B models that rival GPT-5 at HTML extraction while costing 40-80x less - fully open source" src="https://b.thumbs.redditmedia.com/m8yoOZ6gwR1CBn-AO5mIRDQSzZ5G3sXEIoAl_NdSUMk.jpg" title="We built 3B and 8B models that rival GPT-5 at HTML extraction while costing 40-80x less - fully open source" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;em&gt;Disclaimer: I work for&lt;/em&gt; &lt;a href="http://Inference.net"&gt;&lt;em&gt;Inference.net&lt;/em&gt;&lt;/a&gt;&lt;em&gt;, creator of the Schematron model family&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Hey everyone, wanted to share something we've been working on at Inference.net: Schematron, a family of small models for web extraction.&lt;/p&gt; &lt;p&gt;Our goal was to make a small, fast model for taking HTML from website and extracting JSON that perfectly adheres to a schema.&lt;/p&gt; &lt;p&gt;We distilled a frontier model down to 8B params and managed to keep basically all the output quality for this task. Schematron-8B scores 4.64 on LLM-as-a-judge evals vs GPT-4.1's 4.74 and Gemma 3B's 2.24. Schematron-3B scores 4.41 while being even faster. The main benefit of this model is that it costs 40-80x less than GPT-5 at comparable quality (slightly worse than GPT-5, as good as Gemini 2.5 Flash).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Technical details:&lt;/strong&gt; We fine-tuned &lt;a href="https://huggingface.co/meta-llama/Llama-3.1-8B"&gt;Llama-3.1-8B&lt;/a&gt;, expanded it to a 128K context window, quantized to FP8 without quality loss, and trained until it outputted strict JSON with 100% schema compliance. We also built a smaller 3B variant that's even cheaper and faster, but still maintains most of the accuracy of the 8B variant. We recommend using the 3B for most tasks, and trying 8B if it fails or most of your documents are pushing the context limit.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How we trained it:&lt;/strong&gt; We started with 1M real web pages from Common Crawl and built a synthetic dataset by clustering websites and generating schemas that mirror real-world usage patterns. We used a frontier model as a teacher and applied curriculum learning to progressively train on longer context lengths--training with context parallelism and FSDP to scale efficiently--which is why the models stay accurate even at the 128K token limit.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why this matters:&lt;/strong&gt; Processing 1 million pages daily with GPT-5 would cost you around $20,000. With Schematron-8B, that same workload runs about $480. With Schematron-3B, it's $240.&lt;/p&gt; &lt;p&gt;The speed matters too. Schematron processes pages 10x faster than frontier models. On average, Schamatron can scrape a page in 0.54 seconds, compared to 6 seconds for GPT-5. These latency gains compound very quickly for something like a browser-use agent.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Real-world impact on LLM factuality:&lt;/strong&gt; We tested this on SimpleQA to see how much it improves accuracy when paired with web search. When GPT-5 Nano was paired with Schematron-8B to extract structured data from search results provided by Exa, it went from answering barely any questions correctly (8.54% on SimpleQA) to getting over 85% right. The structured extraction approach means this was done processing lean, clean JSON (very little additional cost) instead of dumping ~8k tokens of raw HTML into your context window per page retrieved (typically LLMs are grounded with 5-10 pages/search).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Getting started:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;If you're using our &lt;a href="https://inference.net/models/schematron-3b"&gt;serverless API&lt;/a&gt;, you only need to pass your Pydantic, Zod, or JSON Schema and the HTML. We handle all the prompting in the backend for you in the backend. You get $10 in free credits to start.&lt;/p&gt; &lt;p&gt;If you're running locally, there are a few things to watch out for. You need to follow the prompting guidelines carefully and make sure you're using structured extraction properly, otherwise the model won't perform as well.&lt;/p&gt; &lt;p&gt;The models are on &lt;a href="https://huggingface.co/inference-net/Schematron-3B"&gt;HuggingFace&lt;/a&gt; and &lt;a href="https://ollama.com/Inference/Schematron"&gt;Ollama&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Full benchmarks and code examples are in our blog post: &lt;a href="https://inference.net/blog/schematron"&gt;https://inference.net/blog/schematron&lt;/a&gt;, &lt;a href="https://docs.inference.net/workhorse-models/schematron"&gt;docs&lt;/a&gt;, and &lt;a href="https://github.com/context-labs/inference-samples/blob/main/examples/schematron-scrape-companies/schematron-scrape-companies.ipynb"&gt;samples repo&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Happy to answer any technical questions about the training process or architecture. Also interested in how this would be helpful in your current scraping workflows!&lt;/p&gt; &lt;p&gt;Edit 9/17/2025:&lt;/p&gt; &lt;p&gt;After running some more LLM-as-a-Judge benchmarks today, we found that Schematron-8B scored 4.64, Gemini 2.5 Flash scored 4.65, Gemini 2.5 Pro scored 4.85, and Schematron-3B scored 4.38.&lt;/p&gt; &lt;p&gt;An earlier version of this post implied that Schematron-8B is better than Gemini 2.5 Flash at web extraction, that was incorrect and has been updated. On the sample we tested, their mean judge scores are effectively equivalent (Δ = −0.01).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TerrificMist"&gt; /u/TerrificMist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1o8m0ti"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8m0ti/we_built_3b_and_8b_models_that_rival_gpt5_at_html/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8m0ti/we_built_3b_and_8b_models_that_rival_gpt5_at_html/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T23:43:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1o9agkl</id>
    <title>Local multimodal RAG with Qwen3-VL — text + image retrieval</title>
    <updated>2025-10-17T19:17:24+00:00</updated>
    <author>
      <name>/u/AlanzhuLy</name>
      <uri>https://old.reddit.com/user/AlanzhuLy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Built a small demo showing how to run a full multimodal RAG pipeline locally using &lt;strong&gt;Qwen3-VL-GGUF&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;It loads and chunks your docs, embeds both text and images, retrieves the most relevant pieces for any question, and sends everything to Qwen3-VL for reasoning. The UI is just Gradio&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1o9agkl/video/ni6pd59g1qvf1/player"&gt;https://reddit.com/link/1o9agkl/video/ni6pd59g1qvf1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You can tweak chunk size, Top-K, or even swap in your own inference and embedding model.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/NexaAI/nexa-sdk/tree/main/demos/RAG-Qwen3VL"&gt;See GitHub for code and README instructions&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlanzhuLy"&gt; /u/AlanzhuLy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9agkl/local_multimodal_rag_with_qwen3vl_text_image/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9agkl/local_multimodal_rag_with_qwen3vl_text_image/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o9agkl/local_multimodal_rag_with_qwen3vl_text_image/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T19:17:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8m5ua</id>
    <title>Meta just dropped MobileLLM-Pro, a new 1B foundational language model on Huggingface</title>
    <updated>2025-10-16T23:49:16+00:00</updated>
    <author>
      <name>/u/Sad_Consequence5629</name>
      <uri>https://old.reddit.com/user/Sad_Consequence5629</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Meta just published MobileLLM-Pro, a new 1B parameter foundational language model (pre-trained and instruction fine-tuned) on Huggingface&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/facebook/MobileLLM-Pro"&gt;https://huggingface.co/facebook/MobileLLM-Pro&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The model seems to outperform Gemma 3-1B and Llama 3-1B by quite a large margin in pre-training and shows decent performance after instruction-tuning (Looks like it works pretty well for API calling, rewriting, coding and summarization).&lt;br /&gt; The model is already in GradIO and can be directly chatted with in the browser:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/spaces/akhaliq/MobileLLM-Pro"&gt;https://huggingface.co/spaces/akhaliq/MobileLLM-Pro&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(Tweet source: &lt;a href="https://x.com/_akhaliq/status/1978916251456925757"&gt;https://x.com/_akhaliq/status/1978916251456925757&lt;/a&gt; )&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sad_Consequence5629"&gt; /u/Sad_Consequence5629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8m5ua/meta_just_dropped_mobilellmpro_a_new_1b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8m5ua/meta_just_dropped_mobilellmpro_a_new_1b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8m5ua/meta_just_dropped_mobilellmpro_a_new_1b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T23:49:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1o9en0w</id>
    <title>Built a 100% Local AI Medical Assistant in an afternoon - Zero Cloud, using LlamaFarm</title>
    <updated>2025-10-17T22:04:05+00:00</updated>
    <author>
      <name>/u/badgerbadgerbadgerWI</name>
      <uri>https://old.reddit.com/user/badgerbadgerbadgerWI</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to show off the power of local AI and got tired of uploading my lab results to ChatGPT and trusting some API with my medical data. Got this up and running in 4 hours. It has 125K+ medical knowledge chunks to ground it in truth and a multi-step RAG retrieval strategy to get the best responses. Plus, it is open source (link down below)! &lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it does:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Upload a PDF of your medical records/lab results or ask it a quick question. It explains what's abnormal, why it matters, and what questions to ask your doctor. Uses actual medical textbooks (Harrison's Internal Medicine, Schwartz's Surgery, etc.), not just info from Reddit posts scraped by an agent a few months ago (yeah, I know the irony). &lt;/p&gt; &lt;p&gt;Check out the video:&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1o9en0w/video/3oef2nhjvqvf1/player"&gt;Walk through of the local medical helper&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The privacy angle:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;PDFs parsed in your browser (PDF.js) - never uploaded anywhere&lt;/li&gt; &lt;li&gt;All AI runs locally with LlamaFarm config; easy to reproduce&lt;/li&gt; &lt;li&gt;Your data literally never leaves your computer&lt;/li&gt; &lt;li&gt;Perfect for sensitive medical docs or very personal questions.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Tech stack:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Next.js frontend&lt;/li&gt; &lt;li&gt;gemma3:1b (134MB) + qwen3:1.7B (1GB) local models via Ollama&lt;/li&gt; &lt;li&gt;18 medical textbooks, 125k knowledge chunks&lt;/li&gt; &lt;li&gt;Multi-hop RAG (way smarter than basic RAG)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The RAG approach actually works:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Instead of one dumb query, the system generates 4-6 specific questions from your document and searches in parallel. So if you upload labs with high cholesterol, low Vitamin D, and high glucose, it automatically creates separate queries for each issue and retrieves comprehensive info about ALL of them.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What I learned:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Small models (gemma3:1b is 134MB!) are shockingly good for structured tasks if you use XML instead of JSON&lt;/li&gt; &lt;li&gt;Multi-hop RAG retrieves 3-4x more relevant info than single-query&lt;/li&gt; &lt;li&gt;Streaming with multiple &lt;code&gt;&amp;lt;think&amp;gt;&lt;/code&gt; blocks is a pain in the butt to parse&lt;/li&gt; &lt;li&gt;Its not that slow; the multi-hop and everything takes a 30-45 seconds, but its doing a lot and it is 100% local.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;How to try it:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Setup takes about 10 minutes + 2-3 hours for dataset processing (one-time) - We are shipping a way to not have to populate the database in the future. I am using Ollama right now, but will be shipping a runtime soon.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# Install Ollama, pull models ollama pull gemma3:1b ollama pull qwen3:1.7B # Clone repo git clone https://github.com/llama-farm/local-ai-apps.git cd Medical-Records-Helper # Full instructions in README &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;After initial setup, everything is instant and offline. No API costs, no rate limits, no spying.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Requirements:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;8GB RAM (4GB might work)&lt;/li&gt; &lt;li&gt;Docker&lt;/li&gt; &lt;li&gt;Ollama&lt;/li&gt; &lt;li&gt;~3GB disk space&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Full docs, troubleshooting, architecture details: &lt;a href="https://github.com/llama-farm/local-ai-apps/tree/main/Medical-Records-Helper"&gt;https://github.com/llama-farm/local-ai-apps/tree/main/Medical-Records-Helper&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="/r/LlamaFarm"&gt;r/LlamaFarm&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Roadmap:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You tell me. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Disclaimer:&lt;/strong&gt; Educational only, not medical advice, talk to real doctors, etc. Open source, MIT licensed. Built most of it in an afternoon once I figured out the multi-hop RAG pattern.&lt;/p&gt; &lt;p&gt;What features would you actually use? Thinking about adding wearable data analysis next.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/badgerbadgerbadgerWI"&gt; /u/badgerbadgerbadgerWI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9en0w/built_a_100_local_ai_medical_assistant_in_an/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9en0w/built_a_100_local_ai_medical_assistant_in_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o9en0w/built_a_100_local_ai_medical_assistant_in_an/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T22:04:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1o9b08m</id>
    <title>So I guess I accidentally became one of you guys</title>
    <updated>2025-10-17T19:38:42+00:00</updated>
    <author>
      <name>/u/cobalt1137</name>
      <uri>https://old.reddit.com/user/cobalt1137</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have kind of always dismissed the idea of getting a computer that is good enough to run anything locally, but decided to upgrade my current setup and got a mac m4 mini desktop computer. I know this isn't like the best thing ever and doesn't have some massive GPU on it, but I'm wondering if there is anything interesting that you guys think I could do locally with some type of model that would run locally with this m4 chip? Personally, I'm kind of interested in more like productivity things/computer use/potential coding use cases or other things in this ballpark ideally. Let me know if there's a certain model that you have in mind also. I'm lacking myself right now.&lt;/p&gt; &lt;p&gt;I also decided to just to get this chip because I feel like it might enable a future generation of products a bit more than buying a random $200 laptop.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cobalt1137"&gt; /u/cobalt1137 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9b08m/so_i_guess_i_accidentally_became_one_of_you_guys/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9b08m/so_i_guess_i_accidentally_became_one_of_you_guys/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o9b08m/so_i_guess_i_accidentally_became_one_of_you_guys/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T19:38:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1o9eo4f</id>
    <title>Qwen3-VL testout - open-source VL GOAT</title>
    <updated>2025-10-17T22:05:26+00:00</updated>
    <author>
      <name>/u/Zealousideal-Fox-76</name>
      <uri>https://old.reddit.com/user/Zealousideal-Fox-76</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been waiting on Qwen3-VL and finally ran the 4B on scanned tables, color-blind plates, UI screenshots, and small “sort these images” sets. For “read text fast and accurately,” ramp-up was near zero. Tables came out clean with headers and merged cells handled better than Qwen2.5-VL. Color perception is clearly improved—the standard plates that used to trip it now pass across runs. For simple ranking tasks, it got the ice-cream series right; mushrooms were off but the rationale was reasonable and still ahead of most open-source VL peers I’ve tried.&lt;/p&gt; &lt;p&gt;For GUI work, the loop is straightforward: recognize → locate → act. It reliably finds on-screen elements and returns usable boxes, so basic desktop/mobile flows can close. On charts and figures, it not only reads values but also does the arithmetic; visual data + reasoning feels stronger than last gen.&lt;/p&gt; &lt;p&gt;Two areas lag. Screenshot → HTML/CSS replication is weak in my tests; skeletons don’t match layout closely. Spatial transforms improved just enough to identify the main view correctly, but complex rotations and occlusions still cause slips. World knowledge mix-ups remain too: it still confuses Shanghai’s Jin Mao Tower with Shanghai Tower.&lt;/p&gt; &lt;p&gt;Variant behavior matters. The Think build tends to over-explain and sometimes lands wrong. The Instruct build stays steadier for perception, grounding, and “read + point” jobs. My pattern is simple: let 4B handle recognition and coordinates, then hand multi-step reasoning or code-gen to a larger text model. That stays stable.&lt;/p&gt; &lt;p&gt;Net take: big lift in perception, grounding, and visual math; still weak on faithful webpage replication and hard spatial transforms. As of today, it feels like the top open-source VL at this size.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Fox-76"&gt; /u/Zealousideal-Fox-76 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9eo4f/qwen3vl_testout_opensource_vl_goat/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9eo4f/qwen3vl_testout_opensource_vl_goat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o9eo4f/qwen3vl_testout_opensource_vl_goat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T22:05:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1o9it7v</id>
    <title>[Benchmark Visualization] RTX Pro 6000 vs DGX Spark - I visualized the LMSYS data and the results are interesting</title>
    <updated>2025-10-18T01:18:03+00:00</updated>
    <author>
      <name>/u/Spare-Solution-787</name>
      <uri>https://old.reddit.com/user/Spare-Solution-787</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9it7v/benchmark_visualization_rtx_pro_6000_vs_dgx_spark/"&gt; &lt;img alt="[Benchmark Visualization] RTX Pro 6000 vs DGX Spark - I visualized the LMSYS data and the results are interesting" src="https://external-preview.redd.it/o-FGzNvhg8l8VVDovZxd73JtKM597iNiithcrypGneg.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=83cef7040edd7231023da7fa0f5e5f6ccb36d889" title="[Benchmark Visualization] RTX Pro 6000 vs DGX Spark - I visualized the LMSYS data and the results are interesting" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/pdrox73furvf1.png?width=1346&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=55c2ef1d2d887273a5f37747b6834d7a4bcb441b"&gt;https://preview.redd.it/pdrox73furvf1.png?width=1346&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=55c2ef1d2d887273a5f37747b6834d7a4bcb441b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I was curious how the RTX Pro 6000 Workstation Edition compares to the new DGX Spark, so I dove into the &lt;a href="https://lmsys.org/blog/2025-10-13-nvidia-dgx-spark/"&gt;LMSYS benchmark data&lt;/a&gt; (which tested both sglang and ollama). The results were so interesting I created visualizations for it.&lt;/p&gt; &lt;p&gt;GitHub repo with charts: &lt;a href="https://github.com/casualcomputer/rtx_pro_6000_vs_dgx_spark"&gt;https://github.com/casualcomputer/rtx_pro_6000_vs_dgx_spark&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;TL;DR&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;RTX Pro 6000 is 6-7x faster&lt;/strong&gt; for LLM inference across every batch size and model tested. This isn't a small difference - we're talking 100 seconds vs 14 seconds for a 4k token conversation with Llama 3.1 8B.&lt;/p&gt; &lt;h1&gt;The Numbers (FP8, SGLang, 2k in/2k out)&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Llama 3.1 8B - Batch Size 1:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;DGX Spark: 100.1s end-to-end&lt;/li&gt; &lt;li&gt;RTX Pro 6000: 14.3s end-to-end&lt;/li&gt; &lt;li&gt;&lt;strong&gt;7.0x faster&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Llama 3.1 70B - Batch Size 1:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;DGX Spark: 772s (almost 13 minutes!)&lt;/li&gt; &lt;li&gt;RTX Pro 6000: 100s&lt;/li&gt; &lt;li&gt;&lt;strong&gt;7.7x faster&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Performance stays consistent across batch sizes 1-32.&lt;/strong&gt; The RTX just keeps winning by ~6x regardless of whether you're running single user or multi-tenant.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why Though?&lt;/strong&gt; LLM inference is memory-bound. You're constantly loading model weights from memory for every token generation. The RTX Pro 6000 has 6.5x more memory bandwidth (1,792 GB/s) than DGX-Spark (273 GB/s), and surprise - it's 6x faster. The math seems to check out.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Spare-Solution-787"&gt; /u/Spare-Solution-787 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9it7v/benchmark_visualization_rtx_pro_6000_vs_dgx_spark/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9it7v/benchmark_visualization_rtx_pro_6000_vs_dgx_spark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o9it7v/benchmark_visualization_rtx_pro_6000_vs_dgx_spark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-18T01:18:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1o90ixr</id>
    <title>LlamaBarn — A macOS menu bar app for running local LLMs (open source)</title>
    <updated>2025-10-17T12:54:41+00:00</updated>
    <author>
      <name>/u/erusev_</name>
      <uri>https://old.reddit.com/user/erusev_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o90ixr/llamabarn_a_macos_menu_bar_app_for_running_local/"&gt; &lt;img alt="LlamaBarn — A macOS menu bar app for running local LLMs (open source)" src="https://preview.redd.it/nmcd9kwwvnvf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5c988f1b4b79b949ebee5a449831ea15aab801ef" title="LlamaBarn — A macOS menu bar app for running local LLMs (open source)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;code&gt;r/LocalLLaMA&lt;/code&gt;! We just released this in beta and would love to get your feedback.&lt;/p&gt; &lt;p&gt;Here: &lt;a href="https://github.com/ggml-org/LlamaBarn"&gt;https://github.com/ggml-org/LlamaBarn&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What it does: - Download models from a curated catalog - Run models with one click — it auto-configures them for your system - Built-in web UI and REST API (via &lt;code&gt;llama.cpp&lt;/code&gt; server)&lt;/p&gt; &lt;p&gt;It's a small native app (~12 MB, 100% Swift) that wraps &lt;code&gt;llama.cpp&lt;/code&gt; to make running local models easier.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/erusev_"&gt; /u/erusev_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nmcd9kwwvnvf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o90ixr/llamabarn_a_macos_menu_bar_app_for_running_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o90ixr/llamabarn_a_macos_menu_bar_app_for_running_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T12:54:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1o9e7of</id>
    <title>Yet another unemployment-fueled Perplexity clone</title>
    <updated>2025-10-17T21:46:03+00:00</updated>
    <author>
      <name>/u/Opti_Dev</name>
      <uri>https://old.reddit.com/user/Opti_Dev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9e7of/yet_another_unemploymentfueled_perplexity_clone/"&gt; &lt;img alt="Yet another unemployment-fueled Perplexity clone" src="https://external-preview.redd.it/f_xEzNTJ4pROCB6zPKxq9ZMay-N5DHhAeVgu-fY73wU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9bb4ae36fae9b5478e04295bc66d5c3baf012f38" title="Yet another unemployment-fueled Perplexity clone" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I lost my Data Analyst job so i figured it was the perfect time to get back into coding.&lt;/p&gt; &lt;p&gt;I tried to selfhost SearxNG and Perplexica&lt;/p&gt; &lt;p&gt;SearxNG is great but Perplexica is not, (not fully configurable, no Katex support) generally the features of Perplexica didn't feat my use case (neither for Morphic)&lt;/p&gt; &lt;p&gt;So i started to code my own Perplexity alternative using langchain and React.&lt;/p&gt; &lt;p&gt;My solution have a cool and practical unified config file, better providers support, Katex support and expose a tool to the model allowing it to generate maps (i love this feature).&lt;/p&gt; &lt;p&gt;I thought you guys could like such a project. (even if it's yet-another 0 stars Perplexity clone)&lt;/p&gt; &lt;p&gt;I’d really appreciate your feedback: which features would you find useful, what’s missing, and any tips on managing a serious open-source project (since this is my biggest one so far).&lt;/p&gt; &lt;p&gt;Here is the repo &lt;a href="https://github.com/edoigtrd/ubiquite"&gt;https://github.com/edoigtrd/ubiquite&lt;/a&gt;&lt;/p&gt; &lt;p&gt;P.S. I was unemployed when I started Ubiquité, I’ve got a job now though!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/olvmqd8jiqvf1.png?width=2560&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=16b2ec4ef2775e85810c4091b59b0944d0c29558"&gt;https://preview.redd.it/olvmqd8jiqvf1.png?width=2560&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=16b2ec4ef2775e85810c4091b59b0944d0c29558&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/gett2h7jiqvf1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f66727a6000424655287fcb0db81908df21832be"&gt;https://preview.redd.it/gett2h7jiqvf1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f66727a6000424655287fcb0db81908df21832be&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Opti_Dev"&gt; /u/Opti_Dev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9e7of/yet_another_unemploymentfueled_perplexity_clone/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9e7of/yet_another_unemploymentfueled_perplexity_clone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o9e7of/yet_another_unemploymentfueled_perplexity_clone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T21:46:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1o9evno</id>
    <title>What is considered to be a top tier Speech To Text model, with speaker identification</title>
    <updated>2025-10-17T22:14:42+00:00</updated>
    <author>
      <name>/u/ImmediateFudge02</name>
      <uri>https://old.reddit.com/user/ImmediateFudge02</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking to locally run a speech to text model, with the highest accuracy on the transcripts. ideally want it to not break when there is gaps in speech or &amp;quot;ums&amp;quot;. I can guarantee high quality audio for the model, however I just need it to work when there is silence. I tried Whisper.CPP but it struggles with silence and it is not the most accurate. Additionally it does not identify or split the transcripts among the speakers. &lt;/p&gt; &lt;p&gt;Any insights would be much appreciated!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ImmediateFudge02"&gt; /u/ImmediateFudge02 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9evno/what_is_considered_to_be_a_top_tier_speech_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9evno/what_is_considered_to_be_a_top_tier_speech_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o9evno/what_is_considered_to_be_a_top_tier_speech_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T22:14:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1o99s2u</id>
    <title>ROCm 7.0 Install for Mi50 32GB | Ubuntu 24.04 LTS</title>
    <updated>2025-10-17T18:51:45+00:00</updated>
    <author>
      <name>/u/legit_split_</name>
      <uri>https://old.reddit.com/user/legit_split_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o99s2u/rocm_70_install_for_mi50_32gb_ubuntu_2404_lts/"&gt; &lt;img alt="ROCm 7.0 Install for Mi50 32GB | Ubuntu 24.04 LTS" src="https://external-preview.redd.it/QZgvW0JuHPNTo3BakCvCal_DO30UZr-SjSZAX09mNCA.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6e9ed4a0e6428b54b70c9ffefc80e3952a1b20e2" title="ROCm 7.0 Install for Mi50 32GB | Ubuntu 24.04 LTS" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I shared a comment on how to do this &lt;a href="https://www.reddit.com/r/linux4noobs/comments/1ly8rq6/comment/nb9uiye/"&gt;here&lt;/a&gt;, but I still see people asking for help so I decided to make a video tutorial.&lt;/p&gt; &lt;h1&gt;Text guide:&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;Copy &amp;amp; paste all the commands from the quick install &lt;a href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/quick-start.html"&gt;https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/quick-start.html&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Before rebooting to complete the install, download the 6.4 rocblas from the AUR: &lt;a href="https://archlinux.org/packages/extra/x86_64/rocblas/"&gt;https://archlinux.org/packages/extra/x86_64/rocblas/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Extract it &lt;/li&gt; &lt;li&gt;Copy all tensor files that contain gfx906 in &lt;code&gt;rocblas-6.4.3-3-x86_64.pkg/opt/rocm/lib/rocblas/library&lt;/code&gt; to &lt;code&gt;/opt/rocm/lib/rocblas/library&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Reboot&lt;/li&gt; &lt;li&gt;Check if it worked by running sudo update-alternatives --display rocm&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&amp;#8203;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# To build llama.cpp with ROCm + flash attention (adjust j value according to number of threads): HIPCXX=&amp;quot;$(hipconfig -l)/clang&amp;quot; HIP_PATH=&amp;quot;$(hipconfig -R)&amp;quot; \ cmake -S . -B build -DGGML_HIP=ON -DAMDGPU_TARGETS=gfx906 -DGGML_HIP_ROCWMMA_FATTN=ON -DCMAKE_BUILD_TYPE=Release \ &amp;amp;&amp;amp; cmake --build build --config Release -- -j 16 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note: This guide can be adapted for 6.4 if more stability is needed when working with PyTorch or vllm. Most performance improvements were already present in 6.4 (roughly 20-30% over 6.3), so 7.0.2 serves to offer more compatibility together with the latest AMD cards :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/legit_split_"&gt; /u/legit_split_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=xcI0pyE8VN8"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o99s2u/rocm_70_install_for_mi50_32gb_ubuntu_2404_lts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o99s2u/rocm_70_install_for_mi50_32gb_ubuntu_2404_lts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T18:51:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1o9g4if</id>
    <title>Ling-1T-GGUF on ik_llama.cpp</title>
    <updated>2025-10-17T23:09:36+00:00</updated>
    <author>
      <name>/u/VoidAlchemy</name>
      <uri>https://old.reddit.com/user/VoidAlchemy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9g4if/ling1tgguf_on_ik_llamacpp/"&gt; &lt;img alt="Ling-1T-GGUF on ik_llama.cpp" src="https://external-preview.redd.it/R-9hsMR8vUhTujexAftojKeBUYgoMQ0LKYIfxmVtxHE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=98540d3830dcf459c8003a9578801440c349b88b" title="Ling-1T-GGUF on ik_llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'll try to fixup the namespace ASAP but wanted to rush out some test quants of Ling-1T 1000B model. For now you'll need roughly 256GiB RAM + 24-32+ GiB VRAM to fit the available quants. Hope to release more after fixing up the 403 uploading issues.&lt;/p&gt; &lt;p&gt;Big thanks to ik and CISC for all the help figuring out how to quantize this beast, and of course thanks to Wendell at level1techs for the hardware support and also the aifoundry folks supporting me to come out to SF for the upcoming AI Plumbers Unconference next week!&lt;/p&gt; &lt;p&gt;In early testing I got out to roughly 40k context depth in ~6 turns of chat and it was doing okay reading some papers and generating diff patches without going off the rails at least.&lt;/p&gt; &lt;p&gt;Please give it a test and lemme know what you find!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VoidAlchemy"&gt; /u/VoidAlchemy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/ubergarm2/Ling-1T-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9g4if/ling1tgguf_on_ik_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o9g4if/ling1tgguf_on_ik_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T23:09:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1o9glvt</id>
    <title>Diagnosing layer sensitivity during post training quantization</title>
    <updated>2025-10-17T23:31:57+00:00</updated>
    <author>
      <name>/u/elinaembedl</name>
      <uri>https://old.reddit.com/user/elinaembedl</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9glvt/diagnosing_layer_sensitivity_during_post_training/"&gt; &lt;img alt="Diagnosing layer sensitivity during post training quantization" src="https://preview.redd.it/ibr4vouwarvf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=79ed43ec6d43bdb26d7c463a7a28500778a67f70" title="Diagnosing layer sensitivity during post training quantization" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have written a blog post on using layerwise PSNR to diagnose where models break during post-training quantization.&lt;/p&gt; &lt;p&gt;Instead of only checking output accuracy, layerwise metrics let you spot exactly which layers are sensitive (e.g. softmax, SE blocks), making it easier to debug and decide what to keep in higher precision.&lt;/p&gt; &lt;p&gt;If you’re experimenting with quantization for local or edge inference, you might find this interesting:&lt;br /&gt; &lt;a href="https://hub.embedl.com"&gt;Quantization – Diagnosing layer sensitivity during post training quantization&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to hear if anyone has tried similar layerwise diagnostics.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/elinaembedl"&gt; /u/elinaembedl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ibr4vouwarvf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9glvt/diagnosing_layer_sensitivity_during_post_training/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o9glvt/diagnosing_layer_sensitivity_during_post_training/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T23:31:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8wuyj</id>
    <title>Valve Developer Contributes Major Improvement To RADV Vulkan For Llama.cpp AI</title>
    <updated>2025-10-17T09:37:37+00:00</updated>
    <author>
      <name>/u/FastDecode1</name>
      <uri>https://old.reddit.com/user/FastDecode1</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FastDecode1"&gt; /u/FastDecode1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.phoronix.com/news/RADV-Valve-Boost-Llama.cpp"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8wuyj/valve_developer_contributes_major_improvement_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8wuyj/valve_developer_contributes_major_improvement_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T09:37:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1o9agyg</id>
    <title>New model from inclusionAI - LLaDA2.0-mini-preview</title>
    <updated>2025-10-17T19:17:48+00:00</updated>
    <author>
      <name>/u/edward-dev</name>
      <uri>https://old.reddit.com/user/edward-dev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9agyg/new_model_from_inclusionai_llada20minipreview/"&gt; &lt;img alt="New model from inclusionAI - LLaDA2.0-mini-preview" src="https://external-preview.redd.it/xv_Z1skcqtXjop4d-0l1Usyn5XM0UbKgNLHO0wCID8I.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f43fd9b3305330d175918ca63404b046858d8a03" title="New model from inclusionAI - LLaDA2.0-mini-preview" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;LLaDA2-mini-preview is a diffusion language model featuring a 16BA1B Mixture-of-Experts (MoE) architecture. As an enhanced, instruction-tuned iteration of the LLaDA series, it is optimized for practical applications.&lt;/p&gt; &lt;p&gt;From the benchmarks the preview looks 'not as good' as ling mini 2.0, but it's still a preview, not the final model, and this is a diffusion language model which makes it interesting &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/edward-dev"&gt; /u/edward-dev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/inclusionAI/LLaDA2.0-mini-preview"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9agyg/new_model_from_inclusionai_llada20minipreview/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o9agyg/new_model_from_inclusionai_llada20minipreview/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T19:17:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1o96mwq</id>
    <title>Using llamacpp and RCP, managed to improve promt processing by 4x times (160 t/s to 680 t/s) and text generation by 2x times (12.67 t/s to 22.52 t/s) by changing the device order including RPC. GLM 4.6 IQ4_XS multiGPU + RPC.</title>
    <updated>2025-10-17T16:52:21+00:00</updated>
    <author>
      <name>/u/panchovix</name>
      <uri>https://old.reddit.com/user/panchovix</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello guys, hoping you're having a good day.&lt;/p&gt; &lt;p&gt;As you know, llamacpp has RPC since time ago.&lt;/p&gt; &lt;p&gt;I have 2 PCs in my home:&lt;/p&gt; &lt;p&gt;My &amp;quot;Server&amp;quot;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;AM5 MSI X670E Carbon&lt;/li&gt; &lt;li&gt;AMD Ryzen 9 9900X&lt;/li&gt; &lt;li&gt;192GB DDR5 6000Mhz CL32&lt;/li&gt; &lt;li&gt;7 GPUs &lt;ul&gt; &lt;li&gt;5090x2&lt;/li&gt; &lt;li&gt;4090x2&lt;/li&gt; &lt;li&gt;A6000&lt;/li&gt; &lt;li&gt;3090x2&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;MCX314A-BCCT 40Gbps NIC (totally overkill, prob 10Gbps is fine)&lt;/li&gt; &lt;li&gt;OS: Fedora 42&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;And my &amp;quot;Gaming&amp;quot; PC:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;AM5 Gigabyte X670 Aorus Master (I wouldn't recommend this board btw)&lt;/li&gt; &lt;li&gt;AMD Ryzen 7 7800X3D&lt;/li&gt; &lt;li&gt;64GB DDR5 6000Mhz CL30&lt;/li&gt; &lt;li&gt;RTX 5090&lt;/li&gt; &lt;li&gt;MCX314A-BCCT 40Gbps NIC&lt;/li&gt; &lt;li&gt;OS: Windows 11&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;PC1 and PC2 (Server and Gaming) are connected via the MCX314A-BCCT 40Gbps NIC. As info, the max bandwidth used I have seen on llamacpp was about 10-11 Gbps when loading the model (I think here I'm either SSD bound or CPU bound) and about 3-4 Gbps on first prompt processing.&lt;/p&gt; &lt;p&gt;So for the test, I &amp;quot;disabled&amp;quot; one 3090 and replaced it layers with my 5090 via RPC.&lt;/p&gt; &lt;p&gt;I'm running GLM 4.6 IQ4_XS (~180GB) with (very complex, don't judge me):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;LLAMA_SET_ROWS=1 ./llama-server \ -m '/models/GLM-4.6-IQ4_XS.gguf' \ -c 32768 \ --no-mmap \ --rpc 192.168.50.2:50052 \ -ngl 999 \ -ot &amp;quot;blk.(0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15).ffn.=CUDA0&amp;quot; \ -ot &amp;quot;blk.(16|17|18|19|20|21|22|23|24|25).ffn.=CUDA1&amp;quot; \ -ot &amp;quot;blk.(27|28|29|30|31|32|33|34|35|36).ffn.=CUDA2&amp;quot; \ -ot &amp;quot;blk.(38|39|40|41|42|43|44|45|46|47|48|49|50).ffn.=CUDA3&amp;quot; \ -ot &amp;quot;blk.(51|52|53|54|55|56|57|58|59).ffn.=CUDA4&amp;quot; \ -ot &amp;quot;blk.(61|62|63|64|65|66|67|68|69|70).ffn.=RPC0[192.168.50.2:50052]&amp;quot; \ -ot &amp;quot;blk.(72|73|74|75|76|77|78|79|80|81|82|83|84|85|86|87|88|89|90|91).ffn.=CUDA5&amp;quot; \ -ot &amp;quot;blk.26.ffn_(norm|gate_inp|gate_shexp|down_shexp|up_shexp).weight=CUDA1&amp;quot; \ -ot &amp;quot;blk.26.ffn_gate_exps.weight=CUDA1&amp;quot; \ -ot &amp;quot;blk.26.ffn_(down_exps|up_exps).weight=CUDA0&amp;quot; \ -ot &amp;quot;blk.37.ffn_(norm|gate_inp|gate_shexp|down_shexp|up_shexp).weight=CUDA2&amp;quot; \ -ot &amp;quot;blk.37.ffn_gate_exps.weight=CUDA2&amp;quot; \ -ot &amp;quot;blk.37.ffn_(down_exps|up_exps).weight=CUDA3&amp;quot; \ -ot &amp;quot;blk.60.ffn_(norm|gate_inp|gate_shexp|down_shexp|up_shexp).weight=CUDA4&amp;quot; \ -ot &amp;quot;blk.60.ffn_gate_exps.weight=CUDA4&amp;quot; \ -ot &amp;quot;blk.60.ffn_(down_exps|up_exps).weight=CUDA5&amp;quot; \ -ot &amp;quot;blk.71.ffn_(norm|gate_inp|gate_shexp|down_shexp|up_shexp).weight=RPC0[192.168.50.2:50052]&amp;quot; \ -ot &amp;quot;blk.71.ffn_gate_exps.weight=RPC0[192.168.50.2:50052]&amp;quot; \ -ot &amp;quot;blk.71.ffn_(down_exps|up_exps).weight=CUDA5&amp;quot; \ -fa on \ -mg 0 \ -ub 1792 \ &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;By default, llamacpp assigns RPC devices as &lt;strong&gt;the first device,&lt;/strong&gt; this means that the RPC device has the bigger buffers and also has to do more processing that the server itself.&lt;/p&gt; &lt;p&gt;So it is like, by the --devices parameters in this case, use:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;--device RPC0,CUDA0,CUDA1,CUDA2,CUDA3,CUDA4,CUDA5&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;And I was getting these speeds:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;prompt eval time = 27661.35 ms / 4410 tokens ( 6.27 ms per token, 159.43 tokens per second) eval time = 140832.84 ms / 1784 tokens ( 78.94 ms per token, 12.67 tokens per second) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;So, I started a question on github here &lt;a href="https://github.com/ggml-org/llama.cpp/discussions/16625"&gt;https://github.com/ggml-org/llama.cpp/discussions/16625&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And &lt;a href="https://github.com/abc-nix"&gt;abc-nix&lt;/a&gt; did the great suggestion to move it.&lt;/p&gt; &lt;p&gt;So then, used&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;--device CUDA0,CUDA1,CUDA2,CUDA3,CUDA4,RPC0,CUDA5&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;And got&lt;/p&gt; &lt;pre&gt;&lt;code&gt;prompt eval time = 6483.46 ms / 4410 tokens ( 1.47 ms per token, 680.19 tokens per second) eval time = 78029.06 ms / 1757 tokens ( 44.41 ms per token, 22.52 tokens per second) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Which is an absolutely insane performance bump.&lt;/p&gt; &lt;p&gt;Now I want to try to dual boot the &amp;quot;Gaming&amp;quot; PC to Linux to see if there's an improvement. As multiGPU by itself is really bad on Windows, not sure if that also affects RPC.&lt;/p&gt; &lt;p&gt;EDIT: If you wonder how do I connect so much on a consumer CPU:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;X16 split into X8/X4/X4 5.0 from CPU (5090 at X8 5.0, 4090/4090 at X4 4.0)&lt;/li&gt; &lt;li&gt;X4/X4 5.0 from CPU from top 2 M2 slots, to PCIe adapters (RTX 5090 at X4 5.0 and Cx314a NIC X4 3.0)&lt;/li&gt; &lt;li&gt;X4 4.0 from Chipset from bottom PCIe slot (RTX A6000)&lt;/li&gt; &lt;li&gt;X4/X4 4.0 from Chipset from bottom M2 slots, to PCIe adapters (3090/3090)&lt;/li&gt; &lt;li&gt;X1 3.0 from NFF Wifi to PCIe adapter (for now it's open, thinking what can I put there).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;EDIT2: For those wondering, I get no money return for this. I haven't rented and I haven't sold anything related to AI either. So just expenses.&lt;/p&gt; &lt;p&gt;EDIT3: I have confirmed this also works perfectly when offloading to CPU.&lt;/p&gt; &lt;p&gt;I.e. for DeepSeek V3, I ran:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;LLAMA_SET_ROWS=1 ./llama-server -m '/models_llm_2tb/DeepSeek-V3-0324-UD-Q3_K_XL.gguf' -c 32768 --no-mmap -ngl 999 \ --rpc 192.168.50.2:50052 \ -ot &amp;quot;blk.(0|1|2|3|4|5|6|7).ffn.=CUDA0&amp;quot; \ -ot &amp;quot;blk.(8|9|10).ffn.=CUDA1&amp;quot; \ -ot &amp;quot;blk.(11|12|13).ffn.=CUDA2&amp;quot; \ -ot &amp;quot;blk.(14|15|16|17|18).ffn.=CUDA3&amp;quot; \ -ot &amp;quot;blk.(19|20|21).ffn.=CUDA4&amp;quot; \ -ot &amp;quot;blk.(22|23|24).ffn.=RPC0[192.168.50.2:50052]&amp;quot; \ -ot &amp;quot;blk.(25|26|27|28|29|30|31).ffn.=CUDA5&amp;quot; \ -ot &amp;quot;blk.32.ffn_(norm|gate_inp|gate_shexp|down_shexp|up_shexp).weight=CUDA1&amp;quot; \ -ot &amp;quot;blk.32.ffn_gate_exps.weight=CUDA1&amp;quot; \ -ot &amp;quot;blk.32.ffn_down_exps.weight=CUDA1&amp;quot; \ -ot &amp;quot;blk.32.ffn_up_exps.weight=CUDA1&amp;quot; \ -ot &amp;quot;blk.33.ffn_(norm|gate_inp|gate_shexp|down_shexp|up_shexp).weight=CUDA2&amp;quot; \ -ot &amp;quot;blk.33.ffn_gate_exps.weight=CUDA2&amp;quot; \ -ot &amp;quot;blk.33.ffn_down_exps.weight=CUDA2&amp;quot; \ -ot &amp;quot;blk.33.ffn_up_exps.weight=CUDA2&amp;quot; \ -ot &amp;quot;blk.34.ffn_(norm|gate_inp|gate_shexp|down_shexp|up_shexp).weight=CUDA5&amp;quot; \ -ot &amp;quot;blk.34.ffn_gate_exps.weight=CUDA5&amp;quot; \ -ot &amp;quot;blk.34.ffn_down_exps.weight=CUDA5&amp;quot; \ -ot &amp;quot;blk.35.ffn_gate_exps.weight=CUDA3&amp;quot; \ -ot &amp;quot;blk.35.ffn_down_exps.weight=CUDA3&amp;quot; \ -ot &amp;quot;exps=CPU&amp;quot; \ -fa on -mg 0 -ub 2560 -b 2560 --device CUDA0,CUDA1,CUDA2,CUDA3,CUDA4,RPC0,CUDA5 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And got about ~10% less perf than connecting the 5090 directly into the server PC.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/panchovix"&gt; /u/panchovix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o96mwq/using_llamacpp_and_rcp_managed_to_improve_promt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o96mwq/using_llamacpp_and_rcp_managed_to_improve_promt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o96mwq/using_llamacpp_and_rcp_managed_to_improve_promt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T16:52:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1o98f57</id>
    <title>New from Cerebras: REAP the Experts: Why Pruning Prevails for One-Shot MoE compression</title>
    <updated>2025-10-17T17:59:47+00:00</updated>
    <author>
      <name>/u/ilzrvch</name>
      <uri>https://old.reddit.com/user/ilzrvch</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o98f57/new_from_cerebras_reap_the_experts_why_pruning/"&gt; &lt;img alt="New from Cerebras: REAP the Experts: Why Pruning Prevails for One-Shot MoE compression" src="https://external-preview.redd.it/WYinqoDP9OerKm8ljzpFUp26G03RA6wo-9izylOPBeM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6b54dc6d0fc61b0246d5aa15915fc1c876cfd68a" title="New from Cerebras: REAP the Experts: Why Pruning Prevails for One-Shot MoE compression" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TLDR: We show that one-shot pruning of experts in large MoEs is better than expert merging when looking at realistic benchmarks, not just perplexity measures. &lt;/p&gt; &lt;p&gt;Using a saliency criterion that measures expected routed contribution of each expert (REAP), we pruned Qwen3-Coder-480B to 363B (25% pruning) and 246B (50% pruning), all in FP8. At 25%, accuracy degradation is minimal across a suite of benchmarks.&lt;/p&gt; &lt;p&gt;Checkpoints on HF:&lt;br /&gt; &lt;a href="https://huggingface.co/cerebras/Qwen3-Coder-REAP-363B-A35B-FP8"&gt;https://huggingface.co/cerebras/Qwen3-Coder-REAP-363B-A35B-FP8&lt;/a&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/cerebras/Qwen3-Coder-REAP-246B-A35B-FP8"&gt;https://huggingface.co/cerebras/Qwen3-Coder-REAP-246B-A35B-FP8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;These can be run with vanilla vLLM, no patches required. &lt;/p&gt; &lt;p&gt;More evals and pruned models on the way!&lt;/p&gt; &lt;p&gt;Link to the paper: &lt;a href="https://arxiv.org/abs/2510.13999"&gt;https://arxiv.org/abs/2510.13999&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6zdkycxjnpvf1.png?width=6884&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ef2e6f9f61b89de730fa9c01d6774998dedee9d8"&gt;https://preview.redd.it/6zdkycxjnpvf1.png?width=6884&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ef2e6f9f61b89de730fa9c01d6774998dedee9d8&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ilzrvch"&gt; /u/ilzrvch &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o98f57/new_from_cerebras_reap_the_experts_why_pruning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o98f57/new_from_cerebras_reap_the_experts_why_pruning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o98f57/new_from_cerebras_reap_the_experts_why_pruning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T17:59:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1o96o9o</id>
    <title>RTX Pro 6000 Blackwell vLLM Benchmark: 120B Model Performance Analysis</title>
    <updated>2025-10-17T16:53:42+00:00</updated>
    <author>
      <name>/u/notaDestroyer</name>
      <uri>https://old.reddit.com/user/notaDestroyer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o96o9o/rtx_pro_6000_blackwell_vllm_benchmark_120b_model/"&gt; &lt;img alt="RTX Pro 6000 Blackwell vLLM Benchmark: 120B Model Performance Analysis" src="https://external-preview.redd.it/12ojQ9khZuJRm7jqdMaOtnKaFtBC6Yo7dfwq4qKZ3jA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4ae7c659a21f868f6dba51b958c810a90c5bfe24" title="RTX Pro 6000 Blackwell vLLM Benchmark: 120B Model Performance Analysis" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Hardware:&lt;/strong&gt; NVIDIA RTX Pro 6000 Blackwell Workstation Edition (96GB VRAM)&lt;br /&gt; &lt;strong&gt;Software:&lt;/strong&gt; vLLM 0.11.0 | CUDA 13.0 | Driver 580.82.09 | FP16/BF16&lt;br /&gt; &lt;strong&gt;Model:&lt;/strong&gt; openai/gpt-oss-120b source: &lt;a href="https://huggingface.co/openai/gpt-oss-120b"&gt;https://huggingface.co/openai/gpt-oss-120b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Ran two test scenarios with 500-token and 1000-2000-token outputs across varying context lengths (1K-128K) and concurrency levels (1-20 users).&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5io6r8cfcpvf1.png?width=6907&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=dfa2bf6d638fcf36f75be97745f4be59c5f5cade"&gt;500 tokens&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/1i5c8lcgcpvf1.png?width=6907&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f6949af68c70e0b95f2462dab8dc6c3a5be7943a"&gt;1000-2000 tokens&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Key Findings&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Peak Performance (500-token output):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;1051 tok/s&lt;/strong&gt; at 20 users, 1K context&lt;/li&gt; &lt;li&gt;Maintains &lt;strong&gt;300-476 tok/s&lt;/strong&gt; at 20 concurrent users across context lengths&lt;/li&gt; &lt;li&gt;TTFT: 200-400ms at low concurrency, scales to 2000-3000ms at 20 users&lt;/li&gt; &lt;li&gt;Average latency: 2.6s (1 user) → 30.2s (20 users) at 128K context&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Extended Output (1000-2000 tokens):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;1016 tok/s&lt;/strong&gt; peak throughput (minimal degradation vs 500-token)&lt;/li&gt; &lt;li&gt;Slightly higher latencies due to longer decode phases&lt;/li&gt; &lt;li&gt;Power draw: 300-600W depending on load&lt;/li&gt; &lt;li&gt;Batch scaling efficiency: &lt;strong&gt;EXCELLENT&lt;/strong&gt; at 2-5 users, still good up to 10 users&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Observations&lt;/h1&gt; &lt;p&gt;The Blackwell architecture handles this 120B model impressively well:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Linear scaling up to ~5 concurrent users&lt;/li&gt; &lt;li&gt;GPU clocks remain stable at 2800+ MHz under load&lt;/li&gt; &lt;li&gt;Inter-token latency stays in the &amp;quot;INSTANT&amp;quot; zone (&amp;lt;50ms) for most configurations&lt;/li&gt; &lt;li&gt;Context length scaling is predictable—throughput halves roughly every 32K context increase&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The 96GB VRAM headroom means no swapping even at 128K context with max concurrency.&lt;/p&gt; &lt;p&gt;Used: &lt;a href="https://github.com/notaDestroyer/vllm-benchmark-suite"&gt;https://github.com/notaDestroyer/vllm-benchmark-suite&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; If you're running 100B+ models locally, the RTX Pro 6000 Blackwell delivers production-grade throughput with excellent multi-user scaling. Power efficiency is reasonable given the compute density.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/notaDestroyer"&gt; /u/notaDestroyer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o96o9o/rtx_pro_6000_blackwell_vllm_benchmark_120b_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o96o9o/rtx_pro_6000_blackwell_vllm_benchmark_120b_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o96o9o/rtx_pro_6000_blackwell_vllm_benchmark_120b_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T16:53:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1o98m76</id>
    <title>NVIDIA sent me a 5090 so I can demo Qwen3-VL GGUF</title>
    <updated>2025-10-17T18:06:56+00:00</updated>
    <author>
      <name>/u/AlanzhuLy</name>
      <uri>https://old.reddit.com/user/AlanzhuLy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o98m76/nvidia_sent_me_a_5090_so_i_can_demo_qwen3vl_gguf/"&gt; &lt;img alt="NVIDIA sent me a 5090 so I can demo Qwen3-VL GGUF" src="https://b.thumbs.redditmedia.com/pBsmzNoWTR_PcJ6HlkTBSSZmdKgKd285MYT5zK95iGA.jpg" title="NVIDIA sent me a 5090 so I can demo Qwen3-VL GGUF" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;3 days ago. We &lt;a href="https://x.com/Alibaba_Qwen/status/1978154384098754943"&gt;partnered with the Qwen team&lt;/a&gt; so the new Qwen3-VL 4B &amp;amp; 8B models run &lt;strong&gt;day-0&lt;/strong&gt; with GGUF, MLX inside &lt;a href="https://github.com/NexaAI/nexa-sdk"&gt;NexaSDK&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; powered by our NexaML Engine — the first and only framework that supports Qwen3-VL GGUF right now. We just received a 5090 from the NVIDIA team and I want to show you how it runs on a 5090&lt;/p&gt; &lt;p&gt;Today, we also made it run &lt;em&gt;locally&lt;/em&gt; inside our desktop UI app Hyperlink, so everyone can try Qwen3VL on their device easily&lt;/p&gt; &lt;p&gt;I tried the same demo examples from the &lt;a href="https://qwen.ai/blog?id=250aaecfcd4828d55be2b2437a76d66a099860da&amp;amp;from=research.research-list"&gt;Qwen2.5-32B blog&lt;/a&gt;, and the new &lt;strong&gt;Qwen3-VL 4B &amp;amp; 8B&lt;/strong&gt; are &lt;em&gt;insane.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Benchmarks on the 5090 (Q4):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Qwen3VL-8B → &lt;strong&gt;187 tok/s, ~8GB VRAM&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Qwen3VL-4B → &lt;strong&gt;267 tok/s, ~6GB VRAM&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Demo:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1o98m76/video/mvvtazwropvf1/player"&gt;https://reddit.com/link/1o98m76/video/mvvtazwropvf1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How to try:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Install Hyperlink with one click: &lt;a href="https://hyperlink.nexa.ai/"&gt;hyperlink.nexa.ai&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Then go to &lt;em&gt;Discover Models → download Qwen3-VL GGUF to test.&lt;/em&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;How does it do on your setup? Do you see similar performance between Qwen3VL 8B and Qwen2.5-32B?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlanzhuLy"&gt; /u/AlanzhuLy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o98m76/nvidia_sent_me_a_5090_so_i_can_demo_qwen3vl_gguf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o98m76/nvidia_sent_me_a_5090_so_i_can_demo_qwen3vl_gguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o98m76/nvidia_sent_me_a_5090_so_i_can_demo_qwen3vl_gguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T18:06:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8uxh6</id>
    <title>Write three times the word potato</title>
    <updated>2025-10-17T07:32:29+00:00</updated>
    <author>
      <name>/u/TooManyPascals</name>
      <uri>https://old.reddit.com/user/TooManyPascals</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8uxh6/write_three_times_the_word_potato/"&gt; &lt;img alt="Write three times the word potato" src="https://b.thumbs.redditmedia.com/VWK4WzyVVfvV7xJuANrLzK-bH1UfvcQckXM3kS4Llno.jpg" title="Write three times the word potato" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was testing how well Qwen3-0.6B could follow simple instructions... &lt;/p&gt; &lt;p&gt;and it accidentally created a trolling masterpiece.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TooManyPascals"&gt; /u/TooManyPascals &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1o8uxh6"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8uxh6/write_three_times_the_word_potato/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8uxh6/write_three_times_the_word_potato/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T07:32:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
