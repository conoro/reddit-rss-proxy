<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-27T19:48:52+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ohl4tx</id>
    <title>LM studio RoCM runtime much slower than Vulcan runtime</title>
    <updated>2025-10-27T17:43:30+00:00</updated>
    <author>
      <name>/u/Only_Comfortable_224</name>
      <uri>https://old.reddit.com/user/Only_Comfortable_224</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Tested open ai oss 20b on windows 11+rx9070. Vulcan gets 133tks while rocm only gets 99tks. Thatâ€™s 25% slowerâ€¦ Anyone has same experience?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Only_Comfortable_224"&gt; /u/Only_Comfortable_224 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohl4tx/lm_studio_rocm_runtime_much_slower_than_vulcan/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohl4tx/lm_studio_rocm_runtime_much_slower_than_vulcan/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohl4tx/lm_studio_rocm_runtime_much_slower_than_vulcan/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T17:43:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1oh8jt8</id>
    <title>Lightweight coding model for 4 GB Vram</title>
    <updated>2025-10-27T07:58:44+00:00</updated>
    <author>
      <name>/u/HiqhAim</name>
      <uri>https://old.reddit.com/user/HiqhAim</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, i was wondering if there is lightweight model for writing code that works on 4 GB Vram and 16 GB ram. Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HiqhAim"&gt; /u/HiqhAim &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oh8jt8/lightweight_coding_model_for_4_gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oh8jt8/lightweight_coding_model_for_4_gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oh8jt8/lightweight_coding_model_for_4_gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T07:58:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohij3h</id>
    <title>Best open source offline TTS that can be fully trained with voice samples?</title>
    <updated>2025-10-27T16:07:29+00:00</updated>
    <author>
      <name>/u/Twigling</name>
      <uri>https://old.reddit.com/user/Twigling</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I&amp;quot;m new to voice cloning and TTS and I've recently been dabbling with Chatterbox and, while it's impressive, I'm not happy with the overall prosody despite tweaking what is possible in this fork. It just doesn't sound quite as I'd like it to.&lt;/p&gt; &lt;p&gt;I'm looking to get as accurate a representation of my voice as possible, the idea being to provide samples and transcripts and, once the TTS has learned how I want the output to sound, provide it with the full public domain book text to convert to speech.&lt;/p&gt; &lt;p&gt;Which out of the many available options is the best for this?&lt;/p&gt; &lt;p&gt;Preferably something that not only sounds great but is easy to install and use and which will work within 12GB of VRAM on a 3060 GPU. &lt;/p&gt; &lt;p&gt;All that said, I may consider upgrading the GPU if the best software requires it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Twigling"&gt; /u/Twigling &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohij3h/best_open_source_offline_tts_that_can_be_fully/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohij3h/best_open_source_offline_tts_that_can_be_fully/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohij3h/best_open_source_offline_tts_that_can_be_fully/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T16:07:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1oh6vqf</id>
    <title>Token-Oriented Object Notation (TOON) - JSON for LLMs at half the token cost</title>
    <updated>2025-10-27T06:04:44+00:00</updated>
    <author>
      <name>/u/monnef</name>
      <uri>https://old.reddit.com/user/monnef</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oh6vqf/tokenoriented_object_notation_toon_json_for_llms/"&gt; &lt;img alt="Token-Oriented Object Notation (TOON) - JSON for LLMs at half the token cost" src="https://external-preview.redd.it/VrvLLB_yWC1wqD45_cvqK9l_om9VPq0R0Yc-ww1E9Aw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=500c22355aceea8d22944381237591529ec9d1ae" title="Token-Oriented Object Notation (TOON) - JSON for LLMs at half the token cost" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/monnef"&gt; /u/monnef &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/johannschopplich/toon"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oh6vqf/tokenoriented_object_notation_toon_json_for_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oh6vqf/tokenoriented_object_notation_toon_json_for_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T06:04:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohoe9r</id>
    <title>Still kinda new to all this. Currently using "LibreChat" + "TailScale" for my local frontend and remote access... was wondering if you guys could recommend any better local frontends that supports MCP, uploading files to a RAG system, and Prompt caching.</title>
    <updated>2025-10-27T19:44:47+00:00</updated>
    <author>
      <name>/u/WoodenTableForest</name>
      <uri>https://old.reddit.com/user/WoodenTableForest</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I really like LibreChat, It does about everything I want.. and I could probably integrate what I need for MCP. But was just wondering what else is out there.&lt;/p&gt; &lt;p&gt;Also, any suggestions for the best local models for tool calling as well as good social nuance understanding.&lt;/p&gt; &lt;p&gt;I&amp;quot;m currently being spoiled by sonnet 4.5 API but it is expensive&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WoodenTableForest"&gt; /u/WoodenTableForest &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohoe9r/still_kinda_new_to_all_this_currently_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohoe9r/still_kinda_new_to_all_this_currently_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohoe9r/still_kinda_new_to_all_this_currently_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T19:44:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohoebn</id>
    <title>qwen3 30B 2507 weird thinking output</title>
    <updated>2025-10-27T19:44:50+00:00</updated>
    <author>
      <name>/u/JsThiago5</name>
      <uri>https://old.reddit.com/user/JsThiago5</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am trying to use the 2507 version of the 30B through ollama, and it's outputting like this: &lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;[thiago@server p106docker]$ ollama run qwen3:30b-a3b-thinking-2507-q4_K_M&lt;/p&gt; &lt;p&gt;&amp;gt;&amp;gt;&amp;gt; hi what are you?&lt;/p&gt; &lt;p&gt;Thinking...&lt;/p&gt; &lt;p&gt;Okay, the user asked, hi what are you? I need to respond in a friendly and helpful way. First, I should introduce myself as Qwen, the large language model developed by Tongyi Lab. I should mention my capabilities, like answering questions, creating text, coding, etc. Keep it simple and not too technical.&lt;/p&gt; &lt;p&gt;The user's query is very short, so they might be new to this. I should avoid jargon. Maybe they want to know if I can help with something specific. But since they just asked what I am, I'll stick to the basics. Also, check if they need help with anything else. Keep the tone warm and inviting. Make sure to mention I'm here to assist&lt;/p&gt; &lt;p&gt;with various tasks. Let me structure the response: greeting, introduction, key features, offer help. Avoid any markdown. Keep it natural. Let me draft that.&lt;/p&gt; &lt;p&gt;Wait, the user said &amp;quot;hi what are you?&amp;quot; so I should correct the typo in &amp;quot;what&amp;quot; but not point it out. Just answer. Make sure the response is concise. Don't overcomplicate. Let me check for any errors. Alright, ready to write the response.&lt;/p&gt; &lt;p&gt;...done thinking.&lt;/p&gt; &lt;p&gt;Hi! I'm Qwen, a large language model developed by Tongyi Lab. I can help with answering questions, writing stories, emails, scripts, performing logical reasoning, coding, and more. How can I assist you today? ðŸ˜Š&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;As you can see, it is not using &amp;lt;think&amp;gt;&amp;lt;/think&amp;gt; but Thinking... ...done thinking. Is this the new way it is now? All tools I am using are buggy because of this&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JsThiago5"&gt; /u/JsThiago5 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohoebn/qwen3_30b_2507_weird_thinking_output/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohoebn/qwen3_30b_2507_weird_thinking_output/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohoebn/qwen3_30b_2507_weird_thinking_output/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T19:44:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohj1jq</id>
    <title>Core Ultra 7 265K, Ryzen 9 7900X, Ryzen 9 9950X, or is it irrelevant?</title>
    <updated>2025-10-27T16:26:24+00:00</updated>
    <author>
      <name>/u/illicITparameters</name>
      <uri>https://old.reddit.com/user/illicITparameters</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Currently refreshing my home workstation setup and I am looking to get more into local LLMs for professional reasons. Currently using the 7900X, have a new 265K that I was planning to move to so I had QuickSync, but wouldn't be against upgrading to the 9950X if it's worth it. Going to be pairing them with 2x48gb ddr5 6000 memory and a 3090.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/illicITparameters"&gt; /u/illicITparameters &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohj1jq/core_ultra_7_265k_ryzen_9_7900x_ryzen_9_9950x_or/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohj1jq/core_ultra_7_265k_ryzen_9_7900x_ryzen_9_9950x_or/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohj1jq/core_ultra_7_265k_ryzen_9_7900x_ryzen_9_9950x_or/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T16:26:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1ogybvr</id>
    <title>Qwen's VLM is strong!</title>
    <updated>2025-10-26T22:46:45+00:00</updated>
    <author>
      <name>/u/dulldata</name>
      <uri>https://old.reddit.com/user/dulldata</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogybvr/qwens_vlm_is_strong/"&gt; &lt;img alt="Qwen's VLM is strong!" src="https://preview.redd.it/jc97wpepbjxf1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0895935673cc7bbe35bc8ea3a71d20d4837c8861" title="Qwen's VLM is strong!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dulldata"&gt; /u/dulldata &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jc97wpepbjxf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogybvr/qwens_vlm_is_strong/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ogybvr/qwens_vlm_is_strong/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-26T22:46:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1ogwf6b</id>
    <title>M5 Neural Accelerator benchmark results from Llama.cpp</title>
    <updated>2025-10-26T21:24:33+00:00</updated>
    <author>
      <name>/u/auradragon1</name>
      <uri>https://old.reddit.com/user/auradragon1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h2&gt;Summary&lt;/h2&gt; &lt;p&gt;&lt;strong&gt;LLaMA 7B&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;SoC&lt;/th&gt; &lt;th align="right"&gt;BW [GB/s]&lt;/th&gt; &lt;th align="right"&gt;GPU Cores&lt;/th&gt; &lt;th align="right"&gt;F16 PP [t/s]&lt;/th&gt; &lt;th align="right"&gt;F16 TG [t/s]&lt;/th&gt; &lt;th align="right"&gt;Q8_0 PP [t/s]&lt;/th&gt; &lt;th align="right"&gt;Q8_0 TG [t/s]&lt;/th&gt; &lt;th align="right"&gt;Q4_0 PP [t/s]&lt;/th&gt; &lt;th align="right"&gt;Q4_0 TG [t/s]&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;âœ… M1 [1]&lt;/td&gt; &lt;td align="right"&gt;68&lt;/td&gt; &lt;td align="right"&gt;7&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;108.21&lt;/td&gt; &lt;td align="right"&gt;7.92&lt;/td&gt; &lt;td align="right"&gt;107.81&lt;/td&gt; &lt;td align="right"&gt;14.19&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;âœ… M1 [1]&lt;/td&gt; &lt;td align="right"&gt;68&lt;/td&gt; &lt;td align="right"&gt;8&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;117.25&lt;/td&gt; &lt;td align="right"&gt;7.91&lt;/td&gt; &lt;td align="right"&gt;117.96&lt;/td&gt; &lt;td align="right"&gt;14.15&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;âœ… M1 Pro [1]&lt;/td&gt; &lt;td align="right"&gt;200&lt;/td&gt; &lt;td align="right"&gt;14&lt;/td&gt; &lt;td align="right"&gt;262.65&lt;/td&gt; &lt;td align="right"&gt;12.75&lt;/td&gt; &lt;td align="right"&gt;235.16&lt;/td&gt; &lt;td align="right"&gt;21.95&lt;/td&gt; &lt;td align="right"&gt;232.55&lt;/td&gt; &lt;td align="right"&gt;35.52&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;âœ… M1 Pro [1]&lt;/td&gt; &lt;td align="right"&gt;200&lt;/td&gt; &lt;td align="right"&gt;16&lt;/td&gt; &lt;td align="right"&gt;302.14&lt;/td&gt; &lt;td align="right"&gt;12.75&lt;/td&gt; &lt;td align="right"&gt;270.37&lt;/td&gt; &lt;td align="right"&gt;22.34&lt;/td&gt; &lt;td align="right"&gt;266.25&lt;/td&gt; &lt;td align="right"&gt;36.41&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;âœ… M1 Max [1]&lt;/td&gt; &lt;td align="right"&gt;400&lt;/td&gt; &lt;td align="right"&gt;24&lt;/td&gt; &lt;td align="right"&gt;453.03&lt;/td&gt; &lt;td align="right"&gt;22.55&lt;/td&gt; &lt;td align="right"&gt;405.87&lt;/td&gt; &lt;td align="right"&gt;37.81&lt;/td&gt; &lt;td align="right"&gt;400.26&lt;/td&gt; &lt;td align="right"&gt;54.61&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;âœ… M1 Max [1]&lt;/td&gt; &lt;td align="right"&gt;400&lt;/td&gt; &lt;td align="right"&gt;32&lt;/td&gt; &lt;td align="right"&gt;599.53&lt;/td&gt; &lt;td align="right"&gt;23.03&lt;/td&gt; &lt;td align="right"&gt;537.37&lt;/td&gt; &lt;td align="right"&gt;40.20&lt;/td&gt; &lt;td align="right"&gt;530.06&lt;/td&gt; &lt;td align="right"&gt;61.19&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;âœ… M1 Ultra [1]&lt;/td&gt; &lt;td align="right"&gt;800&lt;/td&gt; &lt;td align="right"&gt;48&lt;/td&gt; &lt;td align="right"&gt;875.81&lt;/td&gt; &lt;td align="right"&gt;33.92&lt;/td&gt; &lt;td align="right"&gt;783.45&lt;/td&gt; &lt;td align="right"&gt;55.69&lt;/td&gt; &lt;td align="right"&gt;772.24&lt;/td&gt; &lt;td align="right"&gt;74.93&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;âœ… M1 Ultra [1]&lt;/td&gt; &lt;td align="right"&gt;800&lt;/td&gt; &lt;td align="right"&gt;64&lt;/td&gt; &lt;td align="right"&gt;1168.89&lt;/td&gt; &lt;td align="right"&gt;37.01&lt;/td&gt; &lt;td align="right"&gt;1042.95&lt;/td&gt; &lt;td align="right"&gt;59.87&lt;/td&gt; &lt;td align="right"&gt;1030.04&lt;/td&gt; &lt;td align="right"&gt;83.73&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;âœ… M2 [2]&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;8&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;147.27&lt;/td&gt; &lt;td align="right"&gt;12.18&lt;/td&gt; &lt;td align="right"&gt;145.91&lt;/td&gt; &lt;td align="right"&gt;21.70&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;âœ… M2 [2]&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;10&lt;/td&gt; &lt;td align="right"&gt;201.34&lt;/td&gt; &lt;td align="right"&gt;6.72&lt;/td&gt; &lt;td align="right"&gt;181.40&lt;/td&gt; &lt;td align="right"&gt;12.21&lt;/td&gt; &lt;td align="right"&gt;179.57&lt;/td&gt; &lt;td align="right"&gt;21.91&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;âœ… M2 Pro [2]&lt;/td&gt; &lt;td align="right"&gt;200&lt;/td&gt; &lt;td align="right"&gt;16&lt;/td&gt; &lt;td align="right"&gt;312.65&lt;/td&gt; &lt;td align="right"&gt;12.47&lt;/td&gt; &lt;td align="right"&gt;288.46&lt;/td&gt; &lt;td align="right"&gt;22.70&lt;/td&gt; &lt;td align="right"&gt;294.24&lt;/td&gt; &lt;td align="right"&gt;37.87&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;âœ… M2 Pro [2]&lt;/td&gt; &lt;td align="right"&gt;200&lt;/td&gt; &lt;td align="right"&gt;19&lt;/td&gt; &lt;td align="right"&gt;384.38&lt;/td&gt; &lt;td align="right"&gt;13.06&lt;/td&gt; &lt;td align="right"&gt;344.50&lt;/td&gt; &lt;td align="right"&gt;23.01&lt;/td&gt; &lt;td align="right"&gt;341.19&lt;/td&gt; &lt;td align="right"&gt;38.86&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;âœ… M2 Max [2]&lt;/td&gt; &lt;td align="right"&gt;400&lt;/td&gt; &lt;td align="right"&gt;30&lt;/td&gt; &lt;td align="right"&gt;600.46&lt;/td&gt; &lt;td align="right"&gt;24.16&lt;/td&gt; &lt;td align="right"&gt;540.15&lt;/td&gt; &lt;td align="right"&gt;39.97&lt;/td&gt; &lt;td align="right"&gt;537.60&lt;/td&gt; &lt;td align="right"&gt;60.99&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;âœ… M2 Max [2]&lt;/td&gt; &lt;td align="right"&gt;400&lt;/td&gt; &lt;td align="right"&gt;38&lt;/td&gt; &lt;td align="right"&gt;755.67&lt;/td&gt; &lt;td align="right"&gt;24.65&lt;/td&gt; &lt;td align="right"&gt;677.91&lt;/td&gt; &lt;td align="right"&gt;41.83&lt;/td&gt; &lt;td align="right"&gt;671.31&lt;/td&gt; &lt;td align="right"&gt;65.95&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;âœ… M2 Ultra [2]&lt;/td&gt; &lt;td align="right"&gt;800&lt;/td&gt; &lt;td align="right"&gt;60&lt;/td&gt; &lt;td align="right"&gt;1128.59&lt;/td&gt; &lt;td align="right"&gt;39.86&lt;/td&gt; &lt;td align="right"&gt;1003.16&lt;/td&gt; &lt;td align="right"&gt;62.14&lt;/td&gt; &lt;td align="right"&gt;1013.81&lt;/td&gt; &lt;td align="right"&gt;88.64&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;âœ… M2 Ultra [2]&lt;/td&gt; &lt;td align="right"&gt;800&lt;/td&gt; &lt;td align="right"&gt;76&lt;/td&gt; &lt;td align="right"&gt;1401.85&lt;/td&gt; &lt;td align="right"&gt;41.02&lt;/td&gt; &lt;td align="right"&gt;1248.59&lt;/td&gt; &lt;td align="right"&gt;66.64&lt;/td&gt; &lt;td align="right"&gt;1238.48&lt;/td&gt; &lt;td align="right"&gt;94.27&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;ðŸŸ¨ M3 [3]&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;10&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;187.52&lt;/td&gt; &lt;td align="right"&gt;12.27&lt;/td&gt; &lt;td align="right"&gt;186.75&lt;/td&gt; &lt;td align="right"&gt;21.34&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;ðŸŸ¨ M3 Pro [3]&lt;/td&gt; &lt;td align="right"&gt;150&lt;/td&gt; &lt;td align="right"&gt;14&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;272.11&lt;/td&gt; &lt;td align="right"&gt;17.44&lt;/td&gt; &lt;td align="right"&gt;269.49&lt;/td&gt; &lt;td align="right"&gt;30.65&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;âœ… M3 Pro [3]&lt;/td&gt; &lt;td align="right"&gt;150&lt;/td&gt; &lt;td align="right"&gt;18&lt;/td&gt; &lt;td align="right"&gt;357.45&lt;/td&gt; &lt;td align="right"&gt;9.89&lt;/td&gt; &lt;td align="right"&gt;344.66&lt;/td&gt; &lt;td align="right"&gt;17.53&lt;/td&gt; &lt;td align="right"&gt;341.67&lt;/td&gt; &lt;td align="right"&gt;30.74&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;âœ… M3 Max [3]&lt;/td&gt; &lt;td align="right"&gt;300&lt;/td&gt; &lt;td align="right"&gt;30&lt;/td&gt; &lt;td align="right"&gt;589.41&lt;/td&gt; &lt;td align="right"&gt;19.54&lt;/td&gt; &lt;td align="right"&gt;566.40&lt;/td&gt; &lt;td align="right"&gt;34.30&lt;/td&gt; &lt;td align="right"&gt;567.59&lt;/td&gt; &lt;td align="right"&gt;56.58&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;âœ… M3 Max [3]&lt;/td&gt; &lt;td align="right"&gt;400&lt;/td&gt; &lt;td align="right"&gt;40&lt;/td&gt; &lt;td align="right"&gt;779.17&lt;/td&gt; &lt;td align="right"&gt;25.09&lt;/td&gt; &lt;td align="right"&gt;757.64&lt;/td&gt; &lt;td align="right"&gt;42.75&lt;/td&gt; &lt;td align="right"&gt;759.70&lt;/td&gt; &lt;td align="right"&gt;66.31&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;âœ… M3 Ultra [3]&lt;/td&gt; &lt;td align="right"&gt;800&lt;/td&gt; &lt;td align="right"&gt;60&lt;/td&gt; &lt;td align="right"&gt;1121.80&lt;/td&gt; &lt;td align="right"&gt;42.24&lt;/td&gt; &lt;td align="right"&gt;1085.76&lt;/td&gt; &lt;td align="right"&gt;63.55&lt;/td&gt; &lt;td align="right"&gt;1073.09&lt;/td&gt; &lt;td align="right"&gt;88.40&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;âœ… M3 Ultra [3]&lt;/td&gt; &lt;td align="right"&gt;800&lt;/td&gt; &lt;td align="right"&gt;80&lt;/td&gt; &lt;td align="right"&gt;1538.34&lt;/td&gt; &lt;td align="right"&gt;39.78&lt;/td&gt; &lt;td align="right"&gt;1487.51&lt;/td&gt; &lt;td align="right"&gt;63.93&lt;/td&gt; &lt;td align="right"&gt;1471.24&lt;/td&gt; &lt;td align="right"&gt;92.14&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;âœ… M4 [4]&lt;/td&gt; &lt;td align="right"&gt;120&lt;/td&gt; &lt;td align="right"&gt;10&lt;/td&gt; &lt;td align="right"&gt;230.18&lt;/td&gt; &lt;td align="right"&gt;7.43&lt;/td&gt; &lt;td align="right"&gt;223.64&lt;/td&gt; &lt;td align="right"&gt;13.54&lt;/td&gt; &lt;td align="right"&gt;221.29&lt;/td&gt; &lt;td align="right"&gt;24.11&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;âœ… M4 Pro [4]&lt;/td&gt; &lt;td align="right"&gt;273&lt;/td&gt; &lt;td align="right"&gt;16&lt;/td&gt; &lt;td align="right"&gt;381.14&lt;/td&gt; &lt;td align="right"&gt;17.19&lt;/td&gt; &lt;td align="right"&gt;367.13&lt;/td&gt; &lt;td align="right"&gt;30.54&lt;/td&gt; &lt;td align="right"&gt;364.06&lt;/td&gt; &lt;td align="right"&gt;49.64&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;âœ… M4 Pro [4]&lt;/td&gt; &lt;td align="right"&gt;273&lt;/td&gt; &lt;td align="right"&gt;20&lt;/td&gt; &lt;td align="right"&gt;464.48&lt;/td&gt; &lt;td align="right"&gt;17.18&lt;/td&gt; &lt;td align="right"&gt;449.62&lt;/td&gt; &lt;td align="right"&gt;30.69&lt;/td&gt; &lt;td align="right"&gt;439.78&lt;/td&gt; &lt;td align="right"&gt;50.74&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;âœ… M4 Max [4]&lt;/td&gt; &lt;td align="right"&gt;546&lt;/td&gt; &lt;td align="right"&gt;40&lt;/td&gt; &lt;td align="right"&gt;922.83&lt;/td&gt; &lt;td align="right"&gt;31.64&lt;/td&gt; &lt;td align="right"&gt;891.94&lt;/td&gt; &lt;td align="right"&gt;54.05&lt;/td&gt; &lt;td align="right"&gt;885.68&lt;/td&gt; &lt;td align="right"&gt;83.06&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;âœ… &lt;strong&gt;M5 (Neural Accel)&lt;/strong&gt; [5]&lt;/td&gt; &lt;td align="right"&gt;153&lt;/td&gt; &lt;td align="right"&gt;10&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;608.05&lt;/strong&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;26.59&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;âœ… &lt;strong&gt;M5 (no Accel)&lt;/strong&gt; [5]&lt;/td&gt; &lt;td align="right"&gt;153&lt;/td&gt; &lt;td align="right"&gt;10&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;252.82&lt;/strong&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;27.55&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;M5 source: &lt;a href="https://github.com/ggml-org/llama.cpp/pull/16634"&gt;https://github.com/ggml-org/llama.cpp/pull/16634&lt;/a&gt;&lt;/p&gt; &lt;p&gt;All Apple Silicon results: &lt;a href="https://github.com/ggml-org/llama.cpp/discussions/4167"&gt;https://github.com/ggml-org/llama.cpp/discussions/4167&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/auradragon1"&gt; /u/auradragon1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogwf6b/m5_neural_accelerator_benchmark_results_from/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogwf6b/m5_neural_accelerator_benchmark_results_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ogwf6b/m5_neural_accelerator_benchmark_results_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-26T21:24:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohjyoc</id>
    <title>Dataset streaming for distributed SOTA model training</title>
    <updated>2025-10-27T17:00:31+00:00</updated>
    <author>
      <name>/u/qlhoest</name>
      <uri>https://old.reddit.com/user/qlhoest</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;Streaming datasets: 100x More Efficient&amp;quot; is a new blog post sharing improvements on dataset streaming to train AI models.&lt;/p&gt; &lt;p&gt;Link: &lt;a href="https://huggingface.co/blog/streaming-datasets"&gt;https://huggingface.co/blog/streaming-datasets&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Summary of the blog post:&lt;/p&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;There is also a 1min video explaining the impact of this: &lt;a href="https://x.com/andimarafioti/status/1982829207471419879"&gt;https://x.com/andimarafioti/status/1982829207471419879&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/qlhoest"&gt; /u/qlhoest &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohjyoc/dataset_streaming_for_distributed_sota_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohjyoc/dataset_streaming_for_distributed_sota_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohjyoc/dataset_streaming_for_distributed_sota_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T17:00:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1oh9gai</id>
    <title>How powerful are phones for AI workloads today?</title>
    <updated>2025-10-27T09:00:58+00:00</updated>
    <author>
      <name>/u/Henrie_the_dreamer</name>
      <uri>https://old.reddit.com/user/Henrie_the_dreamer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I ran a quick experiment to understand how many activated params a model needs to perform optimally on phones. &lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;File size&lt;/th&gt; &lt;th&gt;Nothing 3a &amp;amp; Pixel 6a CPU&lt;/th&gt; &lt;th&gt;Galaxy S25 Ultra &amp;amp; iPhone 17 Pro CPU&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Gemma3-270M-INT8&lt;/td&gt; &lt;td&gt;170mb&lt;/td&gt; &lt;td&gt;~30 toks/sec&lt;/td&gt; &lt;td&gt;~148 toks/sec&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;LFM2-350M-INT8&lt;/td&gt; &lt;td&gt;233mb&lt;/td&gt; &lt;td&gt;~26 toks/sec&lt;/td&gt; &lt;td&gt;~130 toks/sec&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3-600M-INT8&lt;/td&gt; &lt;td&gt;370mb&lt;/td&gt; &lt;td&gt;~20 toks/sec&lt;/td&gt; &lt;td&gt;~75 toks/sec&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;LFM2-750M-INT8&lt;/td&gt; &lt;td&gt;467mb&lt;/td&gt; &lt;td&gt;~20 toks/sec&lt;/td&gt; &lt;td&gt;~75 toks/sec&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Gemma3-1B-INT8&lt;/td&gt; &lt;td&gt;650mb&lt;/td&gt; &lt;td&gt;~14 toks/sec&lt;/td&gt; &lt;td&gt;~48 toks/sec&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;LFM-1.2B-INT8&lt;/td&gt; &lt;td&gt;722mb&lt;/td&gt; &lt;td&gt;~13 toks/sec&lt;/td&gt; &lt;td&gt;~44 toks/sec&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3-1.7B-INT8&lt;/td&gt; &lt;td&gt;1012mb&lt;/td&gt; &lt;td&gt;~8 toks/sec&lt;/td&gt; &lt;td&gt;~27 toks/sec&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;So, it might be tempting to suggest 8B-A1B model, but battery drain and heating makes it unusable in reality.&lt;/p&gt; &lt;p&gt;MOE makes sense since Qwen3-Next showed that 80B-A3B can beat dense 32B Qwen. &lt;/p&gt; &lt;p&gt;Task-specific models make sense because most mobile tasks are not that massive to need frontier models, and SLMs trained on specific tasks compete with generalist models 20x their size on the tasks. &lt;/p&gt; &lt;p&gt;An ideal setup would be 1B-A200m task-specific models. The file size at INT4 would be 330mb and the speed will go from 80-350 tokens/sec depending on the device. &lt;/p&gt; &lt;p&gt;What do you think?&lt;/p&gt; &lt;p&gt;N/B: The benchmarks were computed using &lt;a href="https://github.com/cactus-compute/cactus"&gt;Cactus&lt;/a&gt;. - Context size for benchmarks 128, simple KVCache. - Used CPU only since not every phone ships NPUs yet.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Henrie_the_dreamer"&gt; /u/Henrie_the_dreamer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oh9gai/how_powerful_are_phones_for_ai_workloads_today/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oh9gai/how_powerful_are_phones_for_ai_workloads_today/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oh9gai/how_powerful_are_phones_for_ai_workloads_today/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T09:00:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohfk05</id>
    <title>Made my own Local AI Research Agent | Need suggestions how to improve prompt/execution</title>
    <updated>2025-10-27T14:15:10+00:00</updated>
    <author>
      <name>/u/FriendshipCreepy8045</name>
      <uri>https://old.reddit.com/user/FriendshipCreepy8045</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohfk05/made_my_own_local_ai_research_agent_need/"&gt; &lt;img alt="Made my own Local AI Research Agent | Need suggestions how to improve prompt/execution" src="https://preview.redd.it/adft1ikiwnxf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5103180332a6e1cf251a0358796ec069d99e5ed5" title="Made my own Local AI Research Agent | Need suggestions how to improve prompt/execution" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone!&lt;br /&gt; So, in short I built my own local AI research assistant in Python ðŸ¦Š. &lt;/p&gt; &lt;p&gt;It reads Wikipedia, Arxiv, and news, then outputs professional research summaries directly in the terminal. Everything runs fully offline using Ollama! This is my first time exploring the agentic world, understanding how tool-calling and reasoning flow actually work. &lt;/p&gt; &lt;p&gt;Iâ€™ve always been a frontend engineer, and honestly, I didnâ€™t realize how far the AI world had come â€” the progress is unbelievable. After just 7 days of studying and 1 day of building, I made this small project. Itâ€™s definitely not perfect. &lt;/p&gt; &lt;p&gt;Iâ€™m still using pre-built tools instead of making things from scratch, but the outcome feels like a light version of ChatGPT, running locally!&lt;br /&gt; Iâ€™d really love to hear your thoughts and suggestions on how I can improve this or what I should learn next to move closer to becoming an AI Engineer.&lt;br /&gt; Hereâ€™s the GitHub link: &lt;a href="https://github.com/vedas-dixit/LocalAgent"&gt;https://github.com/vedas-dixit/LocalAgent&lt;/a&gt; If you try it locally, let me know what you think! &lt;/p&gt; &lt;p&gt;Thanks in advance :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FriendshipCreepy8045"&gt; /u/FriendshipCreepy8045 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/adft1ikiwnxf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohfk05/made_my_own_local_ai_research_agent_need/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohfk05/made_my_own_local_ai_research_agent_need/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T14:15:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1oh6k6u</id>
    <title>Some usage notes on low-end CPU LLMs and home applications (/r/frugal meets /r/localLlama)</title>
    <updated>2025-10-27T05:44:43+00:00</updated>
    <author>
      <name>/u/___positive___</name>
      <uri>https://old.reddit.com/user/___positive___</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So a few weeks ago I discovered that Qwen3-4b is actually usable on any old laptop with CPU-only inference. Since then, I've been working on getting a simple home smart station set up using small LLMs. These are some notes on the LLMs and their usage that will hopefully be useful for anyone else thinking of doing similar hobby projects with dirt cheap components.&lt;/p&gt; &lt;p&gt;I scored a used Thinkpad for $200 with a Ryzen 4650U and 32GB DDR4 3200, perfect cosmetic condition. The key here is the 32GB RAM. I installed Ubuntu 24.04. I'm not a big Linux guy but it was painless and everything worked perfectly on the first try. The idea is to have a small self-contained system with a built-in monitor and keyboard to act like a smart whiteboard + Alexa.&lt;/p&gt; &lt;p&gt;Here are some inference numbers , pardon the plain formatting, all run with llama.cpp built for CPU only, all q4, using short test prompts:&lt;/p&gt; &lt;p&gt;Qwen3-4B-Instruct-2507 (q4): 29 tok/sec (PP), 11 tok/sec (TG), 1 sec (model load time). Running in Balanced Mode versus Performance Mode power settings had negligible difference.&lt;/p&gt; &lt;p&gt;Qwen3-30B-A3B-Instruct-2507 (q4): 38 tok/sec (PP), 15 tok/sec (TG), 26 sec (model load time) for Balanced Mode. 44 tok/sec (PP), 15 tok/sec (TG), 17 sec (model load time) for Performance Mode.&lt;/p&gt; &lt;p&gt;Mistral-Small-3.2-24B-Instruct-2506 (q4): 5 tok/sec (PP), 2 tok/sec (TG), 12 sec (model load time) for Balanced mode. 5 tok/sec (PP), 2 tok/sec (TG), 4 sec (model load time) for Performance Mode.&lt;/p&gt; &lt;p&gt;Qwen3-30b-a3b is actually FASTER than Qwen3-4b and also performed better in my benchmarks for relevant tasks. But you need a lot of RAM to load it, which is why I specifically looked for the cheapest 32GB RAM laptop. Also, in my testing I found that the Qwen3-4b Thinking model would think for 3000 tokens to give a final 100 token result, which gave an effective generation rate of 0.1-0.2 tok/sec. So I would actually prefer a super slow non-instruct model like Mistral 24b at 2 tok/sec to a thinking model. However, Qwen3-30b-a3b is a nice compromise between speed and reliability.&lt;/p&gt; &lt;p&gt;Most of my use cases are non-interactive, like giving it an email to process and update a calendar. I do not need real time responses. For that reason, I didn't care about slow inference times within reason.&lt;/p&gt; &lt;p&gt;To get reliable performance, I had to split up tasks into simple subtasks. For example, I will ask the LLM to simply list all the topics from an email in the first step. In a second step, I ask the LLM to evaluate the relevancy of each topic in small batches. Then, I ask the LLM to extract JSON structures for each relevant event in order to update the calendar. On a 1000 word email with very high topic density (like a newsletter), Qwen3-30b-a3b would take roughly 9 minutes to process the entire workflow. I tweaked the workflow with various optimizations and could cut it down to about half. That's good enough for me.&lt;/p&gt; &lt;p&gt;I want to keep the power usage low, which means I'm not keeping the models warm. (I also stick to Balanced Mode.) That's why I wanted to record model load times as well. Again, most use cases are non-interactive. If I input a single event, like type &amp;quot;add this event on this time at this date&amp;quot;, the LLM will spin up and add it in under a minute.&lt;/p&gt; &lt;p&gt;I do have some light interactive uses. An example of that is asking for a timer while cooking. I might say &amp;quot;Alexa, set the timer for five minutes.&amp;quot; So here are some notes on that.&lt;/p&gt; &lt;p&gt;First, I use Openwakeword to trigger the whole process so that my laptop is not always running models and recording sound. Openwakeword is pre-tuned for a few wake words, which is why I am using &amp;quot;Alexa&amp;quot; as the wake word for now. I believe this can be tuned in the future. As soon as the wake word is detected, I immediately fire up faster-distil-whisper-small.en and LFM2-8b-a1b. They only take a second each to load, and I'm talking for a few seconds, so there is no lag this way.&lt;/p&gt; &lt;p&gt;LFM2-8b-a1b loads in about 1 second for me and runs at about 25 tok/sec TG (forgot to write down the PP but it is fast too). It is much faster than the other models but not as good with anything requiring reasoning. However, I was surprised at how well it performs in two tasks: topic identification and JSON extraction. So in a 1000 word newsletter filled with 18 topics, LFM2-8b-a1b can reliably extract all 18 topics pretty much as well as Qwen3-30b-a3b. So it's great at summarization, essentially. LFM2-8b-a1b can also reliably form JSON structures. By the way, I am using the model at q8. q4 definitely performs worse. This model, however, is not good at reasoning. For example, if I ask the model to determine if a certain event is relevant or not, it does not perform well. So it is good for fast topic identification and JSON extraction.&lt;/p&gt; &lt;p&gt;I tried various whisper models. I ended up finding the faster-distil-whisper-small.en to be a good compromise between speed and reliability. A sentence like &amp;quot;Alexa, set the timer for 5 minutes&amp;quot; will get parsed in 1 sec, but not as well as I would like. However, if I set the beam_size to 10 (5 is the default, typically), then it takes 2 seconds but with decent reliability. The medium model is too slow, around 5+ seconds even with reduced beam_size, and the base model has horrible accuracy. So that worked for me.&lt;/p&gt; &lt;p&gt;However, to boost the reliability further, I take the output from faster-distil-whisper-small.en and pass it to LFM2-8b-a1b, which gives me a JSON with an action field and a parameter field or two. That gets used to trigger the downstream python script. The LFM2 inference adds about an additional second or so. I don't care about waiting a tiny amount in this case, so that works for me.&lt;/p&gt; &lt;p&gt;For voice commands for adding reminders or calendar events, I will use the LFM2 JSON extraction to trigger re-transcription of the recorded voice message with whisper-largev3. Then, throw it to Qwen3-30b-a3b for processing, since quality is more important than speed.&lt;/p&gt; &lt;p&gt;I almost forgot! Super important, but the built-in mic quality isn't great on laptops. I ended getting a cheap USB wired conference speakerphone for &amp;lt;$20 off ebay. The brand is EMEET, but I think any modern one probably works. Python interacts with the microphone using Pipewire. The microphone made a big difference in transcription quality. It has hardware level sound processing, noise cancellation, etc.&lt;/p&gt; &lt;p&gt;Basically, I am using Qwen3-30b-a3b to process messy inputs (typing, voice, emails) slowly and LFM2-8b-a1b to process messy voice transcription quickly. Again, this all runs on a dirt cheap, old 4650U processor.&lt;/p&gt; &lt;p&gt;This is an ongoing hobby project. I want to eventually see if I can take pictures with the built-in webcam of physical mail or receipts and get one of the VL models or an OCR model to process it. There are trivial things to add, like verbal commands to check the weather and such. A whole bunch of other ideas.&lt;/p&gt; &lt;p&gt;I am loving the low-end LLM ecosystem. The cool part is that the stuff you make actually affects people around you! Like it actually gets used! The Qwen3 and LFM2 models I use are my favorites so far.&lt;/p&gt; &lt;p&gt;Okay, now back to you guys with your 8 x H100 basement setups...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/___positive___"&gt; /u/___positive___ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oh6k6u/some_usage_notes_on_lowend_cpu_llms_and_home/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oh6k6u/some_usage_notes_on_lowend_cpu_llms_and_home/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oh6k6u/some_usage_notes_on_lowend_cpu_llms_and_home/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T05:44:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohnuxy</id>
    <title>How are you preventing production AI agents from going rogue? (Cost overruns, unsafe tool use, etc.)</title>
    <updated>2025-10-27T19:24:09+00:00</updated>
    <author>
      <name>/u/ClearstoneDev</name>
      <uri>https://old.reddit.com/user/ClearstoneDev</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My team is moving our LangChain/LangGraph agents from prototype to production, and we're looking at risks of autonomous execution.&lt;/p&gt; &lt;p&gt;We're trying to solve problems like:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Preventing an agent from getting stuck in a loop and blowing our OpenAI budget.&lt;/li&gt; &lt;li&gt;Enforcing strict rules about which tools certain user roles can trigger (e.g., guests can't use a delete_files tool).&lt;/li&gt; &lt;li&gt;Requiring manual human approval before an agent performs a high-stakes action (like for example a financial transaction).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Right now, our code is getting messy with if/else checks for permissions and budget limits. It feels brittle and hard to audit... How are you all handling this in production?&lt;/p&gt; &lt;p&gt;Are you using framework features (like LangChain's new middleware), external tools (like OPA), or just building custom logic? What are the trade-offs you've found (especially around latency and complexity)?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ClearstoneDev"&gt; /u/ClearstoneDev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohnuxy/how_are_you_preventing_production_ai_agents_from/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohnuxy/how_are_you_preventing_production_ai_agents_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohnuxy/how_are_you_preventing_production_ai_agents_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T19:24:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohm80t</id>
    <title>Newegg has 32gb AMD r9700 for $1,300</title>
    <updated>2025-10-27T18:23:03+00:00</updated>
    <author>
      <name>/u/Terminator857</name>
      <uri>https://old.reddit.com/user/Terminator857</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://videocardz.com/newz/amd-radeon-pro-ai-r9700-is-now-available-32gb-memory-and-full-navi-48-gpu"&gt;https://videocardz.com/newz/amd-radeon-pro-ai-r9700-is-now-available-32gb-memory-and-full-navi-48-gpu&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Phoronix did a poor job of benchmarking it. Would prefer benchmarking a 30gb model like qwen3 coder, but instead focuses on 8gb model: &lt;a href="https://www.phoronix.com/review/amd-radeon-ai-pro-r9700"&gt;https://www.phoronix.com/review/amd-radeon-ai-pro-r9700&lt;/a&gt; Doesn't bother to compare it to 4090/5090. This video does gaming benchmarks: &lt;a href="https://www.youtube.com/watch?v=x0YJ32Q0mNw"&gt;https://www.youtube.com/watch?v=x0YJ32Q0mNw&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm guestimating 20 tokens per second (TPS) for qwen3 coder.&lt;/p&gt; &lt;p&gt;Also found at: &lt;a href="https://www.amazon.com/XFX-Radeon-R9700-GDDR6-RX-97XPROAIY/dp/B0FXTRGHL9"&gt;https://www.amazon.com/XFX-Radeon-R9700-GDDR6-RX-97XPROAIY/dp/B0FXTRGHL9&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Terminator857"&gt; /u/Terminator857 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohm80t/newegg_has_32gb_amd_r9700_for_1300/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohm80t/newegg_has_32gb_amd_r9700_for_1300/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohm80t/newegg_has_32gb_amd_r9700_for_1300/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T18:23:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohmado</id>
    <title>LM Studio Local Server hidden and always running</title>
    <updated>2025-10-27T18:25:30+00:00</updated>
    <author>
      <name>/u/JustSayin_thatuknow</name>
      <uri>https://old.reddit.com/user/JustSayin_thatuknow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys, can someone else confirm that LM Studio, even if you have local server turned off, it is actively listening to localhost port 41343? How is this possible? If you're on windows, try this cmd &amp;quot;netstat -ano | findstr 41343&amp;quot; (if on other OS you'll know how to do it). Mine outputs this &amp;quot;TCP 127.0.0.1:41343 0.0.0.0:0 LISTENING 17200&amp;quot; so when I run this &amp;quot;tasklist /FI &amp;quot;PID eq 17200&amp;quot;&amp;quot; it returns this &amp;quot;LM Studio.exe 17200 Console 1 97,804 K&amp;quot; so I went digging everywhere and can't find anyone with this same issue.. Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JustSayin_thatuknow"&gt; /u/JustSayin_thatuknow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohmado/lm_studio_local_server_hidden_and_always_running/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohmado/lm_studio_local_server_hidden_and_always_running/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohmado/lm_studio_local_server_hidden_and_always_running/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T18:25:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohjayo</id>
    <title>Llama.cpp New Ram halves inference speed at a higher context</title>
    <updated>2025-10-27T16:36:05+00:00</updated>
    <author>
      <name>/u/easyrider99</name>
      <uri>https://old.reddit.com/user/easyrider99</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I am just starting to debug this and wondered if anyone else has run into this issue.&lt;/p&gt; &lt;p&gt;I am running a W7-3455 ( Xeon 8 channel DDR5 ). I recently upgraded from 8x64GB DDR5 to 8x96GB. The original kit was a high performance V-color kit with lower CL timings, so the performance on MLC is about a ~5% decrease. In any case, the speed is very good according to MLC ( ~ 240GB/s ).&lt;/p&gt; &lt;p&gt;When running the same parameters with llama-server, I initially get the same inference speeds. However, at about 25K context, the inference speed just drops by half.&lt;/p&gt; &lt;p&gt;Example running DeepSeekV3.1-Terminus at Q4_K_XL:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;srv params_from_: Chat format: DeepSeek V3.1 slot get_availabl: id 0 | task 0 | selected slot by LRU, t_last = 55080165780 slot launch_slot_: id 0 | task 138 | processing task slot update_slots: id 0 | task 138 | new prompt, n_ctx_slot = 164096, n_keep = 0, n_prompt_tokens = 24619 slot update_slots: id 0 | task 138 | n_past = 2, memory_seq_rm [2, end) slot update_slots: id 0 | task 138 | prompt processing progress, n_past = 2050, n_tokens = 2048, progress = 0.083188 slot update_slots: id 0 | task 138 | n_past = 2050, memory_seq_rm [2050, end) slot update_slots: id 0 | task 138 | prompt processing progress, n_past = 4098, n_tokens = 2048, progress = 0.166376 slot update_slots: id 0 | task 138 | n_past = 4098, memory_seq_rm [4098, end) slot update_slots: id 0 | task 138 | prompt processing progress, n_past = 6146, n_tokens = 2048, progress = 0.249563 slot update_slots: id 0 | task 138 | n_past = 6146, memory_seq_rm [6146, end) slot update_slots: id 0 | task 138 | prompt processing progress, n_past = 8194, n_tokens = 2048, progress = 0.332751 slot update_slots: id 0 | task 138 | n_past = 8194, memory_seq_rm [8194, end) slot update_slots: id 0 | task 138 | prompt processing progress, n_past = 10242, n_tokens = 2048, progress = 0.415939 slot update_slots: id 0 | task 138 | n_past = 10242, memory_seq_rm [10242, end) slot update_slots: id 0 | task 138 | prompt processing progress, n_past = 12290, n_tokens = 2048, progress = 0.499127 slot update_slots: id 0 | task 138 | n_past = 12290, memory_seq_rm [12290, end) slot update_slots: id 0 | task 138 | prompt processing progress, n_past = 14338, n_tokens = 2048, progress = 0.582314 slot update_slots: id 0 | task 138 | n_past = 14338, memory_seq_rm [14338, end) slot update_slots: id 0 | task 138 | prompt processing progress, n_past = 16386, n_tokens = 2048, progress = 0.665502 slot update_slots: id 0 | task 138 | n_past = 16386, memory_seq_rm [16386, end) slot update_slots: id 0 | task 138 | prompt processing progress, n_past = 18434, n_tokens = 2048, progress = 0.748690 slot update_slots: id 0 | task 138 | n_past = 18434, memory_seq_rm [18434, end) slot update_slots: id 0 | task 138 | prompt processing progress, n_past = 20482, n_tokens = 2048, progress = 0.831878 slot update_slots: id 0 | task 138 | n_past = 20482, memory_seq_rm [20482, end) slot update_slots: id 0 | task 138 | prompt processing progress, n_past = 22530, n_tokens = 2048, progress = 0.915066 slot update_slots: id 0 | task 138 | n_past = 22530, memory_seq_rm [22530, end) slot update_slots: id 0 | task 138 | prompt processing progress, n_past = 24578, n_tokens = 2048, progress = 0.998253 slot update_slots: id 0 | task 138 | n_past = 24578, memory_seq_rm [24578, end) slot update_slots: id 0 | task 138 | prompt processing progress, n_past = 24619, n_tokens = 41, progress = 0.999919 slot update_slots: id 0 | task 138 | prompt done, n_past = 24619, n_tokens = 41 slot release: id 0 | task 138 | stop processing: n_past = 25332, truncated = 0 slot print_timing: id 0 | task 138 | prompt eval time = 977896.21 ms / 24617 tokens ( 39.72 ms per token, 25.17 tokens per second) eval time = 88448.57 ms / 714 tokens ( 123.88 ms per token, 8.07 tokens per second) total time = 1066344.78 ms / 25331 tokens &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then the following prompt:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;srv update_slots: all slots are idle srv log_server_r: request: POST /v1/chat/completions 10.0.0.40 200 srv params_from_: Chat format: DeepSeek V3.1 slot get_availabl: id 0 | task 138 | selected slot by lcs similarity, lcs_len = 24618, similarity = 0.972 (&amp;gt; 0.100 thold) slot launch_slot_: id 0 | task 865 | processing task slot update_slots: id 0 | task 865 | new prompt, n_ctx_slot = 164096, n_keep = 0, n_prompt_tokens = 25756 slot update_slots: id 0 | task 865 | n_past = 24618, memory_seq_rm [24618, end) slot update_slots: id 0 | task 865 | prompt processing progress, n_past = 25756, n_tokens = 1138, progress = 0.044184 slot update_slots: id 0 | task 865 | prompt done, n_past = 25756, n_tokens = 1138 slot release: id 0 | task 865 | stop processing: n_past = 26212, truncated = 0 slot print_timing: id 0 | task 865 | prompt eval time = 51948.00 ms / 1138 tokens ( 45.65 ms per token, 21.91 tokens per second) eval time = 94955.55 ms / 457 tokens ( 207.78 ms per token, 4.81 tokens per second) total time = 146903.55 ms / 1595 tokens &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This never happened with my previous RAM kit. The inference speed would decrease as context increased, but rather linearly rather than this huge drop. &lt;/p&gt; &lt;p&gt;Any tips?&lt;/p&gt; &lt;p&gt;My current llama-server command:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;numactl --interleave=all ./build/bin/llama-server --model /mnt/home_extend/models/unsloth_DeepSeek-V3.1-Terminus-GGUF/UD-Q4_K_XL/DeepSeek-V3.1-Terminus-UD-Q4_K_XL-00001-of-00008.gguf --alias DeepSeek-V3.1 --threads 44 --ctx-size 120000 --n-gpu-layers 99 --cpu-moe --temp 0.6 --top-p 0.95 -fa 1 --host 0.0.0.0 --jinja --port 8099 --threads 48 --no-host &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/easyrider99"&gt; /u/easyrider99 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohjayo/llamacpp_new_ram_halves_inference_speed_at_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohjayo/llamacpp_new_ram_halves_inference_speed_at_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohjayo/llamacpp_new_ram_halves_inference_speed_at_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T16:36:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohfuea</id>
    <title>Last week in Multimodal AI - Local Edition</title>
    <updated>2025-10-27T14:26:44+00:00</updated>
    <author>
      <name>/u/Vast_Yak_4147</name>
      <uri>https://old.reddit.com/user/Vast_Yak_4147</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohfuea/last_week_in_multimodal_ai_local_edition/"&gt; &lt;img alt="Last week in Multimodal AI - Local Edition" src="https://external-preview.redd.it/GG_nI8HdJYOWQ3BUPyYaxU74wbTZUS40_TzupervzGM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a7b79f3bb4c84db3fbd07ad6d0a3cdb30315acbb" title="Last week in Multimodal AI - Local Edition" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I curate a weekly newsletter on multimodal AI. Here are the local/edge highlights from last week:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;DeepSeek OCR - Efficient Document Parsing&lt;/strong&gt;&lt;br /&gt; â€¢ Uses optical 2D mapping with lossy compression for 97% OCR accuracy at 10x compression.&lt;br /&gt; â€¢ Processes 200k+ pages daily on a single A100 GPU, ideal for local document digitization.&lt;br /&gt; â€¢ &lt;a href="https://github.com/deepseek-ai/DeepSeek-OCR"&gt;GitHub&lt;/a&gt; | &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-OCR"&gt;Hugging Face&lt;/a&gt; | &lt;a href="https://arxiv.org/abs/2510.18234"&gt;Paper&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8mt2da5wynxf1.png?width=1456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=09575812d8fc3336db32cda7148fb8fbc9c0857c"&gt;https://preview.redd.it/8mt2da5wynxf1.png?width=1456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=09575812d8fc3336db32cda7148fb8fbc9c0857c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;LightOnOCR-1B - Multimodal OCR for Edge&lt;/strong&gt;&lt;br /&gt; â€¢ 1B parameter model transcribes full pages to Markdown at 5.71 pages/second on an H100.&lt;br /&gt; â€¢ Distilled from a 72B teacher, optimized for low-resource local setups with SOTA efficiency.&lt;br /&gt; â€¢ &lt;a href="https://huggingface.co/lightonai/LightOnOCR-1B-1025"&gt;Hugging Face&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Tencent Hunyuan World 1.1 (WorldMirror)&lt;/strong&gt;&lt;br /&gt; â€¢ Feed-forward 3D reconstruction from video or multi-view, running on a single GPU.&lt;br /&gt; â€¢ Delivers production-ready 3D assets in seconds for local VR and gaming workflows.&lt;br /&gt; â€¢ &lt;a href="https://3d-models.hunyuan.tencent.com/world/"&gt;Project Page&lt;/a&gt; | &lt;a href="https://github.com/Tencent-Hunyuan/HunyuanWorld-Mirror"&gt;GitHub&lt;/a&gt; | &lt;a href="https://huggingface.co/tencent/HunyuanWorld-Mirror"&gt;Hugging Face&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1ohfuea/video/1arpw5h6znxf1/player"&gt;https://reddit.com/link/1ohfuea/video/1arpw5h6znxf1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Krea Realtime - Real-Time Video Generation&lt;/strong&gt;&lt;br /&gt; â€¢ 14B model generates video at 11 fps on a single B200 GPU.&lt;br /&gt; â€¢ Enables real-time interactive video for edge-based creative applications.&lt;br /&gt; â€¢ &lt;a href="https://huggingface.co/krea/krea-realtime-video"&gt;Hugging Face&lt;/a&gt; | &lt;a href="https://x.com/krea_ai/status/1980358158376988747?s=42"&gt;Announcement&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1ohfuea/video/ula998hcznxf1/player"&gt;https://reddit.com/link/1ohfuea/video/ula998hcznxf1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;AGILE - Agentic Jigsaw Interaction Learning&lt;/strong&gt;&lt;br /&gt; â€¢ Trains VLMs via trial-and-error puzzle solving, boosting accuracy from 9.5% to 82.8%.&lt;br /&gt; â€¢ Lightweight and interactive, ideal for edge-based vision task improvement.&lt;br /&gt; â€¢ &lt;a href="https://yuzeng0-0.github.io/AGILE/"&gt;Project Page&lt;/a&gt; | &lt;a href="https://arxiv.org/abs/2510.01304"&gt;Paper&lt;/a&gt; | &lt;a href="https://github.com/yuzeng0-0/AGILE"&gt;GitHub&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/cqdgb04gznxf1.jpg?width=1456&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=2790f8e9b1e4627202fd96de5485540f4c6456ca"&gt;https://preview.redd.it/cqdgb04gznxf1.jpg?width=1456&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=2790f8e9b1e4627202fd96de5485540f4c6456ca&lt;/a&gt;&lt;/p&gt; &lt;p&gt;See the full newsletter for more demos, papers, and more resources: &lt;a href="https://open.substack.com/pub/thelivingedge/p/multimodal-monday-30-smarter-agents"&gt;https://open.substack.com/pub/thelivingedge/p/multimodal-monday-30-smarter-agents&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vast_Yak_4147"&gt; /u/Vast_Yak_4147 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohfuea/last_week_in_multimodal_ai_local_edition/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohfuea/last_week_in_multimodal_ai_local_edition/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohfuea/last_week_in_multimodal_ai_local_edition/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T14:26:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohlikz</id>
    <title>MiniMax-M2 quants?</title>
    <updated>2025-10-27T17:57:26+00:00</updated>
    <author>
      <name>/u/Tasty_Lynx2378</name>
      <uri>https://old.reddit.com/user/Tasty_Lynx2378</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's still early after release, but not seeing any early quants yet of M2:&lt;br /&gt; Are there any impediments to GGUF and MLX quants of this model?&lt;br /&gt; Have any of you tried making quants yet?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tasty_Lynx2378"&gt; /u/Tasty_Lynx2378 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohlikz/minimaxm2_quants/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohlikz/minimaxm2_quants/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohlikz/minimaxm2_quants/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T17:57:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohlhdx</id>
    <title>Phoronix benchmarks single and dual AMD R9700 GPUs against a single NVIDIA RTX 6000 Ada GPU</title>
    <updated>2025-10-27T17:56:13+00:00</updated>
    <author>
      <name>/u/Brian-Puccio</name>
      <uri>https://old.reddit.com/user/Brian-Puccio</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Brian-Puccio"&gt; /u/Brian-Puccio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.phoronix.com/review/amd-radeon-ai-pro-r9700"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohlhdx/phoronix_benchmarks_single_and_dual_amd_r9700/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohlhdx/phoronix_benchmarks_single_and_dual_amd_r9700/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T17:56:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1oh57ys</id>
    <title>MiniMaxAI/MiniMax-M2 Â· Hugging Face</title>
    <updated>2025-10-27T04:23:41+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oh57ys/minimaxaiminimaxm2_hugging_face/"&gt; &lt;img alt="MiniMaxAI/MiniMax-M2 Â· Hugging Face" src="https://external-preview.redd.it/UWFNDndMvPJsO1Z9iKM9CbvnTGrRp8W6-SXVbMO4N1g.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0f00892c38fccd0c77d2f3f510b6bc20576cdae9" title="MiniMaxAI/MiniMax-M2 Â· Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/MiniMaxAI/MiniMax-M2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oh57ys/minimaxaiminimaxm2_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oh57ys/minimaxaiminimaxm2_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T04:23:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1oh5asg</id>
    <title>ðŸš€ New Model from the MiniMax team: MiniMax-M2, an impressive 230B-A10B LLM.</title>
    <updated>2025-10-27T04:28:24+00:00</updated>
    <author>
      <name>/u/chenqian615</name>
      <uri>https://old.reddit.com/user/chenqian615</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oh5asg/new_model_from_the_minimax_team_minimaxm2_an/"&gt; &lt;img alt="ðŸš€ New Model from the MiniMax team: MiniMax-M2, an impressive 230B-A10B LLM." src="https://a.thumbs.redditmedia.com/b3_JzXejnThTVzO8xn7-iIWxAt3NTQCnaQo4XEvZSC0.jpg" title="ðŸš€ New Model from the MiniMax team: MiniMax-M2, an impressive 230B-A10B LLM." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Officially positioned as an â€œend-to-end coding + tool-using agent.â€ From the public evaluations and model setup, it looks well-suited for teams that need end to end development and toolchain agents, prioritizing lower latency and higher throughput. For real engineering workflows that advance in small but continuous steps, it should offer strong cost-effectiveness. Iâ€™ve collected a few points to help with evaluation:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;End-to-end workflow oriented, emphasizing multi-file editing, code, run, fix loops, testing/verification, and long-chain tool orchestration across terminal/browser/retrieval/code execution. These capabilities matter more than just chatting when deploying agents.&lt;/li&gt; &lt;li&gt;Publicly described as â€œ~10B activated parameters (total ~200B).â€ The design aims to reduce inference latency and per unit cost while preserving coding and tool-calling capabilities, making it suitable for high concurrency and batch sampling.&lt;/li&gt; &lt;li&gt;Benchmark coverage spans end-to-end software engineering (SWE-bench, Terminal-Bench, ArtifactsBench), browsing/retrieval tasks (BrowseComp, FinSearchComp), and holistic intelligence profiling (AA Intelligence).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Position in public benchmarks (not the absolute strongest, but well targeted)&lt;/p&gt; &lt;p&gt;Here are a few developer-relevant metrics I pulled from public tables:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;SWE-bench Verified: 69.4&lt;/li&gt; &lt;li&gt;Terminal-Bench: 46.3&lt;/li&gt; &lt;li&gt;ArtifactsBench: 66.8&lt;/li&gt; &lt;li&gt;BrowseComp: 44.0 (BrowseComp-zh in Chinese: 48.5)&lt;/li&gt; &lt;li&gt;Ï„Â²-Bench: 77.2&lt;/li&gt; &lt;li&gt;FinSearchComp-global: 65.5&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;From the scores, on tasks that require real toolchain collaboration, this model looks like a balanced choice prioritizing efficiency and stability. Some closed-source models score higher on certain benchmarks, but for end to end development/ agent pipelines, its price performance orientation is appealing. On SWE-bench / Multi-SWE-Bench, steadily completing the modify test modify again loop is often more important than a one-shot perfect fix. These scores and its positioning suggest it can keep pushing the loop toward a runnable solution. A Terminal-Bench score of 46.3 indicates decent robustness in command execution, error recovery, and retries worth trying in a real CI sandbox for small-scale tasks.&lt;/p&gt; &lt;p&gt;References&lt;/p&gt; &lt;p&gt;HF:&lt;a href="https://huggingface.co/MiniMaxAI/MiniMax-M2"&gt;https://huggingface.co/MiniMaxAI/MiniMax-M2&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chenqian615"&gt; /u/chenqian615 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1oh5asg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oh5asg/new_model_from_the_minimax_team_minimaxm2_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oh5asg/new_model_from_the_minimax_team_minimaxm2_an/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T04:28:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohbcu1</id>
    <title>Experience with the new model MiniMax M2 and some cost saving tips</title>
    <updated>2025-10-27T11:00:41+00:00</updated>
    <author>
      <name>/u/thalacque</name>
      <uri>https://old.reddit.com/user/thalacque</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohbcu1/experience_with_the_new_model_minimax_m2_and_some/"&gt; &lt;img alt="Experience with the new model MiniMax M2 and some cost saving tips" src="https://b.thumbs.redditmedia.com/UVUhaaqNSDCk6qFXVB2lhPVQVQLnJtZQw5XfM0IrY1I.jpg" title="Experience with the new model MiniMax M2 and some cost saving tips" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I saw the discussion about MiniMax M2 in the group chat a couple of days ago, and since their API and agent are free to use, I thought Iâ€™d test it out. First, the conclusion: in my own use, M2 delivers better than expected efficiency and stability. You can feel the team has pushed the modelâ€™s strengths close to top closed models. In some scenarios it reaches top results at clearly lower cost, so it fits as the default executor, with closed models kept for final polish when needed.&lt;/p&gt; &lt;p&gt;My comparison across models:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;A three service monorepo dependency and lock file mess (Node.js + Express). The three services used different versions of jsonwebtoken and had lock file conflicts. The goal was to unify versions, upgrade jwt.verify from callback to Promise, and add an npm run bootstrap script for one click dependency setup and alignment.&lt;/li&gt; &lt;/ol&gt; &lt;ul&gt; &lt;li&gt;M2: breaks down todos, understands the task well, reads files first, lists a plan, then edits step by step. It detects three version drifts and proposes an alignment strategy, adds the bootstrap script, runs one round of install and startup checks. Small fixes are quick, friendly to regression runs, and it feels ready to drop into a pipeline for repeated runs. Claude: strong first pass, but cross service consistency sometimes needed repeated reminders, took more rounds, and usage cost was higher. GLM/Kimi: can get the main path working, but more likely to leave rough edges in lock files and scripts that I had to clean up.&lt;/li&gt; &lt;/ul&gt; &lt;ol&gt; &lt;li&gt;An online 3x3 Rubikâ€™s Cube (a small front end interaction project): rotate a layer to a target angle, buttons to choose a face, show the 3x3 color grid.&lt;/li&gt; &lt;/ol&gt; &lt;ul&gt; &lt;li&gt;M2: To be honest, the first iteration wasnâ€™t great, major issues like text occlusion and non-functional rotation werenâ€™t addressed. The bright spot is that interaction bugs (e.g., rotation state desynchronization) could be fixed in a single pass once pointed out, without introducing new regressions. After subsequent rounds of refinement, the final result actually became the most usable and presentable, fully supporting 3D dragging. GLM/Kimi: The first round results were decent, but both ran into problems in the second round. GLM didnâ€™t resolve the Rubikâ€™s Cube floating/hover position issue, and Kimi, after the second round feedback, ended up not being three-dimensional. Claude performed excellently after the first round of prompts, with all features working normally, but even after multiple later rounds it still didnâ€™t demonstrate an understanding of a 3D cube (in the image, Claudeâ€™s Rubikâ€™s Cube is flat and the view canâ€™t be rotated).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Metrics echo this feel: SWE bench Verified 69.4, Terminal Bench 46.3, ArtifactsBench 66.8, BrowseComp 44.0, FinSearchComp global 65.5. It is not first in every category, but on the runnable and fixable engineering loop, the structure score looks better. From my use, the strengths are proposing a plan, checking its own work, and favoring short fast iterations that clear blockers one by one.&lt;/p&gt; &lt;p&gt;Replace most closed model usage without sacrificing the reliability of the engineering loop. M2 is already enough and surprisingly handy. Set it as the default executor and run regressions for two days; the difference will be clear. After putting it into the pipeline, with the same budget you can run more in parallel, and you do save money.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/MiniMaxAI/MiniMax-M2"&gt;https://huggingface.co/MiniMaxAI/MiniMax-M2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/MiniMax-AI/MiniMax-M2"&gt;https://github.com/MiniMax-AI/MiniMax-M2&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thalacque"&gt; /u/thalacque &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ohbcu1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohbcu1/experience_with_the_new_model_minimax_m2_and_some/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohbcu1/experience_with_the_new_model_minimax_m2_and_some/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T11:00:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohihvo</id>
    <title>Another Banger from Inclusion AI: Ming-flash-omni-Preview</title>
    <updated>2025-10-27T16:06:14+00:00</updated>
    <author>
      <name>/u/Finanzamt_Endgegner</name>
      <uri>https://old.reddit.com/user/Finanzamt_Endgegner</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohihvo/another_banger_from_inclusion_ai/"&gt; &lt;img alt="Another Banger from Inclusion AI: Ming-flash-omni-Preview" src="https://external-preview.redd.it/PFGMqHZG1FenJLDckxpcToXwao333pejl_fZNW4bWqk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=04ce1f6abd38ec8e123f358b2f5b41d3b28a30d6" title="Another Banger from Inclusion AI: Ming-flash-omni-Preview" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/inclusionAI/Ming-flash-omni-Preview"&gt;https://huggingface.co/inclusionAI/Ming-flash-omni-Preview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Based on &lt;a href="https://huggingface.co/inclusionAI/Ling-flash-2.0"&gt;Ling-Flash-2.0&lt;/a&gt; this model has 100b total parameters and 6b active ones and supports context aware asr, text to speech, image generation and editing, segmentation etc (well its an omni modal model so you know the drill). Since its fairly sparse it is very efficient and while I couldn't test it myself the benchmarks seem promising, and it also supports voice cloning (;&lt;/p&gt; &lt;p&gt;It says it can do dialect-aware ASR, though im not sure if that will only work with Chinese ðŸ¤”&lt;/p&gt; &lt;p&gt;Anyways, if im not mistaken this is the biggest open sourced omni modal model yet so thanks to the mad lads at inclusion ai!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/qml9ai33goxf1.png?width=2972&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=29df775ad390dc4e6cb4306e302540f231bdf556"&gt;https://preview.redd.it/qml9ai33goxf1.png?width=2972&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=29df775ad390dc4e6cb4306e302540f231bdf556&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1ohihvo/video/oh86jahegoxf1/player"&gt;https://reddit.com/link/1ohihvo/video/oh86jahegoxf1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1ohihvo/video/zbxb11vnhoxf1/player"&gt;https://reddit.com/link/1ohihvo/video/zbxb11vnhoxf1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Finanzamt_Endgegner"&gt; /u/Finanzamt_Endgegner &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohihvo/another_banger_from_inclusion_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohihvo/another_banger_from_inclusion_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohihvo/another_banger_from_inclusion_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T16:06:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohdl9q</id>
    <title>Silicon Valley is migrating from expensive closed-source models to cheaper open-source alternatives</title>
    <updated>2025-10-27T12:53:12+00:00</updated>
    <author>
      <name>/u/xiaoruhao</name>
      <uri>https://old.reddit.com/user/xiaoruhao</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohdl9q/silicon_valley_is_migrating_from_expensive/"&gt; &lt;img alt="Silicon Valley is migrating from expensive closed-source models to cheaper open-source alternatives" src="https://external-preview.redd.it/cDlpaWtncThobnhmMYyuPjWRTezxeRfqB3upVJ5ATISaueUIVVjdl6ikWaxE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=49803f057b43fcece9390b3b966fe6ba4de209b3" title="Silicon Valley is migrating from expensive closed-source models to cheaper open-source alternatives" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Chamath Palihapitiya said his team migrated a large number of workloads to Kimi K2 because it was significantly more performant and much cheaper than both OpenAI and Anthropic.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xiaoruhao"&gt; /u/xiaoruhao &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/avwpphq8hnxf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohdl9q/silicon_valley_is_migrating_from_expensive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohdl9q/silicon_valley_is_migrating_from_expensive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T12:53:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohdzxs</id>
    <title>AMA Announcement: Liquid AI, the team behind Liquid Foundational Models, LEAP and Apollo (Thu, Oct 30 â€¢ 10 AM â€“ 1 PM PDT)</title>
    <updated>2025-10-27T13:10:46+00:00</updated>
    <author>
      <name>/u/LiquidAI_Team</name>
      <uri>https://old.reddit.com/user/LiquidAI_Team</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohdzxs/ama_announcement_liquid_ai_the_team_behind_liquid/"&gt; &lt;img alt="AMA Announcement: Liquid AI, the team behind Liquid Foundational Models, LEAP and Apollo (Thu, Oct 30 â€¢ 10 AM â€“ 1 PM PDT)" src="https://preview.redd.it/47wfyylmlnxf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c359ceba4921b523ecd2e493f9cc84bd8b3e7881" title="AMA Announcement: Liquid AI, the team behind Liquid Foundational Models, LEAP and Apollo (Thu, Oct 30 â€¢ 10 AM â€“ 1 PM PDT)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;When: Thursday 10/30, 10 AM â€“ 1 PM PST&lt;/h1&gt; &lt;p&gt;The Liquid AI team will also continue answering questions for the following 24 hours, so jump in anytime!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Who will be there:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Jacob Marks (Data)&lt;/li&gt; &lt;li&gt;Jimmy Smith (Pre-Training)&lt;/li&gt; &lt;li&gt;Maxime Labonne (Post-Training)&lt;/li&gt; &lt;li&gt;Fernando Fernandes (Post-training)&lt;/li&gt; &lt;li&gt;Anna Banaszak (LFM2-VL)&lt;/li&gt; &lt;li&gt;Arthur BÃ¶Ã¶k (LFM2-Audio)&lt;/li&gt; &lt;li&gt;Yuri Khrustalev (Inference engine, llama.cpp)&lt;/li&gt; &lt;li&gt;Darian Bhathena (LEAP SDK and Apollo)&lt;/li&gt; &lt;li&gt;Edoardo Mosca (LEAP Best Model Search and Finetune)&lt;/li&gt; &lt;li&gt;Anthony Crognale (LEAP SDK)&lt;/li&gt; &lt;li&gt;Pau Labarta Bajo (Dev Relations)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Want to get started?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;â†’ &lt;a href="https://leap.liquid.ai/models?utm_source=reddit&amp;amp;utm_medium=devrel"&gt;Deploy your first model on-device today&lt;/a&gt;&lt;br /&gt; â†’ &lt;a href="https://huggingface.co/LiquidAI?utm_source=reddit&amp;amp;utm_medium=devrel"&gt;Check out our models on Hugging Face&lt;/a&gt;&lt;br /&gt; â†’ &lt;a href="https://www.liquid.ai/apollo?utm_source=reddit&amp;amp;utm_medium=devrel"&gt;Play with models on Apollo&lt;/a&gt;&lt;br /&gt; â†’ &lt;a href="https://www.liquid.ai/company/news?utm_source=reddit&amp;amp;utm_medium=devrel"&gt;Learn more about our recent releases&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LiquidAI_Team"&gt; /u/LiquidAI_Team &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/47wfyylmlnxf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohdzxs/ama_announcement_liquid_ai_the_team_behind_liquid/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohdzxs/ama_announcement_liquid_ai_the_team_behind_liquid/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T13:10:46+00:00</published>
  </entry>
</feed>
