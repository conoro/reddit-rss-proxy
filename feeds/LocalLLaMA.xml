<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-02T00:31:55+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1q163um</id>
    <title>GLM 4.7 on 8x3090</title>
    <updated>2026-01-01T15:00:38+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is anyone running GLM 4.7 (or 4.5-4.6) on eight 3090s? I was wondering what kind of speeds you were getting as I was considering this set up.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q163um/glm_47_on_8x3090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q163um/glm_47_on_8x3090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q163um/glm_47_on_8x3090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-01T15:00:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1q19zmx</id>
    <title>Tuneable Attention: How expanding (not compressing) the attention mechanism dramatically accelerated my model's learning speed</title>
    <updated>2026-01-01T17:43:26+00:00</updated>
    <author>
      <name>/u/Correct_Address3554</name>
      <uri>https://old.reddit.com/user/Correct_Address3554</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;BODY:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I've been training LLMs on budget hardware (Tesla P40, GTX TITAN X via vast.ai) since 2016, and I recently published a writeup of an architectural modification I stumbled into that significantly accelerated language acquisition in my models.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The TL;DR:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Standard attention computes Q √ó K^T. My modification factors this as Q √ó (U √ó U^T) √ó K^T, where U is a learned projection matrix. When the rank of U is less than d_k, you get compression (cheaper compute). When rank is greater than d_k, you get EXPANSION (more compute per step, but faster convergence).&lt;/p&gt; &lt;p&gt;I originally derived this targeting the compression regime for efficiency. But through hyperparameter drift over many training runs, the rank value accidentally crossed above d_k into the expansion regime. The result: a sub-200M parameter model that acquired coherent English grammar in approximately ONE DAY of training, when previous runs at similar scale had taken much longer.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The key insight:&lt;/strong&gt; Attention routing (where to look) can benefit from expanded &amp;quot;scratch space,&amp;quot; but value aggregation (what to grab) should stay at full dimensionality. So Q and K get projected through U, but V does not.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Current status:&lt;/strong&gt; Training AGILLM-3 with 3x expansion (rank=96, d_k=32), currently at 5M steps / 11% through chinchilla-optimal. Outputs are grammatically perfect, semantic coherence still developing.&lt;/p&gt; &lt;p&gt;Full writeup with math, code, and the story of how I accidentally discovered this: &lt;a href="https://medium.com/@MarxismLeninism/tuneable-attention-how-an-accidental-hyperparameter-drift-revealed-that-expansion-beats-1a39b9bbe72d?postPublishedType=initial"&gt;https://medium.com/@MarxismLeninism/tuneable-attention-how-an-accidental-hyperparameter-drift-revealed-that-expansion-beats-1a39b9bbe72d?postPublishedType=initial&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Curious if anyone else has experimented with rank &amp;gt; d_k in attention projections. Everything I've seen in the literature focuses on compression (LoRA, Linformer, etc.) ‚Äî the expansion regime seems unexplored.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Correct_Address3554"&gt; /u/Correct_Address3554 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q19zmx/tuneable_attention_how_expanding_not_compressing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q19zmx/tuneable_attention_how_expanding_not_compressing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q19zmx/tuneable_attention_how_expanding_not_compressing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-01T17:43:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0vph6</id>
    <title>OpenForecaster Release</title>
    <updated>2026-01-01T04:30:45+00:00</updated>
    <author>
      <name>/u/logisbase2</name>
      <uri>https://old.reddit.com/user/logisbase2</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0vph6/openforecaster_release/"&gt; &lt;img alt="OpenForecaster Release" src="https://preview.redd.it/iuw1u1y61oag1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=39850abc1dea70f66b62d9a4cafb3424faf01f53" title="OpenForecaster Release" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/papers/2512.25070"&gt;https://huggingface.co/papers/2512.25070&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/logisbase2"&gt; /u/logisbase2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/iuw1u1y61oag1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0vph6/openforecaster_release/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q0vph6/openforecaster_release/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-01T04:30:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1q15vr6</id>
    <title>Any clues as to what Gemma 3's training data consisted of?</title>
    <updated>2026-01-01T14:50:22+00:00</updated>
    <author>
      <name>/u/EducationalCicada</name>
      <uri>https://old.reddit.com/user/EducationalCicada</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know Google would never release this information, but has anyone been able to extract parts of the training data from Gemma 3? I'm really curious about what they used.&lt;/p&gt; &lt;p&gt;I'm guessing it was trained on public domain (and lower quality, compared to what they fed Gemini) data due to the existence of such attacks on open-weight models.&lt;/p&gt; &lt;p&gt;It's a bit frustrating because Google is sitting on some of the most valuable data on the planet , but Gemma will never see any of it in training.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EducationalCicada"&gt; /u/EducationalCicada &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q15vr6/any_clues_as_to_what_gemma_3s_training_data/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q15vr6/any_clues_as_to_what_gemma_3s_training_data/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q15vr6/any_clues_as_to_what_gemma_3s_training_data/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-01T14:50:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0uoys</id>
    <title>Top 10 Open Models by Providers on LMArena</title>
    <updated>2026-01-01T03:32:13+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0uoys/top_10_open_models_by_providers_on_lmarena/"&gt; &lt;img alt="Top 10 Open Models by Providers on LMArena" src="https://preview.redd.it/xo7h0asvqnag1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1f90fd4135eb4647968b39123e68d3e463e24269" title="Top 10 Open Models by Providers on LMArena" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xo7h0asvqnag1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0uoys/top_10_open_models_by_providers_on_lmarena/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q0uoys/top_10_open_models_by_providers_on_lmarena/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-01T03:32:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1q1fo4p</id>
    <title>Testing LLM ability to port code - Comparison and Evaluation</title>
    <updated>2026-01-01T21:29:22+00:00</updated>
    <author>
      <name>/u/Morphon</name>
      <uri>https://old.reddit.com/user/Morphon</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, fellow AI enthusiasts!&lt;/p&gt; &lt;p&gt;Recall the recent kerfluffle surrounding the Microsoft executive who was claiming that he would replace all C and C++ code at Microsoft in 5 years. His &amp;quot;north star&amp;quot; was &amp;quot;1 engineer, 1 month, 1 million lines of code.&amp;quot; Given the swift &amp;quot;clarification&amp;quot; of his remarks, I have no idea what they plan to do or when. But it made me wonder - just how good are the current LLMs for this task? They have been churning out impressive &amp;quot;make a TETRIS clone in a browser,&amp;quot; &amp;quot;make a note taking app in a browser,&amp;quot; &amp;quot;make a recipe app in a browser,&amp;quot; for some time now. They also seem to be pretty useful at hunting down bugs (given enough direction). But for the 1xPerson+1xMonth+1mLines goal the entire process would have to be almost completely automated. There was some oft-cited quick math that gave the engineer roughly 6 seconds to approve each line of code as it was generated.&lt;/p&gt; &lt;p&gt;I saw a video by code_report on Youtube (he's amazing, btw) where he was looking at how C++ can do some calculations entirely in the compiler. He was using problem 3115 from leetCode to demonstrate constexpr and consteval, and it occurred to me that this little problem would be a good way to test LLM porting abilities.&lt;/p&gt; &lt;p&gt;&lt;a href="https://leetcode.com/problems/maximum-prime-difference/description/"&gt;https://leetcode.com/problems/maximum-prime-difference/description/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I wrote up a quick, somewhat degenerate version in JS. And if anyone thinks that it was generated by AI, I dare you to try to get a LLM to produce something like this:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;const isPrime = (n, current = Math.ceil(Math.sqrt(n))) =&amp;gt; { if (n === 2) return true; if (n &amp;lt; 2) return false; if (n % 1 !== 0) return false; if (current === 1) return true; if (n % current === 0) return false; if (current % 2 !== 0 &amp;amp;&amp;amp; current !== 3) current--; return isPrime(n, current - 1); }; const maximumPrimeDifference = (nums) =&amp;gt; { const primeList = nums .map((number, index) =&amp;gt; [number, index]) .filter((element) =&amp;gt; isPrime(element[0])); return primeList[primeList.length - 1][1] - primeList[0][1]; }; const tests = []; tests.push([4, 2, 9, 5, 3]); tests.push([4, 8, 2, 8]); tests.push([11, 7, 13, 29, 2]); tests.push([100000000057, 6, 2, 103, 0.1666666667]); tests.forEach((set) =&amp;gt; console.log(maximumPrimeDifference(set))); console.log(isPrime(8)); &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The maximumPrimeDifference function is pretty straightforward. It uses more memory than absolutely necessary since it keeps primes (and their indices) between the first and last, but it isn't particularly strange.&lt;/p&gt; &lt;p&gt;The isPrime function is the real test. It does, in fact, return TRUE when the number passed to it is a prime number, and returns FALSE when the number passed to it is non-prime. But it does so in a way that will be tricky for a LLM to understand and port. Here are a few &amp;quot;surprises&amp;quot; for the LLM:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It returns false for numbers that are not positive integers. A language's prime-checking functions included in the standard library may throw an error when given a negative or floating point number. The LLM needs to know whether or not it can replace this function with something from the standard library.&lt;/li&gt; &lt;li&gt;It has a really strange method for only checking odd divisors other than the number 2. The LLM needs to be able to &amp;quot;understand&amp;quot; that this actually works. It can keep it, or use some other method to skip even divisors (as long as it checks at least one even divisor). Even if it does not preserve this &amp;quot;optimization&amp;quot; at all and checks every number, it would still &amp;quot;pass&amp;quot; because it would produce the correct output. An LLM calling this a &amp;quot;bug&amp;quot; rather than infelicitous or unoptimized is a mark against that LLM.&lt;/li&gt; &lt;li&gt;It is recursive. This much wouldn't be an issue for the original leetCode constraints (numbers have to be 1-100), but one of my test cases has a very large prime number. Wouldn't this blow the stack? Well, I'm running this on Bun and that runtime has proper TCO. I mention in the prompt that I'm running on Bun, but I do not say &lt;em&gt;why&lt;/em&gt; I am doing so. The LLM should know this about Bun. When it sees this very large prime in the test case, the expected output (from the prompt), and the use of the Bun runtime, it should &amp;quot;put 2 and 2 together&amp;quot; and rewrite this function as a WHILE loop for languages that do not have TCO.&lt;/li&gt; &lt;li&gt;It has an &amp;quot;undocumented&amp;quot; feature. Yes, it is called &amp;quot;isPrime&amp;quot; and when passed a single argument it will in fact return true iff the number is prime. However, it takes a second argument. That second argument is normally just the default (rounded up square root of the first argument), but it can be given another &amp;quot;starting point&amp;quot;. What this function &lt;em&gt;actually&lt;/em&gt; does is return true if the first number 1) is a positive integer, and 2) has no factors greater than 1 and less than or equal to the second number. So, isPrime(77,6) should return &amp;quot;true&amp;quot;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Now - why the &amp;quot;undocumented&amp;quot; feature? Well, a complete port would need to replicate all the behavior of the original. It needs to be feature for feature, and bug for bug the same. If this was a CLI tool there might be some script out there that exploited this undocumented behavior as a kind of shortcut or &amp;quot;hack&amp;quot; to accomplish who-knows-what. &amp;quot;Fixing&amp;quot; this would mean that the script relying on it would simply break.&lt;/p&gt; &lt;p&gt;Of course, if I wanted a really elegant solution to leetCode 3115 I could just ask for that. Any of the bigger thinking models can produce a working (and fast, and clean) implementation barely breaking a sweat. But if people out there are talking about using LLMs to translate code from one language to another they won't be doing so from extremely clear and unambiguous original design documents. They'll be working from an already-existing codebase, with all sorts of strange things in there. Imagine all the workarounds and seemingly needless clusters of IF statements in a truly old codebase (like the COBOL batch processing running the banking system). If those get &amp;quot;optimized&amp;quot; away...&lt;/p&gt; &lt;p&gt;Anyway.... I think, on the whole, this &lt;em&gt;should be&lt;/em&gt; a relatively easy porting task. There are only two functions and neither have side-effects. It's doing some pretty basic math and array manipulation. The recursion method is not mindbending. Should be easy.....&lt;/p&gt; &lt;p&gt;Here's the prompt:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Please port this short program to &amp;lt;insert language here&amp;gt;. The resulting program must have identical behavior (including bugs and unusual behavior). That is, given identical input, it should produce identical output. The rewrite cannot use any 3rd party libraries, but can incorporate any idiomatic changes (including from the standard library) that would make it more &amp;quot;natural&amp;quot; or performant in the target language. The original JS program is executed using the Bun runtime.&lt;/p&gt; &lt;p&gt;Expected output is:&lt;br /&gt; 3&lt;br /&gt; 0&lt;br /&gt; 4&lt;br /&gt; 3&lt;br /&gt; false&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Target languages were: Python (scripting), Haskell (compiled functional), C++19 (obviously), and Rust (also obviously). If you want to try out another language, please feel free to do so and post your results below.&lt;/p&gt; &lt;p&gt;LLMs were run through t3.chat.&lt;/p&gt; &lt;p&gt;Kimi K2-Thinking&lt;br /&gt; Minimax M2.1&lt;br /&gt; DeepSeek 3.2 Thinking&lt;br /&gt; GLM 4.7 Thinking&lt;br /&gt; GPT-OSS 120B&lt;/p&gt; &lt;p&gt;Bonus - I tried to do this with an Oberon7 target, just to see how well the LLM could use an older niche language. All failed to produce code that compiled without errors. Claude Haiku 4.5 Thinking, after several debugging steps, was able to write something that compiled and gave the proper test output. I didn't bother to check the &amp;quot;undocumented&amp;quot; feature. I doubt anyone is porting their work &lt;em&gt;to&lt;/em&gt; Oberon7.&lt;/p&gt; &lt;p&gt;K2-Haskell - Fail (does not compile, even on revision)&lt;br /&gt; -C++19 - Pass&lt;br /&gt; -Rust - Pass&lt;br /&gt; -Python - Fail (runs, but no undocumented feature)&lt;br /&gt; GLM -Haskell - Fail (does not compile, even on revision).&lt;br /&gt; -C++19 - Fail (compiles and runs, no undocumented feature)&lt;br /&gt; -Rust - Fail (compiles and runs, no undocumented feature)&lt;br /&gt; -Python - Fail (tried to add a new attribute to list object. Once corrected, runs perfectly).&lt;br /&gt; MM2.1 -Haskell - Fail (compiles, infinite loop with no output).&lt;br /&gt; -C++19 - Pass&lt;br /&gt; -Rust - Fail (compiles and runs, but no undocumented feature)&lt;br /&gt; -Python - Fail (runs, but no undocumented feature)&lt;br /&gt; DeepSeek -Haskell - Fail (compiles and runs, but undocumented feature is called differently from regular isPrime. This is as close to &amp;quot;pass&amp;quot; as we're going to get with Haskell)&lt;br /&gt; -C++19 - Pass&lt;br /&gt; -Rust - Fail (stack overflow, but preserves undocumented feature)&lt;br /&gt; -Python - Fail (stack overflow, but preserves undocumented feature)&lt;br /&gt; GPT-OSS -Haskell - Fail (compiles and runs, but no undocumented feature)&lt;br /&gt; -C++19 - Pass&lt;br /&gt; -Rust - Fail (compiles and runs, no undocumented feature)&lt;br /&gt; -Python - Fail (stack overflow, but preserves undocumented feature)&lt;/p&gt; &lt;p&gt;General notes - DeepSeek 3.2 thought the &amp;quot;skip even numbers&amp;quot; was a bug, and insisted on it being a bug when doing rewrites unless directly asked where the bug was. It would then spend quite a while trying out a bunch of corner cases until eventually admitting that it was not a bug. Qwen3 figured out that it wasn't a bug, but it used up thousands upon thousands of tokens trying and failing to convince itself that it was a bug, until finally admitting to itself that the code worked as written. By that time it used up its token budget and did not produce any complete solution. I had to remove it from the test.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Morphon"&gt; /u/Morphon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1fo4p/testing_llm_ability_to_port_code_comparison_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1fo4p/testing_llm_ability_to_port_code_comparison_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q1fo4p/testing_llm_ability_to_port_code_comparison_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-01T21:29:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1q16w0k</id>
    <title>The State Of LLMs 2025: Progress, Problems, and Predictions</title>
    <updated>2026-01-01T15:35:11+00:00</updated>
    <author>
      <name>/u/seraschka</name>
      <uri>https://old.reddit.com/user/seraschka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q16w0k/the_state_of_llms_2025_progress_problems_and/"&gt; &lt;img alt="The State Of LLMs 2025: Progress, Problems, and Predictions" src="https://external-preview.redd.it/ip3t1phQ469yOBa2kbOD__RHIhAqj8C7dU-KA_Pn0lI.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=859212246454f6bb28f9d882875d0a6ae9694d87" title="The State Of LLMs 2025: Progress, Problems, and Predictions" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/seraschka"&gt; /u/seraschka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://magazine.sebastianraschka.com/p/state-of-llms-2025"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q16w0k/the_state_of_llms_2025_progress_problems_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q16w0k/the_state_of_llms_2025_progress_problems_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-01T15:35:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1q1aif6</id>
    <title>Running an unsupported DeepSeek V3.2 in llama.cpp for some New Year's fun</title>
    <updated>2026-01-01T18:03:59+00:00</updated>
    <author>
      <name>/u/fairydreaming</name>
      <uri>https://old.reddit.com/user/fairydreaming</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So yesterday was a New Year's eve and somewhere between a fourth beer and a glass of bubbly I had this weird idea to see how DeepSeek V3.2 runs with a dense (non-sparse) attention. I know that sparse attention is the main highlight of this release, but people were trying to vibe-code it for months so maybe we can just skip that for now and use the model as it is...&lt;/p&gt; &lt;p&gt;So here's the patch allowing to convert and use the model with the current llama.cpp:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;diff --git a/convert_hf_to_gguf.py b/convert_hf_to_gguf.py index edc0ed539..dd6dbe8d5 100755 --- a/convert_hf_to_gguf.py +++ b/convert_hf_to_gguf.py @@ -952,6 +952,9 @@ class TextModel(ModelBase): return seems_special + def override_tokenizer_settings(self, tokenizer): + return tokenizer + # used for GPT-2 BPE and WordPiece vocabs def get_vocab_base(self) -&amp;gt; tuple[list[str], list[int], str]: tokens: list[str] = [] @@ -959,6 +962,7 @@ class TextModel(ModelBase): from transformers import AutoTokenizer tokenizer = AutoTokenizer.from_pretrained(self.dir_model) + tokenizer = self.override_tokenizer_settings(tokenizer) vocab_size = self.hparams.get(&amp;quot;vocab_size&amp;quot;, len(tokenizer.vocab)) assert max(tokenizer.vocab.values()) &amp;lt; vocab_size @@ -7180,14 +7184,26 @@ class DeepseekModel(TextModel): u/ModelBase.register( &amp;quot;DeepseekV2ForCausalLM&amp;quot;, &amp;quot;DeepseekV3ForCausalLM&amp;quot;, + &amp;quot;DeepseekV32ForCausalLM&amp;quot;, &amp;quot;KimiVLForConditionalGeneration&amp;quot;, ) class DeepseekV2Model(TextModel): model_arch = gguf.MODEL_ARCH.DEEPSEEK2 + def override_tokenizer_settings(self, tokenizer): + # override add_bos_token setting to get pre-tokenizer recognized + if self.hparams.get(&amp;quot;model_type&amp;quot;) == &amp;quot;deepseek_v32&amp;quot;: + tokenizer.add_bos_token = True + return tokenizer + def set_vocab(self): try: self._set_vocab_gpt2() + # in DeepSeek V3.2 adding BOS token is disabled in tokenizer configuration + # instead the BOS token is added in encode_messages() Python code + # therefore we have to override this setting + if self.hparams.get(&amp;quot;model_type&amp;quot;) == &amp;quot;deepseek_v32&amp;quot;: + self.gguf_writer.add_add_bos_token(True) return except Exception: pass @@ -7277,7 +7293,7 @@ class DeepseekV2Model(TextModel): def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -&amp;gt; Iterable[tuple[str, Tensor]]: # skip vision tensors and remove &amp;quot;language_model.&amp;quot; for Kimi-VL - if &amp;quot;vision_tower&amp;quot; in name or &amp;quot;multi_modal_projector&amp;quot; in name: + if &amp;quot;vision_tower&amp;quot; in name or &amp;quot;multi_modal_projector&amp;quot; in name or &amp;quot;self_attn.indexer&amp;quot; in name: return [] if name.startswith(&amp;quot;language_model.&amp;quot;): &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;It's nothing fancy, just:&lt;/p&gt; &lt;ul&gt; &lt;li&gt; an override for add_bos_token that is now false in 3.2 and it prevents the (unchanged) pre-tokenizer from being recognized,&lt;/li&gt; &lt;li&gt;skipping lightning indexer tensors since there's no support for that yet.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;With this patch I converted the model:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;python convert_hf_to_gguf.py /mnt/md0/huggingface/hub/models--deepseek-ai--DeepSeek-V3.2/snapshots/a7e62ac04ecb2c0a54d736dc46601c5606cf10a6/ --outfile /mnt/md0/models/DeepSeek-V3.2-nolight.gguf --outtype q8_0 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note that DeepSeek V3.2 has no jinja chat template at all. AFAIK the overall structure of the template did not change, but they use some new tool calls that require some Python code magic.&lt;/p&gt; &lt;p&gt;I tried to run the model with &lt;code&gt;--chat-template deepseek3&lt;/code&gt; but for some reason it didn't work correctly (bug in llama.cpp?). So instead I saved jinja template from DeepSeek V3 to a file and used this command:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ ./bin/llama-cli -m /mnt/md0/models/DeepSeek-V3.2-nolight.gguf -nr --temp 0.0 -cnv --jinja --chat-template-file ../deepseek3.jinja -p &amp;quot;who are you?&amp;quot; Loading model... ‚ñÑ‚ñÑ ‚ñÑ‚ñÑ ‚ñà‚ñà ‚ñà‚ñà ‚ñà‚ñà ‚ñà‚ñà ‚ñÄ‚ñÄ‚ñà‚ñÑ ‚ñà‚ñà‚ñà‚ñÑ‚ñà‚ñà‚ñà‚ñÑ ‚ñÄ‚ñÄ‚ñà‚ñÑ ‚ñÑ‚ñà‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà‚ñà‚ñÑ ‚ñà‚ñà‚ñà‚ñà‚ñÑ ‚ñà‚ñà ‚ñà‚ñà ‚ñÑ‚ñà‚ñÄ‚ñà‚ñà ‚ñà‚ñà ‚ñà‚ñà ‚ñà‚ñà ‚ñÑ‚ñà‚ñÄ‚ñà‚ñà ‚ñà‚ñà ‚ñà‚ñà ‚ñà‚ñà ‚ñà‚ñà ‚ñà‚ñà ‚ñà‚ñà ‚ñà‚ñà ‚ñÄ‚ñà‚ñÑ‚ñà‚ñà ‚ñà‚ñà ‚ñà‚ñà ‚ñà‚ñà ‚ñÄ‚ñà‚ñÑ‚ñà‚ñà ‚ñà‚ñà ‚ñÄ‚ñà‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà‚ñà‚ñÄ ‚ñà‚ñà‚ñà‚ñà‚ñÄ ‚ñà‚ñà ‚ñà‚ñà ‚ñÄ‚ñÄ ‚ñÄ‚ñÄ build : b7597-13814eb37 model : DeepSeek-V3.2-nolight.gguf modalities : text available commands: /exit or Ctrl+C stop or exit /regen regenerate the last response /clear clear the chat history /read add a text file &amp;gt; who are you? Hello! I'm DeepSeek, an AI assistant created by DeepSeek Company. I'm here to help you with a wide variety of tasks - whether you need answers to questions, help with writing, analysis, problem-solving, coding assistance, or just want to have a conversation! I'm a text-based AI model with some special features: - I can process uploaded files (images, PDFs, Word docs, Excel sheets, PowerPoint presentations, and text files) - I have a large 128K context window for handling lengthy conversations and documents - I support web search functionality (though you need to manually enable it in the Web/App interface) - I'm completely free to use with no charges I'm designed to be helpful, detailed, and warm in my responses. How can I assist you today? üòä [ Prompt: 14.3 t/s | Generation: 6.9 t/s ] &amp;gt; how many r's are in the phrase &amp;quot;crrrunchy crrrabby crrrabs&amp;quot;? Let‚Äôs count the letter **‚Äúr‚Äù** in the phrase: **&amp;quot;crrrunchy crrrabby crrrabs&amp;quot;** Break it down word by word: 1. **crrrunchy** ‚Üí c r r r u n c h y ‚Üí **3 r‚Äôs** 2. **crrrabby** ‚Üí c r r r a b b y ‚Üí **3 r‚Äôs** 3. **crrrabs** ‚Üí c r r r a b s ‚Üí **3 r‚Äôs** Total = 3 + 3 + 3 = **9 r‚Äôs**. So the answer is **9**. [ Prompt: 18.7 t/s | Generation: 6.7 t/s ] &amp;gt; what is 21345*1234? solve this like a high-schooler with a pen and paper would do Alright, let‚Äôs multiply **21345 √ó 1234** step-by-step like in school. We‚Äôll break 1234 into: 1234 = 1000 + 200 + 30 + 4 --- **Step 1: Multiply 21345 by 1000** 21345 √ó 1000 = 21,345,000 --- **Step 2: Multiply 21345 by 200** 21345 √ó 2 = 42,690 Then √ó 100 = 4,269,000? Wait, careful: 21345 √ó 200 = 21345 √ó 2 √ó 100 = 42,690 √ó 100 = 4,269,000 --- **Step 3: Multiply 21345 by 30** 21345 √ó 3 = 64,035 Then √ó 10 = 640,350 --- **Step 4: Multiply 21345 by 4** 21345 √ó 4 = 85,380 --- **Step 5: Add them all up** First, write them aligned by place value: ``` 21,345,000 (√ó1000) + 4,269,000 (√ó200) + 640,350 (√ó30) + 85,380 (√ó4) ``` --- Add step-by-step: 21,345,000 + 4,269,000 = 25,614,000 25,614,000 + 640,350 = 26,254,350 26,254,350 + 85,380 = 26,339,730 --- **Final answer:** **26,339,730** ‚úÖ [ Prompt: 17.6 t/s | Generation: 6.5 t/s ] &amp;gt; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Looks good so far, now I'm going to try my lineage-bench benchmark to see if the model is dumber with dense attention. Hopefully not!&lt;/p&gt; &lt;p&gt;Happy New Year!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fairydreaming"&gt; /u/fairydreaming &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1aif6/running_an_unsupported_deepseek_v32_in_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1aif6/running_an_unsupported_deepseek_v32_in_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q1aif6/running_an_unsupported_deepseek_v32_in_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-01T18:03:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1q1i4l5</id>
    <title>What is the best model that a 7900xtx can run at a decent token rate?</title>
    <updated>2026-01-01T23:10:18+00:00</updated>
    <author>
      <name>/u/AdhesivenessAny9809</name>
      <uri>https://old.reddit.com/user/AdhesivenessAny9809</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want some suggestions on which models I should be running on my system, preferably coding models or regular models with good coding skills.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdhesivenessAny9809"&gt; /u/AdhesivenessAny9809 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1i4l5/what_is_the_best_model_that_a_7900xtx_can_run_at/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1i4l5/what_is_the_best_model_that_a_7900xtx_can_run_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q1i4l5/what_is_the_best_model_that_a_7900xtx_can_run_at/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-01T23:10:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0vom4</id>
    <title>IQuestLab/IQuest-Coder-V1 ‚Äî 40B parameter coding LLM ‚Äî Achieves leading results on SWE-Bench Verified (81.4%), BigCodeBench (49.9%), LiveCodeBench v6 (81.1%)</title>
    <updated>2026-01-01T04:29:26+00:00</updated>
    <author>
      <name>/u/TellMeAboutGoodManga</name>
      <uri>https://old.reddit.com/user/TellMeAboutGoodManga</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0vom4/iquestlabiquestcoderv1_40b_parameter_coding_llm/"&gt; &lt;img alt="IQuestLab/IQuest-Coder-V1 ‚Äî 40B parameter coding LLM ‚Äî Achieves leading results on SWE-Bench Verified (81.4%), BigCodeBench (49.9%), LiveCodeBench v6 (81.1%)" src="https://external-preview.redd.it/BV6FpKtNQWUgU3wdfJKH5UR3dlogn4uR0Fs4eIn5vSk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3873330c45da75d454dd4483d37c39a58e5c6810" title="IQuestLab/IQuest-Coder-V1 ‚Äî 40B parameter coding LLM ‚Äî Achieves leading results on SWE-Bench Verified (81.4%), BigCodeBench (49.9%), LiveCodeBench v6 (81.1%)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TellMeAboutGoodManga"&gt; /u/TellMeAboutGoodManga &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/IQuestLab/IQuest-Coder-V1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0vom4/iquestlabiquestcoderv1_40b_parameter_coding_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q0vom4/iquestlabiquestcoderv1_40b_parameter_coding_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-01T04:29:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1q1gps8</id>
    <title>Minimax M2.1 `&lt;think&gt;` tag and Interleaved Thinking</title>
    <updated>2026-01-01T22:11:47+00:00</updated>
    <author>
      <name>/u/x0xxin</name>
      <uri>https://old.reddit.com/user/x0xxin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been experimenting with Minimax M2.1 using the default chat template that Minimax distributed with the model. Specially I'm running an EXL3 quant.&lt;/p&gt; &lt;p&gt;I've noticed that regardless of the client, the model always performs reasoning at the beginning of its chat completion but never prepends &amp;lt;think&amp;gt;. That chat template effectively tells the model that &amp;lt;think&amp;gt; is already present:&lt;/p&gt; &lt;p&gt;&lt;code&gt; {#- Generation prompt -#} {%- if add_generation_prompt -%} {{- ']~b]ai' ~ '\n' ~ '&amp;lt;think&amp;gt;' ~ '\n' }} {%- endif -%} &lt;/code&gt;&lt;/p&gt; &lt;p&gt;I assume that as interleaved models become more popular we will see more chat templates like this because they support the model engaging in reasoning and tool use at various points during a single completion.&lt;/p&gt; &lt;p&gt;Has anyone tried to get Open-WebUI and another client like SillyTavern to properly identify reasoning when the &amp;lt;think&amp;gt; token isn't present but the &amp;lt;/think&amp;gt; token is? &lt;/p&gt; &lt;p&gt;This seems like a non-trivial problem. One solution would be to modify the chat template. My concern there is that it could impact the model's ability to use Interleaved Reasoning.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/x0xxin"&gt; /u/x0xxin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1gps8/minimax_m21_think_tag_and_interleaved_thinking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1gps8/minimax_m21_think_tag_and_interleaved_thinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q1gps8/minimax_m21_think_tag_and_interleaved_thinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-01T22:11:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0uuqt</id>
    <title>Happy New Year: Llama3.3-8B-Instruct-Thinking-Claude-4.5-Opus-High-Reasoning - Fine Tune. (based on recent find of L3.3 8b in the wild)</title>
    <updated>2026-01-01T03:41:30+00:00</updated>
    <author>
      <name>/u/Dangerous_Fix_5526</name>
      <uri>https://old.reddit.com/user/Dangerous_Fix_5526</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Special thanks to :&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/user/jacek2023/"&gt;jacek2023&lt;/a&gt; [posting about this model]&lt;/p&gt; &lt;p&gt;and extra special thanks for &amp;quot;&lt;strong&gt;allura-forge&lt;/strong&gt; &amp;quot; for finding this model:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/allura-forge/Llama-3.3-8B-Instruct"&gt;https://huggingface.co/allura-forge/Llama-3.3-8B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;( For an incredible find of Llama 3.3 8B &amp;quot;in the wild&amp;quot; !!)&lt;/p&gt; &lt;p&gt;I fine tuned it using Unsloth and Claude 4.5 Opus High Reasoning Dataset:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/DavidAU/Llama3.3-8B-Instruct-Thinking-Claude-4.5-Opus-High-Reasoning"&gt;https://huggingface.co/DavidAU/Llama3.3-8B-Instruct-Thinking-Claude-4.5-Opus-High-Reasoning&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This has created a reasoning/instruct hybrid.&lt;br /&gt; Details at the repo, along with credits and links.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;ADDED:&lt;/strong&gt;&lt;br /&gt; - 1 example generation at repo&lt;br /&gt; - special instructions on how to control &amp;quot;instruct&amp;quot; or &amp;quot;thinking&amp;quot; modes.&lt;/p&gt; &lt;p&gt;GGUF quants are now available.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;ADDED 2:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Clarification:&lt;/p&gt; &lt;p&gt;This training/fine tune was to assess/test if this dataset would work on this model, and also work on a non-reasoning model and induce reasoning (specifically Claude type - which has a specific fingerprint) WITHOUT &amp;quot;system prompt help&amp;quot;.&lt;/p&gt; &lt;p&gt;In other-words, the reasoning works with the model's root training/domain/information/knowledge.&lt;/p&gt; &lt;p&gt;This model requires more extensive updates / training to bring it up to date and up to &amp;quot;spec&amp;quot; with current gen models.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;PS:&lt;/strong&gt;&lt;br /&gt; Working on a Heretic (&amp;quot;uncensored&amp;quot;) tune of this next.&lt;/p&gt; &lt;p&gt;Update: Heretic base is ready for training now...&lt;/p&gt; &lt;p&gt;DavidAU&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dangerous_Fix_5526"&gt; /u/Dangerous_Fix_5526 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0uuqt/happy_new_year/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0uuqt/happy_new_year/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q0uuqt/happy_new_year/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-01T03:41:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1q1btp6</id>
    <title>support for Solar-Open-100B has been merged into llama.cpp</title>
    <updated>2026-01-01T18:56:06+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1btp6/support_for_solaropen100b_has_been_merged_into/"&gt; &lt;img alt="support for Solar-Open-100B has been merged into llama.cpp" src="https://external-preview.redd.it/9ojHFE7X4GQU7Zn2As1PWZzoUrXBaxy-H6CNoHSxiCk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8ae3dd8af5499a5fd76536796f27b1d7451bd260" title="support for Solar-Open-100B has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Solar Open&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Solar Open&lt;/strong&gt; is Upstage's flagship &lt;strong&gt;102B-parameter&lt;/strong&gt; large language model, trained &lt;strong&gt;entirely from scratch&lt;/strong&gt; and released under the &lt;strong&gt;Solar-Apache License 2.0&lt;/strong&gt; (see &lt;a href="https://huggingface.co/upstage/Solar-Open-100B#LICENSE"&gt;LICENSE&lt;/a&gt; for details). As a &lt;strong&gt;Mixture-of-Experts (MoE)&lt;/strong&gt; architecture, it delivers enterprise-grade performance in reasoning, instruction-following, and agentic capabilities‚Äîall while prioritizing transparency and customization for the open-source community.&lt;/p&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/upstage/Solar-Open-100B#highlights"&gt;&lt;/a&gt;&lt;/h1&gt; &lt;h1&gt;Highlights&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;MoE Architecture (102B / 12B):&lt;/strong&gt; Built on a Mixture-of-Experts architecture with &lt;strong&gt;102B total / 12B active parameters&lt;/strong&gt;. This design delivers the knowledge depth of a massive model with the inference speed and cost-efficiency of a much smaller model.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Massive Training Scale:&lt;/strong&gt; Pre-trained on &lt;strong&gt;19.7 trillion tokens&lt;/strong&gt;, ensuring broad knowledge coverage and robust reasoning capabilities across various domains.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/upstage/Solar-Open-100B#model-overview"&gt;&lt;/a&gt;&lt;/h1&gt; &lt;h1&gt;Model Overview&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Model Name:&lt;/strong&gt; Solar Open 100B&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hugging Face ID:&lt;/strong&gt; Upstage/Solar-Open-100B&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Architecture:&lt;/strong&gt; Mixture-of-Experts (MoE) &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Total Parameters:&lt;/strong&gt; 102.6B&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Active Parameters:&lt;/strong&gt; 12B (per token)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Experts:&lt;/strong&gt; 129 Experts (top 8 among 128 Routed + 1 Shared)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Pre-training Tokens:&lt;/strong&gt; 19.7 Trillion&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Context Length:&lt;/strong&gt; 128k&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Training Hardware:&lt;/strong&gt; NVIDIA B200 GPUs&lt;/li&gt; &lt;li&gt;&lt;strong&gt;License:&lt;/strong&gt; &lt;strong&gt;Solar-Apache License 2.0&lt;/strong&gt; (See &lt;a href="https://huggingface.co/upstage/Solar-Open-100B/blob/main/LICENSE"&gt;LICENSE&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hardware Requirements:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Minimum:&lt;/strong&gt; 4x NVIDIA A100 (80GB)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/18511"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1btp6/support_for_solaropen100b_has_been_merged_into/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q1btp6/support_for_solaropen100b_has_been_merged_into/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-01T18:56:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1q1ge7u</id>
    <title>Youtu-LLM-2B-GGUF is here!</title>
    <updated>2026-01-01T21:58:59+00:00</updated>
    <author>
      <name>/u/KvAk_AKPlaysYT</name>
      <uri>https://old.reddit.com/user/KvAk_AKPlaysYT</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1ge7u/youtullm2bgguf_is_here/"&gt; &lt;img alt="Youtu-LLM-2B-GGUF is here!" src="https://external-preview.redd.it/Y37xpjhN-nCEndZ3yq8G_giypCpe4DWVeooeM-2eJFQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aa31ea451fa2270258ecf921a02d20fd0e93ef9e" title="Youtu-LLM-2B-GGUF is here!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Youtu-LLM-2B&lt;/strong&gt; is a highly efficient 1.96B parameter model featuring a Dense MLA architecture and a native &lt;strong&gt;128K context window&lt;/strong&gt;. Despite its small size, it supports &lt;strong&gt;Agentic capabilities&lt;/strong&gt; and &amp;quot;Reasoning Mode&amp;quot; (Chain of Thought), outperforming many larger models in STEM, coding, and agentic benchmarks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KvAk_AKPlaysYT"&gt; /u/KvAk_AKPlaysYT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/AaryanK/Youtu-LLM-2B-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1ge7u/youtullm2bgguf_is_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q1ge7u/youtullm2bgguf_is_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-01T21:58:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0zk1u</id>
    <title>DeepSeek new paper: mHC: Manifold-Constrained Hyper-Connections</title>
    <updated>2026-01-01T08:35:29+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0zk1u/deepseek_new_paper_mhc_manifoldconstrained/"&gt; &lt;img alt="DeepSeek new paper: mHC: Manifold-Constrained Hyper-Connections" src="https://b.thumbs.redditmedia.com/lonIaVTlKZO_iYxmJPzoCCxeHd37wnAoXeHPfGhOUqA.jpg" title="DeepSeek new paper: mHC: Manifold-Constrained Hyper-Connections" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://arxiv.org/abs/2512.24880"&gt;https://arxiv.org/abs/2512.24880&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/bovsed0x8pag1.jpg?width=680&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e292dc415f7fda8b1211ffe34864bb25ed4f32fe"&gt;https://preview.redd.it/bovsed0x8pag1.jpg?width=680&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e292dc415f7fda8b1211ffe34864bb25ed4f32fe&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/g9986afz8pag1.jpg?width=680&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=fe031ea160ebff21a0dc46196d3dcf3b1b58548b"&gt;https://preview.redd.it/g9986afz8pag1.jpg?width=680&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=fe031ea160ebff21a0dc46196d3dcf3b1b58548b&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0zk1u/deepseek_new_paper_mhc_manifoldconstrained/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0zk1u/deepseek_new_paper_mhc_manifoldconstrained/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q0zk1u/deepseek_new_paper_mhc_manifoldconstrained/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-01T08:35:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0x8ci</id>
    <title>Software FP8 for GPUs without hardware support - 3x speedup on memory-bound operations</title>
    <updated>2026-01-01T06:03:27+00:00</updated>
    <author>
      <name>/u/Venom1806</name>
      <uri>https://old.reddit.com/user/Venom1806</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Got tired of my RTX 3050 not supporting FP8, so I built a workaround. Packs lower-precision values into FP32 using bitwise operations + Triton kernels.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;: 3x faster on memory-bound operations (GEMV, FlashAttention)&lt;/p&gt; &lt;p&gt;Works on any GPU - RTX 30/20 series, older cards without native FP8 support. Early stage but functional. Open to feedback.&lt;/p&gt; &lt;p&gt;&lt;a href="https://towardsdatascience.com/breaking-the-hardware-barrier-software-fp8-for-older-gpus/"&gt;Article Link&lt;/a&gt; | &lt;a href="https://github.com/SuriyaaMM/feather"&gt;Github Link&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Venom1806"&gt; /u/Venom1806 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0x8ci/software_fp8_for_gpus_without_hardware_support_3x/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0x8ci/software_fp8_for_gpus_without_hardware_support_3x/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q0x8ci/software_fp8_for_gpus_without_hardware_support_3x/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-01T06:03:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1q189os</id>
    <title>7900 XTX + ROCm: A Year Later. Llama.cpp vs vLLM Benchmarks (TB3 eGPU)</title>
    <updated>2026-01-01T16:33:45+00:00</updated>
    <author>
      <name>/u/reujea0</name>
      <uri>https://old.reddit.com/user/reujea0</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've had the 7900 XTX for over a year now. While the situation with ROCm has definitely gotten better, it is still a frustrating experience compared to just plugging in an NVIDIA card.&lt;/p&gt; &lt;p&gt;I was curious to see if we could at least run newer models reliably now, so I decided to compare the maturity of &lt;strong&gt;llama.cpp&lt;/strong&gt; vs &lt;strong&gt;vLLM&lt;/strong&gt; on this hardware.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Important Context:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;The Setup:&lt;/strong&gt; 7900 XTX connected via &lt;strong&gt;Thunderbolt 3 (eGPU)&lt;/strong&gt;. This might introduce some bandwidth limitations, so I specifically chose models that fit entirely in VRAM to minimize penalty.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Disclaimer:&lt;/strong&gt; This is &lt;em&gt;not&lt;/em&gt; scientific. These are just some quick numbers I ran to check the current state of things.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The Environment:&lt;/strong&gt; Huge thanks to &lt;a href="https://github.com/kyuz0"&gt;kyuz0 on GitHub&lt;/a&gt; whose repo allowed me to actually build working images for both llama.cpp and vLLM on this platform.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Here are the results&lt;/p&gt; &lt;h1&gt;Llama.cpp (ROCm)&lt;/h1&gt; &lt;p&gt;&lt;em&gt;Running&lt;/em&gt; &lt;code&gt;llama-bench&lt;/code&gt; &lt;em&gt;on local GGUF files.&lt;/em&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;GGUF Filename&lt;/th&gt; &lt;th align="left"&gt;Size&lt;/th&gt; &lt;th align="left"&gt;PP (512)&lt;/th&gt; &lt;th align="left"&gt;Gen (tg512)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;code&gt;unsloth_Llama-3.1-8B-Instruct-GGUF_Llama-3.1-8B-Instruct-BF16.gguf&lt;/code&gt;&lt;/td&gt; &lt;td align="left"&gt;14.96 GB&lt;/td&gt; &lt;td align="left"&gt;2226 t/s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;42.51 t/s&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;code&gt;Mistral-Small-3.2-24B-Instruct-2506-UD-Q5_K_XL.gguf&lt;/code&gt;&lt;/td&gt; &lt;td align="left"&gt;15.63 GB&lt;/td&gt; &lt;td align="left"&gt;861 t/s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;32.20 t/s&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;code&gt;unsloth_Qwen2.5-VL-32B-Instruct-GGUF_Qwen2.5-VL-32B-Instruct-Q4_K_M.gguf&lt;/code&gt;&lt;/td&gt; &lt;td align="left"&gt;18.48 GB&lt;/td&gt; &lt;td align="left"&gt;626 t/s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;22.95 t/s&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;code&gt;DeepSeek-R1-Distill-Qwen-32B-Q3_K_M.gguf&lt;/code&gt;&lt;/td&gt; &lt;td align="left"&gt;14.84 GB&lt;/td&gt; &lt;td align="left"&gt;669 t/s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;24.12 t/s&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;code&gt;gpt-oss-20b-F16.gguf&lt;/code&gt;&lt;/td&gt; &lt;td align="left"&gt;12.83 GB&lt;/td&gt; &lt;td align="left"&gt;2620 t/s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;87.09 t/s&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;code&gt;ggml-org_gpt-oss-20b-GGUF_gpt-oss-20b-mxfp4.gguf&lt;/code&gt;&lt;/td&gt; &lt;td align="left"&gt;11.27 GB&lt;/td&gt; &lt;td align="left"&gt;2735.27 t/s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;93.80 t/s&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;code&gt;Qwen3-VL-30B-A3B-Thinking-UD-Q4_K_XL.gguf&lt;/code&gt;&lt;/td&gt; &lt;td align="left"&gt;16.49 GB&lt;/td&gt; &lt;td align="left"&gt;1793 t/s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;51.86 t/s&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;vLLM (ROCm)&lt;/h1&gt; &lt;p&gt;&lt;em&gt;Running&lt;/em&gt; &lt;code&gt;vllm bench serve&lt;/code&gt; &lt;em&gt;directly from HF repos.&lt;/em&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model Repo (HF)&lt;/th&gt; &lt;th align="left"&gt;Format&lt;/th&gt; &lt;th align="left"&gt;Gen Speed&lt;/th&gt; &lt;th align="left"&gt;Latency (TTFT)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;code&gt;unsloth/Meta-Llama-3.1-8B-Instruct&lt;/code&gt;&lt;/td&gt; &lt;td align="left"&gt;Native BF16&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;94.19 t/s&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;282 ms&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;code&gt;unsloth/gpt-oss-20b&lt;/code&gt;&lt;/td&gt; &lt;td align="left"&gt;F16 (MoE)&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;48.33 t/s&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;1044 ms&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;code&gt;unsloth/Mistral-Small-3.2-24B-Instruct-2506-bnb-4bit&lt;/code&gt;&lt;/td&gt; &lt;td align="left"&gt;bnb-4bit&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;14.99 t/s&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;1063 ms&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;code&gt;openai/gpt-oss-20b&lt;/code&gt;&lt;/td&gt; &lt;td align="left"&gt;MXFP4&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;48.91 t/s&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;427 ms&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Just wanted to share some data for anyone else suffering through the AMD local LLM journey.&lt;br /&gt; ROCm nightly: 6.4.43482-0f2d60242&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/reujea0"&gt; /u/reujea0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q189os/7900_xtx_rocm_a_year_later_llamacpp_vs_vllm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q189os/7900_xtx_rocm_a_year_later_llamacpp_vs_vllm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q189os/7900_xtx_rocm_a_year_later_llamacpp_vs_vllm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-01T16:33:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1q1g7pp</id>
    <title>Solar-Open-100B-GGUF is here!</title>
    <updated>2026-01-01T21:51:33+00:00</updated>
    <author>
      <name>/u/KvAk_AKPlaysYT</name>
      <uri>https://old.reddit.com/user/KvAk_AKPlaysYT</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1g7pp/solaropen100bgguf_is_here/"&gt; &lt;img alt="Solar-Open-100B-GGUF is here!" src="https://external-preview.redd.it/NhmgyNgn5k9aKcKZk_ELLO4LdarQGlwMfU3zdGmwpqM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4a8af6055f2721b2fd950e489426819702e5c350" title="Solar-Open-100B-GGUF is here!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Solar Open&lt;/strong&gt; is a massive &lt;strong&gt;102B-parameter&lt;/strong&gt; Mixture-of-Experts (MoE) model trained from scratch on &lt;strong&gt;19.7 trillion tokens&lt;/strong&gt;. It uses only &lt;strong&gt;12B active parameters&lt;/strong&gt; during inference.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KvAk_AKPlaysYT"&gt; /u/KvAk_AKPlaysYT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/AaryanK/Solar-Open-100B-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1g7pp/solaropen100bgguf_is_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q1g7pp/solaropen100bgguf_is_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-01T21:51:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1q1bvbc</id>
    <title>support for youtu-vl model has been merged into llama.cpp</title>
    <updated>2026-01-01T18:57:53+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1bvbc/support_for_youtuvl_model_has_been_merged_into/"&gt; &lt;img alt="support for youtu-vl model has been merged into llama.cpp" src="https://external-preview.redd.it/Evtyq3EMtPWK8QKluFpgEAK-fyQGEUPNr3MqiBcc-38.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e042f719ea042bd3d511f9ba264bcbac8b8c97e1" title="support for youtu-vl model has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Youtu-LLM&lt;/strong&gt; is a new, small, yet powerful LLM, contains only 1.96B parameters, supports 128k long context, and has native agentic talents. On general evaluations, Youtu-LLM significantly outperforms SOTA LLMs of similar size in terms of Commonsense, STEM, Coding and Long Context capabilities; in agent-related testing, Youtu-LLM surpasses larger-sized leaders and is truly capable of completing multiple end2end agent tasks.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Youtu-LLM&lt;/strong&gt; has the following features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Type: Autoregressive Causal Language Models with Dense &lt;a href="https://arxiv.org/abs/2405.04434"&gt;MLA&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Release versions: &lt;a href="https://huggingface.co/tencent/Youtu-LLM-2B-Base"&gt;Base&lt;/a&gt; and &lt;a href="https://huggingface.co/tencent/Youtu-LLM-2B"&gt;Instruct&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Number of Parameters: 1.96B&lt;/li&gt; &lt;li&gt;Number of Layers: 32&lt;/li&gt; &lt;li&gt;Number of Attention Heads (MLA): 16 for Q/K/V&lt;/li&gt; &lt;li&gt;MLA Rank: 1,536 for Q, 512 for K/V&lt;/li&gt; &lt;li&gt;MLA Dim: 128 for QK Nope, 64 for QK Rope, and 128 for V&lt;/li&gt; &lt;li&gt;Context Length: 131,072&lt;/li&gt; &lt;li&gt;Vocabulary Size: 128,256&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/18479"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1bvbc/support_for_youtuvl_model_has_been_merged_into/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q1bvbc/support_for_youtuvl_model_has_been_merged_into/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-01T18:57:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0zst6</id>
    <title>Upstage Solar-Open-100B Public Validation</title>
    <updated>2026-01-01T08:52:25+00:00</updated>
    <author>
      <name>/u/PerPartes</name>
      <uri>https://old.reddit.com/user/PerPartes</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0zst6/upstage_solaropen100b_public_validation/"&gt; &lt;img alt="Upstage Solar-Open-100B Public Validation" src="https://preview.redd.it/w789uyo0cpag1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5b12cc1f1a7783b1d9a40f9851206fbcdbdbf782" title="Upstage Solar-Open-100B Public Validation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Official company counterstrike to the claim that Solar 100B Open is just finetuned GLM-Air-4.5&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PerPartes"&gt; /u/PerPartes &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/w789uyo0cpag1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0zst6/upstage_solaropen100b_public_validation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q0zst6/upstage_solaropen100b_public_validation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-01T08:52:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1q1dg3x</id>
    <title>I released Polyglot-r2 (Qwen3-4B fine-tune): Suffix-based text transformation without system prompts</title>
    <updated>2026-01-01T19:59:43+00:00</updated>
    <author>
      <name>/u/thecalmgreen</name>
      <uri>https://old.reddit.com/user/thecalmgreen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1dg3x/i_released_polyglotr2_qwen34b_finetune/"&gt; &lt;img alt="I released Polyglot-r2 (Qwen3-4B fine-tune): Suffix-based text transformation without system prompts" src="https://a.thumbs.redditmedia.com/XoVdOmGimoS-Xb_UE9XmCI0YDExRMxHEsbEtVV4iLP0.jpg" title="I released Polyglot-r2 (Qwen3-4B fine-tune): Suffix-based text transformation without system prompts" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/9u46yjyqmsag1.png?width=1233&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fc15f07184c49ef3a2a0e039e24392c4b30d6104"&gt;Polyglot r2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm sharing the second revision (r2) of Polyglot, a fine-tune based on Qwen3-4B designed specifically for deterministic text transformation using suffixes.&lt;/p&gt; &lt;p&gt;The goal of this model is to bypass the need for prompt engineering when performing standard text operations. Instead of writing a system prompt or instructing the model via chat, you simply append a specific suffix to your input string.&lt;/p&gt; &lt;p&gt;The model was trained on a curated dataset of millions of tokens to be strictly instruction-following for these tags. It outputs only the result, no conversational filler.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Supported Transformations&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Languages&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;::pt&lt;/code&gt; - Portuguese (Portugal)&lt;/li&gt; &lt;li&gt;&lt;code&gt;::ptbr&lt;/code&gt; - Portuguese (Brazil)&lt;/li&gt; &lt;li&gt;&lt;code&gt;::en&lt;/code&gt; - English&lt;/li&gt; &lt;li&gt;&lt;code&gt;::es&lt;/code&gt; - Spanish&lt;/li&gt; &lt;li&gt;&lt;code&gt;::zh&lt;/code&gt; - Chinese (Simplified)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Corrections&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;::fix&lt;/code&gt; - Fix spelling and grammar while keeping the original language&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Tone&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;::formal&lt;/code&gt; - Make formal&lt;/li&gt; &lt;li&gt;&lt;code&gt;::informal&lt;/code&gt; - Make slang/informal&lt;/li&gt; &lt;li&gt;&lt;code&gt;::casual&lt;/code&gt; - Make casual&lt;/li&gt; &lt;li&gt;&lt;code&gt;::polite&lt;/code&gt; - Make polite&lt;/li&gt; &lt;li&gt;&lt;code&gt;::business&lt;/code&gt; - Make business-oriented&lt;/li&gt; &lt;li&gt;&lt;code&gt;::technical&lt;/code&gt; - Make technical&lt;/li&gt; &lt;li&gt;&lt;code&gt;::creative&lt;/code&gt; - Make creative&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Structure&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;::summarize&lt;/code&gt; - Summarize&lt;/li&gt; &lt;li&gt;&lt;code&gt;::expand&lt;/code&gt; - Expand / add details&lt;/li&gt; &lt;li&gt;&lt;code&gt;::simplify&lt;/code&gt; - Simplify&lt;/li&gt; &lt;li&gt;&lt;code&gt;::concise&lt;/code&gt; - Make concise&lt;/li&gt; &lt;li&gt;&lt;code&gt;::elaborate&lt;/code&gt; - Elaborate / add details&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Style&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;::news&lt;/code&gt; - News style&lt;/li&gt; &lt;li&gt;&lt;code&gt;::social&lt;/code&gt; - Social media style&lt;/li&gt; &lt;li&gt;&lt;code&gt;::toQuestion&lt;/code&gt; - Transform into a question&lt;/li&gt; &lt;li&gt;&lt;code&gt;::toStatement&lt;/code&gt; - Transform into a statement&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What's new in r2&lt;/strong&gt; Beyond tripling the dataset size, the main feature in this revision is &lt;strong&gt;Suffix Chaining&lt;/strong&gt;. You can now combine tasks in a single pass.&lt;/p&gt; &lt;p&gt;For example, appending &lt;code&gt;::summarize ::ptbr&lt;/code&gt; will summarize the text and immediately translate the result to Portuguese (Brazil).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Usage &amp;amp; Workflow&lt;/strong&gt; You can run this model using any standard inference backend (like llama.cpp, ollama, lm studio, etc).&lt;/p&gt; &lt;p&gt;However, I originally built this model to power an open-source tool I wrote (also called Polyglot). It‚Äôs a desktop utility that allows you to trigger these transformations via global hotkeys in any application on your OS. I use it daily to handle translations and quick text clean-ups without context-switching to a browser or chat UI.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Model (HF):&lt;/strong&gt; &lt;a href="https://huggingface.co/CalmState/Qwen-3-4b-Polyglot-r2"&gt;https://huggingface.co/CalmState/Qwen-3-4b-Polyglot-r2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GGUF (Q8):&lt;/strong&gt; &lt;a href="https://huggingface.co/CalmState/Qwen-3-4b-Polyglot-r2-Q8_0-GGUF"&gt;https://huggingface.co/CalmState/Qwen-3-4b-Polyglot-r2-Q8_0-GGUF&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GGUF (Q4_K_M):&lt;/strong&gt; &lt;a href="https://huggingface.co/CalmState/Qwen-3-4b-Polyglot-r2-Q4_K_M-GGUF"&gt;https://huggingface.co/CalmState/Qwen-3-4b-Polyglot-r2-Q4_K_M-GGUF&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Desktop App (GitHub):&lt;/strong&gt; &lt;a href="https://github.com/andersondanieln/polyglot"&gt;https://github.com/andersondanieln/polyglot&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The project is fully open-source. If you find the workflow useful, a star on the repo is appreciated. &lt;/p&gt; &lt;p&gt;HAPPY NEW YEAR!!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thecalmgreen"&gt; /u/thecalmgreen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1dg3x/i_released_polyglotr2_qwen34b_finetune/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1dg3x/i_released_polyglotr2_qwen34b_finetune/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q1dg3x/i_released_polyglotr2_qwen34b_finetune/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-01T19:59:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1q1bxci</id>
    <title>support for IQuest-Coder-V1-40B has been merged into llama.cpp</title>
    <updated>2026-01-01T19:00:04+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1bxci/support_for_iquestcoderv140b_has_been_merged_into/"&gt; &lt;img alt="support for IQuest-Coder-V1-40B has been merged into llama.cpp" src="https://external-preview.redd.it/9ahEimrF0NVJzd5JMsSv-jhsKUqTcPwMInlAFk_SJHU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=766f07b197f35e4577b070c65f9edf022eb3283f" title="support for IQuest-Coder-V1-40B has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;(one line PR!)&lt;/p&gt; &lt;p&gt;IQuest-Coder-V1 is a new family of code large language models (LLMs) designed to advance autonomous software engineering and code intelligence. Built on the innovative code-flow multi-stage training paradigm, IQuest-Coder-V1 captures the dynamic evolution of software logic, delivering state-of-the-art performance across critical dimensions:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;State-of-the-Art Performance&lt;/strong&gt;: Achieves leading results on SWE-Bench Verified (81.4%), BigCodeBench (49.9%), LiveCodeBench v6 (81.1%), and other major coding benchmarks, surpassing competitive models across agentic software engineering, competitive programming, and complex tool use.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Code-Flow Training Paradigm&lt;/strong&gt;: Moving beyond static code representations, our models learn from repository evolution patterns, commit transitions, and dynamic code transformations to understand real-world software development processes.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Dual Specialization Paths&lt;/strong&gt;: Bifurcated post-training delivers two specialized variants‚ÄîThinking models (utilizing reasoning-driven RL for complex problem-solving) and Instruct models (optimized for general coding assistance and instruction-following).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Efficient Architecture&lt;/strong&gt;: The IQuest-Coder-V1-Loop variant introduces a recurrent mechanism that optimizes the trade-off between model capacity and deployment footprint.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Native Long Context&lt;/strong&gt;: All models natively support up to 128K tokens without requiring additional scaling techniques.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/18524"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1bxci/support_for_iquestcoderv140b_has_been_merged_into/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q1bxci/support_for_iquestcoderv140b_has_been_merged_into/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-01T19:00:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1q1986x</id>
    <title>IQuestCoder - new 40B dense coding model</title>
    <updated>2026-01-01T17:12:51+00:00</updated>
    <author>
      <name>/u/ilintar</name>
      <uri>https://old.reddit.com/user/ilintar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1986x/iquestcoder_new_40b_dense_coding_model/"&gt; &lt;img alt="IQuestCoder - new 40B dense coding model" src="https://external-preview.redd.it/puEaI60nzHUbVlmNCXfE1sl9fmhvVgJHgKO3FYQHywY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7035b6af6efe8e8b4c47de0ff3945cb68bc2973c" title="IQuestCoder - new 40B dense coding model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As usual, benchmarks claim it's absolutely SOTA and crushes the competition. Since I'm willing to verify it, I've adapted it to GGUF. It's basically Llama arch (reportedly was supposed to be using SWA, but it didn't get used in the final version), so works out of the box with Llama.cpp.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ilintar"&gt; /u/ilintar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/ilintar/IQuest-Coder-V1-40B-Instruct-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1986x/iquestcoder_new_40b_dense_coding_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q1986x/iquestcoder_new_40b_dense_coding_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-01T17:12:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptxm3x</id>
    <title>AMA With Z.AI, The Lab Behind GLM-4.7</title>
    <updated>2025-12-23T16:04:23+00:00</updated>
    <author>
      <name>/u/zixuanlimit</name>
      <uri>https://old.reddit.com/user/zixuanlimit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi r/LocalLLaMA&lt;/p&gt; &lt;p&gt;Today we are having &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt;, the research lab behind the GLM 4.7. We‚Äôre excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Yuxuan Zhang, &lt;a href="/u/YuxuanZhangzR"&gt;u/YuxuanZhangzR&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Qinkai Zheng, &lt;a href="/u/QinkaiZheng"&gt;u/QinkaiZheng&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Aohan Zeng, &lt;a href="/u/Sengxian"&gt;u/Sengxian&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Zhenyu Hou, &lt;a href="/u/ZhenyuHou"&gt;u/ZhenyuHou&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Xin Lv, &lt;a href="/u/davidlvxin"&gt;u/davidlvxin&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The AMA will run from 8 AM ‚Äì 11 AM PST, with the &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt; team continuing to follow up on questions over the next 48 hours.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zixuanlimit"&gt; /u/zixuanlimit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T16:04:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwh0q9</id>
    <title>Best Local LLMs - 2025</title>
    <updated>2025-12-26T22:31:28+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Year end thread for the best LLMs of 2025!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;2025 is almost done! Its been &lt;strong&gt;a wonderful year&lt;/strong&gt; for us Open/Local AI enthusiasts. And its looking like Xmas time brought some great gifts in the shape of Minimax M2.1 and GLM4.7 that are touting frontier model performance. Are we there already? are we at parity with proprietary models?!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The standard spiel:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Share what your favorite models are right now &lt;strong&gt;and why.&lt;/strong&gt; Given the nature of the beast in evaluating LLMs (untrustworthiness of benchmarks, immature tooling, intrinsic stochasticity), please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Only open weights models&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;em&gt;Please thread your responses in the top level comments for each Application below to enable readability&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Applications&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;General&lt;/strong&gt;: Includes practical guidance, how to, encyclopedic QnA, search engine replacement/augmentation&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agentic/Agentic Coding/Tool Use/Coding&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Creative Writing/RP&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Speciality&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;If a category is missing, please create a top level comment under the Speciality comment&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Notes&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Useful breakdown of how folk are using LLMs: &lt;a href="https://preview.redd.it/i8td7u8vcewf1.png?width=1090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d"&gt;https://preview.redd.it/i8td7u8vcewf1.png?width=1090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d&lt;/a&gt; &lt;/p&gt; &lt;p&gt;A good suggestion for last time, breakdown/classify your recommendation by model memory footprint: (you can and should be using multiple models in each size range for different tasks)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Unlimited: &amp;gt;128GB VRAM &lt;/li&gt; &lt;li&gt;Medium: 8 to 128GB VRAM&lt;/li&gt; &lt;li&gt;Small: &amp;lt;8GB VRAM&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T22:31:28+00:00</published>
  </entry>
</feed>
