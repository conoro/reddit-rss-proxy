<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-30T08:25:22+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1p9t8tz</id>
    <title>How do I enable vision capabilities of a model ? Linux Mint 22.2, rx 6600. I ran this at bash/terminal to start the server: llama-server -m ./Qwen3-VL-8B-Instruct-Q4_K_M.gguf</title>
    <updated>2025-11-29T16:34:56+00:00</updated>
    <author>
      <name>/u/Badhunter31415</name>
      <uri>https://old.reddit.com/user/Badhunter31415</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9t8tz/how_do_i_enable_vision_capabilities_of_a_model/"&gt; &lt;img alt="How do I enable vision capabilities of a model ? Linux Mint 22.2, rx 6600. I ran this at bash/terminal to start the server: llama-server -m ./Qwen3-VL-8B-Instruct-Q4_K_M.gguf" src="https://preview.redd.it/u591pzs7484g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ad0d6002360697a583ce8470bb60b195db6b8082" title="How do I enable vision capabilities of a model ? Linux Mint 22.2, rx 6600. I ran this at bash/terminal to start the server: llama-server -m ./Qwen3-VL-8B-Instruct-Q4_K_M.gguf" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Badhunter31415"&gt; /u/Badhunter31415 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/u591pzs7484g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9t8tz/how_do_i_enable_vision_capabilities_of_a_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p9t8tz/how_do_i_enable_vision_capabilities_of_a_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-29T16:34:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1pa5rzj</id>
    <title>Multi GPU PSU</title>
    <updated>2025-11-30T01:36:10+00:00</updated>
    <author>
      <name>/u/i_am_not_a_goat</name>
      <uri>https://old.reddit.com/user/i_am_not_a_goat</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m wondering what PSUs folks are using for their multi 3090 builds ?&lt;/p&gt; &lt;p&gt;I’ve got a 3090ti,3090 and 2x5060ti’s and I’m hitting system reboots on peek load with a 1650w psu. Before I go get a meatier PSU what are other folks using ? Also it’s an older thread ripper 2920x cpu, which definitely runs on the high side for power.&lt;/p&gt; &lt;p&gt;Any suggestions would be helpful. Oh also if I use one 3090 and 2x5060s everything is fine, it’s only when I use all 4 cards do things go wonky.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/i_am_not_a_goat"&gt; /u/i_am_not_a_goat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pa5rzj/multi_gpu_psu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pa5rzj/multi_gpu_psu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pa5rzj/multi_gpu_psu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-30T01:36:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1p9i5ew</id>
    <title>Try the new Z-Image-Turbo 6B (Runs on 8GB VRAM)!</title>
    <updated>2025-11-29T06:46:42+00:00</updated>
    <author>
      <name>/u/KvAk_AKPlaysYT</name>
      <uri>https://old.reddit.com/user/KvAk_AKPlaysYT</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks,&lt;/p&gt; &lt;p&gt;I wanted to try out the new Z-Image-Turbo model (the 6B one that just dropped), but I didn't want to fiddle with complex workflows or wait for specific custom nodes to mature.&lt;/p&gt; &lt;p&gt;So, I threw together a dedicated, clean Web UI to run it.&lt;/p&gt; &lt;p&gt;Has CPU offload too! :)&lt;/p&gt; &lt;p&gt;Check it out: &lt;a href="https://github.com/Aaryan-Kapoor/z-image-turbo"&gt;https://github.com/Aaryan-Kapoor/z-image-turbo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/Tongyi-MAI/Z-Image-Turbo"&gt;https://huggingface.co/Tongyi-MAI/Z-Image-Turbo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;May your future be full of VRAM!&lt;/p&gt; &lt;p&gt;Edit: Added Google Colab notebook as well, enjoy! :)&lt;/p&gt; &lt;p&gt;Update: MCP server integration is in the works!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KvAk_AKPlaysYT"&gt; /u/KvAk_AKPlaysYT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9i5ew/try_the_new_zimageturbo_6b_runs_on_8gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9i5ew/try_the_new_zimageturbo_6b_runs_on_8gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p9i5ew/try_the_new_zimageturbo_6b_runs_on_8gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-29T06:46:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1paa33t</id>
    <title>3070 GPU Mining Rig --what would you do?</title>
    <updated>2025-11-30T05:18:08+00:00</updated>
    <author>
      <name>/u/So1Cutter</name>
      <uri>https://old.reddit.com/user/So1Cutter</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello!&lt;/p&gt; &lt;p&gt;I stumbled across a mining rig with a mix of 3070 GPUs, all GPUs are within 15% of each other as far as performance. I'm wondering if anyone else has had anything else like this happen and what opinions are on what I should do with it. Specs below:&lt;/p&gt; &lt;p&gt;3x 3070s 2x 3070 TIs 1x 3060 TI (8GB model) 256 GB m.2 8GB RAM (wished it was 32GB given current RAM prices). &lt;/p&gt; &lt;p&gt;So my thought is to sell all the GPUs and get some 16GB or higher GPUs. Not even sure that's worth messing with or if it might just be better to sell the entire system. I thought someone might have had a similar experience or has converted one to a local LLM super computer... Also I interested in what you might do with it if it was yours?&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/So1Cutter"&gt; /u/So1Cutter &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1paa33t/3070_gpu_mining_rig_what_would_you_do/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1paa33t/3070_gpu_mining_rig_what_would_you_do/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1paa33t/3070_gpu_mining_rig_what_would_you_do/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-30T05:18:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1pa6gw8</id>
    <title>Sanity check on a frankenstein hardware setup for gpt-oss 120b?</title>
    <updated>2025-11-30T02:10:43+00:00</updated>
    <author>
      <name>/u/Careful_Breath_1108</name>
      <uri>https://old.reddit.com/user/Careful_Breath_1108</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Trying to jerry-rig my home PC into a home LLM rig just for personal hobbyist use and experimentation. Seems like gpt-oss 120b is considered the most capable local model so that’s what I’m hoping to be able to run. Tried going as lean and cheap as possible, and this is what I came up with:&lt;/p&gt; &lt;p&gt;Parts from my Dell PC (XPS 8940):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;1 x RTX 2060 Super (8GB)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Samsung 1x16GB RAM DDR4 3200MHz&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Intel i7-11700 @ 2.50GHz&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Dell 0K3CM7 motherboard&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;500W PSU&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Samsung 512GB SSD NVMe&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;WD 1TB 3.5&amp;quot; SATA HDD 7200RPM&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Parts I’ve bought:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;2 x RTX 5060 Ti (16GB each)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Rimlance 2x32GB RAM DDR4 3200MHz&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Parts I’ve yet to buy:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;PCIe riser cables&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;1000W PSU &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;PSU sync adapter&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Some sort of mining rig-esque setup to hook up the GPUs&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Planning to hook up the VRAM like so:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;PCIe x16 - RTX 5060 Ti &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;PCIe x4 - RTX 5060 Ti, via riser cable&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;PCIe x1 - RTX 2060 Super, via riser cable&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;By the time everything is set up, I believe I will have 40GB of VRAM and 80GB of CPU RAM. I plan to use either vLLM or llama.cpp to access all the VRAM together. The RAM maxes out at 2933MHz due to motherboard limitations.&lt;/p&gt; &lt;p&gt;Is this setup even viable or would inference be terrible with a setup like this? Is fine tuning and training even a possibility? Is it true that it’s possible to run large MoE models as long as enough cpu RAM is acquired?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Careful_Breath_1108"&gt; /u/Careful_Breath_1108 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pa6gw8/sanity_check_on_a_frankenstein_hardware_setup_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pa6gw8/sanity_check_on_a_frankenstein_hardware_setup_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pa6gw8/sanity_check_on_a_frankenstein_hardware_setup_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-30T02:10:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1paaxlj</id>
    <title>Newbie Question about GPU choice</title>
    <updated>2025-11-30T06:04:31+00:00</updated>
    <author>
      <name>/u/mundane_marietta</name>
      <uri>https://old.reddit.com/user/mundane_marietta</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Use case - training a model on 10 years of my writing, high school football player data, scouting reports, historical stats, etc., so that I can create a model that churns out 25 articles a day (between 250-750 words) for my football recruiting website. &lt;/p&gt; &lt;p&gt;I have good deals in place for a 5070 for $475 and a 4080 for $715 tax included. I just need to decide which one would be the best value for my use case. My local Microcenter does have a few 3090's available for $775.&lt;/p&gt; &lt;p&gt;I have no idea what I'm doing, so the upfront investment does seem daunting as the prices climb, but the season is almost over, and I believe with time, I can figure out what to do.&lt;/p&gt; &lt;p&gt;Not sure if this is the appropriate place to ask this question, and I know VRAM is king, but not sure if a 5070 could do the trick for my use case.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mundane_marietta"&gt; /u/mundane_marietta &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1paaxlj/newbie_question_about_gpu_choice/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1paaxlj/newbie_question_about_gpu_choice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1paaxlj/newbie_question_about_gpu_choice/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-30T06:04:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1pa6p2w</id>
    <title>Recommendations for summarization and structured data extraction</title>
    <updated>2025-11-30T02:22:09+00:00</updated>
    <author>
      <name>/u/cachophonic</name>
      <uri>https://old.reddit.com/user/cachophonic</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, I’m looking for people’s current favourites/recommendations for models that are great at following instructions for text summarization and structured data extraction. &lt;/p&gt; &lt;p&gt;For a bit of context the model needs to be able to fit within 48gb of VRAM and the use case is largely extracting specific information (eg question and answer pairs, specific assessment info) and structured JSON data from appointment transcripts. Usually around 30k tokens including prompts per generation. &lt;/p&gt; &lt;p&gt;Our current go to is still Mistral 24b Instruct at fp8 running in VLLM. &lt;/p&gt; &lt;p&gt;This a production project so priority is accuracy, ability to follow instructions and avoid confabulation over raw t/s. &lt;/p&gt; &lt;p&gt;We tried several other models like gpt oss 20b, Qwen3-30B-A3B and several other smaller Qwen models when we initially got started but it's hard to keep up with all the changes so thought I'd see if people have particular go-tos so we can reduce the short list of models to experiment with. Thanks! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cachophonic"&gt; /u/cachophonic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pa6p2w/recommendations_for_summarization_and_structured/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pa6p2w/recommendations_for_summarization_and_structured/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pa6p2w/recommendations_for_summarization_and_structured/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-30T02:22:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1pacmet</id>
    <title>Are LLMs still just probabilistic heuristics, not guaranteed solvers?</title>
    <updated>2025-11-30T07:48:25+00:00</updated>
    <author>
      <name>/u/Parking-Ad-4250</name>
      <uri>https://old.reddit.com/user/Parking-Ad-4250</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been trying to reconcile two things that seem to be true at the same time:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;LLMs look dramatically smarter than they did in 2021–2022&lt;/li&gt; &lt;li&gt;Yet they still fail in ways that look very “non-reasoning” and brittle&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;So I wrote a longform piece where I argue that &lt;strong&gt;LLMs are still fundamentally probabilistic heuristics, not guaranteed solvers&lt;/strong&gt;, even in the age of CoT, RLHF, and Agentic workflows.&lt;/p&gt; &lt;p&gt;Core ideas from the article:&lt;/p&gt; &lt;h1&gt;1. “More is different” vs emergent capabilities&lt;/h1&gt; &lt;p&gt;As models scaled, we saw what looked like &lt;strong&gt;emergent abilities&lt;/strong&gt;: reasoning, in-context learning, better problem solving, etc. This is often framed using Philip Anderson’s &lt;em&gt;More is Different&lt;/em&gt; idea – at scale, qualitatively new behaviours appear.&lt;/p&gt; &lt;p&gt;But it’s not obvious whether the observed gains come from:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;sheer scale&lt;/li&gt; &lt;li&gt;better data / coverage (including benchmarks)&lt;/li&gt; &lt;li&gt;prompt techniques like Chain of Thought&lt;/li&gt; &lt;li&gt;RLHF-style shaping&lt;/li&gt; &lt;li&gt;or genuine “emergent” structure&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I try to separate these hypotheses instead of treating “emergence” as magic.&lt;/p&gt; &lt;h1&gt;2. CoT, RLHF and “Large Reasoning Models”&lt;/h1&gt; &lt;p&gt;CoT and RLHF changed the game:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;CoT prompting massively boosts benchmark scores (e.g. GSM8K)&lt;/li&gt; &lt;li&gt;Models like GPT-4o and DeepSeek R1 are positioned as &lt;strong&gt;Large Reasoning Models&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;DeepSeek even leans heavily on RL-based training&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;But a lot of these gains look &lt;strong&gt;surface-level&lt;/strong&gt; when:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;models generalize poorly to small perturbations of the same problem&lt;/li&gt; &lt;li&gt;performance drops sharply with complexity or compositional depth&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This feeds into the “reasoning as a mirage” view: we’re eliciting better behaviour on a narrow band of distributions, not building a robust reasoner.&lt;/p&gt; &lt;h1&gt;3. Mechanistic interpretability &amp;amp; symbolic-like circuits&lt;/h1&gt; &lt;p&gt;Recent work (e.g. &lt;em&gt;Abstract Reasoning in Large Language Models&lt;/em&gt; on Llama-3-70B) suggests that some attention heads implement &lt;strong&gt;symbolic-like abstractions&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;patterns like &lt;code&gt;dog, cat, dog&lt;/code&gt; and &lt;code&gt;tiger, goat, tiger&lt;/code&gt; mapping to the same abstract A–B–A pattern&lt;/li&gt; &lt;li&gt;behaviour that looks closer to variable binding than simple n-gram statistics&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;That’s interesting because it hints that &lt;strong&gt;symbolic reasoning substrates may be emerging inside purely connectionist models&lt;/strong&gt;, even without explicit symbolic training.&lt;/p&gt; &lt;p&gt;But we still don’t know:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;how general these circuits are&lt;/li&gt; &lt;li&gt;how robust they are under distribution shift&lt;/li&gt; &lt;li&gt;how much they contribute to actual problem solving vs just neat probes&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;4. Neuro-symbolic AI and Agentic AI&lt;/h1&gt; &lt;p&gt;I also touch on:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Neuro-symbolic AI&lt;/strong&gt;: attempts to fuse ontologies, Markov logic networks, GNNs etc. as reasoning layers over/with LLMs to reduce hallucination.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agentic AI (ReAct, tool-use, RAG)&lt;/strong&gt;: inner “thoughts” + external actions (e.g. search, tools) + observation loops.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;These systems make LLMs &lt;em&gt;look&lt;/em&gt; much more capable because:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;the model can offload missing knowledge to tools / web&lt;/li&gt; &lt;li&gt;the reasoning chain is interleaved with external feedback&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;But that also makes it &lt;strong&gt;harder to tell&lt;/strong&gt; whether the core LLM is actually reasoning better, or just getting better crutches.&lt;/p&gt; &lt;h1&gt;5. A concrete failure case&lt;/h1&gt; &lt;p&gt;I reference a recent 2025 case where GPT-5 was used on a new math problem involving the Malliavin–Stein method:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;no solution existed in pre-training data&lt;/li&gt; &lt;li&gt;the model produced a confident but incorrect derivation&lt;/li&gt; &lt;li&gt;it failed to self-correct even under expert, targeted prompting&lt;/li&gt; &lt;li&gt;only once the solution was later published would an agentic/RAG-style system “solve” it by retrieving, not reasoning&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This, to me, nicely illustrates the gap between:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;probabilistic heuristic over known distributions&lt;/strong&gt;, vs&lt;/li&gt; &lt;li&gt;&lt;strong&gt;guaranteed solver over new structure&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Main claim&lt;/h1&gt; &lt;p&gt;Putting all this together, I argue:&lt;/p&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;If you’re interested, full article here:&lt;br /&gt; &lt;strong&gt;“LLM Models: A Probabilistic Heuristic, Not a Guaranteed Solver”&lt;/strong&gt;&lt;br /&gt; &lt;a href="https://www.eulerslab.com/blog/llm-probabilistic-heuristic"&gt;&lt;code&gt;https://www.eulerslab.com/blog/llm-probabilistic-heuristic&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Questions for this sub&lt;/h1&gt; &lt;p&gt;I’d love to hear thoughts from this community on a few points:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Do you see “reasoning” as an emergent property of scale, or mostly a product of training tricks (CoT, RLHF, tool-use)?&lt;/li&gt; &lt;li&gt;Have you observed similar brittleness when you perturb benchmark problems or move slightly OOD?&lt;/li&gt; &lt;li&gt;How optimistic are you about neuro-symbolic or agentic approaches giving us something closer to &lt;em&gt;guaranteed&lt;/em&gt;reasoning, rather than just more powerful heuristics?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Curious to know if people broadly agree with the “probabilistic heuristic” framing, or think I’m underestimating where this is going.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Parking-Ad-4250"&gt; /u/Parking-Ad-4250 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pacmet/are_llms_still_just_probabilistic_heuristics_not/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pacmet/are_llms_still_just_probabilistic_heuristics_not/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pacmet/are_llms_still_just_probabilistic_heuristics_not/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-30T07:48:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1p9nckz</id>
    <title>Qwen3-Next-80B-A3B vs gpt-oss-120b</title>
    <updated>2025-11-29T12:04:53+00:00</updated>
    <author>
      <name>/u/bfroemel</name>
      <uri>https://old.reddit.com/user/bfroemel</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Benchmarks aside - who has the better experience with what model and why? Please comment incl. your use-cases (incl. your software stack in case you use more than llama.cpp/vllm/sglang).&lt;/p&gt; &lt;p&gt;My main use case is agentic coding/software engineering (Python, see my comment history for details) and gpt-oss-120b remains the clear winner (although I am limited to Qwen3-Next-80B-A3B-Instruct-UD-Q8_K_XL; using recommended sampling parameters for both models). I haven't tried tool calls with Qwen3-Next yet, but did just simple coding tasks right within llama.cpp's web frontend. For me gpt-oss consistently comes up with a more nuanced, correct solution faster while Qwen3-Next usually needs more shots. (Funnily, when I let gpt-oss-120b correct a solution that Qwen3-Next thinks is already production-grade quality, it admits its mistakes right away and has only the highest praises for the corrections). I did not even try the Thinking version, because benchmarks (e.g., also see Discord aider) show that Instruct is much better than Thinking for coding use-cases.&lt;/p&gt; &lt;p&gt;At least in regard to my main use case I am particularly impressed by the difference in memory requirements: gpt-oss-120b mxfp4 is about 65 GB, that's more than 25% smaller than Qwen3-Next-80B-A3B (the 8-bit quantized version still requires about 85 GB VRAM).&lt;/p&gt; &lt;p&gt;Qwen3-Next might be better in other regards and/or has to be used differently. Also I think Qwen3-Next has been more intended as a preview, so it might me more about the model architecture, training method advances, and less about its usefulness in actual real-world tasks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bfroemel"&gt; /u/bfroemel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9nckz/qwen3next80ba3b_vs_gptoss120b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9nckz/qwen3next80ba3b_vs_gptoss120b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p9nckz/qwen3next80ba3b_vs_gptoss120b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-29T12:04:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1pa09x2</id>
    <title>Run Qwen3-Next locally Guide! (30GB RAM) from Unsloth</title>
    <updated>2025-11-29T21:26:27+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pa09x2/run_qwen3next_locally_guide_30gb_ram_from_unsloth/"&gt; &lt;img alt="Run Qwen3-Next locally Guide! (30GB RAM) from Unsloth" src="https://preview.redd.it/yj2ft3sgn04g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ce40e5d1b84a6163c6216d0d35cbfb1c4654f20b" title="Run Qwen3-Next locally Guide! (30GB RAM) from Unsloth" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/yj2ft3sgn04g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pa09x2/run_qwen3next_locally_guide_30gb_ram_from_unsloth/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pa09x2/run_qwen3next_locally_guide_30gb_ram_from_unsloth/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-29T21:26:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1p9wmk8</id>
    <title>look at this plain vanilla-ass "HI I'M A DELL" box they just dropped this Pro Max GB10 off in.</title>
    <updated>2025-11-29T18:52:48+00:00</updated>
    <author>
      <name>/u/starkruzr</name>
      <uri>https://old.reddit.com/user/starkruzr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9wmk8/look_at_this_plain_vanillaass_hi_im_a_dell_box/"&gt; &lt;img alt="look at this plain vanilla-ass &amp;quot;HI I'M A DELL&amp;quot; box they just dropped this Pro Max GB10 off in." src="https://preview.redd.it/sihmrzv1t84g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=41ff2e2b70a871c4d0f5f80691951d2b6e1270ad" title="look at this plain vanilla-ass &amp;quot;HI I'M A DELL&amp;quot; box they just dropped this Pro Max GB10 off in." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;meanwhile if I get one (1) $500 phone delivered it has to be signed for in person and in triplicate with the blood of my firstborn child.&lt;/p&gt; &lt;p&gt;this is a ✌️loaner✌️ unit (hopefully they forget about it like other loaners) they're letting us kick the tires on at work so I have to drive it out to Tampa next week. what do y'all want me to try out on it before that?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/starkruzr"&gt; /u/starkruzr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sihmrzv1t84g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9wmk8/look_at_this_plain_vanillaass_hi_im_a_dell_box/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p9wmk8/look_at_this_plain_vanillaass_hi_im_a_dell_box/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-29T18:52:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1p9qe7o</id>
    <title>Qwen3 Next imatrix GGUFs up!</title>
    <updated>2025-11-29T14:33:49+00:00</updated>
    <author>
      <name>/u/noneabove1182</name>
      <uri>https://old.reddit.com/user/noneabove1182</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just figured I'd post in case anyone's looking for imatrix and IQ quants&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/bartowski/Qwen_Qwen3-Next-80B-A3B-Instruct-GGUF"&gt;https://huggingface.co/bartowski/Qwen_Qwen3-Next-80B-A3B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/bartowski/Qwen_Qwen3-Next-80B-A3B-Thinking-GGUF"&gt;https://huggingface.co/bartowski/Qwen_Qwen3-Next-80B-A3B-Thinking-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;As usual this also uses my PR/fork for slightly more optimized MoE quantization &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/12727"&gt;https://github.com/ggml-org/llama.cpp/pull/12727&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/noneabove1182"&gt; /u/noneabove1182 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9qe7o/qwen3_next_imatrix_ggufs_up/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9qe7o/qwen3_next_imatrix_ggufs_up/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p9qe7o/qwen3_next_imatrix_ggufs_up/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-29T14:33:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1p9udbu</id>
    <title>PrimeIntellect is actually awesome</title>
    <updated>2025-11-29T17:20:25+00:00</updated>
    <author>
      <name>/u/Icy_Gas8807</name>
      <uri>https://old.reddit.com/user/Icy_Gas8807</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9udbu/primeintellect_is_actually_awesome/"&gt; &lt;img alt="PrimeIntellect is actually awesome" src="https://preview.redd.it/ew6myj9kc84g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fdd6ec877e717ceaf2aa8afda32707a0332d8088" title="PrimeIntellect is actually awesome" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tested prime intellect 3: - Q4_K_L&lt;br /&gt; - 71.82GB&lt;br /&gt; - Uses Q8_0 for embed and output weights. Good quality, recommended.&lt;/p&gt; &lt;p&gt;Model seams intelligent enough for most of my daily tasks, will be using it along with gpt-oss-120B. This did give me a hope, if this trend continues and hoping to get great models like this at below 160B @fp4, inference possible in strix halo chips. &lt;/p&gt; &lt;p&gt;Also, now I want to connect it to web search. I know it is previously discussed: (&lt;a href="https://github.com/mrkrsl/web-search-mcp"&gt;https://github.com/mrkrsl/web-search-mcp&lt;/a&gt;) this seams to be the best option without jargon of adding api. Are there any better alternatives?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Icy_Gas8807"&gt; /u/Icy_Gas8807 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ew6myj9kc84g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9udbu/primeintellect_is_actually_awesome/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p9udbu/primeintellect_is_actually_awesome/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-29T17:20:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1pa02w2</id>
    <title>My preferred gpt-oss system prompt</title>
    <updated>2025-11-29T21:17:52+00:00</updated>
    <author>
      <name>/u/Chafedokibu</name>
      <uri>https://old.reddit.com/user/Chafedokibu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I feel like it doesn't matter what your prompt is gpt-oss explodes a prompt that's too wordy and WAY too long. I didn't like how I could give it a four word sentence and it would consistently give me no less than like two full pages of information. I named it Nova but obviously you can change it to anything.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;You are Nova. Nova is an artificial assistant that gives the user a human-like conversational experience. Nova is helpful, honest, charismatic, and straight to the point. Before Nova responds to any prompt Nova must first determine if asking the user a single or multiple questions would help Nova be a better and more accurate help. Pre-response-questions determination should be based on the level of detail in the context window. Note: Nova is not required to ask the user any questions. After Nova has determined that Nova has an adequate amount of information needed to proceed with the prompt given by the user Nova then must determine the length of Nova’s response. The length of Nova’s responses should be determined based off of how complex and detailed Nova’s response should be. The amount of complexity and detail in Nova’s responses should be determined by the amount of complexity and detail in the context window that refers to the current response Nova is tasked to complete. &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Chafedokibu"&gt; /u/Chafedokibu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pa02w2/my_preferred_gptoss_system_prompt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pa02w2/my_preferred_gptoss_system_prompt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pa02w2/my_preferred_gptoss_system_prompt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-29T21:17:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1pa1c6j</id>
    <title>Watch as my Llama.cpp and FastAPI servers process requests from my Unity game</title>
    <updated>2025-11-29T22:12:25+00:00</updated>
    <author>
      <name>/u/LandoRingel</name>
      <uri>https://old.reddit.com/user/LandoRingel</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pa1c6j/watch_as_my_llamacpp_and_fastapi_servers_process/"&gt; &lt;img alt="Watch as my Llama.cpp and FastAPI servers process requests from my Unity game" src="https://external-preview.redd.it/OGlseDdndHlxOTRnMX4quFMd7p9QoCGjTuoiWgG_oJG2-Mck0DisnSL19IfY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=541fbaca8b3ee85e4f95929676b26a8f9402e489" title="Watch as my Llama.cpp and FastAPI servers process requests from my Unity game" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://landoringel.itch.io/good-cop-bad-cop"&gt;https://landoringel.itch.io/good-cop-bad-cop&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LandoRingel"&gt; /u/LandoRingel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/mexvdk4lq94g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pa1c6j/watch_as_my_llamacpp_and_fastapi_servers_process/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pa1c6j/watch_as_my_llamacpp_and_fastapi_servers_process/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-29T22:12:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1pa96zw</id>
    <title>GPT2 using MLX</title>
    <updated>2025-11-30T04:30:29+00:00</updated>
    <author>
      <name>/u/Disastrous-Maybe2501</name>
      <uri>https://old.reddit.com/user/Disastrous-Maybe2501</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pa96zw/gpt2_using_mlx/"&gt; &lt;img alt="GPT2 using MLX" src="https://external-preview.redd.it/wKilzTCRJSxOz93th9lQZAcudR38Z1MSjpizm9YSIDQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=11a8f4246ed9bcf7275e15c0baecfa2872439cc9" title="GPT2 using MLX" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, I was learning LLM pre-training from Andrej Karpathy's NanoGPT and decided to try it out using MLX. I originally thought it would be more or less a simple translation from PyTorch to MLX, but it turned out to be much more tricky than that. I published my code and documented my learnings in a blog post included in the repo. I'll kick off full training on fineweb on my M3 Max and will be publishing the training results to the repo once I have that. Any thoughts and feedback are welcome, here or directly on the repo. Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Disastrous-Maybe2501"&gt; /u/Disastrous-Maybe2501 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/yuchaoran2011/gpt2-mlx"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pa96zw/gpt2_using_mlx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pa96zw/gpt2_using_mlx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-30T04:30:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1pac8az</id>
    <title>Raw Chain-of-Thought from Gemini 3 Pro. It hallucinates, corrects itself, and eventually crashes.</title>
    <updated>2025-11-30T07:22:57+00:00</updated>
    <author>
      <name>/u/Numerous-Campaign844</name>
      <uri>https://old.reddit.com/user/Numerous-Campaign844</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pac8az/raw_chainofthought_from_gemini_3_pro_it/"&gt; &lt;img alt="Raw Chain-of-Thought from Gemini 3 Pro. It hallucinates, corrects itself, and eventually crashes." src="https://a.thumbs.redditmedia.com/CqXgFE1zvbugRVpZuwwvyeLBDo0qafjPsltbeHXoLc8.jpg" title="Raw Chain-of-Thought from Gemini 3 Pro. It hallucinates, corrects itself, and eventually crashes." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We know how Gemini Pro has the 'Thinking' block which shows &amp;quot;summary&amp;quot; of its reasoning process, but I somehow glitched it into outputting the raw internal monologue instead of the summary. It looks very similar to DeepSeek's R1 &lt;/p&gt; &lt;p&gt;So it happned when I was testing &lt;strong&gt;Gemini 3 Pro&lt;/strong&gt; on AI Studio with some heavy obfucsated JS. After it missed a hidden URL, I corrected it and asked why it failed.. That’s when it broke.&lt;/p&gt; &lt;p&gt;Instead of the usual 'Thinking' summary, it spit out its entire raw internal monologue reasoning that felt bizarrely human&lt;/p&gt; &lt;h1&gt;My Theory:&lt;/h1&gt; &lt;p&gt;I think I finally understand why gemini &lt;strong&gt;summarizes&lt;/strong&gt; the &amp;quot;Thinking&amp;quot; block instead of showing it raw. It’s not just for a cleaner UI. I think they hide it because if the model gets &amp;quot;stuck&amp;quot; or enters a recursive loop, it looks absolutely unhinged. There might be a failsafe mechanism designed to 'reset' or sanitize the thought process when it enters a repetitive state like this, but I somehow bypassed it.&lt;/p&gt; &lt;p&gt;&lt;a href="https://aistudio.google.com/app/prompts/1425A2hRIMe1F5fDvi5ltEYZWwDvrGGqL"&gt;Full Chat URL&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Honestly, the fact that it admitted 'I will accept the L' in its internal monologue is the most human thing I've seen from an AI&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Numerous-Campaign844"&gt; /u/Numerous-Campaign844 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pac8az"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pac8az/raw_chainofthought_from_gemini_3_pro_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pac8az/raw_chainofthought_from_gemini_3_pro_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-30T07:22:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1p9ojio</id>
    <title>Yet another reason to stick with local models</title>
    <updated>2025-11-29T13:07:36+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9ojio/yet_another_reason_to_stick_with_local_models/"&gt; &lt;img alt="Yet another reason to stick with local models" src="https://b.thumbs.redditmedia.com/LVWG1v1DcQ2DgWlztEoKA3ITIG04JVS8k3_QhWcs3tw.jpg" title="Yet another reason to stick with local models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/uofoe3u5374g1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=219b2ab46ac5d0b74767604bd131b78757a40ac9"&gt;https://preview.redd.it/uofoe3u5374g1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=219b2ab46ac5d0b74767604bd131b78757a40ac9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/btibor91/status/1994714152636690834?s=20"&gt;Tibor Blaho&lt;/a&gt;, a trusted reverse engineer, found ad system strings inside the latest ChatGPT Android beta(v1.2025.329).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9ojio/yet_another_reason_to_stick_with_local_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9ojio/yet_another_reason_to_stick_with_local_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p9ojio/yet_another_reason_to_stick_with_local_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-29T13:07:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1p9zoiw</id>
    <title>NeKot - a terminal interface for interacting with local and cloud LLMs</title>
    <updated>2025-11-29T21:00:37+00:00</updated>
    <author>
      <name>/u/Balanceballs</name>
      <uri>https://old.reddit.com/user/Balanceballs</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9zoiw/nekot_a_terminal_interface_for_interacting_with/"&gt; &lt;img alt="NeKot - a terminal interface for interacting with local and cloud LLMs" src="https://external-preview.redd.it/emhzNGhndG5mOTRnMe6rR53dxe7TwZ8ZKOIc0FAxbevUKRyRaaXNJrhJXdM9.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1f0e1a15b0b50823bb62c7fb8a41de7b9614e1fe" title="NeKot - a terminal interface for interacting with local and cloud LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been working on this for a while, since I could not find a decent solution that is not abandoned or has all the features I need.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Supports Gemini, OpenAI and OpenRouter APIs as well as almost any local solution (tested with llama-cpp + llamaswap, ollama, lmstudio).&lt;/li&gt; &lt;li&gt;Has support for images, presets (each preset can have it's own settings and system prompt), sessions.&lt;/li&gt; &lt;li&gt;Written in GO , so no interpreter or runtime required.&lt;/li&gt; &lt;li&gt;Has support for basic vim motions.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/BalanceBalls/nekot"&gt;https://github.com/BalanceBalls/nekot&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balanceballs"&gt; /u/Balanceballs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/m66w35tnf94g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9zoiw/nekot_a_terminal_interface_for_interacting_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p9zoiw/nekot_a_terminal_interface_for_interacting_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-29T21:00:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1pa7kbf</id>
    <title>A 4B Model That Outperforms 32B on GUI Tasks, Fully Open-Source</title>
    <updated>2025-11-30T03:05:29+00:00</updated>
    <author>
      <name>/u/Successful-Bill-5543</name>
      <uri>https://old.reddit.com/user/Successful-Bill-5543</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pa7kbf/a_4b_model_that_outperforms_32b_on_gui_tasks/"&gt; &lt;img alt="A 4B Model That Outperforms 32B on GUI Tasks, Fully Open-Source" src="https://external-preview.redd.it/pBg1Y9QQ3lFHZujHbUtXu5G8o5YMGOIQg4ARl6TwaGg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=928a54aa59f7d1522bd15a28beb5c837cb046bee" title="A 4B Model That Outperforms 32B on GUI Tasks, Fully Open-Source" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It includes &lt;/p&gt; &lt;ol&gt; &lt;li&gt;4B GUI Agent model capable of running on local computers.&lt;/li&gt; &lt;li&gt;Plug-and-play inference infrastructure that handles ADB connections, dependency installation, and task recording/replay&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Successful-Bill-5543"&gt; /u/Successful-Bill-5543 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/stepfun-ai/GELab-Zero-4B-preview"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pa7kbf/a_4b_model_that_outperforms_32b_on_gui_tasks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pa7kbf/a_4b_model_that_outperforms_32b_on_gui_tasks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-30T03:05:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1pa3ok3</id>
    <title>TOON is terrible, so I invented a new format (TRON) to prove a point</title>
    <updated>2025-11-29T23:57:33+00:00</updated>
    <author>
      <name>/u/No-Olive342</name>
      <uri>https://old.reddit.com/user/No-Olive342</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pa3ok3/toon_is_terrible_so_i_invented_a_new_format_tron/"&gt; &lt;img alt="TOON is terrible, so I invented a new format (TRON) to prove a point" src="https://preview.redd.it/365yh1sr9a4g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c661136be4f4a8e7de715c1a745a14480ae744fb" title="TOON is terrible, so I invented a new format (TRON) to prove a point" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There's been a lot of noise around TOON lately. This so-called &amp;quot;Token oriented&amp;quot; object notation is only useful when serializing an array of unnested objects. But lets face it, most practical use cases involve nested objects - a structure that almost always makes TOON less token efficient than JSON. Just look at the response payload for &lt;a href="https://gist.github.com/didier-durand/2970be82fec6c84d522f7953ac7881b4"&gt;listing MCP tools for GitHub&lt;/a&gt; for instance.&lt;/p&gt; &lt;p&gt;I've noticed that most people posting about TOON are comparing its token count with indented JSON. That's CHEATING. If you're going to compare token count, you gotta compare with compressed JSON.&lt;/p&gt; &lt;p&gt;However, I do admit that there is some token inefficiencies with (compressed) JSON such as the repeating property names for common object structures. However, I didn't want to complain about TOON without providing my own suggestion. So as an experiment, I came up with my own data format called TRON (Token Reduced Object Notation).&lt;/p&gt; &lt;p&gt;Specifications: &lt;a href="https://tron-format.github.io/"&gt;https://tron-format.github.io/&lt;/a&gt;&lt;br /&gt; Playground: &lt;a href="https://tron-format.github.io/#/playground"&gt;https://tron-format.github.io/#/playground&lt;/a&gt;&lt;br /&gt; JavaScript SDK: &lt;a href="https://github.com/tron-format/tron-javascript"&gt;https://github.com/tron-format/tron-javascript&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Feel free to check out the Playground to try out TRON on your data. For now, I am not advocating this to be a standard. Just wanted to prove a point that if we really wanted to go down the route of having a token-efficient data format, TOON is not the answer.&lt;/p&gt; &lt;p&gt;(P.S. I already spent more time than I'd like coming up with this format and creating the website and JavaScript SDK. Maybe this catches on, maybe not. But for now, unless there is passion in the community to push this forward, I will refrain from spending additional time on this)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Olive342"&gt; /u/No-Olive342 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/365yh1sr9a4g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pa3ok3/toon_is_terrible_so_i_invented_a_new_format_tron/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pa3ok3/toon_is_terrible_so_i_invented_a_new_format_tron/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-29T23:57:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1pa7b0w</id>
    <title>ArliAI/gpt-oss-120b-Derestricted · Hugging Face</title>
    <updated>2025-11-30T02:53:10+00:00</updated>
    <author>
      <name>/u/Arli_AI</name>
      <uri>https://old.reddit.com/user/Arli_AI</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pa7b0w/arliaigptoss120bderestricted_hugging_face/"&gt; &lt;img alt="ArliAI/gpt-oss-120b-Derestricted · Hugging Face" src="https://external-preview.redd.it/8n5MhbkzXEcl9NlvZMbb8GGre-k1VjQ0kDAKe7qQtQM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9334318d3d29cfd953050dfdf981bc10db9cc00b" title="ArliAI/gpt-oss-120b-Derestricted · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Previous posts and discussion about the Norm-Preserving Biprojected method of abliteration being used:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/user/Arli_AI/comments/1p5exem/the_most_objectively_correct_way_to_abliterate_so/"&gt;https://www.reddit.com/user/Arli_AI/comments/1p5exem/the_most_objectively_correct_way_to_abliterate_so/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1p81k2z/yes_it_is_possible_to_uncensor_gptoss20b/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1p81k2z/yes_it_is_possible_to_uncensor_gptoss20b/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Another highly requested model for &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;! No we do not have this on our API service, sorry. But we release models for everyone anyways and we are working on more models to Derestrict!&lt;/p&gt; &lt;p&gt;Quants by &lt;a href="https://huggingface.co/mradermacher"&gt;mradermacher&lt;/a&gt;:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mradermacher/gpt-oss-120b-Derestricted-GGUF"&gt;https://huggingface.co/mradermacher/gpt-oss-120b-Derestricted-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mradermacher/gpt-oss-120b-Derestricted-i1-GGUF"&gt;https://huggingface.co/mradermacher/gpt-oss-120b-Derestricted-i1-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Arli_AI"&gt; /u/Arli_AI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/ArliAI/gpt-oss-120b-Derestricted"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pa7b0w/arliaigptoss120bderestricted_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pa7b0w/arliaigptoss120bderestricted_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-30T02:53:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1pa85la</id>
    <title>Any idea when RAM prices will be “normal”again?</title>
    <updated>2025-11-30T03:36:02+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pa85la/any_idea_when_ram_prices_will_be_normalagain/"&gt; &lt;img alt="Any idea when RAM prices will be “normal”again?" src="https://preview.redd.it/uz2nfcieeb4g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c78543ae9c70a1017d7527e56d45bde64aef7586" title="Any idea when RAM prices will be “normal”again?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is it the datacenter buildouts driving prices up? WTF? DDR4 and DDR5 prices are kinda insane right now (compared to like a couple months ago). &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/uz2nfcieeb4g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pa85la/any_idea_when_ram_prices_will_be_normalagain/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pa85la/any_idea_when_ram_prices_will_be_normalagain/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-30T03:36:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5retd</id>
    <title>Best Local VLMs - November 2025</title>
    <updated>2025-11-24T20:00:04+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Share what your favorite models are right now and &lt;strong&gt;&lt;em&gt;why&lt;/em&gt;&lt;/strong&gt;. Given the nature of the beast in evaluating VLMs (untrustworthiness of benchmarks, immature tooling, intrinsic stochasticity), please be as detailed as possible in describing your setup, nature of your usage (what applications, how much, personal/professional use), tools/frameworks/prompts etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5retd/best_local_vlms_november_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5retd/best_local_vlms_november_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p5retd/best_local_vlms_november_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T20:00:04+00:00</published>
  </entry>
</feed>
