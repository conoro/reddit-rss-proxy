<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-04T07:48:53+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1onbqtv</id>
    <title>gemma-3-27b-it vs qwen3-32B (non-thinking)</title>
    <updated>2025-11-03T13:28:27+00:00</updated>
    <author>
      <name>/u/RepulsiveMousse3992</name>
      <uri>https://old.reddit.com/user/RepulsiveMousse3992</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In my experience, for general reasoning tasks (code, parsing data, following instructions, answering tricky questions), qwen3-32b seems strictly superior to gemma-3-27b, *if allowed to use thinking*.&lt;/p&gt; &lt;p&gt;But if you disable thinking for qwen3-32b how do they compare? Anyone got any experience with this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RepulsiveMousse3992"&gt; /u/RepulsiveMousse3992 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onbqtv/gemma327bit_vs_qwen332b_nonthinking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onbqtv/gemma327bit_vs_qwen332b_nonthinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1onbqtv/gemma327bit_vs_qwen332b_nonthinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-03T13:28:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1onz3fi</id>
    <title>Discord Server for NVIDIA DGX Spark and Clone Discussion</title>
    <updated>2025-11-04T05:18:16+00:00</updated>
    <author>
      <name>/u/MontageKapalua6302</name>
      <uri>https://old.reddit.com/user/MontageKapalua6302</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://discord.gg/F4VrUqNt"&gt;https://discord.gg/F4VrUqNt&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Getting owners together will be good. For instance, we already confirmed across two users that the default ASUS Ascent GX10 has a broken Docker install. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MontageKapalua6302"&gt; /u/MontageKapalua6302 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onz3fi/discord_server_for_nvidia_dgx_spark_and_clone/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onz3fi/discord_server_for_nvidia_dgx_spark_and_clone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1onz3fi/discord_server_for_nvidia_dgx_spark_and_clone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T05:18:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1onlqz7</id>
    <title>First LangFlow Flow Official Release - Elephant v1.0</title>
    <updated>2025-11-03T19:42:17+00:00</updated>
    <author>
      <name>/u/LoserLLM</name>
      <uri>https://old.reddit.com/user/LoserLLM</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I started a YouTube channel a few weeks ago called LoserLLM. The goal of the channel is to teach others how they can download and host open source models on their own hardware using only two tools; LM Studio and LangFlow.&lt;/p&gt; &lt;p&gt;Last night I completed my first goal with an open source LangFlow flow. It has custom components for accessing the file system, using Playwright to access the internet, and a code runner component for running code, including bash commands.&lt;/p&gt; &lt;p&gt;Here is the video which also contains the link to download the flow that can then be imported:&lt;/p&gt; &lt;p&gt;&lt;a href="https://youtu.be/qhJUEVHvYQo?si=-pvLI-YCQP0p9ggM"&gt;Official Flow Release: Elephant v1.0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know if you have any ideas for future flows or have a prompt you'd like me to run through the flow. I will make a video about the first 5 prompts that people share with results.&lt;/p&gt; &lt;p&gt;Link directly to the flow on Google Drive: &lt;a href="https://drive.google.com/file/d/1HgDRiReQDdU3R2xMYzYv7UL6Cwbhzhuf/view?usp=sharing"&gt;https://drive.google.com/file/d/1HgDRiReQDdU3R2xMYzYv7UL6Cwbhzhuf/view?usp=sharing&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LoserLLM"&gt; /u/LoserLLM &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onlqz7/first_langflow_flow_official_release_elephant_v10/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onlqz7/first_langflow_flow_official_release_elephant_v10/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1onlqz7/first_langflow_flow_official_release_elephant_v10/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-03T19:42:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1omst7q</id>
    <title>Polish is the most effective language for prompting AI, study reveals</title>
    <updated>2025-11-02T21:02:40+00:00</updated>
    <author>
      <name>/u/tengo_harambe</name>
      <uri>https://old.reddit.com/user/tengo_harambe</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omst7q/polish_is_the_most_effective_language_for/"&gt; &lt;img alt="Polish is the most effective language for prompting AI, study reveals" src="https://external-preview.redd.it/HLkT8hEFTM_i4ECT9hWztFsoDf9RouYG6ZWJzAhQOUY.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9e82f2cea2a7637a951d648c1b316bc8d9248d9c" title="Polish is the most effective language for prompting AI, study reveals" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tengo_harambe"&gt; /u/tengo_harambe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.euronews.com/next/2025/11/01/polish-to-be-the-most-effective-language-for-prompting-ai-new-study-reveals"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omst7q/polish_is_the_most_effective_language_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1omst7q/polish_is_the_most_effective_language_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T21:02:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1ontkdv</id>
    <title>What is SOTA currently for audio-to-audio speech models?</title>
    <updated>2025-11-04T00:49:54+00:00</updated>
    <author>
      <name>/u/Ok_Construction_3021</name>
      <uri>https://old.reddit.com/user/Ok_Construction_3021</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, I was looking for audio models that are SOTA currently. Mainly to understand their architecture and how they achieved their performance.&lt;/p&gt; &lt;p&gt;Side note, what are the current new architecture/layers that have helped smaller models perform better. In the case of audio, I've seen FastConformer do quite good for Nvidia Parakeet models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Construction_3021"&gt; /u/Ok_Construction_3021 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ontkdv/what_is_sota_currently_for_audiotoaudio_speech/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ontkdv/what_is_sota_currently_for_audiotoaudio_speech/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ontkdv/what_is_sota_currently_for_audiotoaudio_speech/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T00:49:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1onubfl</id>
    <title>IPEX-LLM llama.cpp portable GPU and NPU working really well on laptop</title>
    <updated>2025-11-04T01:23:23+00:00</updated>
    <author>
      <name>/u/pdmk</name>
      <uri>https://old.reddit.com/user/pdmk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;IPEX-LLM llama.cpp portable GPU and NPU (llama-cpp-ipex-llm-2.3.0b20250424-win-npu) working really well on laptop with Intel(R) Core(TM) Ultra 7 155H (3.80 GHz) withe no discrete GPU and 16GB memory.&lt;/p&gt; &lt;p&gt;I am getting around 13 tokens/second on both which is usable:&lt;/p&gt; &lt;p&gt;&lt;code&gt;DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf and&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Llama-3.2-3B-Instruct-Q6_K.gguf&lt;/code&gt;&lt;/p&gt; &lt;p&gt;One thing I noticed is that with the ## NPU version fans don't kick in at all whereas with the GPU verison a lot of heat is produced and fans start spinning at full speed. This is so much better for laptop battery and overall heat production!!!&lt;/p&gt; &lt;p&gt;Hopefully intel keeps on releasing more model support for NPUs.&lt;/p&gt; &lt;p&gt;Has anyone else tried them? I am trying to run them locally for agentic app development for stuff like email summarizaiton locally. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pdmk"&gt; /u/pdmk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onubfl/ipexllm_llamacpp_portable_gpu_and_npu_working/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onubfl/ipexllm_llamacpp_portable_gpu_and_npu_working/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1onubfl/ipexllm_llamacpp_portable_gpu_and_npu_working/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T01:23:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1oo19qg</id>
    <title>why don't cerebras add more models like glm, minimax etc?</title>
    <updated>2025-11-04T07:31:11+00:00</updated>
    <author>
      <name>/u/DataScientia</name>
      <uri>https://old.reddit.com/user/DataScientia</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo19qg/why_dont_cerebras_add_more_models_like_glm/"&gt; &lt;img alt="why don't cerebras add more models like glm, minimax etc?" src="https://a.thumbs.redditmedia.com/sOPCwbYXE8nwcmg1MMcFk09p_3TXNz28-OHFipXwkn8.jpg" title="why don't cerebras add more models like glm, minimax etc?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/hql8pvrg07zf1.png?width=615&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=37f1afbbb47e42e493c77bf491e00242f3c61c6d"&gt;https://preview.redd.it/hql8pvrg07zf1.png?width=615&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=37f1afbbb47e42e493c77bf491e00242f3c61c6d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;why don't cerebras add more models like glm, minimax etc? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DataScientia"&gt; /u/DataScientia &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo19qg/why_dont_cerebras_add_more_models_like_glm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo19qg/why_dont_cerebras_add_more_models_like_glm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oo19qg/why_dont_cerebras_add_more_models_like_glm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T07:31:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1ongwng</id>
    <title>I want to run 8x 5060 ti to run gpt-oss 120b</title>
    <updated>2025-11-03T16:48:13+00:00</updated>
    <author>
      <name>/u/Active_String2216</name>
      <uri>https://old.reddit.com/user/Active_String2216</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am currently making a rough plan for a system under $5000 to run/experiment with LLMs. The purpose? I want to have fun, and PC building has always been my hobby.&lt;/p&gt; &lt;p&gt;I first want to start off with 4x or even 2x 5060 ti (not really locked in on the gpu chocie fyi) but I'd like to be able to expand to 8x gpus at some point. &lt;/p&gt; &lt;p&gt;Now, I have a couple questions:&lt;/p&gt; &lt;p&gt;1) Can the CPU bottleneck the GPUs?&lt;br /&gt; 2) Can the amount of RAM bottleneck running LLMs?&lt;br /&gt; 3) Does the &amp;quot;speed&amp;quot; of CPU and/or RAM matter?&lt;br /&gt; 4) Is the 5060 ti a decent choice for something like a 8x gpu system? (note that the &amp;quot;speed&amp;quot; for me doesn't really matter - I just want to be able to run large models)&lt;br /&gt; 5) This is a dumbass question; if I run this LLM pc running gpt-oss 20b on ubuntu using vllm, is it typical to have the UI/GUI on the same PC or do people usually have a web ui on a different device &amp;amp; control things from that end?&lt;/p&gt; &lt;p&gt;Please keep in mind that I am in the very beginning stages of this planning. Thank you all for your help.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Active_String2216"&gt; /u/Active_String2216 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ongwng/i_want_to_run_8x_5060_ti_to_run_gptoss_120b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ongwng/i_want_to_run_8x_5060_ti_to_run_gptoss_120b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ongwng/i_want_to_run_8x_5060_ti_to_run_gptoss_120b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-03T16:48:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1onc0hn</id>
    <title>My cheapest &amp; most consistent approach for AI 3D models so far - MiniMax-M2</title>
    <updated>2025-11-03T13:39:51+00:00</updated>
    <author>
      <name>/u/spacespacespapce</name>
      <uri>https://old.reddit.com/user/spacespacespapce</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onc0hn/my_cheapest_most_consistent_approach_for_ai_3d/"&gt; &lt;img alt="My cheapest &amp;amp; most consistent approach for AI 3D models so far - MiniMax-M2" src="https://preview.redd.it/fwg2juf4o1zf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5ddf149fb8f9ba5ba949bc1b468af863cecadb62" title="My cheapest &amp;amp; most consistent approach for AI 3D models so far - MiniMax-M2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been experimenting with MiniMax2 locally for 3D asset generation and wanted to share some early results. I'm finding it surprisingly effective for agentic coding tasks (like tool calling). Especially like the balance of speed/cost &amp;amp; consistent quality compared to the larger models I've tried.&lt;/p&gt; &lt;p&gt;This is a &amp;quot;Jack O' Lantern&amp;quot; I generated with a prompt to an &lt;a href="https://native-blend-app.vercel.app/"&gt;agent&lt;/a&gt; using MiniMax2, and I've been able to add basic lighting and carving details pretty reliably with the pipeline.&lt;/p&gt; &lt;p&gt;Curious if anyone else here is using local LLMs for creative tasks, or what techniques you're finding for efficient generations.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/spacespacespapce"&gt; /u/spacespacespapce &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fwg2juf4o1zf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onc0hn/my_cheapest_most_consistent_approach_for_ai_3d/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1onc0hn/my_cheapest_most_consistent_approach_for_ai_3d/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-03T13:39:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1onz3n2</id>
    <title>Where are you all sourcing/annotating custom datasets for vision-based LLaMA projects?</title>
    <updated>2025-11-04T05:18:34+00:00</updated>
    <author>
      <name>/u/Due_Construction5400</name>
      <uri>https://old.reddit.com/user/Due_Construction5400</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been playing with local object detection (sports + vehicles), but the hardest part is dataset prep.&lt;br /&gt; I used TagX to scrape and annotate some structured data worked pretty well.&lt;br /&gt; Wondering what the community prefers: DIY annotation, open datasets, or outsourced labeling?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Due_Construction5400"&gt; /u/Due_Construction5400 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onz3n2/where_are_you_all_sourcingannotating_custom/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onz3n2/where_are_you_all_sourcingannotating_custom/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1onz3n2/where_are_you_all_sourcingannotating_custom/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T05:18:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1on8kye</id>
    <title>MiniMax LLM head confirms: new model M2.1 coming soon</title>
    <updated>2025-11-03T10:48:13+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1on8kye/minimax_llm_head_confirms_new_model_m21_coming/"&gt; &lt;img alt="MiniMax LLM head confirms: new model M2.1 coming soon" src="https://b.thumbs.redditmedia.com/zeE60rAYP1oJJCxWUZL0idGJf1QomfF834XO5Og7MvQ.jpg" title="MiniMax LLM head confirms: new model M2.1 coming soon" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Pengyu Zhao, head of MiniMax LLM, said that to achieve the vision of &amp;quot;Intelligence with Everyone,&amp;quot; the company will continue open-sourcing its models to promote the ongoing development of the AI community. As part of the plan, he confirmed that the new model M2.1 will be released soon.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/4tscghepu0zf1.jpg?width=1293&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=f9636c4ecf40f3f278afca1a3391a3178bb32f88"&gt;https://preview.redd.it/4tscghepu0zf1.jpg?width=1293&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=f9636c4ecf40f3f278afca1a3391a3178bb32f88&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In social media interactions, when asked about the launch date of the subscription plan, Pengyu Zhao replied &amp;quot;very soon,&amp;quot; specifying it would be within one to two weeks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1on8kye/minimax_llm_head_confirms_new_model_m21_coming/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1on8kye/minimax_llm_head_confirms_new_model_m21_coming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1on8kye/minimax_llm_head_confirms_new_model_m21_coming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-03T10:48:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1omyytq</id>
    <title>Reporter: ‚ÄúPOLISH: THE SUPREME LANGUAGE OF AI.‚Äù</title>
    <updated>2025-11-03T01:31:02+00:00</updated>
    <author>
      <name>/u/Mindless_Pain1860</name>
      <uri>https://old.reddit.com/user/Mindless_Pain1860</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omyytq/reporter_polish_the_supreme_language_of_ai/"&gt; &lt;img alt="Reporter: ‚ÄúPOLISH: THE SUPREME LANGUAGE OF AI.‚Äù" src="https://preview.redd.it/jlwd6xkh3yyf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7ba1b50f272c2870a74364026d750bd194a9f243" title="Reporter: ‚ÄúPOLISH: THE SUPREME LANGUAGE OF AI.‚Äù" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Please read the paper before making any comments.&lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2503.01996"&gt;https://arxiv.org/pdf/2503.01996&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mindless_Pain1860"&gt; /u/Mindless_Pain1860 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jlwd6xkh3yyf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omyytq/reporter_polish_the_supreme_language_of_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1omyytq/reporter_polish_the_supreme_language_of_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-03T01:31:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1onfjk6</id>
    <title>multi-model coding agents hitting 76% on swe-bench. could we replicate this with local models?</title>
    <updated>2025-11-03T15:58:23+00:00</updated>
    <author>
      <name>/u/rwhitman05</name>
      <uri>https://old.reddit.com/user/rwhitman05</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;saw some benchmark results where a coding agent hit 76.1% on swe-bench verified using multi-model approach&lt;/p&gt; &lt;p&gt;the interesting part: different models for different tasks. one for navigation, one for coding, one for review. plus auto-verification loop&lt;/p&gt; &lt;p&gt;got me thinking - could we build something similar with local models? or are we not there yet?&lt;/p&gt; &lt;p&gt;different models have different strengths right. some are better at &amp;quot;find this function across 50k lines&amp;quot; vs &amp;quot;write this specific function&amp;quot;&lt;/p&gt; &lt;p&gt;like if youre fixing a bug that touches multiple files, one model finds all references, another writes the fix, then checks for side effects. makes sense to use specialized models instead of one doing everything&lt;/p&gt; &lt;p&gt;auto-verification is interesting. writes code, runs tests, fails, fixes bug, runs tests again. repeat until pass. basically automates the debug cycle&lt;/p&gt; &lt;p&gt;so could this work locally? thinking qwen2.5-coder for coding, deepseek for navigation, maybe another for review. orchestration with langchain or custom code. verification is just pytest/eslint running automatically&lt;/p&gt; &lt;p&gt;main challenges would be context management across models, when to switch models, keeping them in sync. not sure how hard that is&lt;/p&gt; &lt;p&gt;that benchmark used thinking tokens which helped (+0.7% improvement to 76.1%)&lt;/p&gt; &lt;p&gt;wondering if local models could get to 60-70% with similar architecture. would still be super useful. plus you get privacy and no api costs&lt;/p&gt; &lt;p&gt;has anyone tried multi-model orchestration locally? what models would you use? qwen? deepseek? llama? how would you handle orchestration?&lt;/p&gt; &lt;p&gt;saw some commercial tools doing this now (verdent got that 76% score, aider with different models, cursor's multi-model thing) but wondering if we can build it ourselves with local models&lt;/p&gt; &lt;p&gt;or is this just not feasible yet. would love to hear from anyone whos experimented with this&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rwhitman05"&gt; /u/rwhitman05 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onfjk6/multimodel_coding_agents_hitting_76_on_swebench/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onfjk6/multimodel_coding_agents_hitting_76_on_swebench/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1onfjk6/multimodel_coding_agents_hitting_76_on_swebench/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-03T15:58:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1onu74b</id>
    <title>Agent Flow</title>
    <updated>2025-11-04T01:17:52+00:00</updated>
    <author>
      <name>/u/Loud_Communication68</name>
      <uri>https://old.reddit.com/user/Loud_Communication68</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anybody tried Agent Flow? Seems 200b performance from an 8b model feels like the holy grail of local llm.&lt;/p&gt; &lt;p&gt;&lt;a href="https://agentflow.stanford.edu/"&gt;https://agentflow.stanford.edu/&lt;/a&gt; &lt;a href="https://huggingface.co/spaces/AgentFlow/agentflow"&gt;https://huggingface.co/spaces/AgentFlow/agentflow&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Loud_Communication68"&gt; /u/Loud_Communication68 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onu74b/agent_flow/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onu74b/agent_flow/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1onu74b/agent_flow/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T01:17:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1onxd65</id>
    <title>This might be a dumb question but can VRAM and Unified memory work together on those AMD NPUs?</title>
    <updated>2025-11-04T03:47:07+00:00</updated>
    <author>
      <name>/u/NoFudge4700</name>
      <uri>https://old.reddit.com/user/NoFudge4700</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Can one put in a graphics card along? Or attach externally? Because 128 GB of unified memory is not enough. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NoFudge4700"&gt; /u/NoFudge4700 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onxd65/this_might_be_a_dumb_question_but_can_vram_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onxd65/this_might_be_a_dumb_question_but_can_vram_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1onxd65/this_might_be_a_dumb_question_but_can_vram_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T03:47:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1onobpg</id>
    <title>Last week in Multimodal AI - Local Edition</title>
    <updated>2025-11-03T21:17:10+00:00</updated>
    <author>
      <name>/u/Vast_Yak_4147</name>
      <uri>https://old.reddit.com/user/Vast_Yak_4147</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onobpg/last_week_in_multimodal_ai_local_edition/"&gt; &lt;img alt="Last week in Multimodal AI - Local Edition" src="https://b.thumbs.redditmedia.com/aLTkUql_MhTgqwbeVrNzD0Z5C8Ybd58BW69RF7bMFBc.jpg" title="Last week in Multimodal AI - Local Edition" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I curate a weekly newsletter on multimodal AI. Here are the local/edge highlights from last week:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Emu3.5 - Open-Source World Learner&lt;/strong&gt;&lt;br /&gt; ‚Ä¢ Matches Gemini 2.5 Flash performance while running entirely on your hardware.&lt;br /&gt; ‚Ä¢ Native next-state prediction across text, images, and video for embodied tasks.&lt;br /&gt; ‚Ä¢ &lt;a href="https://arxiv.org/pdf/2510.26583"&gt;Paper&lt;/a&gt; | &lt;a href="https://emu.world/pages/web/landingPage"&gt;Project Page&lt;/a&gt; | &lt;a href="https://huggingface.co/BAAI/Emu3.5"&gt;Hugging Face&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1onobpg/video/n6d1ekmty3zf1/player"&gt;https://reddit.com/link/1onobpg/video/n6d1ekmty3zf1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;NVIDIA Surgical Qwen2.5-VL&lt;/strong&gt;&lt;br /&gt; ‚Ä¢ 7B fine-tuned model for surgical video understanding, runs locally.&lt;br /&gt; ‚Ä¢ Real-time surgical assistance without cloud dependencies.&lt;br /&gt; ‚Ä¢ &lt;a href="https://huggingface.co/nvidia/Qwen2.5-VL-7B-Surg-CholecT50"&gt;Hugging Face&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;NVIDIA ChronoEdit - Physics-Aware Editing&lt;/strong&gt;&lt;br /&gt; ‚Ä¢ 14B model for temporal image editing with physics simulation.&lt;br /&gt; ‚Ä¢ Runs on consumer GPUs for realistic local image manipulation.&lt;br /&gt; ‚Ä¢ &lt;a href="https://huggingface.co/nvidia/ChronoEdit-14B-Diffusers"&gt;Hugging Face&lt;/a&gt; | &lt;a href="https://arxiv.org/abs/2510.04290"&gt;Paper&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Wan2GP - Video Generation for GPU Poor&lt;/strong&gt;&lt;br /&gt; ‚Ä¢ Fast video generation optimized for regular consumer GPUs.&lt;br /&gt; ‚Ä¢ Makes video synthesis accessible without high-end hardware.&lt;br /&gt; ‚Ä¢ &lt;a href="https://github.com/deepbeepmeep/Wan2GP/"&gt;GitHub&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/smjap08zy3zf1.png?width=1895&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a52b0646bf062aaad45d704a28e9516c4da52d9c"&gt;https://preview.redd.it/smjap08zy3zf1.png?width=1895&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a52b0646bf062aaad45d704a28e9516c4da52d9c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;LongCat-Flash-Omni&lt;/strong&gt;&lt;br /&gt; ‚Ä¢ 560B-parameter MoE model for real-time audio-visual interaction.&lt;br /&gt; ‚Ä¢ Efficient mixture-of-experts design for local deployment.&lt;br /&gt; ‚Ä¢ &lt;a href="https://github.com/meituan-longcat/LongCat-Flash-Omni"&gt;GitHub&lt;/a&gt; | &lt;a href="https://longcat.chat/"&gt;Project Page&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Ming-flash-omni Preview&lt;/strong&gt;&lt;br /&gt; ‚Ä¢ AntGroup's new multimodal foundation model optimized for edge deployment.&lt;br /&gt; ‚Ä¢ Handles text, vision, and audio tasks locally.&lt;br /&gt; ‚Ä¢ &lt;a href="https://huggingface.co/inclusionAI/Ming-flash-omni-Preview"&gt;Hugging Face&lt;/a&gt; | &lt;a href="https://arxiv.org/abs/2510.24821"&gt;Paper&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Checkout the &lt;a href="https://open.substack.com/pub/thelivingedge/p/multimodal-monday-31-visual-thinking?r=12l7fk&amp;amp;utm_campaign=post&amp;amp;utm_medium=web&amp;amp;showWelcomeOnShare=false"&gt;full newsletter&lt;/a&gt; for more demos, papers, and resources.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vast_Yak_4147"&gt; /u/Vast_Yak_4147 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onobpg/last_week_in_multimodal_ai_local_edition/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onobpg/last_week_in_multimodal_ai_local_edition/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1onobpg/last_week_in_multimodal_ai_local_edition/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-03T21:17:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1onaops</id>
    <title>‚ö°Ô∏è Scaling Coding-Agent RL to 32x H100s. Achieving 160% improvement on Stanford's TerminalBench</title>
    <updated>2025-11-03T12:41:41+00:00</updated>
    <author>
      <name>/u/DanAiTuning</name>
      <uri>https://old.reddit.com/user/DanAiTuning</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onaops/scaling_codingagent_rl_to_32x_h100s_achieving_160/"&gt; &lt;img alt="‚ö°Ô∏è Scaling Coding-Agent RL to 32x H100s. Achieving 160% improvement on Stanford's TerminalBench" src="https://b.thumbs.redditmedia.com/jSQzd0HL5GuJP640MvEdr9aB6Jcc16z589SDLzlVOhs.jpg" title="‚ö°Ô∏è Scaling Coding-Agent RL to 32x H100s. Achieving 160% improvement on Stanford's TerminalBench" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;üëã Trekking along the forefront of applied AI is rocky territory, but it is the best place to be! My RL trained multi-agent-coding model Orca-Agent-v0.1 reached a 160% higher relative score than its base model on Stanford's TerminalBench. Which is cool! The trek across RL was at times painful, and at other times slightly less painful üòÖ I've open sourced everything.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What I did:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I trained a 14B orchestrator model to better coordinate explorer &amp;amp; coder subagents (subagents are tool calls for orchestrator)&lt;/li&gt; &lt;li&gt;Scaled to 32x H100s that were pushed to their limits across 4 bare-metal nodes&lt;/li&gt; &lt;li&gt;Scaled to 256 Docker environments rolling out simultaneously, automatically distributed across the cluster&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Key results:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Qwen3-14B jumped from &lt;strong&gt;7% ‚Üí 18.25%&lt;/strong&gt; on TerminalBench after training&lt;/li&gt; &lt;li&gt;Model now within striking distance of Qwen3-Coder-480B (19.7%)&lt;/li&gt; &lt;li&gt;Training was stable with smooth entropy decrease and healthy gradient norms&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Key learnings:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&amp;quot;Intelligently crafted&amp;quot; reward functions pale in performance to simple unit tests. Keep it simple!&lt;/li&gt; &lt;li&gt;RL is not a quick fix for improving agent performance. It is still very much in the early research phase, and in most cases prompt engineering with the latest SOTA is likely the way to go.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Training approach:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Reward design and biggest learning: Kept it simple - **just unit tests**. Every &amp;quot;smart&amp;quot; reward signal I tried to craft led to policy collapse üòÖ&lt;/p&gt; &lt;p&gt;Curriculum learning:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Stage-1: Tasks where base model succeeded 1-2/3 times (41 tasks)&lt;/li&gt; &lt;li&gt;Stage-2: Tasks where Stage-1 model succeeded 1-4/5 times&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Dataset: Used synthetically generated RL environments and unit tests&lt;/p&gt; &lt;p&gt;&lt;strong&gt;More details:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I have added lots more details in the repo:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;‚≠êÔ∏è&lt;/strong&gt; &lt;a href="https://github.com/Danau5tin/Orca-Agent-RL"&gt;&lt;strong&gt;Orca-Agent-RL repo&lt;/strong&gt;&lt;/a&gt; - training code, model weights, datasets.&lt;/p&gt; &lt;p&gt;Huge thanks to:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Taras for providing the compute and believing in open source&lt;/li&gt; &lt;li&gt;Prime Intellect team for building prime-rl and dealing with my endless questions üòÖ&lt;/li&gt; &lt;li&gt;Alex Dimakis for the conversation that sparked training the orchestrator model&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I am sharing this because I believe agentic AI is going to change everybody's lives, and so I feel it is important (and super fun!) for us all to share knowledge around this area, and also have enjoy exploring what is possible.&lt;/p&gt; &lt;p&gt;Thanks for reading!&lt;/p&gt; &lt;p&gt;Dan&lt;/p&gt; &lt;p&gt;(Evaluated on the excellent TerminalBench benchmark by Stanford &amp;amp; Laude Institute)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DanAiTuning"&gt; /u/DanAiTuning &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1onaops"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onaops/scaling_codingagent_rl_to_32x_h100s_achieving_160/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1onaops/scaling_codingagent_rl_to_32x_h100s_achieving_160/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-03T12:41:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1onxdqx</id>
    <title>GLM-4.5-Air-REAP-82B-A12B-LIMI</title>
    <updated>2025-11-04T03:47:57+00:00</updated>
    <author>
      <name>/u/CoruNethronX</name>
      <uri>https://old.reddit.com/user/CoruNethronX</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi. I'm in search of a HW grant to make this model a reality. Plan is to fine-tune cerebras/GLM-4.5-Air-REAP-82B-A12B model using GAIR/LIMI dataset. As per arXiv:2509.17567 , we could expect great gain of agentic model abilities. Script can be easily adapted from github.com/GAIR-NLP/LIMI as authors were initially fine-tuned a full GLM4.5 Air 106B model. I would expect the whole process to require about 12 hour on 8xH100 or equivalent H200 or B200 cluster. As a result I'll publish a trained 82B model with (hopefully) increased agentic abilities, a transparent evaluation report and also GGUF and MLX quants under permissive license. I expect 82B q4 quants to behave better than any 106B q3 quants on e.g. 64Gb apple HW. If you're able to provide temporary ssh acess to abovementioned GPU cluster, please contact me and let's do this.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CoruNethronX"&gt; /u/CoruNethronX &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onxdqx/glm45airreap82ba12blimi/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onxdqx/glm45airreap82ba12blimi/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1onxdqx/glm45airreap82ba12blimi/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T03:47:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1onjm1u</id>
    <title>I made a simple tool to get deterministic, instant responses from my LLM setup</title>
    <updated>2025-11-03T18:24:18+00:00</updated>
    <author>
      <name>/u/MarkZealousideal7572</name>
      <uri>https://old.reddit.com/user/MarkZealousideal7572</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;I've been working on a project to solve a problem I'm sure many of you have seen: you get fantastic, fast responses from your local models, but if you ask the &lt;em&gt;exact same question&lt;/em&gt; in a slightly different way, the model has to run the full inference again.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;Query 1: &amp;quot;how do I cancel my order&amp;quot;&lt;/code&gt; ‚Üí &lt;strong&gt;Full Generation (e.g., 5 seconds)&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;Query 2: &amp;quot;I want to cancel an order&amp;quot;&lt;/code&gt; ‚Üí &lt;strong&gt;Full Generation (e.g., 5 seconds)&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;Query 3: &amp;quot;what's the cancellation process&amp;quot;&lt;/code&gt; ‚Üí &lt;strong&gt;Full Generation (e.g., 5 seconds)&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This felt like a waste of resources, especially for common/repetitive queries in my apps (like for customer support or RAG).&lt;/p&gt; &lt;p&gt;So, I built &lt;code&gt;constraint-cache&lt;/code&gt;, a simple Python pattern that sits &lt;em&gt;in front&lt;/em&gt; of the LLM.&lt;/p&gt; &lt;p&gt;It's not semantic search. It's a deterministic normalization algorithm. It turns similar queries into a single, identical cache key.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;&amp;quot;how do I cancel my order&amp;quot;&lt;/code&gt; ‚Üí &lt;code&gt;normalize&lt;/code&gt; ‚Üí &lt;code&gt;&amp;quot;cancel_order&amp;quot;&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;&amp;quot;I want to cancel an order&amp;quot;&lt;/code&gt; ‚Üí &lt;code&gt;normalize&lt;/code&gt; ‚Üí &lt;code&gt;&amp;quot;cancel_order&amp;quot;&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;&amp;quot;what's the cancellation process&amp;quot;&lt;/code&gt; ‚Üí &lt;code&gt;normalize&lt;/code&gt; ‚Üí &lt;code&gt;&amp;quot;cancel_order&amp;quot;&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The result:&lt;/strong&gt; The first query hits the LLM, but the next two are instant &lt;strong&gt;&amp;lt;1ms cache hits&lt;/strong&gt; from Redis.&lt;/p&gt; &lt;p&gt;For those of us building agentic workflows or UIs on top of local models, this has two huge benefits:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Massive Speed Up:&lt;/strong&gt; Your app feels &lt;em&gt;instantaneous&lt;/em&gt; for 90% of common user questions.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;100% Deterministic:&lt;/strong&gt; You get the &lt;em&gt;exact&lt;/em&gt; same, perfect answer every time for that &amp;quot;intent,&amp;quot; which is great for testing and reliability. No more slightly different phrasing or hallucinations on solved problems.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I tested this on a 27,000-query customer support dataset and it got a &lt;strong&gt;99.9% cache hit rate&lt;/strong&gt; after the initial intents were cached.&lt;/p&gt; &lt;p&gt;It's all open-source, uses standard Redis, and is just a few lines of Python to implement. It's a perfect L1 cache to use before you even decide to hit your model.&lt;/p&gt; &lt;p&gt;Would love for you all to check it out, break it, and give me feedback.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub Repo:&lt;/strong&gt; &lt;a href="https://github.com/BitUnwiseOperator/constraint-cache"&gt;&lt;code&gt;https://github.com/BitUnwiseOperator/constraint-cache&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MarkZealousideal7572"&gt; /u/MarkZealousideal7572 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onjm1u/i_made_a_simple_tool_to_get_deterministic_instant/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onjm1u/i_made_a_simple_tool_to_get_deterministic_instant/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1onjm1u/i_made_a_simple_tool_to_get_deterministic_instant/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-03T18:24:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1onhdob</id>
    <title>How does cerebras get 2000toks/s?</title>
    <updated>2025-11-03T17:05:10+00:00</updated>
    <author>
      <name>/u/npmbad</name>
      <uri>https://old.reddit.com/user/npmbad</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm wondering, what sort of GPU do I need to rent and under what settings to get that speed? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/npmbad"&gt; /u/npmbad &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onhdob/how_does_cerebras_get_2000tokss/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onhdob/how_does_cerebras_get_2000tokss/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1onhdob/how_does_cerebras_get_2000tokss/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-03T17:05:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1on628o</id>
    <title>Google pulls Gemma from AI Studio after Senator Blackburn accuses model of defamation</title>
    <updated>2025-11-03T08:04:32+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1on628o/google_pulls_gemma_from_ai_studio_after_senator/"&gt; &lt;img alt="Google pulls Gemma from AI Studio after Senator Blackburn accuses model of defamation" src="https://b.thumbs.redditmedia.com/otiqqwWrYAPyBQSbvIHZ-yCKbfdiJZGic1vlE0jFFxk.jpg" title="Google pulls Gemma from AI Studio after Senator Blackburn accuses model of defamation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/0hnvozwh10zf1.png?width=1198&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ab171458093a1ad5f07a0eaa42ac44e2c5ab5681"&gt;Google Official Statement&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://techcrunch.com/2025/11/02/google-pulls-gemma-from-ai-studio-after-senator-blackburn-accuses-model-of-defamation/"&gt;Source&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Fortunately, we can still download the weights from HF and run them locally.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1on628o/google_pulls_gemma_from_ai_studio_after_senator/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1on628o/google_pulls_gemma_from_ai_studio_after_senator/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1on628o/google_pulls_gemma_from_ai_studio_after_senator/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-03T08:04:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1oo1159</id>
    <title>Anyone else feel like GPU pricing is still the biggest barrier for open-source AI?</title>
    <updated>2025-11-04T07:15:29+00:00</updated>
    <author>
      <name>/u/frentro_max</name>
      <uri>https://old.reddit.com/user/frentro_max</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Even with cheap clouds popping up, costs still hit fast when you train or fine-tune.&lt;br /&gt; How do you guys manage GPU spend for experiments?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/frentro_max"&gt; /u/frentro_max &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo1159/anyone_else_feel_like_gpu_pricing_is_still_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo1159/anyone_else_feel_like_gpu_pricing_is_still_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oo1159/anyone_else_feel_like_gpu_pricing_is_still_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T07:15:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1on8qe5</id>
    <title>basketball players recognition with RF-DETR, SAM2, SigLIP and ResNet</title>
    <updated>2025-11-03T10:57:38+00:00</updated>
    <author>
      <name>/u/RandomForests92</name>
      <uri>https://old.reddit.com/user/RandomForests92</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1on8qe5/basketball_players_recognition_with_rfdetr_sam2/"&gt; &lt;img alt="basketball players recognition with RF-DETR, SAM2, SigLIP and ResNet" src="https://external-preview.redd.it/d240ODlsYmJ3MHpmMRIAV1OZPMFu-DibzoX2jf4rOivExvgg5eIy0W2GXihc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=279c74da96e009360fea0b2b573c2a5636ed406e" title="basketball players recognition with RF-DETR, SAM2, SigLIP and ResNet" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Models I used:&lt;/p&gt; &lt;p&gt;- RF-DETR ‚Äì a DETR-style real-time object detector. We fine-tuned it to detect players, jersey numbers, referees, the ball, and even shot types.&lt;/p&gt; &lt;p&gt;- SAM2 ‚Äì a segmentation and tracking. It re-identifies players after occlusions and keeps IDs stable through contact plays.&lt;/p&gt; &lt;p&gt;- SigLIP + UMAP + K-means ‚Äì vision-language embeddings plus unsupervised clustering. This separates players into teams using uniform colors and textures, without manual labels.&lt;/p&gt; &lt;p&gt;- SmolVLM2 ‚Äì a compact vision-language model originally trained on OCR. After fine-tuning on NBA jersey crops, it jumped from 56% to 86% accuracy.&lt;/p&gt; &lt;p&gt;- ResNet-32 ‚Äì a classic CNN fine-tuned for jersey number classification. It reached 93% test accuracy, outperforming the fine-tuned SmolVLM2.&lt;/p&gt; &lt;p&gt;Links:&lt;/p&gt; &lt;p&gt;- code: &lt;a href="https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/basketball-ai-how-to-detect-track-and-identify-basketball-players.ipynb"&gt;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/basketball-ai-how-to-detect-track-and-identify-basketball-players.ipynb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- blogpost: &lt;a href="https://blog.roboflow.com/identify-basketball-players"&gt;https://blog.roboflow.com/identify-basketball-players&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- detection dataset: &lt;a href="https://universe.roboflow.com/roboflow-jvuqo/basketball-player-detection-3-ycjdo/dataset/6"&gt;https://universe.roboflow.com/roboflow-jvuqo/basketball-player-detection-3-ycjdo/dataset/6&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- numbers OCR dataset: &lt;a href="https://universe.roboflow.com/roboflow-jvuqo/basketball-jersey-numbers-ocr/dataset/3"&gt;https://universe.roboflow.com/roboflow-jvuqo/basketball-jersey-numbers-ocr/dataset/3&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RandomForests92"&gt; /u/RandomForests92 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/367omkbbw0zf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1on8qe5/basketball_players_recognition_with_rfdetr_sam2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1on8qe5/basketball_players_recognition_with_rfdetr_sam2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-03T10:57:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1onytak</id>
    <title>How much does the average person value a private LLM?</title>
    <updated>2025-11-04T05:02:49+00:00</updated>
    <author>
      <name>/u/SelectLadder8758</name>
      <uri>https://old.reddit.com/user/SelectLadder8758</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been thinking a lot about the future of local LLMs lately. My current take is that while it will eventually be possible (or maybe already is) for everyone to run very capable models locally, I‚Äôm not sure how many people will. For example, many people could run an email server themselves but everyone uses Gmail. DuckDuckGo is a perfectly viable alternative but Google still prevails. &lt;/p&gt; &lt;p&gt;Will LLMs be the same way or will there eventually be enough advantages of running locally (including but not limited to privacy) for them to realistically challenge cloud providers? Is privacy alone enough?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SelectLadder8758"&gt; /u/SelectLadder8758 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onytak/how_much_does_the_average_person_value_a_private/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onytak/how_much_does_the_average_person_value_a_private/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1onytak/how_much_does_the_average_person_value_a_private/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T05:02:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1onzrg9</id>
    <title>Qwen is roughly matching the entire American open model ecosystem today</title>
    <updated>2025-11-04T05:57:18+00:00</updated>
    <author>
      <name>/u/Old-School8916</name>
      <uri>https://old.reddit.com/user/Old-School8916</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onzrg9/qwen_is_roughly_matching_the_entire_american_open/"&gt; &lt;img alt="Qwen is roughly matching the entire American open model ecosystem today" src="https://preview.redd.it/zvugibssj6zf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e1b76885ebcc9a9fe34b1f3215330df073cc1f12" title="Qwen is roughly matching the entire American open model ecosystem today" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Old-School8916"&gt; /u/Old-School8916 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zvugibssj6zf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onzrg9/qwen_is_roughly_matching_the_entire_american_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1onzrg9/qwen_is_roughly_matching_the_entire_american_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T05:57:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1olq14f</id>
    <title>[MEGATHREAD] Local AI Hardware - November 2025</title>
    <updated>2025-11-01T15:02:17+00:00</updated>
    <author>
      <name>/u/eck72</name>
      <uri>https://old.reddit.com/user/eck72</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is the monthly thread for sharing your local AI setups and the models you're running.&lt;/p&gt; &lt;p&gt;Whether you're using a single CPU, a gaming GPU, or a full rack, post what you're running and how it performs.&lt;/p&gt; &lt;p&gt;Post in any format you like. The list below is just a guide:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Hardware: CPU, GPU(s), RAM, storage, OS&lt;/li&gt; &lt;li&gt;Model(s): name + size/quant&lt;/li&gt; &lt;li&gt;Stack: (e.g. llama.cpp + custom UI)&lt;/li&gt; &lt;li&gt;Performance: t/s, latency, context, batch etc.&lt;/li&gt; &lt;li&gt;Power consumption&lt;/li&gt; &lt;li&gt;Notes: purpose, quirks, comments&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please share setup pics for eye candy!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Quick reminder&lt;/strong&gt;: You can share hardware purely to ask questions or get feedback. All experience levels welcome.&lt;/p&gt; &lt;p&gt;House rules: no buying/selling/promo.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eck72"&gt; /u/eck72 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T15:02:17+00:00</published>
  </entry>
</feed>
