<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-13T11:35:18+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ovxvke</id>
    <title>Which LocalLLM I Can Use On My MacBook</title>
    <updated>2025-11-13T10:41:41+00:00</updated>
    <author>
      <name>/u/AegirAsura</name>
      <uri>https://old.reddit.com/user/AegirAsura</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, i recently bought a MacBook M4 Max with 48gb of ram and want to get into the LLM's, my use case is general chatting, some school work and run simulations (like battles, historical events, alternate timelines etc.) for a project. Gemini and ChatGPT told me to download LM Studio and use Llama 3.3 70B 4-bit and i downloaded this version llama-3.3-70b-instruct-dwq from mlx community but unfortunately it needs 39gb ram and i have 37 if i want to run it i needed to manually allocate more ram to the gpu. So which LLM should i use for my use case, is quality of 70B models are significantly better?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AegirAsura"&gt; /u/AegirAsura &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovxvke/which_localllm_i_can_use_on_my_macbook/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovxvke/which_localllm_i_can_use_on_my_macbook/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ovxvke/which_localllm_i_can_use_on_my_macbook/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-13T10:41:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1ov2eoz</id>
    <title>Kimi K2 Thinking: The One Point Everyone Overlooks, Interleave Thinking</title>
    <updated>2025-11-12T11:13:26+00:00</updated>
    <author>
      <name>/u/Great_Shop_4356</name>
      <uri>https://old.reddit.com/user/Great_Shop_4356</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ov2eoz/kimi_k2_thinking_the_one_point_everyone_overlooks/"&gt; &lt;img alt="Kimi K2 Thinking: The One Point Everyone Overlooks, Interleave Thinking" src="https://b.thumbs.redditmedia.com/SH8DMO2v4iobrEVSkK4oOgtmTNUnl5VcZau2mvcO-iU.jpg" title="Kimi K2 Thinking: The One Point Everyone Overlooks, Interleave Thinking" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/vxcayce98t0g1.jpg?width=954&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=fed91c942279f8311f0cf04673d450065b8b53b1"&gt;https://preview.redd.it/vxcayce98t0g1.jpg?width=954&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=fed91c942279f8311f0cf04673d450065b8b53b1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Kimi K2 Thinking supports multi-turn tool calls with interleaved thinking (think â†’ call tool â†’ reflect â†’ call another tool â†’ act). While DeepSeek's reasoning models do not support tool calls, which many people overlook. When your workflow or CLI relies on tools (grep, code-run, web_search, etc.), this difference is decisive.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/0dbz7jfc7t0g1.jpg?width=2900&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=9e1863d14935b00f24be50cddd1bdf582862ff85"&gt;DeepSeek's doc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Most &amp;quot;reasoning&amp;quot; demos still look like a single blob of chain-of-thought followed by one action. In real agents, the loop needs to be: reason â†’ probe with a tool â†’ update beliefs â†’ take the next action. That feedback loop is where quality jumps, especially for coding and multi-step ops.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Great_Shop_4356"&gt; /u/Great_Shop_4356 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ov2eoz/kimi_k2_thinking_the_one_point_everyone_overlooks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ov2eoz/kimi_k2_thinking_the_one_point_everyone_overlooks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ov2eoz/kimi_k2_thinking_the_one_point_everyone_overlooks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-12T11:13:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1ovxyhd</id>
    <title>Help choosing AI workstation hardware (budget 5â€“10k) â€“ A100 vs 2Ã—4090 for RAG + chat completions?</title>
    <updated>2025-11-13T10:46:42+00:00</updated>
    <author>
      <name>/u/Melodic-Bit7032</name>
      <uri>https://old.reddit.com/user/Melodic-Bit7032</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;Iâ€™m looking to build (or buy) an AI setup for work and would really appreciate some hardware advice.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Budget:&lt;/strong&gt;&lt;br /&gt; Roughly &lt;strong&gt;5,000â€“10,000&lt;/strong&gt; (EUR/USD range) for the whole system.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Main use case:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Running a &lt;strong&gt;Chat-Completion style API&lt;/strong&gt; (similar to OpenAIâ€™s &lt;code&gt;/chat/completions&lt;/code&gt; endpoint)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Streaming support&lt;/strong&gt; for real-time responses&lt;/li&gt; &lt;li&gt;Support for &lt;strong&gt;system / user / assistant roles&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Control over &lt;strong&gt;temperature, max tokens, top_p&lt;/strong&gt;, etc.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Embedding generation&lt;/strong&gt; for documents&lt;/li&gt; &lt;li&gt;Used in a &lt;strong&gt;RAG setup&lt;/strong&gt; (Retrieval Augmented Generation)&lt;/li&gt; &lt;li&gt;Target &lt;strong&gt;latency &amp;lt; 3 seconds per request&lt;/strong&gt; under normal load&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;My main questions:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;For this kind of workload, would you recommend: &lt;ul&gt; &lt;li&gt;a &lt;strong&gt;single A100&lt;/strong&gt;, or&lt;/li&gt; &lt;li&gt;&lt;strong&gt;2 Ã— RTX 4090&lt;/strong&gt; (or similar high-end consumer GPUs)?&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Are there any &lt;strong&gt;recommended system configurations&lt;/strong&gt; (CPU, RAM, storage, PSU, cooling, etc.) youâ€™d suggest for this price range?&lt;/li&gt; &lt;li&gt;Any &lt;strong&gt;build guides, example setups, or blog posts&lt;/strong&gt; youâ€™d recommend that are focused on local LLM/RAG backends for production-like use?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Iâ€™m mainly interested in a stable, future-proof setup that can handle multiple concurrent chat requests with low latency and also do embedding generation efficiently.&lt;/p&gt; &lt;p&gt;Thanks in advance for any tips, parts lists, or real-world experience you can share!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Melodic-Bit7032"&gt; /u/Melodic-Bit7032 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovxyhd/help_choosing_ai_workstation_hardware_budget_510k/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovxyhd/help_choosing_ai_workstation_hardware_budget_510k/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ovxyhd/help_choosing_ai_workstation_hardware_budget_510k/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-13T10:46:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ovl9ic</id>
    <title>Claude cli with LMStudio</title>
    <updated>2025-11-12T23:32:08+00:00</updated>
    <author>
      <name>/u/ImaginaryRea1ity</name>
      <uri>https://old.reddit.com/user/ImaginaryRea1ity</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I used claude cli but I don't want to use cloud ai. Any way to do the same with lmstudio?&lt;/p&gt; &lt;p&gt;Like letting a private llm access a folder.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ImaginaryRea1ity"&gt; /u/ImaginaryRea1ity &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovl9ic/claude_cli_with_lmstudio/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovl9ic/claude_cli_with_lmstudio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ovl9ic/claude_cli_with_lmstudio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-12T23:32:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1ovyqdi</id>
    <title>Venez faites Max dâ€™argent avec moi ðŸ’¸ðŸ’°</title>
    <updated>2025-11-13T11:32:41+00:00</updated>
    <author>
      <name>/u/Farmajo123</name>
      <uri>https://old.reddit.com/user/Farmajo123</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovyqdi/venez_faites_max_dargent_avec_moi/"&gt; &lt;img alt="Venez faites Max dâ€™argent avec moi ðŸ’¸ðŸ’°" src="https://preview.redd.it/h92hduwxf01g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b448aaf06853dcfeca0839bab9ff2e15b04f624d" title="Venez faites Max dâ€™argent avec moi ðŸ’¸ðŸ’°" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Farmajo123"&gt; /u/Farmajo123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/h92hduwxf01g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovyqdi/venez_faites_max_dargent_avec_moi/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ovyqdi/venez_faites_max_dargent_avec_moi/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-13T11:32:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1ovyro6</id>
    <title>What's the easiest way to setup AI Image/Videogen on Debian?</title>
    <updated>2025-11-13T11:34:48+00:00</updated>
    <author>
      <name>/u/FunnyGarbage4092</name>
      <uri>https://old.reddit.com/user/FunnyGarbage4092</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've made countless attempts and it seems like either the guide goes crossways, something doesn't work, or for some reason it insists on a NVIDIA card when I have an AMD Card. My rig is at 16gb with an RX 6600 XT 8GB And an I5-12400f&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FunnyGarbage4092"&gt; /u/FunnyGarbage4092 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovyro6/whats_the_easiest_way_to_setup_ai_imagevideogen/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovyro6/whats_the_easiest_way_to_setup_ai_imagevideogen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ovyro6/whats_the_easiest_way_to_setup_ai_imagevideogen/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-13T11:34:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1ovvkl7</id>
    <title>Vim: Fill in the Middle code completion</title>
    <updated>2025-11-13T08:11:35+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Any Vim users here who use FIM with vim? If so, what is your set-up? I'm currently using vim-ai but was looking for something that might have more intelligent context provision.&lt;/p&gt; &lt;p&gt;I'm wondering if I need to switch to a dedicated editor for FIM/AI support.&lt;/p&gt; &lt;p&gt;Any recommendations for a lightweight editor for Linux?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovvkl7/vim_fill_in_the_middle_code_completion/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovvkl7/vim_fill_in_the_middle_code_completion/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ovvkl7/vim_fill_in_the_middle_code_completion/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-13T08:11:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1ovic54</id>
    <title>Cross-GPU prefix KV reuse with RDMA / NVLink - early experimental results</title>
    <updated>2025-11-12T21:37:52+00:00</updated>
    <author>
      <name>/u/nsomani</name>
      <uri>https://old.reddit.com/user/nsomani</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been experimenting with a small prototype to reuse transformer KV attention states across GPUs. Current inference frameworks only reuse KV prefixes locally, so multi-GPU setups redo prefill work even when the prefix is identical.&lt;/p&gt; &lt;p&gt;I implemented a simple path where one process exports its prefix KV tensors, and another process with the same prefix imports them directly over GPU-to-GPU links. Under optimistic conditions Iâ€™m seeing about 15 percent latency reduction in early experiments.&lt;/p&gt; &lt;p&gt;Iâ€™d love feedback from anyone who has worked on multi-tier KV caching, RDMA/NVLink transports, or distributed inference scheduling. I made a small repo and a fork of vLLM that integrates it. (Link in the comments)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nsomani"&gt; /u/nsomani &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovic54/crossgpu_prefix_kv_reuse_with_rdma_nvlink_early/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovic54/crossgpu_prefix_kv_reuse_with_rdma_nvlink_early/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ovic54/crossgpu_prefix_kv_reuse_with_rdma_nvlink_early/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-12T21:37:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1ovcxu0</id>
    <title>[Followup] Qwen3 VL 30b a3b is pure love (or not so much)</title>
    <updated>2025-11-12T18:20:03+00:00</updated>
    <author>
      <name>/u/Njee_</name>
      <uri>https://old.reddit.com/user/Njee_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A couple of days ago I posted here showcasing a video of the webapp I'm currently making. Qwen3-VL 30B-A3B MoE got me back into this project because it amazed how good it is! (Self promotion at the end: My Project is now open sourced and avaialalbe as an easy to deploy docker container...)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Original post:&lt;/strong&gt; &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1omr9rc/qwen3_vl_30b_a3b_is_pure_love/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1omr9rc/qwen3_vl_30b_a3b_is_pure_love/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; This project provides an easy way to turn images into structured data. But Qwen3-VL 30B-A3B is not following the promt to not extract data that is not visible from images. Instead it confidently generates fake data that passes formatting checks, making it unsuitable for some fully automated tasks.&lt;/p&gt; &lt;p&gt;Well, actually using the model together with my app made me realize that it is not actually as good as expected. It's still pretty good though, to be honest.&lt;/p&gt; &lt;p&gt;However, I ran into a really interesting problem:&lt;/p&gt; &lt;p&gt;Remember that post from a few months or a year ago, where someone showed an image of a cat with 5 photoshopped legs to a Vision LLM with the question &amp;quot;how many legs&amp;quot;? The answer would always be 4. Simply because the LLM learned cats have 4 legs â†’ therefore this cat has 4 legs. It's not actually counting the legs in the image. Instead it sees a cat and answers 4.&lt;/p&gt; &lt;p&gt;Same thing happened to me using Qwen3-VL 30B-A3B.&lt;/p&gt; &lt;p&gt;I tried to extract structured data from chemical containers. Asking for CAS numbers which have a specific format. I specifically asked the model to not write down a CAS number if it's not visible. Any number that does not fit the specific format can not be a CAS number (Maybe thats even the fault - ill try to not specify the format)&lt;/p&gt; &lt;p&gt;Gemini models would respect that instruction. Qwen3 4B would also respect it (Instead it would sometimes misinterpret other numbers as CAS, ignoring the format instructions, which would then result in them not passing formatting checks).&lt;/p&gt; &lt;p&gt;But Qwen3 30B-A3B would simply ignore my prompt to not make up numbers if they are not visible. Even worse: it's smart enough to make up CAS numbers that fit the formatting rules, and the inbuilt checksum. They seem totally legitimate but are still wrong. Hence I wouldn't be able to filter those with simple postprocessing, but would pollute my dataset if id take the extracted data unreviewed.&lt;/p&gt; &lt;p&gt;I've done a detailed comparison of Qwen3-VL 30B-A3B, Qwen3-VL 4B, and Gemini 2.5 Flash in these scenarios. You can find numbers, plots, and methodology here, have a read if you want to.&lt;/p&gt; &lt;p&gt;&lt;a href="https://janbndrf.github.io/Tabtin/#Qwen"&gt;https://janbndrf.github.io/Tabtin/#Qwen&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The Webapp youre seeing in the Video is now available as an easy-to-deploy Docker container. I called it Tabtin. It works with local models, Google AI Studio, and OpenRouter.&lt;/p&gt; &lt;p&gt;Check it out: &lt;a href="https://github.com/janbndrf/tabtin"&gt;https://github.com/janbndrf/tabtin&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Njee_"&gt; /u/Njee_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovcxu0/followup_qwen3_vl_30b_a3b_is_pure_love_or_not_so/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovcxu0/followup_qwen3_vl_30b_a3b_is_pure_love_or_not_so/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ovcxu0/followup_qwen3_vl_30b_a3b_is_pure_love_or_not_so/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-12T18:20:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1ovbssf</id>
    <title>Is Polish better for prompting LLMs? Case study: Logical puzzles</title>
    <updated>2025-11-12T17:40:03+00:00</updated>
    <author>
      <name>/u/Substantial_Sail_668</name>
      <uri>https://old.reddit.com/user/Substantial_Sail_668</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovbssf/is_polish_better_for_prompting_llms_case_study/"&gt; &lt;img alt="Is Polish better for prompting LLMs? Case study: Logical puzzles" src="https://b.thumbs.redditmedia.com/iebRkPWZ9kphj1SEpLB2U4o11_A6ic666lV0hdKGkHo.jpg" title="Is Polish better for prompting LLMs? Case study: Logical puzzles" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, recently this article made waves within many LLM communities: &lt;a href="https://www.euronews.com/next/2025/11/01/polish-to-be-the-most-effective-language-for-prompting-ai-new-study-reveals"&gt;https://www.euronews.com/next/2025/11/01/polish-to-be-the-most-effective-language-for-prompting-ai-new-study-reveals&lt;/a&gt; as it claimed (based on a study by researchers from The University of Maryland and Microsoft) that Polish is the best language for prompting LLMs.&lt;/p&gt; &lt;p&gt;So I decided to put it to a small test. I have dug up a couple of books with puzzles and chose some random ones, translated them from the original Polish into English and made them into two Benchmarks. Run it on a bunch of LLMs and here are the results. Not so obvious after all:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/iij23lcx2v0g1.png?width=1889&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=382a824c1a766a14f4bad1b86f158c232463dd5f"&gt;https://preview.redd.it/iij23lcx2v0g1.png?width=1889&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=382a824c1a766a14f4bad1b86f158c232463dd5f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;On the left you see the results for the original Polish dataset, on the right the English version.&lt;/p&gt; &lt;p&gt;Some quick insights:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Overall the &lt;strong&gt;average accuracy&lt;/strong&gt; was a little over 2 percentage points higher on Polish.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Grok models:&lt;/strong&gt; Exceptional multilingual consistency&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Google models:&lt;/strong&gt; Mixedâ€”flagship dropped, flash variants improved&lt;/li&gt; &lt;li&gt;&lt;strong&gt;DeepSeek models:&lt;/strong&gt; Strong English bias&lt;/li&gt; &lt;li&gt;&lt;strong&gt;OpenAI models:&lt;/strong&gt; Both ChatGPT-4o and GPT-4o performed worse in Polish&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you want me to run the Benchmarks on any other models or do a comparison for a different field, let me know.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Substantial_Sail_668"&gt; /u/Substantial_Sail_668 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovbssf/is_polish_better_for_prompting_llms_case_study/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovbssf/is_polish_better_for_prompting_llms_case_study/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ovbssf/is_polish_better_for_prompting_llms_case_study/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-12T17:40:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1ove1px</id>
    <title>Why Ampere Workstation/Datacenter/Server GPUs are still so expensive after 5+ years?</title>
    <updated>2025-11-12T18:59:35+00:00</updated>
    <author>
      <name>/u/panchovix</name>
      <uri>https://old.reddit.com/user/panchovix</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello guys, just an small discussion that came to my mind after reading this post &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ovatvf/where_are_all_the_data_centers_dumping_their_old/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1ovatvf/where_are_all_the_data_centers_dumping_their_old/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I feel I guess it does a bit of sense that Ada Workstation/Datacenter/Server are still expensive, as they support fp8, and have way more compute than Ampere, i.e.:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;RTX 6000 Ada (48GB), on ebay for about 5000 USD.&lt;/li&gt; &lt;li&gt;RTX 5000 Ada (32GB), on ebay for about 2800-3000 USD.&lt;/li&gt; &lt;li&gt;RTX 4000 Ada (24GB), on ebay for about 1200 USD.&lt;/li&gt; &lt;li&gt;NVIDIA L40 (48GB), on ebay for about 7000 USD.&lt;/li&gt; &lt;li&gt;NVIDIA L40S (48GB), on ebay for about 7000USD.&lt;/li&gt; &lt;li&gt;NVIDIA L4 (24 GB), on ebay for about 2200 to 2800 USD.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;While, for Ampere, we have these cases:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;RTX A6000 (48GB), on ebay for about 4000-4500 USD.&lt;/li&gt; &lt;li&gt;RTX A5000 (24GB), on ebay for about 1400 USD.&lt;/li&gt; &lt;li&gt;RTX A4000 (16GB), on ebay for about 750 USD.&lt;/li&gt; &lt;li&gt;NVIDIA A40 (48GB), on ebay for about 4000 USD.&lt;/li&gt; &lt;li&gt;NVIDIA A100 (40GB) PCIe, on ebay for about 4000 USD.&lt;/li&gt; &lt;li&gt;NVIDIA A100 (80GB) PCIe, on ebay for about 7000 USD.&lt;/li&gt; &lt;li&gt;NVIDIA A10 (24GB), on ebat for about 1800 USD.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So these cards are slower (about half perf compared to Ada), some less VRAM and don't support FP8.&lt;/p&gt; &lt;p&gt;Why are they still so expensive, what do you guys think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/panchovix"&gt; /u/panchovix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ove1px/why_ampere_workstationdatacenterserver_gpus_are/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ove1px/why_ampere_workstationdatacenterserver_gpus_are/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ove1px/why_ampere_workstationdatacenterserver_gpus_are/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-12T18:59:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1ov9lug</id>
    <title>Has the USA/EU given up on open weight models?</title>
    <updated>2025-11-12T16:20:46+00:00</updated>
    <author>
      <name>/u/justDeveloperHere</name>
      <uri>https://old.reddit.com/user/justDeveloperHere</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In the last couple of months, we only see Chinese models (thank God). I don't remember that in recent months we had any open model that came from the USA/EU. Do you think they changed their tactics and don't care anymore?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/justDeveloperHere"&gt; /u/justDeveloperHere &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ov9lug/has_the_usaeu_given_up_on_open_weight_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ov9lug/has_the_usaeu_given_up_on_open_weight_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ov9lug/has_the_usaeu_given_up_on_open_weight_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-12T16:20:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1ovww60</id>
    <title>What Modell to run on 8x A100 (40GB)?</title>
    <updated>2025-11-13T09:39:30+00:00</updated>
    <author>
      <name>/u/Not_Black_is_taken</name>
      <uri>https://old.reddit.com/user/Not_Black_is_taken</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone, &lt;/p&gt; &lt;p&gt;I just got access to a 8x A100 GPU server. Do you have some interesting models I should try to run and or benchmark?&lt;/p&gt; &lt;p&gt;Here are the specs of the system: 8x A100 40GB (320GB total) AMD EPYC 7302 (16 Cores / 32 Threads) 1TB of RAM&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Not_Black_is_taken"&gt; /u/Not_Black_is_taken &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovww60/what_modell_to_run_on_8x_a100_40gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovww60/what_modell_to_run_on_8x_a100_40gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ovww60/what_modell_to_run_on_8x_a100_40gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-13T09:39:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1ovx2jr</id>
    <title>Qwen Chat Bot - Inaccessible Source Links</title>
    <updated>2025-11-13T09:51:07+00:00</updated>
    <author>
      <name>/u/middyy95</name>
      <uri>https://old.reddit.com/user/middyy95</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So when I prompted the Qwen AI chatbot to provide me links/sources to its claims, all (like all the links) the links do not work at all&lt;/p&gt; &lt;p&gt;- I understand that some links are behind paywalls but I have tried over 50+ links and they're all 'broken'/non-existent links&lt;/p&gt; &lt;p&gt;Due to the lack of actual sources/links, it seems risky to even believe the slightest form of answer it gives.&lt;/p&gt; &lt;p&gt;Does anyone have the same issue?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/middyy95"&gt; /u/middyy95 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovx2jr/qwen_chat_bot_inaccessible_source_links/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovx2jr/qwen_chat_bot_inaccessible_source_links/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ovx2jr/qwen_chat_bot_inaccessible_source_links/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-13T09:51:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1ovmyux</id>
    <title>Uncensored models</title>
    <updated>2025-11-13T00:44:18+00:00</updated>
    <author>
      <name>/u/NotoriousKekabidze</name>
      <uri>https://old.reddit.com/user/NotoriousKekabidze</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone, Iâ€™m new to the thread and Iâ€™m not sure if Iâ€™m asking my question in the right place. Still, Iâ€™m wondering: are there any AI models for local use that are as uncensored as, or even more uncensored than, Venice.ai? Or would it be better to just run regular open-source LLMs locally and try to look for jailbreaks?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NotoriousKekabidze"&gt; /u/NotoriousKekabidze &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovmyux/uncensored_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovmyux/uncensored_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ovmyux/uncensored_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-13T00:44:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1ovra2o</id>
    <title>Open source x 3: GRPO training with OpenEnv, vLLM, and Oumi</title>
    <updated>2025-11-13T04:03:41+00:00</updated>
    <author>
      <name>/u/PrincipleFar6835</name>
      <uri>https://old.reddit.com/user/PrincipleFar6835</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;You may have seen the release of open source OpenEnv a fews weeks ago at the PyTorch Conference. I wanted to share a tutorial showing how you can actually do GRPO training using an OpenEnv environment server and vLLM: &lt;a href="https://github.com/oumi-ai/oumi/blob/main/notebooks/Oumi%20-%20OpenEnv%20GRPO%20with%20trl.ipynb"&gt;https://github.com/oumi-ai/oumi/blob/main/notebooks/Oumi%20-%20OpenEnv%20GRPO%20with%20trl.ipynb&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PrincipleFar6835"&gt; /u/PrincipleFar6835 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovra2o/open_source_x_3_grpo_training_with_openenv_vllm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovra2o/open_source_x_3_grpo_training_with_openenv_vllm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ovra2o/open_source_x_3_grpo_training_with_openenv_vllm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-13T04:03:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1ovw4gk</id>
    <title>RAG Paper 25.11.12</title>
    <updated>2025-11-13T08:48:22+00:00</updated>
    <author>
      <name>/u/Cheryl_Apple</name>
      <uri>https://old.reddit.com/user/Cheryl_Apple</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ol&gt; &lt;li&gt;&lt;p&gt;&lt;a href="http://arxiv.org/abs/2511.08181v1"&gt;MARC: Multimodal and Multi-Task Agentic Retrieval-Augmented Generation for Cold-Start Recommender System&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;a href="http://arxiv.org/abs/2511.08505v1"&gt;Structured RAG for Answering Aggregative Questions&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;a href="http://arxiv.org/abs/2511.08245v1"&gt;Prompt Tuning for Natural Language to SQL with Embedding Fine-Tuning and RAG&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;a href="http://arxiv.org/abs/2511.08029v1"&gt;BiCA: Effective Biomedical Dense Retrieval with Citation-Aware Hard Negatives&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;a href="http://arxiv.org/abs/2511.08507v1"&gt;Introducing A Bangla Sentence - Gloss Pair Dataset for Bangla Sign Language Translation and Research&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;a href="http://arxiv.org/abs/2511.08274v1"&gt;Multi-Agent GraphRAG: A Text-to-Cypher Framework for Labeled Property Graphs&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;a href="http://arxiv.org/abs/2511.08343v1"&gt;JobSphere: An AI-Powered Multilingual Career Copilot for Government Employment Platforms&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;a href="http://arxiv.org/abs/2511.07943v1"&gt;Thinker: Training LLMs in Hierarchical Thinking for Deep Search via Multi-Turn Interaction&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;a href="http://arxiv.org/abs/2511.07982v1"&gt;NOTAM-Evolve: A Knowledge-Guided Self-Evolving Optimization Framework with LLMs for NOTAM Interpretation&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Collected by OpenBMB, transferred by&lt;/strong&gt; &lt;a href="https://www.ragview.ai/"&gt;&lt;strong&gt;RagView.ai&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;/&lt;/strong&gt; &lt;a href="https://github.com/RagView/RagView"&gt;&lt;strong&gt;github/RagView&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cheryl_Apple"&gt; /u/Cheryl_Apple &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovw4gk/rag_paper_251112/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovw4gk/rag_paper_251112/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ovw4gk/rag_paper_251112/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-13T08:48:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ovm3jd</id>
    <title>Kimi K2 Thinking Creative Writing Test</title>
    <updated>2025-11-13T00:06:55+00:00</updated>
    <author>
      <name>/u/kennydotun123</name>
      <uri>https://old.reddit.com/user/kennydotun123</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Whenever a new model is dropped, either from one of the established labs, or from a new lab, the first thing I do is to give it a creative writing test. I am not a coder. I am more interested in creative writing. And so, my expectations are usually a bit different from most of the people involved in the AI scene. The test I use is simple. I give the AI some background information and worldbuilding details, and then a very rough prologue sketch, including a list of agents that I want the AI to use to edit the prose. Using those agents, the AI is to stretch and refine the sketch to a prologue that is about 2000 words. I have done this consistently for months, and before moving on with my main point, I will list some of my observations-&lt;/p&gt; &lt;p&gt;Lets start with Chatgpt- The newer models are solid. Very, very good. Arguably the best. No complaints. At least for the first couple chapters. To note moving forward, this goes for chatgpt as well as the other models, they all seem to decline in quality in like the third chapter, and more so after that. So, to me these are not long term companions. Honestly, if that could be fixed, I could see AI being used more in the literary scene. &lt;/p&gt; &lt;p&gt;Moving on to Gemini- Was not good until 2.0Pro came, then it got surprisingly better, then 2.5pro came, then it got really good, good enough that I became tempted to start plotting more chapters. Which is usually a good sign. The quality usually declines immediately after, for this and all other models, in my opinion, however, when the prologue is solid, that's a good sign. I go back to Gemini and I am surprised again at how good the writing got. &lt;/p&gt; &lt;p&gt;Claude- Really good, could be the best, but got stagnant/limited. Claude used to be my go to AI for creative writing. I remember there was a time when everyone boasted about Claude's writing chops. I was one of those people. Don't get me wrong, the writing is amazing, still is, but it feels less like Claude got better and more like the others caught up in my opinion. Claude's writing was what made it stand out in the whole field, now the field appears full in my opinion. And I know this because sometimes, I use the old models, and the prose there maintains a kind of elegance. Indicating that while the newer models did improve in certain areas, the AI more or less stagnated. Which is fine, I'm not complaining, but it feels like, if that's the case, then they should focus more on longevity. And that is when it is good. Often it gets over ambitious, it starts doing too much, and weirdly enough, the writing gets awful then. But sometimes, it writes like it really gets you. My relationship with Claude is complex. &lt;/p&gt; &lt;p&gt;Grok- Okay. Fine. &lt;/p&gt; &lt;p&gt;Now, I know that each of these AI's have different models, with different capabilities, but I more or less breezed through these differences for the sake of brevity. Just assume that I am talking about the latest models. Now moving on the the open source models-&lt;/p&gt; &lt;p&gt;Gemma- Not good. &lt;/p&gt; &lt;p&gt;GPT-OSS- Not good. &lt;/p&gt; &lt;p&gt;Llama- Not good. At best, okay. &lt;/p&gt; &lt;p&gt;Now we will move to the Chinese models, one of which, this post centers around. Many of then are either open or quasi open. &lt;/p&gt; &lt;p&gt;Ling and Ring 1T- For some reason, they kept spazzing out. I would look at the reasoning and it was like a guy was driving, then suddenly got super drunk and flew off the road. I never even got any write ups from them, the whole thing would just crash. &lt;/p&gt; &lt;p&gt;Deepseek- It writes like it does not care for creative writing, and in turn, I don't care for it much. &lt;/p&gt; &lt;p&gt;Qwen- Same as Deepseek. &lt;/p&gt; &lt;p&gt;Kimi- When Kimi first came out. I was interested. Everyone raved about it, and so I did the test, it was the first lab that did not spaz out on me, did not start inserting random Chinese letters in the text, it was not good, alright average, but unlike Deepseek and Qwen, it seemed like it cared somewhat. So I decided to put an eye on it. K2 thinking came out. And I noticed instantly, the writing was good. Really good. About as good as the other labs. In my opinion, in terms of creative writing, it is the one that somewhat captures the heart of the story I suppose. Although Claude seems to get it as well. Anyhoo, I'll put the link below to the writing tests. &lt;/p&gt; &lt;p&gt;Here's the link;&lt;br /&gt; &lt;a href="https://docs.google.com/document/d/1ln9txx6vOtyNcYnmb_yBvjMPtzzqlCZTBKJVIsEdjdw/edit?usp=sharing"&gt;https://docs.google.com/document/d/1ln9txx6vOtyNcYnmb_yBvjMPtzzqlCZTBKJVIsEdjdw/edit?usp=sharing&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kennydotun123"&gt; /u/kennydotun123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovm3jd/kimi_k2_thinking_creative_writing_test/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovm3jd/kimi_k2_thinking_creative_writing_test/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ovm3jd/kimi_k2_thinking_creative_writing_test/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-13T00:06:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1ov3dkb</id>
    <title>AELLA: 100M+ research papers: an open-science initiative to make scientific research accessible via structured summaries created by LLMs</title>
    <updated>2025-11-12T12:06:13+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ov3dkb/aella_100m_research_papers_an_openscience/"&gt; &lt;img alt="AELLA: 100M+ research papers: an open-science initiative to make scientific research accessible via structured summaries created by LLMs" src="https://external-preview.redd.it/Ym1xdmdzdXRldDBnMR0L-Ennn3ovi4auFkXdc601F67-ibAb8bxVVAjHQXSP.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7c6b63f193e162db80ace947cf0df279cfbc1423" title="AELLA: 100M+ research papers: an open-science initiative to make scientific research accessible via structured summaries created by LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Blog: &lt;a href="https://inference.net/blog/project-aella"&gt;https://inference.net/blog/project-aella&lt;/a&gt;&lt;br /&gt; Models: &lt;a href="https://huggingface.co/inference-net"&gt;https://huggingface.co/inference-net&lt;/a&gt;&lt;br /&gt; Visualizer: &lt;a href="https://aella.inference.net/"&gt;https://aella.inference.net&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/du59aiutet0g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ov3dkb/aella_100m_research_papers_an_openscience/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ov3dkb/aella_100m_research_papers_an_openscience/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-12T12:06:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ovc3sj</id>
    <title>Live VLM WebUI - Web interface for Ollama vision models with real-time video streaming</title>
    <updated>2025-11-12T17:50:56+00:00</updated>
    <author>
      <name>/u/lektoq</name>
      <uri>https://old.reddit.com/user/lektoq</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovc3sj/live_vlm_webui_web_interface_for_ollama_vision/"&gt; &lt;img alt="Live VLM WebUI - Web interface for Ollama vision models with real-time video streaming" src="https://preview.redd.it/n5cc10ph5v0g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=badc8223252000cebb903a5e944f40eb1d1caa53" title="Live VLM WebUI - Web interface for Ollama vision models with real-time video streaming" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;! ðŸ‘‹&lt;/p&gt; &lt;p&gt;I'm a Technical Marketing Engineer at NVIDIA working on Jetson, and we just open-sourced &lt;a href="https://github.com/nvidia-ai-iot/live-vlm-webui"&gt;&lt;strong&gt;Live VLM WebUI&lt;/strong&gt;&lt;/a&gt; - a tool for testing Vision Language Models locally with real-time video streaming.&lt;/p&gt; &lt;h1&gt;What is it?&lt;/h1&gt; &lt;p&gt;Stream your webcam to any Ollama vision model (or other VLM backends) and get real-time AI analysis overlaid on your video feed. Think of it as a convenient interface for testing vision models in real-time scenarios.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it does:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Stream live video to the model (not screenshot-by-screenshot)&lt;/li&gt; &lt;li&gt;Show you exactly how fast it's processing frames&lt;/li&gt; &lt;li&gt;Monitor GPU/VRAM usage in real-time&lt;/li&gt; &lt;li&gt;Work across different hardware (PC, Mac, Jetson)&lt;/li&gt; &lt;li&gt;Support multiple backends (Ollama, vLLM, NVIDIA API Catalog, OpenAI)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Key Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;WebRTC video streaming&lt;/strong&gt; - Low latency, works with any webcam&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ollama native support&lt;/strong&gt; - Auto-detect &lt;code&gt;http://localhost:11434&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Real-time metrics&lt;/strong&gt; - See inference time, GPU usage, VRAM, tokens/sec&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multi-backend&lt;/strong&gt; - Also works with vLLM, NVIDIA API Catalog, OpenAI&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cross-platform&lt;/strong&gt; - Linux PC, DGX Spark, Jetson, Mac, WSL&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Easy install&lt;/strong&gt; - &lt;code&gt;pip install live-vlm-webui&lt;/code&gt; and you're done&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Apache 2.0&lt;/strong&gt; - Fully open source, accepting community contributions&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;ðŸš€ Quick Start with Ollama&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;# 1. Make sure Ollama is running with a vision model ollama pull gemma:4b # 2. Install and run pip install live-vlm-webui live-vlm-webui # 3. Open https://localhost:8090 # 4. Select &amp;quot;Ollama&amp;quot; backend and your model &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Use Cases I've Found Helpful&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Model comparison&lt;/strong&gt; - Testing &lt;code&gt;gemma:4b&lt;/code&gt; vs &lt;code&gt;gemma:12b&lt;/code&gt; vs &lt;code&gt;llama3.2-vision&lt;/code&gt; the same scenes&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Performance benchmarking&lt;/strong&gt; - See actual inference speed on your hardware&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Interactive demos&lt;/strong&gt; - Show people what vision models can do in real-time&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Real-time prompt engineering&lt;/strong&gt; - Tune your vision prompt as seeing the result in real-time&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Development&lt;/strong&gt; - Quick feedback loop when working with VLMs&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Models That Work Great&lt;/h1&gt; &lt;p&gt;Any Ollama vision model:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;gemma3:4b&lt;/code&gt;, &lt;code&gt;gemma3:12b&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;llama3.2-vision:11b&lt;/code&gt;, &lt;code&gt;llama3.2-vision:90b&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;qwen2.5-vl:3b&lt;/code&gt;, &lt;code&gt;qwen2.5-vl:7b&lt;/code&gt;, &lt;code&gt;qwen2.5-vl:32b&lt;/code&gt;, &lt;code&gt;qwen2.5-vl:72b&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;qwen3-vl:2b&lt;/code&gt;, &lt;code&gt;qwen3-vl:4b&lt;/code&gt;, all the way up to &lt;code&gt;qwen3-vl:235b&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;llava:7b&lt;/code&gt;, &lt;code&gt;llava:13b&lt;/code&gt;, &lt;code&gt;llava:34b&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;minicpm-v:8b&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Docker Alternative&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;docker run -d --gpus all --network host \ ghcr.io/nvidia-ai-iot/live-vlm-webui:latest &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;What's Next?&lt;/h1&gt; &lt;p&gt;Planning to add:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Analysis result copy to clipboard, log and export&lt;/li&gt; &lt;li&gt;Model comparison view (side-by-side)&lt;/li&gt; &lt;li&gt;Better prompt templates&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Links&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/nvidia-ai-iot/live-vlm-webui"&gt;https://github.com/nvidia-ai-iot/live-vlm-webui&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Docs:&lt;/strong&gt; &lt;a href="https://github.com/nvidia-ai-iot/live-vlm-webui/tree/main/docs"&gt;https://github.com/nvidia-ai-iot/live-vlm-webui/tree/main/docs&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;PyPI:&lt;/strong&gt; &lt;a href="https://pypi.org/project/live-vlm-webui/"&gt;https://pypi.org/project/live-vlm-webui/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to hear what you think! What features would make this more useful for your workflows? PRs and issues welcome - this is meant to be a community tool.&lt;/p&gt; &lt;blockquote&gt; &lt;h2&gt;A bit of background&lt;/h2&gt; &lt;p&gt;This community has been a huge inspiration for our work. When we launched the &lt;a href="https://developer.nvidia.com/blog/bringing-generative-ai-to-life-with-jetson/"&gt;Jetson Generative AI Lab&lt;/a&gt;, &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; was literally cited as one of the key communities driving the local AI movement.&lt;/p&gt; &lt;p&gt;WebRTC integration for real-time camera streaming into VLMs on Jetson was pioneered by our colleague a while back. It was groundbreaking but tightly coupled to specific setups. Then Ollama came along and with their standardized API we suddenly could serve vision models in a way that works anywhere.&lt;/p&gt; &lt;p&gt;We realized we could take that WebRTC streaming approach and modernize it: make it work with any VLM backend through standard APIs, run on any platform, and give people a better experience than uploading images on Open WebUI and waiting for responses.&lt;/p&gt; &lt;p&gt;So this is kind of the evolution of that original work - taking what we learned on Jetson and making it accessible to the broader local AI community.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Happy to answer any questions about setup, performance, or implementation details!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lektoq"&gt; /u/lektoq &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/n5cc10ph5v0g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovc3sj/live_vlm_webui_web_interface_for_ollama_vision/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ovc3sj/live_vlm_webui_web_interface_for_ollama_vision/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-12T17:50:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1ovatvf</id>
    <title>Where are all the data centers dumping their old decommissioned GPUs?</title>
    <updated>2025-11-12T17:05:19+00:00</updated>
    <author>
      <name>/u/AffectSouthern9894</name>
      <uri>https://old.reddit.com/user/AffectSouthern9894</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In 2022, I purchased a lot of Tesla P40s on eBay, but unfortunately, because of their outdated architecture, they are now practically useless for what I want to do. It seems like newer-generation GPUs arenâ€™t finding their way into consumers' hands. I asked my data center connection and he said they are recycling them, but theyâ€™ve always been doing this and we could still get hardware.&lt;/p&gt; &lt;p&gt;With the amount of commercial GPUs in the market right now, you would think there would be some overflow?&lt;/p&gt; &lt;p&gt;I hope to be wrong and suck at resourcing now, any help?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AffectSouthern9894"&gt; /u/AffectSouthern9894 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovatvf/where_are_all_the_data_centers_dumping_their_old/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovatvf/where_are_all_the_data_centers_dumping_their_old/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ovatvf/where_are_all_the_data_centers_dumping_their_old/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-12T17:05:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ovv95d</id>
    <title>Stanford's new Equivariant Encryption enables private AI inference with zero slowdown - works with any symmetric encryption</title>
    <updated>2025-11-13T07:51:50+00:00</updated>
    <author>
      <name>/u/Proof-Possibility-54</name>
      <uri>https://old.reddit.com/user/Proof-Possibility-54</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just came across this paper (arXiv:2502.01013) that could be huge for private local model deployment.&lt;/p&gt; &lt;p&gt;The researchers achieved 99.999% accuracy on encrypted neural network inference with literally zero additional latency. Not &amp;quot;minimal&amp;quot; overhead - actually zero.&lt;/p&gt; &lt;p&gt;The key insight: instead of using homomorphic encryption (10,000x slowdown), they train networks to use &amp;quot;equivariant functions&amp;quot; that commute with encryption operations. So you can compute directly on AES or ChaCha20 encrypted data.&lt;/p&gt; &lt;p&gt;What this means for local LLMs:&lt;/p&gt; &lt;p&gt;- Your prompts could remain encrypted in memory&lt;/p&gt; &lt;p&gt;- Model weights could be encrypted at rest&lt;/p&gt; &lt;p&gt;- No performance penalty for privacy&lt;/p&gt; &lt;p&gt;The catch: you need to retrain models with their specific architecture constraints. Can't just plug this into existing models.&lt;/p&gt; &lt;p&gt;Paper: &lt;a href="https://arxiv.org/abs/2502.01013"&gt;https://arxiv.org/abs/2502.01013&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also made a technical breakdown analyzing the limitations they gloss over: &lt;a href="https://youtu.be/PXKO5nkVLI4"&gt;https://youtu.be/PXKO5nkVLI4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Anyone see potential applications for local assistant privacy? The embedding layer limitations seem like the biggest bottleneck for LLM applications.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Proof-Possibility-54"&gt; /u/Proof-Possibility-54 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovv95d/stanfords_new_equivariant_encryption_enables/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovv95d/stanfords_new_equivariant_encryption_enables/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ovv95d/stanfords_new_equivariant_encryption_enables/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-13T07:51:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1ovsqs7</id>
    <title>Insane week for LLMs</title>
    <updated>2025-11-13T05:20:18+00:00</updated>
    <author>
      <name>/u/Interesting-Gur4782</name>
      <uri>https://old.reddit.com/user/Interesting-Gur4782</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovsqs7/insane_week_for_llms/"&gt; &lt;img alt="Insane week for LLMs" src="https://a.thumbs.redditmedia.com/2aUG7XKWHqAxYbI-FDYGKyjPfg5dj4REmjXzm6-2nm8.jpg" title="Insane week for LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In the past week, we've gotten...&lt;/p&gt; &lt;p&gt;- GPT 5.1&lt;/p&gt; &lt;p&gt;- Kimi K2 Thinking&lt;/p&gt; &lt;p&gt;- 12+ stealth endpoints across LMArena, Design Arena, and OpenRouter, with more coming in just the past day&lt;/p&gt; &lt;p&gt;- Speculation about an imminent GLM 5 drop on X&lt;/p&gt; &lt;p&gt;- A 4B model that beats several SOTA models on front-end fine-tuned using a new agentic reward system&lt;/p&gt; &lt;p&gt;It's a great time for new models and an even better time to be running a local setup. Looking forward to what the labs can cook up before the end of the year (looking at you Z.ai)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/b46881agly0g1.png?width=1892&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=16dfc05b6c2989ae933201911e8d326c473a3402"&gt;https://preview.redd.it/b46881agly0g1.png?width=1892&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=16dfc05b6c2989ae933201911e8d326c473a3402&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Interesting-Gur4782"&gt; /u/Interesting-Gur4782 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovsqs7/insane_week_for_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovsqs7/insane_week_for_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ovsqs7/insane_week_for_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-13T05:20:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1ovxksu</id>
    <title>Jan-v2-VL: 8B model for long-horizon tasks, improving Qwen3-VL-8Bâ€™s agentic capabilities almost 10x</title>
    <updated>2025-11-13T10:22:48+00:00</updated>
    <author>
      <name>/u/Delicious_Focus3465</name>
      <uri>https://old.reddit.com/user/Delicious_Focus3465</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovxksu/janv2vl_8b_model_for_longhorizon_tasks_improving/"&gt; &lt;img alt="Jan-v2-VL: 8B model for long-horizon tasks, improving Qwen3-VL-8Bâ€™s agentic capabilities almost 10x" src="https://external-preview.redd.it/bmthZnk4cjV4ejBnMYgdXr3Xr8K8l3LMKEIqfiXLStzaSkNnB6704_pmF3PX.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0626efa53bd219b2126a6e5fa2884ec700c482b3" title="Jan-v2-VL: 8B model for long-horizon tasks, improving Qwen3-VL-8Bâ€™s agentic capabilities almost 10x" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, this is Bach from the Jan team. Weâ€™re releasing Jan-v2-VL, an 8B visionâ€“language model aimed at long-horizon, multi-step tasks starting from browser use.&lt;/p&gt; &lt;p&gt;Jan-v2-VL-high executes 49 steps without failure on the Long-Horizon Execution benchmark, while the base model (Qwen3-VL-8B-Thinking) stops at 5 and other similar-scale VLMs stop between 1 and 2.&lt;/p&gt; &lt;p&gt;Across text and multimodal benchmarks, it matches or slightly improves on the base model, so you get higher long-horizon stability without giving up reasoning or vision quality.&lt;/p&gt; &lt;p&gt;We're releasing 3 variants:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Jan-v2-VL-low (efficiency-oriented)&lt;/li&gt; &lt;li&gt;Jan-v2-VL-med (balanced)&lt;/li&gt; &lt;li&gt;Jan-v2-VL-high (deeper reasoning and longer execution)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;How to run the model&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Download Jan-v2-VL from the Model Hub in Jan&lt;/li&gt; &lt;li&gt;Open the modelâ€™s settings and enable Tools and Vision&lt;/li&gt; &lt;li&gt;Enable BrowserUse MCP (or your preferred MCP setup for browser control)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can also run the model with vLLM or llama.cpp.&lt;/p&gt; &lt;p&gt;Recommended parameters&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;temperature: 1.0&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;top_p: 0.95&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;top_k: 20&lt;/code&gt;&lt;/li&gt; &lt;li&gt;repetition_penalty&lt;code&gt;: 1.0&lt;/code&gt;&lt;/li&gt; &lt;li&gt;presence_penalty&lt;code&gt;: 1.5&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/collections/janhq/jan-v2-vl"&gt;https://huggingface.co/collections/janhq/jan-v2-vl&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Jan app: &lt;a href="https://github.com/janhq/jan"&gt;https://github.com/janhq/jan&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We're also working on a browser extension to make model-driven browser automation faster and more reliable on top of this.&lt;/p&gt; &lt;p&gt;Credit to the Qwen team for the Qwen3-VL-8B-Thinking base model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Delicious_Focus3465"&gt; /u/Delicious_Focus3465 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/go4j38r5xz0g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovxksu/janv2vl_8b_model_for_longhorizon_tasks_improving/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ovxksu/janv2vl_8b_model_for_longhorizon_tasks_improving/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-13T10:22:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1ovs6ut</id>
    <title>llama.cpp and Qwen 2.5 running on bare metal Windows XP x64 without any compatibility layers</title>
    <updated>2025-11-13T04:50:33+00:00</updated>
    <author>
      <name>/u/PANCHO7532</name>
      <uri>https://old.reddit.com/user/PANCHO7532</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovs6ut/llamacpp_and_qwen_25_running_on_bare_metal/"&gt; &lt;img alt="llama.cpp and Qwen 2.5 running on bare metal Windows XP x64 without any compatibility layers" src="https://preview.redd.it/hg1xeqvuey0g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=223d93e64bfc8f846a673473dfbaaae88ede30a6" title="llama.cpp and Qwen 2.5 running on bare metal Windows XP x64 without any compatibility layers" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Slowness aside, surprisingly llama.cpp can be cross-compiled using MinGW and you can actually run it on Windows XP with only a few tweaks! I only have the x64 edition on this laptop so not really sure if it also works on x86&lt;/p&gt; &lt;p&gt;All tools are working without any problems, even the CLI and server tools (pictured), though i'm fairly sure that you can squeeze a token or two more by using the CLI instead of the server&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PANCHO7532"&gt; /u/PANCHO7532 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hg1xeqvuey0g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovs6ut/llamacpp_and_qwen_25_running_on_bare_metal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ovs6ut/llamacpp_and_qwen_25_running_on_bare_metal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-13T04:50:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1oth5pw</id>
    <title>AMA With Moonshot AI, The Open-source Frontier Lab Behind Kimi K2 Thinking Model</title>
    <updated>2025-11-10T15:44:10+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/"&gt; &lt;img alt="AMA With Moonshot AI, The Open-source Frontier Lab Behind Kimi K2 Thinking Model" src="https://b.thumbs.redditmedia.com/yz9_FpdLcHNiCkaH5fLEIoXS2f5u5twNBr7SQ9Go3AI.jpg" title="AMA With Moonshot AI, The Open-source Frontier Lab Behind Kimi K2 Thinking Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Today we are having &lt;strong&gt;Moonshot AI&lt;/strong&gt;, the research lab behind the &lt;strong&gt;Kimi&lt;/strong&gt; &lt;strong&gt;models&lt;/strong&gt;. Weâ€™re excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="/u/ComfortableAsk4494"&gt;u/ComfortableAsk4494&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="/u/zxytim"&gt;u/zxytim&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="/u/ppwwyyxx"&gt;u/ppwwyyxx&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 8 AM â€“ 11 AM PST, with the Kimi team continuing to follow up on questions over the next 24 hours.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5yg0ncsn7g0g1.png?width=3525&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5318680204ef7502ad349aec148147d9e3398f87"&gt;https://preview.redd.it/5yg0ncsn7g0g1.png?width=3525&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5318680204ef7502ad349aec148147d9e3398f87&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Thanks everyone for joining our AMA. The live part has ended and the Kimi team will be following up with more answers sporadically over the next 24 hours.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T15:44:10+00:00</published>
  </entry>
</feed>
