<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-26T01:12:03+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qmtjxi</id>
    <title>A Tool to Calculate If a LLM Will Fit Your GPU</title>
    <updated>2026-01-25T19:58:02+00:00</updated>
    <author>
      <name>/u/colt7r</name>
      <uri>https://old.reddit.com/user/colt7r</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! üëã&lt;/p&gt; &lt;p&gt;I‚Äôve been working on a small open-source project to take some of the guesswork out of running LLMs locally.&lt;/p&gt; &lt;p&gt;üëâ LocalInferenceCalculator ‚Äî a tool that tells you whether a given model will fit on a specific GPU based on the context length you want to use.&lt;/p&gt; &lt;p&gt;It currently knows about:&lt;/p&gt; &lt;p&gt;common LLM sizes (7B, 13B, 34B, 70B, etc.)&lt;/p&gt; &lt;p&gt;GPUs from consumer to data center&lt;/p&gt; &lt;p&gt;realistic memory overhead and KV cache calculations&lt;/p&gt; &lt;p&gt;You just input the context size (in tokens) and it evaluates each model √ó GPU pair and tells you:&lt;/p&gt; &lt;p&gt;‚úÖ Runs ‚ùå Doesn‚Äôt Run ‚ö†Ô∏è Runs at the Limit&lt;/p&gt; &lt;p&gt;All based on conservative VRAM estimates, not optimistic benchmarks. The goal is to help you plan and choose before wasting hours trying to load something that doesn‚Äôt fit. üòÑ&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/ygorml/local_inference_calculator"&gt;https://github.com/ygorml/local_inference_calculator&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What you‚Äôll find in the repo:&lt;/p&gt; &lt;p&gt;what‚Äôs already implemented&lt;/p&gt; &lt;p&gt;a clear roadmap&lt;/p&gt; &lt;p&gt;formulas for VRAM, KV cache, overhead&lt;/p&gt; &lt;p&gt;settings for future quantization, multi-GPU, CPU offload support&lt;/p&gt; &lt;p&gt;Future topics on the roadmap:&lt;/p&gt; &lt;p&gt;INT8 / INT4 quantization support&lt;/p&gt; &lt;p&gt;multi-GPU / offload strategies&lt;/p&gt; &lt;p&gt;CLI and maybe a web UI&lt;/p&gt; &lt;p&gt;Would love feedback from this community ‚Äî especially: ‚≠ê what models you care about ‚≠ê GPUs you use ‚≠ê features that would make this tool actually useful for your workflows&lt;/p&gt; &lt;p&gt;Curious to hear what you think! üöÄ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/colt7r"&gt; /u/colt7r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmtjxi/a_tool_to_calculate_if_a_llm_will_fit_your_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmtjxi/a_tool_to_calculate_if_a_llm_will_fit_your_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmtjxi/a_tool_to_calculate_if_a_llm_will_fit_your_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T19:58:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmzz8e</id>
    <title>Low-cost alternatives to Firecrawl?</title>
    <updated>2026-01-26T00:00:15+00:00</updated>
    <author>
      <name>/u/Dangerous_Ad1567</name>
      <uri>https://old.reddit.com/user/Dangerous_Ad1567</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been looking into Firecrawl for a CRM enrichment use case. I plan to scrape websites using Firecrawl, then use 4o-mini to extract information from the JSON. Looking at their pricing for 500,000 pages, we end up paying $0.0008 per page. &lt;/p&gt; &lt;p&gt;Are there any lower cost alternatives?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dangerous_Ad1567"&gt; /u/Dangerous_Ad1567 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmzz8e/lowcost_alternatives_to_firecrawl/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmzz8e/lowcost_alternatives_to_firecrawl/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmzz8e/lowcost_alternatives_to_firecrawl/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-26T00:00:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmp02r</id>
    <title>Built a 100% local AI agentic workflow that automatically tests chatbots using GPT-OSS 20B via llama.cpp + Agno workflow framework.</title>
    <updated>2026-01-25T17:15:06+00:00</updated>
    <author>
      <name>/u/switchdoor1</name>
      <uri>https://old.reddit.com/user/switchdoor1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmp02r/built_a_100_local_ai_agentic_workflow_that/"&gt; &lt;img alt="Built a 100% local AI agentic workflow that automatically tests chatbots using GPT-OSS 20B via llama.cpp + Agno workflow framework." src="https://preview.redd.it/9jgcnzd03jfg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8b78448eca7d34ac66b090e11ac5facec8f13ce8" title="Built a 100% local AI agentic workflow that automatically tests chatbots using GPT-OSS 20B via llama.cpp + Agno workflow framework." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! Ultimate goal with this project was to build an agentic testing framework that can automatically stress-test chatbots across multiple dimensions - off-topic handling, safety concerns, hallucination detection, system prompt extraction attempts, and more. The system uses AI agents to generate diverse test personalities and scenarios, then runs them against your chatbot and evaluates the responses. Set it up and you can start stacking up test data for continuous improvement.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/Prajwal-Nagaraj/Chatbot-Simulation-Workflow"&gt;https://github.com/Prajwal-Nagaraj/Chatbot-Simulation-Workflow&lt;/a&gt;&lt;/p&gt; &lt;h4&gt;Stack:&lt;/h4&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;LLM&lt;/strong&gt;: GPT-OSS 20B running via llama.cpp server (local, no API keys needed)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Workflow Engine&lt;/strong&gt;: Agno framework for orchestrating multi-agent workflows&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Backend&lt;/strong&gt;: FastAPI with async support for long-running test suites&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Frontend&lt;/strong&gt;: Modern but basic web ui using js and html&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Database&lt;/strong&gt;: SQLite&lt;/li&gt; &lt;/ul&gt; &lt;h4&gt;Features:&lt;/h4&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;AI-Powered Testing&lt;/strong&gt;: LLM generates realistic user personalities and test scenarios and also communicates with the chatbot endpoint&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LLM-as-a-Judge Evaluation&lt;/strong&gt;: Automated scoring of chatbot responses using LLM as a judge&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multiple Test Types&lt;/strong&gt;: off topic, safety, hallucination, system prompt testing, financial advice&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Flexible Configuration&lt;/strong&gt;: CLI, YAML configs, or web UI&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Async Execution&lt;/strong&gt;: Long test suites run in background&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Database Persistence&lt;/strong&gt;: All test sessions, personalities, scenarios, and results stored in a sqlite binary The workflow is pretty wild - it generates personalities, creates scenarios for each, runs conversations, and uses an LLM judge to evaluate everything automatically. You just point it at your Openai compatible chatbot endpoint and let it rip.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/switchdoor1"&gt; /u/switchdoor1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9jgcnzd03jfg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmp02r/built_a_100_local_ai_agentic_workflow_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmp02r/built_a_100_local_ai_agentic_workflow_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T17:15:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmo3do</id>
    <title>LM Studio - Why does my system RAM fill up and go OOM if the model says Full GPU Offload Possible?</title>
    <updated>2026-01-25T16:42:16+00:00</updated>
    <author>
      <name>/u/Nytse</name>
      <uri>https://old.reddit.com/user/Nytse</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Using Windows, RTX 3090 (24 GB VRAM) upgraded recently from a GTX 1080 (8 GB), 32 GB RAM&lt;/p&gt; &lt;p&gt;With Firefox open with many tabs I use ~18 GB RAM. GPU stays at ~3 GB.&lt;/p&gt; &lt;p&gt;Then, in LM Studio, loading the OpenAI GPT‚ÄëOSS 20B model shows ‚ÄúFull GPU Offload Possible‚Äù. After load, VRAM jumps to ~14 GB and system RAM climbs to 32 GB, then the program crashes with OOM.&lt;/p&gt; &lt;p&gt;I have Strict Guardrails enabled, swap is on.&lt;/p&gt; &lt;p&gt;How can I avoid high RAM usage and the OOM when loading this model while using by browser? How do I know how much allocated RAM the model will have?&lt;/p&gt; &lt;p&gt;I thought that the gguf file size is similar to the VRAM allocation and only like 1 GB RAM is reserved if the model fits in the GPU. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nytse"&gt; /u/Nytse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmo3do/lm_studio_why_does_my_system_ram_fill_up_and_go/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmo3do/lm_studio_why_does_my_system_ram_fill_up_and_go/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmo3do/lm_studio_why_does_my_system_ram_fill_up_and_go/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T16:42:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1qlzbhh</id>
    <title>[Release] Qwen3-TTS: Ultra-Low Latency (97ms), Voice Cloning &amp; OpenAI-Compatible API</title>
    <updated>2026-01-24T21:21:50+00:00</updated>
    <author>
      <name>/u/blackstoreonline</name>
      <uri>https://old.reddit.com/user/blackstoreonline</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlzbhh/release_qwen3tts_ultralow_latency_97ms_voice/"&gt; &lt;img alt="[Release] Qwen3-TTS: Ultra-Low Latency (97ms), Voice Cloning &amp;amp; OpenAI-Compatible API" src="https://b.thumbs.redditmedia.com/tY-PA8qRCq6_itenx-ibWJJ7urdsbE45bXySDC1FH4s.jpg" title="[Release] Qwen3-TTS: Ultra-Low Latency (97ms), Voice Cloning &amp;amp; OpenAI-Compatible API" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;The Qwen team just dropped &lt;strong&gt;Qwen3-TTS&lt;/strong&gt;, and it‚Äôs a significant step forward for local speech synthesis. If you‚Äôve been looking for a high-quality, open-source alternative to ElevenLabs or OpenAI‚Äôs TTS that you can actually run on your own hardware, this is it.&lt;/p&gt; &lt;p&gt;We‚Äôve put together a repository that provides an &lt;strong&gt;OpenAI-compatible FastAPI server&lt;/strong&gt;, meaning you can use it as a drop-in replacement for any app already using OpenAI‚Äôs TTS endpoints. Streaming support out of the box, plug and play with Open-Webui.&lt;/p&gt; &lt;h1&gt;Why this is a big deal:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Insane Speed:&lt;/strong&gt; It features a dual-track hybrid architecture that hits ~97ms end-to-end latency for streaming. It starts talking almost the instant you send the text.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Natural Voice Control:&lt;/strong&gt; You don't just send text; you can give it natural language instructions like &lt;em&gt;&amp;quot;Say this in an incredibly angry tone&amp;quot;&lt;/em&gt; or &lt;em&gt;&amp;quot;A shaky, nervous 17-year-old voice&amp;quot;&lt;/em&gt; and it actually follows through.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Easy Voice Cloning:&lt;/strong&gt; Give it a 3-second reference clip, and it can clone the timbre and emotion remarkably well.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;OpenAI Drop-in:&lt;/strong&gt; Works natively with the OpenAI Python client. Just change your &lt;code&gt;base_url&lt;/code&gt; to localhost.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multilingual:&lt;/strong&gt; Supports 10+ languages (ZH, EN, JP, KR, DE, FR, RU, PT, ES, IT).&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Getting Started (The Quick Way)&lt;/h1&gt; &lt;p&gt;If you have Docker and a GPU, you can get this running in seconds:&lt;/p&gt; &lt;p&gt;Bash&lt;/p&gt; &lt;pre&gt;&lt;code&gt;git clone https://github.com/groxaxo/Qwen3-TTS-Openai-Fastapi docker build -t qwen3-tts-api . docker run --gpus all -p 8880:8880 qwen3-tts-api &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Python Usage (OpenAI Style)&lt;/h1&gt; &lt;p&gt;Python&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from openai import OpenAI client = OpenAI(base_url=&amp;quot;http://localhost:8880/v1&amp;quot;, api_key=&amp;quot;not-needed&amp;quot;) response = client.audio.speech.create( model=&amp;quot;qwen3-tts&amp;quot;, voice=&amp;quot;Vivian&amp;quot;, # 9 premium voices included input=&amp;quot;This sounds way too human for a local model.&amp;quot;, speed=1.0 ) response.stream_to_file(&amp;quot;output.mp3&amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Technical Highlights&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Architecture:&lt;/strong&gt; It uses the new &lt;strong&gt;Qwen3-TTS-Tokenizer-12Hz&lt;/strong&gt; for acoustic compression. It skips the traditional &amp;quot;LM + DiT&amp;quot; bottleneck, which is why the latency is so low.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model Sizes:&lt;/strong&gt; Available in &lt;strong&gt;0.6B&lt;/strong&gt; (super fast/light) and &lt;strong&gt;1.7B&lt;/strong&gt; (high fidelity) versions.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;VRAM Friendly:&lt;/strong&gt; Supports FlashAttention 2 to keep memory usage down.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Links to dive deeper:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/collections/Qwen/qwen3-tts"&gt;ü§ó Hugging Face Collection&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://arxiv.org/abs/2601.15621"&gt;üìÑ Research Paper on arXiv&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/QwenLM/Qwen3-TTS"&gt;üíª Github Repo&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I‚Äôm really curious to see how the community integrates this into local LLM agents. The 97ms latency makes real-time voice conversation feel actually... real.&lt;/p&gt; &lt;p&gt;Let me know if you run into any issues setting it up!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/sa9itpxw6dfg1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7fe58c44a2d0b9d03a5bf099024f18752d48949d"&gt;https://preview.redd.it/sa9itpxw6dfg1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7fe58c44a2d0b9d03a5bf099024f18752d48949d&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/blackstoreonline"&gt; /u/blackstoreonline &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlzbhh/release_qwen3tts_ultralow_latency_97ms_voice/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlzbhh/release_qwen3tts_ultralow_latency_97ms_voice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qlzbhh/release_qwen3tts_ultralow_latency_97ms_voice/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T21:21:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmutct</id>
    <title>anyone running local llm on iphone for meeting summaries? heres what im using</title>
    <updated>2026-01-25T20:43:51+00:00</updated>
    <author>
      <name>/u/xerdink</name>
      <uri>https://old.reddit.com/user/xerdink</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;been messing around with local inference on ios for a meeting notes app im building. wanted to share what works and what doesnt&lt;/p&gt; &lt;p&gt;setup: - whisper for transcription (the small model runs surprisingly well on neural engine) - tried a few different llms for summaries&lt;/p&gt; &lt;p&gt;what i learned: - quantized models are basically required, anything bigger than 2-3B params is too slow - coreml conversion is a pain but worth it for speed - battery drain is real lol, gotta be careful with inference frequency&lt;/p&gt; &lt;p&gt;the whole thing runs offline which was the main goal. didnt want any cloud nonsense after reading about all those &lt;a href="http://otter.ai"&gt;otter.ai&lt;/a&gt; privacy issues&lt;/p&gt; &lt;p&gt;curious what models you guys are using for on device stuff? esp interested in anything good for summarization thats small enough for mobile&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xerdink"&gt; /u/xerdink &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmutct/anyone_running_local_llm_on_iphone_for_meeting/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmutct/anyone_running_local_llm_on_iphone_for_meeting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmutct/anyone_running_local_llm_on_iphone_for_meeting/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T20:43:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmxhc5</id>
    <title>What set up do I need to query a GitHub repository?</title>
    <updated>2026-01-25T22:21:59+00:00</updated>
    <author>
      <name>/u/MrMrsPotts</name>
      <uri>https://old.reddit.com/user/MrMrsPotts</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I can zip up and upload an entire GitHub repository to chatgpt. I can then query the repository, which I have found massively useful. How can you do something similar with a local model?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrMrsPotts"&gt; /u/MrMrsPotts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmxhc5/what_set_up_do_i_need_to_query_a_github_repository/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmxhc5/what_set_up_do_i_need_to_query_a_github_repository/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmxhc5/what_set_up_do_i_need_to_query_a_github_repository/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T22:21:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmlyhn</id>
    <title>GLM-4.7-flash on RTX 6000 pro</title>
    <updated>2026-01-25T15:24:37+00:00</updated>
    <author>
      <name>/u/gittb</name>
      <uri>https://old.reddit.com/user/gittb</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Update; Spent the day gridding options, well, under guidance and structure, our good friend claude manned the helm..&lt;/p&gt; &lt;p&gt;Here is the results:&lt;/p&gt; &lt;h1&gt;GLM-4.7-Flash on RTX PRO 6000 Blackwell - Docker Configs&lt;/h1&gt; &lt;p&gt;Benchmarked GLM-4.7-Flash (MoE) on 2x RTX PRO 6000 Blackwell. Here are the best configs.&lt;/p&gt; &lt;h2&gt;Results&lt;/h2&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Config&lt;/th&gt; &lt;th&gt;Throughput&lt;/th&gt; &lt;th&gt;Memory&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;FP8 Single GPU&lt;/td&gt; &lt;td&gt;5825 tok/s&lt;/td&gt; &lt;td&gt;29 GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;FP8 Dual GPU (TP=2+EP)&lt;/td&gt; &lt;td&gt;7029 tok/s&lt;/td&gt; &lt;td&gt;~15 GB/GPU&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h2&gt;Single GPU - FP8&lt;/h2&gt; &lt;p&gt;```yaml&lt;/p&gt; &lt;h1&gt;compose.vllm-fp8-single.yaml&lt;/h1&gt; &lt;p&gt;services: vllm: image: vllm-glm47-flash:local # see Custom Container section below ports: - &amp;quot;8000:8000&amp;quot; shm_size: &amp;quot;16g&amp;quot; ipc: host environment: - VLLM_USE_V1=1 - VLLM_ATTENTION_BACKEND=TRITON_MLA volumes: - /path/to/models:/models command: - --model - /models/GLM-4.7-Flash-FP8 - --served-model-name - glm-4.7-flash - --gpu-memory-utilization - &amp;quot;0.95&amp;quot; - --max-model-len - &amp;quot;131072&amp;quot; - --trust-remote-code deploy: resources: reservations: devices: - driver: nvidia count: 1 capabilities: [gpu] ```&lt;/p&gt; &lt;h2&gt;Dual GPU - FP8 with TP + Expert Parallel&lt;/h2&gt; &lt;p&gt;```yaml&lt;/p&gt; &lt;h1&gt;compose.vllm-fp8-tp2-ep.yaml&lt;/h1&gt; &lt;p&gt;services: vllm: image: vllm-glm47-flash:local # see Custom Container section below ports: - &amp;quot;8000:8000&amp;quot; shm_size: &amp;quot;32g&amp;quot; ipc: host environment: - VLLM_USE_V1=1 - VLLM_ATTENTION_BACKEND=TRITON_MLA volumes: - /path/to/models:/models command: - --model - /models/GLM-4.7-Flash-FP8 - --served-model-name - glm-4.7-flash - --gpu-memory-utilization - &amp;quot;0.95&amp;quot; - --max-model-len - &amp;quot;131072&amp;quot; - --tensor-parallel-size - &amp;quot;2&amp;quot; - --enable-expert-parallel - --trust-remote-code deploy: resources: reservations: devices: - driver: nvidia count: all capabilities: [gpu] ```&lt;/p&gt; &lt;h2&gt;Custom Container (Required)&lt;/h2&gt; &lt;p&gt;The official vLLM images don't support GLM-4.7-Flash yet because it uses the &lt;code&gt;glm4_moe_lite&lt;/code&gt; architecture which requires transformers from source. Build a custom image:&lt;/p&gt; &lt;p&gt;```dockerfile&lt;/p&gt; &lt;h1&gt;Dockerfile&lt;/h1&gt; &lt;p&gt;FROM vllm/vllm-openai:nightly&lt;/p&gt; &lt;h1&gt;Install transformers from source for glm4_moe_lite architecture support&lt;/h1&gt; &lt;p&gt;RUN apt-get update &amp;amp;&amp;amp; apt-get install -y --no-install-recommends git \ &amp;amp;&amp;amp; rm -rf /var/lib/apt/lists/* \ &amp;amp;&amp;amp; pip install --no-cache-dir -U git+&lt;a href="https://github.com/huggingface/transformers.git"&gt;https://github.com/huggingface/transformers.git&lt;/a&gt; ```&lt;/p&gt; &lt;p&gt;Build it: &lt;code&gt;bash docker build -t vllm-glm47-flash:local . &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Then use &lt;code&gt;image: vllm-glm47-flash:local&lt;/code&gt; in the compose files above.&lt;/p&gt; &lt;h2&gt;Notes&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Model: &lt;a href="https://huggingface.co/neuralmagic/GLM-4.7-Flash-FP8-dynamic"&gt;GLM-4.7-Flash-FP8&lt;/a&gt; (pre-quantized)&lt;/li&gt; &lt;li&gt;Custom container required - official images don't have &lt;code&gt;glm4_moe_lite&lt;/code&gt; support yet&lt;/li&gt; &lt;li&gt;vLLM nightly base required for MLA attention fix (PR #32614)&lt;/li&gt; &lt;li&gt;Expert Parallel distributes MoE experts across GPUs - slight edge over plain TP=2&lt;/li&gt; &lt;li&gt;SGLang doesn't work on Blackwell yet (attention backend issues)&lt;/li&gt; &lt;li&gt;Pipeline parallel (PP=2) is actually slower than single GPU - avoid it&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Old post:&lt;/p&gt; &lt;p&gt;Hello, I‚Äôm getting horrible throughput considering the models size with vLLM.&lt;/p&gt; &lt;p&gt;Currently with 2x cards and DP 2 @ FP16 I‚Äôm getting around 370 gen TPS with 10x requests.&lt;/p&gt; &lt;p&gt;Anyone have a fix or a ‚Äúworking‚Äù config for 1 or two cards?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gittb"&gt; /u/gittb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmlyhn/glm47flash_on_rtx_6000_pro/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmlyhn/glm47flash_on_rtx_6000_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmlyhn/glm47flash_on_rtx_6000_pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T15:24:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmhvuz</id>
    <title>Quantifying Hallucinations: By calculating a multi-dimensional 'Trust Score' for LLM outputs.</title>
    <updated>2026-01-25T12:30:24+00:00</updated>
    <author>
      <name>/u/Charming_Group_2950</name>
      <uri>https://old.reddit.com/user/Charming_Group_2950</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;The problem:&lt;/strong&gt;&lt;br /&gt; You build a RAG system. It gives an answer. It sounds right.&lt;br /&gt; But is it actually grounded in your data, or just hallucinating with confidence?&lt;br /&gt; A single &amp;quot;correctness&amp;quot; or &amp;quot;relevance&amp;quot; score doesn‚Äôt cut it anymore, especially in enterprise, regulated, or governance-heavy environments. We need to know why it failed. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;My solution:&lt;/strong&gt;&lt;br /&gt; Introducing &lt;strong&gt;TrustifAI&lt;/strong&gt; ‚Äì a framework designed to quantify, explain, and debug the trustworthiness of AI responses. &lt;/p&gt; &lt;p&gt;Instead of pass/fail, it computes a multi-dimensional Trust Score using signals like:&lt;br /&gt; * Evidence Coverage: Is the answer actually supported by retrieved documents?&lt;br /&gt; * Epistemic Consistency: Does the model stay stable across repeated generations?&lt;br /&gt; * Semantic Drift: Did the response drift away from the given context?&lt;br /&gt; * Source Diversity: Is the answer overly dependent on a single document?&lt;br /&gt; * Generation Confidence: Uses token-level log probabilities at inference time to quantify how confident the model was while generating the answer (not after judging it).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why this matters:&lt;/strong&gt;&lt;br /&gt; TrustifAI doesn‚Äôt just give you a number - it gives you traceability.&lt;br /&gt; It builds &lt;strong&gt;Reasoning Graphs (DAGs)&lt;/strong&gt; and &lt;strong&gt;Mermaid visualizations&lt;/strong&gt; that show why a response was flagged as reliable or suspicious.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How is this different from LLM Evaluation frameworks:&lt;/strong&gt;&lt;br /&gt; All popular Eval frameworks measure how good your RAG system is, but&lt;br /&gt; TrustifAI tells you why you should (or shouldn‚Äôt) trust a specific answer - with explainability in mind.&lt;/p&gt; &lt;p&gt;Since the library is in its early stages, I‚Äôd genuinely love community feedback.&lt;br /&gt; ‚≠ê the repo if it helps üòÑ&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Get started:&lt;/strong&gt; &lt;code&gt;pip install trustifai&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Github link:&lt;/strong&gt; &lt;a href="https://github.com/Aaryanverma/trustifai"&gt;https://github.com/Aaryanverma/trustifai&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charming_Group_2950"&gt; /u/Charming_Group_2950 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qmhvuz"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmhvuz/quantifying_hallucinations_by_calculating_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmhvuz/quantifying_hallucinations_by_calculating_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T12:30:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmttek</id>
    <title>Built a Clickable 3D Solar System Explorer in 15 Minutes with MiniMax Agent</title>
    <updated>2026-01-25T20:07:21+00:00</updated>
    <author>
      <name>/u/Grand_Excuse1776</name>
      <uri>https://old.reddit.com/user/Grand_Excuse1776</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been playing around with MiniMax Agent a lot lately, and I just built this interactive Solar System Explorer web app purely from prompts‚Äîno manual coding or hosting needed. Live link: &lt;a href="https://fdgzvcv7zkki.space.minimax.io/"&gt;https://fdgzvcv7zkki.space.minimax.io/&lt;/a&gt; What I actually use MiniMax Agent for the most (why I keep coming back): Honestly, I use it a ton for quick personal productivity and creative side stuff. Like: ‚Ä¢ Automating repetitive reports/tasks (e.g., turning notes into formatted summaries or site inspection outlines if I‚Äôm brainstorming workflows). ‚Ä¢ Building mini web tools/dashboards fast like this explorer for learning/teaching, or simple calculators/trackers. ‚Ä¢ Prototyping business ideas without dev time (e.g., mocked-up landing pages, forms, or even basic apps I can share/test). ‚Ä¢ Fun educational/exploratory things like simulations, games, or visuals from prompts (their multimodal stuff shines here). It‚Äôs agentic (plans multi-step, codes, deploys), cheaper/faster than some alternatives for what I need, and the instant deployment is addictive. No more ‚ÄúI‚Äôll build this later‚Äù excuses. This solar system one is just a fun example to show how accessible it is for non-coders too. If you‚Äôre into AI agents or no-code-ish building, definitely worth trying with new users get starter credits, and shares like this earn more #minimaxagent #minimax&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Grand_Excuse1776"&gt; /u/Grand_Excuse1776 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmttek/built_a_clickable_3d_solar_system_explorer_in_15/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmttek/built_a_clickable_3d_solar_system_explorer_in_15/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmttek/built_a_clickable_3d_solar_system_explorer_in_15/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T20:07:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmdf2a</id>
    <title>Has anyone got GLM 4.7 flash to not be shit?</title>
    <updated>2026-01-25T08:14:08+00:00</updated>
    <author>
      <name>/u/synth_mania</name>
      <uri>https://old.reddit.com/user/synth_mania</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Real talk. I feel like everyday I'm downloading a new quant and trying it out and not once have I got it to consistently work without looping.&lt;/p&gt; &lt;p&gt;I've tried with and without the suggested settings from unsloth, &lt;a href="http://z.ai"&gt;z.ai&lt;/a&gt;, and others, to no avail.&lt;/p&gt; &lt;p&gt;Additionally, this has to be the slowest inference I've ever seen from a 30B A3B model. In all fairness, my only point of reference is Qwen3 Coder, but compared to that at least, the token generation speed feels positively lethargic.&lt;/p&gt; &lt;p&gt;If anybody has any tips, please let me know because I feel like I'm going in circles here. I don't think I've ever seen a modern release that had this many issues right off the bat, with no apparent improvement after a few supposed fixes.&lt;/p&gt; &lt;p&gt;It's really unfortunate because I can see the potential this model has. The chain of thought in particular seems uniquely coherent.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/synth_mania"&gt; /u/synth_mania &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmdf2a/has_anyone_got_glm_47_flash_to_not_be_shit/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmdf2a/has_anyone_got_glm_47_flash_to_not_be_shit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmdf2a/has_anyone_got_glm_47_flash_to_not_be_shit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T08:14:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmv41m</id>
    <title>Made an app for auto-captioning videos with Parakeet and rendering them locally in-browser</title>
    <updated>2026-01-25T20:54:43+00:00</updated>
    <author>
      <name>/u/zoomertechlead</name>
      <uri>https://old.reddit.com/user/zoomertechlead</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmv41m/made_an_app_for_autocaptioning_videos_with/"&gt; &lt;img alt="Made an app for auto-captioning videos with Parakeet and rendering them locally in-browser" src="https://external-preview.redd.it/NDd1ZXBuN3o0a2ZnMc5yujhoMa0OCB6yhbpGA5VU17AxP5yz80rX57T93wrK.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7486233a1cfb85de29f70dc56adcf11fcd531fcc" title="Made an app for auto-captioning videos with Parakeet and rendering them locally in-browser" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I noticed there weren't really any good free options for this since CapCut put their autocaption feature behind a paywall so I vibecoded this in a few days: &lt;a href="https://kinoscribe.com/"&gt;https://kinoscribe.com/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It uses SileroVAD to chunk the audio and for transcription you can pick between Parakeet v2 and v3.&lt;br /&gt; Both run entirely locally in browser. No need to make an account or upload your content to a server.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zoomertechlead"&gt; /u/zoomertechlead &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/astf137z4kfg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmv41m/made_an_app_for_autocaptioning_videos_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmv41m/made_an_app_for_autocaptioning_videos_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T20:54:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmrjlh</id>
    <title>Do you power off your LLM/AI/SV PC when not using it to save on electricity, or keep it on 24/7? MultiGPU adds a lot of power!</title>
    <updated>2026-01-25T18:45:59+00:00</updated>
    <author>
      <name>/u/panchovix</name>
      <uri>https://old.reddit.com/user/panchovix</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi there guys, hoping you're fine.&lt;/p&gt; &lt;p&gt;Wondering here, as electricity is about 0.28USD per kWh on Chile, so I'm kinda forced to have it off most of the time.&lt;/p&gt; &lt;p&gt;My idle power is about 270W with multiple GPUs (7) and no PCIe switches (5090x3,4090x2,A40x1,A6000x1, 9900X), but with a Gen 5 100 lanes switch and a Gen 4 96 lanes switch, I idle at about 370W.&lt;/p&gt; &lt;p&gt;At load it goes it ranges from 900W to 2500W, depending of the backend.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/panchovix"&gt; /u/panchovix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmrjlh/do_you_power_off_your_llmaisv_pc_when_not_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmrjlh/do_you_power_off_your_llmaisv_pc_when_not_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmrjlh/do_you_power_off_your_llmaisv_pc_when_not_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T18:45:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmrrr4</id>
    <title>ClaraVerse | Local AI workspace (4 months ago) -&gt; Your feedback -&gt; Back with improvements.</title>
    <updated>2026-01-25T18:54:07+00:00</updated>
    <author>
      <name>/u/BadBoy17Ge</name>
      <uri>https://old.reddit.com/user/BadBoy17Ge</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmrrr4/claraverse_local_ai_workspace_4_months_ago_your/"&gt; &lt;img alt="ClaraVerse | Local AI workspace (4 months ago) -&amp;gt; Your feedback -&amp;gt; Back with improvements." src="https://a.thumbs.redditmedia.com/7F-eZ7FXWPk0GBmRwW4IWCGKcKvAemtDakgsLWI_f-8.jpg" title="ClaraVerse | Local AI workspace (4 months ago) -&amp;gt; Your feedback -&amp;gt; Back with improvements." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;We built an AI workspace that actually gets things done locally (not just another chatbot or AI slope)&lt;/h1&gt; &lt;p&gt;I've been grinding on ClaraVerse for the past few months, and we just dropped a major update. If you're tired of AI tools that just... talk at you, this might be your vibe.&lt;/p&gt; &lt;h1&gt;The TL;DR&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Run it anywhere&lt;/strong&gt;: CLI tool that works on your laptop, VPS, cloud, whatever. No platform lock-in BS.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;50+ integrations&lt;/strong&gt;: Gmail, Sheets, Discord, Slack, you name it. Want more? Just ask.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Actual automation&lt;/strong&gt;: Build agents that DO things, not just answer questions.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Chat-first workflow builder&lt;/strong&gt;: Like n8n/Zapier but for AI. Chat your way through creating workflows ask, create, iterate.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Everything becomes an API&lt;/strong&gt;: Seriously, every workflow you build = instant API endpoint or schedule it daily, hourly your choice.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;One-liner:&lt;/strong&gt; It's an all-in-one platform (chat, image gen, agents, docs, search). Every tool is part of the package.&lt;/p&gt; &lt;p&gt;What's actually new (beyond UI polish)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Built-in tools that agents and chats need:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;PPT, PDF, XLSX readers and creators&lt;/li&gt; &lt;li&gt;Isolated code execution with dependency management&lt;/li&gt; &lt;li&gt;Interactive chat so local LLMs can ask clarifying questions mid-prompt&lt;/li&gt; &lt;li&gt;Search, scrape, image search, API tools, and memory all default&lt;/li&gt; &lt;li&gt;Tool router if you have too many tools&lt;/li&gt; &lt;li&gt;Memories that can remember and forget based on your usage&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;50+ integrations ready to go:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Gmail, Sheets, Discord, Slack, and more&lt;/li&gt; &lt;li&gt;Build agents that trigger actual actions, not just suggestions&lt;/li&gt; &lt;li&gt;Schedule workflows and forget about them&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;For n8n lovers who hate boilerplate:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Auto-generate workflows from prompts&lt;/li&gt; &lt;li&gt;Chain multiple AI models together&lt;/li&gt; &lt;li&gt;Structured outputs, multi-tool agents, the works&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Better chat UX:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Interactive prompts that ask clarifying questions&lt;/li&gt; &lt;li&gt;Generate images, PDFs, slides, charts in-chat&lt;/li&gt; &lt;li&gt;All integrations work in both chat AND workflows&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Admin and Model Manger:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Manage models and provider in one place&lt;/li&gt; &lt;li&gt;Assign models based on their abilities (tools, text, code, vision, image)&lt;/li&gt; &lt;li&gt;Create alias, check usage and so on with multiple user in same instance&lt;/li&gt; &lt;li&gt;Simple UI works on phone responsive as hell&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Try it and let us know&lt;/h1&gt; &lt;ul&gt; &lt;li&gt; GitHub: &lt;a href="https://github.com/claraverse-space/ClaraVerse"&gt;github.com/claraverse-space/ClaraVerse&lt;/a&gt; &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We're open source and privacy-first (chat and data stored in browser or DB, even when self-hosted - user's choice). &lt;/p&gt; &lt;p&gt;I use this myself every day. Honestly, I've seen worse tools raise fund and then lock everything behind subscriptions. This community helped build this with feedback, so it's staying free and open-source.&lt;/p&gt; &lt;p&gt;Happy to answer questions, take feature requests, or hear about how it crashes on your machine so we can fix and improve. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BadBoy17Ge"&gt; /u/BadBoy17Ge &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qmrrr4"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmrrr4/claraverse_local_ai_workspace_4_months_ago_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmrrr4/claraverse_local_ai_workspace_4_months_ago_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T18:54:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1qn0dl8</id>
    <title>Backporting FP8 to the RTX 3090 (No H100 Required)</title>
    <updated>2026-01-26T00:16:50+00:00</updated>
    <author>
      <name>/u/one_does_not_just</name>
      <uri>https://old.reddit.com/user/one_does_not_just</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Worked on this project over the weekend, was curious if I can get fp8 compute going without decoding to fp16 in global memory or storing fp16 intermediates. Sacrificed some compute perf, but did achieve the intended VRAM savings. I did add a torch extension, if you wanna try it in your workflow.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/one_does_not_just"&gt; /u/one_does_not_just &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://amohan.dev/blog/2026/fp8-as-storage-imma-ampere/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qn0dl8/backporting_fp8_to_the_rtx_3090_no_h100_required/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qn0dl8/backporting_fp8_to_the_rtx_3090_no_h100_required/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-26T00:16:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmxq3b</id>
    <title>Specializing Large Language Models</title>
    <updated>2026-01-25T22:31:05+00:00</updated>
    <author>
      <name>/u/Available-Craft-5795</name>
      <uri>https://old.reddit.com/user/Available-Craft-5795</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am currently working on &lt;a href="https://huggingface.co/CompactAI"&gt;https://huggingface.co/CompactAI&lt;/a&gt; by taking large models and specializing them to a task, this is all automated by a script so results may vary. Is this something more people should be doing?&lt;br /&gt; I am welcome to any model suggestions (MOE Supported)!&lt;/p&gt; &lt;p&gt;I cant explain the benchmarks on how they appear to get smarter in benchmarks, the temp is forced to 0.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Available-Craft-5795"&gt; /u/Available-Craft-5795 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmxq3b/specializing_large_language_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmxq3b/specializing_large_language_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmxq3b/specializing_large_language_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T22:31:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmxexe</id>
    <title>How are people actually learning/building real-world AI agents (money, legal, business), not demos?</title>
    <updated>2026-01-25T22:19:32+00:00</updated>
    <author>
      <name>/u/Altruistic-Law-4750</name>
      <uri>https://old.reddit.com/user/Altruistic-Law-4750</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;#x200b;&lt;/p&gt; &lt;p&gt;I‚Äôm trying to understand how people are actually learning and building *real-world* AI agents ‚Äî the kind that integrate into businesses, touch money, workflows, contracts, and carry real responsibility.&lt;/p&gt; &lt;p&gt;Not chat demos, not toy copilots, not ‚ÄúLLM + tools‚Äù weekend projects.&lt;/p&gt; &lt;p&gt;What I‚Äôm struggling with:&lt;/p&gt; &lt;p&gt;- There are almost no reference repos for serious agents&lt;/p&gt; &lt;p&gt;- Most content is either shallow, fragmented, or stops at orchestration&lt;/p&gt; &lt;p&gt;- Blogs talk about ‚Äúagents‚Äù but avoid accountability, rollback, audit, or failure&lt;/p&gt; &lt;p&gt;- Anything real seems locked behind IP, internal systems, or closed companies&lt;/p&gt; &lt;p&gt;I get *why* ‚Äî this stuff is risky and not something people open-source casually.&lt;/p&gt; &lt;p&gt;But clearly people are building these systems.&lt;/p&gt; &lt;p&gt;So I‚Äôm trying to understand from those closer to the work:&lt;/p&gt; &lt;p&gt;- How did you personally learn this layer?&lt;/p&gt; &lt;p&gt;- What should someone study first: infra, systems design, distributed systems, product, legal constraints?&lt;/p&gt; &lt;p&gt;- Are most teams just building traditional software systems with LLMs embedded (and ‚Äúagent‚Äù is mostly a label)?&lt;/p&gt; &lt;p&gt;- How are responsibility, human-in-the-loop, and failure handled in production?&lt;/p&gt; &lt;p&gt;- Where do serious discussions about this actually happen?&lt;/p&gt; &lt;p&gt;I‚Äôm not looking for shortcuts or magic repos.&lt;/p&gt; &lt;p&gt;I‚Äôm trying to build the correct **mental model and learning path** for production-grade systems, not demos.&lt;/p&gt; &lt;p&gt;If you‚Äôve worked on this, studied it deeply, or know where real practitioners share knowledge ‚Äî I‚Äôd really appreciate guidance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Altruistic-Law-4750"&gt; /u/Altruistic-Law-4750 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmxexe/how_are_people_actually_learningbuilding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmxexe/how_are_people_actually_learningbuilding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmxexe/how_are_people_actually_learningbuilding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T22:19:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmljeb</id>
    <title>What are the best open source coding ideas you can share?</title>
    <updated>2026-01-25T15:08:39+00:00</updated>
    <author>
      <name>/u/secopsml</name>
      <uri>https://old.reddit.com/user/secopsml</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmljeb/what_are_the_best_open_source_coding_ideas_you/"&gt; &lt;img alt="What are the best open source coding ideas you can share?" src="https://preview.redd.it/zcf9q42bgifg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e8d5b9a2f9d2a8bbb940fa1ae8f1c616ca45968f" title="What are the best open source coding ideas you can share?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to build a place for my friends so they can try and learn ai assisted engineering/vibe coding. Some of them are 50 yrs experienced devs familiar with enterprise standards, some 16 yrs old vibe coders that want to build their first scripts.&lt;/p&gt; &lt;p&gt;How would you structure guide for newcomers? Any favourite tools I should add/replace?&lt;/p&gt; &lt;p&gt;What would you choose for 24h hackathon and what is more suitable for weeks/months project?&lt;/p&gt; &lt;p&gt;repo: &lt;a href="https://github.com/dontriskit/awesome-ai-software-engineering"&gt;https://github.com/dontriskit/awesome-ai-software-engineering&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/secopsml"&gt; /u/secopsml &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zcf9q42bgifg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmljeb/what_are_the_best_open_source_coding_ideas_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmljeb/what_are_the_best_open_source_coding_ideas_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T15:08:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmsk9w</id>
    <title>LLM Reasoning Efficiency - lineage-bench accuracy vs generated tokens</title>
    <updated>2026-01-25T19:22:14+00:00</updated>
    <author>
      <name>/u/fairydreaming</name>
      <uri>https://old.reddit.com/user/fairydreaming</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmsk9w/llm_reasoning_efficiency_lineagebench_accuracy_vs/"&gt; &lt;img alt="LLM Reasoning Efficiency - lineage-bench accuracy vs generated tokens" src="https://preview.redd.it/gai51kz2pjfg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f44d218e6b2a5dc8982ffb434c4c01e0cf195277" title="LLM Reasoning Efficiency - lineage-bench accuracy vs generated tokens" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Generated from lineage-128 and lineage-192 &lt;a href="http://github.com/fairydreaming/lineage-bench"&gt;lineage-bench&lt;/a&gt; &lt;a href="https://github.com/fairydreaming/lineage-bench-results/tree/main/lineage-8_64_128_192"&gt;benchmark results&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Sorry for overlapping labels.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fairydreaming"&gt; /u/fairydreaming &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gai51kz2pjfg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmsk9w/llm_reasoning_efficiency_lineagebench_accuracy_vs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmsk9w/llm_reasoning_efficiency_lineagebench_accuracy_vs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T19:22:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmu1a1</id>
    <title>GLM-4.7-Flash context slowdown</title>
    <updated>2026-01-25T20:15:06+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmu1a1/glm47flash_context_slowdown/"&gt; &lt;img alt="GLM-4.7-Flash context slowdown" src="https://b.thumbs.redditmedia.com/1XaF-XYj4di3wcMyTbjLQ43nM77xN3jdiZdZndwBcDM.jpg" title="GLM-4.7-Flash context slowdown" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;UPDATE &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1qmvny5/glm47flash_is_even_faster_now/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1qmvny5/glm47flash_is_even_faster_now/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;to check on your setup, run:&lt;br /&gt; (you can use higher -p and -n and modify -d to your needs)&lt;/p&gt; &lt;pre&gt;&lt;code&gt;jacek@AI-SuperComputer:~$ CUDA_VISIBLE_DEVICES=0,1,2 llama-bench -m /mnt/models1/GLM/GLM-4.7-Flash-Q8_0.gguf -d 0,5000,10000,15000,20000,25000,30000,35000,40000,45000,50000 -p 200 -n 200 -fa 1 ggml_cuda_init: found 3 CUDA devices: Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes Device 1: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes Device 2: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes | model | size | params | backend | ngl | fa | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | -: | --------------: | -------------------: | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | pp200 | 1985.41 ¬± 11.02 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | tg200 | 95.65 ¬± 0.44 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | pp200 @ d5000 | 1392.15 ¬± 12.63 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | tg200 @ d5000 | 81.83 ¬± 0.67 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | pp200 @ d10000 | 1027.56 ¬± 13.50 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | tg200 @ d10000 | 72.60 ¬± 0.07 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | pp200 @ d15000 | 824.05 ¬± 8.08 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | tg200 @ d15000 | 64.24 ¬± 0.46 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | pp200 @ d20000 | 637.06 ¬± 79.79 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | tg200 @ d20000 | 58.46 ¬± 0.14 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | pp200 @ d25000 | 596.69 ¬± 11.13 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | tg200 @ d25000 | 53.31 ¬± 0.18 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | pp200 @ d30000 | 518.71 ¬± 5.25 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | tg200 @ d30000 | 49.41 ¬± 0.02 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | pp200 @ d35000 | 465.65 ¬± 2.69 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | tg200 @ d35000 | 45.80 ¬± 0.04 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | pp200 @ d40000 | 417.97 ¬± 1.67 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | tg200 @ d40000 | 42.65 ¬± 0.05 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | pp200 @ d45000 | 385.33 ¬± 1.80 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | tg200 @ d45000 | 40.01 ¬± 0.03 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | pp200 @ d50000 | 350.91 ¬± 2.17 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | tg200 @ d50000 | 37.63 ¬± 0.02 | build: 8f91ca54e (7822) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;real usage of opencode (with 200000 context):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;slot launch_slot_: id 0 | task 2495 | processing task, is_child = 0 slot update_slots: id 0 | task 2495 | new prompt, n_ctx_slot = 200192, n_keep = 0, task.n_tokens = 66276 slot update_slots: id 0 | task 2495 | n_tokens = 63140, memory_seq_rm [63140, end) slot update_slots: id 0 | task 2495 | prompt processing progress, n_tokens = 65188, batch.n_tokens = 2048, progress = 0.983584 slot update_slots: id 0 | task 2495 | n_tokens = 65188, memory_seq_rm [65188, end) slot update_slots: id 0 | task 2495 | prompt processing progress, n_tokens = 66276, batch.n_tokens = 1088, progress = 1.000000 slot update_slots: id 0 | task 2495 | prompt done, n_tokens = 66276, batch.n_tokens = 1088 slot init_sampler: id 0 | task 2495 | init sampler, took 8.09 ms, tokens: text = 66276, total = 66276 slot print_timing: id 0 | task 2495 | prompt eval time = 10238.44 ms / 3136 tokens ( 3.26 ms per token, 306.30 tokens per second) eval time = 11570.90 ms / 355 tokens ( 32.59 ms per token, 30.68 tokens per second) total time = 21809.34 ms / 3491 tokens &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;n_tokens = 66276, 306.30t/s, 30.68t/s&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qmu1a1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmu1a1/glm47flash_context_slowdown/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmu1a1/glm47flash_context_slowdown/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T20:15:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmir5d</id>
    <title>What do you actually want from a private AI chat on your phone?</title>
    <updated>2026-01-25T13:12:00+00:00</updated>
    <author>
      <name>/u/AppDeveloperAsdf</name>
      <uri>https://old.reddit.com/user/AppDeveloperAsdf</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmir5d/what_do_you_actually_want_from_a_private_ai_chat/"&gt; &lt;img alt="What do you actually want from a private AI chat on your phone?" src="https://external-preview.redd.it/b2k1d3JkaHV0aGZnMbzKSbNJeiRdJL3Vv6uz8BgUY-ES1g_l6yTqUuzYy_d7.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8f45f222f3ee31c2716f286b9cf0998d79f80e6f" title="What do you actually want from a private AI chat on your phone?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey friends. We are building zerotap - an Android app where AI can control your phone like a human (taps, scrolls, reads screen). It supports Ollama, proxies like OpenRouter and Straico and models directly such as OpenAI, Claude, Gemini and DeepSeek.&lt;/p&gt; &lt;p&gt;Recently we added a chat interface, so now it works like a regular AI chat that can take over your device when needed.&lt;/p&gt; &lt;p&gt;Now we are planning what to focus on next and we'd love your input. Some options we're considering:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;MCP servers&lt;/strong&gt; - connect your chat to external tools and services&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Deep research&lt;/strong&gt; - letting the AI browse and gather information for you&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multi-modality&lt;/strong&gt; ‚Äî image read &amp;amp; write (generation)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;On-device models&lt;/strong&gt; ‚Äî we are working on Gemma 3n and Qwen support, but small context windows are hurting performance so much&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Speaking of which - for those of you running Ollama: do you expose your instance to the internet or keep it local network only?&lt;/p&gt; &lt;p&gt;Honest question: what would make an AI chat on your phone actually useful for you on a daily basis? Not as a toy, but as something you would rely on - what's missing from current mobile AI apps (that supports ollama) that annoys you the most?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AppDeveloperAsdf"&gt; /u/AppDeveloperAsdf &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/em3174huthfg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmir5d/what_do_you_actually_want_from_a_private_ai_chat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmir5d/what_do_you_actually_want_from_a_private_ai_chat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T13:12:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmlpjp</id>
    <title>Internet blackout and Local LLMs</title>
    <updated>2026-01-25T15:15:05+00:00</updated>
    <author>
      <name>/u/DunderSunder</name>
      <uri>https://old.reddit.com/user/DunderSunder</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Due to protests and massacre in Iran we are facing severe internet blackout which has been ongoing for 400 HOURS. only after a few days 3 websites got white-listed: google, chatgpt, deepseek. everything else is blocked even subdomains like Gmail. at the very least few people have Starlink (which is illegal) and share their connection. Finding a working vpn is really hard (I busted my ass to load reddit).&lt;/p&gt; &lt;p&gt;Meanwhile, I've been using my local uncensored Gemma3 12B and Qwen3 8B (on 8gb VRAM with llama.cpp). Then we got access to chatgpt which was pretty good since we could ask it to read contents of some pages or get latest news. But still chatgpt is VERY unhelpful in terms of finding solutions to circumvent internet censorship even if I explain the truly fucked up situation it refuses, and deepseek is worse. This is where a large uncensored local LLM could be very helpful.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DunderSunder"&gt; /u/DunderSunder &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmlpjp/internet_blackout_and_local_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmlpjp/internet_blackout_and_local_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmlpjp/internet_blackout_and_local_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T15:15:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmjzx1</id>
    <title>KV cache fix for GLM 4.7 Flash</title>
    <updated>2026-01-25T14:06:55+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmjzx1/kv_cache_fix_for_glm_47_flash/"&gt; &lt;img alt="KV cache fix for GLM 4.7 Flash" src="https://external-preview.redd.it/Yd6yP0tYXhTq7c3g8_wDa0Z1Zijr0IAXDTPXGjQc7ts.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=697845700fcf489c797d62fb0a23359703d41821" title="KV cache fix for GLM 4.7 Flash" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;tl;dr: remove Air from GLM 4.7 Flash&lt;/p&gt; &lt;p&gt;KV cache uses a lot of VRAM. GLM 4.7 Flash doesn‚Äôt even use V in the KV cache. With long contexts, this means gigabytes of VRAM saved, so you can run much longer context on the same setup.&lt;/p&gt; &lt;p&gt;UPDATE &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1qmvny5/glm47flash_is_even_faster_now/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1qmvny5/glm47flash_is_even_faster_now/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/19067"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmjzx1/kv_cache_fix_for_glm_47_flash/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmjzx1/kv_cache_fix_for_glm_47_flash/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T14:06:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmvny5</id>
    <title>GLM-4.7-Flash is even faster now</title>
    <updated>2026-01-25T21:14:50+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmvny5/glm47flash_is_even_faster_now/"&gt; &lt;img alt="GLM-4.7-Flash is even faster now" src="https://external-preview.redd.it/y3hK5MFwhoK-QUOGog7BpAan8RKjGCnfL7Xowe9Lb4E.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=880502683b1e21f9efe3ec41ebe19f6a59040622" title="GLM-4.7-Flash is even faster now" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/19092"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmvny5/glm47flash_is_even_faster_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmvny5/glm47flash_is_even_faster_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T21:14:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
