<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-17T19:35:00+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1m1foz1</id>
    <title>CUDA is coming to MLX</title>
    <updated>2025-07-16T15:31:43+00:00</updated>
    <author>
      <name>/u/mrfakename0</name>
      <uri>https://old.reddit.com/user/mrfakename0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1foz1/cuda_is_coming_to_mlx/"&gt; &lt;img alt="CUDA is coming to MLX" src="https://external-preview.redd.it/w8edStcv8JcRcgUOJ4-eZrp8x-ns7z_4bZz-mt8i8eE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9049533811e0bc40173ac835b90eb4f9943876f0" title="CUDA is coming to MLX" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looks like we will soon get CUDA support in MLX - this means that we‚Äôll be able to run MLX programs on both Apple Silicon and CUDA GPUs.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mrfakename0"&gt; /u/mrfakename0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ml-explore/mlx/pull/1983"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1foz1/cuda_is_coming_to_mlx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m1foz1/cuda_is_coming_to_mlx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-16T15:31:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1m1h0fy</id>
    <title>Support for diffusion models (Dream 7B) has been merged into llama.cpp</title>
    <updated>2025-07-16T16:20:32+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1h0fy/support_for_diffusion_models_dream_7b_has_been/"&gt; &lt;img alt="Support for diffusion models (Dream 7B) has been merged into llama.cpp" src="https://external-preview.redd.it/OqAAbOs6fFLPZaNF0M6vIqHJqNLZwtArB7hBcX1IZ7M.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8971ce6047ae48ffddcf53ec22de6523ddaa226e" title="Support for diffusion models (Dream 7B) has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Diffusion models are a new kind of language model that generate text by denoising random noise step-by-step, instead of predicting tokens left to right like traditional LLMs.&lt;/p&gt; &lt;p&gt;This PR adds basic support for diffusion models, using Dream 7B instruct as base. DiffuCoder-7B is built on the same arch so it should be trivial to add after this.&lt;br /&gt; [...]&lt;br /&gt; &lt;strong&gt;Another cool/gimmicky thing is you can see the diffusion unfold&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;In a joint effort with Huawei Noah‚Äôs Ark Lab, we release &lt;strong&gt;Dream 7B&lt;/strong&gt; (Diffusion reasoning model), the most powerful open diffusion large language model to date.&lt;/p&gt; &lt;p&gt;In short, Dream 7B:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;consistently outperforms existing diffusion language models by a large margin;&lt;/li&gt; &lt;li&gt;matches or exceeds top-tier Autoregressive (AR) language models of similar size on the general, math, and coding abilities;&lt;/li&gt; &lt;li&gt;demonstrates strong planning ability and inference flexibility that naturally benefits from the diffusion modeling.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/14644"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1h0fy/support_for_diffusion_models_dream_7b_has_been/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m1h0fy/support_for_diffusion_models_dream_7b_has_been/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-16T16:20:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1m25rnu</id>
    <title>How does Devstral Medium 2507 compare?</title>
    <updated>2025-07-17T12:05:50+00:00</updated>
    <author>
      <name>/u/z_3454_pfk</name>
      <uri>https://old.reddit.com/user/z_3454_pfk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone used this model? I‚Äôve heard it‚Äôs very good for tool calling but can‚Äôt any specifics on performance. Can anyone share their experiences?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/z_3454_pfk"&gt; /u/z_3454_pfk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m25rnu/how_does_devstral_medium_2507_compare/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m25rnu/how_does_devstral_medium_2507_compare/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m25rnu/how_does_devstral_medium_2507_compare/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-17T12:05:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1m2el95</id>
    <title>Wordle-like game using your photos and on-device Small Language Models (SLMs)</title>
    <updated>2025-07-17T18:01:40+00:00</updated>
    <author>
      <name>/u/dokasto_</name>
      <uri>https://old.reddit.com/user/dokasto_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2el95/wordlelike_game_using_your_photos_and_ondevice/"&gt; &lt;img alt="Wordle-like game using your photos and on-device Small Language Models (SLMs)" src="https://b.thumbs.redditmedia.com/Xq8alQ3on8uPHLvGduh7jS_CL8SusHfOulBaczcSXqQ.jpg" title="Wordle-like game using your photos and on-device Small Language Models (SLMs)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, long-term lurker, first-time poster here!&lt;/p&gt; &lt;p&gt;I‚Äôve been working on a game idea inspired by Wordle, but with a unique twist: it uses your own photos to generate guessing words. Here‚Äôs how it works: the app picks a random picture from your gallery. It uses a small language model (SLM), running entirely on your phone, to identify a word from the image. The chosen word could describe an object, the mood, or any notable feature in the picture. You then try to guess the word, just like Wordle.&lt;/p&gt; &lt;p&gt;The app is entirely offline, private, and doesn‚Äôt require internet access. I‚Äôve always been fascinated by the possibilities of small language models on devices, and I have more ideas I‚Äôd like to explore in the future.&lt;/p&gt; &lt;p&gt;I currently have a rough prototype ready, but developing this further is quite time-consuming as I also have a full-time job. Before investing more time into refining it, I‚Äôd love to know if this concept sounds appealing and if using your own gallery photos is something you‚Äôd find engaging.&lt;/p&gt; &lt;p&gt;Thanks in advance for your insights!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dokasto_"&gt; /u/dokasto_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2el95/wordlelike_game_using_your_photos_and_ondevice/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2el95/wordlelike_game_using_your_photos_and_ondevice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m2el95/wordlelike_game_using_your_photos_and_ondevice/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-17T18:01:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1m2981a</id>
    <title>UTCP Golang prototype</title>
    <updated>2025-07-17T14:36:33+00:00</updated>
    <author>
      <name>/u/Revolutionary_Sir140</name>
      <uri>https://old.reddit.com/user/Revolutionary_Sir140</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone, I've started to port utcp-python to golang&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Raezil/UTCP"&gt;https://github.com/Raezil/UTCP&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I've created working prototype right now.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Revolutionary_Sir140"&gt; /u/Revolutionary_Sir140 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2981a/utcp_golang_prototype/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2981a/utcp_golang_prototype/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m2981a/utcp_golang_prototype/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-17T14:36:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1m2acb8</id>
    <title>LoRA adapter on emails to mimic users style of writing from their emails</title>
    <updated>2025-07-17T15:19:39+00:00</updated>
    <author>
      <name>/u/Mindless_Paint6516</name>
      <uri>https://old.reddit.com/user/Mindless_Paint6516</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I'm working on a project where I want to fine-tune a language model to mimic a user‚Äôs personal writing style ‚Äî specifically by training on their own email history (with full consent and access via API).&lt;/p&gt; &lt;p&gt;The goal is to generate email replies that sound like the user actually wrote them.&lt;/p&gt; &lt;h1&gt;I‚Äôm curious to know:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Has anyone here tried something similar using &lt;strong&gt;LoRA adapters&lt;/strong&gt; or &lt;strong&gt;QLoRA&lt;/strong&gt;?&lt;/li&gt; &lt;li&gt;What would the training dataset look like in practice? Just the raw email threads, or should I include metadata like recipient, subject, or response time?&lt;/li&gt; &lt;li&gt;What‚Äôs the most &lt;strong&gt;practical open-source LLM&lt;/strong&gt; for this use case that can be trained with &lt;strong&gt;48GB of VRAM&lt;/strong&gt;? &lt;ul&gt; &lt;li&gt;I‚Äôve been considering &lt;strong&gt;LLaMA 3 8B&lt;/strong&gt;, &lt;strong&gt;Qwen 2.5 14B&lt;/strong&gt;, and &lt;strong&gt;Vicuna 13B&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;I know LLaMA 70B is out of scope for my setup.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Any recommendations, lessons learned, or repo links would be really helpful!&lt;/p&gt; &lt;p&gt;Thanks in advance üôè&lt;/p&gt; &lt;p&gt;&lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mindless_Paint6516"&gt; /u/Mindless_Paint6516 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2acb8/lora_adapter_on_emails_to_mimic_users_style_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2acb8/lora_adapter_on_emails_to_mimic_users_style_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m2acb8/lora_adapter_on_emails_to_mimic_users_style_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-17T15:19:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1m2gios</id>
    <title>Given that powerful models like K2 are available cheaply on hosted platforms with great inference speed, are you regretting investing in hardware for LLMs?</title>
    <updated>2025-07-17T19:15:12+00:00</updated>
    <author>
      <name>/u/Sky_Linx</name>
      <uri>https://old.reddit.com/user/Sky_Linx</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I stopped running local models on my Mac a couple of months ago because with my M4 Pro I cannot run very large and powerful models. And to be honest I no longer see the point.&lt;/p&gt; &lt;p&gt;At the moment for example I am using Kimi K2 as default model for basically everything via Groq inference, which is shockingly fast for a 1T params model, and it costs me only $1 per million input tokens and $3 per million output tokens. I mean... seriously, I get the privacy concerns some might have, but if you use LLMs for serious work, not just for playing, it really doesn't make much sense to run local LLMs anymore apart from very simple tasks.&lt;/p&gt; &lt;p&gt;So my question is mainly for those of you who have recently invested quite some chunk of cash in more powerful hardware to run LLMs locally: are you regretting it at all considering what's available on hosted platforms like Groq and OpenRouter and their prices and performance?&lt;/p&gt; &lt;p&gt;Please don't downvote right away. I am not criticizing anyone and until recently I also had some fun running some LLMs locally. I am just wondering if others agree with me that it's no longer convenient when you take performance and cost into account.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sky_Linx"&gt; /u/Sky_Linx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2gios/given_that_powerful_models_like_k2_are_available/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2gios/given_that_powerful_models_like_k2_are_available/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m2gios/given_that_powerful_models_like_k2_are_available/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-17T19:15:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1m1np9n</id>
    <title>Sometime‚Ä¶ in the next 3 to 5 decades‚Ä¶.</title>
    <updated>2025-07-16T20:32:09+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1np9n/sometime_in_the_next_3_to_5_decades/"&gt; &lt;img alt="Sometime‚Ä¶ in the next 3 to 5 decades‚Ä¶." src="https://preview.redd.it/obmnyjusqadf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=44af76a5484872eea14e9f64b8e559cdd32a58c0" title="Sometime‚Ä¶ in the next 3 to 5 decades‚Ä¶." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/obmnyjusqadf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1np9n/sometime_in_the_next_3_to_5_decades/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m1np9n/sometime_in_the_next_3_to_5_decades/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-16T20:32:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1m2ex4z</id>
    <title>[2506.00045] ACE-Step: A Step Towards Music Generation Foundation Model</title>
    <updated>2025-07-17T18:14:12+00:00</updated>
    <author>
      <name>/u/TheRealMasonMac</name>
      <uri>https://old.reddit.com/user/TheRealMasonMac</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This was released a month ago for &lt;a href="https://github.com/ace-step/ACE-Step"&gt;https://github.com/ace-step/ACE-Step&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheRealMasonMac"&gt; /u/TheRealMasonMac &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2506.00045"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2ex4z/250600045_acestep_a_step_towards_music_generation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m2ex4z/250600045_acestep_a_step_towards_music_generation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-17T18:14:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1m23efn</id>
    <title>ARGO - A Local-First, Offline AI Agent That Puts You in Control</title>
    <updated>2025-07-17T09:52:41+00:00</updated>
    <author>
      <name>/u/yushiqi</name>
      <uri>https://old.reddit.com/user/yushiqi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;We're building ARGO, an open-source AI Agent client focused on privacy, power, and ease of use. Our goal is to let everyone have their own exclusive super AI agent, without giving up control of their data.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; ARGO is a desktop client that lets you easily build and use AI agents that can think for themselves, plan, and execute complex tasks. It runs on Windows, Mac, and Linux, works completely offline, and keeps 100% of your data stored locally. It integrates with local models via Ollama and major API providers, has a powerful RAG for your own documents, and a built-in &amp;quot;Agent Factory&amp;quot; to create specialized assistants for any scenario.&lt;/p&gt; &lt;p&gt;You can check out the repo here: &lt;a href="https://github.com/xark-argo/argo"&gt;&lt;code&gt;https://github.com/xark-argo/argo&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We built ARGO because we believe you shouldn't have to choose between powerful AI and your privacy. Instead of being locked into a single cloud provider or worrying about where your data is going, ARGO gives you a single, secure, and controllable hub for all your AI agent needs. No registration, no configuration hell, just plug-and-play.&lt;/p&gt; &lt;p&gt;Here are some of the features we've implemented:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;üîí &lt;strong&gt;Local First, Privacy Above All:&lt;/strong&gt; ARGO supports full offline operation and stores 100% of your data on your local machine. It‚Äôs a native app for Windows, macOS, and Linux that you can use right away without any complex setup. Perfect for anyone who is privacy-conscious.&lt;/li&gt; &lt;li&gt;üöÄ &lt;strong&gt;A Task Engine That Actually Gets Things Done:&lt;/strong&gt; This isn't just a chatbot. ARGO uses a Multi-Agent engine that can autonomously understand your intent, break down complex tasks into steps, use tools, and generate a final report. You can even review and edit its plan in natural language before it starts.&lt;/li&gt; &lt;li&gt;‚öôÔ∏è &lt;strong&gt;Agent Factory:&lt;/strong&gt; You can visually build and customize your own dedicated agents. Need a travel planner, a research analyst, or a coding assistant? Just describe what you need, bind a model, add tools, and you‚Äôre good to go.&lt;/li&gt; &lt;li&gt;üì¶ &lt;strong&gt;Integrates Ollama and All Major Providers:&lt;/strong&gt; We made using local models dead simple. ARGO has one-click Ollama integration to download and manage local models without touching the command line. It also supports APIs from OpenAI, Claude, DeepSeek, and more, letting you seamlessly switch between local and API models to balance cost and performance.&lt;/li&gt; &lt;li&gt;üß© &lt;strong&gt;Your Own Local Knowledge Base (Agentic RAG):&lt;/strong&gt; Feed ARGO your local files, folders, or even websites to create a secure, private knowledge base. It can dynamically sync with a folder, so your agent's knowledge is always up-to-date. The Agentic mode intelligently breaks down complex questions to give more complete and reliable answers based on your documents.&lt;/li&gt; &lt;li&gt;üõ†Ô∏è &lt;strong&gt;Powerful, Extensible Toolset:&lt;/strong&gt; It comes with built-in tools like a web crawler, browser control, and local file management. It also supports custom tools via the MCP protocol, so you can easily integrate your own.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The project is fully open-source and self-hostable using Docker.&lt;/p&gt; &lt;p&gt;Getting started is easy:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Desktop App:&lt;/strong&gt; Just download the installer for your OS and you're done.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Docker:&lt;/strong&gt; We have one-line Docker commands to get you up and run.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;ARGO is still in the early stages of active development, so we'd greatly appreciate any feedback, ideas, or contributions you might have. Let us know what you think!&lt;/p&gt; &lt;p&gt;If you are interested in ARGO, give us a star üåü on GitHub to follow our progress!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yushiqi"&gt; /u/yushiqi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m23efn/argo_a_localfirst_offline_ai_agent_that_puts_you/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m23efn/argo_a_localfirst_offline_ai_agent_that_puts_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m23efn/argo_a_localfirst_offline_ai_agent_that_puts_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-17T09:52:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1m1ylw0</id>
    <title>My simple test: Qwen3-32b &gt; Qwen3-14B ‚âà DS Qwen3-8 ‚â≥ Qwen3-4B &gt; Mistral 3.2 24B &gt; Gemma3-27b-it,</title>
    <updated>2025-07-17T04:52:18+00:00</updated>
    <author>
      <name>/u/BestLeonNA</name>
      <uri>https://old.reddit.com/user/BestLeonNA</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have an article to instruct those models to rewrite in a different style without missing information, Qwen3-32B did an excellent job, it keeps the meaning but almost rewrite everything.&lt;/p&gt; &lt;p&gt;Qwen3-14B,8B tend to miss some information but acceptable&lt;/p&gt; &lt;p&gt;Qwen3-4B miss 50% of information&lt;/p&gt; &lt;p&gt;Mistral 3.2, on the other hand does not miss anything but almost copied the original with minor changes.&lt;/p&gt; &lt;p&gt;Gemma3-27: almost a true copy, just stupid&lt;/p&gt; &lt;p&gt;Structured data generation: Another test is to extract Json from raw html, Qweb3-4b fakes data and all others performs well.&lt;/p&gt; &lt;p&gt;Article classification: long messy reddit posts with simple prompt to classify if the post is looking for help, Qwen3-8,14,32 all made it 100% correct, Qwen3-4b mostly correct, Mistral and Gemma always make some mistakes to classify.&lt;/p&gt; &lt;p&gt;Overall, I should say 8b is the best one to do such tasks especially for long articles, the model consumes less vRam allows more vRam allocated to KV Cache&lt;/p&gt; &lt;p&gt;Just my small and simple test today, hope it helps if someone is looking for this use case.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BestLeonNA"&gt; /u/BestLeonNA &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1ylw0/my_simple_test_qwen332b_qwen314b_ds_qwen38/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1ylw0/my_simple_test_qwen332b_qwen314b_ds_qwen38/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m1ylw0/my_simple_test_qwen332b_qwen314b_ds_qwen38/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-17T04:52:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1m28r3c</id>
    <title>AI devs in NYC ‚Äî heads up about the RAISE Act</title>
    <updated>2025-07-17T14:17:41+00:00</updated>
    <author>
      <name>/u/AI_Alliance</name>
      <uri>https://old.reddit.com/user/AI_Alliance</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyone in the NYC AI dev space paying attention to the &lt;strong&gt;RAISE Act&lt;/strong&gt;? It‚Äôs a new bill that could shape how AI systems get built and deployed‚Äîespecially open-source stuff.&lt;/p&gt; &lt;p&gt;I‚Äôm attending a virtual meetup today (July 17 @ 12PM ET) to learn more. If you‚Äôre working on agents, LLM stacks, or tool-use pipelines, this might be a good convo to drop in on.&lt;/p&gt; &lt;p&gt;Details + free registration: &lt;a href="https://events.thealliance.ai/how-the-raise-act-affects-you"&gt;https://events.thealliance.ai/how-the-raise-act-affects-you&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hoping it‚Äôll clarify what counts as ‚Äúhigh-risk‚Äù and what role open devs can play in shaping the policy. Might be useful if you're worried about future liability or compliance headache&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AI_Alliance"&gt; /u/AI_Alliance &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m28r3c/ai_devs_in_nyc_heads_up_about_the_raise_act/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m28r3c/ai_devs_in_nyc_heads_up_about_the_raise_act/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m28r3c/ai_devs_in_nyc_heads_up_about_the_raise_act/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-17T14:17:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1m28oqc</id>
    <title>Anyone here experimenting with LLMs for translation QA ‚Äî not rewriting, just evaluating?</title>
    <updated>2025-07-17T14:15:06+00:00</updated>
    <author>
      <name>/u/NataliaShu</name>
      <uri>https://old.reddit.com/user/NataliaShu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m28oqc/anyone_here_experimenting_with_llms_for/"&gt; &lt;img alt="Anyone here experimenting with LLMs for translation QA ‚Äî not rewriting, just evaluating?" src="https://b.thumbs.redditmedia.com/DOzsE3kDEUuQywlws06uZepPnsVOAqjt3dXV5HNeD7E.jpg" title="Anyone here experimenting with LLMs for translation QA ‚Äî not rewriting, just evaluating?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi folks, has anyone used LLMs specifically to evaluate translation quality rather than generate translations? I mean using them to catch issues like dropped meaning, inconsistent terminology, awkward phrasing, and so on.&lt;/p&gt; &lt;p&gt;I‚Äôm on a team experimenting with LLMs (GPT-4, Claude, etc.) for automated translation QA. Not to create translations, but to score, flag problems, and suggest batch corrections. The tool we‚Äôre working on is called Alconost.MT/Evaluate, here's what it looks like: &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/kgu312b80gdf1.jpg?width=1200&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=ef7e69dd35bdac400f972e9b58fb94487e39b0ef"&gt;https://preview.redd.it/kgu312b80gdf1.jpg?width=1200&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=ef7e69dd35bdac400f972e9b58fb94487e39b0ef&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I‚Äôm curious: what kinds of metrics or output formats would actually be useful for you guys when comparing translation providers or assessing quality, especially when you can‚Äôt get a full human review? (I‚Äôm old-school enough to believe nothing beats a real linguist‚Äôs eyeballs, but hey, sometimes you gotta trust the bots‚Ä¶ or at least let them do the heavy lifting before the humans jump in.)&lt;/p&gt; &lt;p&gt;Cheers!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NataliaShu"&gt; /u/NataliaShu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m28oqc/anyone_here_experimenting_with_llms_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m28oqc/anyone_here_experimenting_with_llms_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m28oqc/anyone_here_experimenting_with_llms_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-17T14:15:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1m2gnnk</id>
    <title>Running an open source AI anime girl avatar</title>
    <updated>2025-07-17T19:20:31+00:00</updated>
    <author>
      <name>/u/mapppo</name>
      <uri>https://old.reddit.com/user/mapppo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2gnnk/running_an_open_source_ai_anime_girl_avatar/"&gt; &lt;img alt="Running an open source AI anime girl avatar" src="https://external-preview.redd.it/azUzamVqZ3FpaGRmMUstPxAQzeLBZZJeAt5drdnVhSzTD0UR9O7yYNnwsX72.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=37334781ea18e9aad3cbc86b76fb7bd676c22989" title="Running an open source AI anime girl avatar" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;after seeing a lot of posts about a certain expensive &amp;amp; cringy anime girlfriend, i wanted to see if there was a better way to get AI avatars. This is from &lt;a href="https://github.com/Open-LLM-VTuber/Open-LLM-VTuber"&gt;https://github.com/Open-LLM-VTuber/Open-LLM-VTuber&lt;/a&gt; (not my work) using 4o API and groq whisper, but it can use any API, or run entirely locally. You can use it with any live2d vtuber, I grabbed a random free one and did not configure the animations right. You can also change the personality prompt as you want. Serving it to mobile devices should work too but I don't care enough to try.&lt;/p&gt; &lt;p&gt;Thoughts? Would you pay for a Grokfriend? Are any of you crazy enough to date your computer?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mapppo"&gt; /u/mapppo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/rn1rxkgqihdf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2gnnk/running_an_open_source_ai_anime_girl_avatar/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m2gnnk/running_an_open_source_ai_anime_girl_avatar/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-17T19:20:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1m1vf6g</id>
    <title>Kimi K2 on Aider Polyglot Coding Leaderboard</title>
    <updated>2025-07-17T02:07:06+00:00</updated>
    <author>
      <name>/u/aratahikaru5</name>
      <uri>https://old.reddit.com/user/aratahikaru5</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1vf6g/kimi_k2_on_aider_polyglot_coding_leaderboard/"&gt; &lt;img alt="Kimi K2 on Aider Polyglot Coding Leaderboard" src="https://preview.redd.it/wvr0xh2jecdf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bb91bffcec670286acac6811feb1de48da1d6a7d" title="Kimi K2 on Aider Polyglot Coding Leaderboard" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aratahikaru5"&gt; /u/aratahikaru5 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wvr0xh2jecdf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1vf6g/kimi_k2_on_aider_polyglot_coding_leaderboard/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m1vf6g/kimi_k2_on_aider_polyglot_coding_leaderboard/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-17T02:07:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1m22w76</id>
    <title>Securing AI Agents with Honeypots, catch prompt injections before they bite</title>
    <updated>2025-07-17T09:20:00+00:00</updated>
    <author>
      <name>/u/mario_candela</name>
      <uri>https://old.reddit.com/user/mario_candela</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks üëã &lt;/p&gt; &lt;p&gt;Imagine your AI agent getting hijacked by a prompt-injection attack without you knowing. I'm the founder and maintainer of Beelzebub, an open-source project that hides &amp;quot;honeypot&amp;quot; functions inside your agent using MCP. If the model calls them... üö® BEEP! üö® You get an instant compromise alert, with detailed logs for quick investigations.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Zero false positives: Only real calls trigger the alarm.&lt;/li&gt; &lt;li&gt;Plug-and-play telemetry for tools like Grafana or ELK Stack.&lt;/li&gt; &lt;li&gt;Guard-rails fine-tuning: Every real attack strengthens the guard-rails with human input.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Read the full write-up ‚Üí &lt;a href="https://beelzebub-honeypot.com/blog/securing-ai-agents-with-honeypots/"&gt;https://beelzebub-honeypot.com/blog/securing-ai-agents-with-honeypots/&lt;/a&gt; &lt;/p&gt; &lt;p&gt;What do you think? Is it a smart defense against AI attacks, or just flashy theater? Share feedback, improvement ideas, or memes. &lt;/p&gt; &lt;p&gt;I'm all ears! üòÑ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mario_candela"&gt; /u/mario_candela &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m22w76/securing_ai_agents_with_honeypots_catch_prompt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m22w76/securing_ai_agents_with_honeypots_catch_prompt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m22w76/securing_ai_agents_with_honeypots_catch_prompt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-17T09:20:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1m2c9w6</id>
    <title>LLMs Playing Competitive Games Emerge Critical Reasoning: A Latest Study Showing Surprising Results</title>
    <updated>2025-07-17T16:33:53+00:00</updated>
    <author>
      <name>/u/MarketingNetMind</name>
      <uri>https://old.reddit.com/user/MarketingNetMind</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2c9w6/llms_playing_competitive_games_emerge_critical/"&gt; &lt;img alt="LLMs Playing Competitive Games Emerge Critical Reasoning: A Latest Study Showing Surprising Results" src="https://external-preview.redd.it/GMmAQl8cXhjszVZRasZjEE7PH09yiLGlFTDIar7oBtk.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4fe7497b5d0c0e775a986c8fd1afb9bbc017574a" title="LLMs Playing Competitive Games Emerge Critical Reasoning: A Latest Study Showing Surprising Results" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/ypc8zweungdf1.jpg?width=750&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=56d1c0515e0a511341268c19f0f578b9bf06baf0"&gt;https://preview.redd.it/ypc8zweungdf1.jpg?width=750&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=56d1c0515e0a511341268c19f0f578b9bf06baf0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Self-play has long been a key topic in artificial intelligence research. By allowing AI to compete against itself, researchers have been able to observe the emergence of intelligence. Numerous algorithms have already demonstrated that agents trained through self-play can surpass human experts.&lt;/p&gt; &lt;p&gt;So, what happens if we apply self-play to large language models (LLMs)? Can LLMs become even more intelligent with self-play training?&lt;/p&gt; &lt;p&gt;A recent study conducted by researchers from institutions including the National University of Singapore, Centre for Frontier AI Research (CFAR), Northeastern University, Sea AI Lab, Plastic Labs, and the University of Washington confirms this: LLM agents trained through self-play can significantly enhance their reasoning capabilities!&lt;/p&gt; &lt;p&gt;Read our interpretation of this groundbreaking paper here:&lt;br /&gt; &lt;a href="https://blog.netmind.ai/article/LLMs_Playing_Competitive_Games_Emerge_Critical_Reasoning%3A_A_Latest_Study_Showing_Surprising_Results"&gt;https://blog.netmind.ai/article/LLMs_Playing_Competitive_Games_Emerge_Critical_Reasoning%3A_A_Latest_Study_Showing_Surprising_Results&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MarketingNetMind"&gt; /u/MarketingNetMind &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2c9w6/llms_playing_competitive_games_emerge_critical/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2c9w6/llms_playing_competitive_games_emerge_critical/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m2c9w6/llms_playing_competitive_games_emerge_critical/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-17T16:33:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1m1sjsn</id>
    <title>MCPS are awesome!</title>
    <updated>2025-07-16T23:52:02+00:00</updated>
    <author>
      <name>/u/iChrist</name>
      <uri>https://old.reddit.com/user/iChrist</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1sjsn/mcps_are_awesome/"&gt; &lt;img alt="MCPS are awesome!" src="https://preview.redd.it/p3766l11qbdf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8a379b48132b0610b66d0e97e1fa3f988c317315" title="MCPS are awesome!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have set up like 17 MCP servers to use with open-webui and local models, and its been amazing!&lt;br /&gt; The ai can decide if it needs to use tools like web search, windows-cli, reddit posts, wikipedia articles.&lt;br /&gt; The usefulness of LLMS became that much bigger!&lt;/p&gt; &lt;p&gt;In the picture above I asked Qwen14B to execute this command in powershell:&lt;/p&gt; &lt;p&gt;python -c &amp;quot;import psutil,GPUtil,json;print(json.dumps({'cpu':psutil.cpu_percent(interval=1),'ram':psutil.virtual_memory().percent,'gpu':[{'name':g.name,'load':g.load*100,'mem_used':g.memoryUsed,'mem_total':g.memoryTotal,'temp':g.temperature} for g in GPUtil.getGPUs()]}))&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iChrist"&gt; /u/iChrist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/p3766l11qbdf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1sjsn/mcps_are_awesome/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m1sjsn/mcps_are_awesome/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-16T23:52:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1m2c4hz</id>
    <title>Kimi K2 Fiction.liveBench: On-par with DeepSeek V3, behind GPT-4.1</title>
    <updated>2025-07-17T16:27:53+00:00</updated>
    <author>
      <name>/u/fictionlive</name>
      <uri>https://old.reddit.com/user/fictionlive</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2c4hz/kimi_k2_fictionlivebench_onpar_with_deepseek_v3/"&gt; &lt;img alt="Kimi K2 Fiction.liveBench: On-par with DeepSeek V3, behind GPT-4.1" src="https://preview.redd.it/in8sapsyngdf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3f072d23b2d5e641467cb234ea0435163e5f1b18" title="Kimi K2 Fiction.liveBench: On-par with DeepSeek V3, behind GPT-4.1" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fictionlive"&gt; /u/fictionlive &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/in8sapsyngdf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2c4hz/kimi_k2_fictionlivebench_onpar_with_deepseek_v3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m2c4hz/kimi_k2_fictionlivebench_onpar_with_deepseek_v3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-17T16:27:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1m2gp16</id>
    <title>Just a reminder that today OpenAI was going to release a SOTA open source model‚Ä¶ until Kimi dropped.</title>
    <updated>2025-07-17T19:22:01+00:00</updated>
    <author>
      <name>/u/__JockY__</name>
      <uri>https://old.reddit.com/user/__JockY__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Nothing further, just posting this for the lulz. Kimi is amazing. Who even needs OpenAI at this point?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/__JockY__"&gt; /u/__JockY__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2gp16/just_a_reminder_that_today_openai_was_going_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2gp16/just_a_reminder_that_today_openai_was_going_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m2gp16/just_a_reminder_that_today_openai_was_going_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-17T19:22:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1m1i922</id>
    <title>He‚Äôs out of line but he‚Äôs right</title>
    <updated>2025-07-16T17:06:31+00:00</updated>
    <author>
      <name>/u/EstablishmentFun3205</name>
      <uri>https://old.reddit.com/user/EstablishmentFun3205</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1i922/hes_out_of_line_but_hes_right/"&gt; &lt;img alt="He‚Äôs out of line but he‚Äôs right" src="https://preview.redd.it/dqx9wlf3q9df1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d71f6c8f3707ff6aae1011b47babeb593bd890e1" title="He‚Äôs out of line but he‚Äôs right" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EstablishmentFun3205"&gt; /u/EstablishmentFun3205 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/dqx9wlf3q9df1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1i922/hes_out_of_line_but_hes_right/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m1i922/hes_out_of_line_but_hes_right/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-16T17:06:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1m2asou</id>
    <title>Kimi-k2 on lmarena</title>
    <updated>2025-07-17T15:37:09+00:00</updated>
    <author>
      <name>/u/BreakfastFriendly728</name>
      <uri>https://old.reddit.com/user/BreakfastFriendly728</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2asou/kimik2_on_lmarena/"&gt; &lt;img alt="Kimi-k2 on lmarena" src="https://b.thumbs.redditmedia.com/fy5-o3tc0GF-I3bCGJzPb2bsjXpQ9yAyleERp4yhbOw.jpg" title="Kimi-k2 on lmarena" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;overall:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ahbceguvegdf1.png?width=2450&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fa83e349894e7d76cf5d4f222fdcf183c322582e"&gt;https://preview.redd.it/ahbceguvegdf1.png?width=2450&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fa83e349894e7d76cf5d4f222fdcf183c322582e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;hard prompts:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7epol170fgdf1.png?width=2458&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0002ee1409a3cc4f14458b01bd5a7ba86176f392"&gt;https://preview.redd.it/7epol170fgdf1.png?width=2458&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0002ee1409a3cc4f14458b01bd5a7ba86176f392&lt;/a&gt;&lt;/p&gt; &lt;p&gt;coding:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/gp74ghd8fgdf1.png?width=2442&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=89da241fe7d8d85c40b41ea8604006581a025d6a"&gt;https://preview.redd.it/gp74ghd8fgdf1.png?width=2442&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=89da241fe7d8d85c40b41ea8604006581a025d6a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://lmarena.ai/leaderboard/text"&gt;https://lmarena.ai/leaderboard/text&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BreakfastFriendly728"&gt; /u/BreakfastFriendly728 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2asou/kimik2_on_lmarena/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2asou/kimik2_on_lmarena/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m2asou/kimik2_on_lmarena/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-17T15:37:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1m1xqv1</id>
    <title>We have hit 500,000 members! We have come a long way from the days of the leaked LLaMA 1 models</title>
    <updated>2025-07-17T04:04:21+00:00</updated>
    <author>
      <name>/u/NixTheFolf</name>
      <uri>https://old.reddit.com/user/NixTheFolf</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1xqv1/we_have_hit_500000_members_we_have_come_a_long/"&gt; &lt;img alt="We have hit 500,000 members! We have come a long way from the days of the leaked LLaMA 1 models" src="https://preview.redd.it/zfvdqak3zcdf1.png?width=216&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7a4c2e85087da70112018731aafb9b5d409cf823" title="We have hit 500,000 members! We have come a long way from the days of the leaked LLaMA 1 models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NixTheFolf"&gt; /u/NixTheFolf &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zfvdqak3zcdf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1xqv1/we_have_hit_500000_members_we_have_come_a_long/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m1xqv1/we_have_hit_500000_members_we_have_come_a_long/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-17T04:04:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1m253n6</id>
    <title>expectation: "We'll fire thousands of junior programmers and replace them with ten seniors and AI"</title>
    <updated>2025-07-17T11:31:00+00:00</updated>
    <author>
      <name>/u/MelodicRecognition7</name>
      <uri>https://old.reddit.com/user/MelodicRecognition7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;reality: HR's use AI to parse resum√©s and companies hire vibecoders with fake senior resum√©s written by the AI&lt;/p&gt; &lt;p&gt;stage of acceptance: &amp;quot;we'll hire information security specialists to fix all that crap made by the vibecoders&amp;quot;&lt;/p&gt; &lt;p&gt;harsh reality: HR's using AI hire vibeDevSecOpses with fake resum√©s written by the AI and vibeDevSecOpses use AI to &amp;quot;fix&amp;quot; the crap made by the vibecoders using AI&lt;/p&gt; &lt;p&gt;clown world: you are here&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MelodicRecognition7"&gt; /u/MelodicRecognition7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m253n6/expectation_well_fire_thousands_of_junior/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m253n6/expectation_well_fire_thousands_of_junior/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m253n6/expectation_well_fire_thousands_of_junior/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-17T11:31:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1m2bigh</id>
    <title>Mistral announces Deep Research, Voice mode, multilingual reasoning and Projects for Le Chat</title>
    <updated>2025-07-17T16:04:03+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2bigh/mistral_announces_deep_research_voice_mode/"&gt; &lt;img alt="Mistral announces Deep Research, Voice mode, multilingual reasoning and Projects for Le Chat" src="https://external-preview.redd.it/QLQU1soiMTzFAm8GzW6EPDbaX5jrcYYFqy1ql5NYoiQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4c3b97e1405ebb7916bf71d7b9a3da9a44efaea7" title="Mistral announces Deep Research, Voice mode, multilingual reasoning and Projects for Le Chat" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;New in Le Chat:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Deep Research mode: Lightning fast, structured research reports on even the most complex topics.&lt;/li&gt; &lt;li&gt;Voice mode: Talk to Le Chat instead of typing with our new Voxtral model.&lt;/li&gt; &lt;li&gt;Natively multilingual reasoning: Tap into thoughtful answers, powered by our reasoning model ‚Äî Magistral.&lt;/li&gt; &lt;li&gt;Projects: Organize your conversations into context-rich folders.&lt;/li&gt; &lt;li&gt;Advanced image editing directly in Le Chat, in partnership with Black Forest Labs.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Not local, but much of their underlying models (like Voxtral and Magistral) are, with permissible licenses. For me that makes it worth supporting!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://mistral.ai/news/le-chat-dives-deep"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2bigh/mistral_announces_deep_research_voice_mode/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m2bigh/mistral_announces_deep_research_voice_mode/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-17T16:04:03+00:00</published>
  </entry>
</feed>
