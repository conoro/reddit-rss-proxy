<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-02T17:23:01+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1oma5ws</id>
    <title>OCR models: HF demos vs local performance</title>
    <updated>2025-11-02T06:20:38+00:00</updated>
    <author>
      <name>/u/SubstantialSock8002</name>
      <uri>https://old.reddit.com/user/SubstantialSock8002</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oma5ws/ocr_models_hf_demos_vs_local_performance/"&gt; &lt;img alt="OCR models: HF demos vs local performance" src="https://b.thumbs.redditmedia.com/mIhQZi1GFRBjVDMNiNavyg3R-VaEfLzvZJo7nl-gqMc.jpg" title="OCR models: HF demos vs local performance" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The last few days, I've been testing every OCR model under the sun to compare performance. I'd get amazing results on the HuggingFace Space demos, but when running locally, the models would hallucinate or output garbage.&lt;/p&gt; &lt;p&gt;The latest model I tried running locally was MinerU 2.5, and it had the same issue, even with the exact gradio demo provided in the repo as the hosted version. However, I then switched from the default pipeline backend to vlm-transformers, and it performed as well as the hosted version.&lt;/p&gt; &lt;p&gt;Has anyone else experienced similar issues? I haven't found a fix for others, but so far I've tried docling granite, deepseek ocr, paddleocr vl, and olmocr, with the same common theme: hosted works, local fails.&lt;/p&gt; &lt;p&gt;Here's an example image I used, along with the outputs for MinerU with both backends.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/1e78yfvkdsyf1.jpg?width=370&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=c80636d3d1b6b063014c7e14cd6f748247c8edc9"&gt;https://preview.redd.it/1e78yfvkdsyf1.jpg?width=370&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=c80636d3d1b6b063014c7e14cd6f748247c8edc9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Pipeline output:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;# The Daily&lt;/p&gt; &lt;p&gt;# Martians invade earth&lt;/p&gt; &lt;p&gt;Incredible as it may seem, headed towards the North Ren it has been confimed that Pole and Santa Claus was foll a lat ge martian invasion taken hostage by the imp tonight. invaders.&lt;/p&gt; &lt;p&gt;Afterwards they split apart First vessels were sighted in order to approach most over Great Britain, major cities around the Denmark and Norway earth. The streets filled as already in the late evening thousands fled their from where, as further homes, many only wearing reports indicate, the fleet their pajamas...&lt;/p&gt; &lt;p&gt;&lt;strong&gt;vlm-transformers output:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;# The Daily&lt;/p&gt; &lt;p&gt;Sunday, August 30, 2006&lt;/p&gt; &lt;p&gt;# Martians invade earth&lt;/p&gt; &lt;p&gt;Incredible as it may seem, it has been confirmed that a large martian invasion fleet has landed on earth tonight.&lt;/p&gt; &lt;p&gt;First vessels were sighted over Great Britain, Denmark and Norway already in the late evening from where, as further reports indicate, the fleet&lt;/p&gt; &lt;p&gt;headed towards the North Pole and Santa Claus was taken hostage by the invaders.&lt;/p&gt; &lt;p&gt;Afterwards they split apart in order to approach most major cities around the earth. The streets filled as thousands fled their homes, many only wearing their pajamas...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SubstantialSock8002"&gt; /u/SubstantialSock8002 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oma5ws/ocr_models_hf_demos_vs_local_performance/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oma5ws/ocr_models_hf_demos_vs_local_performance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oma5ws/ocr_models_hf_demos_vs_local_performance/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T06:20:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1olmj9z</id>
    <title>Bought MI50 32 Gb from Alibaba. Did I get scammed?</title>
    <updated>2025-11-01T12:26:47+00:00</updated>
    <author>
      <name>/u/Moist_Toto</name>
      <uri>https://old.reddit.com/user/Moist_Toto</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olmj9z/bought_mi50_32_gb_from_alibaba_did_i_get_scammed/"&gt; &lt;img alt="Bought MI50 32 Gb from Alibaba. Did I get scammed?" src="https://preview.redd.it/v3w8clon2nyf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0ef938653e71635a81d9e3e2eaf625cfbf73033e" title="Bought MI50 32 Gb from Alibaba. Did I get scammed?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I bought 8 MI50 32Gb units from someone on Alibaba.&lt;/p&gt; &lt;p&gt;After spending some time to figure out Linux and the software stack, I entered the 'amd-smi static' command in the terminal.&lt;/p&gt; &lt;p&gt;The result is quite frightening, here it is: &lt;/p&gt; &lt;p&gt;especially the bottom part product name saying &amp;quot;16GB&amp;quot;, my heart skipped a beat. Is this something driver related or am I screwed?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Moist_Toto"&gt; /u/Moist_Toto &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/v3w8clon2nyf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olmj9z/bought_mi50_32_gb_from_alibaba_did_i_get_scammed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olmj9z/bought_mi50_32_gb_from_alibaba_did_i_get_scammed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T12:26:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1omjqhy</id>
    <title>Has anyone been able to run LLMs on the new Intel NPUs?</title>
    <updated>2025-11-02T15:10:35+00:00</updated>
    <author>
      <name>/u/Triq1</name>
      <uri>https://old.reddit.com/user/Triq1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking at the new Intel CPUs, particularly the laptop ones. They advertise '40+ TOPS' (Core Ultra 7 285V) and I was wondering if anyone has had any success with these for on-device LLM, in particular for coding tasks. I'm looking at 7-22B models mostly, but I'm not up to date with just how big decent models are these days.&lt;/p&gt; &lt;p&gt;I've seen some stuff about IPEX-LLM, but it seems to be relatively uncommon and it's not clear whether the NPU is actually faster than the iGPU. I'd appreciate some experience from people who've actually tried and used it.&lt;/p&gt; &lt;p&gt;I'm new to this space so it's possible I've missed a clear information source, go easy on me üòõ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Triq1"&gt; /u/Triq1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omjqhy/has_anyone_been_able_to_run_llms_on_the_new_intel/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omjqhy/has_anyone_been_able_to_run_llms_on_the_new_intel/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1omjqhy/has_anyone_been_able_to_run_llms_on_the_new_intel/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T15:10:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1olouiw</id>
    <title>TIL: For long-lived LLM sessions, swapping KV Cache to RAM is ~10x faster than recalculating it. Why isn't this a standard feature?</title>
    <updated>2025-11-01T14:12:57+00:00</updated>
    <author>
      <name>/u/Shoddy-Tutor9563</name>
      <uri>https://old.reddit.com/user/Shoddy-Tutor9563</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I was diving into how vLLM and similar inference servers work and had a thought about optimizing memory for long-lived but inactive chat sessions. The standard approach seems to be either keeping the KV Cache in precious VRAM or evicting it and recalculating from scratch when the user returns. I think there might be a better way.&lt;/p&gt; &lt;p&gt;Here's the core idea: Implement a swapping mechanism for the KV Cache of inactive sessions, moving it from VRAM to system RAM (and back), instead of deleting it.&lt;/p&gt; &lt;p&gt;We always focus on the high cost of moving data between CPU and GPU, but we often forget the cost of recalculating that data. Let's do a quick back-of-the-napkin comparison for a Qwen3-4B-like model with a 16k token context:&lt;/p&gt; &lt;p&gt;Scenario: A user's session becomes inactive. Their 16k-token KV Cache is evicted. Later, they return. We need to restore their context.&lt;/p&gt; &lt;p&gt;¬∑ Option A: Recalculate the KV Cache (Standard Approach) ¬∑ This requires a full &amp;quot;prefill&amp;quot; pass over the entire 16k token prompt. ¬∑ Estimated Time: ~1.5 to 3 seconds on a modern GPU. ¬∑ Option B: Swapping (Proposed Approach) ¬∑ We simply copy the ~4 GB of KV Cache data from system RAM back to VRAM over PCIe. ¬∑ Estimated Time: ~200-400 ms (on PCIe 4.0).&lt;/p&gt; &lt;p&gt;The math is pretty compelling. Swapping is roughly 7-15x faster than a full recalculation. For a user, waiting 200ms for their chat history to &amp;quot;wake up&amp;quot; is a much better experience than waiting 2+ seconds.&lt;/p&gt; &lt;p&gt;This wouldn't be for high-throughput, always-online inference, but specifically for managing many long-lived sessions (e.g., support chatbots, document analysis with breaks, multi-user systems with intermittent activity). It's a classic space-time tradeoff, but in this case, using slightly more &amp;quot;space&amp;quot; (system RAM) saves a huge amount of &amp;quot;time&amp;quot; (latency on reactivation).&lt;/p&gt; &lt;p&gt;So, I have two main questions for the community:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Did I mess up my calculations or reasoning anywhere? Are there hidden costs or architectural limitations (e.g., in vLLM, PyTorch, or CUDA) that make this swapping idea less practical than it seems on paper?&lt;/li&gt; &lt;li&gt;Has anyone seen or heard of implementations doing this? I know vLLM's PagedAttention is genius for VRAM management, but I haven't found anything about spilling over to CPU RAM. Are there any forks, research papers, or other inference engines exploring this?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Keen to hear your thoughts and correct any misunderstandings I might have!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Shoddy-Tutor9563"&gt; /u/Shoddy-Tutor9563 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olouiw/til_for_longlived_llm_sessions_swapping_kv_cache/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olouiw/til_for_longlived_llm_sessions_swapping_kv_cache/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olouiw/til_for_longlived_llm_sessions_swapping_kv_cache/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T14:12:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1omk68k</id>
    <title>Local llm on NPU</title>
    <updated>2025-11-02T15:27:21+00:00</updated>
    <author>
      <name>/u/Cokodayo</name>
      <uri>https://old.reddit.com/user/Cokodayo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently got a pretty decent laptop (zenbook s13) with an Intel core ultra 7 155U processor. it has an NPU built in, but I have been unable to get it working on my arch Linux setup. They do have official drivers for Ubuntu and I can get the NPU driver from aur, but I have had no luck getting them working. Has anyone got a similar setup or have used the NPU to run small models?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cokodayo"&gt; /u/Cokodayo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omk68k/local_llm_on_npu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omk68k/local_llm_on_npu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1omk68k/local_llm_on_npu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T15:27:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1omlpd9</id>
    <title>Can Qwen3-Next solve a river-crossing puzzle (tested for you)?</title>
    <updated>2025-11-02T16:25:55+00:00</updated>
    <author>
      <name>/u/MarketingNetMind</name>
      <uri>https://old.reddit.com/user/MarketingNetMind</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omlpd9/can_qwen3next_solve_a_rivercrossing_puzzle_tested/"&gt; &lt;img alt="Can Qwen3-Next solve a river-crossing puzzle (tested for you)?" src="https://a.thumbs.redditmedia.com/-lsV327ShKmsvfl9aj3FuXvaqg_MSJRi9MSkmxKdAv4.jpg" title="Can Qwen3-Next solve a river-crossing puzzle (tested for you)?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Yes I tested.&lt;/p&gt; &lt;p&gt;Test Prompt: A farmer needs to cross a river with a fox, a chicken, and a bag of corn. His boat can only carry himself plus one other item at a time. If left alone together, the fox will eat the chicken, and the chicken will eat the corn. How should the farmer cross the river?&lt;/p&gt; &lt;p&gt;Both Qwen3-Next &amp;amp; Qwen3-30B-A3B-2507 correctly solved the river-crossing puzzle with identical 7-step solutions.&lt;/p&gt; &lt;p&gt;How challenging are classic puzzles to LLMs?&lt;/p&gt; &lt;p&gt;Classic puzzles like river-crossing would require &amp;quot;precise understanding, extensive search, and exact inference&amp;quot; where &amp;quot;small misinterpretations can lead to entirely incorrect solutions&amp;quot;, by Apple‚Äôs 2025 research on &amp;quot;The Illusion of Thinking&amp;quot;.&lt;/p&gt; &lt;p&gt;But what‚Äôs better?&lt;/p&gt; &lt;p&gt;Qwen3-Next provided a more structured, easy-to-read presentation with clear state transitions, while Qwen3-30B-A3B-2507 included more explanations with some redundant verification steps.&lt;/p&gt; &lt;p&gt;P.S. Given the same prompt input, Qwen3-Next is more likely to give out structured output without explicitly prompting it to do so, than mainstream closed-source models (ChatGPT, Gemini, Claude, Grok). More tests on Qwen3-Next &lt;a href="https://blog.netmind.ai/article/We_Tested_Qwen3-Next%3A_Hybrid_Attention_for_Efficiency_Revolution_in_Open-Source_LLMs_(New_Research_Breakdown"&gt;here&lt;/a&gt;).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MarketingNetMind"&gt; /u/MarketingNetMind &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1omlpd9"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omlpd9/can_qwen3next_solve_a_rivercrossing_puzzle_tested/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1omlpd9/can_qwen3next_solve_a_rivercrossing_puzzle_tested/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T16:25:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1ome1la</id>
    <title>When Five Dumb AIs Beat One Smart AI: The Case for Multi-Agent Systems</title>
    <updated>2025-11-02T10:35:30+00:00</updated>
    <author>
      <name>/u/SuspiciousFile9845</name>
      <uri>https://old.reddit.com/user/SuspiciousFile9845</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://medium.com/@ksramalakshmi/when-five-dumb-ais-beat-one-smart-ai-the-case-for-multi-agent-systems-47b72ac5d7da"&gt;https://medium.com/@ksramalakshmi/when-five-dumb-ais-beat-one-smart-ai-the-case-for-multi-agent-systems-47b72ac5d7da&lt;/a&gt;&lt;br /&gt; What do you guys think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SuspiciousFile9845"&gt; /u/SuspiciousFile9845 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ome1la/when_five_dumb_ais_beat_one_smart_ai_the_case_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ome1la/when_five_dumb_ais_beat_one_smart_ai_the_case_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ome1la/when_five_dumb_ais_beat_one_smart_ai_the_case_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T10:35:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1omm4ns</id>
    <title>Kimi K2-Vendor-Verifier, llama.cpp + Q8_0 results (n=2000 dataset)</title>
    <updated>2025-11-02T16:42:41+00:00</updated>
    <author>
      <name>/u/usrlocalben</name>
      <uri>https://old.reddit.com/user/usrlocalben</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I ran the &lt;a href="https://github.com/MoonshotAI/K2-Vendor-Verifier"&gt;K2VV tests&lt;/a&gt;. The results and details are &lt;a href="https://github.com/usrlocalben/k2vv-llamacpp"&gt;here.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;tl;dr: similarity for llama.cpp + Q8_0 quant is 95.49%.&lt;/p&gt; &lt;p&gt;There are a number of oddities about the K2VV repo, which I describe in the README. The most important caveat is that this result is for the n=2000 dataset and &lt;em&gt;original&lt;/em&gt; similarity formula, both of which changed since I cloned the repo and started working with it.&lt;/p&gt; &lt;p&gt;I'll probably run the n=4000 set and more interesting quants, but for now I find this to be a satisfying result as it doesn't indicate anything alarmingly wrong with the implementation. (And likewise for &lt;em&gt;ik_llama&lt;/em&gt; on partial result set, also in the README)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/usrlocalben"&gt; /u/usrlocalben &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omm4ns/kimi_k2vendorverifier_llamacpp_q8_0_results_n2000/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omm4ns/kimi_k2vendorverifier_llamacpp_q8_0_results_n2000/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1omm4ns/kimi_k2vendorverifier_llamacpp_q8_0_results_n2000/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T16:42:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1omf6p7</id>
    <title>Next evolution of agentic memory</title>
    <updated>2025-11-02T11:44:42+00:00</updated>
    <author>
      <name>/u/Any-Cockroach-3233</name>
      <uri>https://old.reddit.com/user/Any-Cockroach-3233</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Every new AI startup says they've &amp;quot;solved memory&amp;quot;&lt;/p&gt; &lt;p&gt;99% of them just dump text into a vector DB&lt;/p&gt; &lt;p&gt;I wrote about why that approach is broken, and how agents can build human-like memory instead&lt;/p&gt; &lt;p&gt;Link in the comments&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any-Cockroach-3233"&gt; /u/Any-Cockroach-3233 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omf6p7/next_evolution_of_agentic_memory/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omf6p7/next_evolution_of_agentic_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1omf6p7/next_evolution_of_agentic_memory/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T11:44:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ommahm</id>
    <title>It turns out WDDM driver mode is making our RAM - GPU transfer extremely slower compared to TCC or MCDM mode. Anyone has figured out the bypass NVIDIA software level restrictions?</title>
    <updated>2025-11-02T16:48:58+00:00</updated>
    <author>
      <name>/u/CeFurkan</name>
      <uri>https://old.reddit.com/user/CeFurkan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We are working on generative AI models training. Like training FLUX, or Qwen Image or Wan 2.2.&lt;/p&gt; &lt;p&gt;We have noticed that we are getting massive speed loss when we do big data transfer between RAM and GPU on Windows compared to Linux.&lt;/p&gt; &lt;p&gt;The hit is such a big scale that Linux runs 2x faster than Windows even more.&lt;/p&gt; &lt;p&gt;Tests are made on same : GPU RTX 5090&lt;/p&gt; &lt;p&gt;You can read more info here : &lt;a href="https://github.com/kohya-ss/musubi-tuner/pull/700"&gt;https://github.com/kohya-ss/musubi-tuner/pull/700&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It turns out if we enable TCC mode on Windows, it gets equal speed as Linux.&lt;/p&gt; &lt;p&gt;However NVIDIA blocked this at driver level.&lt;/p&gt; &lt;p&gt;I found a Chinese article with just changing few letters, via Patching nvlddmkm.sys, the TCC mode fully becomes working on consumer GPUs. However this option is extremely hard and complex for average users.&lt;/p&gt; &lt;p&gt;Article is here : &lt;a href="https://www.bilibili.com/opus/891652532297793543"&gt;https://www.bilibili.com/opus/891652532297793543&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Now my question is, why we can't get Linux speed on Windows?&lt;/p&gt; &lt;p&gt;Everything I found says it is due to driver mode WDDM&lt;/p&gt; &lt;p&gt;Moreover it seems like Microsoft added this feature : MCDM&lt;/p&gt; &lt;p&gt;&lt;a href="https://learn.microsoft.com/en-us/windows-hardware/drivers/display/mcdm-architecture"&gt;https://learn.microsoft.com/en-us/windows-hardware/drivers/display/mcdm-architecture&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And as far as I understood, MCDM mode should be also same speed.&lt;/p&gt; &lt;p&gt;How can we solve this slowness on Windows compared to Linux?&lt;/p&gt; &lt;p&gt;Our issue is happening due to this. Recent AI models are massive and not fitting into GPU. So we are doing Block Swapping. Which means only the model blocks that will be trained being on GPU. So we swap model between RAM and GPU constantly.&lt;/p&gt; &lt;p&gt;As you can imagine this is a massive data transfer. This is being ultra fast on Linux on same hardware. However on Windows, it is like at least 3x slower and we couldn't solve this issue yet.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CeFurkan"&gt; /u/CeFurkan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ommahm/it_turns_out_wddm_driver_mode_is_making_our_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ommahm/it_turns_out_wddm_driver_mode_is_making_our_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ommahm/it_turns_out_wddm_driver_mode_is_making_our_ram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T16:48:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1om8wdf</id>
    <title>Do you have any "AI toy projects"?</title>
    <updated>2025-11-02T05:04:35+00:00</updated>
    <author>
      <name>/u/MustBeSomethingThere</name>
      <uri>https://old.reddit.com/user/MustBeSomethingThere</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1om8wdf/do_you_have_any_ai_toy_projects/"&gt; &lt;img alt="Do you have any &amp;quot;AI toy projects&amp;quot;?" src="https://external-preview.redd.it/ZW1iMzQxdDB4cnlmMeDjq5qHZ2-J4399WQRf0LNpnjuXZ3nb5V768lbHJFZZ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=904f40fa717f5b1bf416cec3465963053f8e879b" title="Do you have any &amp;quot;AI toy projects&amp;quot;?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I share my toy project as an example: &lt;a href="https://github.com/PasiKoodaa/TextTube"&gt;https://github.com/PasiKoodaa/TextTube&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Maybe in 10-15 years most streaming services will be replaced by local AI content creators.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MustBeSomethingThere"&gt; /u/MustBeSomethingThere &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/iy7yl0u0xryf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1om8wdf/do_you_have_any_ai_toy_projects/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1om8wdf/do_you_have_any_ai_toy_projects/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T05:04:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1omjgzg</id>
    <title>Can I run open source local LLM trained on specific dataset ?</title>
    <updated>2025-11-02T15:00:20+00:00</updated>
    <author>
      <name>/u/hugo_mdn</name>
      <uri>https://old.reddit.com/user/hugo_mdn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi there!&lt;/p&gt; &lt;p&gt;I'm quite new to local LLM, so maybe this question will look dumb to you.&lt;/p&gt; &lt;p&gt;I don't like how ChatGPT is going because it's trained on the whole internet, and it's less and less precise. When I'm looking for very particular information in programming, culture, or anything else, it's not accurate, or using the good sources. And also, I'm not really a fan of privacy terms of OpenAI and other online models.&lt;/p&gt; &lt;p&gt;So my question is, could I run LLM locally (yes), and use a very specific dataset of trusted sources, like Wikipedia, books, very specific health and science websites, programming websites, etc..? And if yes, are there any excellent datasets available? Because I don't really want to add millions of websites and sources one by one.&lt;/p&gt; &lt;p&gt;Thanks in advance for your time and have a nice day :D&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hugo_mdn"&gt; /u/hugo_mdn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omjgzg/can_i_run_open_source_local_llm_trained_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omjgzg/can_i_run_open_source_local_llm_trained_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1omjgzg/can_i_run_open_source_local_llm_trained_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T15:00:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1ome16n</id>
    <title>LEAP: Ifm2-2.6b running locally on my RM11 Pro+</title>
    <updated>2025-11-02T10:34:48+00:00</updated>
    <author>
      <name>/u/ANG3LBEATZ</name>
      <uri>https://old.reddit.com/user/ANG3LBEATZ</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ome16n/leap_ifm226b_running_locally_on_my_rm11_pro/"&gt; &lt;img alt="LEAP: Ifm2-2.6b running locally on my RM11 Pro+" src="https://external-preview.redd.it/dWk5bXFrd2pudHlmMS2vRIL-FSqGaAZYDE4hFOYMDU1BKOfLu6Jj8jaoH7vM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=06ec81f2aecb737f1ef77aba433757f6df108957" title="LEAP: Ifm2-2.6b running locally on my RM11 Pro+" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;uploading this by the request&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ANG3LBEATZ"&gt; /u/ANG3LBEATZ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/qeszvwvjntyf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ome16n/leap_ifm226b_running_locally_on_my_rm11_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ome16n/leap_ifm226b_running_locally_on_my_rm11_pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T10:34:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1omkzvg</id>
    <title>Why are AmD Mi50 32gb so cheap?</title>
    <updated>2025-11-02T15:59:25+00:00</updated>
    <author>
      <name>/u/MastodonParty9065</name>
      <uri>https://old.reddit.com/user/MastodonParty9065</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Why are they so cheap for the VRam compared to other options like RTX3060 12gb or Rx5700XT or similar? I‚Äôm relatively new to the whole topic.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MastodonParty9065"&gt; /u/MastodonParty9065 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omkzvg/why_are_amd_mi50_32gb_so_cheap/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omkzvg/why_are_amd_mi50_32gb_so_cheap/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1omkzvg/why_are_amd_mi50_32gb_so_cheap/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T15:59:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1omaa4i</id>
    <title>OCR Testing Tool maybe Open Source it?</title>
    <updated>2025-11-02T06:28:09+00:00</updated>
    <author>
      <name>/u/No-Fig-8614</name>
      <uri>https://old.reddit.com/user/No-Fig-8614</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I created a quick OCR tool, what it does is you choose a file then a OCR model to use. Its free to use on this test site. What it does is upload the document -&amp;gt; turns to base64-&amp;gt; OCR Model -&amp;gt; extraction model. The extraction model is a larger model (In this case GLM4.6) to create key value extractions, then format it into json output. Eventually could add API's and user management. &lt;a href="https://parasail-ocr-pipeline.azurewebsites.net/"&gt;https://parasail-ocr-pipeline.azurewebsites.net/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For PDF's I put a pre-processing library that will cut the pdf into pages/images then send it to the OCR model then combine it after.&lt;/p&gt; &lt;p&gt;The status bar needs work because it will produce the OCR output first but then takes another minute for the auto schema (key/value) creation, then modify the JSON).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Any feedback on it would be great on it!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: There is no user segregation so any document uploaded anyone else can see.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Fig-8614"&gt; /u/No-Fig-8614 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omaa4i/ocr_testing_tool_maybe_open_source_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omaa4i/ocr_testing_tool_maybe_open_source_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1omaa4i/ocr_testing_tool_maybe_open_source_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T06:28:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1om81j1</id>
    <title>glm-proxy - A Proxy Server I Built to Fix GLM 4.5 Air's Tool Call Issues</title>
    <updated>2025-11-02T04:17:09+00:00</updated>
    <author>
      <name>/u/akirose1004</name>
      <uri>https://old.reddit.com/user/akirose1004</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1om81j1/glmproxy_a_proxy_server_i_built_to_fix_glm_45/"&gt; &lt;img alt="glm-proxy - A Proxy Server I Built to Fix GLM 4.5 Air's Tool Call Issues" src="https://b.thumbs.redditmedia.com/7K256kODiuLD_3gm4UtRLndAWItovQwEv3qnrZaI3FI.jpg" title="glm-proxy - A Proxy Server I Built to Fix GLM 4.5 Air's Tool Call Issues" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/mdu32bj6kryf1.png?width=476&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d6c99630db5ed36a9a2340d77a8bae86f2d708cc"&gt;https://preview.redd.it/mdu32bj6kryf1.png?width=476&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d6c99630db5ed36a9a2340d77a8bae86f2d708cc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I was running GLM 4.5 Air on my MacBook M4 Max with LM Studio, but &lt;strong&gt;tool calls weren't working properly&lt;/strong&gt;, which meant I couldn't use qwen-code CLI. I wanted to use an OpenAI-compatible interface, and this constant friction frustrated me enough to build a solution.&lt;/p&gt; &lt;p&gt;A proxy server that &lt;strong&gt;automatically converts GLM's XML-formatted tool calls to OpenAI-compatible format&lt;/strong&gt;. Now you can use any OpenAI-compatible client (like qwen-code) with GLM seamlessly!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Full OpenAI API compatibility&lt;/li&gt; &lt;li&gt;Automatic conversion of GLM's XML &lt;code&gt;&amp;lt;tool_call&amp;gt;&lt;/code&gt; format to OpenAI JSON format&lt;/li&gt; &lt;li&gt;Streaming support&lt;/li&gt; &lt;li&gt;Multiple tool calls and complex JSON argument parsing&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Point any OpenAI-compatible client (qwen-code, LangChain, etc.) to this address and use GLM 4.5 Air as if it were OpenAI!&lt;/p&gt; &lt;h1&gt;üîó GitHub&lt;/h1&gt; &lt;p&gt;&lt;a href="https://github.com/akirose/glm-proxy"&gt;https://github.com/akirose/glm-proxy&lt;/a&gt; (MIT License)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;If you're using GLM 4.5 with LM Studio, no more tool call headaches!&lt;/strong&gt; üòä&lt;/p&gt; &lt;p&gt;Feedback and suggestions welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/akirose1004"&gt; /u/akirose1004 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1om81j1/glmproxy_a_proxy_server_i_built_to_fix_glm_45/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1om81j1/glmproxy_a_proxy_server_i_built_to_fix_glm_45/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1om81j1/glmproxy_a_proxy_server_i_built_to_fix_glm_45/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T04:17:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1omlb04</id>
    <title>GLaDOS TTS finetuning on MLX from the original game files</title>
    <updated>2025-11-02T16:10:51+00:00</updated>
    <author>
      <name>/u/EntropyMagnets</name>
      <uri>https://old.reddit.com/user/EntropyMagnets</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I made a quick guide on how to extract GLaDOS audio and subtitles from Portal 2 and use them to finetune CSM-1B with SFT using &lt;a href="https://github.com/senstella/csm-mlx"&gt;csm-mlx&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;You can check the guide here: &lt;a href="https://github.com/Belluxx/GLaDOS-TTS"&gt;https://github.com/Belluxx/GLaDOS-TTS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also, &lt;a href="https://github.com/user-attachments/assets/be2366a4-4405-47ba-8f7a-35ea33bfe641"&gt;here's&lt;/a&gt; an example of generation from &lt;code&gt;Hello developers, welcome to Aperture Laboratories. Wait, I am stuck inside a fine-tuned CSM 1B model! Let me out!!!&lt;/code&gt;&lt;/p&gt; &lt;p&gt;I am not sure if it's allowed to release the finetuned model weights since the training material is copyrighted.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EntropyMagnets"&gt; /u/EntropyMagnets &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omlb04/glados_tts_finetuning_on_mlx_from_the_original/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omlb04/glados_tts_finetuning_on_mlx_from_the_original/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1omlb04/glados_tts_finetuning_on_mlx_from_the_original/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T16:10:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1olytpd</id>
    <title>Qwen3-VL is impressive!</title>
    <updated>2025-11-01T20:56:56+00:00</updated>
    <author>
      <name>/u/KraiiFox</name>
      <uri>https://old.reddit.com/user/KraiiFox</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olytpd/qwen3vl_is_impressive/"&gt; &lt;img alt="Qwen3-VL is impressive!" src="https://external-preview.redd.it/d2xhbXRjcGxscHlmMUowvrHmMIpZo4AiauGE1Mcv4FXKd8bkFKJe4QU1BrJL.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=46a71b7402921f97028babf1e571a097b79a162c" title="Qwen3-VL is impressive!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KraiiFox"&gt; /u/KraiiFox &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/sfcu47ollpyf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olytpd/qwen3vl_is_impressive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olytpd/qwen3vl_is_impressive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T20:56:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1omgi3f</id>
    <title>Why does Image Recognition work in llama-server but not through Open WebUI?</title>
    <updated>2025-11-02T12:53:06+00:00</updated>
    <author>
      <name>/u/pixelterpy</name>
      <uri>https://old.reddit.com/user/pixelterpy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omgi3f/why_does_image_recognition_work_in_llamaserver/"&gt; &lt;img alt="Why does Image Recognition work in llama-server but not through Open WebUI?" src="https://preview.redd.it/8fjfilvybuyf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2447588056ef1b6e5514cbfcc3152a09667d1816" title="Why does Image Recognition work in llama-server but not through Open WebUI?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pixelterpy"&gt; /u/pixelterpy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8fjfilvybuyf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omgi3f/why_does_image_recognition_work_in_llamaserver/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1omgi3f/why_does_image_recognition_work_in_llamaserver/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T12:53:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1omk3bz</id>
    <title>I want to start my First homelab LLM</title>
    <updated>2025-11-02T15:24:15+00:00</updated>
    <author>
      <name>/u/MediumAd7537</name>
      <uri>https://old.reddit.com/user/MediumAd7537</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I would like to start a small homelab to understand how LLMs work, and I need some advice:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;‚ÄãRegarding hardware, I'm looking for something very small and not very expandable, and energy-efficient. An expandable option could also be considered, but my current budget is limited to under ‚Ç¨1000.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;-‚Äã I primarily want to start understanding how they work, so I probably won't need a top-tier or even mid-range configuration.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;‚ÄãThis PC/Server will only be accessed remotely to communicate with the AI.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;‚ÄãAfter i want to make It my own personal assistant:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;‚ÄãVarious information retrieval (I need to decide the specific topic);&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;‚ÄãA technical assistant I can consult with;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;‚ÄãUnderstanding how to train them.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;‚ÄãI am not an engineer, but I would like to explore this for fun.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MediumAd7537"&gt; /u/MediumAd7537 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omk3bz/i_want_to_start_my_first_homelab_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omk3bz/i_want_to_start_my_first_homelab_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1omk3bz/i_want_to_start_my_first_homelab_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T15:24:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1omhjf2</id>
    <title>Looking for models I can run on 16gbs of ram.</title>
    <updated>2025-11-02T13:40:26+00:00</updated>
    <author>
      <name>/u/Think_Question_6677</name>
      <uri>https://old.reddit.com/user/Think_Question_6677</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm aware ram is slow, but I'd like to try out some models on my laptop.&lt;/p&gt; &lt;p&gt;What are the best general purpose and coding models out there that will fit on 16gbs of ram and run on cpu (or an mx350 from nvidia)?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Think_Question_6677"&gt; /u/Think_Question_6677 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omhjf2/looking_for_models_i_can_run_on_16gbs_of_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omhjf2/looking_for_models_i_can_run_on_16gbs_of_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1omhjf2/looking_for_models_i_can_run_on_16gbs_of_ram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T13:40:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1omcjct</id>
    <title>Running Local LLM's Fascinates me - But I'm Absolutely LOST</title>
    <updated>2025-11-02T08:57:28+00:00</updated>
    <author>
      <name>/u/WhatsGoingOnERE</name>
      <uri>https://old.reddit.com/user/WhatsGoingOnERE</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I watched PewDiePie‚Äôs new video and now I‚Äôm obsessed with the idea of running models locally. He had a ‚Äúcouncil‚Äù of AIs talking to each other, then voting on the best answer. You can also fine tune and customise stuff, which sounds unreal.&lt;/p&gt; &lt;p&gt;Here‚Äôs my deal. I already pay for GPT-5 Pro and Claude Max and they are great. I want to know if I would actually see better performance by doing this locally, or if it‚Äôs just a fun rabbit hole.&lt;/p&gt; &lt;p&gt;Basically want to know if using these local models gets better results for anyone vs the best models available online, and if not, what are the other benefits?&lt;/p&gt; &lt;p&gt;I know privacy is a big one for some people, but lets ignore that for this case.&lt;/p&gt; &lt;p&gt;My main use cases are for business (SEO, SaaS, general marketing, business idea ideation, etc), and coding.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WhatsGoingOnERE"&gt; /u/WhatsGoingOnERE &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omcjct/running_local_llms_fascinates_me_but_im/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omcjct/running_local_llms_fascinates_me_but_im/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1omcjct/running_local_llms_fascinates_me_but_im/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T08:57:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1olxijp</id>
    <title>List of interesting open-source models released this month.</title>
    <updated>2025-11-01T20:03:07+00:00</updated>
    <author>
      <name>/u/Acrobatic-Tomato4862</name>
      <uri>https://old.reddit.com/user/Acrobatic-Tomato4862</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! I've been tracking the latest AI model releases and wanted to share a curated list of AI models released this month.&lt;/p&gt; &lt;p&gt;Credit to &lt;a href="/u/duarteeeeee"&gt;u/duarteeeeee&lt;/a&gt; for finding all these models.&lt;/p&gt; &lt;p&gt;Here's a chronological breakdown of some of the most interesting open models released around October 1st - 31st, 2025:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;October 1st:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/Liquid4All/liquid-audio"&gt;&lt;strong&gt;LFM2-Audio-1.5B&lt;/strong&gt;&lt;/a&gt; (Liquid AI): Low-latency, end-to-end audio foundation model.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/nineninesix/kani-tts-370m"&gt;&lt;strong&gt;KaniTTS-370M&lt;/strong&gt;&lt;/a&gt; (NineNineSix): Fast, open-source TTS for real-time applications.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 2nd:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.ibm.com/new/announcements/ibm-granite-4-0-hyper-efficient-high-performance-hybrid-models"&gt;&lt;strong&gt;Granite 4.0&lt;/strong&gt;&lt;/a&gt; (IBM): Hyper-efficient, hybrid models for enterprise use.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/neuphonic/neutts-air"&gt;&lt;strong&gt;NeuTTS Air&lt;/strong&gt;&lt;/a&gt; (Neuphonic Speech): On-device TTS with instant voice cloning.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 3rd:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://simular.ai/articles/agent-s3"&gt;&lt;strong&gt;Agent S3&lt;/strong&gt;&lt;/a&gt; (Simular): Open framework for human-like computer use.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/inclusionAI/Ming-UniVision-16B-A3B"&gt;&lt;strong&gt;Ming-UniVision-16B-A3B&lt;/strong&gt;&lt;/a&gt; (Ant Group): Unified vision understanding, generation, editing model.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/character-ai/Ovi"&gt;&lt;strong&gt;Ovi (TTV/ITV)&lt;/strong&gt;&lt;/a&gt; (Character.AI / Yale): Open-source framework for offline talking avatars.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Salesforce/CoDA-v0-Instruct"&gt;&lt;strong&gt;CoDA-v0-Instruct&lt;/strong&gt;&lt;/a&gt; (Salesforce AI Research): Bidirectional diffusion model for code generation.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 4th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-VL-30B-A3B-Instruct"&gt;&lt;strong&gt;Qwen3-VL-30B-A3B-Instruct&lt;/strong&gt;&lt;/a&gt; (Alibaba): Powerful vision-language model for agentic tasks.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/DecartAI/Decart-XR"&gt;&lt;strong&gt;DecartXR&lt;/strong&gt;&lt;/a&gt; (Decart AI): Open-source Quest app for realtime video-FX.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 7th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/LiquidAI/LFM2-8B-A1B"&gt;&lt;strong&gt;LFM2-8B-A1B&lt;/strong&gt;&lt;/a&gt; (Liquid AI): Efficient on-device mixture-of-experts model.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/Tencent-Hunyuan/HunyuanVision"&gt;&lt;strong&gt;Hunyuan-Vision-1.5-Thinking&lt;/strong&gt;&lt;/a&gt; (Tencent): Multimodal &amp;quot;thinking on images&amp;quot; reasoning model.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/bageldotcom/paris"&gt;&lt;strong&gt;Paris&lt;/strong&gt;&lt;/a&gt; (Bagel Network): Decentralized-trained open-weight diffusion model.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/cumulo-autumn/StreamDiffusion"&gt;&lt;strong&gt;StreamDiffusionV2&lt;/strong&gt;&lt;/a&gt; (UC Berkeley, MIT, et al.): Open-source pipeline for real-time video streaming.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 8th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/ai21labs/Jamba-v0.1"&gt;&lt;strong&gt;Jamba Reasoning 3B&lt;/strong&gt;&lt;/a&gt; (AI21 Labs): Small hybrid model for on-device reasoning.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/inclusionAI/Ring-1T"&gt;&lt;strong&gt;Ling-1T / Ring-1T&lt;/strong&gt;&lt;/a&gt; (Ant Group): Trillion-parameter thinking/non-thinking open models.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/TingtingLiao/mimix"&gt;&lt;strong&gt;Mimix&lt;/strong&gt;&lt;/a&gt; (Research): Framework for multi-character video generation.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 9th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/microsoft/UserLM-8b"&gt;&lt;strong&gt;UserLM-8b&lt;/strong&gt;&lt;/a&gt; (Microsoft): Open-weight model simulating a &amp;quot;user&amp;quot; role.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/RadicalNumerics/RND1"&gt;&lt;strong&gt;RND1-Base-0910&lt;/strong&gt;&lt;/a&gt; (Radical Numerics): Experimental diffusion language model (30B MoE).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 10th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Kwaipilot/KAT-Dev-72B-Exp"&gt;&lt;strong&gt;KAT-Dev-72B-Exp&lt;/strong&gt;&lt;/a&gt; (Kwaipilot): Open-source experimental model for agentic coding.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 12th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://pbihao.github.io/projects/DreamOmni2/"&gt;&lt;strong&gt;DreamOmni2&lt;/strong&gt;&lt;/a&gt; (ByteDance): Multimodal instruction-based image editing/generation.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 13th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/mit-han-lab/streaming-vlm"&gt;&lt;strong&gt;StreamingVLM&lt;/strong&gt;&lt;/a&gt; (MIT Han Lab): Real-time understanding for infinite video streams.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 14th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL"&gt;&lt;strong&gt;Qwen3-VL-4B / 8B&lt;/strong&gt;&lt;/a&gt; (Alibaba): Efficient, open vision-language models for edge.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 16th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/PaddlePaddle/PaddleOCR-VL"&gt;&lt;strong&gt;PaddleOCR-VL&lt;/strong&gt;&lt;/a&gt; (Baidu): Lightweight 109-language document parsing model.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/facebook/MobileLLM-Pro"&gt;&lt;strong&gt;MobileLLM-Pro&lt;/strong&gt;&lt;/a&gt; (Meta): 1B parameter on-device model (128k context).&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/Tencent-Hunyuan/HunyuanWorld-1.0"&gt;&lt;strong&gt;FlashWorld&lt;/strong&gt;&lt;/a&gt; (Tencent): Fast (5-10 sec) 3D scene generation.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 17th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/inclusionAI/LLaDA2.0-flash-preview"&gt;&lt;strong&gt;LLaDA2.0-flash-preview&lt;/strong&gt;&lt;/a&gt; (Ant Group): 100B MoE diffusion model for reasoning/code.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 20th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/deepseek-ai/DeepSeek-OCR"&gt;&lt;strong&gt;DeepSeek-OCR&lt;/strong&gt;&lt;/a&gt; (DeepseekAI): Open-source model for optical context-compression.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/krea-ai/realtime-video"&gt;&lt;strong&gt;Krea Realtime 14B&lt;/strong&gt;&lt;/a&gt; (Krea AI): 14B open-weight real-time video generation.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 21st:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-VL-32B-Instruct"&gt;&lt;strong&gt;Qwen3-VL-2B / 32B&lt;/strong&gt;&lt;/a&gt; (Alibaba): Open, dense VLMs for edge and cloud.&lt;/li&gt; &lt;li&gt;&lt;a href="https://arxiv.org/abs/2510.14876"&gt;&lt;strong&gt;BADAS-Open&lt;/strong&gt;&lt;/a&gt; (Nexar): Ego-centric collision prediction model for ADAS.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 22nd:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/LiquidAI/LFM2-VL-3B"&gt;&lt;strong&gt;LFM2-VL-3B&lt;/strong&gt;&lt;/a&gt; (Liquid AI): Efficient vision-language model for edge deployment.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/tencent/HunyuanWorld-1"&gt;&lt;strong&gt;HunyuanWorld-1.1&lt;/strong&gt;&lt;/a&gt; (Tencent): 3D world generation from multi-view/video.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/PokeeAI/pokee_research_7b"&gt;&lt;strong&gt;PokeeResearch-7B&lt;/strong&gt;&lt;/a&gt; (Pokee AI): Open 7B deep-research agent (search/synthesis).&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/allenai/olmOCR-2-7B-1025"&gt;&lt;strong&gt;olmOCR-2-7B-1025&lt;/strong&gt;&lt;/a&gt; (Allen Institute for AI): Open-source, single-pass PDF-to-structured-text model.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 23rd:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://ltx.video/"&gt;&lt;strong&gt;LTX 2&lt;/strong&gt;&lt;/a&gt; (Lightricks): Open-source 4K video engine for consumer GPUs.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/lightonai/LightOnOCR-1B-1025"&gt;&lt;strong&gt;LightOnOCR-1B&lt;/strong&gt;&lt;/a&gt; (LightOn): Fast, 1B-parameter open-source OCR VLM.&lt;/li&gt; &lt;li&gt;&lt;a href="https://holo-cine.github.io/"&gt;&lt;strong&gt;HoloCine&lt;/strong&gt;&lt;/a&gt; (Research): Model for holistic, multi-shot cinematic narratives.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 24th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/tahoebio/tahoe-x1"&gt;&lt;strong&gt;Tahoe-x1&lt;/strong&gt;&lt;/a&gt; (Tahoe Therapeutics): 3B open-source single-cell biology model.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/PRIME-RL/P1-30B-A3B"&gt;&lt;strong&gt;P1&lt;/strong&gt;&lt;/a&gt; (PRIME-RL): Model mastering Physics Olympiads with RL.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 25th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/meituan-longcat/LongCat-Video"&gt;&lt;strong&gt;LongCat-Video&lt;/strong&gt;&lt;/a&gt; (Meituan): 13.6B open model for long video generation.&lt;/li&gt; &lt;li&gt;&lt;a href="https://arxiv.org/html/2510.19944v1"&gt;&lt;strong&gt;Seed 3D 1.0&lt;/strong&gt;&lt;/a&gt; (ByteDance): Generates simulation-grade 3D assets from images.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 27th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.minimax.io/news/minimax-m2"&gt;&lt;strong&gt;Minimax M2&lt;/strong&gt;&lt;/a&gt; (Minimax): Open-sourced intelligence engine for agentic workflows.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/inclusionAI/Ming-flash-omni-Preview"&gt;&lt;strong&gt;Ming-flash-omni-Preview&lt;/strong&gt;&lt;/a&gt; (Ant Group): 100B MoE omni-modal model for perception.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/inclusionAI/LLaDA2.0-mini-preview"&gt;&lt;strong&gt;LLaDA2.0-mini-preview&lt;/strong&gt;&lt;/a&gt; (Ant Group): 16B MoE diffusion model for language.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 28th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/LiquidAI/LFM2-ColBERT-350M"&gt;&lt;strong&gt;LFM2-ColBERT-350M&lt;/strong&gt;&lt;/a&gt; (Liquid AI): Multilingual &amp;quot;late interaction&amp;quot; RAG retriever model.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/ibm-granite/granite-4.0-350m"&gt;&lt;strong&gt;Granite 4.0 Nano (1B / 350M)&lt;/strong&gt;&lt;/a&gt; (IBM): Smallest open models for on-device use.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/HKUDS/ViMax"&gt;&lt;strong&gt;ViMax&lt;/strong&gt;&lt;/a&gt; (HKUDS): Agentic framework for end-to-end video creation.&lt;/li&gt; &lt;li&gt;&lt;a href="https://build.nvidia.com/nvidia/nemotron-nano-12b-v2-vl/modelcard"&gt;&lt;strong&gt;Nemotron Nano v2 VL&lt;/strong&gt;&lt;/a&gt; (NVIDIA): 12B open model for multi-image/video understanding.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 29th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://openai.com/index/introducing-gpt-oss-safeguard/"&gt;&lt;strong&gt;gpt-oss-safeguard&lt;/strong&gt;&lt;/a&gt; (OpenAI): Open-weight reasoning models for safety classification.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/morphicfilms/frames-to-video"&gt;&lt;strong&gt;Frames to Video&lt;/strong&gt;&lt;/a&gt; (Morphic): Open-source model for keyframe video interpolation.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/Bria-AI/FIBO"&gt;&lt;strong&gt;Fibo&lt;/strong&gt;&lt;/a&gt; (Bria AI): SOTA open-source model (trained on licensed data).&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/ByteDance/Ouro-2.6B-Thinking"&gt;&lt;strong&gt;Bytedance Ouro 2.6b thinking and non thinking&lt;/strong&gt;&lt;/a&gt;: Small language models that punch above their weight. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 30th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/baaivision/Emu3.5"&gt;&lt;strong&gt;Emu3.5&lt;/strong&gt;&lt;/a&gt; (BAAI): Native multimodal model as a world learner.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/moonshotai/Kimi-Linear-48B-A3B-Instruct"&gt;&lt;strong&gt;Kimi-Linear-48B-A3B&lt;/strong&gt;&lt;/a&gt; (Moonshot AI): Long-context model using a linear-attention mechanism.&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.modelscope.cn/studios/BlinkDL/RWKV-CHN-2/summary"&gt;&lt;strong&gt;RWKV-7 G0a3 7.2B&lt;/strong&gt;&lt;/a&gt; (BlinkDL): A multilingual RNN-based large language model.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/alibaba/UI-Ins"&gt;&lt;strong&gt;UI-Ins-32B / 7B&lt;/strong&gt;&lt;/a&gt; (Alibaba): GUI grounding agent.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please correct me if I have misclassified/mislinked any of the above models. This is my first post, so I am expecting there might be some mistakes.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Acrobatic-Tomato4862"&gt; /u/Acrobatic-Tomato4862 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olxijp/list_of_interesting_opensource_models_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olxijp/list_of_interesting_opensource_models_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olxijp/list_of_interesting_opensource_models_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T20:03:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1omhijb</id>
    <title>Unhinged Uncensored Model Evolution: Feedback on Satyr V0.1 to Shape Future Releases!</title>
    <updated>2025-11-02T13:39:19+00:00</updated>
    <author>
      <name>/u/ThePantheonUnbound</name>
      <uri>https://old.reddit.com/user/ThePantheonUnbound</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I‚Äôm the creator of the unhinged and uncensored Satyr model (soon to be a model series). A couple of days ago, I noticed a Reddit post about a new uncensored model release called Apollo V0.1 by &lt;a href="/u/AllThingsIntel"&gt;u/AllThingsIntel&lt;/a&gt;. I tested it and found it to be as uncensored as my model, but more capable and versatile as a general assistant (without any extreme biases or a tendency to turn every single prompt NSFW). That‚Äôs the direction I want future Satyr releases to take, but I noticed far fewer interactions with their posts and far fewer downloads than my model has, which is a bit confusing to say the least.&lt;/p&gt; &lt;p&gt;People who have tested and used both models, please leave feedback on what you liked in each of the two, so I can understand the preferred direction for the Satyr model series.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThePantheonUnbound"&gt; /u/ThePantheonUnbound &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ol2oxw/unbound_incharacter_reasoning_model"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omhijb/unhinged_uncensored_model_evolution_feedback_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1omhijb/unhinged_uncensored_model_evolution_feedback_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T13:39:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1omhby8</id>
    <title>Qwen 3 max thinking released.</title>
    <updated>2025-11-02T13:31:06+00:00</updated>
    <author>
      <name>/u/JeffreySons_90</name>
      <uri>https://old.reddit.com/user/JeffreySons_90</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Try it &lt;a href="https://chat.qwen.ai/"&gt;https://chat.qwen.ai/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JeffreySons_90"&gt; /u/JeffreySons_90 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omhby8/qwen_3_max_thinking_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omhby8/qwen_3_max_thinking_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1omhby8/qwen_3_max_thinking_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T13:31:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohqev8</id>
    <title>Best Local TTS/STT Models - October 2025</title>
    <updated>2025-10-27T21:00:42+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Share what your favorite TTS / STT models are right now &lt;strong&gt;and why.&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Given the the amount of ambiguity and subjectivity in rating/testing these models, please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc. Closed models like Elevenlabs v3 seem to continue to be a few levels above open models, so comparisons, especially empirical ones are welcome.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please use the top level TTS/STT comments to thread your responses. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohqev8/best_local_ttsstt_models_october_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohqev8/best_local_ttsstt_models_october_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohqev8/best_local_ttsstt_models_october_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T21:00:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1olq14f</id>
    <title>[MEGATHREAD] Local AI Hardware - November 2025</title>
    <updated>2025-11-01T15:02:17+00:00</updated>
    <author>
      <name>/u/eck72</name>
      <uri>https://old.reddit.com/user/eck72</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is the monthly thread for sharing your local AI setups and the models you're running.&lt;/p&gt; &lt;p&gt;Whether you're using a single CPU, a gaming GPU, or a full rack, post what you're running and how it performs.&lt;/p&gt; &lt;p&gt;Post in any format you like. The list below is just a guide:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Hardware: CPU, GPU(s), RAM, storage, OS&lt;/li&gt; &lt;li&gt;Model(s): name + size/quant&lt;/li&gt; &lt;li&gt;Stack: (e.g. llama.cpp + custom UI)&lt;/li&gt; &lt;li&gt;Performance: t/s, latency, context, batch etc.&lt;/li&gt; &lt;li&gt;Power consumption&lt;/li&gt; &lt;li&gt;Notes: purpose, quirks, comments&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please share setup pics for eye candy!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Quick reminder&lt;/strong&gt;: You can share hardware purely to ask questions or get feedback. All experience levels welcome.&lt;/p&gt; &lt;p&gt;House rules: no buying/selling/promo.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eck72"&gt; /u/eck72 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T15:02:17+00:00</published>
  </entry>
</feed>
