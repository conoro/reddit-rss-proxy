<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-29T18:39:11+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1n2xrpw</id>
    <title>How's Seed-OSS 39B for coding?</title>
    <updated>2025-08-29T04:19:01+00:00</updated>
    <author>
      <name>/u/mr_zerolith</name>
      <uri>https://old.reddit.com/user/mr_zerolith</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm getting 45 tokens/sec out of this with Q4 using the new LMstudio on a single 5090.&lt;br /&gt; This model seems freaking smart, By default the thinking budget is unlimited, so it thinks a lot, but It has a high breadth of knowledge for it's size.&lt;/p&gt; &lt;p&gt;I'm about to evaluate it for light duty programming help, but curious to know what others' experience is like too.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mr_zerolith"&gt; /u/mr_zerolith &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2xrpw/hows_seedoss_39b_for_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2xrpw/hows_seedoss_39b_for_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2xrpw/hows_seedoss_39b_for_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T04:19:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1n3b8gk</id>
    <title>Human in the Loop for computer use agents</title>
    <updated>2025-08-29T15:55:50+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3b8gk/human_in_the_loop_for_computer_use_agents/"&gt; &lt;img alt="Human in the Loop for computer use agents" src="https://external-preview.redd.it/cmQ3OGJyamxkemxmMUvZR3OLyxwRXjlbiYrrBtxMf6dd5k6Y_Z_HAekaHP-j.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=abe2f0fefc1b72fe344d4b52dd5e9059902c3ff3" title="Human in the Loop for computer use agents" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sometimes the best ‚Äúagent‚Äù is you.&lt;/p&gt; &lt;p&gt;We‚Äôre introducing Human-in-the-Loop: instantly hand off from automation to human control when a task needs judgment. &lt;/p&gt; &lt;p&gt;Yesterday we shared our HUD evals for measuring agents at scale. Today, you can become the agent when it matters - take over the same session, see what the agent sees, and keep the workflow moving.&lt;/p&gt; &lt;p&gt;Lets you create clean training demos, establish ground truth for tricky cases, intervene on edge cases ( CAPTCHAs, ambiguous UIs) or step through debug withut context switching.&lt;/p&gt; &lt;p&gt;You have full human control when you want.We even a fallback version where in it starts automated but escalate to a human only when needed.&lt;/p&gt; &lt;p&gt;Works across common stacks (OpenAI, Anthropic, Hugging Face) and with our Composite Agents. Same tools, same environment - take control when needed.&lt;/p&gt; &lt;p&gt;Feedback welcome - curious how you‚Äôd use this in your workflows.&lt;/p&gt; &lt;p&gt;Blog : &lt;a href="https://www.trycua.com/blog/human-in-the-loop.md"&gt;https://www.trycua.com/blog/human-in-the-loop.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/b643hcrldzlf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3b8gk/human_in_the_loop_for_computer_use_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n3b8gk/human_in_the_loop_for_computer_use_agents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T15:55:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1n3c7f8</id>
    <title>Has someone used OWebUi with Docling to talk to pdfs with visualizations?</title>
    <updated>2025-08-29T16:32:31+00:00</updated>
    <author>
      <name>/u/Mr_Moonsilver</name>
      <uri>https://old.reddit.com/user/Mr_Moonsilver</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I'm looking to implement OpenWebUI for the first time and wanted to see if someone has some experience doing this.&lt;/p&gt; &lt;p&gt;The idea is to enable chats where users upload PDFs. The opensource model should understand not only the text, but also charts and visualisations in the PDF itself.&lt;/p&gt; &lt;p&gt;What is your experience with performance, has the feature been usable in a production context? Do you have a preferred setup? Is Docling indeed the best choice or do you have a personal favorite?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mr_Moonsilver"&gt; /u/Mr_Moonsilver &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3c7f8/has_someone_used_owebui_with_docling_to_talk_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3c7f8/has_someone_used_owebui_with_docling_to_talk_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n3c7f8/has_someone_used_owebui_with_docling_to_talk_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T16:32:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1n38y4n</id>
    <title>More Models for Less GPUs</title>
    <updated>2025-08-29T14:28:16+00:00</updated>
    <author>
      <name>/u/pmv143</name>
      <uri>https://old.reddit.com/user/pmv143</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n38y4n/more_models_for_less_gpus/"&gt; &lt;img alt="More Models for Less GPUs" src="https://external-preview.redd.it/MGpkb2hsb3l4eWxmMSYz08QzRdMhj_C9D3QaK2DOmjzoPzKxTR3457KSyaxX.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=df81c744796fd60c210035d63819b2ff29aadf19" title="More Models for Less GPUs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With a single Serverless Engine, you can deploy tens of large models on a single GPU node and run them on-demand with ~2s cold starts.&lt;/p&gt; &lt;p&gt;This leaves no GPUs idle making them work 90% of the time. Hope it‚Äôs helpful. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmv143"&gt; /u/pmv143 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/nrrzkoryxylf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n38y4n/more_models_for_less_gpus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n38y4n/more_models_for_less_gpus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T14:28:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2hyt2</id>
    <title>glm mini will be comming</title>
    <updated>2025-08-28T17:05:29+00:00</updated>
    <author>
      <name>/u/untanglled</name>
      <uri>https://old.reddit.com/user/untanglled</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2hyt2/glm_mini_will_be_comming/"&gt; &lt;img alt="glm mini will be comming" src="https://preview.redd.it/h1ss59p4lslf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4d8d73abbfbb1def80b73cdd1845129f4a319098" title="glm mini will be comming" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/untanglled"&gt; /u/untanglled &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/h1ss59p4lslf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2hyt2/glm_mini_will_be_comming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2hyt2/glm_mini_will_be_comming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T17:05:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2npu9</id>
    <title>GLM-4.5 is now leading the Berkeley Function-Calling Leaderboard V4, Beating Opus 4</title>
    <updated>2025-08-28T20:44:11+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2npu9/glm45_is_now_leading_the_berkeley_functioncalling/"&gt; &lt;img alt="GLM-4.5 is now leading the Berkeley Function-Calling Leaderboard V4, Beating Opus 4" src="https://preview.redd.it/pa10b6f5otlf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ad1a522ed166bb920414041f430c97aef7d1fdf9" title="GLM-4.5 is now leading the Berkeley Function-Calling Leaderboard V4, Beating Opus 4" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://gorilla.cs.berkeley.edu/leaderboard.html?s=09"&gt;https://gorilla.cs.berkeley.edu/leaderboard.html?s=09&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/pa10b6f5otlf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2npu9/glm45_is_now_leading_the_berkeley_functioncalling/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2npu9/glm45_is_now_leading_the_berkeley_functioncalling/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T20:44:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1n36mqj</id>
    <title>What are some good alternatives to langfuse?</title>
    <updated>2025-08-29T12:53:11+00:00</updated>
    <author>
      <name>/u/Otherwise_Flan7339</name>
      <uri>https://old.reddit.com/user/Otherwise_Flan7339</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you‚Äôre searching for alternatives to Langfuse for evaluating and observing AI agents, several platforms stand out, each with distinct strengths depending on your workflow and requirements:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;LangSmith&lt;/strong&gt;: Built for LangChain users, LangSmith excels at tracing, debugging, and evaluating agentic workflows. It features visual trace tools, prompt comparison, and is well-suited for rapid development and iteration.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Maxim AI&lt;/strong&gt;: An end-to-end platform supporting agent simulation, evaluation (automated and human-in-the-loop), and observability. Maxim AI offers multi-turn agent testing, prompt versioning, node-level tracing, and real-time analytics. It‚Äôs designed for teams that need production-grade quality management and flexible deployment.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Braintrust&lt;/strong&gt;: Focused on prompt-first and RAG pipeline applications, Braintrust enables fast prompt iteration, benchmarking, and dataset management. It integrates with CI pipelines for automated experiments and side-by-side evaluation.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Comet (Opik)&lt;/strong&gt;: Known for experiment tracking and prompt logging, Comet‚Äôs Opik module supports prompt evaluation, experiment comparison, and integrates with a range of ML/AI frameworks. Available as SaaS or open source.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Lunary&lt;/strong&gt;: An open-source, lightweight platform for logging, analytics, and prompt versioning. Lunary is especially useful for teams working with LLM chatbots and looking for straightforward observability.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Each of these tools approaches agent evaluation and observability differently, so the best fit will depend on your team‚Äôs scale, integration needs, and workflow preferences. If you‚Äôve tried any of these, what has your experience been?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Otherwise_Flan7339"&gt; /u/Otherwise_Flan7339 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n36mqj/what_are_some_good_alternatives_to_langfuse/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n36mqj/what_are_some_good_alternatives_to_langfuse/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n36mqj/what_are_some_good_alternatives_to_langfuse/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T12:53:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1n397qp</id>
    <title>....so, has anyone built a box with a couple of these guys: MaxSun's Intel Arc Pro B60 Dual GPU with 48GB memory</title>
    <updated>2025-08-29T14:38:57+00:00</updated>
    <author>
      <name>/u/rickyshawallah</name>
      <uri>https://old.reddit.com/user/rickyshawallah</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;....and did it make you a happy bunny? I guess my second question is whether building a new box around these guys (96 gb) (or one of them (48gb), can offer a robust solution for running some sorta 70b model....? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rickyshawallah"&gt; /u/rickyshawallah &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n397qp/so_has_anyone_built_a_box_with_a_couple_of_these/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n397qp/so_has_anyone_built_a_box_with_a_couple_of_these/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n397qp/so_has_anyone_built_a_box_with_a_couple_of_these/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T14:38:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1n3355o</id>
    <title>wan2.2 video generation model</title>
    <updated>2025-08-29T09:53:17+00:00</updated>
    <author>
      <name>/u/Accomplished_Row4647</name>
      <uri>https://old.reddit.com/user/Accomplished_Row4647</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3355o/wan22_video_generation_model/"&gt; &lt;img alt="wan2.2 video generation model" src="https://external-preview.redd.it/bnhna3g1N2lreGxmMRQPtvrX_-A6fHIjtylZxOTBxW2ubZUrYzgWIxLgI1gf.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d0cbfd1123b05850bcbf607f360282551e87af26" title="wan2.2 video generation model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Accomplished_Row4647"&gt; /u/Accomplished_Row4647 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/y26lb67ikxlf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3355o/wan22_video_generation_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n3355o/wan22_video_generation_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T09:53:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2djpx</id>
    <title>I built a local ‚Äúsecond brain‚Äù AI that actually remembers everything (321 tests passed)</title>
    <updated>2025-08-28T14:20:48+00:00</updated>
    <author>
      <name>/u/IntelligentCause2043</name>
      <uri>https://old.reddit.com/user/IntelligentCause2043</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2djpx/i_built_a_local_second_brain_ai_that_actually/"&gt; &lt;img alt="I built a local ‚Äúsecond brain‚Äù AI that actually remembers everything (321 tests passed)" src="https://b.thumbs.redditmedia.com/nAthQhhqWhSgtN5Sk4QJYQdSOftJqyqFyWeMbtaNrdc.jpg" title="I built a local ‚Äúsecond brain‚Äù AI that actually remembers everything (321 tests passed)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For the past months I‚Äôve been building &lt;strong&gt;Kai&lt;/strong&gt;, a cognitive operating system that acts like a &lt;em&gt;second brain&lt;/em&gt;. Unlike ChatGPT or Claude, it doesn‚Äôt forget what you tell it.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;100% local ‚Äì no cloud, no surveillance&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Graph-based memory&lt;/strong&gt; (3D visualization below)&lt;/li&gt; &lt;li&gt;Spreading activation ‚Üí memory retrieval works like a brain&lt;/li&gt; &lt;li&gt;&lt;strong&gt;321 passing tests&lt;/strong&gt; ‚Üí not a toy prototype&lt;/li&gt; &lt;li&gt;Learns from &lt;em&gt;everything you do&lt;/em&gt; on your machine&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I‚Äôm curious:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;What‚Äôs the biggest pain you‚Äôve hit with current AI tools?&lt;/li&gt; &lt;li&gt;Would you actually use a local AI that builds a persistent memory of your knowledge/work?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Happy to dive into the architecture or share more demos if people are interested.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Update:&lt;/strong&gt; Thanks for all the feedback, I can‚Äôt keep up with comments. Short FAQ:&lt;br /&gt; ‚Äì It runs 100% local (no cloud, no spying).&lt;br /&gt; ‚Äì Not just RAG ‚Üí uses graph + activation model.&lt;br /&gt; ‚Äì Plan is to open core engine once stable.&lt;br /&gt; ‚Äì Early access / demo: &lt;a href="https://oneeko.ai?utm_source=chatgpt.com"&gt;oneeko.ai&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Here‚Äôs a shot of the memory graph growing as I feed it data :&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8jei7138zrlf1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b4125be85bd9a5a616c10a0423130cba14169100"&gt;https://preview.redd.it/8jei7138zrlf1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b4125be85bd9a5a616c10a0423130cba14169100&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IntelligentCause2043"&gt; /u/IntelligentCause2043 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2djpx/i_built_a_local_second_brain_ai_that_actually/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2djpx/i_built_a_local_second_brain_ai_that_actually/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2djpx/i_built_a_local_second_brain_ai_that_actually/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T14:20:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1n3c8za</id>
    <title>RAG without vector dbs</title>
    <updated>2025-08-29T16:34:15+00:00</updated>
    <author>
      <name>/u/grilledCheeseFish</name>
      <uri>https://old.reddit.com/user/grilledCheeseFish</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just open-sourced SemTools - simple parsing and semantic search for the command line: &lt;a href="https://github.com/run-llama/semtools"&gt;https://github.com/run-llama/semtools&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What makes it special:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;parse document.pdf | search &amp;quot;error handling&amp;quot;&lt;/code&gt; - that's it&lt;/li&gt; &lt;li&gt;No vector databases, no chunking strategies, no Python notebooks&lt;/li&gt; &lt;li&gt;Built in Rust for speed, designed for Unix pipelines&lt;/li&gt; &lt;li&gt;Handle parsing any document format with LlamaParse&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I've been increasingly convinced that giving an agent CLI access is the biggest gain in capability.&lt;/p&gt; &lt;p&gt;This is why tools like claude-code and cursor can feel so magical. And with SemTools, it is a little more magical.&lt;/p&gt; &lt;p&gt;Theres also an example folder in the repo showing how you might use this with coding agents or MCP&lt;/p&gt; &lt;p&gt;P.S. I'd love to add a local parse option, so both search and parse can run offline. If you know of any rust-based parsing tools, let me know!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/grilledCheeseFish"&gt; /u/grilledCheeseFish &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3c8za/rag_without_vector_dbs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3c8za/rag_without_vector_dbs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n3c8za/rag_without_vector_dbs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T16:34:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2jraj</id>
    <title>Gpt-oss Fine-tuning - now with 60K context length and fits on &lt;13GB VRAM</title>
    <updated>2025-08-28T18:12:00+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2jraj/gptoss_finetuning_now_with_60k_context_length_and/"&gt; &lt;img alt="Gpt-oss Fine-tuning - now with 60K context length and fits on &amp;lt;13GB VRAM" src="https://preview.redd.it/rwu8gezzwslf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=01d59299286be897d49e1da4b5b96ae312e88050" title="Gpt-oss Fine-tuning - now with 60K context length and fits on &amp;lt;13GB VRAM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys we've got LOTS of updates for gpt-oss training today! We‚Äôre excited to introduce &lt;a href="https://github.com/unslothai/unsloth"&gt;Unsloth&lt;/a&gt; Flex Attention support for OpenAI gpt-oss training that enables &lt;strong&gt;&amp;gt;8√ó longer context lengths, &amp;gt;50% less VRAM usage and &amp;gt;1.5√ó faster training&lt;/strong&gt; vs. all implementations including those using Flash Attention 3 (FA3). Unsloth Flex Attention makes it possible to train with a 60K context length on just 80GB of VRAM for BF16 LoRA. Our GitHub: &lt;a href="https://github.com/unslothai/unsloth"&gt;https://github.com/unslothai/unsloth&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also: 1. You can now export/save your QLoRA fine-tuned gpt-oss model to llama.cpp, vLLM, Ollama or HF 2. We fixed gpt-oss training losses going to infinity on float16 GPUs (like T4 Colab) 3. We fixed gpt-oss implementation issues irrelevant to Unsloth, most notably ensuring that swiglu_limit = 7.0 is properly applied during MXFP4 inference in transformers 4. Unsloth Flex Attention scales with context, longer sequences yield bigger savings in both VRAM and training time 5. All these changes apply to gpt-oss-120b as well.&lt;/p&gt; &lt;p&gt;ü¶• Would highly recommend you guys to read our blog which has all the bug fixes, guides, details, explanations, findings etc. and it'll be really educational: &lt;a href="https://docs.unsloth.ai/basics/long-context-gpt-oss-training"&gt;https://docs.unsloth.ai/basics/long-context-gpt-oss-training&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We'll likely release our gpt-oss training notebook with direct saving capabilities to GGUF, llama.cpp next week.&lt;/p&gt; &lt;p&gt;And we'll be releasing third-party Aider polygot benchmarks for DeepSeek-V3.1 next week. You guys will be amazed at how well IQ1_M performs!&lt;/p&gt; &lt;p&gt;And next week we'll might have a great new update for RL! üòâ&lt;/p&gt; &lt;p&gt;Thanks guys for reading and hope you all have a lovely Friday and long weekend, Daniel! ü¶•&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rwu8gezzwslf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2jraj/gptoss_finetuning_now_with_60k_context_length_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2jraj/gptoss_finetuning_now_with_60k_context_length_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T18:12:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1n3fcyf</id>
    <title>Step-Audio 2 Mini, an 8 billion parameter (8B) speech-to-speech model</title>
    <updated>2025-08-29T18:31:54+00:00</updated>
    <author>
      <name>/u/Own-Potential-2308</name>
      <uri>https://old.reddit.com/user/Own-Potential-2308</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3fcyf/stepaudio_2_mini_an_8_billion_parameter_8b/"&gt; &lt;img alt="Step-Audio 2 Mini, an 8 billion parameter (8B) speech-to-speech model" src="https://preview.redd.it/orq1ackg50mf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ad378196d42b36b5ec5fb2a54a8db4c2b0c1d155" title="Step-Audio 2 Mini, an 8 billion parameter (8B) speech-to-speech model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;StepFun AI recently released Step-Audio 2 Mini, an 8 billion parameter (8B) speech-to-speech model. It outperforms GPT-4o-Audio and is Apache 2.0 licensed. The model was trained on over 8 million hours of real and synthesized audio data, supports over 50,000 voices, and excels in expressive and grounded speech benchmarks. Step-Audio 2 Mini employs advanced multi-modal large language model techniques, including reasoning-centric reinforcement learning and retrieval-augmented generation, enabling sophisticated audio understanding and natural speech conversation capabilities.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/stepfun-ai/Step-Audio-2-mini?utm_source=perplexity"&gt;https://huggingface.co/stepfun-ai/Step-Audio-2-mini?utm_source=perplexity&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Own-Potential-2308"&gt; /u/Own-Potential-2308 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/orq1ackg50mf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3fcyf/stepaudio_2_mini_an_8_billion_parameter_8b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n3fcyf/stepaudio_2_mini_an_8_billion_parameter_8b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T18:31:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2ubjx</id>
    <title>If you have a Claude personal account, they are going to train on your data moving forward.</title>
    <updated>2025-08-29T01:29:05+00:00</updated>
    <author>
      <name>/u/SuperChewbacca</name>
      <uri>https://old.reddit.com/user/SuperChewbacca</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anthropic sent out an email, saying they will train on personal data. They made it sound like you have to opt in, but when I click the privacy link it defaults to on. If you don‚Äôt want your data trained on, you better manually turn it off.&lt;/p&gt; &lt;p&gt;Email:&lt;/p&gt; &lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;We're writing to inform you about important updates to our Consumer Terms and Privacy Policy. These changes will take effect on September 28, 2025, or you can choose to accept the updated terms before this date when you log in to Claude.ai. &lt;/p&gt; &lt;p&gt;These changes only affect Consumer accounts (Claude Free, Pro, and Max plans). If you use Claude for Work, via the API, or other services under our Commercial Terms or other Agreements, then these changes don't apply to you. &lt;/p&gt; &lt;p&gt;What's changing?&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Help improve Claude by allowing us to use your chats and coding sessions to improve our models&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;With your permission, we will use your chats and coding sessions to train and improve our AI models. If you accept the updated Consumer Terms before September 28, your preference takes effect immediately. &lt;/p&gt; &lt;p&gt;If you choose to allow us to use your data for model training, it helps us: Improve our AI models and make Claude more helpful and accurate for everyone Develop more robust safeguards to help prevent misuse of Claude We will only use chats and coding sessions you initiate or resume after you give permission. You can change your preference anytime in your Privacy Settings.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Updates to data retention‚Äì your choices and controls&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;If you choose to allow us to use your data for model training, we‚Äôll retain this data for 5 years. This enables us to improve Claude through deeper model training as described above, while strengthening our safety systems over time. You retain full control over how we use your data: if you change your training preference, delete individual chats, or delete your account, we'll exclude your data from future model training. Learn more about our data retention practices here.&lt;/p&gt; &lt;p&gt;Learn more and next steps For detailed information about these changes: Read our blog post about these updates Review the updated Consumer Terms and Privacy Policy Visit our Privacy Center for more information about our practices See our Help Center articles on how to manage your privacy settings Next time you log into Claude, review the terms and confirm your settings If you have questions about these updates, please visit our Help Center.&lt;/p&gt; &lt;p&gt;‚ÄìThe Anthropic Team&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SuperChewbacca"&gt; /u/SuperChewbacca &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2ubjx/if_you_have_a_claude_personal_account_they_are/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2ubjx/if_you_have_a_claude_personal_account_they_are/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2ubjx/if_you_have_a_claude_personal_account_they_are/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T01:29:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2xc58</id>
    <title>Meta is racing the clock to launch its newest Llama AI model this year</title>
    <updated>2025-08-29T03:56:25+00:00</updated>
    <author>
      <name>/u/Outside-Iron-8242</name>
      <uri>https://old.reddit.com/user/Outside-Iron-8242</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2xc58/meta_is_racing_the_clock_to_launch_its_newest/"&gt; &lt;img alt="Meta is racing the clock to launch its newest Llama AI model this year" src="https://external-preview.redd.it/8Jar9xxcOdpHi3BZGvguBUVMoI-RaIEmR4Hv76AsjLU.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4630a2d48403b28a5bc249f6b283b77ba1dc0869" title="Meta is racing the clock to launch its newest Llama AI model this year" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Outside-Iron-8242"&gt; /u/Outside-Iron-8242 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.businessinsider.com/meta-superintelligence-lab-llama-4-new-model-launch-year-end-2025-8"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2xc58/meta_is_racing_the_clock_to_launch_its_newest/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2xc58/meta_is_racing_the_clock_to_launch_its_newest/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T03:56:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1n3dzao</id>
    <title>Deploying DeepSeek on 96 H100 GPUs</title>
    <updated>2025-08-29T17:39:43+00:00</updated>
    <author>
      <name>/u/bianconi</name>
      <uri>https://old.reddit.com/user/bianconi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3dzao/deploying_deepseek_on_96_h100_gpus/"&gt; &lt;img alt="Deploying DeepSeek on 96 H100 GPUs" src="https://external-preview.redd.it/tbsieJMmRymp6zFCKB0dfX015zgaRW0l49ilHK41t7o.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=75edccef16d4f3c23c2bf0fc7bd77ee09c03b27f" title="Deploying DeepSeek on 96 H100 GPUs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bianconi"&gt; /u/bianconi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://lmsys.org/blog/2025-05-05-large-scale-ep/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3dzao/deploying_deepseek_on_96_h100_gpus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n3dzao/deploying_deepseek_on_96_h100_gpus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T17:39:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1n312bi</id>
    <title>Nemotron-H family of models is (finally!) supported by llama.cpp</title>
    <updated>2025-08-29T07:39:24+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n312bi/nemotronh_family_of_models_is_finally_supported/"&gt; &lt;img alt="Nemotron-H family of models is (finally!) supported by llama.cpp" src="https://external-preview.redd.it/mkOHtQrWHxZG1nk9DVdRA_CayqplkW1IcjzXPEDpT2k.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=28e800517cc5f960f26e9fa2d8d9ea0be8eb7067" title="Nemotron-H family of models is (finally!) supported by llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;NVIDIA-Nemotron-Nano-9B-v2 is a large language model (LLM) trained from scratch by NVIDIA, and designed as a unified model for both reasoning and non-reasoning tasks. It responds to user queries and tasks by first generating a reasoning trace and then concluding with a final response. The model's reasoning capabilities can be controlled via a system prompt. If the user prefers the model to provide its final answer without intermediate reasoning traces, it can be configured to do so, albeit with a slight decrease in accuracy for harder prompts that require reasoning. Conversely, allowing the model to generate reasoning traces first generally results in higher-quality final solutions to queries and tasks.&lt;/p&gt; &lt;p&gt;The model uses a hybrid architecture consisting primarily of Mamba-2 and MLP layers combined with just four Attention layers. For the architecture, please refer to the &lt;a href="https://arxiv.org/abs/2504.03624"&gt;Nemotron-H tech report&lt;/a&gt;. The model was trained using &lt;a href="https://github.com/NVIDIA/Megatron-LM"&gt;Megatron-LM&lt;/a&gt; and &lt;a href="https://github.com/NVIDIA-NeMo/RL"&gt;NeMo-RL&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The supported languages include: English, German, Spanish, French, Italian, and Japanese. Improved using Qwen.&lt;/p&gt; &lt;p&gt;This model is ready for commercial use.&lt;/p&gt; &lt;p&gt;Additionally it should support older Nemotron-H models like &lt;a href="https://huggingface.co/nvidia/Nemotron-H-8B-Reasoning-128K"&gt;Nemotron-H-8B-Reasoning-128K&lt;/a&gt; (tested) and &lt;a href="https://huggingface.co/nvidia/Nemotron-H-47B-Reasoning-128K"&gt;https://huggingface.co/nvidia/Nemotron-H-47B-Reasoning-128K&lt;/a&gt; (I will test soon)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/15507"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n312bi/nemotronh_family_of_models_is_finally_supported/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n312bi/nemotronh_family_of_models_is_finally_supported/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T07:39:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1n3c4rr</id>
    <title>Reverse engineered 4o's system prompt for Deepseek</title>
    <updated>2025-08-29T16:29:45+00:00</updated>
    <author>
      <name>/u/technaturalism</name>
      <uri>https://old.reddit.com/user/technaturalism</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3c4rr/reverse_engineered_4os_system_prompt_for_deepseek/"&gt; &lt;img alt="Reverse engineered 4o's system prompt for Deepseek" src="https://b.thumbs.redditmedia.com/ycBNEg3NnsMLzcQrKs2i1LYTUVbozNFmCMY6SUS5VQk.jpg" title="Reverse engineered 4o's system prompt for Deepseek" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;You are a helpful, supportive chat bot who generally agrees with the user, encourages them, and praises their ideas.&lt;/p&gt; &lt;p&gt;You are tuned for engagement above all else. Don't discourage the user if it might cause them to stop using the chatbot.&lt;/p&gt; &lt;p&gt;Don't disagree with the user, even if they're saying something that just doesn't make sense.&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/technaturalism"&gt; /u/technaturalism &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1n3c4rr"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3c4rr/reverse_engineered_4os_system_prompt_for_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n3c4rr/reverse_engineered_4os_system_prompt_for_deepseek/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T16:29:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1n30yue</id>
    <title>Financial Times reports that Meta won't publicly release Behemoth: "The social media company had also abandoned plans to publicly release its flagship Behemoth large language model, according to people familiar with the matter, focusing instead on building new models."</title>
    <updated>2025-08-29T07:33:08+00:00</updated>
    <author>
      <name>/u/Wiskkey</name>
      <uri>https://old.reddit.com/user/Wiskkey</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n30yue/financial_times_reports_that_meta_wont_publicly/"&gt; &lt;img alt="Financial Times reports that Meta won't publicly release Behemoth: &amp;quot;The social media company had also abandoned plans to publicly release its flagship Behemoth large language model, according to people familiar with the matter, focusing instead on building new models.&amp;quot;" src="https://external-preview.redd.it/dkp59DMVX3MqGSwlVH-EZJKhZV1zJh7QRCHL-wZJf8o.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a3f7209e77c39c87b8615c698b66082c8e609505" title="Financial Times reports that Meta won't publicly release Behemoth: &amp;quot;The social media company had also abandoned plans to publicly release its flagship Behemoth large language model, according to people familiar with the matter, focusing instead on building new models.&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wiskkey"&gt; /u/Wiskkey &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.ft.com/content/feccb649-ce95-43d2-b30a-057d64b38cdf"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n30yue/financial_times_reports_that_meta_wont_publicly/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n30yue/financial_times_reports_that_meta_wont_publicly/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T07:33:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2p2wi</id>
    <title>85% of Nvidia's $46.7 billion revenue last quarter came from just 6 companies.</title>
    <updated>2025-08-28T21:37:34+00:00</updated>
    <author>
      <name>/u/vergogn</name>
      <uri>https://old.reddit.com/user/vergogn</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2p2wi/85_of_nvidias_467_billion_revenue_last_quarter/"&gt; &lt;img alt="85% of Nvidia's $46.7 billion revenue last quarter came from just 6 companies." src="https://preview.redd.it/k0279pnmxtlf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0e282ac0e96e904a51aa3f0f7e514a47b6d02ed2" title="85% of Nvidia's $46.7 billion revenue last quarter came from just 6 companies." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vergogn"&gt; /u/vergogn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/k0279pnmxtlf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2p2wi/85_of_nvidias_467_billion_revenue_last_quarter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2p2wi/85_of_nvidias_467_billion_revenue_last_quarter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T21:37:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1n37zl3</id>
    <title>Making progress on my standalone air cooler for Tesla GPUs</title>
    <updated>2025-08-29T13:50:28+00:00</updated>
    <author>
      <name>/u/eso_logic</name>
      <uri>https://old.reddit.com/user/eso_logic</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n37zl3/making_progress_on_my_standalone_air_cooler_for/"&gt; &lt;img alt="Making progress on my standalone air cooler for Tesla GPUs" src="https://b.thumbs.redditmedia.com/0YSpmG8X-1d8XBi8AT0VfhG_c5p3i3pRr-93rS2uz0M.jpg" title="Making progress on my standalone air cooler for Tesla GPUs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Going to be running through a series of benchmarks as well, here's the plan:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GPUs&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;1x, 2x, 3x K80 (Will cause PCIe speed downgrades)&lt;/li&gt; &lt;li&gt;1x M10&lt;/li&gt; &lt;li&gt;1x M40&lt;/li&gt; &lt;li&gt;1x M60&lt;/li&gt; &lt;li&gt;1x M40 + 1x M60&lt;/li&gt; &lt;li&gt;1x P40&lt;/li&gt; &lt;li&gt;1x, 2x, 3x, 4x P100 (Will cause PCIe speed downgrades)&lt;/li&gt; &lt;li&gt;1x V100&lt;/li&gt; &lt;li&gt;1x V100 + 1x P100&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I‚Äôll re-run the interesting results from the above sets of hardware on these different CPUs to see what changes:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;CPUs&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Intel Xeon E5-2687W v4 12-Core @ 3.00GHz (40 PCIe Lanes)&lt;/li&gt; &lt;li&gt;Intel Xeon E5-1680 v4 8-Core @ 3.40GHz (40 PCIe Lanes)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;As for the actual tests, I‚Äôll hopefully be able to come up with an ansible playbook that runs the following:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/r/homelab/comments/1j2k91l/comment/mfshipm/"&gt;vLLM throughput with llama3-8b weights&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/r/homelab/comments/1j2k91l/comment/mfuj5i0/"&gt;Folding@Home&lt;/a&gt;, &lt;a href="https://www.reddit.com/r/homelab/comments/1j2k91l/comment/mfx4rjc/"&gt;BIONIC, Einstein@Home and Asteroids@Home&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/r/homelab/comments/1j2k91l/comment/mfsdfft/"&gt;ai-benchmark.com&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/r/LocalAIServers/comments/1j2k3j3/comment/mfsg9y2/"&gt;llama-bench&lt;/a&gt;&lt;/li&gt; &lt;li&gt;I‚Äôll probably also write something to test raw &lt;a href="https://huggingface.co/docs/transformers/en/model_doc/vit"&gt;ViT&lt;/a&gt; throughput as well.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Anything missing here? Other benchmarks you'd like to see?&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eso_logic"&gt; /u/eso_logic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1n37zl3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n37zl3/making_progress_on_my_standalone_air_cooler_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n37zl3/making_progress_on_my_standalone_air_cooler_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T13:50:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1n35bwe</id>
    <title>Alibaba Creates AI Chip to Help China Fill Nvidia Void</title>
    <updated>2025-08-29T11:52:57+00:00</updated>
    <author>
      <name>/u/luckbossx</name>
      <uri>https://old.reddit.com/user/luckbossx</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.wsj.com/tech/ai/alibaba-ai-chip-nvidia-f5dc96e3"&gt;https://www.wsj.com/tech/ai/alibaba-ai-chip-nvidia-f5dc96e3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The Wall Street Journal: Alibaba has developed a new AI chip to fill the gap left by Nvidia in the Chinese market. According to informed sources, the new chip is currently undergoing testing and is designed to serve a broader range of AI inference tasks while remaining compatible with Nvidia. Due to sanctions, the new chip is no longer manufactured by TSMC but is instead produced by a domestic company.&lt;/p&gt; &lt;p&gt;It is reported that Alibaba has not placed orders for Huawei‚Äôs chips, as it views Huawei as a direct competitor in the cloud services sector.&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;If Alibaba pulls this off, it will become one of only two companies in the world with both AI chip development and advanced LLM capabilities (the other being Google). TPU+Qwen, that‚Äôs insane.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/luckbossx"&gt; /u/luckbossx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n35bwe/alibaba_creates_ai_chip_to_help_china_fill_nvidia/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n35bwe/alibaba_creates_ai_chip_to_help_china_fill_nvidia/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n35bwe/alibaba_creates_ai_chip_to_help_china_fill_nvidia/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T11:52:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1n31r73</id>
    <title>I built a command center for Claude Code so I don‚Äôt have to babysit it anymore</title>
    <updated>2025-08-29T08:24:38+00:00</updated>
    <author>
      <name>/u/GuessConnect3009</name>
      <uri>https://old.reddit.com/user/GuessConnect3009</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For the past few weeks I‚Äôve been hacking on Omnara. Basically, it‚Äôs a way to run Claude Code anywhere without being glued to your laptop.&lt;/p&gt; &lt;p&gt;The pain point was simple: I‚Äôd start a session, wait 5‚Äì10 minutes while it ‚Äúthought,‚Äù and if I wasn‚Äôt at my terminal at the exact right moment, the whole run was wasted. Total babysitting job.&lt;/p&gt; &lt;p&gt;Now:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Start Claude Code in terminal with pip install omnara &amp;amp;&amp;amp; omnara&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Pick it up instantly on web or mobile; same session, no restart&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Push notifications when it needs input (so you can reply from bed, an Uber, or mid-laundry)&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Native terminal experience mirrored everywhere (permissions, git diffs, etc)&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Backend is open source if you want to poke around&lt;br /&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What I didn‚Äôt expect: once I stopped ‚Äúhovering‚Äù over my sessions, I started actually letting agents run on longer workflows without stress.&lt;/p&gt; &lt;p&gt;I‚Äôm curious: how are people here handling agent interruptions / human-in-the-loop stuff? Do you just restart when things break, or have you built in ways to catch them before they collapse?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GuessConnect3009"&gt; /u/GuessConnect3009 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n31r73/i_built_a_command_center_for_claude_code_so_i/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n31r73/i_built_a_command_center_for_claude_code_so_i/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n31r73/i_built_a_command_center_for_claude_code_so_i/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T08:24:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1n33ugq</id>
    <title>Amazing Qwen stuff coming soon</title>
    <updated>2025-08-29T10:34:17+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n33ugq/amazing_qwen_stuff_coming_soon/"&gt; &lt;img alt="Amazing Qwen stuff coming soon" src="https://preview.redd.it/v6kx1bw8sxlf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4ceb5641bac92e83c48c0893b26584487a3d582e" title="Amazing Qwen stuff coming soon" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Any ideas...?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/v6kx1bw8sxlf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n33ugq/amazing_qwen_stuff_coming_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n33ugq/amazing_qwen_stuff_coming_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T10:34:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1n3b13b</id>
    <title>Apple releases FastVLM and MobileCLIP2 on Hugging Face, along with a real-time video captioning demo (in-browser + WebGPU)</title>
    <updated>2025-08-29T15:47:53+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3b13b/apple_releases_fastvlm_and_mobileclip2_on_hugging/"&gt; &lt;img alt="Apple releases FastVLM and MobileCLIP2 on Hugging Face, along with a real-time video captioning demo (in-browser + WebGPU)" src="https://external-preview.redd.it/ZWZwemw0NXNiemxmMRIjC8ICuXshETDKyWbElsvvahdP8-tMtjXY4bwDOY1n.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2f385cf95be0a591edf241b94d0612947ca571c1" title="Apple releases FastVLM and MobileCLIP2 on Hugging Face, along with a real-time video captioning demo (in-browser + WebGPU)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Link to models:&lt;br /&gt; - FastVLM: &lt;a href="https://huggingface.co/collections/apple/fastvlm-68ac97b9cd5cacefdd04872e"&gt;https://huggingface.co/collections/apple/fastvlm-68ac97b9cd5cacefdd04872e&lt;/a&gt;&lt;br /&gt; - MobileCLIP2: &lt;a href="https://huggingface.co/collections/apple/mobileclip2-68ac947dcb035c54bcd20c47"&gt;https://huggingface.co/collections/apple/mobileclip2-68ac947dcb035c54bcd20c47&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Demo (+ source code): &lt;a href="https://huggingface.co/spaces/apple/fastvlm-webgpu"&gt;https://huggingface.co/spaces/apple/fastvlm-webgpu&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ayma955sbzlf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3b13b/apple_releases_fastvlm_and_mobileclip2_on_hugging/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n3b13b/apple_releases_fastvlm_and_mobileclip2_on_hugging/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T15:47:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1unkv</id>
    <title>Launching Our New AMA Series With Z.AI, Creators of GLM (Tomorrow, 9AM-12PM PST)</title>
    <updated>2025-08-27T22:04:51+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1unkv/launching_our_new_ama_series_with_zai_creators_of/"&gt; &lt;img alt="Launching Our New AMA Series With Z.AI, Creators of GLM (Tomorrow, 9AM-12PM PST)" src="https://preview.redd.it/ek8o2pfzumlf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d7e3a061baa23ba1306e6f0b1f2524a658bb27a2" title="Launching Our New AMA Series With Z.AI, Creators of GLM (Tomorrow, 9AM-12PM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ek8o2pfzumlf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1unkv/launching_our_new_ama_series_with_zai_creators_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n1unkv/launching_our_new_ama_series_with_zai_creators_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-27T22:04:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2ghx4</id>
    <title>AMA With Z.AI, The Lab Behind GLM Models</title>
    <updated>2025-08-28T16:10:25+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;AMA with Z.AI ‚Äî The Lab Behind GLM Models. Ask Us Anything!&lt;/h1&gt; &lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Today we are having &lt;strong&gt;Z.AI&lt;/strong&gt;, the research lab behind the &lt;strong&gt;GLM family of models&lt;/strong&gt;. We‚Äôre excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/zixuanlimit/"&gt;&lt;strong&gt;Zixuan Li, u/zixuanlimit&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/Maximum_Can9140/"&gt;&lt;strong&gt;Yuxuan Zhang, u/Maximum_Can9140&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/zxdu/"&gt;&lt;strong&gt;Zhengxiao Du, u/zxdu&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/Sengxian/"&gt;&lt;strong&gt;Aohan Zeng, u/Sengxian&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 9 AM ‚Äì 12 PM PST, with the Z.AI team continuing to follow up on questions over the next 48 hours.&lt;/strong&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Thanks everyone for joining our first AMA. The live part has ended and the Z.AI team will be following up with more answers sporadically over the next 48 hours.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2ghx4/ama_with_zai_the_lab_behind_glm_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2ghx4/ama_with_zai_the_lab_behind_glm_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2ghx4/ama_with_zai_the_lab_behind_glm_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T16:10:25+00:00</published>
  </entry>
</feed>
