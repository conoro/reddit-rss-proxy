<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-03T18:25:57+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1n7h91t</id>
    <title>What models to test on my first machine?</title>
    <updated>2025-09-03T15:04:05+00:00</updated>
    <author>
      <name>/u/IamLuckyy</name>
      <uri>https://old.reddit.com/user/IamLuckyy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So here is my build: MSI MAG Tomahawk B550 Corsair Vengeance 64gb 3200mhz Ryzen 7 5800x 2x Nvidia P102-100 10gb vram&lt;/p&gt; &lt;p&gt;So far I have played around with a few models in the 14b-27b range. I tend to like using the Gemma model the most but gpt-oss wasnâ€™t bad either. I mainly want to use this as a deep researching llm that I can also train of pdf data. Gemma3:27b runs fine but itâ€™s slow and running about a 87% gpu and 13% cpu, and Iâ€™m new to this so Iâ€™m not sure if I can optimize it to be fast or not. My main gripe with thinking models like gpt-oss or deepseek is that the thinking feels like it takes forever and it hasnâ€™t really proven to give me any better answers so far, in fact sometimes itâ€™s more stubborn and tells me stuff is down right impossible. ANYWAYS, any advice would be must appreciated but for right now Iâ€™ll keep playing with Gemma and maybe trying mistral-small.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IamLuckyy"&gt; /u/IamLuckyy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7h91t/what_models_to_test_on_my_first_machine/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7h91t/what_models_to_test_on_my_first_machine/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7h91t/what_models_to_test_on_my_first_machine/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T15:04:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6vzfe</id>
    <title>WEBGEN-4B: Quality Web Design Generation</title>
    <updated>2025-09-02T21:20:22+00:00</updated>
    <author>
      <name>/u/smirkishere</name>
      <uri>https://old.reddit.com/user/smirkishere</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6vzfe/webgen4b_quality_web_design_generation/"&gt; &lt;img alt="WEBGEN-4B: Quality Web Design Generation" src="https://b.thumbs.redditmedia.com/hz9KVQoDGH5SuRnQfWtoik62ndFYILrISKyncU-X0Cc.jpg" title="WEBGEN-4B: Quality Web Design Generation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Tesslate/WEBGEN-4B is a 4B model that produces quality tailwind websites. We trained it on 100k samples with synthetic data exclusively generated from GPT-OSS. WEBGEN is fast, controllable, and can drop right into your agentic workflows.&lt;/p&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/Tesslate/WEBGEN-4B-Preview"&gt;https://huggingface.co/Tesslate/WEBGEN-4B-Preview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GGUF: &lt;a href="https://huggingface.co/gabriellarson/WEBGEN-4B-Preview-GGUF"&gt;https://huggingface.co/gabriellarson/WEBGEN-4B-Preview-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Over the course of this week and next week, we will be dropping a few more models or open sourced software based on the innovations we've made in this space!&lt;/p&gt; &lt;p&gt;Please reach out for API keys to test it out if needed. On the model card and below in the comments will be our designer platform (which we will open source soon) where you can use the model for free. &lt;/p&gt; &lt;p&gt;In other news, we are open sourcing our UIGEN-T2 Dataset at Tesslate/UIGEN-T2&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/smirkishere"&gt; /u/smirkishere &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1n6vzfe"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6vzfe/webgen4b_quality_web_design_generation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6vzfe/webgen4b_quality_web_design_generation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T21:20:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7m6sr</id>
    <title>Building an AI-Powered Tamagotchi Using Local LLMs</title>
    <updated>2025-09-03T18:05:16+00:00</updated>
    <author>
      <name>/u/YungMixtape2004</name>
      <uri>https://old.reddit.com/user/YungMixtape2004</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7m6sr/building_an_aipowered_tamagotchi_using_local_llms/"&gt; &lt;img alt="Building an AI-Powered Tamagotchi Using Local LLMs" src="https://external-preview.redd.it/LEjK_wGRA8Cp1687LU47n7GRURKSyczAji-aKZOXfhQ.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=284a80c33d6a900b5f426ae09b37b2b5cd9ae725" title="Building an AI-Powered Tamagotchi Using Local LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/YungMixtape2004"&gt; /u/YungMixtape2004 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/DhO5tcjnb9A"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7m6sr/building_an_aipowered_tamagotchi_using_local_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7m6sr/building_an_aipowered_tamagotchi_using_local_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T18:05:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6rbi2</id>
    <title>Piracy is for Trillion Dollar Companies | Fair Use, Copyright Law, &amp; Meta AI</title>
    <updated>2025-09-02T18:24:09+00:00</updated>
    <author>
      <name>/u/prusswan</name>
      <uri>https://old.reddit.com/user/prusswan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6rbi2/piracy_is_for_trillion_dollar_companies_fair_use/"&gt; &lt;img alt="Piracy is for Trillion Dollar Companies | Fair Use, Copyright Law, &amp;amp; Meta AI" src="https://external-preview.redd.it/FUP5JRh_hs7L2Yd_DmiTAO0WgUYYJ4skdrhkm8MNDKc.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fcef23df49c3448a2d625ddb43fe346cbd8bdd05" title="Piracy is for Trillion Dollar Companies | Fair Use, Copyright Law, &amp;amp; Meta AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So acquiring copyrighted material for the purpose of training LLMs is deemed transformative and qualifies under fair use? Gonna call this Meta's Defence from now on.. I have a huge stash of ebooks to run through&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/prusswan"&gt; /u/prusswan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=sdtBgB7iS8c"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6rbi2/piracy_is_for_trillion_dollar_companies_fair_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6rbi2/piracy_is_for_trillion_dollar_companies_fair_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T18:24:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7cz7y</id>
    <title>YanoljaNEXT-Rosetta: A Collection of Translation Models in Different Sizes</title>
    <updated>2025-09-03T12:06:20+00:00</updated>
    <author>
      <name>/u/SummerFantastic5457</name>
      <uri>https://old.reddit.com/user/SummerFantastic5457</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;YanoljaNEXT-Rosetta is designed for translating structured data in JSON format. Itâ€™s built on top of either &lt;strong&gt;Gemma-3&lt;/strong&gt; or &lt;strong&gt;GPT-OSS&lt;/strong&gt;, depending on the configuration.&lt;/p&gt; &lt;p&gt;If your language isnâ€™t listed on the card, donâ€™t worryâ€”this model supports many more languages than those shown.&lt;/p&gt; &lt;p&gt;In evaluations, YanoljaNEXT-Rosetta outperformed proprietary models on &lt;strong&gt;BLEU&lt;/strong&gt; and &lt;strong&gt;CHrF++&lt;/strong&gt; scores, while scoring slightly lower on &lt;strong&gt;MetricX24&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;ðŸ“‚ Hugging Face collection: &lt;a href="https://huggingface.co/collections/yanolja/yanoljanext-rosetta-68b82bd4718e49b5c7a745b5?utm_source=chatgpt.com"&gt;here&lt;/a&gt;&lt;br /&gt; ðŸ”¹ &lt;a href="https://huggingface.co/yanolja/YanoljaNEXT-Rosetta-4B?utm_source=chatgpt.com"&gt;4B model&lt;/a&gt;&lt;br /&gt; ðŸ”¹ &lt;a href="https://huggingface.co/yanolja/YanoljaNEXT-Rosetta-12B?utm_source=chatgpt.com"&gt;12B model&lt;/a&gt;&lt;br /&gt; ðŸ”¹ &lt;a href="https://huggingface.co/yanolja/YanoljaNEXT-Rosetta-20B?utm_source=chatgpt.com"&gt;20B model&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SummerFantastic5457"&gt; /u/SummerFantastic5457 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7cz7y/yanoljanextrosetta_a_collection_of_translation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7cz7y/yanoljanextrosetta_a_collection_of_translation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7cz7y/yanoljanextrosetta_a_collection_of_translation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T12:06:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7hanu</id>
    <title>Does VRAM correlate with model quality?</title>
    <updated>2025-09-03T15:05:46+00:00</updated>
    <author>
      <name>/u/ta394283509</name>
      <uri>https://old.reddit.com/user/ta394283509</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I use a 3060 12GB. I've tried many different 8B models at different quantizations, and their error rate seem to be about equal. By errors I mean how they repeat previous paragraphs (even with temp set to max) or forget important details from earlier in the conversation (even from a few messages earlier).&lt;/p&gt; &lt;p&gt;I was thinking of upgrading to a 3090 or 4090 for for SDXL training (my 12GB only works for training SD1.5), and was wondering if this upgrade would grant an improvement in how my LLMs run. For context, I use local LLMs exclusively for horny RP&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ta394283509"&gt; /u/ta394283509 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7hanu/does_vram_correlate_with_model_quality/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7hanu/does_vram_correlate_with_model_quality/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7hanu/does_vram_correlate_with_model_quality/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T15:05:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7mien</id>
    <title>Best current NSFW TTS model?</title>
    <updated>2025-09-03T18:17:17+00:00</updated>
    <author>
      <name>/u/Stock-Fault5734</name>
      <uri>https://old.reddit.com/user/Stock-Fault5734</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Which one? And how to use it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Stock-Fault5734"&gt; /u/Stock-Fault5734 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7mien/best_current_nsfw_tts_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7mien/best_current_nsfw_tts_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7mien/best_current_nsfw_tts_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T18:17:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7fux7</id>
    <title>Reasoning Vectors: Transferring Chain-of-Thought Capabilities via Task Arithmetic</title>
    <updated>2025-09-03T14:10:03+00:00</updated>
    <author>
      <name>/u/LowChance4561</name>
      <uri>https://old.reddit.com/user/LowChance4561</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The paper shows that reasoning ability can be extracted as a vector from RL-trained models and added to others via simple arithmetic to boost reasoning without retraining&lt;br /&gt; would appreciate an upvote &lt;a href="https://huggingface.co/papers/2509.01363"&gt;https://huggingface.co/papers/2509.01363&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LowChance4561"&gt; /u/LowChance4561 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7fux7/reasoning_vectors_transferring_chainofthought/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7fux7/reasoning_vectors_transferring_chainofthought/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7fux7/reasoning_vectors_transferring_chainofthought/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T14:10:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1n770a1</id>
    <title>I made a Chrome extension that uses your local LLMs to filter Reddit content in real-time</title>
    <updated>2025-09-03T06:03:07+00:00</updated>
    <author>
      <name>/u/yuyangchee98</name>
      <uri>https://old.reddit.com/user/yuyangchee98</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n770a1/i_made_a_chrome_extension_that_uses_your_local/"&gt; &lt;img alt="I made a Chrome extension that uses your local LLMs to filter Reddit content in real-time" src="https://external-preview.redd.it/B50ELvs9yWP89z_ZcFK2UCF9ieaTQI3VL80AVGhBmaU.jpeg?width=108&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b1278541976a13214da5dc7333c05607ca273ef3" title="I made a Chrome extension that uses your local LLMs to filter Reddit content in real-time" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I built a Chrome extension that uses local models to filter content based on rules you write in plain English.&lt;/p&gt; &lt;p&gt;Some examples are: &amp;quot;No political content or culture wars&amp;quot;, &amp;quot;Remove clickbait and rage bait&amp;quot;, &amp;quot;Hide celebrity gossip and drama&amp;quot;, &amp;quot;No sports or entertainment news&amp;quot;.&lt;/p&gt; &lt;p&gt;It works with Ollama, LM Studio, and your custom defined OpenAI compatible endpoint. Let me know if you use some other way to host your local LLMs.&lt;/p&gt; &lt;p&gt;Currently only works on Reddit but planning to add more sites.&lt;/p&gt; &lt;p&gt;Link is here: &lt;a href="https://chromewebstore.google.com/detail/takeback/paiidckpbpkkjhicmbgmohnmjcdbchef"&gt;https://chromewebstore.google.com/detail/takeback/paiidckpbpkkjhicmbgmohnmjcdbchef&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1n770a1/video/1bvu3z3a4wmf1/player"&gt;https://reddit.com/link/1n770a1/video/1bvu3z3a4wmf1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yuyangchee98"&gt; /u/yuyangchee98 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n770a1/i_made_a_chrome_extension_that_uses_your_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n770a1/i_made_a_chrome_extension_that_uses_your_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n770a1/i_made_a_chrome_extension_that_uses_your_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T06:03:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6mi81</id>
    <title>German "Who Wants to Be a Millionaire" Benchmark</title>
    <updated>2025-09-02T15:24:56+00:00</updated>
    <author>
      <name>/u/Available_Load_5334</name>
      <uri>https://old.reddit.com/user/Available_Load_5334</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6mi81/german_who_wants_to_be_a_millionaire_benchmark/"&gt; &lt;img alt="German &amp;quot;Who Wants to Be a Millionaire&amp;quot; Benchmark" src="https://preview.redd.it/du3iq68grrmf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=486736a10efedf5ea83f05d63d41d7eda1e92ac7" title="German &amp;quot;Who Wants to Be a Millionaire&amp;quot; Benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i have created a benchmark for german &amp;quot;who wants to be millionaire&amp;quot; questions. there are 45x15 questions, all 45 rounds go from easy to hard and all tested models ran through all 45 rounds and got kicked out of a round if the answer was wrong, keeping the current winnings. no jokers.&lt;/p&gt; &lt;p&gt;i am a bit limited with the selection of llm's since i run them on my framework laptop 13 (amd ryzen 5 7640u with 32 gb ram), so i mainly used smaller llm's. also, qwen3's thinking went on for way to long for each question so i just tested non-thinking models except for gpt-oss-20b (low). but in my initial testing for qwen3-4b-thinking-2507, it seemed to worsen the quality of answers at least for the first questions.&lt;/p&gt; &lt;p&gt;the first few questions are often word-play and idioms questions needing great understanding of the german language. these proved to be very hard for most llm's but are easily solvable by the average german. once the first few questions were solved the models had an easier time answering.&lt;/p&gt; &lt;p&gt;i tried to use optimal model settings and included them in the table, let me know if they could be improved. all models are quant Q4_K_M.&lt;/p&gt; &lt;p&gt;i have close to no python coding ability so the main script was created with qwen3-coder. the project (with detailed results for each model, and the queationaire) is open souce and available on github.&lt;br /&gt; &lt;a href="https://github.com/ikiruneo/millionaire-bench"&gt;https://github.com/ikiruneo/millionaire-bench&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Available_Load_5334"&gt; /u/Available_Load_5334 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/du3iq68grrmf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6mi81/german_who_wants_to_be_a_millionaire_benchmark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6mi81/german_who_wants_to_be_a_millionaire_benchmark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T15:24:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7923o</id>
    <title>Has anyone run 256GB of DDR5 6000 stable on an AM5 platform?</title>
    <updated>2025-09-03T08:15:25+00:00</updated>
    <author>
      <name>/u/kitgary</name>
      <uri>https://old.reddit.com/user/kitgary</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to upgrade my system to 256GB so I can run a larger model with my GPU. Iâ€™m wondering if anyone has been able to run 256GB of DDR5 6000 stable on an AM5 platform. I donâ€™t want to upgrade to Threadripper since itâ€™s out of my budget. Which motherboard and RAM did you use?&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.msi.com/news/detail/MSI-Release-the-Latest-AMD-AGESA-Combo-PI-1-2-0-3e-BIOS--Supporting-all-64GBx4-DRAM-Chips-and-New-CPU-146587"&gt;https://www.msi.com/news/detail/MSI-Release-the-Latest-AMD-AGESA-Combo-PI-1-2-0-3e-BIOS--Supporting-all-64GBx4-DRAM-Chips-and-New-CPU-146587&lt;/a&gt;&lt;/p&gt; &lt;p&gt;MSI claims their motherboard can still achieve a stable overclocking speed of 6000MT/s even with four 64GB DRAM fully installed.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kitgary"&gt; /u/kitgary &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7923o/has_anyone_run_256gb_of_ddr5_6000_stable_on_an/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7923o/has_anyone_run_256gb_of_ddr5_6000_stable_on_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7923o/has_anyone_run_256gb_of_ddr5_6000_stable_on_an/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T08:15:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1n71b95</id>
    <title>Any actual downside to 4 x 3090 ($2400 total) vs RTX pro 6000 ($9000) other than power?</title>
    <updated>2025-09-03T01:09:09+00:00</updated>
    <author>
      <name>/u/devshore</name>
      <uri>https://old.reddit.com/user/devshore</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Can I run the same models (ie qwen 3 coder, or GLM 4.5 air) with 4 x 3090? Is the only real difference slight speed difference and a few dollars more a month in electricity? Secondly, are there any consumer motherboards (currently using an intel 265K) that support 4 GPUs, or would I need a new chipset / cpu / mobo etc?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/devshore"&gt; /u/devshore &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n71b95/any_actual_downside_to_4_x_3090_2400_total_vs_rtx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n71b95/any_actual_downside_to_4_x_3090_2400_total_vs_rtx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n71b95/any_actual_downside_to_4_x_3090_2400_total_vs_rtx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T01:09:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7m1ig</id>
    <title>Mapping LLM Style and Range in Flash Fiction</title>
    <updated>2025-09-03T18:00:11+00:00</updated>
    <author>
      <name>/u/zero0_one1</name>
      <uri>https://old.reddit.com/user/zero0_one1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7m1ig/mapping_llm_style_and_range_in_flash_fiction/"&gt; &lt;img alt="Mapping LLM Style and Range in Flash Fiction" src="https://b.thumbs.redditmedia.com/u40TUFkM22cScfVpJOWkzRp8n5pZSql8r4GWBthStMo.jpg" title="Mapping LLM Style and Range in Flash Fiction" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Additional charts and analysis: &lt;a href="https://github.com/lechmazur/writing_styles"&gt;https://github.com/lechmazur/writing_styles&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Based on 400 flash-fiction pieces of 600â€“800 words per LLM. Prompts include required elements to keep content varied.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zero0_one1"&gt; /u/zero0_one1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1n7m1ig"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7m1ig/mapping_llm_style_and_range_in_flash_fiction/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7m1ig/mapping_llm_style_and_range_in_flash_fiction/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T18:00:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7fsgd</id>
    <title>I made Spring AI Playground - a self-hosted UI for local LLMs, RAG, and MCP tools</title>
    <updated>2025-09-03T14:07:15+00:00</updated>
    <author>
      <name>/u/kr-jmlab</name>
      <uri>https://old.reddit.com/user/kr-jmlab</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7fsgd/i_made_spring_ai_playground_a_selfhosted_ui_for/"&gt; &lt;img alt="I made Spring AI Playground - a self-hosted UI for local LLMs, RAG, and MCP tools" src="https://b.thumbs.redditmedia.com/lVGU8L5zqMIzkaT28XyJJJ-eN-kz7vnn4Hr4DJVSTqI.jpg" title="I made Spring AI Playground - a self-hosted UI for local LLMs, RAG, and MCP tools" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I made an open-source project called Spring AI Playground â€” a self-hosted web UI for experimenting with local LLMs, RAG, and MCP tools.&lt;/p&gt; &lt;p&gt;Itâ€™s a self-hosted web UI (Docker image available) that lets you:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Run local LLMs with &lt;strong&gt;Ollama&lt;/strong&gt; (you can switch to OpenAI/Anthropic too).&lt;/li&gt; &lt;li&gt;Upload docs â†’ chunk, embed, search, and inspect vector-DB retrieval &lt;strong&gt;with score details&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Connect to &lt;strong&gt;MCP servers directly&lt;/strong&gt;, test each tool, and even run end-to-end chat flows combining RAG + MCP.&lt;/li&gt; &lt;li&gt;Swap vector DBs or select MCP tools dynamically - thanks to the Spring AI framework under the hood.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Why I built it:&lt;/p&gt; &lt;p&gt;I wanted a sandbox where I could mash things together quickly, test retrieval quality, debug tools, and keep everything running locally. Open WebUI is fantastic for chat-centric experiments, but my focus was to make &lt;strong&gt;RAG + MCP first-class playgrounds&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/JM-Lab/spring-ai-playground"&gt;https://github.com/JM-Lab/spring-ai-playground&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love feedback from this community - especially from those running local models or playing with MCP. Curious if this would fit into your workflow, or if there are rough edges I should improve.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kr-jmlab"&gt; /u/kr-jmlab &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1n7fsgd"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7fsgd/i_made_spring_ai_playground_a_selfhosted_ui_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7fsgd/i_made_spring_ai_playground_a_selfhosted_ui_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T14:07:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7meyo</id>
    <title>Intel launches Arc Pro B50 graphics card at $349</title>
    <updated>2025-09-03T18:13:50+00:00</updated>
    <author>
      <name>/u/reps_up</name>
      <uri>https://old.reddit.com/user/reps_up</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/reps_up"&gt; /u/reps_up &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.phoronix.com/review/intel-arc-pro-b50-linux"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7meyo/intel_launches_arc_pro_b50_graphics_card_at_349/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7meyo/intel_launches_arc_pro_b50_graphics_card_at_349/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T18:13:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7b5xl</id>
    <title>New Swiss fully-open multilingual Model</title>
    <updated>2025-09-03T10:29:46+00:00</updated>
    <author>
      <name>/u/braincrowd</name>
      <uri>https://old.reddit.com/user/braincrowd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7b5xl/new_swiss_fullyopen_multilingual_model/"&gt; &lt;img alt="New Swiss fully-open multilingual Model" src="https://external-preview.redd.it/KeZfybYf994Jltq2xFXUUTTUg9fRGIDbb5FdVf9Sh70.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=415bc651c08564e7f0fb8fbbfdc78d40ba8ad377" title="New Swiss fully-open multilingual Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/braincrowd"&gt; /u/braincrowd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/swiss-ai/Apertus-70B-2509"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7b5xl/new_swiss_fullyopen_multilingual_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7b5xl/new_swiss_fullyopen_multilingual_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T10:29:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7l5kg</id>
    <title>Intel launches Arc Pro B50 graphics card at $349</title>
    <updated>2025-09-03T17:27:29+00:00</updated>
    <author>
      <name>/u/levian_</name>
      <uri>https://old.reddit.com/user/levian_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7l5kg/intel_launches_arc_pro_b50_graphics_card_at_349/"&gt; &lt;img alt="Intel launches Arc Pro B50 graphics card at $349" src="https://preview.redd.it/357rwwhaizmf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=066c5073e108f00168fb16f32dcc905e00df9cae" title="Intel launches Arc Pro B50 graphics card at $349" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Initial review, source:&lt;a href="https://videocardz.com/newz/intel-launches-arc-pro-b50-graphics-card-at-349"&gt;https://videocardz.com/newz/intel-launches-arc-pro-b50-graphics-card-at-349&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/levian_"&gt; /u/levian_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/357rwwhaizmf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7l5kg/intel_launches_arc_pro_b50_graphics_card_at_349/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7l5kg/intel_launches_arc_pro_b50_graphics_card_at_349/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T17:27:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7ilou</id>
    <title>Switzerland launches its own open source model</title>
    <updated>2025-09-03T15:54:28+00:00</updated>
    <author>
      <name>/u/ananas_tacos</name>
      <uri>https://old.reddit.com/user/ananas_tacos</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7ilou/switzerland_launches_its_own_open_source_model/"&gt; &lt;img alt="Switzerland launches its own open source model" src="https://external-preview.redd.it/vqsLOQwLzSpFCY0wZKGHoR70wxX3Zo0oDI880u-Ya_o.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=659f786877c6c3ce79ac169974bd64f43e3484fc" title="Switzerland launches its own open source model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ananas_tacos"&gt; /u/ananas_tacos &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.engadget.com/ai/switzerland-launches-its-own-open-source-ai-model-133051578.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7ilou/switzerland_launches_its_own_open_source_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7ilou/switzerland_launches_its_own_open_source_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T15:54:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7c1tg</id>
    <title>Le Chat. Custom MCP connectors. Memories.</title>
    <updated>2025-09-03T11:19:20+00:00</updated>
    <author>
      <name>/u/According_to_Mission</name>
      <uri>https://old.reddit.com/user/According_to_Mission</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7c1tg/le_chat_custom_mcp_connectors_memories/"&gt; &lt;img alt="Le Chat. Custom MCP connectors. Memories." src="https://external-preview.redd.it/QLQU1soiMTzFAm8GzW6EPDbaX5jrcYYFqy1ql5NYoiQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4c3b97e1405ebb7916bf71d7b9a3da9a44efaea7" title="Le Chat. Custom MCP connectors. Memories." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Le Chat now integrates with 20+ enterprise platformsâ€”powered by MCPâ€”and remembers what matters with Memories.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/According_to_Mission"&gt; /u/According_to_Mission &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://mistral.ai/news/le-chat-mcp-connectors-memories"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7c1tg/le_chat_custom_mcp_connectors_memories/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7c1tg/le_chat_custom_mcp_connectors_memories/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T11:19:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7bqgm</id>
    <title>LangExtract by Google: many people don't know about this yet!</title>
    <updated>2025-09-03T11:02:22+00:00</updated>
    <author>
      <name>/u/fuckAIbruhIhateCorps</name>
      <uri>https://old.reddit.com/user/fuckAIbruhIhateCorps</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7bqgm/langextract_by_google_many_people_dont_know_about/"&gt; &lt;img alt="LangExtract by Google: many people don't know about this yet!" src="https://external-preview.redd.it/n9THNRvTBgabZmzyX_O8lEw2GxXkLfCbQBuYD0khQMY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=70d9ab8760012176bf18e519e0590b3f5f3d4bab" title="LangExtract by Google: many people don't know about this yet!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fuckAIbruhIhateCorps"&gt; /u/fuckAIbruhIhateCorps &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/google/langextract"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7bqgm/langextract_by_google_many_people_dont_know_about/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7bqgm/langextract_by_google_many_people_dont_know_about/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T11:02:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7jmiz</id>
    <title>Drummer's Skyfall 31B v4 Â· A Mistral 24B upscaled to 31B with more creativity!</title>
    <updated>2025-09-03T16:31:18+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7jmiz/drummers_skyfall_31b_v4_a_mistral_24b_upscaled_to/"&gt; &lt;img alt="Drummer's Skyfall 31B v4 Â· A Mistral 24B upscaled to 31B with more creativity!" src="https://external-preview.redd.it/uylRGwq1HYqH_9GVZQwtt7vjMGVse2R0k3BHHixg9iQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bd30ec44d7dcdb6ab67aaebd28d83444619fea7e" title="Drummer's Skyfall 31B v4 Â· A Mistral 24B upscaled to 31B with more creativity!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'd also like to take this opportunity to share some benchmarks for Cydonia 24B v4.1: &lt;a href="https://huggingface.co/TheDrummer/Cydonia-24B-v4.1/discussions/2"&gt;https://huggingface.co/TheDrummer/Cydonia-24B-v4.1/discussions/2&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Skyfall-31B-v4"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7jmiz/drummers_skyfall_31b_v4_a_mistral_24b_upscaled_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7jmiz/drummers_skyfall_31b_v4_a_mistral_24b_upscaled_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T16:31:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7jfpt</id>
    <title>Qwen3 30B A3B Thinking 2507 Hybrid !!</title>
    <updated>2025-09-03T16:24:34+00:00</updated>
    <author>
      <name>/u/Not4Fame</name>
      <uri>https://old.reddit.com/user/Not4Fame</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7jfpt/qwen3_30b_a3b_thinking_2507_hybrid/"&gt; &lt;img alt="Qwen3 30B A3B Thinking 2507 Hybrid !!" src="https://external-preview.redd.it/3-YBimUSWKbnR7AwkACpdqNr5hKT1fY59SClnp7z_yM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b5bdb4b82debdc147a98f0aa03728d4703fe317e" title="Qwen3 30B A3B Thinking 2507 Hybrid !!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all, with some creative merge from YOYO-AI, and some love from me, now you have Qwen3 30B A3B Thinking 2507 in hybrid mode, just like the old hybrid mode, but 2507 weights. First give the creator some love &lt;a href="https://huggingface.co/YOYO-AI/Qwen3-30B-A3B-Mixture-2507/discussions"&gt;here&lt;/a&gt; and next, read my instructions and get the chat template &lt;a href="https://huggingface.co/YOYO-AI/Qwen3-30B-A3B-Mixture-2507-Q4_K_M-GGUF/discussions/1"&gt;here&lt;/a&gt; finally, go and download the model &lt;a href="https://huggingface.co/mradermacher/Qwen3-30B-A3B-Mixture-2507-GGUF"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;No coffee needed, whatever I do, I do for love, not for fame ;)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/i6jep5s67zmf1.png?width=1151&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=00577ea14074f247cf2491ae18a0fd5bf3cbfbb4"&gt;Qwen3 30B A3B Thinking 2507 Hybrid !&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Edit:&lt;br /&gt; OOPS, I've linked wrong models, sorry&lt;/p&gt; &lt;p&gt;Model is this &lt;a href="https://huggingface.co/YOYO-AI/Qwen3-30B-A3B-Mixture-2507"&gt;YOYO-AI/Qwen3-30B-A3B-Mixture-2507 Â· Hugging Face&lt;/a&gt;&lt;/p&gt; &lt;p&gt;and GGUF is this &lt;a href="https://huggingface.co/mradermacher/Qwen3-30B-A3B-Mixture-2507-GGUF"&gt;mradermacher/Qwen3-30B-A3B-Mixture-2507-GGUF Â· Hugging Face&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Moar Edit: I've also corrected the links in post body as well now, my comment explaining the usage and the chat template, sadly, is written under wrong model, but hey, nobody is perfect :)&lt;/p&gt; &lt;p&gt;Even Moar Edit: I've also moved my comment under the correct model now and linked this post's link correctly to it, so all links are now correct, phew... (yeah I'm stoned... so ? :P)&lt;/p&gt; &lt;p&gt;Moar Bonus Edit:&lt;/p&gt; &lt;p&gt;YOYO-AI also has &lt;a href="https://huggingface.co/mradermacher/Qwen3-30B-A3B-CoderThinking-YOYO-linear-GGUF"&gt;this&lt;/a&gt; interesting merge, Qwen3 coder 2507 with Qwen3 Thinking 2507 merge, which in my experience makes the coder somewhat less rigid and a bit less resilient/more instruction following etc. I recommend you check that out too !&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Not4Fame"&gt; /u/Not4Fame &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7jfpt/qwen3_30b_a3b_thinking_2507_hybrid/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7jfpt/qwen3_30b_a3b_thinking_2507_hybrid/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7jfpt/qwen3_30b_a3b_thinking_2507_hybrid/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T16:24:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1n75z15</id>
    <title>GPT-OSS 120B is now the top open-source model in the world according to the new intelligence index by Artificial Analysis that incorporates tool call and agentic evaluations</title>
    <updated>2025-09-03T05:01:51+00:00</updated>
    <author>
      <name>/u/obvithrowaway34434</name>
      <uri>https://old.reddit.com/user/obvithrowaway34434</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n75z15/gptoss_120b_is_now_the_top_opensource_model_in/"&gt; &lt;img alt="GPT-OSS 120B is now the top open-source model in the world according to the new intelligence index by Artificial Analysis that incorporates tool call and agentic evaluations" src="https://preview.redd.it/6c1jae9atvmf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f69c39fa3f7051f8ad4c85418e9c6c975491e18b" title="GPT-OSS 120B is now the top open-source model in the world according to the new intelligence index by Artificial Analysis that incorporates tool call and agentic evaluations" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Full benchmarking methodology here: &lt;a href="https://artificialanalysis.ai/methodology/intelligence-benchmarking"&gt;https://artificialanalysis.ai/methodology/intelligence-benchmarking&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/obvithrowaway34434"&gt; /u/obvithrowaway34434 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6c1jae9atvmf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n75z15/gptoss_120b_is_now_the_top_opensource_model_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n75z15/gptoss_120b_is_now_the_top_opensource_model_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T05:01:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7g0c2</id>
    <title>German "Who Wants to Be a Millionaire" Benchmark w/ Leading Models</title>
    <updated>2025-09-03T14:15:58+00:00</updated>
    <author>
      <name>/u/facethef</name>
      <uri>https://old.reddit.com/user/facethef</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7g0c2/german_who_wants_to_be_a_millionaire_benchmark_w/"&gt; &lt;img alt="German &amp;quot;Who Wants to Be a Millionaire&amp;quot; Benchmark w/ Leading Models" src="https://b.thumbs.redditmedia.com/-7S29tlnbmvmCdgKuNNinsiLgi6LQ83T8Ar-ZV3lCVs.jpg" title="German &amp;quot;Who Wants to Be a Millionaire&amp;quot; Benchmark w/ Leading Models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;First off, big thanks to &lt;a href="/u/Available_Load_5334"&gt;u/Available_Load_5334&lt;/a&gt; for creating the original German &lt;strong&gt;Wer wird MillionÃ¤r?&lt;/strong&gt; Benchmark and open-sourcing it. &lt;a href="https://github.com/ikiruneo/millionaire-bench"&gt;https://github.com/ikiruneo/millionaire-bench&lt;/a&gt; &lt;/p&gt; &lt;p&gt;After speaking, we said it would be fun to run the same benchmark on a set of leading models, and that's what we did here. &lt;/p&gt; &lt;p&gt;The rules and data stayed the same, 45 rounds, each with 15 multiple-choice questions from easy to hard. One wrong answer ends the program and you keep the current winnings. No lifelines. Answers are single letters Aâ€“D. same public WWM question corpus used in the original. &lt;a href="https://github.com/GerritKainz/wer_wird_millionaer"&gt;https://github.com/GerritKainz/wer_wird_millionaer&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Questions remain in German for inference, but we included parallel English text so non-German readers can follow along. See fragen_antworten_en.json in the repo. Scripts to run many programs quickly and rebuild results from per-model outputs (millionaire-run.py, rebuild_leaderboard.py). Weâ€™ll attach a screenshot of the leaderboard instead of pasting a table here. same scoring and structure as the original, packaged for quick reruns.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/Jose-Sabater/millionaire-bench-opper"&gt;https://github.com/Jose-Sabater/millionaire-bench-opper&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Again thanks to &lt;a href="/u/Available_Load_5334"&gt;u/Available_Load_5334&lt;/a&gt; for the idea and groundwork. If you try more models or tweak settings, feel free to open a PR or drop results in the comments.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/facethef"&gt; /u/facethef &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1n7g0c2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7g0c2/german_who_wants_to_be_a_millionaire_benchmark_w/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7g0c2/german_who_wants_to_be_a_millionaire_benchmark_w/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T14:15:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7fdy4</id>
    <title>Introducing Kimi K2-0905</title>
    <updated>2025-09-03T13:51:27+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7fdy4/introducing_kimi_k20905/"&gt; &lt;img alt="Introducing Kimi K2-0905" src="https://b.thumbs.redditmedia.com/lyzeYJ2XI6oIjcCbfXBgsYvdUpg2tM8OGWtELhu--Xc.jpg" title="Introducing Kimi K2-0905" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What's new:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/u8oxbcfyfymf1.png?width=2178&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=87daf02d6f257631f0a0a8847de7180dc9d9eed8"&gt;https://preview.redd.it/u8oxbcfyfymf1.png?width=2178&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=87daf02d6f257631f0a0a8847de7180dc9d9eed8&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7fdy4/introducing_kimi_k20905/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7fdy4/introducing_kimi_k20905/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7fdy4/introducing_kimi_k20905/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T13:51:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7j5z2</id>
    <title>Our 2nd AMA: Hugging Face Science Team, Creators of SmolLM, SmolVLM, and more! (Tomorrow, 8AM-11AM PST)</title>
    <updated>2025-09-03T16:14:51+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7j5z2/our_2nd_ama_hugging_face_science_team_creators_of/"&gt; &lt;img alt="Our 2nd AMA: Hugging Face Science Team, Creators of SmolLM, SmolVLM, and more! (Tomorrow, 8AM-11AM PST)" src="https://preview.redd.it/wdx4ivdw3zmf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=876855c03867ead70389d15b60f24b91d478f835" title="Our 2nd AMA: Hugging Face Science Team, Creators of SmolLM, SmolVLM, and more! (Tomorrow, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wdx4ivdw3zmf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7j5z2/our_2nd_ama_hugging_face_science_team_creators_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7j5z2/our_2nd_ama_hugging_face_science_team_creators_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T16:14:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
