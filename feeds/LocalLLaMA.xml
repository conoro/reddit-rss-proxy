<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-02T05:36:33+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1om4f9k</id>
    <title>up to date cloud services for fine-tuning ?</title>
    <updated>2025-11-02T01:07:02+00:00</updated>
    <author>
      <name>/u/jiii95</name>
      <uri>https://old.reddit.com/user/jiii95</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a short question, I will be fine tuning some models in the next years, and I want a reliable cloud service. My company offers AWS, but for personal use, I want to use something not as expensive as AWS. I am based in Europe, I was looking at something like:&lt;/p&gt; &lt;p&gt;&lt;a href="https://lyceum.technology/"&gt;https://lyceum.technology/&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&lt;a href="https://www.together.ai/pricing#fine-tuning"&gt;https://www.together.ai/pricing#fine-tuning&lt;/a&gt; &lt;/p&gt; &lt;p&gt;I read that runpod is not reliable, nor vast.ai. &lt;/p&gt; &lt;p&gt;Any valid solid responses please, something European also you suggest ?&lt;/p&gt; &lt;p&gt;I have an Acer with RTX 4080, but the noises and so on are making me irritated sometimes :) I am going to return this laptop and buy a buy MAC Studio Max which I can afford, as I am making a transition to macOS, as windows is starting to get on my nerves with all the crashes and driver updates and display issues. What do you think ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jiii95"&gt; /u/jiii95 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1om4f9k/up_to_date_cloud_services_for_finetuning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1om4f9k/up_to_date_cloud_services_for_finetuning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1om4f9k/up_to_date_cloud_services_for_finetuning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T01:07:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1om97o3</id>
    <title>I have a 3090 on Windows, I'm using an up to date Docker Desktop, got the unsloth image, made a container, ran it, but I can't get CUDA to install in it. The problem in NOT unsloth_zoo.</title>
    <updated>2025-11-02T05:22:48+00:00</updated>
    <author>
      <name>/u/oodelay</name>
      <uri>https://old.reddit.com/user/oodelay</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When I try to install the CUDA toolkit via the exec window, I get that the user unsloth is not allowed to run sudo install. I get: Sorry, user unsloth is not allowed to execute '/usr/bin/apt-get update' as root on cfc8375fe886.&lt;/p&gt; &lt;p&gt;I know unsloth_zoo is installed&lt;/p&gt; &lt;p&gt;Here is the part of the notebook:&lt;/p&gt; &lt;p&gt;from unsloth import FastModel&lt;/p&gt; &lt;p&gt;import torch&lt;/p&gt; &lt;p&gt;fourbit_models = [&lt;/p&gt; &lt;p&gt;# 4bit dynamic quants for superior accuracy and low memory use&lt;/p&gt; &lt;p&gt;&amp;quot;unsloth/gemma-3-1b-it-unsloth-bnb-4bit&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;unsloth/gemma-3-4b-it-unsloth-bnb-4bit&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;unsloth/gemma-3-12b-it-unsloth-bnb-4bit&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;unsloth/gemma-3-27b-it-unsloth-bnb-4bit&amp;quot;,&lt;/p&gt; &lt;p&gt;# Other popular models!&lt;/p&gt; &lt;p&gt;&amp;quot;unsloth/Llama-3.1-8B&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;unsloth/Llama-3.2-3B&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;unsloth/Llama-3.3-70B&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;unsloth/mistral-7b-instruct-v0.3&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;unsloth/Phi-4&amp;quot;,&lt;/p&gt; &lt;p&gt;] # More models at &lt;a href="https://huggingface.co/unsloth"&gt;https://huggingface.co/unsloth&lt;/a&gt;&lt;/p&gt; &lt;p&gt;model, tokenizer = FastModel.from_pretrained(&lt;/p&gt; &lt;p&gt;model_name = &amp;quot;unsloth/gemma-3-4b-it&amp;quot;,&lt;/p&gt; &lt;p&gt;max_seq_length = 2048, # Choose any for long context!&lt;/p&gt; &lt;p&gt;load_in_4bit = True, # 4 bit quantization to reduce memory&lt;/p&gt; &lt;p&gt;load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory&lt;/p&gt; &lt;p&gt;full_finetuning = False, # [NEW!] We have full finetuning now!&lt;/p&gt; &lt;p&gt;# token = &amp;quot;hf_...&amp;quot;, # use one if using gated models&lt;/p&gt; &lt;p&gt;)&lt;/p&gt; &lt;p&gt;Here is the error I get:&lt;/p&gt; &lt;p&gt;---------------------------------------------------------------------------&lt;/p&gt; &lt;p&gt;NotImplementedError Traceback (most recent call last)&lt;/p&gt; &lt;p&gt;File /opt/conda/lib/python3.11/site-packages/unsloth/__init__.py:91&lt;/p&gt; &lt;p&gt;83 # if os.environ.get(&amp;quot;UNSLOTH_DISABLE_AUTO_UPDATES&amp;quot;, &amp;quot;0&amp;quot;) == &amp;quot;0&amp;quot;:&lt;/p&gt; &lt;p&gt;84 # try:&lt;/p&gt; &lt;p&gt;85 # os.system(&amp;quot;pip install --upgrade --no-cache-dir --no-deps unsloth_zoo&amp;quot;)&lt;/p&gt; &lt;p&gt;(...) 89 # except:&lt;/p&gt; &lt;p&gt;90 # raise ImportError(&amp;quot;Unsloth: Please update unsloth_zoo via `pip install --upgrade --no-cache-dir --no-deps unsloth_zoo`&amp;quot;)&lt;/p&gt; &lt;p&gt;---&amp;gt; 91 import unsloth_zoo&lt;/p&gt; &lt;p&gt;92 except:&lt;/p&gt; &lt;p&gt;File /opt/conda/lib/python3.11/site-packages/unsloth_zoo/__init__.py:126&lt;/p&gt; &lt;p&gt;124 pass&lt;/p&gt; &lt;p&gt;--&amp;gt; 126 from .device_type import (&lt;/p&gt; &lt;p&gt;127 is_hip,&lt;/p&gt; &lt;p&gt;128 get_device_type,&lt;/p&gt; &lt;p&gt;129 DEVICE_TYPE,&lt;/p&gt; &lt;p&gt;130 DEVICE_TYPE_TORCH,&lt;/p&gt; &lt;p&gt;131 DEVICE_COUNT,&lt;/p&gt; &lt;p&gt;132 ALLOW_PREQUANTIZED_MODELS,&lt;/p&gt; &lt;p&gt;133 )&lt;/p&gt; &lt;p&gt;135 # Torch 2.9 removed PYTORCH_HIP_ALLOC_CONF and PYTORCH_CUDA_ALLOC_CONF&lt;/p&gt; &lt;p&gt;File /opt/conda/lib/python3.11/site-packages/unsloth_zoo/device_type.py:56&lt;/p&gt; &lt;p&gt;55 pass&lt;/p&gt; &lt;p&gt;---&amp;gt; 56 DEVICE_TYPE : str = get_device_type()&lt;/p&gt; &lt;p&gt;57 # HIP fails for autocast and other torch functions. Use CUDA instead&lt;/p&gt; &lt;p&gt;File /opt/conda/lib/python3.11/site-packages/unsloth_zoo/device_type.py:46, in get_device_type()&lt;/p&gt; &lt;p&gt;45 if not torch.accelerator.is_available():&lt;/p&gt; &lt;p&gt;---&amp;gt; 46 raise NotImplementedError(&amp;quot;Unsloth cannot find any torch accelerator? You need a GPU.&amp;quot;)&lt;/p&gt; &lt;p&gt;47 accelerator = str(torch.accelerator.current_accelerator())&lt;/p&gt; &lt;p&gt;NotImplementedError: Unsloth cannot find any torch accelerator? You need a GPU.&lt;/p&gt; &lt;p&gt;During handling of the above exception, another exception occurred:&lt;/p&gt; &lt;p&gt;ImportError Traceback (most recent call last)&lt;/p&gt; &lt;p&gt;Cell In[1], line 1&lt;/p&gt; &lt;p&gt;----&amp;gt; 1 from unsloth import FastModel&lt;/p&gt; &lt;p&gt;2 import torch&lt;/p&gt; &lt;p&gt;4 fourbit_models = [&lt;/p&gt; &lt;p&gt;5 # 4bit dynamic quants for superior accuracy and low memory use&lt;/p&gt; &lt;p&gt;6 &amp;quot;unsloth/gemma-3-1b-it-unsloth-bnb-4bit&amp;quot;,&lt;/p&gt; &lt;p&gt;(...) 16 &amp;quot;unsloth/Phi-4&amp;quot;,&lt;/p&gt; &lt;p&gt;17 ] # More models at &lt;a href="https://huggingface.co/unsloth"&gt;https://huggingface.co/unsloth&lt;/a&gt;&lt;/p&gt; &lt;p&gt;File /opt/conda/lib/python3.11/site-packages/unsloth/__init__.py:93&lt;/p&gt; &lt;p&gt;91 import unsloth_zoo&lt;/p&gt; &lt;p&gt;92 except:&lt;/p&gt; &lt;p&gt;---&amp;gt; 93 raise ImportError(&amp;quot;Unsloth: Please install unsloth_zoo via `pip install unsloth_zoo`&amp;quot;)&lt;/p&gt; &lt;p&gt;94 pass&lt;/p&gt; &lt;p&gt;96 from unsloth_zoo.device_type import (&lt;/p&gt; &lt;p&gt;97 is_hip,&lt;/p&gt; &lt;p&gt;98 get_device_type,&lt;/p&gt; &lt;p&gt;(...) 102 ALLOW_PREQUANTIZED_MODELS,&lt;/p&gt; &lt;p&gt;103 )&lt;/p&gt; &lt;p&gt;ImportError: Unsloth: Please install unsloth_zoo via `pip install unsloth_zoo`&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/oodelay"&gt; /u/oodelay &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1om97o3/i_have_a_3090_on_windows_im_using_an_up_to_date/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1om97o3/i_have_a_3090_on_windows_im_using_an_up_to_date/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1om97o3/i_have_a_3090_on_windows_im_using_an_up_to_date/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T05:22:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1okz8qz</id>
    <title>pewdiepie dropped a video about running local ai</title>
    <updated>2025-10-31T16:28:56+00:00</updated>
    <author>
      <name>/u/topfpflanze187</name>
      <uri>https://old.reddit.com/user/topfpflanze187</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1okz8qz/pewdiepie_dropped_a_video_about_running_local_ai/"&gt; &lt;img alt="pewdiepie dropped a video about running local ai" src="https://external-preview.redd.it/WddxiFHLc3dMB9LBPGHmNWXXrzglB78uxpSOk1Y4d6E.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d41e205151bfdcec37d1be377abc09d05a02773e" title="pewdiepie dropped a video about running local ai" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/topfpflanze187"&gt; /u/topfpflanze187 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=qw4fDU18RcU"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1okz8qz/pewdiepie_dropped_a_video_about_running_local_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1okz8qz/pewdiepie_dropped_a_video_about_running_local_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T16:28:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1om6cp7</id>
    <title>Mac Studio listings too good to be true on eBay.</title>
    <updated>2025-11-02T02:43:45+00:00</updated>
    <author>
      <name>/u/NoFudge4700</name>
      <uri>https://old.reddit.com/user/NoFudge4700</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôll just link one but there‚Äôs a ton. Not sure if I should be even linking one but this one is sold and it‚Äôs definitely fake. I think they have bots and will sometimes continue to bid back until the price is in the range they plan on selling the hardware for. Also, seller doesn‚Äôt accept items back and if they do they return fee is on buyer. &lt;/p&gt; &lt;p&gt;All, not all but most of these listings are from China. üá®üá≥ be safe y‚Äôall. &lt;/p&gt; &lt;p&gt;&lt;a href="https://ebay.us/m/43wwkf"&gt;https://ebay.us/m/43wwkf&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NoFudge4700"&gt; /u/NoFudge4700 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1om6cp7/mac_studio_listings_too_good_to_be_true_on_ebay/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1om6cp7/mac_studio_listings_too_good_to_be_true_on_ebay/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1om6cp7/mac_studio_listings_too_good_to_be_true_on_ebay/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T02:43:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1olsh0j</id>
    <title>Best setup for running local LLMs? Budget up to $4,000</title>
    <updated>2025-11-01T16:40:51+00:00</updated>
    <author>
      <name>/u/Future_Inventor</name>
      <uri>https://old.reddit.com/user/Future_Inventor</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks, I‚Äôm looking to build or buy a setup for running language models locally and could use some advice.&lt;/p&gt; &lt;p&gt;More about my requirements: - Budget: up to $4,000 USD (but fine with cheaper if it‚Äôs enough). - I'm open to Windows, macOS, or Linux. - Laptop or desktop, whichever makes more sense. - I'm an experienced software engineer, but new to working with local LLMs. - I plan to use it for testing, local inference, and small-scale app development, maybe light fine-tuning later on.&lt;/p&gt; &lt;p&gt;What would you recommend? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Future_Inventor"&gt; /u/Future_Inventor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olsh0j/best_setup_for_running_local_llms_budget_up_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olsh0j/best_setup_for_running_local_llms_budget_up_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olsh0j/best_setup_for_running_local_llms_budget_up_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T16:40:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1om1pa9</id>
    <title>Looking for a RAG UI manager to meet our needs to replace Zapier</title>
    <updated>2025-11-01T23:00:24+00:00</updated>
    <author>
      <name>/u/kingharrison</name>
      <uri>https://old.reddit.com/user/kingharrison</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We have new AI servers in our company and we are looking at ways to replace our AI services that we pay for. &lt;/p&gt; &lt;p&gt;One of them is looking to replace our reliance on Zapier for a chat agent. Zapier does a good job of delivering an easy to embed chat agent where you can create a knowledge base based off uploaded documents, scraping websites, and google docs AND setting up a resync schedule to pull in newer version. &lt;/p&gt; &lt;p&gt;Honestly very much a fan of Zapier. &lt;/p&gt; &lt;p&gt;However, there is a limit to how they manage their knowledge base that is making it difficult to achieve our goals &lt;/p&gt; &lt;p&gt;Note, I did reach out to Zapier to see if they could add these features, but I didnt get solid answers. I tried to suggest features, they were not accepted. So I feel like I have exhausted the 'please service provider, supply these features i would happily pay for!'. &lt;/p&gt; &lt;p&gt;So what I am looking to do is have some type of web based RAG management system. (this is important because in our company the people who would manage the RAG are not developer level technical, but they are experts in our business processes). &lt;/p&gt; &lt;p&gt;I am looking for the ability to create knowledge bases. Distinctly name these knowledge bases. &lt;/p&gt; &lt;p&gt;These knowledge bases need the ability to scrape website URLs I provide (we use a lot of scribes). It will pull in the text from the link (i am not worried about interpreting the images, but others might need that). This would also be google drive docs. &lt;/p&gt; &lt;p&gt;Then the ability to schedule rescraping of those links on a schedule. So we can update them, and theres a process that automatically updates whats in the RAG. &lt;/p&gt; &lt;p&gt;Last, a way we can attach multiple RAGs (or multiple knowledge bases... my vocab might be off so focus on the concept) to a requesting call on Ollama. &lt;/p&gt; &lt;p&gt;So send in a prompt on 11434, and say which RAGs / Knowledge bases to use. &lt;/p&gt; &lt;p&gt;Is all that possible? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kingharrison"&gt; /u/kingharrison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1om1pa9/looking_for_a_rag_ui_manager_to_meet_our_needs_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1om1pa9/looking_for_a_rag_ui_manager_to_meet_our_needs_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1om1pa9/looking_for_a_rag_ui_manager_to_meet_our_needs_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T23:00:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1om86jo</id>
    <title>I built a full hands-on vector search setup in Milvus using HuggingFace/Local embeddings ‚Äî no OpenAI key needed</title>
    <updated>2025-11-02T04:24:51+00:00</updated>
    <author>
      <name>/u/Humble_Preference_89</name>
      <uri>https://old.reddit.com/user/Humble_Preference_89</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone üëã&lt;br /&gt; I‚Äôve been exploring RAG foundations, and I wanted to share a step-by-step approach to get Milvus running locally, insert embeddings, and perform scalar + vector search through Python.&lt;/p&gt; &lt;p&gt;Here‚Äôs what the demo includes:&lt;br /&gt; ‚Ä¢ Milvus database + collection setup&lt;br /&gt; ‚Ä¢ Inserting text data with HuggingFace/Local embeddings&lt;br /&gt; ‚Ä¢ Querying with vector search&lt;br /&gt; ‚Ä¢ How this all connects to LLM-based RAG systems&lt;/p&gt; &lt;p&gt;Happy to answer ANY questions ‚Äî here‚Äôs the video walkthrough if it helps: &lt;a href="https://youtu.be/pEkVzI5spJ0"&gt;&lt;em&gt;https://youtu.be/pEkVzI5spJ0&lt;/em&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you have feedback or suggestions for improving this series,&lt;br /&gt; I would love to hear from you in the comments/discussion!&lt;/p&gt; &lt;p&gt;&lt;em&gt;P.S. Local Embeddings are only for hands-on educational purposes. They are not in league with optimized production performance.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Humble_Preference_89"&gt; /u/Humble_Preference_89 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1om86jo/i_built_a_full_handson_vector_search_setup_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1om86jo/i_built_a_full_handson_vector_search_setup_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1om86jo/i_built_a_full_handson_vector_search_setup_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T04:24:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1olr4uq</id>
    <title>Optimizations using llama.cpp command?</title>
    <updated>2025-11-01T15:47:12+00:00</updated>
    <author>
      <name>/u/pmttyji</name>
      <uri>https://old.reddit.com/user/pmttyji</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;sup&gt;Why are we not seeing threads like this frequently? Most of the time we see threads related to Big Hardware, Large GPU, etc., I really want to see more threads related to Optimizations, Tips/Tricks, Performance, CPU Only inference, etc., which are more useful for low config systems and more importantly we could get 100% performance benchmarks(Like what's the maximum t/s possible from 8GB model without any GPU&lt;/sup&gt;) with low level systems first by using those stuff. To put simply, we must try &lt;strong&gt;&lt;sup&gt;extreme possibilities from limited hardware&lt;/sup&gt;&lt;/strong&gt; &lt;sup&gt;first before buying new or additional rigs.&lt;/sup&gt;&lt;/p&gt; &lt;p&gt;All right, here my questions related to title.&lt;/p&gt; &lt;p&gt;1] &lt;strong&gt;-ot vs -ncmoe&lt;/strong&gt; .... I still see some people do use -ot even after -ncmoe. For Dense models, -ot is the way. But any reasons for -ot with MOE models when we have -ncmoe?(&lt;strong&gt;EDIT&lt;/strong&gt;: Exception - Multi GPUs case) Please share sample command examples.&lt;/p&gt; &lt;p&gt;2] Anyone use both -ot &amp;amp; -ncmoe &lt;strong&gt;together&lt;/strong&gt;? Will both work together first of all? If it is, what are possibilities to get more performance?&lt;/p&gt; &lt;p&gt;3] &lt;strong&gt;What else&lt;/strong&gt; can give us more performance? Apart from quantized KVCache, Flash Attention, threads. Am I missing &lt;strong&gt;any other important parameters&lt;/strong&gt;? or should I change value of existing parameters?&lt;/p&gt; &lt;p&gt;I'm hoping to get &lt;strong&gt;50 t/s&lt;/strong&gt; (&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1o7kkf0/poor_gpu_club_8gb_vram_moe_models_ts_with_llamacpp/"&gt;Currently getting 33 t/s without context&lt;/a&gt;) from Q4 of Qwen3-30B-A3B with my 8GB VRAM + 32GB RAM if possible. Expecting some experts/legends in this sub share their secret stash. My current command is below.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-bench -m E:\LLM\models\Qwen3-30B-A3B-UD-Q4_K_XL.gguf -ngl 99 -ncmoe 29 -fa 1 -ctk q8_0 -ctv q8_0 -b 2048 -ub 512 -t 8 | model | size | params | backend | ngl | threads | type_k | type_v | fa | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ------: | -----: | -----: | -: | --------------: | -------------------: | | qwen3moe 30B.A3B Q4_K - Medium | 16.49 GiB | 30.53 B | CUDA | 99 | 8 | q8_0 | q8_0 | 1 | pp512 | 160.45 ¬± 18.06 | | qwen3moe 30B.A3B Q4_K - Medium | 16.49 GiB | 30.53 B | CUDA | 99 | 8 | q8_0 | q8_0 | 1 | tg128 | 33.73 ¬± 0.74 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The reason I'm trying to squeeze this more, so I could get decent 20-30 t/s after adding 32-64K context(which is mandatory for agentic coding tools such as Roo code). Thanks a lot.&lt;/p&gt; &lt;p&gt;One other reason for this thread is, still some people not aware of both -ot &amp;amp; -ncmoe. Use it folks, don't leave any tokens at the table. You welcome.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmttyji"&gt; /u/pmttyji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olr4uq/optimizations_using_llamacpp_command/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olr4uq/optimizations_using_llamacpp_command/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olr4uq/optimizations_using_llamacpp_command/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T15:47:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1oltmre</id>
    <title>NVIDIA Nemotron Nano 12B V2 VL, vision and other models</title>
    <updated>2025-11-01T17:27:03+00:00</updated>
    <author>
      <name>/u/RobotRobotWhatDoUSee</name>
      <uri>https://old.reddit.com/user/RobotRobotWhatDoUSee</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I stumbled across &lt;a href="https://developer.nvidia.com/blog/develop-specialized-ai-agents-with-new-nvidia-nemotron-vision-rag-and-guardrail-models/"&gt;this&lt;/a&gt; the other day. Apparently one of these models has launched:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nvidia/NVIDIA-Nemotron-Nano-12B-v2-VL-BF16"&gt;Nemotron Nano 12B V2 VL&lt;/a&gt;&lt;/p&gt; &lt;p&gt;...and others are on the way. &lt;/p&gt; &lt;p&gt;Anyone played around with these new vision models yet?&lt;/p&gt; &lt;p&gt;Edit: in particular, I'm interested is anyone has them running in llama.cpp&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RobotRobotWhatDoUSee"&gt; /u/RobotRobotWhatDoUSee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oltmre/nvidia_nemotron_nano_12b_v2_vl_vision_and_other/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oltmre/nvidia_nemotron_nano_12b_v2_vl_vision_and_other/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oltmre/nvidia_nemotron_nano_12b_v2_vl_vision_and_other/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T17:27:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1om2nqy</id>
    <title>[P] Training Better LLMs with 30% Less Data ‚Äì Entropy-Based Data Distillation</title>
    <updated>2025-11-01T23:43:09+00:00</updated>
    <author>
      <name>/u/Jolly-Act9349</name>
      <uri>https://old.reddit.com/user/Jolly-Act9349</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1om2nqy/p_training_better_llms_with_30_less_data/"&gt; &lt;img alt="[P] Training Better LLMs with 30% Less Data ‚Äì Entropy-Based Data Distillation" src="https://a.thumbs.redditmedia.com/FvGLkMq1HBa1qbDFZdnwVjHyXma7vwQ3uhUXFySyDr4.jpg" title="[P] Training Better LLMs with 30% Less Data ‚Äì Entropy-Based Data Distillation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been experimenting with data-efficient LLM training as part of a project I'm calling &lt;strong&gt;Oren&lt;/strong&gt;, focused on entropy-based dataset filtering.&lt;/p&gt; &lt;p&gt;The philosophy behind this emerged from knowledge distillation pipelines, where student models basically inherit the same limitations of intelligence as the teacher models have. Thus, the goal of Oren is to change LLM training completely ‚Äì from the current frontier approach of rapidly upscaling in compute costs and GPU hours to a new strategy: optimizing training datasets for smaller, smarter models.&lt;/p&gt; &lt;p&gt;The experimentation setup: two identical 100M-parameter language models. &lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Model A:&lt;/strong&gt; trained on 700M raw tokens&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model B:&lt;/strong&gt; trained on the top 70% of samples (500M tokens) selected via entropy-based filtering&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Result:&lt;/strong&gt; Model B matched Model A in performance, while using 30% less data, time, and compute. No architecture or hyperparameter changes.&lt;/p&gt; &lt;p&gt;Open-source models:&lt;/p&gt; &lt;p&gt;ü§ó &lt;a href="https://huggingface.co/vitalune/nanochat-d10-raw-700m"&gt;Model A - Raw (700M tokens)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ü§ó &lt;a href="https://huggingface.co/vitalune/nanochat-d10-filtered-500m"&gt;Model B - Filtered (500M tokens)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'd love feedback, especially on how to generalize this into a reusable pipeline that can be directly applied onto LLMs before training and/or fine-tuning. Would love feedback from anyone here who has tried entropy or loss-based filtering and possibly even scaled it&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/2ywguh21eqyf1.png?width=4461&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ad3684760ca19910e37a7594255b8e935a20c7ac"&gt;https://preview.redd.it/2ywguh21eqyf1.png?width=4461&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ad3684760ca19910e37a7594255b8e935a20c7ac&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Jolly-Act9349"&gt; /u/Jolly-Act9349 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1om2nqy/p_training_better_llms_with_30_less_data/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1om2nqy/p_training_better_llms_with_30_less_data/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1om2nqy/p_training_better_llms_with_30_less_data/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T23:43:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1om7ccz</id>
    <title>If I want to train, fine tune, and do image gen then... DGX Spark?</title>
    <updated>2025-11-02T03:37:51+00:00</updated>
    <author>
      <name>/u/MontageKapalua6302</name>
      <uri>https://old.reddit.com/user/MontageKapalua6302</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If I want to train, fine tune, and do image gen, then do those reasons make the DGX Spark and clones worthwhile?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;From what I've heard on the positive:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Diffusion performance is strong.&lt;/p&gt; &lt;p&gt;MXFP4 performance is strong and doesn't make much of a quality hit.&lt;/p&gt; &lt;p&gt;Training performance is strong compared to the Strix Halo.&lt;/p&gt; &lt;p&gt;I can put two together to get 256 GB of memory and get significantly better performance as well as fit larger models or, more importantly, train larger models than I could with, say, Strix Halo or a 6000 Pro. Even if it's too slow or memory constrained for a larger model, I can proof of concept it.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;More specifically what I want to do (in order of importance):&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Fine tune (or train?) a model for niche text editing, using &amp;lt;5 GB of training data. Too much to fit into context by far. Start with a single machine and a smaller model. If that works well enough, buy another or rent time on a big machine, though I'm loathe to put my life's work on somebody else's computer. Then run that model on the DGX or another machine, depending on performance. Hopefully have enough space &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Image generation and editing for fun without annoying censorship. I keep asking for innocuous things, and I keep getting denied by online generators.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Play around with drone AI training.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I don't want to game, use Windows, or do anything else with the box. Except for the above needs, I don't care if it's on the CUDA stack. I own NVIDIA, AMD, and Apple hardware. I am agnostic towards these companies.&lt;/p&gt; &lt;p&gt;I can also wait for the M5 Ultra, but that could be more than a year away.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MontageKapalua6302"&gt; /u/MontageKapalua6302 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1om7ccz/if_i_want_to_train_fine_tune_and_do_image_gen/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1om7ccz/if_i_want_to_train_fine_tune_and_do_image_gen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1om7ccz/if_i_want_to_train_fine_tune_and_do_image_gen/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T03:37:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1om43ng</id>
    <title>A much, much easier math problem. Can your LLM solve it?</title>
    <updated>2025-11-02T00:51:45+00:00</updated>
    <author>
      <name>/u/Suspicious-Host9042</name>
      <uri>https://old.reddit.com/user/Suspicious-Host9042</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1om43ng/a_much_much_easier_math_problem_can_your_llm/"&gt; &lt;img alt="A much, much easier math problem. Can your LLM solve it?" src="https://b.thumbs.redditmedia.com/dGCXco2G7lNEGWGuP-Rr40mKuYCjMTuemiDmbV6zjxs.jpg" title="A much, much easier math problem. Can your LLM solve it?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ok43o7/comment/nm9v4fd/"&gt;Follow up of my previous thread&lt;/a&gt; where there was some controversy as to how easy the question is. I decided to use an easier problem. Here it is:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Let $M$ be an $R$-module ($R$ is a commutative ring) and $a \in R$ is not a zero divisor. What is $Ext^1_R(R/(a), M)$? Hint: use the projective resolution $... 0 \rightarrrow 0 \rightarrrow R \rightarrrow^{\times a} R \rightarrrow R/(a) \rightarrrow 0$&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;The correct answer is M/aM - &lt;a href="https://math.stackexchange.com/a/351560"&gt;Here's a link to the solution&lt;/a&gt; and &lt;a href="https://en.wikipedia.org/wiki/Ext_functor#Properties_of_Ext"&gt;the solution on Wikipedia.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here are my tests:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;gemma-3-12b&lt;/strong&gt; : got it wrong, said 0&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fvm83dufrqyf1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f0ea04980e9d3f5f17939d0ef3d337fd31cb0ada"&gt;https://preview.redd.it/fvm83dufrqyf1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f0ea04980e9d3f5f17939d0ef3d337fd31cb0ada&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;gpt-oss-20b&lt;/strong&gt; : thought for a few seconds, then got the correct answer.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/q38os8mgrqyf1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4ca670a024bd61f8683770454bc15b33be76680a"&gt;https://preview.redd.it/q38os8mgrqyf1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4ca670a024bd61f8683770454bc15b33be76680a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;qwen3-30b-a3b-instruct-2507&lt;/strong&gt; : kept on second guessing itself, but eventually got it.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ri2cow9hrqyf1.png?width=1163&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7a879285cb395535a2ee4c3b88ae96820fab491b"&gt;https://preview.redd.it/ri2cow9hrqyf1.png?width=1163&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7a879285cb395535a2ee4c3b88ae96820fab491b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;mn-violet-lotus&lt;/strong&gt; : got it in seconds.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/cu2v8hrhrqyf1.png?width=1163&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b9669f3ca912de5d8c81fda77b22f4fb862f094f"&gt;https://preview.redd.it/cu2v8hrhrqyf1.png?width=1163&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b9669f3ca912de5d8c81fda77b22f4fb862f094f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Does your LLM get the correct answer?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Suspicious-Host9042"&gt; /u/Suspicious-Host9042 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1om43ng/a_much_much_easier_math_problem_can_your_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1om43ng/a_much_much_easier_math_problem_can_your_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1om43ng/a_much_much_easier_math_problem_can_your_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T00:51:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1olrjvc</id>
    <title>Google's new AI model (C2S-Scale 27B) - innovation or hype</title>
    <updated>2025-11-01T16:03:56+00:00</updated>
    <author>
      <name>/u/Emergency-Loss-5961</name>
      <uri>https://old.reddit.com/user/Emergency-Loss-5961</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Recently, Google introduced a new AI model (C2S-Scale 27B) that helped identify a potential combination therapy for cancer, pairing silmitasertib with interferon to make ‚Äúcold‚Äù tumors more visible to the immune system. &lt;/p&gt; &lt;p&gt;On paper, that sounds incredible. An AI model generating new biological hypotheses that are then experimentally validated. But here‚Äôs a thought I couldn‚Äôt ignore. If the model simply generated hundreds or thousands of possible combinations and researchers later found one that worked, is that truly intelligence or just statistical luck? &lt;/p&gt; &lt;p&gt;If it actually narrowed down the list through meaningful biological insight, that‚Äôs a real step forward. But if not, it risks being a ‚Äúshotgun‚Äù approach, flooding researchers with possibilities they still need to manually validate. &lt;/p&gt; &lt;p&gt;So, what do you think? Does this kind of result represent genuine AI innovation in science or just a well-packaged form of computational trial and error?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Emergency-Loss-5961"&gt; /u/Emergency-Loss-5961 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olrjvc/googles_new_ai_model_c2sscale_27b_innovation_or/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olrjvc/googles_new_ai_model_c2sscale_27b_innovation_or/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olrjvc/googles_new_ai_model_c2sscale_27b_innovation_or/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T16:03:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1om7s1y</id>
    <title>What am I doing wrong with GPT-OSS 120b on 2x 7900 XT w/ 128GB DDR5?</title>
    <updated>2025-11-02T04:02:44+00:00</updated>
    <author>
      <name>/u/InfinityApproach</name>
      <uri>https://old.reddit.com/user/InfinityApproach</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've often run across numbers like the attached on GPT-OSS 120b. Despite me having 40GB of VRAM, I cannot get any faster than 350 t/s pp and 30 t/s tg. Yet a system with only 12GB of VRAM is getting 25 tg! What am I doing wrong?&lt;/p&gt; &lt;p&gt;Here's the best settings I've found: &lt;/p&gt; &lt;p&gt;&lt;code&gt;llama-bench -m &amp;quot;F:\LLMs\unsloth\gpt-oss-120b-GGUF\gpt-oss-120b-Q4_K_S-00001-of-00002.gguf&amp;quot; -fa 1 -ngl 999 -ncmoe 16 -ub 4096 -mmp 0 -mg 0 -ts &amp;quot;0.65;0.35&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&amp;quot;-ncmoe 16&amp;quot; is the sweet spot for offloading moe layers to my two GPUs&lt;/li&gt; &lt;li&gt;I'm doing a tensor split of 0.65;0.35 to account for my primary GPU having less usable VRAM because of the Windows desktop. Both GPUs are loaded to just under 20GB.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Specs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Win 11&lt;/li&gt; &lt;li&gt;Ryzen 7900x&lt;/li&gt; &lt;li&gt;128 GB DDR5 @ 6000, two sticks of 64GB&lt;/li&gt; &lt;li&gt;2x Radeon 7900xt GPUs, 20GB each&lt;/li&gt; &lt;li&gt;Latest Radeon PRO drivers&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Here's the best I can muster after lots of tinkering:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;ggml_vulkan: Found 2 Vulkan devices:&lt;/p&gt; &lt;p&gt;ggml_vulkan: 0 = AMD Radeon RX 7900 XT (AMD proprietary driver) | uma: 0 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 32768 | int dot: 1 | matrix cores: KHR_coopmat&lt;/p&gt; &lt;p&gt;ggml_vulkan: 1 = AMD Radeon RX 7900 XT (AMD proprietary driver) | uma: 0 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 32768 | int dot: 1 | matrix cores: KHR_coopmat&lt;/p&gt; &lt;p&gt;| model | size | params | backend | ngl | n_ubatch | fa | ts | mmap | test | t/s |&lt;/p&gt; &lt;p&gt;| ------------------------------ | ---------: | ---------: | ---------- | --: | -------: | -: | ------------ | ---: | --------------: | -------------------: |&lt;/p&gt; &lt;p&gt;| gpt-oss 120B Q4_K - Small | 58.44 GiB | 116.83 B | Vulkan | 999 | 4096 | 1 | 0.65/0.35 | 0 | pp512 | 346.71 ¬± 3.42 |&lt;/p&gt; &lt;p&gt;| gpt-oss 120B Q4_K - Small | 58.44 GiB | 116.83 B | Vulkan | 999 | 4096 | 1 | 0.65/0.35 | 0 | tg128 | 29.98 ¬± 0.49 |&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Other details:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I've found that Vulkan is better than ROCM on my system&lt;/li&gt; &lt;li&gt;When I use a single GPU with 12 layers (maximizing 20GB VRAM), the best I can get is 12 t/s tg. That's compared to a single 4070 TI getting 25 tg.&lt;/li&gt; &lt;li&gt;On LM Studio, which doesn't allow me to tensor-split or offload 16 moe layers, the best I can do is load 20 layers and get 19 t/s tg.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Am I right that these numbers are low for my hardware? What settings should I change to speed it up?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InfinityApproach"&gt; /u/InfinityApproach &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/LocalAIServers/comments/1oj6j9q/gptoss120b_2x_mi50_32gb_update_now_optimized_on/nmi37cn/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1om7s1y/what_am_i_doing_wrong_with_gptoss_120b_on_2x_7900/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1om7s1y/what_am_i_doing_wrong_with_gptoss_120b_on_2x_7900/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T04:02:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1om26g2</id>
    <title>Why don‚Äôt more apps run AI locally?</title>
    <updated>2025-11-01T23:21:27+00:00</updated>
    <author>
      <name>/u/elinaembedl</name>
      <uri>https://old.reddit.com/user/elinaembedl</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been seeing more talk about running small LLMs locally on phones.&lt;/p&gt; &lt;p&gt;Almost every new phone ships with dedicated AI hardware (NPU,GPU, etc). Still, very few apps seem to use them to run models on-device.&lt;/p&gt; &lt;p&gt;What‚Äôs holding local inference back on mobile in your experience?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/elinaembedl"&gt; /u/elinaembedl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1om26g2/why_dont_more_apps_run_ai_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1om26g2/why_dont_more_apps_run_ai_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1om26g2/why_dont_more_apps_run_ai_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T23:21:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1olsliw</id>
    <title>MiniMax-M2-exl3 - now with CatBench‚Ñ¢</title>
    <updated>2025-11-01T16:45:44+00:00</updated>
    <author>
      <name>/u/Unstable_Llama</name>
      <uri>https://old.reddit.com/user/Unstable_Llama</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olsliw/minimaxm2exl3_now_with_catbench/"&gt; &lt;img alt="MiniMax-M2-exl3 - now with CatBench‚Ñ¢" src="https://b.thumbs.redditmedia.com/hZFUwp417AAuD17RUalBhIzjpyePeIINpbOg8MJu3Xo.jpg" title="MiniMax-M2-exl3 - now with CatBench‚Ñ¢" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/turboderp/MiniMax-M2-exl3"&gt;https://huggingface.co/turboderp/MiniMax-M2-exl3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;‚ö†Ô∏è Requires ExLlamaV3 v0.0.12&lt;/p&gt; &lt;p&gt;Use the optimized quants if you can fit them!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3hxgebenboyf1.jpg?width=836&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=339786475037e8109c89de298db8c14dfd6bbb45"&gt;https://preview.redd.it/3hxgebenboyf1.jpg?width=836&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=339786475037e8109c89de298db8c14dfd6bbb45&lt;/a&gt;&lt;/p&gt; &lt;p&gt;True AGI will make the best cat memes. You'll see it here first ;)&lt;/p&gt; &lt;p&gt;Exllama discord: &lt;a href="https://discord.gg/GJmQsU7T"&gt;https://discord.gg/GJmQsU7T&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Unstable_Llama"&gt; /u/Unstable_Llama &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olsliw/minimaxm2exl3_now_with_catbench/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olsliw/minimaxm2exl3_now_with_catbench/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olsliw/minimaxm2exl3_now_with_catbench/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T16:45:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1olkx65</id>
    <title>Gaming PC converted to AI Workstation</title>
    <updated>2025-11-01T10:56:09+00:00</updated>
    <author>
      <name>/u/highdefw</name>
      <uri>https://old.reddit.com/user/highdefw</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olkx65/gaming_pc_converted_to_ai_workstation/"&gt; &lt;img alt="Gaming PC converted to AI Workstation" src="https://preview.redd.it/z55xgdghmmyf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=58f471128678bd5884598fa5d1393664c1759fe5" title="Gaming PC converted to AI Workstation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;RTX Pro 5000 and 4000 just arrived. NVME expansion slot on the bottom. 5950x with 128gb ram. Future upgrade will be a cpu upgrade.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/highdefw"&gt; /u/highdefw &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/z55xgdghmmyf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olkx65/gaming_pc_converted_to_ai_workstation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olkx65/gaming_pc_converted_to_ai_workstation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T10:56:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1om8wdf</id>
    <title>Do you have any "AI toy projects"?</title>
    <updated>2025-11-02T05:04:35+00:00</updated>
    <author>
      <name>/u/MustBeSomethingThere</name>
      <uri>https://old.reddit.com/user/MustBeSomethingThere</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1om8wdf/do_you_have_any_ai_toy_projects/"&gt; &lt;img alt="Do you have any &amp;quot;AI toy projects&amp;quot;?" src="https://external-preview.redd.it/ZW1iMzQxdDB4cnlmMeDjq5qHZ2-J4399WQRf0LNpnjuXZ3nb5V768lbHJFZZ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=904f40fa717f5b1bf416cec3465963053f8e879b" title="Do you have any &amp;quot;AI toy projects&amp;quot;?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I share my toy project as an example: &lt;a href="https://github.com/PasiKoodaa/TextTube"&gt;https://github.com/PasiKoodaa/TextTube&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Maybe in 10-15 years most streaming services will be replaced by local AI content creators.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MustBeSomethingThere"&gt; /u/MustBeSomethingThere &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/iy7yl0u0xryf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1om8wdf/do_you_have_any_ai_toy_projects/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1om8wdf/do_you_have_any_ai_toy_projects/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T05:04:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1olqjxj</id>
    <title>Official GGUFs in Qwen3-VL Collection - 235B/32B/30B/8B/4B/2B</title>
    <updated>2025-11-01T15:23:56+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olqjxj/official_ggufs_in_qwen3vl_collection/"&gt; &lt;img alt="Official GGUFs in Qwen3-VL Collection - 235B/32B/30B/8B/4B/2B" src="https://external-preview.redd.it/_7BYCEiuSe8H_fldVM7chLfCb5j0ciz_pk_F5HpmBuY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=de9f337f24ae40e158287e6812e9408e53add8ae" title="Official GGUFs in Qwen3-VL Collection - 235B/32B/30B/8B/4B/2B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/Qwen/qwen3-vl"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olqjxj/official_ggufs_in_qwen3vl_collection/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olqjxj/official_ggufs_in_qwen3vl_collection/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T15:23:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1om03mi</id>
    <title>AMD EPYC 4565P is a beast</title>
    <updated>2025-11-01T21:50:40+00:00</updated>
    <author>
      <name>/u/coding9</name>
      <uri>https://old.reddit.com/user/coding9</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Haven‚Äôt seen too much coverage on these CPUs but I got a system with it. I can get over 15t/s on gpt-oss 20b with cpu only on 5600mhz ecc ram. &lt;/p&gt; &lt;p&gt;Pretty surprised it‚Äôs this good with the avx 512 instruction set. &lt;/p&gt; &lt;p&gt;Anyone else using these or have any thoughts?&lt;/p&gt; &lt;p&gt;Edit: this wasn‚Äôt purchased for inference so I‚Äôm just excited it can do some basic stuff with it as well&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/coding9"&gt; /u/coding9 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1om03mi/amd_epyc_4565p_is_a_beast/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1om03mi/amd_epyc_4565p_is_a_beast/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1om03mi/amd_epyc_4565p_is_a_beast/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T21:50:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1om81j1</id>
    <title>glm-proxy - A Proxy Server I Built to Fix GLM 4.5 Air's Tool Call Issues</title>
    <updated>2025-11-02T04:17:09+00:00</updated>
    <author>
      <name>/u/akirose1004</name>
      <uri>https://old.reddit.com/user/akirose1004</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1om81j1/glmproxy_a_proxy_server_i_built_to_fix_glm_45/"&gt; &lt;img alt="glm-proxy - A Proxy Server I Built to Fix GLM 4.5 Air's Tool Call Issues" src="https://b.thumbs.redditmedia.com/7K256kODiuLD_3gm4UtRLndAWItovQwEv3qnrZaI3FI.jpg" title="glm-proxy - A Proxy Server I Built to Fix GLM 4.5 Air's Tool Call Issues" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/mdu32bj6kryf1.png?width=476&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d6c99630db5ed36a9a2340d77a8bae86f2d708cc"&gt;https://preview.redd.it/mdu32bj6kryf1.png?width=476&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d6c99630db5ed36a9a2340d77a8bae86f2d708cc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I was running GLM 4.5 Air on my MacBook M4 Max with LM Studio, but &lt;strong&gt;tool calls weren't working properly&lt;/strong&gt;, which meant I couldn't use qwen-code CLI. I wanted to use an OpenAI-compatible interface, and this constant friction frustrated me enough to build a solution.&lt;/p&gt; &lt;p&gt;A proxy server that &lt;strong&gt;automatically converts GLM's XML-formatted tool calls to OpenAI-compatible format&lt;/strong&gt;. Now you can use any OpenAI-compatible client (like qwen-code) with GLM seamlessly!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Full OpenAI API compatibility&lt;/li&gt; &lt;li&gt;Automatic conversion of GLM's XML &lt;code&gt;&amp;lt;tool_call&amp;gt;&lt;/code&gt; format to OpenAI JSON format&lt;/li&gt; &lt;li&gt;Streaming support&lt;/li&gt; &lt;li&gt;Multiple tool calls and complex JSON argument parsing&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Point any OpenAI-compatible client (qwen-code, LangChain, etc.) to this address and use GLM 4.5 Air as if it were OpenAI!&lt;/p&gt; &lt;h1&gt;üîó GitHub&lt;/h1&gt; &lt;p&gt;&lt;a href="https://github.com/akirose/glm-proxy"&gt;https://github.com/akirose/glm-proxy&lt;/a&gt; (MIT License)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;If you're using GLM 4.5 with LM Studio, no more tool call headaches!&lt;/strong&gt; üòä&lt;/p&gt; &lt;p&gt;Feedback and suggestions welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/akirose1004"&gt; /u/akirose1004 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1om81j1/glmproxy_a_proxy_server_i_built_to_fix_glm_45/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1om81j1/glmproxy_a_proxy_server_i_built_to_fix_glm_45/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1om81j1/glmproxy_a_proxy_server_i_built_to_fix_glm_45/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T04:17:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1olmj9z</id>
    <title>Bought MI50 32 Gb from Alibaba. Did I get scammed?</title>
    <updated>2025-11-01T12:26:47+00:00</updated>
    <author>
      <name>/u/Moist_Toto</name>
      <uri>https://old.reddit.com/user/Moist_Toto</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olmj9z/bought_mi50_32_gb_from_alibaba_did_i_get_scammed/"&gt; &lt;img alt="Bought MI50 32 Gb from Alibaba. Did I get scammed?" src="https://preview.redd.it/v3w8clon2nyf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0ef938653e71635a81d9e3e2eaf625cfbf73033e" title="Bought MI50 32 Gb from Alibaba. Did I get scammed?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I bought 8 MI50 32Gb units from someone on Alibaba.&lt;/p&gt; &lt;p&gt;After spending some time to figure out Linux and the software stack, I entered the 'amd-smi static' command in the terminal.&lt;/p&gt; &lt;p&gt;The result is quite frightening, here it is: &lt;/p&gt; &lt;p&gt;especially the bottom part product name saying &amp;quot;16GB&amp;quot;, my heart skipped a beat. Is this something driver related or am I screwed?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Moist_Toto"&gt; /u/Moist_Toto &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/v3w8clon2nyf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olmj9z/bought_mi50_32_gb_from_alibaba_did_i_get_scammed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olmj9z/bought_mi50_32_gb_from_alibaba_did_i_get_scammed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T12:26:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1olouiw</id>
    <title>TIL: For long-lived LLM sessions, swapping KV Cache to RAM is ~10x faster than recalculating it. Why isn't this a standard feature?</title>
    <updated>2025-11-01T14:12:57+00:00</updated>
    <author>
      <name>/u/Shoddy-Tutor9563</name>
      <uri>https://old.reddit.com/user/Shoddy-Tutor9563</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I was diving into how vLLM and similar inference servers work and had a thought about optimizing memory for long-lived but inactive chat sessions. The standard approach seems to be either keeping the KV Cache in precious VRAM or evicting it and recalculating from scratch when the user returns. I think there might be a better way.&lt;/p&gt; &lt;p&gt;Here's the core idea: Implement a swapping mechanism for the KV Cache of inactive sessions, moving it from VRAM to system RAM (and back), instead of deleting it.&lt;/p&gt; &lt;p&gt;We always focus on the high cost of moving data between CPU and GPU, but we often forget the cost of recalculating that data. Let's do a quick back-of-the-napkin comparison for a Qwen3-4B-like model with a 16k token context:&lt;/p&gt; &lt;p&gt;Scenario: A user's session becomes inactive. Their 16k-token KV Cache is evicted. Later, they return. We need to restore their context.&lt;/p&gt; &lt;p&gt;¬∑ Option A: Recalculate the KV Cache (Standard Approach) ¬∑ This requires a full &amp;quot;prefill&amp;quot; pass over the entire 16k token prompt. ¬∑ Estimated Time: ~1.5 to 3 seconds on a modern GPU. ¬∑ Option B: Swapping (Proposed Approach) ¬∑ We simply copy the ~4 GB of KV Cache data from system RAM back to VRAM over PCIe. ¬∑ Estimated Time: ~200-400 ms (on PCIe 4.0).&lt;/p&gt; &lt;p&gt;The math is pretty compelling. Swapping is roughly 7-15x faster than a full recalculation. For a user, waiting 200ms for their chat history to &amp;quot;wake up&amp;quot; is a much better experience than waiting 2+ seconds.&lt;/p&gt; &lt;p&gt;This wouldn't be for high-throughput, always-online inference, but specifically for managing many long-lived sessions (e.g., support chatbots, document analysis with breaks, multi-user systems with intermittent activity). It's a classic space-time tradeoff, but in this case, using slightly more &amp;quot;space&amp;quot; (system RAM) saves a huge amount of &amp;quot;time&amp;quot; (latency on reactivation).&lt;/p&gt; &lt;p&gt;So, I have two main questions for the community:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Did I mess up my calculations or reasoning anywhere? Are there hidden costs or architectural limitations (e.g., in vLLM, PyTorch, or CUDA) that make this swapping idea less practical than it seems on paper?&lt;/li&gt; &lt;li&gt;Has anyone seen or heard of implementations doing this? I know vLLM's PagedAttention is genius for VRAM management, but I haven't found anything about spilling over to CPU RAM. Are there any forks, research papers, or other inference engines exploring this?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Keen to hear your thoughts and correct any misunderstandings I might have!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Shoddy-Tutor9563"&gt; /u/Shoddy-Tutor9563 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olouiw/til_for_longlived_llm_sessions_swapping_kv_cache/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olouiw/til_for_longlived_llm_sessions_swapping_kv_cache/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olouiw/til_for_longlived_llm_sessions_swapping_kv_cache/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T14:12:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1olytpd</id>
    <title>Qwen3-VL is impressive!</title>
    <updated>2025-11-01T20:56:56+00:00</updated>
    <author>
      <name>/u/KraiiFox</name>
      <uri>https://old.reddit.com/user/KraiiFox</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olytpd/qwen3vl_is_impressive/"&gt; &lt;img alt="Qwen3-VL is impressive!" src="https://external-preview.redd.it/d2xhbXRjcGxscHlmMUowvrHmMIpZo4AiauGE1Mcv4FXKd8bkFKJe4QU1BrJL.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=46a71b7402921f97028babf1e571a097b79a162c" title="Qwen3-VL is impressive!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KraiiFox"&gt; /u/KraiiFox &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/sfcu47ollpyf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olytpd/qwen3vl_is_impressive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olytpd/qwen3vl_is_impressive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T20:56:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1olxijp</id>
    <title>List of interesting open-source models released this month.</title>
    <updated>2025-11-01T20:03:07+00:00</updated>
    <author>
      <name>/u/Acrobatic-Tomato4862</name>
      <uri>https://old.reddit.com/user/Acrobatic-Tomato4862</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! I've been tracking the latest AI model releases and wanted to share a curated list of AI models released this month.&lt;/p&gt; &lt;p&gt;Credit to &lt;a href="/u/duarteeeeee"&gt;u/duarteeeeee&lt;/a&gt; for finding all these models.&lt;/p&gt; &lt;p&gt;Here's a chronological breakdown of some of the most interesting open models released around October 1st - 31st, 2025:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;October 1st:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/Liquid4All/liquid-audio"&gt;&lt;strong&gt;LFM2-Audio-1.5B&lt;/strong&gt;&lt;/a&gt; (Liquid AI): Low-latency, end-to-end audio foundation model.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/nineninesix/kani-tts-370m"&gt;&lt;strong&gt;KaniTTS-370M&lt;/strong&gt;&lt;/a&gt; (NineNineSix): Fast, open-source TTS for real-time applications.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 2nd:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.ibm.com/new/announcements/ibm-granite-4-0-hyper-efficient-high-performance-hybrid-models"&gt;&lt;strong&gt;Granite 4.0&lt;/strong&gt;&lt;/a&gt; (IBM): Hyper-efficient, hybrid models for enterprise use.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/neuphonic/neutts-air"&gt;&lt;strong&gt;NeuTTS Air&lt;/strong&gt;&lt;/a&gt; (Neuphonic Speech): On-device TTS with instant voice cloning.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 3rd:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://simular.ai/articles/agent-s3"&gt;&lt;strong&gt;Agent S3&lt;/strong&gt;&lt;/a&gt; (Simular): Open framework for human-like computer use.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/inclusionAI/Ming-UniVision-16B-A3B"&gt;&lt;strong&gt;Ming-UniVision-16B-A3B&lt;/strong&gt;&lt;/a&gt; (Ant Group): Unified vision understanding, generation, editing model.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/character-ai/Ovi"&gt;&lt;strong&gt;Ovi (TTV/ITV)&lt;/strong&gt;&lt;/a&gt; (Character.AI / Yale): Open-source framework for offline talking avatars.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Salesforce/CoDA-v0-Instruct"&gt;&lt;strong&gt;CoDA-v0-Instruct&lt;/strong&gt;&lt;/a&gt; (Salesforce AI Research): Bidirectional diffusion model for code generation.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 4th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-VL-30B-A3B-Instruct"&gt;&lt;strong&gt;Qwen3-VL-30B-A3B-Instruct&lt;/strong&gt;&lt;/a&gt; (Alibaba): Powerful vision-language model for agentic tasks.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/DecartAI/Decart-XR"&gt;&lt;strong&gt;DecartXR&lt;/strong&gt;&lt;/a&gt; (Decart AI): Open-source Quest app for realtime video-FX.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 7th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/LiquidAI/LFM2-8B-A1B"&gt;&lt;strong&gt;LFM2-8B-A1B&lt;/strong&gt;&lt;/a&gt; (Liquid AI): Efficient on-device mixture-of-experts model.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/Tencent-Hunyuan/HunyuanVision"&gt;&lt;strong&gt;Hunyuan-Vision-1.5-Thinking&lt;/strong&gt;&lt;/a&gt; (Tencent): Multimodal &amp;quot;thinking on images&amp;quot; reasoning model.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/bageldotcom/paris"&gt;&lt;strong&gt;Paris&lt;/strong&gt;&lt;/a&gt; (Bagel Network): Decentralized-trained open-weight diffusion model.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/cumulo-autumn/StreamDiffusion"&gt;&lt;strong&gt;StreamDiffusionV2&lt;/strong&gt;&lt;/a&gt; (UC Berkeley, MIT, et al.): Open-source pipeline for real-time video streaming.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 8th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/ai21labs/Jamba-v0.1"&gt;&lt;strong&gt;Jamba Reasoning 3B&lt;/strong&gt;&lt;/a&gt; (AI21 Labs): Small hybrid model for on-device reasoning.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/inclusionAI/Ring-1T"&gt;&lt;strong&gt;Ling-1T / Ring-1T&lt;/strong&gt;&lt;/a&gt; (Ant Group): Trillion-parameter thinking/non-thinking open models.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/TingtingLiao/mimix"&gt;&lt;strong&gt;Mimix&lt;/strong&gt;&lt;/a&gt; (Research): Framework for multi-character video generation.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 9th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/microsoft/UserLM-8b"&gt;&lt;strong&gt;UserLM-8b&lt;/strong&gt;&lt;/a&gt; (Microsoft): Open-weight model simulating a &amp;quot;user&amp;quot; role.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/RadicalNumerics/RND1"&gt;&lt;strong&gt;RND1-Base-0910&lt;/strong&gt;&lt;/a&gt; (Radical Numerics): Experimental diffusion language model (30B MoE).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 10th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Kwaipilot/KAT-Dev-72B-Exp"&gt;&lt;strong&gt;KAT-Dev-72B-Exp&lt;/strong&gt;&lt;/a&gt; (Kwaipilot): Open-source experimental model for agentic coding.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 12th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://pbihao.github.io/projects/DreamOmni2/"&gt;&lt;strong&gt;DreamOmni2&lt;/strong&gt;&lt;/a&gt; (ByteDance): Multimodal instruction-based image editing/generation.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 13th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/mit-han-lab/streaming-vlm"&gt;&lt;strong&gt;StreamingVLM&lt;/strong&gt;&lt;/a&gt; (MIT Han Lab): Real-time understanding for infinite video streams.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 14th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL"&gt;&lt;strong&gt;Qwen3-VL-4B / 8B&lt;/strong&gt;&lt;/a&gt; (Alibaba): Efficient, open vision-language models for edge.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 16th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/PaddlePaddle/PaddleOCR-VL"&gt;&lt;strong&gt;PaddleOCR-VL&lt;/strong&gt;&lt;/a&gt; (Baidu): Lightweight 109-language document parsing model.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/facebook/MobileLLM-Pro"&gt;&lt;strong&gt;MobileLLM-Pro&lt;/strong&gt;&lt;/a&gt; (Meta): 1B parameter on-device model (128k context).&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/Tencent-Hunyuan/HunyuanWorld-1.0"&gt;&lt;strong&gt;FlashWorld&lt;/strong&gt;&lt;/a&gt; (Tencent): Fast (5-10 sec) 3D scene generation.&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.worldlabs.ai/blog/rtfm"&gt;&lt;strong&gt;RTFM (Real-Time Frame Model)&lt;/strong&gt;&lt;/a&gt; (WorldLabs): Real-time, interactive 3D world generation.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 17th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/inclusionAI/LLaDA2.0-flash-preview"&gt;&lt;strong&gt;LLaDA2.0-flash-preview&lt;/strong&gt;&lt;/a&gt; (Ant Group): 100B MoE diffusion model for reasoning/code.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 20th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/deepseek-ai/DeepSeek-OCR"&gt;&lt;strong&gt;DeepSeek-OCR&lt;/strong&gt;&lt;/a&gt; (DeepseekAI): Open-source model for optical context-compression.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/krea-ai/realtime-video"&gt;&lt;strong&gt;Krea Realtime 14B&lt;/strong&gt;&lt;/a&gt; (Krea AI): 14B open-weight real-time video generation.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 21st:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-VL-32B-Instruct"&gt;&lt;strong&gt;Qwen3-VL-2B / 32B&lt;/strong&gt;&lt;/a&gt; (Alibaba): Open, dense VLMs for edge and cloud.&lt;/li&gt; &lt;li&gt;&lt;a href="https://arxiv.org/abs/2510.14876"&gt;&lt;strong&gt;BADAS-Open&lt;/strong&gt;&lt;/a&gt; (Nexar): Ego-centric collision prediction model for ADAS.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 22nd:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/LiquidAI/LFM2-VL-3B"&gt;&lt;strong&gt;LFM2-VL-3B&lt;/strong&gt;&lt;/a&gt; (Liquid AI): Efficient vision-language model for edge deployment.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/tencent/HunyuanWorld-1"&gt;&lt;strong&gt;HunyuanWorld-1.1&lt;/strong&gt;&lt;/a&gt; (Tencent): 3D world generation from multi-view/video.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/PokeeAI/pokee_research_7b"&gt;&lt;strong&gt;PokeeResearch-7B&lt;/strong&gt;&lt;/a&gt; (Pokee AI): Open 7B deep-research agent (search/synthesis).&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/allenai/olmOCR-2-7B-1025"&gt;&lt;strong&gt;olmOCR-2-7B-1025&lt;/strong&gt;&lt;/a&gt; (Allen Institute for AI): Open-source, single-pass PDF-to-structured-text model.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 23rd:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://ltx.video/"&gt;&lt;strong&gt;LTX 2&lt;/strong&gt;&lt;/a&gt; (Lightricks): Open-source 4K video engine for consumer GPUs.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/lightonai/LightOnOCR-1B-1025"&gt;&lt;strong&gt;LightOnOCR-1B&lt;/strong&gt;&lt;/a&gt; (LightOn): Fast, 1B-parameter open-source OCR VLM.&lt;/li&gt; &lt;li&gt;&lt;a href="https://holo-cine.github.io/"&gt;&lt;strong&gt;HoloCine&lt;/strong&gt;&lt;/a&gt; (Research): Model for holistic, multi-shot cinematic narratives.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 24th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/tahoebio/tahoe-x1"&gt;&lt;strong&gt;Tahoe-x1&lt;/strong&gt;&lt;/a&gt; (Tahoe Therapeutics): 3B open-source single-cell biology model.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/PRIME-RL/P1-30B-A3B"&gt;&lt;strong&gt;P1&lt;/strong&gt;&lt;/a&gt; (PRIME-RL): Model mastering Physics Olympiads with RL.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 25th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/meituan-longcat/LongCat-Video"&gt;&lt;strong&gt;LongCat-Video&lt;/strong&gt;&lt;/a&gt; (Meituan): 13.6B open model for long video generation.&lt;/li&gt; &lt;li&gt;&lt;a href="https://arxiv.org/html/2510.19944v1"&gt;&lt;strong&gt;Seed 3D 1.0&lt;/strong&gt;&lt;/a&gt; (ByteDance): Generates simulation-grade 3D assets from images.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 27th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.minimax.io/news/minimax-m2"&gt;&lt;strong&gt;Minimax M2&lt;/strong&gt;&lt;/a&gt; (Minimax): Open-sourced intelligence engine for agentic workflows.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/inclusionAI/Ming-flash-omni-Preview"&gt;&lt;strong&gt;Ming-flash-omni-Preview&lt;/strong&gt;&lt;/a&gt; (Ant Group): 100B MoE omni-modal model for perception.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/inclusionAI/LLaDA2.0-mini-preview"&gt;&lt;strong&gt;LLaDA2.0-mini-preview&lt;/strong&gt;&lt;/a&gt; (Ant Group): 16B MoE diffusion model for language.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 28th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/LiquidAI/LFM2-ColBERT-350M"&gt;&lt;strong&gt;LFM2-ColBERT-350M&lt;/strong&gt;&lt;/a&gt; (Liquid AI): Multilingual &amp;quot;late interaction&amp;quot; RAG retriever model.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/ibm-granite/granite-4.0-350m"&gt;&lt;strong&gt;Granite 4.0 Nano (1B / 350M)&lt;/strong&gt;&lt;/a&gt; (IBM): Smallest open models for on-device use.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/HKUDS/ViMax"&gt;&lt;strong&gt;ViMax&lt;/strong&gt;&lt;/a&gt; (HKUDS): Agentic framework for end-to-end video creation.&lt;/li&gt; &lt;li&gt;&lt;a href="https://build.nvidia.com/nvidia/nemotron-nano-12b-v2-vl/modelcard"&gt;&lt;strong&gt;Nemotron Nano v2 VL&lt;/strong&gt;&lt;/a&gt; (NVIDIA): 12B open model for multi-image/video understanding.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 29th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://openai.com/index/introducing-gpt-oss-safeguard/"&gt;&lt;strong&gt;gpt-oss-safeguard&lt;/strong&gt;&lt;/a&gt; (OpenAI): Open-weight reasoning models for safety classification.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/morphicfilms/frames-to-video"&gt;&lt;strong&gt;Frames to Video&lt;/strong&gt;&lt;/a&gt; (Morphic): Open-source model for keyframe video interpolation.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/Bria-AI/FIBO"&gt;&lt;strong&gt;Fibo&lt;/strong&gt;&lt;/a&gt; (Bria AI): SOTA open-source model (trained on licensed data).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 30th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/baaivision/Emu3.5"&gt;&lt;strong&gt;Emu3.5&lt;/strong&gt;&lt;/a&gt; (BAAI): Native multimodal model as a world learner.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/moonshotai/Kimi-Linear-48B-A3B-Instruct"&gt;&lt;strong&gt;Kimi-Linear-48B-A3B&lt;/strong&gt;&lt;/a&gt; (Moonshot AI): Long-context model using a linear-attention mechanism.&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.modelscope.cn/studios/BlinkDL/RWKV-CHN-2/summary"&gt;&lt;strong&gt;RWKV-7 G0a3 7.2B&lt;/strong&gt;&lt;/a&gt; (BlinkDL): A multilingual RNN-based large language model.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/alibaba/UI-Ins"&gt;&lt;strong&gt;UI-Ins-32B / 7B&lt;/strong&gt;&lt;/a&gt; (Alibaba): GUI grounding agent.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please correct me if I have misclassified/mislinked any of the above models. This is my first post, so I am expecting there might be some mistakes.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Acrobatic-Tomato4862"&gt; /u/Acrobatic-Tomato4862 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olxijp/list_of_interesting_opensource_models_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olxijp/list_of_interesting_opensource_models_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olxijp/list_of_interesting_opensource_models_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T20:03:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohqev8</id>
    <title>Best Local TTS/STT Models - October 2025</title>
    <updated>2025-10-27T21:00:42+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Share what your favorite TTS / STT models are right now &lt;strong&gt;and why.&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Given the the amount of ambiguity and subjectivity in rating/testing these models, please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc. Closed models like Elevenlabs v3 seem to continue to be a few levels above open models, so comparisons, especially empirical ones are welcome.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please use the top level TTS/STT comments to thread your responses. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohqev8/best_local_ttsstt_models_october_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohqev8/best_local_ttsstt_models_october_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohqev8/best_local_ttsstt_models_october_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T21:00:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1olq14f</id>
    <title>[MEGATHREAD] Local AI Hardware - November 2025</title>
    <updated>2025-11-01T15:02:17+00:00</updated>
    <author>
      <name>/u/eck72</name>
      <uri>https://old.reddit.com/user/eck72</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is the monthly thread for sharing your local AI setups and the models you're running.&lt;/p&gt; &lt;p&gt;Whether you're using a single CPU, a gaming GPU, or a full rack, post what you're running and how it performs.&lt;/p&gt; &lt;p&gt;Post in any format you like. The list below is just a guide:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Hardware: CPU, GPU(s), RAM, storage, OS&lt;/li&gt; &lt;li&gt;Model(s): name + size/quant&lt;/li&gt; &lt;li&gt;Stack: (e.g. llama.cpp + custom UI)&lt;/li&gt; &lt;li&gt;Performance: t/s, latency, context, batch etc.&lt;/li&gt; &lt;li&gt;Power consumption&lt;/li&gt; &lt;li&gt;Notes: purpose, quirks, comments&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please share setup pics for eye candy!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Quick reminder&lt;/strong&gt;: You can share hardware purely to ask questions or get feedback. All experience levels welcome.&lt;/p&gt; &lt;p&gt;House rules: no buying/selling/promo.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eck72"&gt; /u/eck72 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T15:02:17+00:00</published>
  </entry>
</feed>
