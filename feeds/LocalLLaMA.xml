<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-13T16:51:06+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1pkxj0i</id>
    <title>Dolphin-v2, Universal Document Parsing Model from ByteDance Open Source</title>
    <updated>2025-12-12T17:18:49+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkxj0i/dolphinv2_universal_document_parsing_model_from/"&gt; &lt;img alt="Dolphin-v2, Universal Document Parsing Model from ByteDance Open Source" src="https://external-preview.redd.it/azQ1aTBvNWwzdDZnMWGXoQeCCsCSbT01XS-4Qf-TxasLW4Bw-m6-HAAzWjW4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f391a2f482d7600f705e7b3beeab48051894acfb" title="Dolphin-v2, Universal Document Parsing Model from ByteDance Open Source" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Dolphin-v2 is an enhanced universal document parsing model that substantially improves upon the original Dolphin.&lt;/p&gt; &lt;p&gt;Dolphin-v2 is built on &lt;strong&gt;Qwen2.5-VL-3B&lt;/strong&gt; backbone with:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Vision encoder based on Native Resolution Vision Transformer (NaViT)&lt;/li&gt; &lt;li&gt;Autoregressive decoder for structured output generation&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/ByteDance/Dolphin-v2#%F0%9F%93%88-performance"&gt;&lt;/a&gt;&lt;/h1&gt; &lt;p&gt;&lt;a href="https://huggingface.co/ByteDance/Dolphin-v2"&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Dolphin-v2 introduces several major enhancements over the original Dolphin:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Universal Document Support&lt;/strong&gt;: Handles both digital-born and photographed documents with realistic distortions&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Expanded Element Coverage&lt;/strong&gt;: Supports 21 element categories (up from 14), including dedicated code blocks and formulas&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Enhanced Precision&lt;/strong&gt;: Uses absolute pixel coordinates for more accurate spatial localization&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hybrid Parsing Strategy&lt;/strong&gt;: Element-wise parallel parsing for digital documents + holistic parsing for photographed documents&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Specialized Modules&lt;/strong&gt;: Dedicated parsing for code blocks with indentation preservation&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://huggingface.co/ByteDance/Dolphin-v2"&gt;Hugging Face Model Card &lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/xkkz615l3t6g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkxj0i/dolphinv2_universal_document_parsing_model_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pkxj0i/dolphinv2_universal_document_parsing_model_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T17:18:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1pl3kzb</id>
    <title>The mistral-vibe CLI can work super well with gpt-oss</title>
    <updated>2025-12-12T21:23:06+00:00</updated>
    <author>
      <name>/u/tarruda</name>
      <uri>https://old.reddit.com/user/tarruda</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;To use it with GPT-OSS, you need my fork which sends reasoning content back to llama.cpp server: &lt;code&gt;uv tool install &amp;quot;mistral-vibe@git+https://github.com/tarruda/mistral-vibe.git@include-reasoning-content&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;I also sent a PR to merge the changes upstream: &lt;a href="https://github.com/mistralai/mistral-vibe/pull/123"&gt;https://github.com/mistralai/mistral-vibe/pull/123&lt;/a&gt;&lt;/p&gt; &lt;p&gt;On GPT-OSS 20b: Sometimes it gets confused with some of the tools. Specifically it sometimes tries to use &lt;code&gt;search_and_replace&lt;/code&gt;(which is designed to edit files) to grep for text.&lt;/p&gt; &lt;p&gt;But IMO it yields a better experience than devstral-2 due to how fast it is. In my testing it is also much better at coding than devstral-2.&lt;/p&gt; &lt;p&gt;I bet with a small dataset it would be possible to finetune gpt-oss to master using mistral-vibe tools.&lt;/p&gt; &lt;p&gt;And of course: If you can run GPT-OSS-120b it should definitely be better.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tarruda"&gt; /u/tarruda &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pl3kzb/the_mistralvibe_cli_can_work_super_well_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pl3kzb/the_mistralvibe_cli_can_work_super_well_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pl3kzb/the_mistralvibe_cli_can_work_super_well_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T21:23:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1pl984y</id>
    <title>Finally finished my 4x GPU water cooled server build!</title>
    <updated>2025-12-13T01:36:44+00:00</updated>
    <author>
      <name>/u/j4ys0nj</name>
      <uri>https://old.reddit.com/user/j4ys0nj</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pl984y/finally_finished_my_4x_gpu_water_cooled_server/"&gt; &lt;img alt="Finally finished my 4x GPU water cooled server build!" src="https://b.thumbs.redditmedia.com/HiqkwFk9tKIlzTB9HIEUweA1Yj69N3ENrR6eUTzCJyQ.jpg" title="Finally finished my 4x GPU water cooled server build!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/xlzrfymwmv6g1.png?width=1130&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=573735e15f46058d9ae44ae5c18cb9ed93678339"&gt;https://preview.redd.it/xlzrfymwmv6g1.png?width=1130&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=573735e15f46058d9ae44ae5c18cb9ed93678339&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GPUs:&lt;br /&gt; - 1x RTX 6000 PRO Blackwell Server Edition&lt;br /&gt; - 2x RTX 5090 FE&lt;br /&gt; - 1x RTX 4090&lt;/p&gt; &lt;p&gt;Water is piped in from an external cooling unit I also built. The unit provides around 4000W of cooling capacity, which is plenty to handle these 4 GPUs, 4 GPUs in another box (A4500s) and a few CPUs. Getting just over 1000 l/h, or 4.5 GPM, of flow.&lt;/p&gt; &lt;p&gt;At idle, everything sits between 26-29¬∫C and while I haven't had everything running at full load yet, when a few GPUs/CPUs are pegged, I haven't seen them go above 40¬∫C.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/kns420yqkv6g1.png?width=1290&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=deee0bd1d0c6af5e299b4eaedf1b2d13b042b4ad"&gt;everything is power limited to 480W as a precaution&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Using Alphacool quick connects &amp;amp; distro plates throughout. GPU &amp;amp; CPU waterblocks are from Bykski, except for the 4090, that's from Alphacool.&lt;/p&gt; &lt;p&gt;I went from 2x 5090s and the RTX 6000 PRO crammed in there, with a loud server fan on the 6000 PRO, no room to add anything else, load temps above 80¬∫C, to being able to fit 1 more GPU (4090) and a free PCIe slot that I'll probably throw an NVMe storage card in. Finally.. the server is cool and quiet!&lt;/p&gt; &lt;p&gt;I am slightly bummed that the 5090s appear to be 1 slot, but actually block the PCIe slot below them. Not that big of a deal I guess.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/j4ys0nj"&gt; /u/j4ys0nj &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pl984y/finally_finished_my_4x_gpu_water_cooled_server/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pl984y/finally_finished_my_4x_gpu_water_cooled_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pl984y/finally_finished_my_4x_gpu_water_cooled_server/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-13T01:36:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1plab3b</id>
    <title>Should i avoid using abliterated models when the base one is already compliant enough?</title>
    <updated>2025-12-13T02:30:24+00:00</updated>
    <author>
      <name>/u/iz-Moff</name>
      <uri>https://old.reddit.com/user/iz-Moff</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Some models, like Mistral family, for example, seem to be uncensored by default, at least in so far as i care to push them. Yet, i still come across abliterated\heretic\whatever versions of them on huggingface. I read that abliteration process can not only reduce the refusal rate, but also introduce various errors that might degrade the model's quality, and indeed i tried a few abliterated qwens and gemmas that seemed completely broken in various ways.&lt;/p&gt; &lt;p&gt;So, is it better to just avoid these until i actually experience a lot of refusals, or are newer methods, like that heretic one, are safe enough and are not likely to mess up the model?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iz-Moff"&gt; /u/iz-Moff &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plab3b/should_i_avoid_using_abliterated_models_when_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plab3b/should_i_avoid_using_abliterated_models_when_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1plab3b/should_i_avoid_using_abliterated_models_when_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-13T02:30:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1plpc6h</id>
    <title>Mistral 3 Large is DeepSeek V3!?</title>
    <updated>2025-12-13T16:24:39+00:00</updated>
    <author>
      <name>/u/seraschka</name>
      <uri>https://old.reddit.com/user/seraschka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plpc6h/mistral_3_large_is_deepseek_v3/"&gt; &lt;img alt="Mistral 3 Large is DeepSeek V3!?" src="https://b.thumbs.redditmedia.com/yYDI9PVijp_cl2RjrFgG1_MnObuOyb3iYTmzt8uL1_I.jpg" title="Mistral 3 Large is DeepSeek V3!?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With Mistral 3 and DeepSeek V3.2, we got two major open-weight LLMs this month already. I looked into DeepSeek V3.2 last week and just caught up with reading through the config of the Mistral 3 architecture in more detail. &lt;/p&gt; &lt;p&gt;Interestingly, based on &lt;a href="https://mistral.ai/news/mistral-3"&gt;their official announcement post&lt;/a&gt;, Mistral 3 and DeepSeek V3.2 have an almost identical size, 671B and 673B, which makes for an interesting comparison, I thought! &lt;/p&gt; &lt;p&gt;Unfortunately, there is no technical report on Mistral 3 that contains more information about the model development. However, since it‚Äôs an open-weight model, we do have the &lt;a href="https://huggingface.co/mistralai/Mistral-Large-3-675B-Instruct-2512-NVFP4/blob/main/params.json"&gt;model weights on the HuggingFace Model Hub&lt;/a&gt;, though. So, l was taking a closer look at Mistral 3 Large yesterday, and it turns out to be exactly the same architecture as DeepSeek V3/V3.1. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/70lznwrbzz6g1.png?width=2846&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=aca49968a91f54b80594024ab98b9cd968be8bdf"&gt;https://preview.redd.it/70lznwrbzz6g1.png?width=2846&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=aca49968a91f54b80594024ab98b9cd968be8bdf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The only difference is that they increased the size of the experts by a factor of 2 while decreasing the number of experts by the same factor. This keeps the number of expert parameters constant, but it should help a bit with latency (1 big expert is faster than 2 smaller experts since there are fewer operations to deal with). &lt;/p&gt; &lt;p&gt;I think that Mistral 3 reusing the DeepSeek V3 architecture is totally fair in the spirit of open source. I am just surprised by it, because I haven't seen anyone mentioning that yet. &lt;/p&gt; &lt;p&gt;However, while it‚Äôs effectively the same architecture, it is likely the Mistral team trained Mistral 3 from scratch rather than initializing it from DeepSeek V3 and further training it, because Mistral uses its own tokenizer. &lt;/p&gt; &lt;p&gt;Next to Kimi K2, Mistral 3 Large is now the second major model to use the DeepSeek V3 architecture. However, where the Kimi K2 team scaled up the model size from 673B to 1 trillion, the Mistral 3 team only changed the expert size ratio and added a vision encoder for multimodal support. But yes, why not? I think DeepSeek V3 is a pretty solid architecture design, plus it has these nice MoE and MLA efficiency aspects to it. So, why change what ain‚Äôt broke? A lot of the secret sauce these days is in the training pipeline as well as the inference scaling strategies.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/seraschka"&gt; /u/seraschka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plpc6h/mistral_3_large_is_deepseek_v3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plpc6h/mistral_3_large_is_deepseek_v3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1plpc6h/mistral_3_large_is_deepseek_v3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-13T16:24:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1plkv9n</id>
    <title>Maxun: Free, Open-Source Web Data for AI Agents &amp; Data Pipelines</title>
    <updated>2025-12-13T13:02:04+00:00</updated>
    <author>
      <name>/u/carishmaa</name>
      <uri>https://old.reddit.com/user/carishmaa</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, everyone&lt;/p&gt; &lt;p&gt;Excited to bring to you &lt;strong&gt;Maxun&lt;/strong&gt; : an open-source, self-hostable web extraction &amp;amp; scraping platform we‚Äôve been building in the open for over a year. &lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/getmaxun/maxun"&gt;https://github.com/getmaxun/maxun&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;What Maxun Does?&lt;/h1&gt; &lt;p&gt;Maxun uses &lt;strong&gt;web robots&lt;/strong&gt; that emulate real user behavior and return clean, structured data or AI-ready content.&lt;/p&gt; &lt;h1&gt;Extract Robots (Structured Data)&lt;/h1&gt; &lt;p&gt;Build them in two ways&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Recorder Mode:&lt;/strong&gt; Browse like a human (click, scroll, paginate). Deterministic and reliable. &lt;ul&gt; &lt;li&gt;Example: Extract 10 Property Listings from Airbnb&lt;/li&gt; &lt;li&gt;Demo: &lt;a href="https://github.com/user-attachments/assets/c6baa75f-b950-482c-8d26-8a8b6c5382c3"&gt;https://github.com/user-attachments/assets/c6baa75f-b950-482c-8d26-8a8b6c5382c3&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;AI Mode&lt;/strong&gt;: Describe what you want in natural language. Works with local LLMs (Ollama) and cloud models. &lt;ul&gt; &lt;li&gt;Example: Extract Names, Rating &amp;amp; Duration of Top 50 Movies from IMDb&lt;/li&gt; &lt;li&gt;Demo: &lt;a href="https://github.com/user-attachments/assets/f714e860-58d6-44ed-bbcd-c9374b629384"&gt;https://github.com/user-attachments/assets/f714e860-58d6-44ed-bbcd-c9374b629384&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Scrape Robots (Content for AI)&lt;/h1&gt; &lt;p&gt;Built for agent pipelines&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Clean HTML, &lt;strong&gt;LLM-ready Markdown or capture Screenshots&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Useful for RAG, embeddings, summarization, and indexing&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;SDK&lt;/h1&gt; &lt;p&gt;Via the SDK, agents can&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Trigger extract or scrape robots&lt;/li&gt; &lt;li&gt;Use LLM or non-LLM extraction&lt;/li&gt; &lt;li&gt;Handle pagination automatically&lt;/li&gt; &lt;li&gt;Run jobs on schedules or via API&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;SDK: &lt;a href="https://github.com/getmaxun/node-sdk"&gt;https://github.com/getmaxun/node-sdk&lt;/a&gt;&lt;br /&gt; Docs: &lt;a href="https://docs.maxun.dev/category/sdk"&gt;https://docs.maxun.dev/category/sdk&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Open Source + Self-Hostable&lt;/h1&gt; &lt;p&gt;Maxun is ~&lt;strong&gt;99% open source&lt;/strong&gt;.&lt;br /&gt; Scheduling, webhooks, robot runs, and management are all available in OSS.&lt;br /&gt; Self-hostable with or without Docker.&lt;/p&gt; &lt;p&gt;Would love feedback, questions and suggestions from folks building agents or data pipelines.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/carishmaa"&gt; /u/carishmaa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plkv9n/maxun_free_opensource_web_data_for_ai_agents_data/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plkv9n/maxun_free_opensource_web_data_for_ai_agents_data/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1plkv9n/maxun_free_opensource_web_data_for_ai_agents_data/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-13T13:02:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1plg9g1</id>
    <title>I was terrified to let Llama 3 query my DB, so I built a WASM-powered "Airgap" Middleware. Here's the code.</title>
    <updated>2025-12-13T08:08:08+00:00</updated>
    <author>
      <name>/u/Electrical_Try_6404</name>
      <uri>https://old.reddit.com/user/Electrical_Try_6404</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to let Llama 3 answer questions from my real Postgres DB. &lt;/p&gt; &lt;p&gt;I couldn‚Äôt bring myself to give it a direct connection. Even read-only felt&lt;br /&gt; unsafe with PII and margins in the schema. &lt;/p&gt; &lt;p&gt;Most ‚ÄúAI SQL guardrails‚Äù rely on regex or JS SQL parsers. That felt flimsy ‚Äî&lt;br /&gt; especially with nested queries and Postgres quirks. &lt;/p&gt; &lt;p&gt;So I treated the model like a hostile user. &lt;/p&gt; &lt;p&gt;Instead of validating SQL in JS, I took the actual Postgres parser&lt;br /&gt; (libpg_query), compiled it to WebAssembly, and run it inside Deno. &lt;/p&gt; &lt;p&gt;When the model sends SQL: ‚Äì the query is parsed by Postgres‚Äôs own C logic (via&lt;br /&gt; WASM) ‚Äì I get the exact AST Postgres would execute ‚Äì I recursively scan for&lt;br /&gt; every table reference (subqueries included) ‚Äì anything not in config.yaml is&lt;br /&gt; blocked before the DB sees it &lt;/p&gt; &lt;p&gt;One interesting finding: If you throw permission errors, agents often spiral. So&lt;br /&gt; instead of failing, I ‚Äúsilently strip‚Äù sensitive columns from results. The model&lt;br /&gt; just adapts and moves on. &lt;/p&gt; &lt;p&gt;Stack: ‚Äì Parser: libpg_query (C ‚Üí WASM) ‚Äì Runtime: Deno ‚Äì Protocol: MCP ‚Äì DB:&lt;br /&gt; Postgres &lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/ahammednibras8/secure-mcp-db"&gt;https://github.com/ahammednibras8/secure-mcp-db&lt;/a&gt; &lt;/p&gt; &lt;p&gt;This is a reference implementation, but the parser layer is real. If you can&lt;br /&gt; think of a SQL payload that slips past the AST walker, I‚Äôd genuinely like to see&lt;br /&gt; it.I wanted to let Llama 3 answer questions from my real Postgres DB. &lt;/p&gt; &lt;p&gt;I couldn‚Äôt bring myself to give it a direct connection. Even read-only felt&lt;br /&gt; unsafe with PII and margins in the schema. &lt;/p&gt; &lt;p&gt;Most ‚ÄúAI SQL guardrails‚Äù rely on regex or JS SQL parsers. That felt flimsy ‚Äî&lt;br /&gt; especially with nested queries and Postgres quirks. &lt;/p&gt; &lt;p&gt;So I treated the model like a hostile user. &lt;/p&gt; &lt;p&gt;Instead of validating SQL in JS, I took the actual Postgres parser&lt;br /&gt; (libpg_query), compiled it to WebAssembly, and run it inside Deno. &lt;/p&gt; &lt;p&gt;When the model sends SQL: ‚Äì the query is parsed by Postgres‚Äôs own C logic (via&lt;br /&gt; WASM) ‚Äì I get the exact AST Postgres would execute ‚Äì I recursively scan for&lt;br /&gt; every table reference (subqueries included) ‚Äì anything not in config.yaml is&lt;br /&gt; blocked before the DB sees it &lt;/p&gt; &lt;p&gt;One interesting finding: If you throw permission errors, agents often spiral. So&lt;br /&gt; instead of failing, I ‚Äúsilently strip‚Äù sensitive columns from results. The model&lt;br /&gt; just adapts and moves on. &lt;/p&gt; &lt;p&gt;Stack: ‚Äì Parser: libpg_query (C ‚Üí WASM) ‚Äì Runtime: Deno ‚Äì Protocol: MCP ‚Äì DB:&lt;br /&gt; Postgres &lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/ahammednibras8/secure-mcp-db"&gt;https://github.com/ahammednibras8/secure-mcp-db&lt;/a&gt; &lt;/p&gt; &lt;p&gt;This is a reference implementation, but the parser layer is real. If you can&lt;br /&gt; think of a SQL payload that slips past the AST walker, I‚Äôd genuinely like to see&lt;br /&gt; it. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Electrical_Try_6404"&gt; /u/Electrical_Try_6404 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plg9g1/i_was_terrified_to_let_llama_3_query_my_db_so_i/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plg9g1/i_was_terrified_to_let_llama_3_query_my_db_so_i/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1plg9g1/i_was_terrified_to_let_llama_3_query_my_db_so_i/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-13T08:08:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkpsee</id>
    <title>Training an LLM only on 1800s London texts - 90GB dataset</title>
    <updated>2025-12-12T11:40:11+00:00</updated>
    <author>
      <name>/u/Remarkable-Trick-177</name>
      <uri>https://old.reddit.com/user/Remarkable-Trick-177</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, you may have seen a few of my posts here a couple months ago. If not, hi. I‚Äôm working on an open source project called TimeCapsuleLLM, where I train LLMs from scratch using only 1800-1875 London texts.&lt;/p&gt; &lt;p&gt;Until recently most of my work has been done on a small scale but over the past 3 months I‚Äôve been working on a much larger dataset for the next model. My newest dataset is 90GB with 135,000 documents, it contains basically every usable document that I could find on the Internet Archive for that time period. &lt;/p&gt; &lt;p&gt;Before doing any training, I ran an inspection across every file and generated a bias report covering temporal bias, gender/pronoun bias and geographic bias. Given the time period it‚Äôs strongly biased, but it‚Äôs important to study this. You can find the report on my GitHub if anyone wants to take a look. I‚Äôve also trained a small evaluation model on a 15GB subset to evaluate the dataset before I scale up to all 90GB. It‚Äôs a LlaMA style model (300M parameters) trained to 10K steps. Example output: &lt;/p&gt; &lt;p&gt;Prompt: Who is Charles Dickens?&lt;/p&gt; &lt;p&gt;Output with fixed spacing: ‚ÄúWho is Charles Dickens? Does that work more of his excellent stirring, in his plays, in the Great Company's farm? What I have yet to quote from Jack Pickett? Do you not know that they were a species of galloping, or sawing of their breasts, or what was to be done about the time when Jackson was looking on the window? What is the success of an Englishman, and which his son has not been discovering to me, whereby to accomplish such a weight? Did you ever make a passage into the old roadway, or to an anchor-breeze at the foot of our boat, which you must leave us? The fact is, that whether the wind would rise up from the plain on Saturday night or noontide, or till the north, or otherwise, we shall be compelled to describe a formidable barrier, with the same effects as the present. In this situation, at least, it is not too much to say that we have left that room. I believe there are three copies in the 'Five Hundred-fold,' to be referred to, as the first number of our readers who wish to.‚Äù&lt;/p&gt; &lt;p&gt;This type of output is expected since 10,000 steps is very early and it‚Äôs not a QA model. The model has already learned long, winding sentence structures, but can‚Äôt connect ideas logically yet. The main goal here was to see how clean the output would be. &lt;/p&gt; &lt;p&gt;One issue that came up was with the tokenizer, it over-split the text, splitting words into individual characters and subparts. So the model by default gives output like this: &lt;/p&gt; &lt;p&gt;Original output: ‚ÄúW ho is Charles D ic ens ? D oes that work more of h ise x cell ent st ir ring , in his pl ays , int he G reat C omp any 's f arm ? What I have y et to qu ote from J ack P ick ett ?‚Äù&lt;/p&gt; &lt;p&gt;It doubled the tokens for the same amount of data, making learning harder. Next steps are training another eval model and then scaling to the full 90GB dataset for a 1.2B parameter model. The eval model is already on Hugging Face and you can find a run script for it on my GitHub. I‚Äôll upload the 15GB subset to Hugging Face once the tokenizer is corrected.&lt;/p&gt; &lt;p&gt;I also want to thank everyone in this subreddit. This is the only place I‚Äôve shared the project other than github, and a lot of the early guidance came directly from here. I really appreciate how generous people here have been with advice. More updates soon.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/haykgrigo3/TimeCapsuleLLM"&gt;haykgrigo3/TimeCapsuleLLM: A LLM trained only on data from certain time periods to reduce modern bias&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/haykgrigorian/v2mini-eval1"&gt;haykgrigorian/v2mini-eval1 ¬∑ Hugging Face&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Remarkable-Trick-177"&gt; /u/Remarkable-Trick-177 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkpsee/training_an_llm_only_on_1800s_london_texts_90gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkpsee/training_an_llm_only_on_1800s_london_texts_90gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pkpsee/training_an_llm_only_on_1800s_london_texts_90gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T11:40:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1pky5u4</id>
    <title>Olmo 3.1 32B Think &amp; Instruct: New Additions to the Olmo Model Family</title>
    <updated>2025-12-12T17:43:36+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pky5u4/olmo_31_32b_think_instruct_new_additions_to_the/"&gt; &lt;img alt="Olmo 3.1 32B Think &amp;amp; Instruct: New Additions to the Olmo Model Family" src="https://preview.redd.it/bwgy5ldc8t6g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9fca8f97eb697220d6926581cc50bcb50e944a99" title="Olmo 3.1 32B Think &amp;amp; Instruct: New Additions to the Olmo Model Family" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Olmo 3.1 32B Think and Olmo 3.1 32B Instruct are the newest 32-billion-parameter models in the Olmo family, each optimized for different yet complementary use cases. &lt;/p&gt; &lt;ul&gt; &lt;li&gt;The &lt;strong&gt;Think model&lt;/strong&gt; is a deep-reasoning specialist, trained with extended reinforcement learning on the Dolci-Think-RL dataset to improve multi-step reasoning, math, logic, and code generation. &lt;/li&gt; &lt;li&gt;In contrast, the &lt;strong&gt;Instruct model&lt;/strong&gt; applies the Olmo instruction-tuning recipe at 32B scale, making it a strong fully open chat and agent foundation focused on instruction following, conversational fluency, and tool-use capabilities.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/allenai/olmo-31"&gt;HuggingFace Model Collection &lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bwgy5ldc8t6g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pky5u4/olmo_31_32b_think_instruct_new_additions_to_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pky5u4/olmo_31_32b_think_instruct_new_additions_to_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T17:43:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1plohsn</id>
    <title>Local multi agent systems</title>
    <updated>2025-12-13T15:49:56+00:00</updated>
    <author>
      <name>/u/SlowFail2433</name>
      <uri>https://old.reddit.com/user/SlowFail2433</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Have there been any interesting developments in local multi agent systems?&lt;/p&gt; &lt;p&gt;What setup/models do you like for the orchestrator/routers and the agents themselves?&lt;/p&gt; &lt;p&gt;Any interesting repos in this area?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SlowFail2433"&gt; /u/SlowFail2433 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plohsn/local_multi_agent_systems/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plohsn/local_multi_agent_systems/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1plohsn/local_multi_agent_systems/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-13T15:49:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1pln8mt</id>
    <title>Llama 3.2 3B fMRI (build update)</title>
    <updated>2025-12-13T14:55:52+00:00</updated>
    <author>
      <name>/u/Due_Hunter_4891</name>
      <uri>https://old.reddit.com/user/Due_Hunter_4891</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pln8mt/llama_32_3b_fmri_build_update/"&gt; &lt;img alt="Llama 3.2 3B fMRI (build update)" src="https://a.thumbs.redditmedia.com/-hmxek3yiq_z5LHZYxo8sbedyvNq3bsGqX6VgcvvnE0.jpg" title="Llama 3.2 3B fMRI (build update)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just wanted to share progress, since it looks like there were a few interested parties yesterday. My goal now is to record turns, and broadcast the individual dims to the rendered space. This lets me identify which individual dimensions activate under different kinds of inputs.&lt;/p&gt; &lt;p&gt;this also allows me to project rotational, grad norm, etc for the same dims and see exactly how the model responds to different kinds of inputs, making AI interp a transparency issue rather than a guessing issue. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/esoaxe75jz6g1.png?width=695&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=71fcff0178eb575321bafcaa8aa2f5d613807c6f"&gt;From the bottom: layers 1, 2, 14 / 15, 27, 28&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Due_Hunter_4891"&gt; /u/Due_Hunter_4891 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pln8mt/llama_32_3b_fmri_build_update/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pln8mt/llama_32_3b_fmri_build_update/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pln8mt/llama_32_3b_fmri_build_update/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-13T14:55:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1plkg6q</id>
    <title>RLAX: Large-Scale, Distributed Reinforcement Learning for Large Language Models on TPUs</title>
    <updated>2025-12-13T12:39:09+00:00</updated>
    <author>
      <name>/u/vladlearns</name>
      <uri>https://old.reddit.com/user/vladlearns</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;apple briefly published, then quickly removed, a paper on arxiv,&lt;br /&gt; but v1 was already out &lt;a href="https://arxiv.org/pdf/2512.06392v1"&gt;https://arxiv.org/pdf/2512.06392v1&lt;/a&gt; and it‚Äôs interesting.&lt;/p&gt; &lt;p&gt;they introduce &lt;strong&gt;rlax&lt;/strong&gt; ‚Äî a scalable rl framework for llms on tpus.&lt;/p&gt; &lt;p&gt;what rlax looks like:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;parameter server architecture&lt;/li&gt; &lt;li&gt;one central trainer updates weights&lt;/li&gt; &lt;li&gt;huge inference fleets pull weights and generate rollouts&lt;/li&gt; &lt;li&gt;built for preemption and extreme parallelism&lt;/li&gt; &lt;li&gt;custom data curation and alignment tricks&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;results:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;+12.8% pass@8 on qwq-32b&lt;/li&gt; &lt;li&gt;in 12h 48m&lt;/li&gt; &lt;li&gt;using 1024 tpu v5p&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;why this matters:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;apple is testing rl at serious scale&lt;/li&gt; &lt;li&gt;tpu-first design = system efficiency focus&lt;/li&gt; &lt;li&gt;gains come from training engineering, not model magic&lt;/li&gt; &lt;li&gt;rl for llms is becoming an industrial pipeline&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vladlearns"&gt; /u/vladlearns &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plkg6q/rlax_largescale_distributed_reinforcement/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plkg6q/rlax_largescale_distributed_reinforcement/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1plkg6q/rlax_largescale_distributed_reinforcement/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-13T12:39:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1plgj0p</id>
    <title>What do you think about GLM-4.6V-Flash?</title>
    <updated>2025-12-13T08:25:38+00:00</updated>
    <author>
      <name>/u/lossless-compression</name>
      <uri>https://old.reddit.com/user/lossless-compression</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The model seems too good to be true in benchmarks and I found positive reviews but I'm not sure real world tests are comparable,what is your experience?&lt;/p&gt; &lt;p&gt;The model is comparable to the MoE one in activated parameters (9B-12B) but the 12B is much more intelligent because usually a 12B activated MoE behaves more like a 20-30B dense in practice.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lossless-compression"&gt; /u/lossless-compression &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plgj0p/what_do_you_think_about_glm46vflash/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plgj0p/what_do_you_think_about_glm46vflash/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1plgj0p/what_do_you_think_about_glm46vflash/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-13T08:25:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1plhvuz</id>
    <title>the json parser that automatically repairs your agent's "json-ish" output</title>
    <updated>2025-12-13T09:58:14+00:00</updated>
    <author>
      <name>/u/Ok_Rub1689</name>
      <uri>https://old.reddit.com/user/Ok_Rub1689</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plhvuz/the_json_parser_that_automatically_repairs_your/"&gt; &lt;img alt="the json parser that automatically repairs your agent's &amp;quot;json-ish&amp;quot; output" src="https://b.thumbs.redditmedia.com/ULJPg2HwtUiCg7FepRaEpY3mnZCGv4RNbURVDKBnCWA.jpg" title="the json parser that automatically repairs your agent's &amp;quot;json-ish&amp;quot; output" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/07r9qxsd2y6g1.png?width=1278&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b04c313654e50e327e4d1c718745e9f120a0f2b7"&gt;https://preview.redd.it/07r9qxsd2y6g1.png?width=1278&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b04c313654e50e327e4d1c718745e9f120a0f2b7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/sigridjineth/agentjson"&gt;https://github.com/sigridjineth/agentjson&lt;/a&gt;&lt;/p&gt; &lt;p&gt;LLMs are great at structured-ish output, but real pipelines still see markdown fences, extra prose trailing commas/smart quotes, missing commas/closers, etc. In Python, Strict parsers (json, orjson, ‚Ä¶) treat that as a hard failure, so that each agent encounters with delayed retries, latency, and brittle tool/function-calls.&lt;/p&gt; &lt;p&gt;So I made agentjson, which is a Rust-powered JSON repair pipeline with Python bindings. Strict JSON parsers fail while agentjson succeeds end‚Äëto‚Äëend. It does the following stuff.&lt;/p&gt; &lt;p&gt;- Extract the JSON span from arbitrary text&lt;br /&gt; - Repair common errors cheaply first (deterministic heuristics)&lt;br /&gt; - Recover intent via probabilistic Top‚ÄëK parsing + confidence + repair trace&lt;br /&gt; - Optionally ask an LLM for a minimal byte-offset patch only when needed, then re-validate&lt;/p&gt; &lt;p&gt;Try pip install agentjson and give it a shot!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Rub1689"&gt; /u/Ok_Rub1689 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plhvuz/the_json_parser_that_automatically_repairs_your/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plhvuz/the_json_parser_that_automatically_repairs_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1plhvuz/the_json_parser_that_automatically_repairs_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-13T09:58:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkpxss</id>
    <title>Someone from NVIDIA made a big mistake and uploaded the parent folder of their upcoming model on Hugging Face</title>
    <updated>2025-12-12T11:49:10+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkpxss/someone_from_nvidia_made_a_big_mistake_and/"&gt; &lt;img alt="Someone from NVIDIA made a big mistake and uploaded the parent folder of their upcoming model on Hugging Face" src="https://preview.redd.it/7r3bnj5ugr6g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0c3d5909063dd5ce912e8ebc203168db53b765be" title="Someone from NVIDIA made a big mistake and uploaded the parent folder of their upcoming model on Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From Xeophon on ùïè: &lt;a href="https://x.com/xeophon_/status/1999394570967089630"&gt;https://x.com/xeophon_/status/1999394570967089630&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7r3bnj5ugr6g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkpxss/someone_from_nvidia_made_a_big_mistake_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pkpxss/someone_from_nvidia_made_a_big_mistake_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T11:49:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1plf33x</id>
    <title>Free Chrome extension to run Kokoro TTS in your browser (local only)</title>
    <updated>2025-12-13T06:53:15+00:00</updated>
    <author>
      <name>/u/Impressive-Sir9633</name>
      <uri>https://old.reddit.com/user/Impressive-Sir9633</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plf33x/free_chrome_extension_to_run_kokoro_tts_in_your/"&gt; &lt;img alt="Free Chrome extension to run Kokoro TTS in your browser (local only)" src="https://preview.redd.it/e6btaupf5x6g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=07336b177f010ecb6599eddac44de82714049c21" title="Free Chrome extension to run Kokoro TTS in your browser (local only)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My site's traffic shot up when I offered free local Kokoro TTS. Thanks for all the love for &lt;a href="https://freevoicereader.com"&gt;https://freevoicereader.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Some of the people on &lt;a href="/r/TextToSpeech"&gt;r/TextToSpeech&lt;/a&gt; asked for a chrome extension. Hopefully, this will make it easier to quickly read anything in the browser.&lt;/p&gt; &lt;p&gt;Free, no ads.&lt;/p&gt; &lt;p&gt;&lt;a href="https://chromewebstore.google.com/detail/freevoice-reader-ai-text/bfhihejhhjfocdggkfpeignglimmpoho"&gt;FreeVoiceReader Chrome Extension&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Highlight text, right click and select FreeVoiceReader, it starts reading.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The difference from other TTS extensions: everything runs locally in your browser via WebGPU.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What that means:&lt;/p&gt; &lt;p&gt;‚Ä¢ Your text never leaves your device ‚Ä¢ No character limits or daily quotas ‚Ä¢ Works offline after initial setup (~80MB model download, cached locally) ‚Ä¢ No account required ‚Ä¢ Can export audio as WAV files&lt;/p&gt; &lt;p&gt;Happy to hear feedback or feature requests. There were a couple of UI glitches that people noticed and I have submitted a fix. Waiting for Chrome team to approve it. &lt;/p&gt; &lt;p&gt;(I have been told that the French language doesn't work - sorry to the folks who need French)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive-Sir9633"&gt; /u/Impressive-Sir9633 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/e6btaupf5x6g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plf33x/free_chrome_extension_to_run_kokoro_tts_in_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1plf33x/free_chrome_extension_to_run_kokoro_tts_in_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-13T06:53:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1pllq0a</id>
    <title>RDMA over Thunderbolt 5 is now possible on MacOS Tahoe 26.2</title>
    <updated>2025-12-13T13:44:37+00:00</updated>
    <author>
      <name>/u/HaAtidChai</name>
      <uri>https://old.reddit.com/user/HaAtidChai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pllq0a/rdma_over_thunderbolt_5_is_now_possible_on_macos/"&gt; &lt;img alt="RDMA over Thunderbolt 5 is now possible on MacOS Tahoe 26.2" src="https://external-preview.redd.it/2uG3mp1grku1teaynrks5JCfpigHSeN1SIYEwOaBaf4.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c8713859505df19de87039a1230aef3e5deaf349" title="RDMA over Thunderbolt 5 is now possible on MacOS Tahoe 26.2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Apple quietly released this. This enables Mac clusters to run tensor parallelism over MLX on larger memory pool.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HaAtidChai"&gt; /u/HaAtidChai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://developer.apple.com/documentation/macos-release-notes/macos-26_2-release-notes#RDMA-over-Thunderbolt"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pllq0a/rdma_over_thunderbolt_5_is_now_possible_on_macos/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pllq0a/rdma_over_thunderbolt_5_is_now_possible_on_macos/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-13T13:44:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1pl4njj</id>
    <title>Running an LLM on a 3DS</title>
    <updated>2025-12-12T22:08:16+00:00</updated>
    <author>
      <name>/u/vreab</name>
      <uri>https://old.reddit.com/user/vreab</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pl4njj/running_an_llm_on_a_3ds/"&gt; &lt;img alt="Running an LLM on a 3DS" src="https://external-preview.redd.it/YXA0dDFoZXFqdTZnMayDKB9rDenP9HyWtMAfrMDzC_OwePMKvB7zq1t1dTfu.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=799b8c5d268a4ce59bfb456e310307c58099033f" title="Running an LLM on a 3DS" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vreab"&gt; /u/vreab &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9545t3eqju6g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pl4njj/running_an_llm_on_a_3ds/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pl4njj/running_an_llm_on_a_3ds/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T22:08:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1pl0ojb</id>
    <title>The new monster-server</title>
    <updated>2025-12-12T19:23:12+00:00</updated>
    <author>
      <name>/u/eribob</name>
      <uri>https://old.reddit.com/user/eribob</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pl0ojb/the_new_monsterserver/"&gt; &lt;img alt="The new monster-server" src="https://preview.redd.it/5kas5xaklt6g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5ebd2b38fa23a6f8f0aca6d1817cac736fa1e6d0" title="The new monster-server" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! &lt;/p&gt; &lt;p&gt;Just wanted to share my upgraded monster-server! I have bought the largest chassi I could reasonably find (Phanteks Enthoo pro 2 server) and filled it to the brim with GPU:s to run local LLM:s alongside my homelab. I am very happy how it has evloved / turned out! &lt;/p&gt; &lt;p&gt;&lt;strong&gt;I call it the &amp;quot;Monster server&amp;quot; :)&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Based on my trusted old X570 Taichi motherboard (extremely good!) and the Ryzen 3950x that I bought in 2019, that is still PLENTY fast today. I did not feel like spending a lot of money on a EPYC CPU/motherboard and new RAM, so instead I maxed out what I had. &lt;/p&gt; &lt;p&gt;The 24 PCI-e lanes are divided among the following: &lt;/p&gt; &lt;p&gt;3 GPU:s&lt;br /&gt; - 2 x RTX 3090 - both dual slot versions (inno3d RTX 3090 x3 and ASUS turbo RTX 3090)&lt;br /&gt; - 1 x RTX 4090 (an extremely chonky boi, 4 slots! ASUS TUF Gaming OC, that I got for reasonably cheap, around 1300USD equivalent). I run it on the &amp;quot;quiet&amp;quot; mode using the hardware switch hehe.&lt;/p&gt; &lt;p&gt;The 4090 runs off an M2 -&amp;gt; oculink -&amp;gt; PCIe adapter and a second PSU. The PSU is plugged in to the adapter board with its 24-pin connector and it powers on automatically when the rest of the system starts, very handy!&lt;br /&gt; &lt;a href="https://www.amazon.se/dp/B0DMTMJ95J"&gt;https://www.amazon.se/dp/B0DMTMJ95J&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Network: I have 10GB fiber internet for around 50 USD per month hehe...&lt;br /&gt; - 1 x 10GBe NIC - also connected using an M2 -&amp;gt; PCIe adapter. I had to mount this card creatively... &lt;/p&gt; &lt;p&gt;Storage:&lt;br /&gt; - 1 x Intel P4510 8TB U.2 enterprise NVMe. Solid storage for all my VM:s!&lt;br /&gt; - 4 x 18TB Seagate Exos HDD:s. For my virtualised TrueNAS. &lt;/p&gt; &lt;p&gt;RAM: 128GB Corsair Vengeance DDR4. Running at 2100MHz because I cannot get it stable when I try to run it faster, but whatever... LLMs are in VRAM anyway. &lt;/p&gt; &lt;p&gt;So what do I run on it?&lt;br /&gt; - GPT-OSS-120B, fully in VRAM, &amp;gt;100t/s tg. I did not yet find a better model, despite trying many... I use it for research, coding, and generally instead of google sometimes...&lt;br /&gt; I tried GLM4.5 air but it does not seem much smarter to me? Also slower. I would like to find a reasonably good model that I could run alongside FLUX1-dev-fp8 though, so I can generate images on the fly without having to switch. I am evaluating Qwen3-VL-32B for this&lt;/p&gt; &lt;p&gt;- Media server, Immich, Gitea, n8n&lt;/p&gt; &lt;p&gt;- My personal cloud using Seafile&lt;/p&gt; &lt;p&gt;- TrueNAS in a VM&lt;/p&gt; &lt;p&gt;- PBS for backups that is synced to a offsite PBS server at my brothers apartment&lt;/p&gt; &lt;p&gt;- a VM for coding, trying out devcontainers. &lt;/p&gt; &lt;p&gt;-&amp;gt; I also have a second server with a virtualised OPNsense VM as router. It runs other more &amp;quot;essential&amp;quot; services like PiHole, Traefik, Authelia, Headscale/tailscale, vaultwarden, a matrix server, anytype-sync and some other stuff... &lt;/p&gt; &lt;p&gt;---&lt;br /&gt; FINALLY: Why did I build this expensive machine? To make money by vibe-coding the next super-website? To cheat the stock market? To become the best AI engineer at Google? NO! Because I think it is fun to tinker around with computers, it is a hobby... &lt;/p&gt; &lt;p&gt;Thanks Reddit for teaching me all I needed to know to set this up! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eribob"&gt; /u/eribob &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5kas5xaklt6g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pl0ojb/the_new_monsterserver/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pl0ojb/the_new_monsterserver/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T19:23:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1plcrg8</id>
    <title>This is how open ai is advertising them selfs on reddit‚Ä¶. They are doomed</title>
    <updated>2025-12-13T04:38:30+00:00</updated>
    <author>
      <name>/u/ThinkExtension2328</name>
      <uri>https://old.reddit.com/user/ThinkExtension2328</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Holly god , after months of telling us they are the best and they will achieve agi and how open models are dangerous. This is how open ai is advertising to normies? Yea open ai is doomed &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThinkExtension2328"&gt; /u/ThinkExtension2328 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3zaedu7ehw6g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plcrg8/this_is_how_open_ai_is_advertising_them_selfs_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1plcrg8/this_is_how_open_ai_is_advertising_them_selfs_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-13T04:38:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1plewrk</id>
    <title>NVIDIA gpt-oss-120b Eagle Throughput model</title>
    <updated>2025-12-13T06:42:30+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plewrk/nvidia_gptoss120b_eagle_throughput_model/"&gt; &lt;img alt="NVIDIA gpt-oss-120b Eagle Throughput model" src="https://external-preview.redd.it/qdVRXmmV8Rf9W9JXBulc6Wu3niNm-zSxeaMwUkpajKs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4a65c4d10380bd77d1bb995e698c64823e1fb437" title="NVIDIA gpt-oss-120b Eagle Throughput model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ul&gt; &lt;li&gt;GPT-OSS-120B-Eagle3-throughput is an &lt;strong&gt;optimized speculative decoding module&lt;/strong&gt; built on top of the &lt;em&gt;OpenAI gpt-oss-120b&lt;/em&gt; base model, designed to improve throughput during text generation. &lt;/li&gt; &lt;li&gt;It uses NVIDIA‚Äôs &lt;strong&gt;Eagle3 speculative decoding&lt;/strong&gt; approach with the Model Optimizer to predict a single draft token efficiently, making it useful for high-concurrency inference scenarios where fast token generation is a priority. &lt;/li&gt; &lt;li&gt;The model is licensed under the &lt;strong&gt;nvidia-open-model-license&lt;/strong&gt; and is intended for commercial and non-commercial use in applications like AI agents, chatbots, retrieval-augmented generation (RAG) systems, and other instruction-following tasks. &lt;a href="https://huggingface.co/nvidia/gpt-oss-120b-Eagle3-throughput"&gt;&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/nvidia/gpt-oss-120b-Eagle3-throughput"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plewrk/nvidia_gptoss120b_eagle_throughput_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1plewrk/nvidia_gptoss120b_eagle_throughput_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-13T06:42:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1plnuqu</id>
    <title>OpenAI's flagship model, ChatGPT-5.2 Thinking, ranks most censored AI on Sansa benchmark.</title>
    <updated>2025-12-13T15:22:27+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plnuqu/openais_flagship_model_chatgpt52_thinking_ranks/"&gt; &lt;img alt="OpenAI's flagship model, ChatGPT-5.2 Thinking, ranks most censored AI on Sansa benchmark." src="https://preview.redd.it/l93slaq9oz6g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bab51a5045543fef0a9b8a60e5d6a113bc1f0cef" title="OpenAI's flagship model, ChatGPT-5.2 Thinking, ranks most censored AI on Sansa benchmark." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/l93slaq9oz6g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plnuqu/openais_flagship_model_chatgpt52_thinking_ranks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1plnuqu/openais_flagship_model_chatgpt52_thinking_ranks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-13T15:22:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1plng6f</id>
    <title>Qwen3 Next generation optimization</title>
    <updated>2025-12-13T15:04:40+00:00</updated>
    <author>
      <name>/u/ilintar</name>
      <uri>https://old.reddit.com/user/ilintar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plng6f/qwen3_next_generation_optimization/"&gt; &lt;img alt="Qwen3 Next generation optimization" src="https://external-preview.redd.it/lCjR6IsnTKIxBcWFwTVHflX0Ssz6EaHSVZWuf8jfKVA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cb67476532fe0a6cc1d75e633320c04f4f773566" title="Qwen3 Next generation optimization" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A lot of people were requesting dedicated optimizations, so here they are.&lt;/p&gt; &lt;p&gt;I added an optimized autoregressive delta net computation that short-circuits all the recurrect decay calculation because for `n_seq_tokens = 1` it all collapses. I also made sure to specifically optimize out all unneeded reshapes / conts in that version.&lt;/p&gt; &lt;p&gt;The end result is a 40% generation speed upgrade on my box. If you want, you can try it out and tell me how it works on your end.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ilintar"&gt; /u/ilintar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/17996"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plng6f/qwen3_next_generation_optimization/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1plng6f/qwen3_next_generation_optimization/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-13T15:04:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1plnjfy</id>
    <title>Nvidia DGX Station GB300 784GB available now! 95,000 USD / 80,000 EUR</title>
    <updated>2025-12-13T15:08:46+00:00</updated>
    <author>
      <name>/u/GPTshop</name>
      <uri>https://old.reddit.com/user/GPTshop</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plnjfy/nvidia_dgx_station_gb300_784gb_available_now/"&gt; &lt;img alt="Nvidia DGX Station GB300 784GB available now! 95,000 USD / 80,000 EUR" src="https://preview.redd.it/gbyd7eublz6g1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=95bd41bd9c2023177ab88f88bba16f8b874d2446" title="Nvidia DGX Station GB300 784GB available now! 95,000 USD / 80,000 EUR" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just got pricing info minutes ago. Lead time is approx. 1 month.&lt;/p&gt; &lt;p&gt;Specs:&lt;br /&gt; 72-core Nvidia Grace CPU&lt;br /&gt; Nvidia Blackwell Ultra B300 Tensor Core GPU&lt;br /&gt; 496GB of LPDDR5X memory with EEC&lt;br /&gt; 288GB of HBM3e memory&lt;br /&gt; 784GB of total fast-access memory&lt;br /&gt; NVlink-C2C: 900 GB/s of bandwidth&lt;br /&gt; 700W to 1400W TDP (CPU + CPU + memory)&lt;br /&gt; High-efficiency 2000W PSU&lt;br /&gt; 3x PCIe gen5 M.2 slots on board&lt;br /&gt; 3x FHFL PCIe Gen5 x16&lt;br /&gt; 5x USB 3.2 port&lt;br /&gt; Nvidia ConnectX-8 2-port 400G total 800G OSFP&lt;br /&gt; WLAN&lt;br /&gt; Sound&lt;br /&gt; Display port&lt;br /&gt; Halogen-free LSZH power cables&lt;br /&gt; Stainless steel bolts&lt;br /&gt; 25 decibels (configurable)&lt;br /&gt; Liquid-cooled&lt;br /&gt; 244 x 567 x 523 mm (20.6 x 9.6 x 22.3&amp;quot;)&lt;br /&gt; 30 kg (66 lbs)&lt;br /&gt; Component manufacturer: Gigabyte, ASUS, MSI, Supermicro&lt;br /&gt; The price does not contain any taxes.&lt;br /&gt; 3-year manufacturer's warranty&lt;br /&gt; Free shipping worldwide.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GPTshop"&gt; /u/GPTshop &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gbyd7eublz6g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plnjfy/nvidia_dgx_station_gb300_784gb_available_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1plnjfy/nvidia_dgx_station_gb300_784gb_available_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-13T15:08:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
