<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-21T12:27:58+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1mw7ush</id>
    <title>Constrained Decoding for Diffusion LLMs</title>
    <updated>2025-08-21T11:06:32+00:00</updated>
    <author>
      <name>/u/nielstron</name>
      <uri>https://old.reddit.com/user/nielstron</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw7ush/constrained_decoding_for_diffusion_llms/"&gt; &lt;img alt="Constrained Decoding for Diffusion LLMs" src="https://external-preview.redd.it/Qa_5iE_ckoouJFq4tJW7zju2MQH8f_JIysTRmOqsM1A.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9c730e3902554ff30748bee5ba1c73339e88659d" title="Constrained Decoding for Diffusion LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all, I recently developed a constrained decoding technique for Diffusion LLMs. Since these are getting more and more popular, though I might share it here.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nielstron"&gt; /u/nielstron &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://constrained-diffusion.ai"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw7ush/constrained_decoding_for_diffusion_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mw7ush/constrained_decoding_for_diffusion_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T11:06:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1mw220v</id>
    <title>US demand for 48GB 4090?</title>
    <updated>2025-08-21T05:13:48+00:00</updated>
    <author>
      <name>/u/CertainlyBright</name>
      <uri>https://old.reddit.com/user/CertainlyBright</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm able to make domestic (US) 48GB 4090's and offer 90 day warranties and videos of the process and testing. (I'm a gpu repair tech of 3 years) The benefit is higher vram and &lt;del&gt;1u&lt;/del&gt; 2 slot coolers for max pcie density. Though the cards will be louder than stock gaming cards.&lt;/p&gt; &lt;p&gt;But with 5090 over supply, and rtx a6000's being available, I was wondering if there's a demand for them in the US at 2900$ each or 900$ as an upgrade service&lt;/p&gt; &lt;p&gt;&lt;strong&gt;(edit, i meant to say 2 slot, not 1u)&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CertainlyBright"&gt; /u/CertainlyBright &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw220v/us_demand_for_48gb_4090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw220v/us_demand_for_48gb_4090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mw220v/us_demand_for_48gb_4090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T05:13:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1mvl0zk</id>
    <title>Qwen-Image-Edit #6 overall on LMArena, best open model image editor</title>
    <updated>2025-08-20T17:17:24+00:00</updated>
    <author>
      <name>/u/vibedonnie</name>
      <uri>https://old.reddit.com/user/vibedonnie</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvl0zk/qwenimageedit_6_overall_on_lmarena_best_open/"&gt; &lt;img alt="Qwen-Image-Edit #6 overall on LMArena, best open model image editor" src="https://preview.redd.it/90yj5wnyj7kf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c77694f269b30f417ff8568342a83f0ba81a1ec2" title="Qwen-Image-Edit #6 overall on LMArena, best open model image editor" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Surprised they didn't vote this one higher, I felt like the edits I saw Qwen make online were pretty good&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vibedonnie"&gt; /u/vibedonnie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/90yj5wnyj7kf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvl0zk/qwenimageedit_6_overall_on_lmarena_best_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mvl0zk/qwenimageedit_6_overall_on_lmarena_best_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T17:17:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1mvn50l</id>
    <title>Guys it's official, the nano banana model on lm arena is Google's</title>
    <updated>2025-08-20T18:32:13+00:00</updated>
    <author>
      <name>/u/Severe-Awareness829</name>
      <uri>https://old.reddit.com/user/Severe-Awareness829</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Severe-Awareness829"&gt; /u/Severe-Awareness829 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/OfficialLoganK/status/1957908528925909391"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvn50l/guys_its_official_the_nano_banana_model_on_lm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mvn50l/guys_its_official_the_nano_banana_model_on_lm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T18:32:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1mw0rm9</id>
    <title>Which weights under 50GB have the best *depth of knowledge*?</title>
    <updated>2025-08-21T04:03:45+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there a benchmark for this that doesn't mix knowledge with reasoning? Just sheer encyclopedia knowledge.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw0rm9/which_weights_under_50gb_have_the_best_depth_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw0rm9/which_weights_under_50gb_have_the_best_depth_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mw0rm9/which_weights_under_50gb_have_the_best_depth_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T04:03:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1mv6go1</id>
    <title>We beat Google Deepmind but got killed by a chinese lab</title>
    <updated>2025-08-20T05:46:26+00:00</updated>
    <author>
      <name>/u/Connect-Employ-4708</name>
      <uri>https://old.reddit.com/user/Connect-Employ-4708</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv6go1/we_beat_google_deepmind_but_got_killed_by_a/"&gt; &lt;img alt="We beat Google Deepmind but got killed by a chinese lab" src="https://external-preview.redd.it/eG8yNGJoZWQyNGtmMVo0YW9szsCgDSDYpHIZftteA0dldCtHqInQOZXGentR.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7913b23ef6b3d159bc028db814e051ecf2742451" title="We beat Google Deepmind but got killed by a chinese lab" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Two months ago, my friends in AI and I asked: What if an AI could actually use a phone like a human?&lt;/p&gt; &lt;p&gt;So we built an agentic framework that taps, swipes, types… and somehow it’s outperforming giant labs like &lt;strong&gt;Google DeepMind&lt;/strong&gt; and &lt;strong&gt;Microsoft Research&lt;/strong&gt; on the AndroidWorld benchmark.&lt;/p&gt; &lt;p&gt;We were thrilled about our results until a massive Chinese lab (Zhipu AI) released its results last week to take the top spot.&lt;/p&gt; &lt;p&gt;They’re slightly ahead, but they have an army of 50+ phds and I don't see how a team like us can compete with them, that does not seem realistic... except that they're closed source.&lt;/p&gt; &lt;p&gt;And we decided to open-source everything. That way, even as a small team, we can make our work count.&lt;/p&gt; &lt;p&gt;We’re currently building our own custom mobile RL gyms, training environments made to push this agent further and get closer to 100% on the benchmark.&lt;/p&gt; &lt;p&gt;What do you think can make a small team like us compete against such giants?&lt;/p&gt; &lt;p&gt;Repo’s here if you want to check it out or contribute: &lt;a href="https://github.com/minitap-ai/mobile-use"&gt;github.com/minitap-ai/mobile-use&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Connect-Employ-4708"&gt; /u/Connect-Employ-4708 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/qvewe6nd24kf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv6go1/we_beat_google_deepmind_but_got_killed_by_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mv6go1/we_beat_google_deepmind_but_got_killed_by_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T05:46:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1mvynft</id>
    <title>Maxsun Dual Intel Arc Pro B60 available at $2,999</title>
    <updated>2025-08-21T02:18:53+00:00</updated>
    <author>
      <name>/u/ConcaveTriangle5761</name>
      <uri>https://old.reddit.com/user/ConcaveTriangle5761</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I emailed Maxsun about availability of their dual B60 cards, and got a response:&lt;/p&gt; &lt;p&gt;&lt;em&gt;Hi,&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;let me introduce Mr. Jason Green, who is our US distributor for B60, he is gonna help you with the purchase, thanks.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Regards,&lt;/em&gt;&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;&lt;em&gt;Hi,&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;I'm Jason from Hydratech Builds, the US distributor for MAXSUN.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;To help you with your purchase, please let me know how many units you are interested in. For orders of fewer than 5 units, you can purchase directly from our website: [&lt;/em&gt;&lt;a href="http://www.hydratechbuilds.com%5D"&gt;&lt;em&gt;www.hydratechbuilds.com]&lt;/em&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Product page (Intel Arc Pro B60 48GB):&lt;/em&gt; &lt;a href="https://www.hydratechbuilds.com/product-page/intel-arc-pro-b60-dual-48g-turbo"&gt;&lt;em&gt;https://www.hydratechbuilds.com/product-page/intel-arc-pro-b60-dual-48g-turbo&lt;/em&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;If you are looking to purchase 5 units or more per SKU, please let me know, and I will send you our US bulk pricelist.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Thanks,&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Jason&lt;/em&gt;&lt;/p&gt; &lt;p&gt;On the product page, the cards are up at $2,999 USD each. I am reasonably confident that this is the official Maxsun US pricing, as the same website is listed under &lt;a href="https://www.maxsun.com/pages/where-to-buy/"&gt;https://www.maxsun.com/pages/where-to-buy/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ConcaveTriangle5761"&gt; /u/ConcaveTriangle5761 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvynft/maxsun_dual_intel_arc_pro_b60_available_at_2999/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvynft/maxsun_dual_intel_arc_pro_b60_available_at_2999/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mvynft/maxsun_dual_intel_arc_pro_b60_available_at_2999/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T02:18:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1mvfdja</id>
    <title>IBM and NASA just dropped Surya: an open‑source AI to forecast solar storms before they hit</title>
    <updated>2025-08-20T13:51:04+00:00</updated>
    <author>
      <name>/u/AskGpts</name>
      <uri>https://old.reddit.com/user/AskGpts</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvfdja/ibm_and_nasa_just_dropped_surya_an_opensource_ai/"&gt; &lt;img alt="IBM and NASA just dropped Surya: an open‑source AI to forecast solar storms before they hit" src="https://preview.redd.it/moddapg5j6kf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b6316f1a44b3898869b91f28f4d1774a35db2491" title="IBM and NASA just dropped Surya: an open‑source AI to forecast solar storms before they hit" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Solar storms don’t just make pretty auroras—they can scramble GPS, disrupt flights, degrade satellite comms, and stress power grids. To get ahead of that, IBM and NASA have open‑sourced Surya on Hugging Face: a foundation model trained on years of Solar Dynamics Observatory (SDO) data to make space‑weather forecasting more accurate and accessible.&lt;/p&gt; &lt;p&gt;What Surya is&lt;/p&gt; &lt;p&gt;A mid‑size foundation model for heliophysics that learns general “features of the Sun” from large SDO image archives.&lt;/p&gt; &lt;p&gt;Built to support zero/few‑shot tasks like flare probability, CME risk, and geomagnetic indices (e.g., Kp/Dst) with fine‑tuning.&lt;/p&gt; &lt;p&gt;Released with open weights and recipes so labs, universities, and startups can adapt it without massive compute.&lt;/p&gt; &lt;p&gt;Why this matters&lt;/p&gt; &lt;p&gt;Early, reliable alerts help airlines reroute, satellite operators safe‑mode hardware, and grid operators harden the network before a hit.&lt;/p&gt; &lt;p&gt;Open sourcing lowers the barrier for regional forecasters and fosters reproducible science (shared baselines, comparable benchmarks).&lt;/p&gt; &lt;p&gt;We’re in an active solar cycle—better lead times now can prevent expensive outages and service disruptions.&lt;/p&gt; &lt;p&gt;How to try it (technical)&lt;/p&gt; &lt;p&gt;Pull the model from Hugging Face and fine‑tune on your target label: flare class prediction, Kp nowcasting, or satellite anomaly detection.&lt;/p&gt; &lt;p&gt;Start with SDO preprocessing pipelines; add lightweight adapters/LoRA for event‑specific fine‑tuning to keep compute modest.&lt;/p&gt; &lt;p&gt;Evaluate on public benchmarks (Kp/Dst) and report lead time vs. skill scores; stress test on extreme events.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AskGpts"&gt; /u/AskGpts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/moddapg5j6kf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvfdja/ibm_and_nasa_just_dropped_surya_an_opensource_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mvfdja/ibm_and_nasa_just_dropped_surya_an_opensource_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T13:51:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1mvjj8q</id>
    <title>Seed-OSS-36B-Instruct</title>
    <updated>2025-08-20T16:23:50+00:00</updated>
    <author>
      <name>/u/NeterOster</name>
      <uri>https://old.reddit.com/user/NeterOster</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/ByteDance-Seed/Seed-OSS-36B-Instruct"&gt;https://huggingface.co/ByteDance-Seed/Seed-OSS-36B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Introduction:&lt;/p&gt; &lt;p&gt;Seed-OSS is a series of open-source large language models developed by ByteDance's Seed Team, designed for powerful long-context, reasoning, agent and general capabilities, and versatile developer-friendly features. Although trained with only 12T tokens, Seed-OSS achieves excellent performance on several popular open benchmarks.&lt;/p&gt; &lt;p&gt;We release this series of models to the open-source community under the Apache-2.0 license.&lt;/p&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;h1&gt;Key Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Flexible Control of Thinking Budget&lt;/strong&gt;: Allowing users to flexibly adjust the reasoning length as needed. This capability of dynamically controlling the reasoning length enhances inference efficiency in practical application scenarios.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Enhanced Reasoning Capability&lt;/strong&gt;: Specifically optimized for reasoning tasks while maintaining balanced and excellent general capabilities.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agentic Intelligence&lt;/strong&gt;: Performs exceptionally well in agentic tasks such as tool-using and issue resolving.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Research-Friendly&lt;/strong&gt;: Given that the inclusion of synthetic instruction data in pre-training may affect the post-training research, we released pre-trained models both with and without instruction data, providing the research community with more diverse options.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Native Long Context&lt;/strong&gt;: Trained with up-to-512K long context natively.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NeterOster"&gt; /u/NeterOster &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvjj8q/seedoss36binstruct/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvjj8q/seedoss36binstruct/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mvjj8q/seedoss36binstruct/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T16:23:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1mw2xci</id>
    <title>monkeSearch's first prototype is now public, And it works! Offline natural language query for local files using a VERY small LLM (Qwen3-0.6b) and it works amazingly right away. With temporal awareness.</title>
    <updated>2025-08-21T06:03:45+00:00</updated>
    <author>
      <name>/u/fuckAIbruhIhateCorps</name>
      <uri>https://old.reddit.com/user/fuckAIbruhIhateCorps</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys, this is a follow up post of my old post, which was about building a local natural language file search engine using qwen0.6b and LangExtract, and today I am very excited to release a very bare bones and working prototype for this!&lt;br /&gt; &lt;a href="https://github.com/monkesearch/monkeSearch"&gt;https://github.com/monkesearch/monkeSearch&lt;/a&gt; &lt;/p&gt; &lt;p&gt;I'd love to get reviews and suggestions for this, and I've used macOS's inbuilt spotlight indexing for the query. There are a lot of modifications and feature additions to be done now but I want you guys to try it out locally. Current file search is only limited to a few file types because I am associating the macOS specific uniform type identifiers with file types, and that has been done manually just for the prototype right now. But I'd love to get ideas on how can I improve this. &lt;/p&gt; &lt;p&gt;No data leaves your pc and it is aimed at being able to run on potato pcs. And I'm currently aiming at a smaller and smarter model (Gemma 3 270M finetune) to increase the accuracy of the tool (even though it's pretty accurate right away with base Qwen3) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fuckAIbruhIhateCorps"&gt; /u/fuckAIbruhIhateCorps &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw2xci/monkesearchs_first_prototype_is_now_public_and_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw2xci/monkesearchs_first_prototype_is_now_public_and_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mw2xci/monkesearchs_first_prototype_is_now_public_and_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T06:03:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1mw86zw</id>
    <title>I’m gonna say it:</title>
    <updated>2025-08-21T11:24:07+00:00</updated>
    <author>
      <name>/u/JLeonsarmiento</name>
      <uri>https://old.reddit.com/user/JLeonsarmiento</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw86zw/im_gonna_say_it/"&gt; &lt;img alt="I’m gonna say it:" src="https://preview.redd.it/gxbn2ofuxckf1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d114958447f9d74be0e026a7a772ce1a19fab790" title="I’m gonna say it:" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JLeonsarmiento"&gt; /u/JLeonsarmiento &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gxbn2ofuxckf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw86zw/im_gonna_say_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mw86zw/im_gonna_say_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T11:24:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1mvy6ai</id>
    <title>Qwen2.5 0.5B vs Qwen3 0.6B answering the same question. Definitely a big improvement.</title>
    <updated>2025-08-21T01:57:14+00:00</updated>
    <author>
      <name>/u/airbus_a360_when</name>
      <uri>https://old.reddit.com/user/airbus_a360_when</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvy6ai/qwen25_05b_vs_qwen3_06b_answering_the_same/"&gt; &lt;img alt="Qwen2.5 0.5B vs Qwen3 0.6B answering the same question. Definitely a big improvement." src="https://a.thumbs.redditmedia.com/QpddyvC1oI2y7Ei80TMeqG06SbkjYBp8kDygMRpJtF8.jpg" title="Qwen2.5 0.5B vs Qwen3 0.6B answering the same question. Definitely a big improvement." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/airbus_a360_when"&gt; /u/airbus_a360_when &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mvy6ai"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvy6ai/qwen25_05b_vs_qwen3_06b_answering_the_same/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mvy6ai/qwen25_05b_vs_qwen3_06b_answering_the_same/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T01:57:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1mw5due</id>
    <title>LiteRP – lightweight open-source frontend for local LLM roleplay</title>
    <updated>2025-08-21T08:38:38+00:00</updated>
    <author>
      <name>/u/sumrix</name>
      <uri>https://old.reddit.com/user/sumrix</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw5due/literp_lightweight_opensource_frontend_for_local/"&gt; &lt;img alt="LiteRP – lightweight open-source frontend for local LLM roleplay" src="https://preview.redd.it/sfwt2l733ckf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e50f5d1c5a6adbcb3044ec3d9dfddeef2047756f" title="LiteRP – lightweight open-source frontend for local LLM roleplay" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been working on a minimal frontend for chatting and roleplay with AI characters, and I’d like to share the first early beta release &lt;strong&gt;LiteRP v0.3&lt;/strong&gt;: &lt;a href="https://github.com/Sumrix/LiteRP"&gt;https://github.com/Sumrix/LiteRP&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Most roleplay frontends (like SillyTavern) are powerful but heavy and complex to set up. LiteRP takes a different approach:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Single compact executable (~17 MB) for Windows, Linux, macOS&lt;/li&gt; &lt;li&gt;No Python, npm, or extra dependencies&lt;/li&gt; &lt;li&gt;Launch the binary → browser opens at http://localhost:5000/&lt;/li&gt; &lt;li&gt;Supports TavernAI v2 character cards (&lt;code&gt;.png&lt;/code&gt;)&lt;/li&gt; &lt;li&gt;Interface similar to ChatGPT/character.ai, simple and familiar&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Right now LiteRP connects through &lt;strong&gt;Ollama&lt;/strong&gt;. That’s the only supported backend for the moment, but the design allows for additional APIs/backends in the future.&lt;/p&gt; &lt;p&gt;Downloads: &lt;a href="https://github.com/Sumrix/LiteRP/releases/latest"&gt;GitHub Releases&lt;/a&gt;&lt;br /&gt; Screenshots: &lt;a href="https://github.com/Sumrix/LiteRP/blob/main/SCREENSHOTS.md"&gt;Gallery&lt;/a&gt;&lt;br /&gt; Roadmap: &lt;a href="https://github.com/Sumrix/LiteRP/blob/main/ROADMAP.md"&gt;ROADMAP&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you’re just looking for a model to try, I’ve had good results with:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;ollama pull nchapman/mn-12b-mag-mell-r1 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Current version is early beta (v0.3). Basic roleplay already works, but features like message editing and other polish are still coming. Feedback is very welcome.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sumrix"&gt; /u/sumrix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sfwt2l733ckf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw5due/literp_lightweight_opensource_frontend_for_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mw5due/literp_lightweight_opensource_frontend_for_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T08:38:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1mw724q</id>
    <title>Introducing Intern-S1-mini, a lightweight version of Intern-S1, which contains an 8B language model and a 0.3B vision encoder.</title>
    <updated>2025-08-21T10:22:21+00:00</updated>
    <author>
      <name>/u/Lynncc6</name>
      <uri>https://old.reddit.com/user/Lynncc6</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw724q/introducing_interns1mini_a_lightweight_version_of/"&gt; &lt;img alt="Introducing Intern-S1-mini, a lightweight version of Intern-S1, which contains an 8B language model and a 0.3B vision encoder." src="https://external-preview.redd.it/iJbqIXj4e8d90OQ705I7po4CzD6K5SM0Vr9TFSrm88U.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c1b47ebb9c988da9c5a585463d2f79deee9f0407" title="Introducing Intern-S1-mini, a lightweight version of Intern-S1, which contains an 8B language model and a 0.3B vision encoder." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lynncc6"&gt; /u/Lynncc6 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/InternLM/Intern-S1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw724q/introducing_interns1mini_a_lightweight_version_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mw724q/introducing_interns1mini_a_lightweight_version_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T10:22:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1mvw3hz</id>
    <title>NVIDIA Achieves 35% Performance Boost for OpenAI’s GPT-OSS-120B Model</title>
    <updated>2025-08-21T00:20:54+00:00</updated>
    <author>
      <name>/u/vibedonnie</name>
      <uri>https://old.reddit.com/user/vibedonnie</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvw3hz/nvidia_achieves_35_performance_boost_for_openais/"&gt; &lt;img alt="NVIDIA Achieves 35% Performance Boost for OpenAI’s GPT-OSS-120B Model" src="https://b.thumbs.redditmedia.com/e1iw4XluNhKx2_uQFrUBfjQ-KXChA-T80tHk2Ay_3VI.jpg" title="NVIDIA Achieves 35% Performance Boost for OpenAI’s GPT-OSS-120B Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vibedonnie"&gt; /u/vibedonnie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mvw3hz"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvw3hz/nvidia_achieves_35_performance_boost_for_openais/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mvw3hz/nvidia_achieves_35_performance_boost_for_openais/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T00:20:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1mw3kmd</id>
    <title>DeepSeek-V3.1 (Thinking and Non Thinking)</title>
    <updated>2025-08-21T06:43:19+00:00</updated>
    <author>
      <name>/u/touhidul002</name>
      <uri>https://old.reddit.com/user/touhidul002</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw3kmd/deepseekv31_thinking_and_non_thinking/"&gt; &lt;img alt="DeepSeek-V3.1 (Thinking and Non Thinking)" src="https://preview.redd.it/131ngchkjbkf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c84b80fbda47f246b1ba5d8d04285fb722a69588" title="DeepSeek-V3.1 (Thinking and Non Thinking)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;DeepSeek-V3.1 is a hybrid model that supports both thinking mode and non-thinking mode. Compared to the previous version, this upgrade brings improvements in multiple aspects:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Hybrid thinking mode&lt;/strong&gt;: One model supports both thinking mode and non-thinking mode by changing the chat template.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Smarter tool calling&lt;/strong&gt;: Through post-training optimization, the model's performance in tool usage and agent tasks has significantly improved.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Higher thinking efficiency&lt;/strong&gt;: DeepSeek-V3.1-Think achieves comparable answer quality to DeepSeek-R1-0528, while responding more quickly.&lt;/li&gt; &lt;/ul&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Category&lt;/th&gt; &lt;th align="left"&gt;Benchmark (Metric)&lt;/th&gt; &lt;th align="left"&gt;DeepSeek V3.1-NonThinking&lt;/th&gt; &lt;th align="left"&gt;DeepSeek V3 0324&lt;/th&gt; &lt;th align="left"&gt;DeepSeek V3.1-Thinking&lt;/th&gt; &lt;th align="left"&gt;DeepSeek R1 0528&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;General&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;MMLU-Redux (EM)&lt;/td&gt; &lt;td align="left"&gt;91.8&lt;/td&gt; &lt;td align="left"&gt;90.5&lt;/td&gt; &lt;td align="left"&gt;93.7&lt;/td&gt; &lt;td align="left"&gt;93.4&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;MMLU-Pro (EM)&lt;/td&gt; &lt;td align="left"&gt;83.7&lt;/td&gt; &lt;td align="left"&gt;81.2&lt;/td&gt; &lt;td align="left"&gt;84.8&lt;/td&gt; &lt;td align="left"&gt;85.0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;GPQA-Diamond (Pass@1)&lt;/td&gt; &lt;td align="left"&gt;74.9&lt;/td&gt; &lt;td align="left"&gt;68.4&lt;/td&gt; &lt;td align="left"&gt;80.1&lt;/td&gt; &lt;td align="left"&gt;81.0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Humanity's Last Exam (Pass@1)&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;td align="left"&gt;15.9&lt;/td&gt; &lt;td align="left"&gt;17.7&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Search Agent&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;BrowseComp&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;td align="left"&gt;30.0&lt;/td&gt; &lt;td align="left"&gt;8.9&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;BrowseComp_zh&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;td align="left"&gt;49.2&lt;/td&gt; &lt;td align="left"&gt;35.7&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Humanity's Last Exam (Python + Search)&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;td align="left"&gt;29.8&lt;/td&gt; &lt;td align="left"&gt;24.8&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;SimpleQA&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;td align="left"&gt;93.4&lt;/td&gt; &lt;td align="left"&gt;92.3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Code&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;LiveCodeBench (2408-2505) (Pass@1)&lt;/td&gt; &lt;td align="left"&gt;56.4&lt;/td&gt; &lt;td align="left"&gt;43.0&lt;/td&gt; &lt;td align="left"&gt;74.8&lt;/td&gt; &lt;td align="left"&gt;73.3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Codeforces-Div1 (Rating)&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;td align="left"&gt;2091&lt;/td&gt; &lt;td align="left"&gt;1930&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Aider-Polyglot (Acc.)&lt;/td&gt; &lt;td align="left"&gt;68.4&lt;/td&gt; &lt;td align="left"&gt;55.1&lt;/td&gt; &lt;td align="left"&gt;76.3&lt;/td&gt; &lt;td align="left"&gt;71.6&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Code Agent&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;SWE Verified (Agent mode)&lt;/td&gt; &lt;td align="left"&gt;66.0&lt;/td&gt; &lt;td align="left"&gt;45.4&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;td align="left"&gt;44.6&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;SWE-bench Multilingual (Agent mode)&lt;/td&gt; &lt;td align="left"&gt;54.5&lt;/td&gt; &lt;td align="left"&gt;29.3&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;td align="left"&gt;30.5&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Terminal-bench (Terminus 1 framework)&lt;/td&gt; &lt;td align="left"&gt;31.3&lt;/td&gt; &lt;td align="left"&gt;13.3&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;td align="left"&gt;5.7&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Math&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;AIME 2024 (Pass@1)&lt;/td&gt; &lt;td align="left"&gt;66.3&lt;/td&gt; &lt;td align="left"&gt;59.4&lt;/td&gt; &lt;td align="left"&gt;93.1&lt;/td&gt; &lt;td align="left"&gt;91.4&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;AIME 2025 (Pass@1)&lt;/td&gt; &lt;td align="left"&gt;49.8&lt;/td&gt; &lt;td align="left"&gt;51.3&lt;/td&gt; &lt;td align="left"&gt;88.4&lt;/td&gt; &lt;td align="left"&gt;87.5&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;HMMT 2025 (Pass@1)&lt;/td&gt; &lt;td align="left"&gt;33.5&lt;/td&gt; &lt;td align="left"&gt;29.2&lt;/td&gt; &lt;td align="left"&gt;84.2&lt;/td&gt; &lt;td align="left"&gt;79.4&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/touhidul002"&gt; /u/touhidul002 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/131ngchkjbkf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw3kmd/deepseekv31_thinking_and_non_thinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mw3kmd/deepseekv31_thinking_and_non_thinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T06:43:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1mw3j7l</id>
    <title>Deepseek V3.1 is not so bad after all..</title>
    <updated>2025-08-21T06:40:50+00:00</updated>
    <author>
      <name>/u/Trevor050</name>
      <uri>https://old.reddit.com/user/Trevor050</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw3j7l/deepseek_v31_is_not_so_bad_after_all/"&gt; &lt;img alt="Deepseek V3.1 is not so bad after all.." src="https://b.thumbs.redditmedia.com/T9qzPo7Bvut8hGvtV6Xg52JupsxwiKXPdU34MXCHojI.jpg" title="Deepseek V3.1 is not so bad after all.." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It seems like it just was a different purpose, speed and agency. Its pretty good at what its meant for&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Trevor050"&gt; /u/Trevor050 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mw3j7l"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw3j7l/deepseek_v31_is_not_so_bad_after_all/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mw3j7l/deepseek_v31_is_not_so_bad_after_all/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T06:40:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1mw7x6f</id>
    <title>New DeepSeek API pricing: -chat prices increasing, -reasoner prices decreasing</title>
    <updated>2025-08-21T11:09:58+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw7x6f/new_deepseek_api_pricing_chat_prices_increasing/"&gt; &lt;img alt="New DeepSeek API pricing: -chat prices increasing, -reasoner prices decreasing" src="https://preview.redd.it/d2xgmwobvckf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2d9ce209cc69d0fc5c15c3d82253b16c89c0b2ac" title="New DeepSeek API pricing: -chat prices increasing, -reasoner prices decreasing" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;New API pricing scheme goes into effect on September 5, 2025: &lt;a href="https://api-docs.deepseek.com/quick_start/pricing"&gt;https://api-docs.deepseek.com/quick_start/pricing&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/d2xgmwobvckf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw7x6f/new_deepseek_api_pricing_chat_prices_increasing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mw7x6f/new_deepseek_api_pricing_chat_prices_increasing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T11:09:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1mw0tc4</id>
    <title>Finally Kimi-VL-A3B-Thinking-2506-GGUF is available</title>
    <updated>2025-08-21T04:06:14+00:00</updated>
    <author>
      <name>/u/kironlau</name>
      <uri>https://old.reddit.com/user/kironlau</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw0tc4/finally_kimivla3bthinking2506gguf_is_available/"&gt; &lt;img alt="Finally Kimi-VL-A3B-Thinking-2506-GGUF is available" src="https://external-preview.redd.it/m2lF_KqN7wgwWcFm1a3lN4x_joA1P4xA8L66Y-aVPXM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6e8aa6a2602eb7ba21a6af8e92b4d296f3365548" title="Finally Kimi-VL-A3B-Thinking-2506-GGUF is available" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Original model: &lt;a href="https://huggingface.co/moonshotai/Kimi-VL-A3B-Thinking-2506"&gt;https://huggingface.co/moonshotai/Kimi-VL-A3B-Thinking-2506&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Supported added in this PR: &lt;a href="https://github.com/ggml-org/llama.cpp/pull/15458"&gt;https://github.com/ggml-org/llama.cpp/pull/15458&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kironlau"&gt; /u/kironlau &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/ggml-org/Kimi-VL-A3B-Thinking-2506-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw0tc4/finally_kimivla3bthinking2506gguf_is_available/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mw0tc4/finally_kimivla3bthinking2506gguf_is_available/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T04:06:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1mw73uz</id>
    <title>DeepSeek has revealed that the next generation of China-made chips is about to be released</title>
    <updated>2025-08-21T10:25:14+00:00</updated>
    <author>
      <name>/u/Dry-Ad8947</name>
      <uri>https://old.reddit.com/user/Dry-Ad8947</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw73uz/deepseek_has_revealed_that_the_next_generation_of/"&gt; &lt;img alt="DeepSeek has revealed that the next generation of China-made chips is about to be released" src="https://b.thumbs.redditmedia.com/Ior1GVKeeGS67aHoSqPUGnAsxjCS7RxbgR-JhoE9meg.jpg" title="DeepSeek has revealed that the next generation of China-made chips is about to be released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/5j7osgkanckf1.png?width=1205&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bad0b7a62ad023889c86de7320fda3c7f4871f03"&gt;https://preview.redd.it/5j7osgkanckf1.png?width=1205&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bad0b7a62ad023889c86de7320fda3c7f4871f03&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In an official post on DeepSeek's official WeChat account, DeepSeek further explained that UE8M0 FP8 is designed for the upcoming next-generation domestic chip.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dry-Ad8947"&gt; /u/Dry-Ad8947 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw73uz/deepseek_has_revealed_that_the_next_generation_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw73uz/deepseek_has_revealed_that_the_next_generation_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mw73uz/deepseek_has_revealed_that_the_next_generation_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T10:25:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1mw2lme</id>
    <title>Frontier AI labs’ publicized 100k-H100 training runs under-deliver because software and systems don’t scale efficiently, wasting massive GPU fleets</title>
    <updated>2025-08-21T05:44:41+00:00</updated>
    <author>
      <name>/u/vladlearns</name>
      <uri>https://old.reddit.com/user/vladlearns</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw2lme/frontier_ai_labs_publicized_100kh100_training/"&gt; &lt;img alt="Frontier AI labs’ publicized 100k-H100 training runs under-deliver because software and systems don’t scale efficiently, wasting massive GPU fleets" src="https://b.thumbs.redditmedia.com/qEAZdKzAv0zTNBneqhh9EDw7ENWMaMbCmlZSPzIj5Hw.jpg" title="Frontier AI labs’ publicized 100k-H100 training runs under-deliver because software and systems don’t scale efficiently, wasting massive GPU fleets" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vladlearns"&gt; /u/vladlearns &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mw2lme"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw2lme/frontier_ai_labs_publicized_100kh100_training/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mw2lme/frontier_ai_labs_publicized_100kh100_training/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T05:44:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1mvnmjo</id>
    <title>My LLM trained from scratch on only 1800s London texts brings up a real protest from 1834</title>
    <updated>2025-08-20T18:49:36+00:00</updated>
    <author>
      <name>/u/Remarkable-Trick-177</name>
      <uri>https://old.reddit.com/user/Remarkable-Trick-177</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvnmjo/my_llm_trained_from_scratch_on_only_1800s_london/"&gt; &lt;img alt="My LLM trained from scratch on only 1800s London texts brings up a real protest from 1834" src="https://external-preview.redd.it/bruJaed8mpWclO3rYYnLL_4tpIRSDSNQT1lxjc08864.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1de2bdb3583ed7a714be2ea7450e90270d3b3e83" title="My LLM trained from scratch on only 1800s London texts brings up a real protest from 1834" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I’ve posted on here a couple times sharing my project. I'm training LLM’s from scratch on 1800’s London texts (no fine tune/modern data). I built a dataset using 7,000 texts published between 1800 to 1875 in the city of London, and also trained a custom tokenizer on the dataset itself to get rid of modern vocab. &lt;/p&gt; &lt;p&gt;So far I’ve trained 3 models, 2 with nanoGPT and the latest using Phi 1.5. After training, I messed around with some prompts and used this one:&lt;/p&gt; &lt;p&gt;&amp;quot;It was the year of our Lord 1834&amp;quot; &lt;/p&gt; &lt;p&gt;Here’s the output:&lt;/p&gt; &lt;p&gt; &amp;quot;It was the year of our Lord 1834 and the streets of London were filled with protest and petition. The cause, as many re counted, was not bound in the way of private, but having taken up the same day in the day of Lord Palmerston, the public will receive a short statement of the difficulties under which the day of law has reached us. It is a matter of deep regret, that the present events in the history of the world are clear, and consequently will be'known. It is not true that the very men who first settled in the Gospel at Jerusalem should have so extensive and so interesting a record of the prosperity and prosperity&amp;quot; (The last sentence is weird but stuff like that shows up a lot probably due to heavy biblical influence)&lt;/p&gt; &lt;p&gt;I was interested to see if a protest had actually occurred in 1834 London and it really did happen but I thought it was maybe just a coincidence. The output also brought up “Lord Palmerston” and after a google search I learned that his actions resulted in the 1834 protests. So this idea is past just mimicking 1800s text and can now actually recall real historical events. &lt;/p&gt; &lt;p&gt;This is all from just 5-6GB of data, imagine the results with 30GB or more. I’m not sure if just scaling the data up will ever result in reasoning but even now it kinda feels like digital time travel. I want to eventually try different cities also, maybe a Chinese, Russian or Indian or even just another English city model. I’m just doing this for fun so if anyone would like to collaborate let me know, I’m open to anything really. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9e997tbsy7kf1.png?width=1332&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bbac7818db44afc70c666c677ccae1f94c4a486e"&gt;https://preview.redd.it/9e997tbsy7kf1.png?width=1332&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bbac7818db44afc70c666c677ccae1f94c4a486e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/haykgrigo3/TimeCapsuleLLM"&gt;https://github.com/haykgrigo3/TimeCapsuleLLM&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Remarkable-Trick-177"&gt; /u/Remarkable-Trick-177 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvnmjo/my_llm_trained_from_scratch_on_only_1800s_london/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvnmjo/my_llm_trained_from_scratch_on_only_1800s_london/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mvnmjo/my_llm_trained_from_scratch_on_only_1800s_london/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T18:49:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1mw3nat</id>
    <title>DeepSeek-V3.1 implements Anthropic API compatibility</title>
    <updated>2025-08-21T06:47:55+00:00</updated>
    <author>
      <name>/u/vibedonnie</name>
      <uri>https://old.reddit.com/user/vibedonnie</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw3nat/deepseekv31_implements_anthropic_api_compatibility/"&gt; &lt;img alt="DeepSeek-V3.1 implements Anthropic API compatibility" src="https://preview.redd.it/0pp8mwjkkbkf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=44e7a0529d636accee5763e0a807e41d636629b2" title="DeepSeek-V3.1 implements Anthropic API compatibility" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://api-docs.deepseek.com/guides/anthropic_api"&gt;https://api-docs.deepseek.com/guides/anthropic_api&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vibedonnie"&gt; /u/vibedonnie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0pp8mwjkkbkf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw3nat/deepseekv31_implements_anthropic_api_compatibility/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mw3nat/deepseekv31_implements_anthropic_api_compatibility/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T06:47:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1mw3c7s</id>
    <title>deepseek-ai/DeepSeek-V3.1 · Hugging Face</title>
    <updated>2025-08-21T06:28:56+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw3c7s/deepseekaideepseekv31_hugging_face/"&gt; &lt;img alt="deepseek-ai/DeepSeek-V3.1 · Hugging Face" src="https://external-preview.redd.it/RJXEgvNDm4zhSkGlks1Mt4ppnLOAENNDWYNaVwpLE9k.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d409de545b1a58fad7e22e741370f1a55018f432" title="deepseek-ai/DeepSeek-V3.1 · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw3c7s/deepseekaideepseekv31_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mw3c7s/deepseekaideepseekv31_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T06:28:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1mw7ecz</id>
    <title>Just need 20 million more H100s and we will have AGI just trust me</title>
    <updated>2025-08-21T10:42:05+00:00</updated>
    <author>
      <name>/u/analgerianabroad</name>
      <uri>https://old.reddit.com/user/analgerianabroad</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw7ecz/just_need_20_million_more_h100s_and_we_will_have/"&gt; &lt;img alt="Just need 20 million more H100s and we will have AGI just trust me" src="https://preview.redd.it/7hwag17tpckf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3affb64c4bda8c741f113f55067b820bbe0716f8" title="Just need 20 million more H100s and we will have AGI just trust me" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/analgerianabroad"&gt; /u/analgerianabroad &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7hwag17tpckf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw7ecz/just_need_20_million_more_h100s_and_we_will_have/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mw7ecz/just_need_20_million_more_h100s_and_we_will_have/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T10:42:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjf5ol</id>
    <title>r/LocalLlama is looking for moderators</title>
    <updated>2025-08-06T20:06:34+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/application/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T20:06:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
