<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-21T08:53:18+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1p2kl8e</id>
    <title>Best RAG Architecture &amp; Stack for 10M+ Text Files? (Semantic Search Assistant)</title>
    <updated>2025-11-21T00:50:04+00:00</updated>
    <author>
      <name>/u/Additional-Oven4640</name>
      <uri>https://old.reddit.com/user/Additional-Oven4640</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am building an AI assistant for a dataset of &lt;strong&gt;10 million text documents&lt;/strong&gt; (PostgreSQL). The goal is to enable deep &lt;strong&gt;semantic search&lt;/strong&gt; and chat capabilities over this data.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Requirements:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Scale:&lt;/strong&gt; The system must handle 10M files efficiently (likely resulting in 100M+ vectors).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Updates:&lt;/strong&gt; I need to easily add/remove documents monthly without re-indexing the whole database.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Maintenance:&lt;/strong&gt; Looking for a system that is relatively easy to manage and cost-effective.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;My Questions:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Architecture:&lt;/strong&gt; Which approach is best for this scale (Standard Hybrid, LightRAG, Modular, etc.)?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Tech Stack:&lt;/strong&gt; Which specific tools (Vector DB, Orchestrator like Dify/LangChain/AnythingLLM, etc.) would you recommend to build this?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Thanks for the advice!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Additional-Oven4640"&gt; /u/Additional-Oven4640 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2kl8e/best_rag_architecture_stack_for_10m_text_files/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2kl8e/best_rag_architecture_stack_for_10m_text_files/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2kl8e/best_rag_architecture_stack_for_10m_text_files/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T00:50:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2gmhu</id>
    <title>Built a fully offline voice assistant on a Orange Pi 5+ | Qwen3-4B + Vosk + Piper, 100% local, zero cloud dependencies</title>
    <updated>2025-11-20T22:04:22+00:00</updated>
    <author>
      <name>/u/anunimo</name>
      <uri>https://old.reddit.com/user/anunimo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey yall, hope you all are having a nice evening.&lt;br /&gt; This is my first time ever interracting with local AI models :P&lt;/p&gt; &lt;p&gt;This little project is fully offline, and runs Qwen3-4B LLM on the 6TOPS NPU&lt;br /&gt; Currently, only running on WEB GUI.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;~15-21 second end-to-end latency (Works for smaller loads).&lt;/li&gt; &lt;li&gt;Multilingual support&lt;/li&gt; &lt;li&gt;100% offline, no cloud&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Im hoping to make this project way smaller hardware sized and add a nice touch screen to it to make it portable.&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1p2gmhu/video/gq0pnvcxih2g1/player"&gt;https://reddit.com/link/1p2gmhu/video/gq0pnvcxih2g1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/anunimo"&gt; /u/anunimo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2gmhu/built_a_fully_offline_voice_assistant_on_a_orange/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2gmhu/built_a_fully_offline_voice_assistant_on_a_orange/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2gmhu/built_a_fully_offline_voice_assistant_on_a_orange/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-20T22:04:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2j2h7</id>
    <title>I gave in for the sake of testing!</title>
    <updated>2025-11-20T23:43:32+00:00</updated>
    <author>
      <name>/u/rogertorque</name>
      <uri>https://old.reddit.com/user/rogertorque</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2j2h7/i_gave_in_for_the_sake_of_testing/"&gt; &lt;img alt="I gave in for the sake of testing!" src="https://preview.redd.it/y4h5gtvp0i2g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=095174dcea57c3179f2238c45c9f18ff6fd784d0" title="I gave in for the sake of testing!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Let‚Äôs see how it‚Äôs does in LoRA, and RAG. Any suggestions?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rogertorque"&gt; /u/rogertorque &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/y4h5gtvp0i2g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2j2h7/i_gave_in_for_the_sake_of_testing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2j2h7/i_gave_in_for_the_sake_of_testing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-20T23:43:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1p20zry</id>
    <title>GigaChat3-702B-A36B-preview</title>
    <updated>2025-11-20T11:41:21+00:00</updated>
    <author>
      <name>/u/swagerka21</name>
      <uri>https://old.reddit.com/user/swagerka21</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;New model from sberai &lt;a href="https://huggingface.co/ai-sage/GigaChat3-702B-A36B-preview"&gt;https://huggingface.co/ai-sage/GigaChat3-702B-A36B-preview&lt;/a&gt; &lt;a href="https://github.com/salute-developers/gigachat3"&gt;https://github.com/salute-developers/gigachat3&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/swagerka21"&gt; /u/swagerka21 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p20zry/gigachat3702ba36bpreview/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p20zry/gigachat3702ba36bpreview/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p20zry/gigachat3702ba36bpreview/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-20T11:41:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2rwgz</id>
    <title>Showcasing a media search engine</title>
    <updated>2025-11-21T06:57:17+00:00</updated>
    <author>
      <name>/u/ivoryavoidance</name>
      <uri>https://old.reddit.com/user/ivoryavoidance</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Have been working on polishing up a local media search engine:&lt;/p&gt; &lt;p&gt;&lt;a href="https://cinestar.sourceforge.io"&gt;https://cinestar.sourceforge.io&lt;/a&gt; . &lt;/p&gt; &lt;p&gt;Yt: &lt;a href="https://youtu.be/upaCDuOZtTs?si=U4DszIv_chsm6mA0"&gt;https://youtu.be/upaCDuOZtTs?si=U4DszIv_chsm6mA0&lt;/a&gt; (Watch at 2x)&lt;/p&gt; &lt;p&gt;Vision: To create an unified search for households or SMEs. One day when steam like boxes become affordable for normal people, like an OP alexa. Devices on the same network can be synced up. With a plugin based ecosystem, the app can be made to behave differently for different consumers, like a normal person can use it like google photos, have google memories style, stylize it maybe &lt;/p&gt; &lt;p&gt;SMEs and enterprises, studios would have NAS, for not putting there data on cloud. (Although for now, one can use a proxy or openai, from config, because it reduces the barrier to entry)&lt;/p&gt; &lt;p&gt;&lt;em&gt;Needs a local ollama install.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Models used:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;moondream v2 (vision)&lt;/li&gt; &lt;li&gt;qwen3:4b (general purpose)&lt;/li&gt; &lt;li&gt;whisper base.en (transcription)&lt;/li&gt; &lt;li&gt;bge for embedding&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The database is sqlite with sqlite vec, but this might change because the embedding search pagination is complicated.&lt;/p&gt; &lt;p&gt;But for other things, like background job processing, fts sqlite would still remain. The multi stage processing is to make content searchable &amp;quot;seemingly&amp;quot; faster. So, refinement is incremental. (Indexing 30-40 photos and trailers take time, and it's dependent on the system, and that's why the need for control over that device)&lt;/p&gt; &lt;p&gt;The video shows the system running on a 16G mac m4 (not mine üòÖ)&lt;/p&gt; &lt;p&gt;Blog: &lt;a href="https://ikouchiha47.github.io/2025/10/02/media-search.html"&gt;https://ikouchiha47.github.io/2025/10/02/media-search.html&lt;/a&gt; (It's a bit old)&lt;/p&gt; &lt;p&gt;The plugin system is in developing stages, the first one would be a dedup and organise. Some sort of taxonomy system.&lt;/p&gt; &lt;p&gt;And the search for making the system more intelligent, by evolving the embedding search layer.&lt;/p&gt; &lt;p&gt;Either I want it to be like VLC, or someway to eventually make some cash, keeping the version free.&lt;/p&gt; &lt;p&gt;Opensourcing plans: In some time, need to move some parts of the system to golang.&lt;/p&gt; &lt;p&gt;I love valve and steam. (Unrelated)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ivoryavoidance"&gt; /u/ivoryavoidance &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2rwgz/showcasing_a_media_search_engine/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2rwgz/showcasing_a_media_search_engine/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2rwgz/showcasing_a_media_search_engine/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T06:57:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2gan6</id>
    <title>Intel Panther Lake H 128GB LPDDR5X-10677 - 180 TOPS</title>
    <updated>2025-11-20T21:51:38+00:00</updated>
    <author>
      <name>/u/f4nt4</name>
      <uri>https://old.reddit.com/user/f4nt4</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://videocardz.com/newz/gmktec-evo-t2-mini-pc-pairs-intel-core-ultra-300-panther-lake-12xe-with-128gb-lpddr5x-memory"&gt;https://videocardz.com/newz/gmktec-evo-t2-mini-pc-pairs-intel-core-ultra-300-panther-lake-12xe-with-128gb-lpddr5x-memory&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Picture translated:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;GMKtec EVO-T2 Intel¬Æ Panther Lake H12Xe Mobile Processor EVO-T2 features Intel‚Äôs PantherLake H12Xe chip, manufactured with the latest 18A process. It supports up to 128GB 1067MT/s LPDDR5X memory, and is equipped with two SSD slots (PCIe 5.0 + PCIe 4.0). Maximum supported storage capacity is 16TB. Built-in AI PC capabilities with 180 TOPS performance. TDP up to 80W. &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/f4nt4"&gt; /u/f4nt4 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2gan6/intel_panther_lake_h_128gb_lpddr5x10677_180_tops/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2gan6/intel_panther_lake_h_128gb_lpddr5x10677_180_tops/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2gan6/intel_panther_lake_h_128gb_lpddr5x10677_180_tops/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-20T21:51:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2l3g4</id>
    <title>Need a good alternative for Gemini 2.5 Flash Preview TTS</title>
    <updated>2025-11-21T01:12:15+00:00</updated>
    <author>
      <name>/u/Extension_Giraffe_82</name>
      <uri>https://old.reddit.com/user/Extension_Giraffe_82</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been using the Gemini 2.5 Flash Preview TTS for a while now, and its quality is excellent across multiple languages. However, I've encountered a problem: it has only a 15 RPD (Requests Per Day) rate limit, which is unacceptable for my needs.&lt;/p&gt; &lt;p&gt;I tried to find a proxy without an RPD limit that I could pay for and use the Gemini API through, but there doesn't appear to be any available yet.&lt;/p&gt; &lt;p&gt;I need a good alternative with relatively high-quality audio and voices for multiple languages. Preferably cheap, open-source, or both. Or at least with feasible pricing (please don't recommend ElevenLabs; my goal isn't to go broke).&lt;/p&gt; &lt;p&gt;Does anyone know of anything like this?&lt;/p&gt; &lt;p&gt;(Note: a created note after some reference checks and found no relieble information how much is the rate limit. Somewhere it says &amp;quot;100&amp;quot; for free plan, somewhere it's 15, somewhere it's 50. The funny part that it's all from official google sources. It's very confusing, but the fact that this model has this low RPD rate limits is enoght for me to try finding alternatives)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Extension_Giraffe_82"&gt; /u/Extension_Giraffe_82 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2l3g4/need_a_good_alternative_for_gemini_25_flash/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2l3g4/need_a_good_alternative_for_gemini_25_flash/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2l3g4/need_a_good_alternative_for_gemini_25_flash/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T01:12:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2sjry</id>
    <title>Could Tabular Models Like Orion-MSP Ever Make Sense in Local LLM Setups?</title>
    <updated>2025-11-21T07:39:17+00:00</updated>
    <author>
      <name>/u/Dan27138</name>
      <uri>https://old.reddit.com/user/Dan27138</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been looking into &lt;strong&gt;Orion-MSP&lt;/strong&gt;, a tabular model that uses multi-scale sparse attention and a Perceiver-style memory module to support in-context learning on structured data. Architecturally, it‚Äôs interesting ‚Äî it tries to process tabular features at different scales while keeping compute manageable.&lt;/p&gt; &lt;p&gt;But it made me wonder how realistic something like this is for &lt;em&gt;local&lt;/em&gt; workflows.&lt;/p&gt; &lt;p&gt;A few things I‚Äôm wrestling with:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Multi-scale sparse attention sounds efficient on paper, but does it actually run well on consumer GPUs?&lt;/li&gt; &lt;li&gt;Perceiver-style memory seems flexible, but is it too heavy for local inference?&lt;/li&gt; &lt;li&gt;For most tabular tasks, would a model with this much architectural machinery even make sense outside research settings?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Curious if anyone here has tried running tabular Transformers locally ‚Äî does a design like Orion-MSP fit the local-LLM ecosystem, or is it fundamentally too large/complex?&lt;/p&gt; &lt;p&gt;(I can share the code/paper in a comment if anyone wants to look.)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dan27138"&gt; /u/Dan27138 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2sjry/could_tabular_models_like_orionmsp_ever_make/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2sjry/could_tabular_models_like_orionmsp_ever_make/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2sjry/could_tabular_models_like_orionmsp_ever_make/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T07:39:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1u9gv</id>
    <title>Spark Cluster!</title>
    <updated>2025-11-20T04:47:00+00:00</updated>
    <author>
      <name>/u/SashaUsesReddit</name>
      <uri>https://old.reddit.com/user/SashaUsesReddit</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1u9gv/spark_cluster/"&gt; &lt;img alt="Spark Cluster!" src="https://preview.redd.it/zmr4gy3ydc2g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7f25d102d17380204b2d6175e9e34708025777a7" title="Spark Cluster!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Doing dev and expanded my spark desk setup to eight!&lt;/p&gt; &lt;p&gt;Anyone have anything fun they want to see run on this HW?&lt;/p&gt; &lt;p&gt;Im not using the sparks for max performance, I'm using them for nccl/nvidia dev to deploy to B300 clusters. Really great platform to do small dev before deploying on large HW&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SashaUsesReddit"&gt; /u/SashaUsesReddit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zmr4gy3ydc2g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1u9gv/spark_cluster/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p1u9gv/spark_cluster/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-20T04:47:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2jbbh</id>
    <title>A small fine-tune of Gemma 3 4B focused on translation and text transformation</title>
    <updated>2025-11-20T23:54:38+00:00</updated>
    <author>
      <name>/u/thecalmgreen</name>
      <uri>https://old.reddit.com/user/thecalmgreen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2jbbh/a_small_finetune_of_gemma_3_4b_focused_on/"&gt; &lt;img alt="A small fine-tune of Gemma 3 4B focused on translation and text transformation" src="https://b.thumbs.redditmedia.com/7uF4A2T7aup6cy5bCwGhHqsKQIOm2Gk2wRf156mMjEE.jpg" title="A small fine-tune of Gemma 3 4B focused on translation and text transformation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/fczv34fm2i2g1.png?width=1233&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=db04767eb2aa0701d4e7a0f33a124b859cfc8cab"&gt;Gemma 3 4B Polyglot v1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hey everyone,&lt;br /&gt; I‚Äôve been working a lot with local models for translation, rewriting and quick text adjustments. Gemma 3 4B is already great, but I wanted something a bit more predictable, a bit more &amp;quot;fluid&amp;quot; and especially something that behaves really well inside my daily workflow.&lt;/p&gt; &lt;p&gt;Because of that, I fine-tuned a version that integrates beautifully with Polyglot Air. It was literally made for that. If you use Polyglot Air to translate selected text, correct grammar, switch tone or summarize with suffix-style commands, this model tends to respond in a cleaner and more consistent way.&lt;/p&gt; &lt;p&gt;It is not a big project. Just a simple fine-tune I made because I wanted smoother translation and rewriting with a local model. Since it improved my workflow, I‚Äôm sharing it here in case someone else finds it useful.&lt;/p&gt; &lt;h1&gt;What this fine-tune improves&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;More natural and accurate translations&lt;/li&gt; &lt;li&gt;Better consistency for grammar correction&lt;/li&gt; &lt;li&gt;Smoother tone shifts and rewriting&lt;/li&gt; &lt;li&gt;More stable behavior with suffix-based text transformations&lt;/li&gt; &lt;li&gt;Lightweight and friendly to run locally&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Model Weights&lt;/h1&gt; &lt;p&gt;HF repo:&lt;br /&gt; &lt;a href="https://huggingface.co/CalmState/gemma-3-4b-polyglot-v1"&gt;https://huggingface.co/CalmState/gemma-3-4b-polyglot-v1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GGUF build:&lt;br /&gt; &lt;a href="https://huggingface.co/CalmState/gemma-3-4b-polyglot-v1-Q8_0-GGUF"&gt;https://huggingface.co/CalmState/gemma-3-4b-polyglot-v1-Q8_0-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you try it, I‚Äôd love to hear any feedback.&lt;br /&gt; Hope it helps make someone's workflow a little calmer and smoother. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thecalmgreen"&gt; /u/thecalmgreen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2jbbh/a_small_finetune_of_gemma_3_4b_focused_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2jbbh/a_small_finetune_of_gemma_3_4b_focused_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2jbbh/a_small_finetune_of_gemma_3_4b_focused_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-20T23:54:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2lxqx</id>
    <title>watercooled server adventures</title>
    <updated>2025-11-21T01:51:20+00:00</updated>
    <author>
      <name>/u/j4ys0nj</name>
      <uri>https://old.reddit.com/user/j4ys0nj</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2lxqx/watercooled_server_adventures/"&gt; &lt;img alt="watercooled server adventures" src="https://b.thumbs.redditmedia.com/yLDRamdBMvfAbRCl11YkKSpSfpMdEpCLlgnmSd52qYQ.jpg" title="watercooled server adventures" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When I set out on this journey, it was not a journey, but now it is.&lt;/p&gt; &lt;p&gt;All I did was buy some cheap waterblocks for the pair of RTX A4500s I had at the time. I did already have a bunch of other GPUs... and now they will feel the cool flow of water over their chips as well.&lt;/p&gt; &lt;p&gt;How do you add watercooling to a server with 2x 5090s and an RTX PRO? Initially I thought 2x or 3x 360mm (120x3) radiators would do it. 3 might, but at full load for a few days... might not. My chassis can fit 2x 360mm rads, but 3.. I'd have to get creative.. or get a new chassis. Fine.&lt;/p&gt; &lt;p&gt;Then I had an idea. I knew Koolance made some external water cooling units.. but they were all out of stock, and cost more than I wanted to pay. &lt;/p&gt; &lt;p&gt;Maybe you see where this has taken me now..&lt;/p&gt; &lt;p&gt;An old 2U chassis, 2x 360mm rads and one.. I don't know what they call these.. 120x9 radiator, lots of EPDM tubing, more quick connects than I wanted to buy, pumps, fans, this aquaero 6 thing to control it all.. that might actually be old stock from like 10 years ago, some supports printed out of carbon fiber nylon and entirely too many G1/4 connectors. Still not sure how I'm going to power it, but I think an old 1U PSU can work.&lt;/p&gt; &lt;p&gt;Also - shout out to Bykski for making cool shit.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.bykski.us/products/bykski-durable-metal-pom-gpu-water-block-and-backplate-for-nvidia-rtx-pro-6000-blackwell-server-edition-n-rtxpro6000-sr-continuous-usage"&gt;RTX PRO 6000 SE Waterblock&lt;/a&gt;&lt;br /&gt; &lt;a href="https://www.bykski.us/products/bykski-durable-metal-pom-gpu-water-block-and-backplate-for-nvidia-geforce-rtx-5090-founders-edition-continuous-usage"&gt;RTX 5090 FE Waterblock&lt;/a&gt;&lt;br /&gt; &lt;a href="https://www.bykski.us/collections/nine-fan-1080mm/products/bykski-1080mm-x-46mm-rd-series-radiator-120mm-x-9-nine-fan-b-rd1080tk-v2"&gt;This big radiator&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I've since grabbed 2 more A4500s with waterblocks, so we'll be looking at 8x watercooled GPUs in the end. Which is about 3200W total. This setup can probably handle 3500W, or thereabouts. It's obviously not done yet.. but solid progress. Once I figure out the power supply thing and where to mount it, I might be good to go.&lt;/p&gt; &lt;p&gt;What you think? Where did I go wrong? How did I end up here... &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/lihi6ofwli2g1.png?width=1127&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b4954aea0a0dd4fbcb6920922412026b0fb4bc13"&gt;quick connects for all of the GPUs + CPU!&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9kwzfaizli2g1.png?width=1123&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7aead302957c715faccb67bf889d9051a371f019"&gt;dry fit, no water in it yet&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/vbb04jf7mi2g1.png?width=1124&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=00692e72d6b0f63d664f8144b065f2d7b4f50d1f"&gt;fill port on the side&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/aicymrnhmi2g1.png?width=1119&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=05649f52b1d08dfbc8fefcbeb19e39a8f4fa1148"&gt;temporary solution for the CPU. 140x60mm rad.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/rnsbohbbni2g1.png?width=1121&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9732c8dd5621e91bb4fd18c3ee05b248d74906df"&gt;Other box with a watercooled 4090. 140x60mm rad mounted on the back, 120x60mm up front. Actually works really well. Everything stays cool, believe it or not.&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/j4ys0nj"&gt; /u/j4ys0nj &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2lxqx/watercooled_server_adventures/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2lxqx/watercooled_server_adventures/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2lxqx/watercooled_server_adventures/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T01:51:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1p21385</id>
    <title>GigaChat3-702B-A36B-preview is now available on Hugging Face</title>
    <updated>2025-11-20T11:46:44+00:00</updated>
    <author>
      <name>/u/Any-Ship9886</name>
      <uri>https://old.reddit.com/user/Any-Ship9886</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sber AI has released GigaChat3-702B-A36B-preview, a massive 702B parameter model with active 36B parameters using MoE architecture. There are versions in fp8 and bf16. This is one of the largest openly available Russian LLMs to date.&lt;/p&gt; &lt;p&gt;Key specifications:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;702B total parameters with 36B active per token&lt;/li&gt; &lt;li&gt;128K context window&lt;/li&gt; &lt;li&gt;Supports Russian, English, and code generation&lt;/li&gt; &lt;li&gt;Released under MIT license&lt;/li&gt; &lt;li&gt;Trained on diverse Russian and multilingual datasets&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The model uses Mixture of Experts routing, making it feasible to run despite the enormous parameter count. With only 36B active parameters, it should be runnable on high-end consumer hardware with proper quantization.&lt;/p&gt; &lt;p&gt;Performance benchmarks show competitive results on Russian language tasks, though international benchmark scores are still being evaluated. Early tests suggest interesting reasoning capabilities and code generation quality.&lt;/p&gt; &lt;p&gt;Model card: &lt;a href="https://huggingface.co/ai-sage/GigaChat3-702B-A36B-preview"&gt;https://huggingface.co/ai-sage/GigaChat3-702B-A36B-preview&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any-Ship9886"&gt; /u/Any-Ship9886 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p21385/gigachat3702ba36bpreview_is_now_available_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p21385/gigachat3702ba36bpreview_is_now_available_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p21385/gigachat3702ba36bpreview_is_now_available_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-20T11:46:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1p24d2c</id>
    <title>Olmo3</title>
    <updated>2025-11-20T14:20:04+00:00</updated>
    <author>
      <name>/u/BreakfastFriendly728</name>
      <uri>https://old.reddit.com/user/BreakfastFriendly728</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p24d2c/olmo3/"&gt; &lt;img alt="Olmo3" src="https://b.thumbs.redditmedia.com/UJppPEN0RZP8y3BZ6uMmEsmjApLD6fufweSjic6DGkY.jpg" title="Olmo3" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ai2 released a series of new olmo 3 weights, including Olmo-3-32B-Think, along with data, code for training and evalution.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/allenai/olmo-3"&gt;https://huggingface.co/collections/allenai/olmo-3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/i98wyfc8bf2g1.png?width=2220&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d52ee933127ce4b88e2e94330d10f9d58bc1b5bb"&gt;https://preview.redd.it/i98wyfc8bf2g1.png?width=2220&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d52ee933127ce4b88e2e94330d10f9d58bc1b5bb&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BreakfastFriendly728"&gt; /u/BreakfastFriendly728 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p24d2c/olmo3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p24d2c/olmo3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p24d2c/olmo3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-20T14:20:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2sg7q</id>
    <title>How do you handle real-time AI monitoring?</title>
    <updated>2025-11-21T07:32:45+00:00</updated>
    <author>
      <name>/u/AdOrdinary5426</name>
      <uri>https://old.reddit.com/user/AdOrdinary5426</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking for ideas on preventing malicious prompts in production. Do you all build your own systems or use something off-the-shelf?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdOrdinary5426"&gt; /u/AdOrdinary5426 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2sg7q/how_do_you_handle_realtime_ai_monitoring/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2sg7q/how_do_you_handle_realtime_ai_monitoring/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2sg7q/how_do_you_handle_realtime_ai_monitoring/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T07:32:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2q5e5</id>
    <title>Small language models for agentic use?</title>
    <updated>2025-11-21T05:15:18+00:00</updated>
    <author>
      <name>/u/Swimming-Ratio4879</name>
      <uri>https://old.reddit.com/user/Swimming-Ratio4879</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I saw some posts about VibeThinker, which is a math model,and I thought why not create a multi-agent system that's literally made of Small language models that has a shared context or can communicate to pass context, like user said 1+1,the router route it to VibeThinker and another model, let's call it AgenticThinker,both models run in parallel based on the router's request,the challenge here would be to keep the answer coherent because the model will be filled with fine-tuning data and for that size it will just start doing calculations because you said &amp;quot;hi&amp;quot; that can be solved because we can use a small model that parallize the context between 2 and generate answer while both are communicating via context or generating the answer,that would lower ram usage (it's not an MoE that must be fully loaded) as it will activate from disk-to-vram based on demand,we can even speed it up by using ram to store the models if the router &amp;quot;thought&amp;quot; that it needs to pick a specific model based on the conversation flow,that would truly allow people with low vram to have multiple models and each activated automatically based on task!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Swimming-Ratio4879"&gt; /u/Swimming-Ratio4879 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2q5e5/small_language_models_for_agentic_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2q5e5/small_language_models_for_agentic_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2q5e5/small_language_models_for_agentic_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T05:15:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2lqi7</id>
    <title>Are any of the M series mac macbooks and mac minis, worth saving up for?</title>
    <updated>2025-11-21T01:41:53+00:00</updated>
    <author>
      <name>/u/No_Strawberry_8719</name>
      <uri>https://old.reddit.com/user/No_Strawberry_8719</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Like for ai locally and general tasks, are the mac m series worth the hype or are there better ways to run ai locally?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Strawberry_8719"&gt; /u/No_Strawberry_8719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2lqi7/are_any_of_the_m_series_mac_macbooks_and_mac/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2lqi7/are_any_of_the_m_series_mac_macbooks_and_mac/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2lqi7/are_any_of_the_m_series_mac_macbooks_and_mac/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T01:41:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2ncrr</id>
    <title>ubergarm/GigaChat3-10B-A1.8B-GGUF ~11GiB Q8_0</title>
    <updated>2025-11-21T02:57:30+00:00</updated>
    <author>
      <name>/u/VoidAlchemy</name>
      <uri>https://old.reddit.com/user/VoidAlchemy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2ncrr/ubergarmgigachat310ba18bgguf_11gib_q8_0/"&gt; &lt;img alt="ubergarm/GigaChat3-10B-A1.8B-GGUF ~11GiB Q8_0" src="https://external-preview.redd.it/rsMYYo-PBl_LaTyTfnfLp1CLd1qQqrGp0cpYiQ-K3U0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cbdff364a576d27ab13892b08d6753835c752f2b" title="ubergarm/GigaChat3-10B-A1.8B-GGUF ~11GiB Q8_0" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Needs a PR to get running for llama.cpp: * &lt;a href="https://github.com/ggml-org/llama.cpp/pull/17420"&gt;https://github.com/ggml-org/llama.cpp/pull/17420&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Issue open for ik_llama.cpp folks: * &lt;a href="https://github.com/ikawrakow/ik_llama.cpp/issues/994"&gt;https://github.com/ikawrakow/ik_llama.cpp/issues/994&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The chat template is missing a docstring out of the middle that wasn't parsing correctly. So you might be able to bring your own chat template using the instructions on the model card and if someone replies here: * &lt;a href="https://huggingface.co/ai-sage/GigaChat3-702B-A36B-preview-bf16/discussions/1"&gt;https://huggingface.co/ai-sage/GigaChat3-702B-A36B-preview-bf16/discussions/1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Though DevQuasar mentioned having a fixed template for the bigger 702B here: * &lt;a href="https://huggingface.co/DevQuasar/ai-sage.GigaChat3-702B-A36B-preview-bf16-GGUF/discussions/1"&gt;https://huggingface.co/DevQuasar/ai-sage.GigaChat3-702B-A36B-preview-bf16-GGUF/discussions/1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VoidAlchemy"&gt; /u/VoidAlchemy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/ubergarm/GigaChat3-10B-A1.8B-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2ncrr/ubergarmgigachat310ba18bgguf_11gib_q8_0/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2ncrr/ubergarmgigachat310ba18bgguf_11gib_q8_0/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T02:57:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1p274rk</id>
    <title>Your local LLM agents can be just as good as closed-source models - I open-sourced Stanford's ACE framework that makes agents learn from mistakes</title>
    <updated>2025-11-20T16:09:43+00:00</updated>
    <author>
      <name>/u/cheetguy</name>
      <uri>https://old.reddit.com/user/cheetguy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I implemented Stanford's &lt;a href="https://arxiv.org/abs/2510.04618"&gt;Agentic Context Engineering paper&lt;/a&gt;. The framework makes agents learn from their own execution feedback through in-context learning instead of fine-tuning.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How it works:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Agent runs task ‚Üí reflects on what worked/failed ‚Üí curates strategies into playbook ‚Üí uses playbook on next run&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Improvement:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Paper shows +17.1pp accuracy improvement vs base LLM (‚âà+40% relative improvement) on agent benchmarks (DeepSeek-V3.1 non-thinking mode), helping close the gap with closed-source models. All through in-context learning (no fine-tuning needed).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My Open-Source Implementation:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Drop into existing agents in ~10 lines of code&lt;/li&gt; &lt;li&gt;Works with local or API models&lt;/li&gt; &lt;li&gt;Real-world test on browser automation agent: &lt;ul&gt; &lt;li&gt;30% ‚Üí 100% success rate&lt;/li&gt; &lt;li&gt;82% fewer steps&lt;/li&gt; &lt;li&gt;65% decrease in token cost&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Get started:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GitHub: &lt;a href="https://github.com/kayba-ai/agentic-context-engine"&gt;https://github.com/kayba-ai/agentic-context-engine&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Local Model Starter Templates (Ollama, LM Studio, LiteLLM): &lt;a href="https://github.com/kayba-ai/agentic-context-engine/tree/main/examples"&gt;https://github.com/kayba-ai/agentic-context-engine/tree/main/examples&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Would love to hear if anyone tries this with their local setups! Especially curious how it performs with different models.&lt;/p&gt; &lt;p&gt;I'm currently actively improving this based on feedback - &lt;a href="https://github.com/kayba-ai/agentic-context-engine"&gt;‚≠ê the repo&lt;/a&gt; so you can stay updated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cheetguy"&gt; /u/cheetguy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p274rk/your_local_llm_agents_can_be_just_as_good_as/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p274rk/your_local_llm_agents_can_be_just_as_good_as/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p274rk/your_local_llm_agents_can_be_just_as_good_as/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-20T16:09:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2ivv2</id>
    <title>Faster NeuTTS: can generate over 200 seconds of audio in a single second!</title>
    <updated>2025-11-20T23:35:16+00:00</updated>
    <author>
      <name>/u/SplitNice1982</name>
      <uri>https://old.reddit.com/user/SplitNice1982</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I previously open sourced FastMaya which was also really fast but then set sights on NeuTTS-air. NeuTTS is much smaller and supports better voice cloning as well. So, I heavily optimized it using LMdeploy and some custom batching code for the codec to make it really fast.&lt;/p&gt; &lt;h1&gt;Benefits of this repo&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Much faster, not only for batching but for single batch sizes(1.8x realtime for Maya1 vs 7x realtime for NeuTTS-air)&lt;/li&gt; &lt;li&gt;Works with multiple gpus using tensor parallel for even more speedups. &lt;/li&gt; &lt;li&gt;Great for not only generating audiobooks but voice assistants and much more&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I am working on supporting the multilingual models as well and adding multi speaker synthesis. Also, streaming support and online inference (for serving to many users) should come as well. Initial results are showing **100ms** latency!&lt;/p&gt; &lt;p&gt;I will also add an upsampler to increase audio quality soon. If you have other requests, I will try my best to fulfill them.&lt;/p&gt; &lt;p&gt;Hope this helps people, thanks! Link: &lt;a href="https://github.com/ysharma3501/FastNeuTTS.git"&gt;https://github.com/ysharma3501/FastNeuTTS.git&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SplitNice1982"&gt; /u/SplitNice1982 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2ivv2/faster_neutts_can_generate_over_200_seconds_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2ivv2/faster_neutts_can_generate_over_200_seconds_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2ivv2/faster_neutts_can_generate_over_200_seconds_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-20T23:35:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1p29jwc</id>
    <title>Leak: Qwen3-15B-A2B-Base</title>
    <updated>2025-11-20T17:40:12+00:00</updated>
    <author>
      <name>/u/TroyDoesAI</name>
      <uri>https://old.reddit.com/user/TroyDoesAI</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Unmolested and Unreleased Base Qwen3 MoE:&lt;br /&gt; &lt;a href="https://huggingface.co/TroyDoesAI/Qwen3-15B-A2B-Base"&gt;https://huggingface.co/TroyDoesAI/Qwen3-15B-A2B-Base&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TroyDoesAI"&gt; /u/TroyDoesAI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p29jwc/leak_qwen315ba2bbase/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p29jwc/leak_qwen315ba2bbase/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p29jwc/leak_qwen315ba2bbase/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-20T17:40:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2l36u</id>
    <title>Echo TTS - 44.1kHz, Fast, Fits under 8GB VRAM - SoTA Voice Cloning</title>
    <updated>2025-11-21T01:11:55+00:00</updated>
    <author>
      <name>/u/HelpfulHand3</name>
      <uri>https://old.reddit.com/user/HelpfulHand3</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;New diffusion based multi-speaker capable TTS model released today by the engineer who made Parakeet (the arch that Dia was based on).&lt;br /&gt; &lt;strong&gt;Voice cloning is available on the&lt;/strong&gt; &lt;a href="https://huggingface.co/spaces/jordand/echo-tts-preview"&gt;HF space&lt;/a&gt; but for safety reasons (voice similarity with this model is very high) he has decided for now not to release the speaker encoder. It does come with a large voice bank however.&lt;/p&gt; &lt;p&gt;Supports some tags like (laughs), (coughs), (applause), (singing) etc.&lt;/p&gt; &lt;p&gt;Runs on consumer cards with at least 8GB VRAM.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Echo is a 2.4B DiT that generates Fish Speech S1-DAC latents (and can thus generate 44.1kHz audio; credit to Fish Speech for having trained such a great autoencoder). On an A100, Echo can generate a single 30-second sample of audio in 1.4 seconds (including decoding).&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;License: &lt;strong&gt;CC-BY-NC due to the S1 DAC autoencoder license&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Release Blog Post: &lt;a href="https://jordandarefsky.com/blog/2025/echo/"&gt;https://jordandarefsky.com/blog/2025/echo/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Demo HF Space: &lt;a href="https://huggingface.co/spaces/jordand/echo-tts-preview"&gt;https://huggingface.co/spaces/jordand/echo-tts-preview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Weights: &lt;a href="https://huggingface.co/jordand/echo-tts-no-speaker"&gt;https://huggingface.co/jordand/echo-tts-no-speaker&lt;/a&gt; &lt;a href="https://huggingface.co/jordand/fish-s1-dac-min"&gt;https://huggingface.co/jordand/fish-s1-dac-min&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Code/Github: Coming soon&lt;/p&gt; &lt;p&gt;I haven't had this much fun playing with a TTS since Higgs. This is easily up there with VibeVoice 7b and Higgs Audio v2 despite being 2.4b.&lt;/p&gt; &lt;p&gt;It can clone voices that no other model has been able to do well for me:&lt;/p&gt; &lt;p&gt;&lt;a href="https://vocaroo.com/19PQroylYsoP"&gt;https://vocaroo.com/19PQroylYsoP&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HelpfulHand3"&gt; /u/HelpfulHand3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2l36u/echo_tts_441khz_fast_fits_under_8gb_vram_sota/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2l36u/echo_tts_441khz_fast_fits_under_8gb_vram_sota/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2l36u/echo_tts_441khz_fast_fits_under_8gb_vram_sota/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T01:11:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2padh</id>
    <title>Unsloth just released their Olmo 3 dynamic quants!</title>
    <updated>2025-11-21T04:28:41+00:00</updated>
    <author>
      <name>/u/Aromatic-Distance817</name>
      <uri>https://old.reddit.com/user/Aromatic-Distance817</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2padh/unsloth_just_released_their_olmo_3_dynamic_quants/"&gt; &lt;img alt="Unsloth just released their Olmo 3 dynamic quants!" src="https://external-preview.redd.it/48d9roHWO9vPhqtCoIVGxdhD9jO5DC8s9h8U3EqHoCc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2c1dca215120c6d942685f73783d2b00bbdb86e8" title="Unsloth just released their Olmo 3 dynamic quants!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aromatic-Distance817"&gt; /u/Aromatic-Distance817 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/unsloth/Olmo-3-32B-Think-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2padh/unsloth_just_released_their_olmo_3_dynamic_quants/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2padh/unsloth_just_released_their_olmo_3_dynamic_quants/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T04:28:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1p24aet</id>
    <title>Ai2 just announced Olmo 3, a leading fully open LM suite built for reasoning, chat, &amp; tool use</title>
    <updated>2025-11-20T14:16:57+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p24aet/ai2_just_announced_olmo_3_a_leading_fully_open_lm/"&gt; &lt;img alt="Ai2 just announced Olmo 3, a leading fully open LM suite built for reasoning, chat, &amp;amp; tool use" src="https://b.thumbs.redditmedia.com/xQA2jdAbLxju3aNrYQFjsQ9bmSXaOmQiXK2aPKXj8vw.jpg" title="Ai2 just announced Olmo 3, a leading fully open LM suite built for reasoning, chat, &amp;amp; tool use" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Try Olmo 3 in the Ai2 Playground ‚Üí &lt;a href="https://playground.allenai.org/"&gt;https://playground.allenai.org/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Download: &lt;a href="https://huggingface.co/collections/allenai/olmo-3-68e80f043cc0d3c867e7efc6"&gt;https://huggingface.co/collections/allenai/olmo-3-68e80f043cc0d3c867e7efc6&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog: &lt;a href="https://allenai.org/blog/olmo3"&gt;https://allenai.org/blog/olmo3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Technical report: &lt;a href="https://allenai.org/papers/olmo3"&gt;https://allenai.org/papers/olmo3&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1p24aet"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p24aet/ai2_just_announced_olmo_3_a_leading_fully_open_lm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p24aet/ai2_just_announced_olmo_3_a_leading_fully_open_lm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-20T14:16:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1ozopxx</id>
    <title>AMA Announcement: MiniMax, The Opensource Lab Behind MiniMax-M2 + Gifts to Our Community (Wednesday, 8AM-11AM PST)</title>
    <updated>2025-11-17T18:50:44+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozopxx/ama_announcement_minimax_the_opensource_lab/"&gt; &lt;img alt="AMA Announcement: MiniMax, The Opensource Lab Behind MiniMax-M2 + Gifts to Our Community (Wednesday, 8AM-11AM PST)" src="https://preview.redd.it/e3scgr4j5v1g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ca0bf85cb4cf2a2b7e12e8d99d515186275c15e3" title="AMA Announcement: MiniMax, The Opensource Lab Behind MiniMax-M2 + Gifts to Our Community (Wednesday, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/e3scgr4j5v1g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozopxx/ama_announcement_minimax_the_opensource_lab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ozopxx/ama_announcement_minimax_the_opensource_lab/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-17T18:50:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1b550</id>
    <title>AMA with MiniMax ‚Äî Ask Us Anything!</title>
    <updated>2025-11-19T15:46:38+00:00</updated>
    <author>
      <name>/u/OccasionNo6699</name>
      <uri>https://old.reddit.com/user/OccasionNo6699</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;! We‚Äôre really excited to be here, thanks for having us.&lt;/p&gt; &lt;p&gt;I‚Äôm Skyler (&lt;a href="https://www.reddit.com/user/OccasionNo6699/"&gt;u/OccasionNo6699&lt;/a&gt;), head of engineering at MiniMax, the lab behind:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://x.com/MiniMax__AI/status/1982674798649160175?s=20"&gt;MiniMax-M2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983382728343994414"&gt;Hailuo 2.3&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983661667872600296"&gt;MiniMax Speech 2.6&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983964920493568296"&gt;MiniMax Music 2.0&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Joining me today are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Pengyu Zhao, &lt;a href="https://www.reddit.com/user/Wise_Evidence9973/"&gt;u/Wise_Evidence9973&lt;/a&gt; ‚Äî Head of LLM Research&lt;/li&gt; &lt;li&gt;Jade Cai, &lt;a href="https://www.reddit.com/user/srtng/"&gt;u/srtng&lt;/a&gt; ‚Äî Head of Developer Community&lt;/li&gt; &lt;li&gt;midnight_compile , &lt;a href="/u/Top_Cattle_2098"&gt;u/Top_Cattle_2098&lt;/a&gt; ‚Äî LLM Researcher&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 8AM-11AM PST with our core MiniMax tech team continuing to follow up on questions over the next 48 hours.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OccasionNo6699"&gt; /u/OccasionNo6699 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T15:46:38+00:00</published>
  </entry>
</feed>
