<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-05T23:35:12+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1opicm8</id>
    <title>Local-only FOSS ops tool ‚Äî no cloud, no Docker, no browser. Thoughts?</title>
    <updated>2025-11-05T23:05:28+00:00</updated>
    <author>
      <name>/u/TrueGoodCraft</name>
      <uri>https://old.reddit.com/user/TrueGoodCraft</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm building a small ops dashboard for makers/shops ‚Äî inventory, vendors, backups, local LLM intagration, reports,etc. &lt;/p&gt; &lt;p&gt;The twist: &lt;/p&gt; &lt;p&gt;- Runs 100% offline on Windows (like a normal program) &lt;/p&gt; &lt;p&gt;- No Docker, no browser, no hosting &lt;/p&gt; &lt;p&gt;- 1.35 MB, fits on a USB stick &lt;/p&gt; &lt;p&gt;- Free core forever (open source) &lt;/p&gt; &lt;p&gt;- $5/mo Pro tier for batch/automation &lt;/p&gt; &lt;p&gt;No telemetry. No cloud. No Electron bloat. &lt;/p&gt; &lt;p&gt;I‚Äôve seen a lot of ‚ÄúFOSS‚Äù tools that still require self-hosting or a subscription to be usable. &lt;/p&gt; &lt;p&gt;This one just‚Ä¶ runs. &lt;/p&gt; &lt;p&gt;**Question:** &lt;/p&gt; &lt;p&gt;Would you actually use something like this? &lt;/p&gt; &lt;p&gt;What‚Äôs missing? &lt;/p&gt; &lt;p&gt;What would make it *not* worth trying? &lt;/p&gt; &lt;p&gt;Just want honest feedback ‚Äî no links, no signups, no pitch. &lt;/p&gt; &lt;p&gt;Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TrueGoodCraft"&gt; /u/TrueGoodCraft &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opicm8/localonly_foss_ops_tool_no_cloud_no_docker_no/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opicm8/localonly_foss_ops_tool_no_cloud_no_docker_no/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1opicm8/localonly_foss_ops_tool_no_cloud_no_docker_no/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-05T23:05:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1opalon</id>
    <title>Simple Chat UI for users</title>
    <updated>2025-11-05T18:17:50+00:00</updated>
    <author>
      <name>/u/TaiMaiShu-71</name>
      <uri>https://old.reddit.com/user/TaiMaiShu-71</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a need to deploy a small lightweight chat interface on a specific subject. I don't need openwebui or anything big. I don't need chat history. I do need a simple light weight, local, no auth, multi turn chat interface though, extra points if it supports mcp servers. It will connect to local models running on the local network (vLLM). Anyone aware of any good open source options?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TaiMaiShu-71"&gt; /u/TaiMaiShu-71 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opalon/simple_chat_ui_for_users/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opalon/simple_chat_ui_for_users/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1opalon/simple_chat_ui_for_users/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-05T18:17:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1op2i14</id>
    <title>Best model to run on dual 3090 (48GB vram)</title>
    <updated>2025-11-05T13:13:42+00:00</updated>
    <author>
      <name>/u/ChopSticksPlease</name>
      <uri>https://old.reddit.com/user/ChopSticksPlease</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What would be your model of choice if you had a 48GB VRAM setup on your desk? In my case it's dual 3090. &lt;/p&gt; &lt;p&gt;For coding I'm leaning towards qwen3-coder:30b-a3b-q8_0 after using qwen2.5-coder:32b-instruct-q8_0&lt;/p&gt; &lt;p&gt;For general chat mostly about work/software/cloud related topics can't decicde between qwq:32b-q8_0 and qwen2.5:72b-instruct-q4_0, i guess more parameters are better but output from qwq is often quite good &lt;/p&gt; &lt;p&gt;Any opinions? Are there other models that can outperform qwen locally?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ChopSticksPlease"&gt; /u/ChopSticksPlease &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1op2i14/best_model_to_run_on_dual_3090_48gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1op2i14/best_model_to_run_on_dual_3090_48gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1op2i14/best_model_to_run_on_dual_3090_48gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-05T13:13:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ooymcm</id>
    <title>Hephaestus: AI workflows that discover and create their own tasks as they work</title>
    <updated>2025-11-05T09:43:16+00:00</updated>
    <author>
      <name>/u/Standard_Excuse7988</name>
      <uri>https://old.reddit.com/user/Standard_Excuse7988</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ooymcm/hephaestus_ai_workflows_that_discover_and_create/"&gt; &lt;img alt="Hephaestus: AI workflows that discover and create their own tasks as they work" src="https://external-preview.redd.it/Y3kwMnpjZGtyZXpmMdAkQHndWWJHnvJDGx1uxfxLVUHIG9vtIHStiltBck3-.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b5ca7c1dc4a5849323fd49664a100572b45c26c5" title="Hephaestus: AI workflows that discover and create their own tasks as they work" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! üëã&lt;/p&gt; &lt;p&gt;A week ago I shared Hephaestus - an open-source framework where AI agents dynamically build workflows based on what they discover. The response has been incredible (500+ stars already!)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Core Idea:&lt;/strong&gt; Instead of predefining every task upfront, you define &lt;em&gt;phase types&lt;/em&gt; (like &amp;quot;Analyze ‚Üí Implement ‚Üí Test&amp;quot;), and agents create specific tasks across these phases based on what they discover as they work.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Real Example:&lt;/strong&gt; Give it a PRD for &amp;quot;Build a REST API with authentication.&amp;quot; A Phase 1 agent analyzes it and spawns 5 implementation tasks (auth system, database, API layer, tests, deployment). A Phase 3 validation agent testing the auth system discovers an elegant caching pattern that could speed up all API routes by 40%. Instead of being stuck or following rigid branching logic, it spawns a Phase 1 investigation task. Another agent picks it up, confirms it's viable, spawns a Phase 2 implementation task. The workflow just branched itself based on discovery.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What makes it different:&lt;/strong&gt; - üîÑ &lt;strong&gt;Self-building workflows&lt;/strong&gt; - Agents spawn tasks dynamically, not predefined branches - üß† &lt;strong&gt;RAG-powered coordination&lt;/strong&gt; - Agents share discoveries through semantic memory - üéØ &lt;strong&gt;Guardian monitoring&lt;/strong&gt; - Continuously tracks agent trajectories to prevent drift - üìä &lt;strong&gt;Kanban coordination&lt;/strong&gt; - Real-time task management with blocking relationships - And so much more...&lt;/p&gt; &lt;p&gt;üîó &lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/Ido-Levi/Hephaestus"&gt;https://github.com/Ido-Levi/Hephaestus&lt;/a&gt; üìö &lt;strong&gt;Docs:&lt;/strong&gt; &lt;a href="https://ido-levi.github.io/Hephaestus/"&gt;https://ido-levi.github.io/Hephaestus/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Fair warning: This is still new and rough around the edges. Issues and feedback are very welcome, and I'm happy to review contributions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Standard_Excuse7988"&gt; /u/Standard_Excuse7988 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/fdwlhddkrezf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ooymcm/hephaestus_ai_workflows_that_discover_and_create/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ooymcm/hephaestus_ai_workflows_that_discover_and_create/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-05T09:43:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1opc1na</id>
    <title>support for openPangu-Embedded in llama.cpp</title>
    <updated>2025-11-05T19:09:34+00:00</updated>
    <author>
      <name>/u/noctrex</name>
      <uri>https://old.reddit.com/user/noctrex</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With the latest llama.cpp, the support for openPangu-Embedded has been added.&lt;/p&gt; &lt;p&gt;Here are some GGUF's:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/noctrex/openPangu-Embedded-1B-V1.1-GGUF"&gt;noctrex/openPangu-Embedded-1B-V1.1-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/noctrex/openPangu-Embedded-7B-V1.1-GGUF"&gt;noctrex/openPangu-Embedded-7B-V1.1-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/noctrex/openPangu-Embedded-7B-DeepDiver-GGUF"&gt;noctrex/openPangu-Embedded-7B-DeepDiver-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/noctrex"&gt; /u/noctrex &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opc1na/support_for_openpanguembedded_in_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opc1na/support_for_openpanguembedded_in_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1opc1na/support_for_openpanguembedded_in_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-05T19:09:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1op3g6z</id>
    <title>Kimi Thinking When?</title>
    <updated>2025-11-05T13:52:55+00:00</updated>
    <author>
      <name>/u/InternationalAsk1490</name>
      <uri>https://old.reddit.com/user/InternationalAsk1490</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1op3g6z/kimi_thinking_when/"&gt; &lt;img alt="Kimi Thinking When?" src="https://b.thumbs.redditmedia.com/MndVbBgYMkDWeRdjxPeCx0iqNEf3LEN1ZZjUFdJ-Z2k.jpg" title="Kimi Thinking When?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/vooceqxl1gzf1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c26cd866dae399e81ce33c794e17c2f9f1c5df04"&gt;https://preview.redd.it/vooceqxl1gzf1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c26cd866dae399e81ce33c794e17c2f9f1c5df04&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I really like Kimi K2. It‚Äôs way more emotionally intelligent than any other AI I‚Äôve tried. like, it never flatters me or sugarcoats things. If I mess up, it‚Äôll directly tell me that actually helps me improve. That kind of trust is rare.&lt;/p&gt; &lt;p&gt;I‚Äôm just sitting here wondering‚Ä¶ Kimi thinking when?&lt;/p&gt; &lt;p&gt;btw, if fix the hallucination issues, I swear this thing will be unstoppable&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InternationalAsk1490"&gt; /u/InternationalAsk1490 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1op3g6z/kimi_thinking_when/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1op3g6z/kimi_thinking_when/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1op3g6z/kimi_thinking_when/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-05T13:52:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1oomxt6</id>
    <title>Tencent + Tsinghua just dropped a paper called Continuous Autoregressive Language Models (CALM)</title>
    <updated>2025-11-04T23:26:53+00:00</updated>
    <author>
      <name>/u/vladlearns</name>
      <uri>https://old.reddit.com/user/vladlearns</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oomxt6/tencent_tsinghua_just_dropped_a_paper_called/"&gt; &lt;img alt="Tencent + Tsinghua just dropped a paper called Continuous Autoregressive Language Models (CALM)" src="https://preview.redd.it/tu7jitwzqbzf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=952d82b0d88e6a4bfa60cec0ff55072522800b4c" title="Tencent + Tsinghua just dropped a paper called Continuous Autoregressive Language Models (CALM)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;STAY CALM! &lt;a href="https://arxiv.org/abs/2510.27688"&gt;https://arxiv.org/abs/2510.27688&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vladlearns"&gt; /u/vladlearns &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/tu7jitwzqbzf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oomxt6/tencent_tsinghua_just_dropped_a_paper_called/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oomxt6/tencent_tsinghua_just_dropped_a_paper_called/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T23:26:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1oogvcw</id>
    <title>I implemented GPT-OSS from scratch in pure Python, without PyTorch or a GPU</title>
    <updated>2025-11-04T19:33:07+00:00</updated>
    <author>
      <name>/u/ultimate_code</name>
      <uri>https://old.reddit.com/user/ultimate_code</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have also written a detailed and beginner friendly blog that explains every single concept, from simple modules such as Softmax and RMSNorm, to more advanced ones like Grouped Query Attention. I tried to justify the architectural decision behind every layer as well. &lt;/p&gt; &lt;p&gt;Key concepts: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Grouped Query Attention: with attention sinks and sliding window.&lt;/li&gt; &lt;li&gt;Mixture of Experts (MoE).&lt;/li&gt; &lt;li&gt;Rotary Position Embeddings (RoPE): with NTK-aware scaling.&lt;/li&gt; &lt;li&gt;Functional Modules: SwiGLU, RMSNorm, Softmax, Linear Layer.&lt;/li&gt; &lt;li&gt;Custom BFloat16 implementation in C++ for numerical precision.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you‚Äôve ever wanted to understand how modern LLMs really work, this repo + blog walk you through everything. I have also made sure that the implementation matches the official one in terms of numerical precision (check the &lt;a href="http://test.py"&gt;test.py&lt;/a&gt; file)&lt;/p&gt; &lt;p&gt;Blog: &lt;a href="https://projektjoe.com/blog/gptoss"&gt;https://projektjoe.com/blog/gptoss&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/projektjoe/gpt-oss"&gt;https://github.com/projektjoe/gpt-oss&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love any feedback, ideas for extensions, or just thoughts from others exploring transformers from first principles!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ultimate_code"&gt; /u/ultimate_code &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oogvcw/i_implemented_gptoss_from_scratch_in_pure_python/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oogvcw/i_implemented_gptoss_from_scratch_in_pure_python/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oogvcw/i_implemented_gptoss_from_scratch_in_pure_python/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T19:33:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1oomyby</id>
    <title>Server DRAM prices surge up to 50% as AI-induced memory shortage hits hyperscaler supply ‚Äî U.S. and Chinese customers only getting 70% order fulfillment</title>
    <updated>2025-11-04T23:27:29+00:00</updated>
    <author>
      <name>/u/IonizedRay</name>
      <uri>https://old.reddit.com/user/IonizedRay</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oomyby/server_dram_prices_surge_up_to_50_as_aiinduced/"&gt; &lt;img alt="Server DRAM prices surge up to 50% as AI-induced memory shortage hits hyperscaler supply ‚Äî U.S. and Chinese customers only getting 70% order fulfillment" src="https://external-preview.redd.it/ZoS_79jqnoHBV3LsnI-z672RSVTI_TXKjzXarTZWduA.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7c34b5423e4b02583d66bdf088e63a71b2cb2167" title="Server DRAM prices surge up to 50% as AI-induced memory shortage hits hyperscaler supply ‚Äî U.S. and Chinese customers only getting 70% order fulfillment" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IonizedRay"&gt; /u/IonizedRay &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.tomshardware.com/pc-components/storage/server-dram-prices-surge-50-percent"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oomyby/server_dram_prices_surge_up_to_50_as_aiinduced/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oomyby/server_dram_prices_surge_up_to_50_as_aiinduced/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T23:27:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1op7kmw</id>
    <title>What are some approaches taken for the problem of memory in LLMs?</title>
    <updated>2025-11-05T16:30:44+00:00</updated>
    <author>
      <name>/u/SrijSriv211</name>
      <uri>https://old.reddit.com/user/SrijSriv211</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Long-term memory is currently one of the most important problems in LLMs.&lt;/p&gt; &lt;p&gt;What are some approaches taken by you or researchers to solve this problem?&lt;/p&gt; &lt;p&gt;For eg, using RAG, using summaries of context, making changes to the model architecture itself to store the memory in form of weights or cache. I very curious.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SrijSriv211"&gt; /u/SrijSriv211 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1op7kmw/what_are_some_approaches_taken_for_the_problem_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1op7kmw/what_are_some_approaches_taken_for_the_problem_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1op7kmw/what_are_some_approaches_taken_for_the_problem_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-05T16:30:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1ooa342</id>
    <title>llama.cpp releases new official WebUI</title>
    <updated>2025-11-04T15:26:30+00:00</updated>
    <author>
      <name>/u/paf1138</name>
      <uri>https://old.reddit.com/user/paf1138</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ooa342/llamacpp_releases_new_official_webui/"&gt; &lt;img alt="llama.cpp releases new official WebUI" src="https://external-preview.redd.it/3mPqb7hXnKE3QMeOYvmnNH3HEJEfsY-FkGb0pZ8tDhU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=23cd274a23b9ffd23182c3f9522388baf8354b97" title="llama.cpp releases new official WebUI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paf1138"&gt; /u/paf1138 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/discussions/16938"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ooa342/llamacpp_releases_new_official_webui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ooa342/llamacpp_releases_new_official_webui/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T15:26:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1op0yep</id>
    <title>Build a DeepSeek Model from Scratch: A Book</title>
    <updated>2025-11-05T12:01:28+00:00</updated>
    <author>
      <name>/u/OtherRaisin3426</name>
      <uri>https://old.reddit.com/user/OtherRaisin3426</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1op0yep/build_a_deepseek_model_from_scratch_a_book/"&gt; &lt;img alt="Build a DeepSeek Model from Scratch: A Book" src="https://b.thumbs.redditmedia.com/QDV-FHGI3pJaqfrG5j7eQL0VW9Xx6TSUjjj1pRQi0Ac.jpg" title="Build a DeepSeek Model from Scratch: A Book" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/13tb69tzhfzf1.jpg?width=2213&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=8efaf667406076744b34c26b91e755e81e2d8683"&gt;https://preview.redd.it/13tb69tzhfzf1.jpg?width=2213&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=8efaf667406076744b34c26b91e755e81e2d8683&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is the first book which teaches everyone how to build your own DeepSeek model completely from scratch, on your local computer!&lt;/p&gt; &lt;p&gt;The idea for this book grew out of our YouTube series ‚ÄúVizuara‚Äôs Build DeepSeek from Scratch‚Äù which launched in February 2025. The series showed a clear demand for hands-on, first-principles material, encouraging us to create this more structured and detailed written guide.&lt;/p&gt; &lt;p&gt;We have worked super hard for 8 months on this project. &lt;/p&gt; &lt;p&gt;The book is structured around a four-stage roadmap, covering the innovations in a logical order:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;The foundational Key-Value (KV) Cache for efficient inference.&lt;/li&gt; &lt;li&gt;The core architectural components: Multi-Head Latent Attention (MLA) and Deepseek&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Mixture-of-Experts (MoE).&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Advanced training techniques, including Multi-Token Prediction (MTP) and FP8 quantization.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Post-training methods like Reinforcement Learning (RL) and Knowledge Distillation.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OtherRaisin3426"&gt; /u/OtherRaisin3426 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1op0yep/build_a_deepseek_model_from_scratch_a_book/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1op0yep/build_a_deepseek_model_from_scratch_a_book/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1op0yep/build_a_deepseek_model_from_scratch_a_book/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-05T12:01:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1oojwpj</id>
    <title>The French Government Launches an LLM Leaderboard Comparable to LMarena, Emphasizing European Languages and Energy Efficiency</title>
    <updated>2025-11-04T21:30:05+00:00</updated>
    <author>
      <name>/u/Imakerocketengine</name>
      <uri>https://old.reddit.com/user/Imakerocketengine</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oojwpj/the_french_government_launches_an_llm_leaderboard/"&gt; &lt;img alt="The French Government Launches an LLM Leaderboard Comparable to LMarena, Emphasizing European Languages and Energy Efficiency" src="https://b.thumbs.redditmedia.com/z3kkwZtyFH5pRyg7rq5O0EZiOQNj_2JQdPBRk1aeUeQ.jpg" title="The French Government Launches an LLM Leaderboard Comparable to LMarena, Emphasizing European Languages and Energy Efficiency" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://comparia.beta.gouv.fr/"&gt;https://comparia.beta.gouv.fr/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Imakerocketengine"&gt; /u/Imakerocketengine &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1oojwpj"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oojwpj/the_french_government_launches_an_llm_leaderboard/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oojwpj/the_french_government_launches_an_llm_leaderboard/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T21:30:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1ophwr6</id>
    <title>importance of prompt engineering</title>
    <updated>2025-11-05T22:47:59+00:00</updated>
    <author>
      <name>/u/leo-k7v</name>
      <uri>https://old.reddit.com/user/leo-k7v</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Douglas Adams on importance of prompt engineering:&lt;/p&gt; &lt;p&gt;Arthur threw away a sixth cup of the liquid. ‚ÄúListen, you machine,‚Äù he said, ‚Äúyou claim you can synthesize any drink in existence, so why do you keep giving me the same undrinkable stuff?‚Äù ‚ÄúNutrition and pleasurable sense data,‚Äù burbled the machine. ‚ÄúShare and Enjoy.‚Äù ‚ÄúIt tastes filthy!‚Äù ‚ÄúIf you have enjoyed the experience of this drink,‚Äù continued the machine, ‚Äúwhy not share it with your friends?‚Äù ‚ÄúBecause,‚Äù said Arthur tartly, ‚ÄúI want to keep them. Will you try to comprehendwhat I'm telling you? That drink ...‚Äù ‚ÄúThat drink,‚Äù said the machine sweetly, ‚Äúwas individually tailored to meet your personal requirements for nutrition and pleasure. ‚Äù ‚ÄúAh,‚Äù said Arthur, ‚Äúso I'm a masochist on diet am I?‚Äù ‚ÄúShare and Enjoy.‚Äù ‚ÄúOh shut up.‚Äù ‚ÄúWill that be all?‚Äù Arthur decided to give up. ‚ÄúYes,‚Äù he said. Then he decided he'd be dammed if he'd give up. ‚ÄúNo,‚Äù he said, ‚Äúlook, it's very, very simple ... all I want ... is a cup of tea. You are going to make one for me. Keep quiet and listen.‚Äù And he sat. He told the Nutri-Matic about India, he told it about China, he told it about Ceylon. He told it about broad leaves drying in the sun. He told it about silver teapots. He told it about summer afternoons on the lawn. He told it about putting in the milk before the tea so it wouldn't get scalded. He even told it (briefly) about the history of the East India Company. ‚ÄúSo that's it, is it?‚Äù said the Nutri-Matic when he had finished. ‚ÄúYes,‚Äù said Arthur, ‚Äúthat is what I want.‚Äù ‚ÄúYou want the taste of dried leaves boiled in water?‚Äù ‚ÄúEr, yes. With milk.‚Äù ‚ÄúSquirted out of a cow?‚Äù ‚ÄúWell, in a manner of speaking I suppose ...‚Äù&lt;/p&gt; &lt;p&gt;..... &amp;lt;some severe side effects of the prompt and finally&amp;gt;&lt;/p&gt; &lt;p&gt;On the delivery plate of the Nutri-Matic Drink Synthesizer was a small tray, on which sat three bone china cups and saucers, a bone china jug of milk, a silver teapot full of the best tea Arthur had ever tasted, ...&lt;/p&gt; &lt;p&gt;PS: I‚Äôve tried several LLMs and SLMs to create catchy video of this quote and failed miserably‚Ä¶ any suggestions how to do it would be appreciated - just because I feel I need some fun this week‚Ä¶&lt;/p&gt; &lt;p&gt;PPS: need some fun this week trying to fix self_extend() and context shift() in llama.cpp for hybrid memory models (and failing)‚Ä¶&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/leo-k7v"&gt; /u/leo-k7v &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ophwr6/importance_of_prompt_engineering/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ophwr6/importance_of_prompt_engineering/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ophwr6/importance_of_prompt_engineering/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-05T22:47:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1opi34d</id>
    <title>Fine-tuning a chat model to mimic one person</title>
    <updated>2025-11-05T22:54:59+00:00</updated>
    <author>
      <name>/u/KnightKingPow</name>
      <uri>https://old.reddit.com/user/KnightKingPow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all, beginner here with some experience running StableDiffusion/WAN models in ComfyUI and LM Studio. I would really appreciate some guidance.&lt;/p&gt; &lt;p&gt;I have several text chat conversations between two people (sometimes three). I would like to fine-tune a model so it learns the writing style, tone, and personality of &lt;strong&gt;only one&lt;/strong&gt; of the participants, so that I can later chat with the model &lt;em&gt;as if I‚Äôm talking to that person&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;The model should ignore or not learn from the other speaker(s).&lt;/p&gt; &lt;p&gt;The language is not English, but I suppose that's not a problem, right?&lt;/p&gt; &lt;p&gt;I have these:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;MacBook M3 Max, 64 GB RAM&lt;/li&gt; &lt;li&gt;Windows PC with an RTX 4090 (24 GB VRAM) &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I could train on both but ideally I'd like to run the final model locally on the Mac with LM Studio.&lt;/p&gt; &lt;p&gt;What base model would be best for this setup and use case?&lt;/p&gt; &lt;p&gt;What are the full beginner-friendly steps from dataset prep ‚Üí fine-tuning ‚Üí exporting/quantizing?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KnightKingPow"&gt; /u/KnightKingPow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opi34d/finetuning_a_chat_model_to_mimic_one_person/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opi34d/finetuning_a_chat_model_to_mimic_one_person/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1opi34d/finetuning_a_chat_model_to_mimic_one_person/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-05T22:54:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1op2d1a</id>
    <title>I made a complete tutorial on fine-tuning Qwen2.5 (1.5B) on a free Colab T4 GPU. Accuracy boosted from 91% to 98% in ~20 mins!</title>
    <updated>2025-11-05T13:07:46+00:00</updated>
    <author>
      <name>/u/Awkward_Run_9982</name>
      <uri>https://old.reddit.com/user/Awkward_Run_9982</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1op2d1a/i_made_a_complete_tutorial_on_finetuning_qwen25/"&gt; &lt;img alt="I made a complete tutorial on fine-tuning Qwen2.5 (1.5B) on a free Colab T4 GPU. Accuracy boosted from 91% to 98% in ~20 mins!" src="https://preview.redd.it/7xx856mftfzf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a7a93e1d29328d3af61a2f1b635a73c9abf0f570" title="I made a complete tutorial on fine-tuning Qwen2.5 (1.5B) on a free Colab T4 GPU. Accuracy boosted from 91% to 98% in ~20 mins!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;I wanted to share a project I've been working on: a full, beginner-friendly tutorial for fine-tuning the &lt;strong&gt;Qwen2.5-Coder-1.5B&lt;/strong&gt; model for a real-world task (Chinese sentiment analysis).&lt;/p&gt; &lt;p&gt;The best part? &lt;strong&gt;You can run the entire thing on a free Google Colab T4 GPU in about 20-30 minutes.&lt;/strong&gt; No local setup needed!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub Repo:&lt;/strong&gt; &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fgithub.com%2FIIIIQIIII%2FMSJ-Factory"&gt;https://github.com/IIIIQIIII/MSJ-Factory&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;‚ñ∂Ô∏è Try it now on Google Colab:&lt;/strong&gt; &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fcolab.research.google.com%2Fgithub%2FIIIIQIIII%2FMSJ-Factory%2Fblob%2Fmain%2FQwen2_5_Sentiment_Fine_tuning_Tutorial.ipynb"&gt;https://colab.research.google.com/github/IIIIQIIII/MSJ-Factory/blob/main/Qwen2_5_Sentiment_Fine_tuning_Tutorial.ipynb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What's inside:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;One-Click Colab Notebook:&lt;/strong&gt; The link above takes you straight there. Just open and run.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Freeze Training Method:&lt;/strong&gt; I only train the last 6 layers. It's super fast, uses ~9GB VRAM, and still gives amazing results.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Clear Results:&lt;/strong&gt; I was able to boost accuracy on the test set from &lt;strong&gt;91.6% to 97.8%&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Full Walkthrough:&lt;/strong&gt; From cloning the repo, to training, evaluating, and even uploading your final model to Hugging Face, all within the notebook.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I tried to make this as easy as possible for anyone who wants to get their hands dirty with fine-tuning but might not have a beefy GPU at home. This method is great for my own quick experiments and for adapting models to new domains without needing an A100.&lt;/p&gt; &lt;p&gt;Hope you find it useful! Let me know if you have any feedback or questions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Awkward_Run_9982"&gt; /u/Awkward_Run_9982 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7xx856mftfzf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1op2d1a/i_made_a_complete_tutorial_on_finetuning_qwen25/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1op2d1a/i_made_a_complete_tutorial_on_finetuning_qwen25/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-05T13:07:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1op73qb</id>
    <title>GLM-4.5V model for local computer use</title>
    <updated>2025-11-05T16:13:48+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1op73qb/glm45v_model_for_local_computer_use/"&gt; &lt;img alt="GLM-4.5V model for local computer use" src="https://external-preview.redd.it/YzA4MDdtb3NxZ3pmMbWADQNBSkImjVESNjfi_q43l9ostHKNGAFX_QJdfnS0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6be507f0bbf2370a00306c48b3deca6f3dbc2931" title="GLM-4.5V model for local computer use" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;On OSWorld-V, it scores 35.8% - beating UI-TARS-1.5, matching Claude-3.7-Sonnet-20250219, and setting SOTA for fully open-source computer-use models.&lt;/p&gt; &lt;p&gt;Run it with Cua either: Locally via Hugging Face Remotely via OpenRouter&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/trycua"&gt;https://github.com/trycua&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Docs + examples: &lt;a href="https://docs.trycua.com/docs/agent-sdk/supported-agents/computer-use-agents#glm-45v"&gt;https://docs.trycua.com/docs/agent-sdk/supported-agents/computer-use-agents#glm-45v&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/p5a328wsqgzf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1op73qb/glm45v_model_for_local_computer_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1op73qb/glm45v_model_for_local_computer_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-05T16:13:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1oozb8v</id>
    <title>aquif-3.5-Max-42B-A3B</title>
    <updated>2025-11-05T10:27:09+00:00</updated>
    <author>
      <name>/u/CoruNethronX</name>
      <uri>https://old.reddit.com/user/CoruNethronX</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oozb8v/aquif35max42ba3b/"&gt; &lt;img alt="aquif-3.5-Max-42B-A3B" src="https://external-preview.redd.it/iQbBlhyprDj6yO7lC6_VBVFWSoqEqFZE8HYk4JH2vHI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bf206a8e60f2f9eb24b0998337979fd6ea8d653c" title="aquif-3.5-Max-42B-A3B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Beats GLM 4.6 according to provided benchmarks Million context Apache 2.0 Works both with GGUF/llama.cpp and MLX/lmstudio out-of-box, as it's qwen3_moe architecture&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CoruNethronX"&gt; /u/CoruNethronX &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/aquif-ai/aquif-3.5-Max-42B-A3B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oozb8v/aquif35max42ba3b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oozb8v/aquif35max42ba3b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-05T10:27:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1opabzi</id>
    <title>Instead of predicting one token at a time, CALM (Continuous Autoregressive Language Models) predicts continuous vectors that represent multiple tokens at once</title>
    <updated>2025-11-05T18:08:24+00:00</updated>
    <author>
      <name>/u/Own-Potential-2308</name>
      <uri>https://old.reddit.com/user/Own-Potential-2308</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Continuous Autoregressive Language Models (CALM) replace the traditional token-by-token generation of language models with a continuous next-vector prediction approach, where an autoencoder compresses chunks of multiple tokens into single continuous vectors that can be reconstructed with over 99.9% accuracy. This drastically reduces the number of generative steps and thus the computational cost. Because probabilities over continuous spaces can‚Äôt be computed via softmax, CALM introduces a likelihood-free framework for training, evaluation (using the new BrierLM metric), and temperature-based sampling. The result is a paradigm that significantly improves efficiency‚Äîachieving comparable performance to strong discrete LLMs while operating far faster‚Äîestablishing next-vector prediction as a powerful new direction for scalable, ultra-efficient language modeling.&lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/abs/2510.27688"&gt;https://arxiv.org/abs/2510.27688&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Own-Potential-2308"&gt; /u/Own-Potential-2308 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opabzi/instead_of_predicting_one_token_at_a_time_calm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opabzi/instead_of_predicting_one_token_at_a_time_calm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1opabzi/instead_of_predicting_one_token_at_a_time_calm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-05T18:08:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1oosnaq</id>
    <title>New Qwen models are unbearable</title>
    <updated>2025-11-05T03:45:53+00:00</updated>
    <author>
      <name>/u/kevin_1994</name>
      <uri>https://old.reddit.com/user/kevin_1994</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been using GPT-OSS-120B for the last couple months and recently thought I'd try Qwen3 32b VL and Qwen3 Next 80B. &lt;/p&gt; &lt;p&gt;They honestly might be worse than peak ChatGPT 4o. &lt;/p&gt; &lt;p&gt;Calling me a genius, telling me every idea of mine is brilliant, &amp;quot;this isnt just a great idea‚Äîyou're redefining what it means to be a software developer&amp;quot; type shit&lt;/p&gt; &lt;p&gt;I cant use these models because I cant trust them at all. They just agree with literally everything I say. &lt;/p&gt; &lt;p&gt;Has anyone found a way to make these models more usable? They have good benchmark scores so perhaps im not using them correctly&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kevin_1994"&gt; /u/kevin_1994 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oosnaq/new_qwen_models_are_unbearable/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oosnaq/new_qwen_models_are_unbearable/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oosnaq/new_qwen_models_are_unbearable/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-05T03:45:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1op0j6j</id>
    <title>Recent VRAM Poll results</title>
    <updated>2025-11-05T11:38:38+00:00</updated>
    <author>
      <name>/u/pmttyji</name>
      <uri>https://old.reddit.com/user/pmttyji</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1op0j6j/recent_vram_poll_results/"&gt; &lt;img alt="Recent VRAM Poll results" src="https://preview.redd.it/i3y27zfpbfzf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0823c607489b2850e90a3ca955b51e1f4428ecdf" title="Recent VRAM Poll results" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1olildc/comment/nmi8ftm/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button"&gt;As mentioned in that post&lt;/a&gt;, That poll missed below ranges.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;9-11GB&lt;/li&gt; &lt;li&gt;25-31GB&lt;/li&gt; &lt;li&gt;97-127GB&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Poll Results below:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;0-8GB - &lt;strong&gt;718&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;12-24GB - &lt;strong&gt;1.1K&lt;/strong&gt; - I think some 10GB folks might have picked this option so this range came with big number.&lt;/li&gt; &lt;li&gt;32-48GB - &lt;strong&gt;348&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;48-96GB - &lt;strong&gt;284&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;128-256GB - &lt;strong&gt;138&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;256+ - &lt;strong&gt;93&lt;/strong&gt; - &lt;sup&gt;Last month someone asked me &amp;quot;Why are you calling yourself GPU Poor when you have 8GB VRAM&amp;quot;&lt;/sup&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Next time onwards below ranges would be better to get better results as it covers all ranges. And this would be more useful for Model creators &amp;amp; Finetuners to pick better model sizes/types(MOE or Dense).&lt;/p&gt; &lt;p&gt;&lt;sup&gt;FYI Poll has only 6 options, otherwise I would add more ranges.&lt;/sup&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;VRAM&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;~12GB&lt;/li&gt; &lt;li&gt;13-32GB&lt;/li&gt; &lt;li&gt;33-64GB&lt;/li&gt; &lt;li&gt;65-96GB&lt;/li&gt; &lt;li&gt;97-128GB&lt;/li&gt; &lt;li&gt;128GB+&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;RAM&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;~32GB&lt;/li&gt; &lt;li&gt;33-64GB&lt;/li&gt; &lt;li&gt;65-128GB&lt;/li&gt; &lt;li&gt;129-256GB&lt;/li&gt; &lt;li&gt;257-512GB&lt;/li&gt; &lt;li&gt;513-1TB&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Somebody please post above poll threads coming week. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmttyji"&gt; /u/pmttyji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/i3y27zfpbfzf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1op0j6j/recent_vram_poll_results/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1op0j6j/recent_vram_poll_results/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-05T11:38:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1ooxple</id>
    <title>GLM 4.6 AIR is coming....?</title>
    <updated>2025-11-05T08:43:40+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ooxple/glm_46_air_is_coming/"&gt; &lt;img alt="GLM 4.6 AIR is coming....?" src="https://preview.redd.it/56uu0u1fiezf1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4748db1df48b7ce94e935ab40a291000e001166f" title="GLM 4.6 AIR is coming....?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;or not yet? What do you think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/56uu0u1fiezf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ooxple/glm_46_air_is_coming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ooxple/glm_46_air_is_coming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-05T08:43:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1oph7jd</id>
    <title>Unified memory is the future, not GPU for local A.I.</title>
    <updated>2025-11-05T22:20:52+00:00</updated>
    <author>
      <name>/u/Terminator857</name>
      <uri>https://old.reddit.com/user/Terminator857</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As model sizes are trending bigger, even the best open weight models hover around half a terabyte, we are not going to be able to run those on GPU, yes on unified memory. Gemini-3 is rumored to be 1.2 terabytes:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reuters.com/business/apple-use-googles-ai-model-run-new-siri-bloomberg-news-reports-2025-11-05/"&gt;https://www.reuters.com/business/apple-use-googles-ai-model-run-new-siri-bloomberg-news-reports-2025-11-05/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So Apple and Strix Halo are on the right track. Intel where art thou? Any one else we can count on to eventually catch the trend? Medusa halo is going to be awesome:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;a href="https://www.youtube.com/shorts/yAcONx3Jxf8"&gt;https://www.youtube.com/shorts/yAcONx3Jxf8&lt;/a&gt; . Quote: Medusa Halo is going to destroy strix halo.&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.techpowerup.com/340216/amd-medusa-halo-apu-leak-reveals-up-to-24-cores-and-48-rdna-5-cus#g340216-3"&gt;https://www.techpowerup.com/340216/amd-medusa-halo-apu-leak-reveals-up-to-24-cores-and-48-rdna-5-cus#g340216-3&lt;/a&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Even longer term 5 years, I'm thinking in memory compute will take over versus current standard of von neumann architecture. Once we crack in memory compute nut then things will get very interesting. Will allow a greater level of parallelization. Every neuron can fire simultaneously like our human brain. In memory compute will dominate for future architectures in 10 years versus von neumann.&lt;/p&gt; &lt;p&gt;What do you think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Terminator857"&gt; /u/Terminator857 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oph7jd/unified_memory_is_the_future_not_gpu_for_local_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oph7jd/unified_memory_is_the_future_not_gpu_for_local_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oph7jd/unified_memory_is_the_future_not_gpu_for_local_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-05T22:20:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1opeu1w</id>
    <title>Visualizing Quantization Types</title>
    <updated>2025-11-05T20:52:02+00:00</updated>
    <author>
      <name>/u/VoidAlchemy</name>
      <uri>https://old.reddit.com/user/VoidAlchemy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opeu1w/visualizing_quantization_types/"&gt; &lt;img alt="Visualizing Quantization Types" src="https://preview.redd.it/brkkf7fs2izf1.gif?width=640&amp;amp;crop=smart&amp;amp;s=69bbd6b8af4c7420aed83b9b70eddb5a51a78d26" title="Visualizing Quantization Types" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've seen some releases of MXFP4 quantized models recently and don't understand why given mxfp4 is kind of like a slightly smaller lower quality q4_0.&lt;/p&gt; &lt;p&gt;So unless the original model was post-trained specifically for MXFP4 like gpt-oss-120b or you yourself did some kind of QAT (quantization aware fine-tuning) targeting specifically mxfp4, then personally I'd go with good old q4_0 or ik's newer iq4_kss.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;mxfp4 4.25bpw&lt;/li&gt; &lt;li&gt;q4_0 4.5bpw&lt;/li&gt; &lt;li&gt;iq4_kss 4.0bpw&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I used the llama.cpp gguf python package to read a uint8 .bmp image, convert it to float16 numpy 2d array, and save that as a .gguf. Then I quantized the gguf to various types using ik_llama.cpp, and then finally re-quantize that back to f16 and save the resulting uint8 .bmp image.&lt;/p&gt; &lt;p&gt;Its kinda neat to visualize the effects of block sizes looking at image data. To me the mxfp4 looks &amp;quot;worse&amp;quot; than the q4_0 and the iq4_kss.&lt;/p&gt; &lt;p&gt;I haven't done perplexity/KLD measurements to directly compare mxfp4, but iq4_kss tends to be one of the best available in that size range in my previous quant release testing.&lt;/p&gt; &lt;p&gt;Finally, it is confusing to me, but nvfp4 is yet &lt;em&gt;a different&lt;/em&gt; quantization type with specific blackwell hardware support which I haven't tried yet myself.&lt;/p&gt; &lt;p&gt;Anyway, in my opinion mxfp4 isn't particularly special or better despite being somewhat newer. What do y'all think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VoidAlchemy"&gt; /u/VoidAlchemy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/brkkf7fs2izf1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opeu1w/visualizing_quantization_types/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1opeu1w/visualizing_quantization_types/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-05T20:52:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1opa6os</id>
    <title>Local Setup</title>
    <updated>2025-11-05T18:03:19+00:00</updated>
    <author>
      <name>/u/mattate</name>
      <uri>https://old.reddit.com/user/mattate</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opa6os/local_setup/"&gt; &lt;img alt="Local Setup" src="https://preview.redd.it/8imhi4icahzf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=eabf7d36f6208d91a8e908e97d3d1a1b1ee6998f" title="Local Setup" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey just figured I would share our local setup. I started building these machines as an experiment to see if I could drop our cost, and so far it has worked out pretty good. The first one was over a year ago, lots of lessons learned getting them up and stable. &lt;/p&gt; &lt;p&gt;The cost of AI APIs has come down drastically, when we started with these machines there was absolutely no competition. It's still cheaper to run your own hardware, but it's much much closer now. This community really I think is providing crazy value allowing company's like mine to experiment and roll things into production without having to drop hundreds of thousands of dollars literally on propritary AI API usage.&lt;/p&gt; &lt;p&gt;Running a mix of used 3090s, new 4090s, 5090s, and RTX 6000 pro's. The 3090 is certainly the king off cost per token without a doubt, but the problems with buying used gpus is not really worth the hassle of you're relying on these machines to get work done. &lt;/p&gt; &lt;p&gt;We process anywhere between 70m and 120m tokens per day, we could probably do more. &lt;/p&gt; &lt;p&gt;Some notes:&lt;/p&gt; &lt;p&gt;ASUS motherboards work well and are pretty stable, running ASUS Pro WS WRX80E-SAGE SE with threadripper gets up to 7 gpus, but usually pair gpus so 6 is the useful max. Will upgrade to the 90 in future machines. &lt;/p&gt; &lt;p&gt;240v power works much better then 120v, this is more about effciency of the power supplies. &lt;/p&gt; &lt;p&gt;Cooling is a huge problem, any more machines them I have now and cooling will become a very significant issue.&lt;/p&gt; &lt;p&gt;We run predominantly vllm these days, mixture of different models as new ones get released. &lt;/p&gt; &lt;p&gt;Happy to answer any other questions. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mattate"&gt; /u/mattate &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8imhi4icahzf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opa6os/local_setup/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1opa6os/local_setup/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-05T18:03:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1olq14f</id>
    <title>[MEGATHREAD] Local AI Hardware - November 2025</title>
    <updated>2025-11-01T15:02:17+00:00</updated>
    <author>
      <name>/u/eck72</name>
      <uri>https://old.reddit.com/user/eck72</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is the monthly thread for sharing your local AI setups and the models you're running.&lt;/p&gt; &lt;p&gt;Whether you're using a single CPU, a gaming GPU, or a full rack, post what you're running and how it performs.&lt;/p&gt; &lt;p&gt;Post in any format you like. The list below is just a guide:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Hardware: CPU, GPU(s), RAM, storage, OS&lt;/li&gt; &lt;li&gt;Model(s): name + size/quant&lt;/li&gt; &lt;li&gt;Stack: (e.g. llama.cpp + custom UI)&lt;/li&gt; &lt;li&gt;Performance: t/s, latency, context, batch etc.&lt;/li&gt; &lt;li&gt;Power consumption&lt;/li&gt; &lt;li&gt;Notes: purpose, quirks, comments&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please share setup pics for eye candy!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Quick reminder&lt;/strong&gt;: You can share hardware purely to ask questions or get feedback. All experience levels welcome.&lt;/p&gt; &lt;p&gt;House rules: no buying/selling/promo.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eck72"&gt; /u/eck72 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T15:02:17+00:00</published>
  </entry>
</feed>
