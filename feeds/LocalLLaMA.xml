<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-06T04:37:00+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1n9pqo7</id>
    <title>I built a native iOS AI client to chat with GPT, Gemini, and Local Models simultaneously, with full API parameter customization.</title>
    <updated>2025-09-06T03:54:29+00:00</updated>
    <author>
      <name>/u/ArtichokePretty8741</name>
      <uri>https://old.reddit.com/user/ArtichokePretty8741</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9pqo7/i_built_a_native_ios_ai_client_to_chat_with_gpt/"&gt; &lt;img alt="I built a native iOS AI client to chat with GPT, Gemini, and Local Models simultaneously, with full API parameter customization." src="https://b.thumbs.redditmedia.com/VHyDfKuFegGPueVCrtYTNVSd9kdkYbn15HgIyPbo-Vs.jpg" title="I built a native iOS AI client to chat with GPT, Gemini, and Local Models simultaneously, with full API parameter customization." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;I was looking for a native iOS client that would let me chat with many AI models simultaneously and with deep customization. Since I couldn't find one that fit my needs perfectly, I built &lt;a href="https://apps.apple.com/us/app/lavachat-your-ai-hub/id6748080403"&gt;LavaChat&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;üåã &lt;strong&gt;(Image 1):&lt;/strong&gt; The core idea is a clean, native iOS interface where you can chat with multiple AIs at once. You can send one prompt and get responses from GPT, Gemini, DeepSeek, and your own local model running on Ollama, all in the same chat.&lt;/p&gt; &lt;p&gt;üåã &lt;strong&gt;(Image 2):&lt;/strong&gt; Responses are stacked like cards. You can easily swipe through them to compare answers. Your next prompt continues the conversation with whichever AI is on top.&lt;/p&gt; &lt;p&gt;üåã &lt;strong&gt;(Image 3):&lt;/strong&gt; A clean, tab-based navigation. The far left is for chats, and right next to it is the management center for all your AI providers, models, and instances.&lt;/p&gt; &lt;p&gt;üåã &lt;strong&gt;(Image 4 &amp;amp; 5):&lt;/strong&gt; This is where it gets interesting. LavaChat is built for customization.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Connect to Anything:&lt;/strong&gt; You can add your own API endpoints. It supports OpenAI, Anthropic, and Google API formats, which means you can connect to local models served via Ollama, llama.cpp, etc.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Full Parameter Control:&lt;/strong&gt; You have granular control over &lt;strong&gt;every&lt;/strong&gt; API parameter. If the model's API exposes it, you can tweak it‚Äîsystem prompts, temperature, and even model-specific JSON parameters.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;üåã &lt;strong&gt;(Image 6):&lt;/strong&gt; Save and insert your frequently used prompts (like character sheets or complex instructions) with a single tap.&lt;/p&gt; &lt;p&gt;üåã &lt;strong&gt;(Image 7):&lt;/strong&gt; Create custom &amp;quot;AI Actions&amp;quot;. For example, create a one-tap action that uses an AI to refine your prompt before sending it, or makes the AI's own response more concise.&lt;/p&gt; &lt;p&gt;üåã &lt;strong&gt;(Image 8):&lt;/strong&gt; Configure different presets for various chat scenarios. This includes context length, search/creativity toggles, and even showing/hiding specific system or AI action buttons.&lt;/p&gt; &lt;p&gt;üåã &lt;strong&gt;(Image 9):&lt;/strong&gt; Easily share and import your setups. You can export your AI instances, chat settings, or entire conversations via a file, iCloud link, or QR code.&lt;/p&gt; &lt;p&gt;It's a free download on the App Store, and I'd love to hear your feedback.&lt;/p&gt; &lt;p&gt;App Store Link: &lt;a href="https://apps.apple.com/us/app/lavachat-your-ai-hub/id6748080403"&gt;https://apps.apple.com/us/app/lavachat-your-ai-hub/id6748080403&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ArtichokePretty8741"&gt; /u/ArtichokePretty8741 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1n9pqo7"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9pqo7/i_built_a_native_ios_ai_client_to_chat_with_gpt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n9pqo7/i_built_a_native_ios_ai_client_to_chat_with_gpt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-06T03:54:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1n910t9</id>
    <title>Where is theBloke?</title>
    <updated>2025-09-05T09:55:49+00:00</updated>
    <author>
      <name>/u/holistic-engine</name>
      <uri>https://old.reddit.com/user/holistic-engine</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Haven‚Äôt seen any posts related to this legend in a while? Where is he, is he okay? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/holistic-engine"&gt; /u/holistic-engine &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n910t9/where_is_thebloke/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n910t9/where_is_thebloke/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n910t9/where_is_thebloke/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-05T09:55:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1n9omqj</id>
    <title>Kimi K2-0905 is a powerhouse VS claude-sonnet-4 @20250514.</title>
    <updated>2025-09-06T02:55:27+00:00</updated>
    <author>
      <name>/u/klippers</name>
      <uri>https://old.reddit.com/user/klippers</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been heavily builidng with claude-sonnet-4@20250514, but threw $5 into OpenRouter and gave K2-0905 and WOW. &lt;/p&gt; &lt;p&gt;Not sure if its a ‚Äúbetter‚Äù model, but seems to chew through tasks in a ‚Äúbetter‚Äù way.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/klippers"&gt; /u/klippers &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9omqj/kimi_k20905_is_a_powerhouse_vs_claudesonnet4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9omqj/kimi_k20905_is_a_powerhouse_vs_claudesonnet4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n9omqj/kimi_k20905_is_a_powerhouse_vs_claudesonnet4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-06T02:55:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1n9aioh</id>
    <title>New kimi-k2 on Fiction.liveBench</title>
    <updated>2025-09-05T16:52:09+00:00</updated>
    <author>
      <name>/u/fictionlive</name>
      <uri>https://old.reddit.com/user/fictionlive</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9aioh/new_kimik2_on_fictionlivebench/"&gt; &lt;img alt="New kimi-k2 on Fiction.liveBench" src="https://preview.redd.it/ww7n9p40mdnf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=10df320d7922c87a70ef2b46ae45783f49983d20" title="New kimi-k2 on Fiction.liveBench" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fictionlive"&gt; /u/fictionlive &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ww7n9p40mdnf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9aioh/new_kimik2_on_fictionlivebench/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n9aioh/new_kimik2_on_fictionlivebench/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-05T16:52:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1n98c25</id>
    <title>Seems new model qwen 3 max preview is already available on qwen chat</title>
    <updated>2025-09-05T15:28:15+00:00</updated>
    <author>
      <name>/u/Independent-Wind4462</name>
      <uri>https://old.reddit.com/user/Independent-Wind4462</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n98c25/seems_new_model_qwen_3_max_preview_is_already/"&gt; &lt;img alt="Seems new model qwen 3 max preview is already available on qwen chat" src="https://preview.redd.it/nzfh1xg27dnf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=73f4d1143429da1ba0af0e95c543b1866fd87af5" title="Seems new model qwen 3 max preview is already available on qwen chat" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Wind4462"&gt; /u/Independent-Wind4462 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nzfh1xg27dnf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n98c25/seems_new_model_qwen_3_max_preview_is_already/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n98c25/seems_new_model_qwen_3_max_preview_is_already/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-05T15:28:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1n9b7mn</id>
    <title>Tenstorrent p150a tested against RTX5090, RTX3090, A100, H100 by Russian blogger</title>
    <updated>2025-09-05T17:18:10+00:00</updated>
    <author>
      <name>/u/No-Refrigerator-1672</name>
      <uri>https://old.reddit.com/user/No-Refrigerator-1672</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Tenstorrent is a startup that aims to create AI accelerators rivaling the GPU; their current best model, &lt;a href="https://tenstorrent.com/hardware/blackhole"&gt;p150a&lt;/a&gt;, featuring 32GB of GDDR6 memory, was tested against numerous GPUs by Russian blogger &lt;a href="https://www.youtube.com/@prohitec"&gt;Pro Hi-Tech&lt;/a&gt; in the following video:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=pIS3Yery4I0"&gt;https://www.youtube.com/watch?v=pIS3Yery4I0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;According to the video, the tests were launched by some kind of Python script on unquantized Llama 3 8B (timestamp 6:48), I assume inference via Transformers library. In such case, he found out the time to first token being slightly faster than 5090 and A100; however, the token generation speed is half of 5090 and on par with A30. Additionally, he disassembled the card and showed the PCB (2:02).&lt;/p&gt; &lt;p&gt;The charts featured in this video:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;7:39 - Time to first token, ms;&lt;/li&gt; &lt;li&gt;8:26 - Inter-token latency, ms;&lt;/li&gt; &lt;li&gt;8:38 - Generation speed, tok/s;&lt;/li&gt; &lt;li&gt;9:07 - Card TDP; it seems like the numbers are as specified by manufacturer, not measured;&lt;/li&gt; &lt;li&gt;9:26 - Performance per watt; I assume it's tok/s/W;&lt;/li&gt; &lt;li&gt;9:57 - Performance per dollar; prices are MSRP, not actual retail prices.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;He calls out numerous &lt;strong&gt;software problems&lt;/strong&gt; with p150a:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The default installation guide is outdated;&lt;/li&gt; &lt;li&gt;The manufacturer supplied model training containers failed to launch;&lt;/li&gt; &lt;li&gt;The telemetry app does not report any of the memory parameters (especially amount of memory utilized);&lt;/li&gt; &lt;li&gt;If telemetry app is launched while doing compute, it will hung up the system, requiring full PC reboot; as a result, it is impossible to measure the chip's temperature under load;&lt;/li&gt; &lt;li&gt;He failed to test any of 14B models he tried (11:01); although he cites OOM error, so I suspect the test script was simply reserving too much KV cache;&lt;/li&gt; &lt;li&gt;The p150a hung up and required full OS reboot after &amp;quot;long-term load&amp;quot;;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It seems that while Tenstorrent offers decent performance for the price, it's software support is too lacking to use it in production.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Refrigerator-1672"&gt; /u/No-Refrigerator-1672 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9b7mn/tenstorrent_p150a_tested_against_rtx5090_rtx3090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9b7mn/tenstorrent_p150a_tested_against_rtx5090_rtx3090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n9b7mn/tenstorrent_p150a_tested_against_rtx5090_rtx3090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-05T17:18:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1n9ba1m</id>
    <title>Qwen3 30B A3B Q40 on 4 x Raspberry Pi 5 8GB 13.04 tok/s (Distributed Llama)</title>
    <updated>2025-09-05T17:20:43+00:00</updated>
    <author>
      <name>/u/thisislewekonto</name>
      <uri>https://old.reddit.com/user/thisislewekonto</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9ba1m/qwen3_30b_a3b_q40_on_4_x_raspberry_pi_5_8gb_1304/"&gt; &lt;img alt="Qwen3 30B A3B Q40 on 4 x Raspberry Pi 5 8GB 13.04 tok/s (Distributed Llama)" src="https://external-preview.redd.it/KUWKhlT5OZYpzmuPdkrY6FyowQ4PaYe23RiUvraDVrQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4733d277f6f6e759fe47794087d1da790f8d36b7" title="Qwen3 30B A3B Q40 on 4 x Raspberry Pi 5 8GB 13.04 tok/s (Distributed Llama)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thisislewekonto"&gt; /u/thisislewekonto &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/b4rtaz/distributed-llama/discussions/255"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9ba1m/qwen3_30b_a3b_q40_on_4_x_raspberry_pi_5_8gb_1304/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n9ba1m/qwen3_30b_a3b_q40_on_4_x_raspberry_pi_5_8gb_1304/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-05T17:20:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1n98t6m</id>
    <title>Qwen released API of Qwen3-Max-Preview (Instruct)</title>
    <updated>2025-09-05T15:46:51+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n98t6m/qwen_released_api_of_qwen3maxpreview_instruct/"&gt; &lt;img alt="Qwen released API of Qwen3-Max-Preview (Instruct)" src="https://preview.redd.it/zw8lhw7eadnf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fb61f9c84a1df5f11a0a7762294f4a826dfa9e29" title="Qwen released API of Qwen3-Max-Preview (Instruct)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Big news: Introducing Qwen3-Max-Preview (Instruct) ‚Äî our biggest model yet, with over 1 trillion parameters! üöÄ&lt;/p&gt; &lt;p&gt;Now available via Qwen Chat &amp;amp; Alibaba Cloud API.&lt;/p&gt; &lt;p&gt;Benchmarks show it beats our previous best, Qwen3-235B-A22B-2507. Internal tests + early user feedback confirm: stronger performance, broader knowledge, better at conversations, agentic tasks &amp;amp; instruction following.&lt;/p&gt; &lt;p&gt;Scaling works ‚Äî and the official release will surprise you even more. Stay tuned!&lt;/p&gt; &lt;p&gt;Qwen Chat: &lt;a href="https://chat.qwen.ai/"&gt;https://chat.qwen.ai/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zw8lhw7eadnf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n98t6m/qwen_released_api_of_qwen3maxpreview_instruct/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n98t6m/qwen_released_api_of_qwen3maxpreview_instruct/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-05T15:46:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1n9om6e</id>
    <title>I have found a solution to chatbot monetization - looking for builder/founder to chat/compensated trial</title>
    <updated>2025-09-06T02:54:38+00:00</updated>
    <author>
      <name>/u/Ok-Pineapple8638</name>
      <uri>https://old.reddit.com/user/Ok-Pineapple8638</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm a small indie builder trying to solve a problem many of us had with chatbots: subscriptions and IAPs turn a lot of users off, but ‚Äúrandom banner ads‚Äù are also kinda ass.&lt;/p&gt; &lt;p&gt;I've been experimenting with a different approach: show a &lt;em&gt;single, relevant&lt;/em&gt; sponsored suggestion only when a user‚Äôs intent clearly matches (e.g., budgeting prompt ‚Üí budgeting app; sleep/self-care prompt ‚Üí a CBT or journaling app). &lt;/p&gt; &lt;p&gt;Frequency is capped and entirely under the builder‚Äôs control, and it can be switched off at any time.&lt;/p&gt; &lt;p&gt;I‚Äôd love to sanity-check this with folks actually shipping AI companion / entertainment chatbots (web or mobile). If you‚Äôre open to trying it, we‚Äôll do a short, &lt;em&gt;paid&lt;/em&gt; trial: small stipend for your time, you keep any ad revenue during the test, and we‚Äôll share back what we learn (win or fail). No pressure to continue afterward.&lt;/p&gt; &lt;p&gt;If this idea sounds off, I genuinely want to hear why;&lt;/p&gt; &lt;p&gt;If you‚Äôre curious and want to get the trial going, DM me and I‚Äôll send details!&lt;/p&gt; &lt;p&gt;Thanks for reading‚Äîhappy to be told this is a bad idea if that‚Äôs the consensus. I‚Äôm here to learn from folks who build these products every day.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Pineapple8638"&gt; /u/Ok-Pineapple8638 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9om6e/i_have_found_a_solution_to_chatbot_monetization/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9om6e/i_have_found_a_solution_to_chatbot_monetization/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n9om6e/i_have_found_a_solution_to_chatbot_monetization/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-06T02:54:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1n9flux</id>
    <title>An Open-Source, Configurable Deepthink Reasoning System That Performs the Same as Gemini Deepthink (Gold Medal at IMO 2025)</title>
    <updated>2025-09-05T20:09:03+00:00</updated>
    <author>
      <name>/u/Ryoiki-Tokuiten</name>
      <uri>https://old.reddit.com/user/Ryoiki-Tokuiten</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9flux/an_opensource_configurable_deepthink_reasoning/"&gt; &lt;img alt="An Open-Source, Configurable Deepthink Reasoning System That Performs the Same as Gemini Deepthink (Gold Medal at IMO 2025)" src="https://external-preview.redd.it/ZzdnNGplb2prZW5mMQYvwHNGGuNUN8or0wrdAaTg9BfB31Jlu0HUBZSFT4Gi.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bf73aae4d4084affa89860b6367ce25c59f84f8b" title="An Open-Source, Configurable Deepthink Reasoning System That Performs the Same as Gemini Deepthink (Gold Medal at IMO 2025)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ryoiki-Tokuiten"&gt; /u/Ryoiki-Tokuiten &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/jhjamaojkenf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9flux/an_opensource_configurable_deepthink_reasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n9flux/an_opensource_configurable_deepthink_reasoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-05T20:09:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1n981di</id>
    <title>Kwai-Klear/Klear-46B-A2.5B-Instruct: Sparse-MoE LLM (46B total / only 2.5B active)</title>
    <updated>2025-09-05T15:16:49+00:00</updated>
    <author>
      <name>/u/paf1138</name>
      <uri>https://old.reddit.com/user/paf1138</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n981di/kwaiklearklear46ba25binstruct_sparsemoe_llm_46b/"&gt; &lt;img alt="Kwai-Klear/Klear-46B-A2.5B-Instruct: Sparse-MoE LLM (46B total / only 2.5B active)" src="https://external-preview.redd.it/YCjYCLowWoUZPOtPjfRsNwF5BBEIscgMQg1iK3Ht-1Q.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b95159fb1ff226ff1812f1b78e632fd38eaf6fc9" title="Kwai-Klear/Klear-46B-A2.5B-Instruct: Sparse-MoE LLM (46B total / only 2.5B active)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paf1138"&gt; /u/paf1138 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Kwai-Klear/Klear-46B-A2.5B-Instruct"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n981di/kwaiklearklear46ba25binstruct_sparsemoe_llm_46b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n981di/kwaiklearklear46ba25binstruct_sparsemoe_llm_46b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-05T15:16:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1n9c0ef</id>
    <title>Bro is thinking about this for 5 minutes, what you mean by "maybe" man, decide it already</title>
    <updated>2025-09-05T17:48:45+00:00</updated>
    <author>
      <name>/u/trxhh36</name>
      <uri>https://old.reddit.com/user/trxhh36</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9c0ef/bro_is_thinking_about_this_for_5_minutes_what_you/"&gt; &lt;img alt="Bro is thinking about this for 5 minutes, what you mean by &amp;quot;maybe&amp;quot; man, decide it already" src="https://preview.redd.it/u6uf4z4kvdnf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9d03e46b798490e8b7beb01b871ef039d65fc462" title="Bro is thinking about this for 5 minutes, what you mean by &amp;quot;maybe&amp;quot; man, decide it already" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GLM 4.5 in Z AI&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/trxhh36"&gt; /u/trxhh36 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/u6uf4z4kvdnf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9c0ef/bro_is_thinking_about_this_for_5_minutes_what_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n9c0ef/bro_is_thinking_about_this_for_5_minutes_what_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-05T17:48:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1n8ues8</id>
    <title>Kimi-K2-Instruct-0905 Released!</title>
    <updated>2025-09-05T03:15:27+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8ues8/kimik2instruct0905_released/"&gt; &lt;img alt="Kimi-K2-Instruct-0905 Released!" src="https://preview.redd.it/6jq7r55ak9nf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0a5eec08b8c7bedbb50e39a668de98e599c3a0b6" title="Kimi-K2-Instruct-0905 Released!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6jq7r55ak9nf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8ues8/kimik2instruct0905_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n8ues8/kimik2instruct0905_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-05T03:15:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1n95fl4</id>
    <title>Unsloth just released their GGUF of Kimi-K2-Instruct-0905!</title>
    <updated>2025-09-05T13:34:21+00:00</updated>
    <author>
      <name>/u/TheAndyGeorge</name>
      <uri>https://old.reddit.com/user/TheAndyGeorge</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n95fl4/unsloth_just_released_their_gguf_of/"&gt; &lt;img alt="Unsloth just released their GGUF of Kimi-K2-Instruct-0905!" src="https://external-preview.redd.it/u42y4pGiiWpLArGTxtLnpU7XIOrkkmzZ5xAid1ozch8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f7d4e7a1b9c3b96563747fc8517c620156b1d622" title="Unsloth just released their GGUF of Kimi-K2-Instruct-0905!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheAndyGeorge"&gt; /u/TheAndyGeorge &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/unsloth/Kimi-K2-Instruct-0905-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n95fl4/unsloth_just_released_their_gguf_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n95fl4/unsloth_just_released_their_gguf_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-05T13:34:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1n99gpq</id>
    <title>LongPage: 300 full novels with reasoning traces for training better writing LLMs</title>
    <updated>2025-09-05T16:11:47+00:00</updated>
    <author>
      <name>/u/Senior_Evidence_3793</name>
      <uri>https://old.reddit.com/user/Senior_Evidence_3793</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n99gpq/longpage_300_full_novels_with_reasoning_traces/"&gt; &lt;img alt="LongPage: 300 full novels with reasoning traces for training better writing LLMs" src="https://external-preview.redd.it/riwdF_EjDqIZtaMr2L8TnhS0xQM36fl9qJv4y9kVTdk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=13dfc405c4b3fc698c8aad905dd57399f797231a" title="LongPage: 300 full novels with reasoning traces for training better writing LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/5zaxpqdsednf1.png?width=1536&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8c0018a54af853ad9c74b9e1e7bd1ac219af544b"&gt;https://preview.redd.it/5zaxpqdsednf1.png?width=1536&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8c0018a54af853ad9c74b9e1e7bd1ac219af544b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Current LLMs struggle with long-form creative writing because they lack hierarchical planning. LongPage solves this by providing the reasoning scaffolds that were missing.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it is:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;300 complete books (Project Gutenberg classics) with full reasoning traces&lt;/li&gt; &lt;li&gt;40,000 to 600,000+ tokens per book&lt;/li&gt; &lt;li&gt;Multi-layered planning: character archetypes, story arcs, world rules, scene breakdowns&lt;/li&gt; &lt;li&gt;Rich structural metadata (dialogue density, pacing, narrative focus)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Why it matters:&lt;/strong&gt; This is the &amp;quot;Chain of Thought for creative writing&amp;quot; - explicit reasoning traces showing models how to plan character development, plot progression, and maintain thematic coherence across entire books.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Training applications:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Cold-start SFT ‚Üí RL workflows with 3-component structure (prompt, thinking, book)&lt;/li&gt; &lt;li&gt;Inference-time scaffolding using reasoning traces as plans&lt;/li&gt; &lt;li&gt;Hierarchical training: book-level plans ‚Üí chapter expansions ‚Üí scene continuations&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Currently 300 books, scaling to 100K. All reasoning generated by Qwen3-32B with iterative agent validation across scene ‚Üí chapter ‚Üí book levels.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;HF Link:&lt;/strong&gt; &lt;a href="https://huggingface.co/datasets/Pageshift-Entertainment/LongPage"&gt;https://huggingface.co/datasets/Pageshift-Entertainment/LongPage&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Anyone working on long-form generation? Would love to hear what training approaches you're planning to try with this.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Senior_Evidence_3793"&gt; /u/Senior_Evidence_3793 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n99gpq/longpage_300_full_novels_with_reasoning_traces/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n99gpq/longpage_300_full_novels_with_reasoning_traces/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n99gpq/longpage_300_full_novels_with_reasoning_traces/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-05T16:11:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1n92jy2</id>
    <title>List of open models released or updated this week on this sub, just in case you missed one.</title>
    <updated>2025-09-05T11:21:44+00:00</updated>
    <author>
      <name>/u/aifeed-fyi</name>
      <uri>https://old.reddit.com/user/aifeed-fyi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A quick list of models updates and new releases mentioned in several posts during the week on LocalLLama. I wanted to include links to posts/models but it didn't go through. &lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Kimi K2-0905&lt;/strong&gt; ‚Äì new release from Moonshot AI&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Wayfarer 2 12B &amp;amp; Nova 70B&lt;/strong&gt; ‚Äì open-sourced narrative roleplay models from AI Dungeon&lt;/li&gt; &lt;li&gt;&lt;strong&gt;EmbeddingGemma (300M)&lt;/strong&gt; ‚Äì Google‚Äôs compact multilingual embedding model&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Apertus&lt;/strong&gt; ‚Äì new open multilingual LLM from ETH Z√ºrich (40%+ non-English training data)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;WEBGEN-4B&lt;/strong&gt; ‚Äì web design generation model trained on 100k synthetic samples&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Lille (130M)&lt;/strong&gt; ‚Äì a truly open-source small language model (trained fully from&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hunyuan-MT-7B &amp;amp; Hunyuan-MT-Chimera-7B&lt;/strong&gt; ‚Äì Tencent‚Äôs new translation &amp;amp; ensemble models&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPT-OSS-120B&lt;/strong&gt; ‚Äì benchmarks updates&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Beens-MiniMax (103M MoE)&lt;/strong&gt; ‚Äì scratch-built, SFT + LoRA experiments&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aifeed-fyi"&gt; /u/aifeed-fyi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n92jy2/list_of_open_models_released_or_updated_this_week/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n92jy2/list_of_open_models_released_or_updated_this_week/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n92jy2/list_of_open_models_released_or_updated_this_week/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-05T11:21:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1n9ap73</id>
    <title>Qwen 3 Max Official Pricing</title>
    <updated>2025-09-05T16:58:58+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9ap73/qwen_3_max_official_pricing/"&gt; &lt;img alt="Qwen 3 Max Official Pricing" src="https://preview.redd.it/tx801h07ndnf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b87412ed2c1257b7012cf473923bdcbd7512d19e" title="Qwen 3 Max Official Pricing" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/tx801h07ndnf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9ap73/qwen_3_max_official_pricing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n9ap73/qwen_3_max_official_pricing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-05T16:58:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1n9hduk</id>
    <title>VibeVoice came back. Though many may not like it.</title>
    <updated>2025-09-05T21:19:36+00:00</updated>
    <author>
      <name>/u/Fresh_Sun_1017</name>
      <uri>https://old.reddit.com/user/Fresh_Sun_1017</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/microsoft/VibeVoice"&gt;VibeVoice&lt;/a&gt; has returned(&lt;em&gt;not&lt;/em&gt; VibeVoice-large); however, Microsoft plans to implement censorship due to people's &amp;quot;misuse of research&amp;quot;. Here's the quote from the repo:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;VibeVoice is an open-source research framework intended to advance collaboration in the speech synthesis community. &lt;strong&gt;After release, we discovered instances where the tool was used in ways inconsistent with the stated intent. Since responsible use of AI is one of Microsoft‚Äôs guiding principles, we have disabled this repo until we are confident that out-of-scope use is no longer possible.&lt;/strong&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;What types of censorship will be implemented? And couldn‚Äôt people just use or share older, unrestricted versions they've already downloaded? That's going to be interesting...&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Edit:&lt;/strong&gt; The VibeVoice-Large model is still available as of now, &lt;a href="https://www.modelscope.cn/models/microsoft/VibeVoice-Large/files"&gt;VibeVoice-Large ¬∑ Models&lt;/a&gt; on Modelscope. It may be deleted soon.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fresh_Sun_1017"&gt; /u/Fresh_Sun_1017 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9hduk/vibevoice_came_back_though_many_may_not_like_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9hduk/vibevoice_came_back_though_many_may_not_like_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n9hduk/vibevoice_came_back_though_many_may_not_like_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-05T21:19:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1n9ong4</id>
    <title>Kimi K2 0905 is a beast at coding</title>
    <updated>2025-09-06T02:56:29+00:00</updated>
    <author>
      <name>/u/adumdumonreddit</name>
      <uri>https://old.reddit.com/user/adumdumonreddit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I've been working on this static website, just a side project where I can do some blogging or some fun javascript experiments, but I've been making this new component, basically implementing custom scrolling and pagination behaviours from scratch.&lt;/p&gt; &lt;p&gt;Anyways, I was facing a bunch of tough bugs, in complete deadlock, even tried asking Deepseek/Gemini/even went for one response from Opus, no luck. Then, decided to try the new Kimi, and bam. One try, instantly solved the issue, and did it with some tastefully commented (think somewhere between Gemini and Qwen levels of comment-ness) and good-practice code.&lt;/p&gt; &lt;p&gt;I was impressed, so I decided to just toss in my entire CSS/HTML skeleton as well as a fuck it, and when it was done, the result was so much prettier than the one I had originally. Damn, I thought, so I decided to toss it a few more problems: implement dark mode handling for the entire skeleton using only CSS and a js button, and implement another style hotswapping feature I had been thinking of.&lt;/p&gt; &lt;p&gt;Five minutes, and they both were done flawlessly.&lt;/p&gt; &lt;p&gt;I'm no javascript wiz, so I imagine all of that would probably have taken me around another two or three hours. With Kimi, I did it in like 10 minutes. What's more is that it cracked bugs that even the previous SOTA models, my go-tos, couldn't do. The consistency is also impressive: all of it was in one try, maybe two if I wanted to clarify my requirements, and all of it was well formatted, had a nice level of comments (I don't know how to explain this one, the comments were just 'good' in a way Gemini comments aren't, for example)&lt;/p&gt; &lt;p&gt;Wow. I'm impressed.&lt;/p&gt; &lt;p&gt;(Sorry, no images; the website is publicly accessible and linked to my real name, so I'd prefer not to link it to this account in any way.)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/adumdumonreddit"&gt; /u/adumdumonreddit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9ong4/kimi_k2_0905_is_a_beast_at_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9ong4/kimi_k2_0905_is_a_beast_at_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n9ong4/kimi_k2_0905_is_a_beast_at_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-06T02:56:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1n9o4em</id>
    <title>ROG Ally X with RTX 6000 Pro Blackwell Max-Q as Makeshift LLM Workstation</title>
    <updated>2025-09-06T02:29:21+00:00</updated>
    <author>
      <name>/u/susmitds</name>
      <uri>https://old.reddit.com/user/susmitds</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9o4em/rog_ally_x_with_rtx_6000_pro_blackwell_maxq_as/"&gt; &lt;img alt="ROG Ally X with RTX 6000 Pro Blackwell Max-Q as Makeshift LLM Workstation" src="https://b.thumbs.redditmedia.com/JhYB2Z_qsB-pJqq4qeo2qmH8su0aVxk10f3FRgO-6QE.jpg" title="ROG Ally X with RTX 6000 Pro Blackwell Max-Q as Makeshift LLM Workstation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So my workstation motherboard stopped working and needed to be sent for replacement in warranty. Leaving my research work and LLM workflow screwed.&lt;/p&gt; &lt;p&gt;Off a random idea stuck one of my RTX 6000 Blackwell into a EGPU enclosure (Aoostar AG02) and tried it on my travel device, the ROG Ally X and it kinda blew my mind on how good this makeshift temporary setup was working. Never thought I would using my Ally for hosting 235B parameter LLM models, yet with the GPU, I was getting very good performance at 1100+ tokens/sec prefill, 25+ tokens/sec decode on Qwen3-235B-A22B-Instruct-2507 with 180K context using a custom quant I made in ik-llama.cpp (attention projections, embeddings, lm_head at q8_0, expert up/gate at iq2_kt, down at iq3_kt, total 75 GB size). Also tested GLM 4.5 Air with unsloth's Q4_K_XL, could easily run with full 128k context. I am perplexed how good the models are all running even at PCIE 4 x 4 on a eGPU.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/susmitds"&gt; /u/susmitds &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1n9o4em"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9o4em/rog_ally_x_with_rtx_6000_pro_blackwell_maxq_as/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n9o4em/rog_ally_x_with_rtx_6000_pro_blackwell_maxq_as/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-06T02:29:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1n98vdp</id>
    <title>Qwen 3 Max Official Benchmarks (possibly open sourcing later..?)</title>
    <updated>2025-09-05T15:49:10+00:00</updated>
    <author>
      <name>/u/Trevor050</name>
      <uri>https://old.reddit.com/user/Trevor050</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n98vdp/qwen_3_max_official_benchmarks_possibly_open/"&gt; &lt;img alt="Qwen 3 Max Official Benchmarks (possibly open sourcing later..?)" src="https://preview.redd.it/eeekht6sadnf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ff85bfdf6253ad3ba0a381c5514ad302898defd3" title="Qwen 3 Max Official Benchmarks (possibly open sourcing later..?)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Trevor050"&gt; /u/Trevor050 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/eeekht6sadnf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n98vdp/qwen_3_max_official_benchmarks_possibly_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n98vdp/qwen_3_max_official_benchmarks_possibly_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-05T15:49:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1n975er</id>
    <title>Qwen 3 max</title>
    <updated>2025-09-05T14:42:28+00:00</updated>
    <author>
      <name>/u/LeatherRub7248</name>
      <uri>https://old.reddit.com/user/LeatherRub7248</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n975er/qwen_3_max/"&gt; &lt;img alt="Qwen 3 max" src="https://external-preview.redd.it/9f9JRaQTq2uR5GC3copbxq5McLsZhYSzNHSbhHCgcmg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1f89b1589b444db5310feea66a3e0335c0591fac" title="Qwen 3 max" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's out&lt;/p&gt; &lt;p&gt;&lt;a href="https://openrouter.ai/qwen/qwen3-max"&gt;https://openrouter.ai/qwen/qwen3-max&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://chat.qwen.ai/"&gt;https://chat.qwen.ai/&lt;/a&gt; (qwen 3 max preview)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/nb9wzcl9bdnf1.png?width=1254&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d22747e9fb863b0412d20782dd88e055fbb87a9f"&gt;https://preview.redd.it/nb9wzcl9bdnf1.png?width=1254&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d22747e9fb863b0412d20782dd88e055fbb87a9f&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LeatherRub7248"&gt; /u/LeatherRub7248 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n975er/qwen_3_max/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n975er/qwen_3_max/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n975er/qwen_3_max/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-05T14:42:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1n9kwwr</id>
    <title>New post flair: "local only"</title>
    <updated>2025-09-05T23:51:46+00:00</updated>
    <author>
      <name>/u/ttkciar</name>
      <uri>https://old.reddit.com/user/ttkciar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A new post flair has been created, &amp;quot;local only&amp;quot;. This is intended to help people find discussion about local LLM technology, which is the reason many of us are here.&lt;/p&gt; &lt;p&gt;Please use this flair on new posts to denote:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Your post is about &lt;strong&gt;local&lt;/strong&gt; LLM technology,&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Comments should be focused primarily on &lt;strong&gt;local&lt;/strong&gt; LLM technology.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If your main interest in this subreddit is to read about / discuss local LLM technology, you can filter your view through the &amp;quot;local only&amp;quot; flair &lt;a href="https://www.reddit.com/r/LocalLLaMA/?f=flair_name%3A%22local%20only%22"&gt;like so,&lt;/a&gt; and all of the noise about closed models, API costs, etc will become hidden from view.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ttkciar"&gt; /u/ttkciar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9kwwr/new_post_flair_local_only/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9kwwr/new_post_flair_local_only/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n9kwwr/new_post_flair_local_only/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-05T23:51:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1n9n1qo</id>
    <title>OpenRouter introduces new stealth models with a 2 million context window</title>
    <updated>2025-09-06T01:35:50+00:00</updated>
    <author>
      <name>/u/Outside-Iron-8242</name>
      <uri>https://old.reddit.com/user/Outside-Iron-8242</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9n1qo/openrouter_introduces_new_stealth_models_with_a_2/"&gt; &lt;img alt="OpenRouter introduces new stealth models with a 2 million context window" src="https://preview.redd.it/mvy1r1af7gnf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4500f29aeab7367202cd2301a150d638a8167820" title="OpenRouter introduces new stealth models with a 2 million context window" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Outside-Iron-8242"&gt; /u/Outside-Iron-8242 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/mvy1r1af7gnf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9n1qo/openrouter_introduces_new_stealth_models_with_a_2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n9n1qo/openrouter_introduces_new_stealth_models_with_a_2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-06T01:35:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1n9gfpt</id>
    <title>Anthropic to pay $1.5 billion to authors in landmark AI settlement</title>
    <updated>2025-09-05T20:41:52+00:00</updated>
    <author>
      <name>/u/cpldcpu</name>
      <uri>https://old.reddit.com/user/cpldcpu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9gfpt/anthropic_to_pay_15_billion_to_authors_in/"&gt; &lt;img alt="Anthropic to pay $1.5 billion to authors in landmark AI settlement" src="https://external-preview.redd.it/2giFHQHB-5T6ma6XiIR2StAHVaV1z6nAKhfbARNarkE.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d341db7b1d508270a9cf44051e58699980b97ccb" title="Anthropic to pay $1.5 billion to authors in landmark AI settlement" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cpldcpu"&gt; /u/cpldcpu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.theverge.com/anthropic/773087/anthropic-to-pay-1-5-billion-to-authors-in-landmark-ai-settlement"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9gfpt/anthropic_to_pay_15_billion_to_authors_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n9gfpt/anthropic_to_pay_15_billion_to_authors_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-05T20:41:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7j5z2</id>
    <title>Our 2nd AMA: Hugging Face Science Team, Creators of SmolLM, SmolVLM, and more! (Tomorrow, 8AM-11AM PST)</title>
    <updated>2025-09-03T16:14:51+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7j5z2/our_2nd_ama_hugging_face_science_team_creators_of/"&gt; &lt;img alt="Our 2nd AMA: Hugging Face Science Team, Creators of SmolLM, SmolVLM, and more! (Tomorrow, 8AM-11AM PST)" src="https://preview.redd.it/wdx4ivdw3zmf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=876855c03867ead70389d15b60f24b91d478f835" title="Our 2nd AMA: Hugging Face Science Team, Creators of SmolLM, SmolVLM, and more! (Tomorrow, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wdx4ivdw3zmf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7j5z2/our_2nd_ama_hugging_face_science_team_creators_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7j5z2/our_2nd_ama_hugging_face_science_team_creators_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T16:14:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1n8c3l2</id>
    <title>AMA with Hugging Face Science, the team behind SmolLM, SmolVLM, Fineweb and more.</title>
    <updated>2025-09-04T14:43:01+00:00</updated>
    <author>
      <name>/u/eliebakk</name>
      <uri>https://old.reddit.com/user/eliebakk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8c3l2/ama_with_hugging_face_science_the_team_behind/"&gt; &lt;img alt="AMA with Hugging Face Science, the team behind SmolLM, SmolVLM, Fineweb and more." src="https://external-preview.redd.it/y8IJElEOEd_2568MHNUZQsP7_aRTCAzyzXUKpDJwl1Y.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4e377887ea8d7eae841499cc497b90b82aa97816" title="AMA with Hugging Face Science, the team behind SmolLM, SmolVLM, Fineweb and more." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We're super excited to do this AMA. Come ask your questions to the researchers behind &lt;strong&gt;SmolLM, SmolVLM, FineWeb&lt;/strong&gt;, and more. You can learn more about our work at &lt;a href="http://hf.co/science"&gt;hf.co/science&lt;/a&gt; ü§ó&lt;/p&gt; &lt;p&gt;If you want to get started in ML, a good place is &lt;a href="https://hf.co/learn"&gt;https://hf.co/learn&lt;/a&gt;&lt;/p&gt; &lt;p&gt;To celebrate the AMA, we release a new &lt;strong&gt;FineVision&lt;/strong&gt; dataset, check it out! &lt;a href="https://huggingface.co/datasets/HuggingFaceM4/FineVision"&gt;https://huggingface.co/datasets/HuggingFaceM4/FineVision&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Our participants:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/eliebak"&gt;Elie Bakouch&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/eliebakk"&gt;u/eliebakk&lt;/a&gt; (SmolLM)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/loubnabnl"&gt;Loubna Ben Allal&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/loubnabnl"&gt;u/loubnabnl&lt;/a&gt; (SmolLM)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/nouamanetazi"&gt;Nouamane Tazi&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/Norlax"&gt;u/Norlax&lt;/a&gt;_42 (Nanotron/SmolLM)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/lvwerra"&gt;Leandro von Werra&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/lvwerra"&gt;u/lvwerra&lt;/a&gt; (Head of Research)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/edbeeching"&gt;Edward Beeching&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/edbeeching"&gt;u/edbeeching&lt;/a&gt; (Post Training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/cmpatino"&gt;Carlos Miguel Pati√±o&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/cmpatino"&gt;u/cmpatino&lt;/a&gt;_ (Post Training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/kashif"&gt;Kashif Rasul&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/krasul"&gt;u/krasul&lt;/a&gt; (Post Training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/lewtun"&gt;Lewis Tunstall&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/lewtun"&gt;u/lewtun&lt;/a&gt; (Post Training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/qgallouedec"&gt;Quentin Gallou√©dec&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/qgallouedec"&gt;u/qgallouedec&lt;/a&gt; (Post Training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/clefourrier"&gt;Cl√©mentine Fourrier&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/clefourrier"&gt;u/clefourrier&lt;/a&gt; (Eval)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/SaylorTwift"&gt;Nathan Habib&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/HauntingMoment"&gt;u/HauntingMoment&lt;/a&gt; (Eval)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/lusxvr"&gt;Luis Wiedmann&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/luswd"&gt;u/luswd&lt;/a&gt; (Multimodal)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/andito"&gt;Andres Marafioti&lt;/a&gt;, &lt;a href="/u/futterneid"&gt;u/futterneid&lt;/a&gt; (Multimodal)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/guipenedo"&gt;Guilherme Penedo&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/PhilipsNostrum"&gt;u/PhilipsNostrum&lt;/a&gt; (Data)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/hynky"&gt;Hynek Kydl√≠ƒçek&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/Other"&gt;u/Other&lt;/a&gt;_Housing8453 (Data)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/reach-vb"&gt;Vaibhav Srivastav,&lt;/a&gt; &lt;a href="/u/vaibhavs10"&gt;u/vaibhavs10&lt;/a&gt; (Head of Developer Experience and Community)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/BrigitteTousi"&gt;Brigitte Tousignant&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/BriggieSmalls1992"&gt;u/BriggieSmalls1992&lt;/a&gt; (Comms)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Xenova"&gt;Xenova&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/xenovatech"&gt;u/xenovatech&lt;/a&gt; (Transformers.js)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/craffel"&gt;Colin Raffel&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/craffel"&gt;u/craffel&lt;/a&gt; (Research)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/ngxson"&gt;Xuan Son Nguyen&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/MediocreProgrammer99"&gt;u/MediocreProgrammer99&lt;/a&gt; (llama.cpp)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you are passionate about open source and open science like us, apply at &lt;a href="https://hf.co/jobs"&gt;https://hf.co/jobs&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 8 AM ‚Äì 11 AM PST, with the Hugging Face team continuing to follow up on questions over the next 24 hours.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/o6moshv0u5nf1.png?width=2013&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ee6a9392c3da8651e8a1425264ed855a51b69135"&gt;https://preview.redd.it/o6moshv0u5nf1.png?width=2013&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ee6a9392c3da8651e8a1425264ed855a51b69135&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Thanks everyone for joining our AMA. The live part has ended but we will still answer question async for the next 24h. Follow our &lt;a href="https://hf.co/science"&gt;Hugging Face Science Org&lt;/a&gt; to be aware of our latest release! ü§ó&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eliebakk"&gt; /u/eliebakk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8c3l2/ama_with_hugging_face_science_the_team_behind/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8c3l2/ama_with_hugging_face_science_the_team_behind/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n8c3l2/ama_with_hugging_face_science_the_team_behind/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T14:43:01+00:00</published>
  </entry>
</feed>
