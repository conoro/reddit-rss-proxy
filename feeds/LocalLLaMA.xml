<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-04T15:48:46+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI. Hey, Tony</subtitle>
  <entry>
    <id>t3_1nxt2r2</id>
    <title>Effective context engineering for AI agents by Anthropic</title>
    <updated>2025-10-04T12:47:27+00:00</updated>
    <author>
      <name>/u/Vast_Comedian_9370</name>
      <uri>https://old.reddit.com/user/Vast_Comedian_9370</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxt2r2/effective_context_engineering_for_ai_agents_by/"&gt; &lt;img alt="Effective context engineering for AI agents by Anthropic" src="https://b.thumbs.redditmedia.com/8DlafRLOkFCcnQ62VNCYkbQcmWQV3iEX4J_sWBGs3Ug.jpg" title="Effective context engineering for AI agents by Anthropic" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vast_Comedian_9370"&gt; /u/Vast_Comedian_9370 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nxt2r2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxt2r2/effective_context_engineering_for_ai_agents_by/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxt2r2/effective_context_engineering_for_ai_agents_by/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T12:47:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1nx1ot4</id>
    <title>Qwen3-VL-30B-A3B-Instruct &amp; Thinking (Now Hidden)</title>
    <updated>2025-10-03T15:06:48+00:00</updated>
    <author>
      <name>/u/TKGaming_11</name>
      <uri>https://old.reddit.com/user/TKGaming_11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nx1ot4/qwen3vl30ba3binstruct_thinking_now_hidden/"&gt; &lt;img alt="Qwen3-VL-30B-A3B-Instruct &amp;amp; Thinking (Now Hidden)" src="https://a.thumbs.redditmedia.com/iNETafBex6Qpbyi8P087geXMh_aBmkILehL6E7qn-m4.jpg" title="Qwen3-VL-30B-A3B-Instruct &amp;amp; Thinking (Now Hidden)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TKGaming_11"&gt; /u/TKGaming_11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nx1ot4"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nx1ot4/qwen3vl30ba3binstruct_thinking_now_hidden/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nx1ot4/qwen3vl30ba3binstruct_thinking_now_hidden/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T15:06:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxnq77</id>
    <title>best coding model under 40b parameters? preferably moe</title>
    <updated>2025-10-04T07:37:00+00:00</updated>
    <author>
      <name>/u/Odd-Ordinary-5922</name>
      <uri>https://old.reddit.com/user/Odd-Ordinary-5922</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;preferably moe&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Odd-Ordinary-5922"&gt; /u/Odd-Ordinary-5922 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxnq77/best_coding_model_under_40b_parameters_preferably/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxnq77/best_coding_model_under_40b_parameters_preferably/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxnq77/best_coding_model_under_40b_parameters_preferably/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T07:37:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1nx8e2l</id>
    <title>Is this expected behaviour from Granite 4 32B? (Unsloth Q4XL, no system prompt)</title>
    <updated>2025-10-03T19:16:29+00:00</updated>
    <author>
      <name>/u/IonizedRay</name>
      <uri>https://old.reddit.com/user/IonizedRay</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nx8e2l/is_this_expected_behaviour_from_granite_4_32b/"&gt; &lt;img alt="Is this expected behaviour from Granite 4 32B? (Unsloth Q4XL, no system prompt)" src="https://preview.redd.it/uq9t3il85ysf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=86fe0496ab662fb43abd450fc0e2e5a75018e96b" title="Is this expected behaviour from Granite 4 32B? (Unsloth Q4XL, no system prompt)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IonizedRay"&gt; /u/IonizedRay &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/uq9t3il85ysf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nx8e2l/is_this_expected_behaviour_from_granite_4_32b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nx8e2l/is_this_expected_behaviour_from_granite_4_32b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T19:16:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxbbxe</id>
    <title>GLM 4.6 new best open weight overall on lmarena</title>
    <updated>2025-10-03T21:11:39+00:00</updated>
    <author>
      <name>/u/r3m8sh</name>
      <uri>https://old.reddit.com/user/r3m8sh</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Third on code after Qwen 235b (lmarena isn't agent based). #3 on hard prompts and #1 on creative writing.&lt;/p&gt; &lt;p&gt;Edit : in thinking mode (default).&lt;/p&gt; &lt;p&gt;&lt;a href="https://lmarena.ai/leaderboard/text/overall"&gt;https://lmarena.ai/leaderboard/text/overall&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/r3m8sh"&gt; /u/r3m8sh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxbbxe/glm_46_new_best_open_weight_overall_on_lmarena/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxbbxe/glm_46_new_best_open_weight_overall_on_lmarena/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxbbxe/glm_46_new_best_open_weight_overall_on_lmarena/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T21:11:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1nx18ax</id>
    <title>GLM 4.6 IS A FUKING AMAZING MODEL AND NOBODY CAN TELL ME OTHERWISE</title>
    <updated>2025-10-03T14:49:34+00:00</updated>
    <author>
      <name>/u/boneMechBoy69420</name>
      <uri>https://old.reddit.com/user/boneMechBoy69420</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Especially fuckin artificial analysis and their bullshit ass benchmark &lt;/p&gt; &lt;p&gt;Been using GLM 4.5 it on prod for a month now and I've got nothing but good feedback from the users , it's got way better autonomy than any other proprietary model I've tried (sonnet , gpt 5 and grok code) and it's probably the best ever model for tool call accuracy &lt;/p&gt; &lt;p&gt;One benchmark id recommend yall follow is the berkley function calling benchmark (v4 ig) &lt;a href="https://gorilla.cs.berkeley.edu/leaderboard.html"&gt;bfcl v4&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/boneMechBoy69420"&gt; /u/boneMechBoy69420 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nx18ax/glm_46_is_a_fuking_amazing_model_and_nobody_can/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nx18ax/glm_46_is_a_fuking_amazing_model_and_nobody_can/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nx18ax/glm_46_is_a_fuking_amazing_model_and_nobody_can/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T14:49:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxo3ao</id>
    <title>Best local model for open code?</title>
    <updated>2025-10-04T08:00:16+00:00</updated>
    <author>
      <name>/u/LastCulture3768</name>
      <uri>https://old.reddit.com/user/LastCulture3768</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Which LLM gives you satisfaction for tasks under open code with 12Go vram ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LastCulture3768"&gt; /u/LastCulture3768 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxo3ao/best_local_model_for_open_code/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxo3ao/best_local_model_for_open_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxo3ao/best_local_model_for_open_code/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T08:00:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxtuiy</id>
    <title>Is MLX in itself somehow making the models a little bit different / more "stupid"?</title>
    <updated>2025-10-04T13:21:20+00:00</updated>
    <author>
      <name>/u/CBW1255</name>
      <uri>https://old.reddit.com/user/CBW1255</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have an MBP M4 128GB RAM.&lt;/p&gt; &lt;p&gt;I run LLMs using LMStudio.&lt;br /&gt; I (nearly) always let LMStudio decide on the temp and other params.&lt;/p&gt; &lt;p&gt;I simply load models and use the chat interface or use them directly from code via the local API.&lt;/p&gt; &lt;p&gt;As a Mac user, I tend to go for the MLX versions of models since they are generally faster than GGUF for Macs.&lt;br /&gt; However, I find myself, now and then, testing the GGUF equivalent of the same model and it's slower but very often presents better solutions and is &amp;quot;more exact&amp;quot;.&lt;/p&gt; &lt;p&gt;I'm writing this to see if anyone else is having the same experience?&lt;/p&gt; &lt;p&gt;Please note that there's no &amp;quot;proof&amp;quot; or anything remotely scientific behind this question. It's just my feeling and I wanted to check if some of you who use MLX have witnessed something simliar.&lt;/p&gt; &lt;p&gt;In fact, it could very well be that I'm expected to do / tweak something that I'm not currently doing. Feel free to bring forward suggestions on what I might be doing wrong. Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CBW1255"&gt; /u/CBW1255 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxtuiy/is_mlx_in_itself_somehow_making_the_models_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxtuiy/is_mlx_in_itself_somehow_making_the_models_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxtuiy/is_mlx_in_itself_somehow_making_the_models_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T13:21:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxvlm8</id>
    <title>Comparison between Qwen-Image, HunyuanImage 2.1, HunyuanImage 3.0</title>
    <updated>2025-10-04T14:33:52+00:00</updated>
    <author>
      <name>/u/Severe-Awareness829</name>
      <uri>https://old.reddit.com/user/Severe-Awareness829</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxvlm8/comparison_between_qwenimage_hunyuanimage_21/"&gt; &lt;img alt="Comparison between Qwen-Image, HunyuanImage 2.1, HunyuanImage 3.0" src="https://b.thumbs.redditmedia.com/vScmLJCz-SW6HqFktT-PwqhUvVAzHRTvQosO2HoTFOU.jpg" title="Comparison between Qwen-Image, HunyuanImage 2.1, HunyuanImage 3.0" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Couple of days ago i asked about the difference between the archticture in HunyuanImage 2.1 and HunyuanImage 3.0 and which is better and as you may have geussed nobody helped me. so, i decided to compare between the three myself and this is the results i got.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/1w6bgzguu3tf1.png?width=1355&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4a2f963da35cfb954942e83f650689ada0964261"&gt;https://preview.redd.it/1w6bgzguu3tf1.png?width=1355&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4a2f963da35cfb954942e83f650689ada0964261&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/tq2boe8xu3tf1.png?width=1355&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a15d14c86c89e7989698937e2145cee8aef97770"&gt;https://preview.redd.it/tq2boe8xu3tf1.png?width=1355&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a15d14c86c89e7989698937e2145cee8aef97770&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3ud9zf60v3tf1.png?width=1313&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e40288150bb9aaa070d9c85cee386a25eedaf266"&gt;https://preview.redd.it/3ud9zf60v3tf1.png?width=1313&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e40288150bb9aaa070d9c85cee386a25eedaf266&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7sk97114v3tf1.png?width=1507&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=49870261ef6119681213b414f41243cae2bf567b"&gt;https://preview.redd.it/7sk97114v3tf1.png?width=1507&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=49870261ef6119681213b414f41243cae2bf567b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6e1vr068v3tf1.png?width=1544&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6cfbd2e84d636a685c070a3408a88d48e9b744e5"&gt;https://preview.redd.it/6e1vr068v3tf1.png?width=1544&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6cfbd2e84d636a685c070a3408a88d48e9b744e5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Based on my assessment i would rank them like this:&lt;br /&gt; 1. &lt;strong&gt;HunyuanImage 3.0&lt;/strong&gt;&lt;br /&gt; 2. Qwen-Image,&lt;br /&gt; 3. HunyuanImage 2.1&lt;/p&gt; &lt;p&gt;Hope someone finds this use&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Severe-Awareness829"&gt; /u/Severe-Awareness829 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxvlm8/comparison_between_qwenimage_hunyuanimage_21/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxvlm8/comparison_between_qwenimage_hunyuanimage_21/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxvlm8/comparison_between_qwenimage_hunyuanimage_21/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T14:33:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxnszs</id>
    <title>Where do you think we'll be at for home inference in 2 years?</title>
    <updated>2025-10-04T07:41:51+00:00</updated>
    <author>
      <name>/u/TumbleweedDeep825</name>
      <uri>https://old.reddit.com/user/TumbleweedDeep825</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I suppose we'll never see any big price reduction jumps? Especially with inflation rising globally?&lt;/p&gt; &lt;p&gt;I'd love to be able to have a home SOTA tier model for under $15k. Like GLM 4.6, etc. But wouldn't we all?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TumbleweedDeep825"&gt; /u/TumbleweedDeep825 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxnszs/where_do_you_think_well_be_at_for_home_inference/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxnszs/where_do_you_think_well_be_at_for_home_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxnszs/where_do_you_think_well_be_at_for_home_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T07:41:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxqxtl</id>
    <title>Anyone running llm on their 16GB android phone?</title>
    <updated>2025-10-04T10:58:27+00:00</updated>
    <author>
      <name>/u/Ok_Warning2146</name>
      <uri>https://old.reddit.com/user/Ok_Warning2146</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My 8gb dual channel phone is dying, so I would like buy a 16gb quad channel android phone to run llm.&lt;/p&gt; &lt;p&gt;I am interested in running gemma3-12b-qat-q4_0 on it. &lt;/p&gt; &lt;p&gt;If you have one, can you run it for me on pocketpal or chatterUI and report the performance (t/s for both prompt processing and inference)? Please also report your phone model such that I can link GPU GFLOPS and memory bandwidth to the performance.&lt;/p&gt; &lt;p&gt;Thanks a lot in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Warning2146"&gt; /u/Ok_Warning2146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxqxtl/anyone_running_llm_on_their_16gb_android_phone/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxqxtl/anyone_running_llm_on_their_16gb_android_phone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxqxtl/anyone_running_llm_on_their_16gb_android_phone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T10:58:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxr4gu</id>
    <title>Smartest model to run on 5090?</title>
    <updated>2025-10-04T11:08:10+00:00</updated>
    <author>
      <name>/u/eCityPlannerWannaBe</name>
      <uri>https://old.reddit.com/user/eCityPlannerWannaBe</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What’s the largest model I should run on 5090 for reasoning? E.g. GLM 4.6 - which version is ideal for one 5090?&lt;/p&gt; &lt;p&gt;Thanks. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eCityPlannerWannaBe"&gt; /u/eCityPlannerWannaBe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxr4gu/smartest_model_to_run_on_5090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxr4gu/smartest_model_to_run_on_5090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxr4gu/smartest_model_to_run_on_5090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T11:08:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxjh4c</id>
    <title>GitHub - huawei-csl/SINQ: Welcome to the official repository of SINQ! A novel, fast and high-quality quantization method designed to make any Large Language Model smaller while preserving accuracy.</title>
    <updated>2025-10-04T03:30:30+00:00</updated>
    <author>
      <name>/u/Aiochedolor</name>
      <uri>https://old.reddit.com/user/Aiochedolor</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxjh4c/github_huaweicslsinq_welcome_to_the_official/"&gt; &lt;img alt="GitHub - huawei-csl/SINQ: Welcome to the official repository of SINQ! A novel, fast and high-quality quantization method designed to make any Large Language Model smaller while preserving accuracy." src="https://external-preview.redd.it/yP0CnjxBFJCXTVacHixSvy4H_F7MTnOAVtKcV29Lggk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c413349140863192c8413b0f7b8e7f32ec48822c" title="GitHub - huawei-csl/SINQ: Welcome to the official repository of SINQ! A novel, fast and high-quality quantization method designed to make any Large Language Model smaller while preserving accuracy." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aiochedolor"&gt; /u/Aiochedolor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/huawei-csl/SINQ"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxjh4c/github_huaweicslsinq_welcome_to_the_official/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxjh4c/github_huaweicslsinq_welcome_to_the_official/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T03:30:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxqabe</id>
    <title>Awesome Local LLM Speech-to-Speech Models &amp; Frameworks</title>
    <updated>2025-10-04T10:19:27+00:00</updated>
    <author>
      <name>/u/tleyden</name>
      <uri>https://old.reddit.com/user/tleyden</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxqabe/awesome_local_llm_speechtospeech_models_frameworks/"&gt; &lt;img alt="Awesome Local LLM Speech-to-Speech Models &amp;amp; Frameworks" src="https://external-preview.redd.it/sv2a4YrAVR9g08yOa0AOrBrIqErmKKSQAYIjNTCx_eI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d1770300a09702157f67115321b5768d01848a3a" title="Awesome Local LLM Speech-to-Speech Models &amp;amp; Frameworks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Did some digging into speech-to-speech models/frameworks for a project recently and ended up with a pretty comprehensive list. Figured I'd drop it here in case it helps anyone else avoid going down the same rabbit hole. &lt;/p&gt; &lt;p&gt;What made the cut:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Has &lt;strong&gt;LLM integration&lt;/strong&gt; (built-in or via modules)&lt;/li&gt; &lt;li&gt;Does &lt;strong&gt;full speech-to-speech&lt;/strong&gt; pipeline, not just STT or TTS alone&lt;/li&gt; &lt;li&gt;Works &lt;strong&gt;locally/self-hosted&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Had to trim quite a bit to keep this readable, but the full list with more details is on GitHub at &lt;a href="https://github.com/tleyden/awesome-llm-speech-to-speech"&gt;tleyden/awesome-llm-speech-to-speech&lt;/a&gt;. PRs welcome if you spot anything wrong or missing! &lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;&lt;strong&gt;Project&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;Open Source&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;Type&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;LLM + Tool Calling&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;Platforms&lt;/strong&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/kyutai-labs/unmute"&gt;&lt;strong&gt;Unmute.sh&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;✅ Yes&lt;/td&gt; &lt;td align="left"&gt;Cascading&lt;/td&gt; &lt;td align="left"&gt;Works with any local LLM · Tool calling not yet but planned&lt;/td&gt; &lt;td align="left"&gt;Linux only&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/fixie-ai/ultravox"&gt;&lt;strong&gt;Ultravox (Fixie)&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;✅ MIT&lt;/td&gt; &lt;td align="left"&gt;Hybrid (audio-native LLM + ASR + TTS)&lt;/td&gt; &lt;td align="left"&gt;Uses Llama/Mistral/Gemma · Full tool-calling via backend LLM&lt;/td&gt; &lt;td align="left"&gt;Windows / Linux&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/KoljaB/RealtimeVoiceChat"&gt;&lt;strong&gt;RealtimeVoiceChat&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;✅ MIT&lt;/td&gt; &lt;td align="left"&gt;Cascading&lt;/td&gt; &lt;td align="left"&gt;Pluggable LLM (local or remote) · Likely supports tool calling&lt;/td&gt; &lt;td align="left"&gt;Linux recommended&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/Lex-au/Vocalis"&gt;&lt;strong&gt;Vocalis&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;✅ Apache-2&lt;/td&gt; &lt;td align="left"&gt;Cascading&lt;/td&gt; &lt;td align="left"&gt;Fine-tuned LLaMA-3-8B-Instruct · Tool calling via backend LLM&lt;/td&gt; &lt;td align="left"&gt;macOS / Windows / Linux (runs on Apple Silicon)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.liquid.ai/blog/liquid-foundation-models-v2-our-second-series-of-generative-ai-models?ref=producthunt"&gt;&lt;strong&gt;LFM2&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;✅ Yes&lt;/td&gt; &lt;td align="left"&gt;End-to-End&lt;/td&gt; &lt;td align="left"&gt;Built-in LLM (E2E) · Native tool calling&lt;/td&gt; &lt;td align="left"&gt;Windows / Linux&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/gpt-omni/mini-omni2"&gt;&lt;strong&gt;Mini-omni2&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;✅ MIT&lt;/td&gt; &lt;td align="left"&gt;End-to-End&lt;/td&gt; &lt;td align="left"&gt;Built-in Qwen2 LLM · Tool calling TBD&lt;/td&gt; &lt;td align="left"&gt;Cross-platform&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/pipecat-ai/pipecat"&gt;&lt;strong&gt;Pipecat&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;✅ Yes&lt;/td&gt; &lt;td align="left"&gt;Cascading&lt;/td&gt; &lt;td align="left"&gt;Pluggable LLM, ASR, TTS · Explicit tool-calling support&lt;/td&gt; &lt;td align="left"&gt;Windows / macOS / Linux / iOS / Android&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Notes&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;“Cascading” = modular ASR → LLM → TTS&lt;/li&gt; &lt;li&gt;“E2E” = end-to-end LLM that directly maps speech-to-speech&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tleyden"&gt; /u/tleyden &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/tleyden/awesome-llm-speech-to-speech"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxqabe/awesome_local_llm_speechtospeech_models_frameworks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxqabe/awesome_local_llm_speechtospeech_models_frameworks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T10:19:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxjhnj</id>
    <title>Behold, the jankiest setup ever</title>
    <updated>2025-10-04T03:31:15+00:00</updated>
    <author>
      <name>/u/T-VIRUS999</name>
      <uri>https://old.reddit.com/user/T-VIRUS999</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxjhnj/behold_the_jankiest_setup_ever/"&gt; &lt;img alt="Behold, the jankiest setup ever" src="https://b.thumbs.redditmedia.com/twOOoKU5XbRq6uFRGfXj_XqIEzieTWVvhWE3zg-T_qA.jpg" title="Behold, the jankiest setup ever" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I plan to get an open test bench, after I get my second P40 in a week or two (which will fit nicely on the other side of that fan) &lt;/p&gt; &lt;p&gt;Performance is as shown, Qwen 3 32B Q4 5.9T/sec&lt;/p&gt; &lt;p&gt;The fan is one of those stupidly powerful Delta electronics server fans that pushes out like 250cfm, so I needed to add a PWM controller to slow it down, and it wouldn't run without that giant capacitor, and it's powered by a Li-ion battery instead of the PSU (for now) &lt;/p&gt; &lt;p&gt;It's not stable at all, the whole system BSODs if a program tries to query the GPU while something else is using it (such as if I try to run GPUZ while LM Studio is running), but if only 1 thing touches the GPU at a time, it works &lt;/p&gt; &lt;p&gt;It has a Ryzen 5 5500GT, 16GB of DDR4, a 1000w PSU, a 512GB SSD, and 1 Nvidia P40 (soon to be 2) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/T-VIRUS999"&gt; /u/T-VIRUS999 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nxjhnj"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxjhnj/behold_the_jankiest_setup_ever/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxjhnj/behold_the_jankiest_setup_ever/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T03:31:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxi82t</id>
    <title>Why do private companies release open source models?</title>
    <updated>2025-10-04T02:26:58+00:00</updated>
    <author>
      <name>/u/desudesu15</name>
      <uri>https://old.reddit.com/user/desudesu15</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I love open source models. I feel they are an alternative for general knowledge, and since I started in this world, I stopped paying for subscriptions and started running models locally.&lt;/p&gt; &lt;p&gt;However, I don't understand the business model of companies like OpenAI launching an open source model. &lt;/p&gt; &lt;p&gt;How do they make money by launching an open source model? &lt;/p&gt; &lt;p&gt;Isn't it counterproductive to their subscription model?&lt;/p&gt; &lt;p&gt;Thank you, and forgive my ignorance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/desudesu15"&gt; /u/desudesu15 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxi82t/why_do_private_companies_release_open_source/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxi82t/why_do_private_companies_release_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxi82t/why_do_private_companies_release_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T02:26:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxv3ke</id>
    <title>It's alive!</title>
    <updated>2025-10-04T14:14:11+00:00</updated>
    <author>
      <name>/u/Illustrious-Dot-6888</name>
      <uri>https://old.reddit.com/user/Illustrious-Dot-6888</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxv3ke/its_alive/"&gt; &lt;img alt="It's alive!" src="https://a.thumbs.redditmedia.com/pFXC-TeVXMXtFoQu77PDa1p-WrHJPId5U0wx7KqN1D8.jpg" title="It's alive!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The H in Granite 4.0-h stands for hilarious!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7y03utomr3tf1.png?width=1138&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3aaaf875911e3a123e0651758a1d1a077225178e"&gt;https://preview.redd.it/7y03utomr3tf1.png?width=1138&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3aaaf875911e3a123e0651758a1d1a077225178e&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Illustrious-Dot-6888"&gt; /u/Illustrious-Dot-6888 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxv3ke/its_alive/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxv3ke/its_alive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxv3ke/its_alive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T14:14:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxjzbn</id>
    <title>Distributed Inference over wifi with 8x 3090 egpus performance</title>
    <updated>2025-10-04T03:57:59+00:00</updated>
    <author>
      <name>/u/Only_Situation_4713</name>
      <uri>https://old.reddit.com/user/Only_Situation_4713</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I smoked some really good weed recently and decided it was a good idea to buy more 3090s.&lt;/p&gt; &lt;p&gt;Naturally I didn't want to use a real build with server parts, put 8 3090s in one build on home depot racks? No thanks I'm lazy.&lt;/p&gt; &lt;p&gt;I got 4 3090 egpus from a guy on facebook. He's cool, sold them to me for 650 each with the egpu. &lt;/p&gt; &lt;p&gt;&lt;a href="https://www.gigabyte.com/Graphics-Card/GV-N3090IXEB-24GD"&gt;https://www.gigabyte.com/Graphics-Card/GV-N3090IXEB-24GD&lt;/a&gt; &amp;lt;--- these are the EGPUs&lt;/p&gt; &lt;p&gt;Then I got 4 other random 3090s of different brands and put them in 3 spare Pcs I have lying around.&lt;/p&gt; &lt;p&gt;Node #1&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Z390 Prime&lt;/li&gt; &lt;li&gt;9900K&lt;/li&gt; &lt;li&gt;64gb of DDR4&lt;/li&gt; &lt;li&gt;3090 (duh)&lt;/li&gt; &lt;li&gt;850W.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Node #2&lt;/p&gt; &lt;ul&gt; &lt;li&gt;MSI Unify ITX z690&lt;/li&gt; &lt;li&gt;12400K&lt;/li&gt; &lt;li&gt;64gb of DDR5&lt;/li&gt; &lt;li&gt;3090 (duh) &lt;/li&gt; &lt;li&gt;650W&lt;/li&gt; &lt;li&gt;2X 3090 EGPUs attached&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Node #3 (Host)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Z790 Maximus Hero&lt;/li&gt; &lt;li&gt;13700k&lt;/li&gt; &lt;li&gt;64gb of DDR5&lt;/li&gt; &lt;li&gt;1200W PSU&lt;/li&gt; &lt;li&gt;2x 3090s &lt;/li&gt; &lt;li&gt;2x 3090 EGPUs attached&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I ran all of it over VLLM with Ray to distribute the load. It's connected over Wifi, I got a good router so speed is about only 10% slower than ethernet from across the house. For now it's all pipeline parallel until the parts arrive then I'll do a 2 node system with 4 gpu each.&lt;/p&gt; &lt;p&gt;&lt;a href="https://rog.asus.com/us/networking/rog-rapture-gt-axe16000-model/"&gt;https://rog.asus.com/us/networking/rog-rapture-gt-axe16000-model/&lt;/a&gt; &amp;lt;--- my router(s).&lt;/p&gt; &lt;p&gt;Results:&lt;/p&gt; &lt;p&gt;At 128k context limit running GLM 4.5 Air AWQ 8 bit (that's Q8 for you gguf folks)&lt;/p&gt; &lt;p&gt;I get 5500 tokens/s prompt processing and 24 tokens a second for a 50k~ ish token prompt. &lt;/p&gt; &lt;p&gt;It works great over Roo.&lt;/p&gt; &lt;p&gt;Ray has a very annoying overhead cost so just assume that each system has like 1gb less vram. Running all my node in headless helps alot too.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Only_Situation_4713"&gt; /u/Only_Situation_4713 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxjzbn/distributed_inference_over_wifi_with_8x_3090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxjzbn/distributed_inference_over_wifi_with_8x_3090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxjzbn/distributed_inference_over_wifi_with_8x_3090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T03:57:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxv7x6</id>
    <title>Performance of GLM 4.6 Q3_K_S on 6x MI50</title>
    <updated>2025-10-04T14:18:58+00:00</updated>
    <author>
      <name>/u/MachineZer0</name>
      <uri>https://old.reddit.com/user/MachineZer0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxv7x6/performance_of_glm_46_q3_k_s_on_6x_mi50/"&gt; &lt;img alt="Performance of GLM 4.6 Q3_K_S on 6x MI50" src="https://external-preview.redd.it/JsfroVEhCwDAyGkCRZ2-n87DtLempLcI758DM9LAUaU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=679e856cb0c9baa56ff4650ed899284d38a0b924" title="Performance of GLM 4.6 Q3_K_S on 6x MI50" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Last night I downloaded the latest GLM 4.6 GGUFs from &lt;a href="https://huggingface.co/unsloth/GLM-4.6-GGUF"&gt;unsloth/GLM-4.6-GGUF · Hugging Face&lt;/a&gt;. I chose Q3_K_S since it was the best size allowing for full context on six AMD Instinct MI50 32gb (192gb). I also took the opportunity to download and rebuild the latest llama.cpp. &lt;strong&gt;I was pleasantly surprised by the 38% lift in text generation and over 200% increase in prompt processing over the previous build.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;My questions for the community:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Would a Vulkan build outperform the current rocm-6.3.4 build?&lt;/li&gt; &lt;li&gt;Is my performance optimal given the hardware?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&amp;#8203;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;/llama.cpp.rocm.20050902$ git rev-parse HEAD 3de008208b9b8a33f49f979097a99b4d59e6e521 srv params_from_: Chat format: Content-only slot launch_slot_: id 0 | task 2449 | processing task slot update_slots: id 0 | task 2449 | new prompt, n_ctx_slot = 131072, n_keep = 0, n_prompt_tokens = 2204 slot update_slots: id 0 | task 2449 | kv cache rm [4, end) slot update_slots: id 0 | task 2449 | prompt processing progress, n_past = 2052, n_tokens = 2048, progress = 0.929220 srv log_server_r: request: OPTIONS /v1/chat/completions 192.168.1.147 200 srv params_from_: Chat format: Content-only slot update_slots: id 0 | task 2449 | kv cache rm [2052, end) slot update_slots: id 0 | task 2449 | prompt processing progress, n_past = 2204, n_tokens = 152, progress = 0.998185 slot update_slots: id 0 | task 2449 | prompt done, n_past = 2204, n_tokens = 152 srv log_server_r: request: OPTIONS /v1/chat/completions 192.168.1.147 200 srv params_from_: Chat format: Content-only slot release: id 0 | task 2449 | stop processing: n_past = 2629, truncated = 0 slot print_timing: id 0 | task 2449 | prompt eval time = 111295.11 ms / 2200 tokens ( 50.59 ms per token, 19.77 tokens per second) eval time = 62451.95 ms / 426 tokens ( 146.60 ms per token, 6.82 tokens per second) total time = 173747.06 ms / 2626 tokens slot launch_slot_: id 0 | task 2451 | processing task slot update_slots: id 0 | task 2451 | new prompt, n_ctx_slot = 131072, n_keep = 0, n_prompt_tokens = 2280 srv log_server_r: request: POST /v1/chat/completions 192.168.1.147 200 slot update_slots: id 0 | task 2451 | kv cache rm [7, end) slot update_slots: id 0 | task 2451 | prompt processing progress, n_past = 2055, n_tokens = 2048, progress = 0.898246 slot update_slots: id 0 | task 2451 | kv cache rm [2055, end) slot update_slots: id 0 | task 2451 | prompt processing progress, n_past = 2280, n_tokens = 225, progress = 0.996930 slot update_slots: id 0 | task 2451 | prompt done, n_past = 2280, n_tokens = 225 slot release: id 0 | task 2451 | stop processing: n_past = 2869, truncated = 0 slot print_timing: id 0 | task 2451 | prompt eval time = 117166.76 ms / 2273 tokens ( 51.55 ms per token, 19.40 tokens per second) eval time = 88855.45 ms / 590 tokens ( 150.60 ms per token, 6.64 tokens per second) total time = 206022.21 ms / 2863 tokens slot launch_slot_: id 0 | task 2513 | processing task slot update_slots: id 0 | task 2513 | new prompt, n_ctx_slot = 131072, n_keep = 0, n_prompt_tokens = 2165 srv log_server_r: request: POST /v1/chat/completions 192.168.1.147 200 slot update_slots: id 0 | task 2513 | kv cache rm [8, end) slot update_slots: id 0 | task 2513 | prompt processing progress, n_past = 2056, n_tokens = 2048, progress = 0.945958 slot update_slots: id 0 | task 2513 | kv cache rm [2056, end) slot update_slots: id 0 | task 2513 | prompt processing progress, n_past = 2165, n_tokens = 109, progress = 0.996305 slot update_slots: id 0 | task 2513 | prompt done, n_past = 2165, n_tokens = 109 slot release: id 0 | task 2513 | stop processing: n_past = 2446, truncated = 0 slot print_timing: id 0 | task 2513 | prompt eval time = 109925.11 ms / 2157 tokens ( 50.96 ms per token, 19.62 tokens per second) eval time = 40961.53 ms / 282 tokens ( 145.25 ms per token, 6.88 tokens per second) total time = 150886.64 ms / 2439 tokens &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;-------------------------------------&lt;/p&gt; &lt;pre&gt;&lt;code&gt;/llama.cpp.rocm.20251004$ git rev-parse HEAD 898acba6816ad23b6a9491347d30e7570bffadfd srv params_from_: Chat format: Content-only slot get_availabl: id 0 | task -1 | selected slot by LRU, t_last = -1 slot launch_slot_: id 0 | task 0 | processing task slot update_slots: id 0 | task 0 | new prompt, n_ctx_slot = 131072, n_keep = 0, n_prompt_tokens = 38 slot update_slots: id 0 | task 0 | n_past = 0, memory_seq_rm [0, end) slot update_slots: id 0 | task 0 | prompt processing progress, n_past = 38, n_tokens = 38, progress = 1.000000 slot update_slots: id 0 | task 0 | prompt done, n_past = 38, n_tokens = 38 slot release: id 0 | task 0 | stop processing: n_past = 2851, truncated = 0 slot print_timing: id 0 | task 0 | prompt eval time = 4300.19 ms / 38 tokens ( 113.16 ms per token, 8.84 tokens per second) eval time = 323842.83 ms / 2814 tokens ( 115.08 ms per token, 8.69 tokens per second) total time = 328143.02 ms / 2852 tokens srv update_slots: all slots are idle srv log_server_r: request: POST /v1/chat/completions 192.168.1.147 200 srv log_server_r: request: OPTIONS /v1/chat/completions 192.168.1.147 200 srv params_from_: Chat format: Content-only slot get_availabl: id 0 | task 0 | selected slot by LRU, t_last = 2724371263681 slot launch_slot_: id 0 | task 2815 | processing task slot update_slots: id 0 | task 2815 | new prompt, n_ctx_slot = 131072, n_keep = 0, n_prompt_tokens = 1734 slot update_slots: id 0 | task 2815 | n_past = 4, memory_seq_rm [4, end) slot update_slots: id 0 | task 2815 | prompt processing progress, n_past = 1734, n_tokens = 1730, progress = 0.997693 slot update_slots: id 0 | task 2815 | prompt done, n_past = 1734, n_tokens = 1730 srv log_server_r: request: OPTIONS /v1/chat/completions 192.168.1.147 200 srv params_from_: Chat format: Content-only slot release: id 0 | task 2815 | stop processing: n_past = 2331, truncated = 0 slot print_timing: id 0 | task 2815 | prompt eval time = 27189.85 ms / 1730 tokens ( 15.72 ms per token, 63.63 tokens per second) eval time = 70550.21 ms / 598 tokens ( 117.98 ms per token, 8.48 tokens per second) total time = 97740.06 ms / 2328 tokens slot get_availabl: id 0 | task 2815 | selected slot by LRU, t_last = 2724469122645 slot launch_slot_: id 0 | task 3096 | processing task slot update_slots: id 0 | task 3096 | new prompt, n_ctx_slot = 131072, n_keep = 0, n_prompt_tokens = 1810 srv log_server_r: request: POST /v1/chat/completions 192.168.1.147 200 slot update_slots: id 0 | task 3096 | n_past = 7, memory_seq_rm [7, end) slot update_slots: id 0 | task 3096 | prompt processing progress, n_past = 1810, n_tokens = 1803, progress = 0.996133 slot update_slots: id 0 | task 3096 | prompt done, n_past = 1810, n_tokens = 1803 srv log_server_r: request: OPTIONS /v1/chat/completions 192.168.1.147 200 srv params_from_: Chat format: Content-only slot release: id 0 | task 3096 | stop processing: n_past = 2434, truncated = 0 slot print_timing: id 0 | task 3096 | prompt eval time = 27702.48 ms / 1803 tokens ( 15.36 ms per token, 65.08 tokens per second) eval time = 74080.73 ms / 625 tokens ( 118.53 ms per token, 8.44 tokens per second) total time = 101783.21 ms / 2428 tokens slot get_availabl: id 0 | task 3096 | selected slot by LRU, t_last = 2724570907348 slot launch_slot_: id 0 | task 3416 | processing task slot update_slots: id 0 | task 3416 | new prompt, n_ctx_slot = 131072, n_keep = 0, n_prompt_tokens = 1695 srv log_server_r: request: POST /v1/chat/completions 192.168.1.147 200 slot update_slots: id 0 | task 3416 | n_past = 8, memory_seq_rm [8, end) slot update_slots: id 0 | task 3416 | prompt processing progress, n_past = 1695, n_tokens = 1687, progress = 0.995280 slot update_slots: id 0 | task 3416 | prompt done, n_past = 1695, n_tokens = 1687 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;-------------------------------------&lt;/p&gt; &lt;p&gt;Command:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;~/llama.cpp.rocm.20251004/build/bin/llama-server --model ~/models/GLM-4.6-Q3_K_S-00001-of-00004.gguf --cache-type-k q8_0 --cache-type-v q8_0 --n-gpu-layers 94 --temp 0.6 --ctx-size 131072 --device ROCm0,ROCm1,ROCm2,ROCm3,ROCm4,ROCm5 --tensor-split 9,8,8,8,9,8 --host 0.0.0.0 --jinja --alias GLM-4.6 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/hyiod0epr3tf1.png?width=965&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f0e2313e04763913efe8f0c15436c59981e3e0af"&gt;https://preview.redd.it/hyiod0epr3tf1.png?width=965&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f0e2313e04763913efe8f0c15436c59981e3e0af&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MachineZer0"&gt; /u/MachineZer0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxv7x6/performance_of_glm_46_q3_k_s_on_6x_mi50/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxv7x6/performance_of_glm_46_q3_k_s_on_6x_mi50/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxv7x6/performance_of_glm_46_q3_k_s_on_6x_mi50/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T14:18:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwx1rx</id>
    <title>The most important AI paper of the decade. No debate</title>
    <updated>2025-10-03T11:55:32+00:00</updated>
    <author>
      <name>/u/PumpkinNarrow6339</name>
      <uri>https://old.reddit.com/user/PumpkinNarrow6339</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwx1rx/the_most_important_ai_paper_of_the_decade_no/"&gt; &lt;img alt="The most important AI paper of the decade. No debate" src="https://preview.redd.it/d2rcvb6nyvsf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0051a67e9886e507e2b0a35679f4d469050fda91" title="The most important AI paper of the decade. No debate" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PumpkinNarrow6339"&gt; /u/PumpkinNarrow6339 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/d2rcvb6nyvsf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwx1rx/the_most_important_ai_paper_of_the_decade_no/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwx1rx/the_most_important_ai_paper_of_the_decade_no/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T11:55:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxs8tr</id>
    <title>GLM 4.6 Makes Incredible Front End Design with 2 prompts</title>
    <updated>2025-10-04T12:08:05+00:00</updated>
    <author>
      <name>/u/dev_is_active</name>
      <uri>https://old.reddit.com/user/dev_is_active</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxs8tr/glm_46_makes_incredible_front_end_design_with_2/"&gt; &lt;img alt="GLM 4.6 Makes Incredible Front End Design with 2 prompts" src="https://external-preview.redd.it/HRx3NTzzZMOIdtM0oRmpT2rIW9OnDaS9AE7D0C1FPSc.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9d08204e2ea136e84ee75e08ffa737f0e7653aea" title="GLM 4.6 Makes Incredible Front End Design with 2 prompts" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I've been playing with GLM 4.6, I've also implemented it inside Claud Code, and I'll be doing a new video on how to set up GLM 4.6 in Cloud Code, but I really wanted to show everybody how great z ai is with front end design.&lt;/p&gt; &lt;p&gt;In this video I take a screenshot of a website and I do one simple prompt and it kicks out a good design and then I ask it to enhance it, and then it turns it into an incredible design, you can watch it here&lt;/p&gt; &lt;p&gt;Would love to know what you think and if any of you are using GLM in Claude Code yet?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dev_is_active"&gt; /u/dev_is_active &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/AvHsytH-K84"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxs8tr/glm_46_makes_incredible_front_end_design_with_2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxs8tr/glm_46_makes_incredible_front_end_design_with_2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T12:08:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxrssl</id>
    <title>This is pretty cool</title>
    <updated>2025-10-04T11:45:15+00:00</updated>
    <author>
      <name>/u/wowsers7</name>
      <uri>https://old.reddit.com/user/wowsers7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxrssl/this_is_pretty_cool/"&gt; &lt;img alt="This is pretty cool" src="https://external-preview.redd.it/fWjpQVd5VjUiyz85oEUZ3MBMlQycdcPTlMPYbRWoE6A.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f573aa728fb79af617ec9e24df900618595c6abb" title="This is pretty cool" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://venturebeat.com/ai/huaweis-new-open-source-technique-shrinks-llms-to-make-them-run-on-less"&gt;https://venturebeat.com/ai/huaweis-new-open-source-technique-shrinks-llms-to-make-them-run-on-less&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/huawei-csl/SINQ/blob/main/README.md"&gt;https://github.com/huawei-csl/SINQ/blob/main/README.md&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wowsers7"&gt; /u/wowsers7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/huawei-csl/SINQ/blob/main/README.md"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxrssl/this_is_pretty_cool/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxrssl/this_is_pretty_cool/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T11:45:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxhfcq</id>
    <title>Qwen3-VL-30B-A3B-Instruct &amp; Thinking are here</title>
    <updated>2025-10-04T01:46:34+00:00</updated>
    <author>
      <name>/u/AlanzhuLy</name>
      <uri>https://old.reddit.com/user/AlanzhuLy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxhfcq/qwen3vl30ba3binstruct_thinking_are_here/"&gt; &lt;img alt="Qwen3-VL-30B-A3B-Instruct &amp;amp; Thinking are here" src="https://external-preview.redd.it/NiG8S3h1NHl5GO5rtLVQY3YBzVNQNV4xvZR0hFbk_vE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=88770649ad1f1c425c3a22e1502363d18f9727dc" title="Qwen3-VL-30B-A3B-Instruct &amp;amp; Thinking are here" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/xwkuqkkt20tf1.png?width=2994&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=16a4068b96a7c20f55817cc29987345c287c76a7"&gt;https://preview.redd.it/xwkuqkkt20tf1.png?width=2994&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=16a4068b96a7c20f55817cc29987345c287c76a7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-VL-30B-A3B-Instruct"&gt;https://huggingface.co/Qwen/Qwen3-VL-30B-A3B-Instruct&lt;/a&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/Qwen/Qwen3-VL-30B-A3B-Thinking"&gt;https://huggingface.co/Qwen/Qwen3-VL-30B-A3B-Thinking&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlanzhuLy"&gt; /u/AlanzhuLy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxhfcq/qwen3vl30ba3binstruct_thinking_are_here/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxhfcq/qwen3vl30ba3binstruct_thinking_are_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxhfcq/qwen3vl30ba3binstruct_thinking_are_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T01:46:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxshw2</id>
    <title>IBM granite 4.0-h-tiny leads the way for extra small MoEs</title>
    <updated>2025-10-04T12:20:23+00:00</updated>
    <author>
      <name>/u/GreenTreeAndBlueSky</name>
      <uri>https://old.reddit.com/user/GreenTreeAndBlueSky</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxshw2/ibm_granite_40htiny_leads_the_way_for_extra_small/"&gt; &lt;img alt="IBM granite 4.0-h-tiny leads the way for extra small MoEs" src="https://preview.redd.it/nlkf3btz73tf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1368a9b2fc469f876a31fef05020119815deb818" title="IBM granite 4.0-h-tiny leads the way for extra small MoEs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I hope the trend for those MoEs carries on. Normies with laverage laptops will soon be able to use decent models with little ressources. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GreenTreeAndBlueSky"&gt; /u/GreenTreeAndBlueSky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nlkf3btz73tf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxshw2/ibm_granite_40htiny_leads_the_way_for_extra_small/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxshw2/ibm_granite_40htiny_leads_the_way_for_extra_small/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T12:20:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxw08b</id>
    <title>Quite accurate</title>
    <updated>2025-10-04T14:50:10+00:00</updated>
    <author>
      <name>/u/Komarov_d</name>
      <uri>https://old.reddit.com/user/Komarov_d</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxw08b/quite_accurate/"&gt; &lt;img alt="Quite accurate" src="https://preview.redd.it/9lms1idpy3tf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e7e75e0118f3c3580e1e97fa7a4bda42ad4520a7" title="Quite accurate" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Komarov_d"&gt; /u/Komarov_d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9lms1idpy3tf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxw08b/quite_accurate/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxw08b/quite_accurate/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T14:50:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvryo4</id>
    <title>AMA Announcement: Prime Intellect — The Open‑Source Distributed Training Lab (Thu, Oct 2 • 10 AM – 1 PM PDT)</title>
    <updated>2025-10-02T02:31:01+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"&gt; &lt;img alt="AMA Announcement: Prime Intellect — The Open‑Source Distributed Training Lab (Thu, Oct 2 • 10 AM – 1 PM PDT)" src="https://preview.redd.it/222kj50x0msf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c1c747c42690f74b8d08690dcdbb1ef23aece13b" title="AMA Announcement: Prime Intellect — The Open‑Source Distributed Training Lab (Thu, Oct 2 • 10 AM – 1 PM PDT)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/222kj50x0msf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T02:31:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwaoyd</id>
    <title>AMA with Prime Intellect — Ask Us Anything!</title>
    <updated>2025-10-02T17:47:44+00:00</updated>
    <author>
      <name>/u/kindacognizant</name>
      <uri>https://old.reddit.com/user/kindacognizant</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;AMA with Prime Intellect — Ask Us Anything!&lt;/h1&gt; &lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;! We’re excited for this AMA, thank you for having us.&lt;/p&gt; &lt;p&gt;I’m Kalomaze (&lt;a href="/u/kindacognizant"&gt;u/kindacognizant&lt;/a&gt;), a researcher at Prime Intellect, the lab behind:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Distributed training &lt;a href="https://www.primeintellect.ai/#research"&gt;efforts&lt;/a&gt; including INTELLECT-1 + INTELLECT-2&lt;/li&gt; &lt;li&gt;Open-source RL efforts including &lt;a href="https://github.com/PrimeIntellect-ai/verifiers"&gt;verifiers&lt;/a&gt;, &lt;a href="https://github.com/PrimeIntellect-ai/prime-rl"&gt;prime-rl&lt;/a&gt;, and the &lt;a href="https://app.primeintellect.ai/dashboard/environments"&gt;Environments Hub&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Our other participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Sami Jaghouar, &lt;a href="/u/samsja19"&gt;u/samsja19&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Will Brown, &lt;a href="/u/willccbb"&gt;u/willccbb&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Jack Min Ong, &lt;a href="/u/Cinamic"&gt;u/Cinamic&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Mika Senghaas, &lt;a href="/u/mikasenghaas"&gt;u/mikasenghaas&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 11:00 AM – 2:00 PM PST, with the Prime Intellect team continuing to follow up on questions over the next 48 hours.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kindacognizant"&gt; /u/kindacognizant &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwaoyd/ama_with_prime_intellect_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwaoyd/ama_with_prime_intellect_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwaoyd/ama_with_prime_intellect_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T17:47:44+00:00</published>
  </entry>
</feed>
