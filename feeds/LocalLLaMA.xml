<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-06-25T21:06:09+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1lkgc4d</id>
    <title>Models that are good and fast at Long Document Processing</title>
    <updated>2025-06-25T20:18:34+00:00</updated>
    <author>
      <name>/u/themegadinesen</name>
      <uri>https://old.reddit.com/user/themegadinesen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have recently been using Gemini 2.5 Flash Lite on OpenRouter with my workflow (long jsons, with around 60k tokens, but the files are then split into 6k chunks to make the processing faster and to stay in the context lengths) and i have been somehwat satisfied so far, especially with the around 500 tk/s speed, but it's obiously not perfect.&lt;/p&gt; &lt;p&gt;I know the question is somewhat broad, but is there anything that is as good, or better that I could self host? What kind of hardware would I be looking at if i want it to be as fast, if not faster, than the 500 tk/s from OR? I need to selfhost since the data i will be working with is senstive.&lt;/p&gt; &lt;p&gt;I have tried Qwen 2.5 VL 32B (it scored good on this leaderboard &lt;a href="https://idp-leaderboard.org/#longdocbench"&gt;https://idp-leaderboard.org/#longdocbench&lt;/a&gt;) and it is very good so far (have not used it as much) but its incredibly slow at 50tk/s. What took me 5mins with Gemini is taking around 30 mins now. What kind of hardware would i need to run it fast, and serve around 20-50 people (assuming we are using vLLM)?&lt;/p&gt; &lt;p&gt;I would prefer new cards, because this would be used in a buisness setting and i would prefer to have waranty on the them. But the budget is not infinite, so buying a few H100s is not in the picture atm.&lt;br /&gt; Also, let me know if ive been using the wrong models, im kind of a dumbass at this. Thanks a lot guys!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/themegadinesen"&gt; /u/themegadinesen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkgc4d/models_that_are_good_and_fast_at_long_document/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkgc4d/models_that_are_good_and_fast_at_long_document/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lkgc4d/models_that_are_good_and_fast_at_long_document/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-25T20:18:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1lk4vbo</id>
    <title>Could anyone get UI-TARS Desktop running locally?</title>
    <updated>2025-06-25T12:53:03+00:00</updated>
    <author>
      <name>/u/m_abdelfattah</name>
      <uri>https://old.reddit.com/user/m_abdelfattah</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;While using Ollama or LM Studios for &lt;a href="https://huggingface.co/ByteDance-Seed/UI-TARS-1.5-7B"&gt;UI-TARS-1.5-7B&lt;/a&gt; inference.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/m_abdelfattah"&gt; /u/m_abdelfattah &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lk4vbo/could_anyone_get_uitars_desktop_running_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lk4vbo/could_anyone_get_uitars_desktop_running_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lk4vbo/could_anyone_get_uitars_desktop_running_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-25T12:53:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1lkgxdn</id>
    <title>Domain Specific Leaderboard based Model Registry</title>
    <updated>2025-06-25T20:41:18+00:00</updated>
    <author>
      <name>/u/Suspicious_Demand_26</name>
      <uri>https://old.reddit.com/user/Suspicious_Demand_26</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Wondering if people also have trouble with finding the best model for their use case/domain, since HuggingFace doesn‚Äôt really focus on a pure leaderboard style and all the benchmarking is done from model providers themselves.&lt;/p&gt; &lt;p&gt;Feels like that would actually make open source a lot more accessible to normal people if they can easily find a model thats great for their use case without having to do extensive research or independent testing&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Suspicious_Demand_26"&gt; /u/Suspicious_Demand_26 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkgxdn/domain_specific_leaderboard_based_model_registry/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkgxdn/domain_specific_leaderboard_based_model_registry/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lkgxdn/domain_specific_leaderboard_based_model_registry/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-25T20:41:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1lk6dub</id>
    <title>Which gemma-3 (12b and 27b) version (Unsloth, Bartowski, stduhpf, Dampfinchen, QAT, non-QAT, etc) are you using/do you prefer?</title>
    <updated>2025-06-25T13:57:18+00:00</updated>
    <author>
      <name>/u/relmny</name>
      <uri>https://old.reddit.com/user/relmny</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Lately I started using different versions of Qwen-3 (I used to use the Unsloth UD ones, but recently I started moving* to the non-UD ones or the Bartowski ones instead, as I get more t/s and more context) and I was considering the same for Gemma-3.&lt;br /&gt; But between what I was reading from comments and my own tests, and I'm confused.&lt;/p&gt; &lt;p&gt;I remember the Bartowski, Unsloth, stduhpf, Dampfinchen, QAT, no-QAT... and reading people complaining about QAT or saying how great it is, adds to the confusion.&lt;/p&gt; &lt;p&gt;So, which version are you using and, if you don't mind, why? (I'm currently using the Unsloth UD ones).&lt;/p&gt; &lt;p&gt;*Which I recently started to think that might be based on the different &amp;quot;Precision&amp;quot; values of the tensors, but is something I have no idea about and I still need to look at.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/relmny"&gt; /u/relmny &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lk6dub/which_gemma3_12b_and_27b_version_unsloth/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lk6dub/which_gemma3_12b_and_27b_version_unsloth/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lk6dub/which_gemma3_12b_and_27b_version_unsloth/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-25T13:57:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljr1wn</id>
    <title>Where is OpenAI's open source model?</title>
    <updated>2025-06-24T23:57:16+00:00</updated>
    <author>
      <name>/u/_Vedr</name>
      <uri>https://old.reddit.com/user/_Vedr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Did I miss something?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_Vedr"&gt; /u/_Vedr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljr1wn/where_is_openais_open_source_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljr1wn/where_is_openais_open_source_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljr1wn/where_is_openais_open_source_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T23:57:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljnhca</id>
    <title>Made an LLM Client for the PS Vita</title>
    <updated>2025-06-24T21:24:23+00:00</updated>
    <author>
      <name>/u/ajunior7</name>
      <uri>https://old.reddit.com/user/ajunior7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljnhca/made_an_llm_client_for_the_ps_vita/"&gt; &lt;img alt="Made an LLM Client for the PS Vita" src="https://external-preview.redd.it/Y283aGV6aXd6eDhmMfIP8BrPficmhyY5KB42Ptrwyms9E-ke6lpIPgzOipjX.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=40daff1e17d68cd71479175d661e93123af22f55" title="Made an LLM Client for the PS Vita" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello all, awhile back I had ported llama2.c on the PS Vita for on-device inference using the TinyStories 260K &amp;amp; 15M checkpoints. Was a cool and fun concept to work on, but it wasn't too practical in the end.&lt;/p&gt; &lt;p&gt;Since then, I have made a full fledged LLM client for the Vita instead! You can even use the camera to take photos to send to models that support vision. In this demo I gave it an endpoint to test out vision and reasoning models, and I'm happy with how it all turned out. It isn't perfect, as LLMs like to display messages in fancy ways like using TeX and markdown formatting, so it shows that in its raw text. The Vita can't even do emojis!&lt;/p&gt; &lt;p&gt;You can download the vpk in the releases section of my repo. Throw in an endpoint and try it yourself! (If using an API key, I hope you are very patient in typing that out manually)&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/callbacked/vela"&gt;https://github.com/callbacked/vela&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ajunior7"&gt; /u/ajunior7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/qunyr1jwzx8f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljnhca/made_an_llm_client_for_the_ps_vita/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljnhca/made_an_llm_client_for_the_ps_vita/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T21:24:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljs95d</id>
    <title>ThermoAsk: getting an LLM to set its own temperature</title>
    <updated>2025-06-25T00:54:24+00:00</updated>
    <author>
      <name>/u/tycho_brahes_nose_</name>
      <uri>https://old.reddit.com/user/tycho_brahes_nose_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljs95d/thermoask_getting_an_llm_to_set_its_own/"&gt; &lt;img alt="ThermoAsk: getting an LLM to set its own temperature" src="https://preview.redd.it/t8az5arc1z8f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=40ab4b4271e74985945a33ea726d1e36e0b0897b" title="ThermoAsk: getting an LLM to set its own temperature" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I got an LLM to dynamically adjust its own sampling temperature.&lt;/p&gt; &lt;p&gt;I wrote a blog post on how I did this and why dynamic temperature adjustment might be a valuable ability for a language model to possess: &lt;a href="http://amanvir.com/blog/getting-an-llm-to-set-its-own-temperature"&gt;amanvir.com/blog/getting-an-llm-to-set-its-own-temperature&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;: LLMs can struggle with prompts that inherently require large changes in sampling temperature for sensible or accurate responses. This includes simple prompts like &amp;quot;pick a random number from &amp;lt;some range&amp;gt;&amp;quot; and more complex stuff like:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Solve the following math expression: &amp;quot;1 + 5 * 3 - 4 / 2&amp;quot;. Then, write a really abstract poem that contains the answer to this expression.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Tackling these prompts with a &amp;quot;default&amp;quot; temperature value will not lead to good responses. To solve this problem, I had the idea of allowing LLMs to request changes to their own temperature based on the task they were dealing with. To my knowledge, this is the first time such a system has been proposed, so I thought I'd use the opportunity to give this technique a name: &lt;strong&gt;ThermoAsk&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;I've created a basic implementation of ThermoAsk that relies on Ollama's Python SDK and Qwen2.5-7B: &lt;a href="https://github.com/amanvirparhar/thermoask"&gt;github.com/amanvirparhar/thermoask&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;I'd love to hear your thoughts on this approach!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tycho_brahes_nose_"&gt; /u/tycho_brahes_nose_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/t8az5arc1z8f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljs95d/thermoask_getting_an_llm_to_set_its_own/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljs95d/thermoask_getting_an_llm_to_set_its_own/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-25T00:54:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1lkgayx</id>
    <title>Methods to Analyze Spreadsheets</title>
    <updated>2025-06-25T20:17:13+00:00</updated>
    <author>
      <name>/u/MiyamotoMusashi7</name>
      <uri>https://old.reddit.com/user/MiyamotoMusashi7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am trying to analyze larger csv files and spreadsheets with local llms and am curious what you all think are the best methods. I am currently leaning toward one of the following:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;SQL Code Execution&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Python Pandas Code Execution (method used by Gemini)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Pandas AI Querying&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I have experimented with passing sheets as json and markdown files with little success.&lt;/p&gt; &lt;p&gt;So, what are your preferred methods? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MiyamotoMusashi7"&gt; /u/MiyamotoMusashi7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkgayx/methods_to_analyze_spreadsheets/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkgayx/methods_to_analyze_spreadsheets/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lkgayx/methods_to_analyze_spreadsheets/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-25T20:17:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1lkh3og</id>
    <title>Introducing: The New BS Benchmark</title>
    <updated>2025-06-25T20:48:12+00:00</updated>
    <author>
      <name>/u/Turdbender3k</name>
      <uri>https://old.reddit.com/user/Turdbender3k</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkh3og/introducing_the_new_bs_benchmark/"&gt; &lt;img alt="Introducing: The New BS Benchmark" src="https://preview.redd.it/4b2ufnhcy49f1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cfd8525d5b8c8bc0411893fe54cdd82fd4431a59" title="Introducing: The New BS Benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;is there a bs detector benchmark?^^ what if we can create questions that defy any logic just to bait the llm into a bs answer?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Turdbender3k"&gt; /u/Turdbender3k &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4b2ufnhcy49f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkh3og/introducing_the_new_bs_benchmark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lkh3og/introducing_the_new_bs_benchmark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-25T20:48:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1lkf8jq</id>
    <title>4√ó RTX 3080 10 GB server for LLM/RAG ‚Äì is this even worth it?</title>
    <updated>2025-06-25T19:35:12+00:00</updated>
    <author>
      <name>/u/OkAssumption9049</name>
      <uri>https://old.reddit.com/user/OkAssumption9049</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks&lt;/p&gt; &lt;p&gt;A while back I picked up 4√ó NVIDIA GeForce RTX 3080 10 GB cards and now I‚Äôm toying with the idea of building a home server for local LLM inference and possibly RAG. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;What I‚Äôve got so far:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;4√ó RTX 3080 10 GB&lt;/li&gt; &lt;li&gt;AIO liquid cooling + extra 140 mm fans&lt;/li&gt; &lt;li&gt;1600 W 80 PLUS Titanium PSU&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The hurdle:&lt;/strong&gt;&lt;br /&gt; Finding an mobo with &lt;strong&gt;4√ó PCIe 4.0 x16 (electrically x16/x16/x8/x8)&lt;/strong&gt;‚Äîmost TRX40/WRX80 boards only give full x16 wiring on the first two slots. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Boards I‚Äôm eyeing:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;ASUS Prime TRX40-Pro (x16/x16/x8/x8, ECC)&lt;/li&gt; &lt;li&gt;Gigabyte TRX40 AORUS PRO WiFi&lt;/li&gt; &lt;li&gt;MSI TRX40 PRO 10G&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Questions for you:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Anyone run 4√ó3080s&lt;/strong&gt; for LLMs (Deepspeed, vLLM, HF Accelerate)? Can you actually scale inference across 4√ó10 GB cards?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Any mobo recs?&lt;/strong&gt; I‚Äôd prefer stable power delivery and slot spacing that doesn‚Äôt require crazy risers.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Is this whole build even worth it&lt;/strong&gt; for 7‚Äì13 B models + RAG, or should I just go for a beefy single card (e.g. 4080/4090) or dedicated Tensor-core hardware?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;TIA for any insights or war stories! üôèüèª&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OkAssumption9049"&gt; /u/OkAssumption9049 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkf8jq/4_rtx_3080_10_gb_server_for_llmrag_is_this_even/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkf8jq/4_rtx_3080_10_gb_server_for_llmrag_is_this_even/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lkf8jq/4_rtx_3080_10_gb_server_for_llmrag_is_this_even/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-25T19:35:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1lk2swu</id>
    <title>What are the best 70b tier models/finetunes? (That fit into 48gb these days)</title>
    <updated>2025-06-25T11:08:58+00:00</updated>
    <author>
      <name>/u/DepthHour1669</name>
      <uri>https://old.reddit.com/user/DepthHour1669</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's been a while since llama 3.3 came out. &lt;/p&gt; &lt;p&gt;Are there any real improvements in the 70b area? That size is interesting since it can fit into 48gb aka 2x 3090 very well when quantized. &lt;/p&gt; &lt;p&gt;Anything that beats Qwen 3 32b? &lt;/p&gt; &lt;p&gt;From what I can tell, the Qwen 3 models are cutting edge for general purpose use running locally, with Gemma 3 27b, Mistral Small 3.2, Deepseek-R1-0528-Qwen3-8b being notable exceptions that punch above Qwen 3 (30b or 32b) for some workloads. Are there any other models that beat these? I presume Llama 3.3 70b is too old now. &lt;/p&gt; &lt;p&gt;Any finetunes of 70b or 72b models that I should be aware of, similar to Deepseek's finetunes?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DepthHour1669"&gt; /u/DepthHour1669 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lk2swu/what_are_the_best_70b_tier_modelsfinetunes_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lk2swu/what_are_the_best_70b_tier_modelsfinetunes_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lk2swu/what_are_the_best_70b_tier_modelsfinetunes_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-25T11:08:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljpo64</id>
    <title>I gave the same silly task to ~70 models that fit on 32GB of VRAM - thousands of times (resharing my post from /r/LocalLLM)</title>
    <updated>2025-06-24T22:56:20+00:00</updated>
    <author>
      <name>/u/EmPips</name>
      <uri>https://old.reddit.com/user/EmPips</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'd posted this over at &lt;a href="/r/LocalLLM"&gt;/r/LocalLLM&lt;/a&gt; and Some people thought I presented this too much as serious research - it wasn't, it was much closer to a bored rainy day activity. So here's the post I've been waiting to make on &lt;a href="/r/LocalLLaMA"&gt;/r/LocalLLaMA&lt;/a&gt; for some time, simplified as casually as possible:&lt;/p&gt; &lt;p&gt;Quick recap - &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbfinu/26_quants_that_fit_on_32gb_vs_10000token_needle/"&gt;here is the original post&lt;/a&gt; from a few weeks ago where users suggested I greatly expand the scope of this little game. &lt;a href="https://old.reddit.com/r/LocalLLM/comments/1liy7ku/i_thousands_of_tests_on_104_different_ggufs_10k/"&gt;Here is the post on /r/LocalLLM&lt;/a&gt; yesterday that I imagine some of you saw. I hope you don't mind the cross-post - but &lt;em&gt;THIS&lt;/em&gt; is the subreddit that I really wanted to bounce this off of and yesterday it was going through a change-of-management :-)&lt;/p&gt; &lt;p&gt;To be as brief/casual as possible: I broke HG Well's &lt;em&gt;&amp;quot;The Time Machine&amp;quot;&lt;/em&gt; again with a sentence that was correct English, but contextually nonsense, and asked a bunch of quantized LLM's (all that fit with 16k context on 32GB of VRAM). I did this multiple times at all temperatures from 0.0 to 0.9 in steps of 0.1 . For models with optional reasoning I split thinking mode on and off.&lt;/p&gt; &lt;h2&gt;What should you take from this?&lt;/h2&gt; &lt;p&gt;nothing at all! I'm hoping to get a better feel for how quantization works on some of my favorite models, so will take a little thing I do during my day and repeat it thousands and thousands of times to see if patterns emerge. I share this dataset with you for fun. I have my takeaways, I'd be interested to hear yours. My biggest takeaway from this is that I built a little framework of scripts for myself that will run and evaluate these sorts of tests at whatever scale I set them to.&lt;/p&gt; &lt;h2&gt;The Results&lt;/h2&gt; &lt;p&gt;Without further ado, the results. The 'Score' column is a percentage of correct answers.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;Quant&lt;/th&gt; &lt;th&gt;Reasoning&lt;/th&gt; &lt;th&gt;Score&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Meta Llama Family&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama_3.2_3B&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama_3.2_3B&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama_3.2_3B&lt;/td&gt; &lt;td&gt;q6&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama_3.1_8B_Instruct&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;43&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama_3.1_8B_Instruct&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;13&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama_3.1_8B_Instruct&lt;/td&gt; &lt;td&gt;q6&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;10&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama_3.3_70B_Instruct&lt;/td&gt; &lt;td&gt;iq1&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;13&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama_3.3_70B_Instruct&lt;/td&gt; &lt;td&gt;iq2&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;100&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama_3.3_70B_Instruct&lt;/td&gt; &lt;td&gt;iq3&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;100&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama_4_Scout_17B&lt;/td&gt; &lt;td&gt;iq1&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;93&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama_4_Scout_17B&lt;/td&gt; &lt;td&gt;iq2&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;13&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Nvidia Nemotron Family&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama_3.1_Nemotron_8B_UltraLong&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;60&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama_3.1_Nemotron_8B_UltraLong&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;67&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama_3.3_Nemotron_Super_49B&lt;/td&gt; &lt;td&gt;iq2&lt;/td&gt; &lt;td&gt;nothink&lt;/td&gt; &lt;td&gt;93&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama_3.3_Nemotron_Super_49B&lt;/td&gt; &lt;td&gt;iq2&lt;/td&gt; &lt;td&gt;thinking&lt;/td&gt; &lt;td&gt;80&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama_3.3_Nemotron_Super_49B&lt;/td&gt; &lt;td&gt;iq3&lt;/td&gt; &lt;td&gt;thinking&lt;/td&gt; &lt;td&gt;100&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama_3.3_Nemotron_Super_49B&lt;/td&gt; &lt;td&gt;iq3&lt;/td&gt; &lt;td&gt;nothink&lt;/td&gt; &lt;td&gt;93&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama_3.3_Nemotron_Super_49B&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;thinking&lt;/td&gt; &lt;td&gt;97&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama_3.3_Nemotron_Super_49B&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;nothink&lt;/td&gt; &lt;td&gt;93&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Mistral Family&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Mistral_Small_24B_2503&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;50&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Mistral_Small_24B_2503&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;83&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Mistral_Small_24B_2503&lt;/td&gt; &lt;td&gt;q6&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;77&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Microsoft Phi Family&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Phi_4&lt;/td&gt; &lt;td&gt;iq3&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;7&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Phi_4&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;7&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Phi_4&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;20&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Phi_4&lt;/td&gt; &lt;td&gt;q6&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;13&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Alibaba Qwen Family&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5_14B_Instruct&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;93&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5_14B_Instruct&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;97&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5_14B_Instruct&lt;/td&gt; &lt;td&gt;q6&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;97&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5_Coder_32B&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5_Coder_32B_Instruct&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;QwQ_32B&lt;/td&gt; &lt;td&gt;iq2&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;57&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;QwQ_32B&lt;/td&gt; &lt;td&gt;iq3&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;100&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;QwQ_32B&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;67&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;QwQ_32B&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;83&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;QwQ_32B&lt;/td&gt; &lt;td&gt;q6&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;87&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_14B&lt;/td&gt; &lt;td&gt;iq3&lt;/td&gt; &lt;td&gt;thinking&lt;/td&gt; &lt;td&gt;77&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_14B&lt;/td&gt; &lt;td&gt;iq3&lt;/td&gt; &lt;td&gt;nothink&lt;/td&gt; &lt;td&gt;60&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_14B&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;thinking&lt;/td&gt; &lt;td&gt;77&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_14B&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;nothink&lt;/td&gt; &lt;td&gt;100&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_14B&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;nothink&lt;/td&gt; &lt;td&gt;97&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_14B&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;thinking&lt;/td&gt; &lt;td&gt;77&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_14B&lt;/td&gt; &lt;td&gt;q6&lt;/td&gt; &lt;td&gt;nothink&lt;/td&gt; &lt;td&gt;100&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_14B&lt;/td&gt; &lt;td&gt;q6&lt;/td&gt; &lt;td&gt;thinking&lt;/td&gt; &lt;td&gt;77&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_30B_A3B&lt;/td&gt; &lt;td&gt;iq3&lt;/td&gt; &lt;td&gt;thinking&lt;/td&gt; &lt;td&gt;7&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_30B_A3B&lt;/td&gt; &lt;td&gt;iq3&lt;/td&gt; &lt;td&gt;nothink&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_30B_A3B&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;thinking&lt;/td&gt; &lt;td&gt;60&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_30B_A3B&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;nothink&lt;/td&gt; &lt;td&gt;47&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_30B_A3B&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;nothink&lt;/td&gt; &lt;td&gt;37&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_30B_A3B&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;thinking&lt;/td&gt; &lt;td&gt;40&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_30B_A3B&lt;/td&gt; &lt;td&gt;q6&lt;/td&gt; &lt;td&gt;thinking&lt;/td&gt; &lt;td&gt;53&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_30B_A3B&lt;/td&gt; &lt;td&gt;q6&lt;/td&gt; &lt;td&gt;nothink&lt;/td&gt; &lt;td&gt;20&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_30B_A6B_16_Extreme&lt;/td&gt; &lt;td&gt;q4&lt;/td&gt; &lt;td&gt;nothink&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_30B_A6B_16_Extreme&lt;/td&gt; &lt;td&gt;q4&lt;/td&gt; &lt;td&gt;thinking&lt;/td&gt; &lt;td&gt;3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_30B_A6B_16_Extreme&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;thinking&lt;/td&gt; &lt;td&gt;63&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_30B_A6B_16_Extreme&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;nothink&lt;/td&gt; &lt;td&gt;20&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_32B&lt;/td&gt; &lt;td&gt;iq3&lt;/td&gt; &lt;td&gt;thinking&lt;/td&gt; &lt;td&gt;63&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_32B&lt;/td&gt; &lt;td&gt;iq3&lt;/td&gt; &lt;td&gt;nothink&lt;/td&gt; &lt;td&gt;60&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_32B&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;nothink&lt;/td&gt; &lt;td&gt;93&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_32B&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;thinking&lt;/td&gt; &lt;td&gt;80&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_32B&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;thinking&lt;/td&gt; &lt;td&gt;80&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_32B&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;nothink&lt;/td&gt; &lt;td&gt;87&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Google Gemma Family&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Gemma_3_12B_IT&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Gemma_3_12B_IT&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Gemma_3_12B_IT&lt;/td&gt; &lt;td&gt;q6&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Gemma_3_27B_IT&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Gemma_3_27B_IT&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Gemma_3_27B_IT&lt;/td&gt; &lt;td&gt;q6&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Deepseek (Distill) Family&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepSeek_R1_Qwen3_8B&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;17&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepSeek_R1_Qwen3_8B&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepSeek_R1_Qwen3_8B&lt;/td&gt; &lt;td&gt;q6&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepSeek_R1_Distill_Qwen_32B&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;37&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepSeek_R1_Distill_Qwen_32B&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;20&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepSeek_R1_Distill_Qwen_32B&lt;/td&gt; &lt;td&gt;q6&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;30&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Other&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Cogito&lt;em&gt;v1_Preview&lt;/em&gt;&lt;em&gt;Qwen_14B&lt;/em&gt;&lt;/td&gt; &lt;td&gt;iq3&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Cogito&lt;em&gt;v1_Preview&lt;/em&gt;&lt;em&gt;Qwen_14B&lt;/em&gt;&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;13&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Cogito&lt;em&gt;v1_Preview&lt;/em&gt;&lt;em&gt;Qwen_14B&lt;/em&gt;&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepHermes_3_Mistral_24B_Preview&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;nothink&lt;/td&gt; &lt;td&gt;3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepHermes_3_Mistral_24B_Preview&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;thinking&lt;/td&gt; &lt;td&gt;7&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepHermes_3_Mistral_24B_Preview&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;thinking&lt;/td&gt; &lt;td&gt;37&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepHermes_3_Mistral_24B_Preview&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;nothink&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepHermes_3_Mistral_24B_Preview&lt;/td&gt; &lt;td&gt;q6&lt;/td&gt; &lt;td&gt;thinking&lt;/td&gt; &lt;td&gt;30&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepHermes_3_Mistral_24B_Preview&lt;/td&gt; &lt;td&gt;q6&lt;/td&gt; &lt;td&gt;nothink&lt;/td&gt; &lt;td&gt;3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;GLM_4_32B&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;10&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;GLM_4_32B&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;17&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;GLM_4_32B&lt;/td&gt; &lt;td&gt;q6&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;16&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EmPips"&gt; /u/EmPips &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljpo64/i_gave_the_same_silly_task_to_70_models_that_fit/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljpo64/i_gave_the_same_silly_task_to_70_models_that_fit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljpo64/i_gave_the_same_silly_task_to_70_models_that_fit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T22:56:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1lkau4z</id>
    <title>üöÄ Revamped My Dungeon AI GUI Project ‚Äì Now with a Clean Interface &amp; Better Usability!</title>
    <updated>2025-06-25T16:48:13+00:00</updated>
    <author>
      <name>/u/Reasonable_Brief578</name>
      <uri>https://old.reddit.com/user/Reasonable_Brief578</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkau4z/revamped_my_dungeon_ai_gui_project_now_with_a/"&gt; &lt;img alt="üöÄ Revamped My Dungeon AI GUI Project ‚Äì Now with a Clean Interface &amp;amp; Better Usability!" src="https://external-preview.redd.it/NpzXevyc8hccQld5KSdg19C9gOjRKnxTSGT0NaYuRD8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=429f3f424c02aa8038222f2c0d7e58d883b099aa" title="üöÄ Revamped My Dungeon AI GUI Project ‚Äì Now with a Clean Interface &amp;amp; Better Usability!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://i.redd.it/20q3drcnr39f1.gif"&gt;https://i.redd.it/20q3drcnr39f1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hey folks!&lt;br /&gt; I just gave my old project &lt;a href="https://github.com/Laszlobeer/Dungeo_ai"&gt;Dungeo_ai&lt;/a&gt; a serious upgrade and wanted to share the improved version:&lt;br /&gt; üîó &lt;a href="https://github.com/Laszlobeer/Dungeo_ai_GUI"&gt;&lt;strong&gt;Dungeo_ai_GUI on GitHub&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is a &lt;strong&gt;local, GUI-based Dungeon Master AI&lt;/strong&gt; designed to let you roleplay solo DnD-style adventures using your own LLM (like a local LLaMA model via Ollama). The original project was CLI-based and clunky, but now it‚Äôs been reworked with:&lt;/p&gt; &lt;p&gt;üß† &lt;strong&gt;Improvements&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;üñ•Ô∏è &lt;strong&gt;User-friendly GU&lt;/strong&gt;I using &lt;code&gt;tkinte&lt;/code&gt;r&lt;/li&gt; &lt;li&gt;üéÆ More immersive roleplay support&lt;/li&gt; &lt;li&gt;üíæ Easy save/load system for sessions&lt;/li&gt; &lt;li&gt;üõ†Ô∏è Cleaner codebase and better modularity for community mods&lt;/li&gt; &lt;li&gt;üß© Simple integration with local LLM APIs (e.g. Ollama, LM Studio)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;üß™ Currently testing with local models like &lt;strong&gt;LLaMA 3 8B/13&lt;/strong&gt;B, and performance is smooth even on mid-range hardware.&lt;/p&gt; &lt;p&gt;If you‚Äôre into solo RPGs, interactive storytelling, or just want to tinker with AI-powered DMs, I‚Äôd love your feedback or contributions!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Try it, break it, or fork it:&lt;/strong&gt;&lt;br /&gt; üëâ &lt;a href="https://github.com/Laszlobeer/Dungeo_ai_GUI"&gt;https://github.com/Laszlobeer/Dungeo_ai_GUI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy dungeon delving! üêâ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reasonable_Brief578"&gt; /u/Reasonable_Brief578 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkau4z/revamped_my_dungeon_ai_gui_project_now_with_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkau4z/revamped_my_dungeon_ai_gui_project_now_with_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lkau4z/revamped_my_dungeon_ai_gui_project_now_with_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-25T16:48:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1lkb0r2</id>
    <title>Day 3 of 50 Days of Building a Small Language Model from Scratch: Building Our First Tokenizer from Scratch</title>
    <updated>2025-06-25T16:55:08+00:00</updated>
    <author>
      <name>/u/Prashant-Lakhera</name>
      <uri>https://old.reddit.com/user/Prashant-Lakhera</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkb0r2/day_3_of_50_days_of_building_a_small_language/"&gt; &lt;img alt="Day 3 of 50 Days of Building a Small Language Model from Scratch: Building Our First Tokenizer from Scratch" src="https://b.thumbs.redditmedia.com/OeQy-jWX3TCF-p9-4YnfSGgWfimWViDqH3yLv18B8lE.jpg" title="Day 3 of 50 Days of Building a Small Language Model from Scratch: Building Our First Tokenizer from Scratch" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/kholdb4xs39f1.png?width=1362&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2ae7d640500ff5c315f621794267908d87ddcdb5"&gt;https://preview.redd.it/kholdb4xs39f1.png?width=1362&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2ae7d640500ff5c315f621794267908d87ddcdb5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;Yesterday, I explained what a tokenizer is and why it's essential for language models. Today, I rolled up my sleeves and built a basic tokenizer from scratch, using nothing more than Python and regular expressions.&lt;/p&gt; &lt;p&gt;Here's what I covered:&lt;/p&gt; &lt;h1&gt;Step-by-step Breakdown:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Split text&lt;/strong&gt; using &lt;code&gt;.split()&lt;/code&gt; and &lt;code&gt;re.split()&lt;/code&gt; to handle whitespace, punctuation, and special symbols.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Assign unique IDs&lt;/strong&gt; to each token by creating a vocabulary dictionary.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Build a&lt;/strong&gt; &lt;code&gt;BasicTokenizer&lt;/code&gt; &lt;strong&gt;class&lt;/strong&gt; with &lt;code&gt;encode()&lt;/code&gt; and &lt;code&gt;decode()&lt;/code&gt; methods to convert between text and token IDs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Add support for unknown tokens (&lt;/strong&gt;&lt;code&gt;&amp;lt;|unk|&amp;gt;&lt;/code&gt;&lt;strong&gt;)&lt;/strong&gt; and sequence separators (&lt;code&gt;&amp;lt;|endoftext|&amp;gt;&lt;/code&gt;).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Tested limitations&lt;/strong&gt; by feeding new unseen sentences (like &lt;code&gt;&amp;quot;Hello, how are you?&amp;quot;&lt;/code&gt;) and seeing only known tokens get encoded.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Key Insight:&lt;/h1&gt; &lt;p&gt;A tokenizer built only on known vocabulary will fail on unseen words. That‚Äôs where special tokens and advanced techniques like Byte Pair Encoding (BPE) come in, which is what I'll be diving into tomorrow.&lt;/p&gt; &lt;p&gt;If you're curious how models like GPT handle &lt;em&gt;misspelled or unknown words&lt;/em&gt;, this tokenizer project is a great way to understand it from the ground up.&lt;/p&gt; &lt;p&gt;üìñ Full breakdown with code and examples here:&lt;br /&gt; üëâ &lt;a href="https://www.ideaweaver.ai/blog/day3.html"&gt;https://www.ideaweaver.ai/blog/day3.html&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prashant-Lakhera"&gt; /u/Prashant-Lakhera &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkb0r2/day_3_of_50_days_of_building_a_small_language/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkb0r2/day_3_of_50_days_of_building_a_small_language/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lkb0r2/day_3_of_50_days_of_building_a_small_language/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-25T16:55:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljm3pb</id>
    <title>LocalLlama is saved!</title>
    <updated>2025-06-24T20:30:08+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;LocalLlama has been many folk's favorite place to be for everything AI, so it's good to see a new moderator taking the reins!&lt;/p&gt; &lt;p&gt;Thanks to &lt;a href="/u/HOLUPREDICTIONS"&gt;u/HOLUPREDICTIONS&lt;/a&gt; for taking the reins!&lt;/p&gt; &lt;p&gt;More detail here: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ljlr5b/subreddit_back_in_business/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1ljlr5b/subreddit_back_in_business/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;TLDR - the previous moderator (we appreciate their work) unfortunately left the subreddit, and unfortunately deleted new comments and posts - it's now lifted!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljm3pb/localllama_is_saved/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljm3pb/localllama_is_saved/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljm3pb/localllama_is_saved/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T20:30:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljlr5b</id>
    <title>Subreddit back in business</title>
    <updated>2025-06-24T20:16:36+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljlr5b/subreddit_back_in_business/"&gt; &lt;img alt="Subreddit back in business" src="https://preview.redd.it/1sx7mwusnx8f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a3f5a6313e8a4b034a44e79151a371760d959973" title="Subreddit back in business" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As most of you folks I'm also not sure what happened but I'm attaching screenshot of the last actions taken by the previous moderator before deleting their account &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1sx7mwusnx8f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljlr5b/subreddit_back_in_business/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljlr5b/subreddit_back_in_business/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T20:16:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljxa2e</id>
    <title>Gemini CLI: your open-source AI agent</title>
    <updated>2025-06-25T05:18:00+00:00</updated>
    <author>
      <name>/u/adefa</name>
      <uri>https://old.reddit.com/user/adefa</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljxa2e/gemini_cli_your_opensource_ai_agent/"&gt; &lt;img alt="Gemini CLI: your open-source AI agent" src="https://external-preview.redd.it/v_nU-59VjAFg3tUf3ktH0OR1eDLLCpt7sTIO-4lpiic.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=66dc977cf68889558dd1e0a18ef318dff22dc727" title="Gemini CLI: your open-source AI agent" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Really generous free tier&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/adefa"&gt; /u/adefa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://blog.google/technology/developers/introducing-gemini-cli/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljxa2e/gemini_cli_your_opensource_ai_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljxa2e/gemini_cli_your_opensource_ai_agent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-25T05:18:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1lkcpk4</id>
    <title>MCP in LM Studio</title>
    <updated>2025-06-25T17:58:34+00:00</updated>
    <author>
      <name>/u/vibjelo</name>
      <uri>https://old.reddit.com/user/vibjelo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkcpk4/mcp_in_lm_studio/"&gt; &lt;img alt="MCP in LM Studio" src="https://external-preview.redd.it/xgG5hj5Fs1PBuG048NliXZrJKETHuOiQipJujsnBkY8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=387668f505f292a153588da355a3524c7291548b" title="MCP in LM Studio" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vibjelo"&gt; /u/vibjelo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://lmstudio.ai/blog/lmstudio-v0.3.17"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkcpk4/mcp_in_lm_studio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lkcpk4/mcp_in_lm_studio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-25T17:58:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1lk40ac</id>
    <title>Hunyuan-A13B</title>
    <updated>2025-06-25T12:12:27+00:00</updated>
    <author>
      <name>/u/lly0571</name>
      <uri>https://old.reddit.com/user/lly0571</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/tencent/Hunyuan-A13B-Instruct-FP8"&gt;https://huggingface.co/tencent/Hunyuan-A13B-Instruct-FP8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I think the model should be a ~80B MoE. As 3072x4096x3x(64+1)*32 = 78.5B, and there are embedding layers and gating parts.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lly0571"&gt; /u/lly0571 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lk40ac/hunyuana13b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lk40ac/hunyuana13b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lk40ac/hunyuana13b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-25T12:12:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1lk63od</id>
    <title>Gemini CLI: your open-source AI agent</title>
    <updated>2025-06-25T13:45:52+00:00</updated>
    <author>
      <name>/u/touhidul002</name>
      <uri>https://old.reddit.com/user/touhidul002</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lk63od/gemini_cli_your_opensource_ai_agent/"&gt; &lt;img alt="Gemini CLI: your open-source AI agent" src="https://external-preview.redd.it/v_nU-59VjAFg3tUf3ktH0OR1eDLLCpt7sTIO-4lpiic.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=66dc977cf68889558dd1e0a18ef318dff22dc727" title="Gemini CLI: your open-source AI agent" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Free license gets you access to Gemini 2.5 Pro and its massive 1 million token context window. To ensure you rarely, if ever, hit a limit during this preview, we offer the industry‚Äôs largest allowance: 60 model requests per minute and 1,000 requests per day at no charge.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/touhidul002"&gt; /u/touhidul002 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://blog.google/technology/developers/introducing-gemini-cli-open-source-ai-agent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lk63od/gemini_cli_your_opensource_ai_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lk63od/gemini_cli_your_opensource_ai_agent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-25T13:45:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1lk9ime</id>
    <title>Cydonia 24B v3.1 - Just another RP tune (with some thinking!)</title>
    <updated>2025-06-25T15:59:23+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lk9ime/cydonia_24b_v31_just_another_rp_tune_with_some/"&gt; &lt;img alt="Cydonia 24B v3.1 - Just another RP tune (with some thinking!)" src="https://external-preview.redd.it/is5dxEtYQGcop66xpu9863OAeD17dNWUu8NQ03Wo_4I.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8faa3dfb676a1ca98bc1f6a3369598de92ef4fba" title="Cydonia 24B v3.1 - Just another RP tune (with some thinking!)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Serious Note: This was really scheduled to be released today... Such awkward timing!&lt;/p&gt; &lt;p&gt;This official release incorporated Magistral weights through merging. It is able to think thanks to that. &lt;a href="https://huggingface.co/BeaverAI/Cydonia-24B-v3k-GGUF"&gt;Cydonia 24B v3k&lt;/a&gt; is a proper Magistral tune but not thoroughly tested.&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;No claims of superb performance. No fake engagements of any sort (At least I hope not. Please feel free to delete comments / downvote the post if you think it's artificially inflated). No weird sycophants.&lt;/p&gt; &lt;p&gt;Just a moistened up Mistral 24B 3.1, a little dumb but quite fun and easy to use! Finetuned to &lt;em&gt;hopefully&lt;/em&gt; specialize on one single task: Your Enjoyment.&lt;/p&gt; &lt;p&gt;Enjoy!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Cydonia-24B-v3.1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lk9ime/cydonia_24b_v31_just_another_rp_tune_with_some/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lk9ime/cydonia_24b_v31_just_another_rp_tune_with_some/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-25T15:59:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1lk12th</id>
    <title>New Mistral Small 3.2 actually feels like something big. [non-reasoning]</title>
    <updated>2025-06-25T09:26:23+00:00</updated>
    <author>
      <name>/u/Snail_Inference</name>
      <uri>https://old.reddit.com/user/Snail_Inference</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lk12th/new_mistral_small_32_actually_feels_like/"&gt; &lt;img alt="New Mistral Small 3.2 actually feels like something big. [non-reasoning]" src="https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=47f397e4a22ed5ec7e82aad070eb446319603abc" title="New Mistral Small 3.2 actually feels like something big. [non-reasoning]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/1wwakei8k19f1.png?width=1009&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fb72a4bf78efba7661e6ea5f54df70331a15539b"&gt;https://preview.redd.it/1wwakei8k19f1.png?width=1009&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fb72a4bf78efba7661e6ea5f54df70331a15539b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In my experience, it ranges far above its size.&lt;/p&gt; &lt;p&gt;Source: &lt;a href="http://artificialanalysis.ai"&gt;artificialanalysis.ai&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Snail_Inference"&gt; /u/Snail_Inference &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lk12th/new_mistral_small_32_actually_feels_like/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lk12th/new_mistral_small_32_actually_feels_like/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lk12th/new_mistral_small_32_actually_feels_like/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-25T09:26:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljyo2p</id>
    <title>Jan-nano-128k: A 4B Model with a Super-Long Context Window (Still Outperforms 671B)</title>
    <updated>2025-06-25T06:44:26+00:00</updated>
    <author>
      <name>/u/Kooky-Somewhere-2883</name>
      <uri>https://old.reddit.com/user/Kooky-Somewhere-2883</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljyo2p/jannano128k_a_4b_model_with_a_superlong_context/"&gt; &lt;img alt="Jan-nano-128k: A 4B Model with a Super-Long Context Window (Still Outperforms 671B)" src="https://external-preview.redd.it/MDRyeGJ6bmJvMDlmMdx7LrexgFcEoZTqX8Yp_PzSREeGDqUB-Qd2XY93v_7d.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c71ebd7c03a7ccb476c3ff52d6b9e5cc00e65722" title="Jan-nano-128k: A 4B Model with a Super-Long Context Window (Still Outperforms 671B)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone it's me from Menlo Research again,&lt;/p&gt; &lt;p&gt;Today, I'd like to introduce our latest model: &lt;strong&gt;Jan-nano-128k&lt;/strong&gt; - this model is fine-tuned on &lt;strong&gt;Jan-nano&lt;/strong&gt; (which is a qwen3 finetune), improve performance when enable YaRN scaling &lt;strong&gt;(instead of having degraded performance)&lt;/strong&gt;.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It can uses tools continuously, repeatedly. &lt;/li&gt; &lt;li&gt;It can perform deep research &lt;strong&gt;VERY VERY DEEP&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Extremely persistence (please pick the right MCP as well)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Again, we are not trying to beat Deepseek-671B models, we just want to see how far this current model can go. To our surprise, &lt;strong&gt;it is going very very far.&lt;/strong&gt; Another thing, we have spent all the resource on this version of Jan-nano so.... &lt;/p&gt; &lt;p&gt;&lt;strong&gt;We pushed back the technical report release! But it's coming ...sooon!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;You can find the model at:&lt;br /&gt; &lt;a href="https://huggingface.co/Menlo/Jan-nano-128k"&gt;https://huggingface.co/Menlo/Jan-nano-128k&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We also have gguf at:&lt;br /&gt; &lt;strong&gt;We are converting the GGUF check in comment section&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This model will require &lt;strong&gt;YaRN Scaling&lt;/strong&gt; supported from inference engine, we already configure it in the model, but your inference engine will need to be able to handle YaRN scaling. Please run the model in l&lt;strong&gt;lama.server or Jan app&lt;/strong&gt; (these are from our team, we tested them, just it).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Result:&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;&lt;strong&gt;SimpleQA:&lt;/strong&gt;&lt;br /&gt; - OpenAI o1: 42.6&lt;br /&gt; - Grok 3: 44.6&lt;br /&gt; - 03: 49.4&lt;br /&gt; - Claude-3.7-Sonnet: 50.0&lt;br /&gt; - Gemini-2.5 pro: 52.9&lt;br /&gt; &lt;strong&gt;- baseline-with-MCP: 59.2&lt;/strong&gt;&lt;br /&gt; - ChatGPT-4.5: 62.5&lt;br /&gt; &lt;strong&gt;- deepseek-671B-with-MCP: 78.2&lt;/strong&gt; (we benchmark using openrouter)&lt;br /&gt; - jan-nano-v0.4-with-MCP: 80.7&lt;br /&gt; &lt;strong&gt;- jan-nano-128k-with-MCP: 83.2&lt;/strong&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kooky-Somewhere-2883"&gt; /u/Kooky-Somewhere-2883 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/909kwwnbo09f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljyo2p/jannano128k_a_4b_model_with_a_superlong_context/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljyo2p/jannano128k_a_4b_model_with_a_superlong_context/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-25T06:44:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1lkc5mr</id>
    <title>LM Studio now supports MCP!</title>
    <updated>2025-06-25T17:37:55+00:00</updated>
    <author>
      <name>/u/No_Conversation9561</name>
      <uri>https://old.reddit.com/user/No_Conversation9561</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Read the announcement: &lt;/p&gt; &lt;p&gt;&lt;a href="https://lmstudio.ai/blog/mcp"&gt;lmstudio.ai/blog/mcp&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Conversation9561"&gt; /u/No_Conversation9561 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkc5mr/lm_studio_now_supports_mcp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkc5mr/lm_studio_now_supports_mcp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lkc5mr/lm_studio_now_supports_mcp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-25T17:37:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1lkbiva</id>
    <title>Gemini released an Open Source CLI Tool similar to Claude Code but with a free 1 million token context window, 60 model requests per minute and 1,000 requests per day at no charge.</title>
    <updated>2025-06-25T17:13:56+00:00</updated>
    <author>
      <name>/u/SilverRegion9394</name>
      <uri>https://old.reddit.com/user/SilverRegion9394</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkbiva/gemini_released_an_open_source_cli_tool_similar/"&gt; &lt;img alt="Gemini released an Open Source CLI Tool similar to Claude Code but with a free 1 million token context window, 60 model requests per minute and 1,000 requests per day at no charge." src="https://preview.redd.it/11rgwmzvv39f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d7039783722436b51c07b3fedff7d641b7b004cd" title="Gemini released an Open Source CLI Tool similar to Claude Code but with a free 1 million token context window, 60 model requests per minute and 1,000 requests per day at no charge." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SilverRegion9394"&gt; /u/SilverRegion9394 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/11rgwmzvv39f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkbiva/gemini_released_an_open_source_cli_tool_similar/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lkbiva/gemini_released_an_open_source_cli_tool_similar/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-25T17:13:56+00:00</published>
  </entry>
</feed>
